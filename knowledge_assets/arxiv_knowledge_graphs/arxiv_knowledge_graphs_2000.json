[
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05240v1",
            "title": "A new method for instrumental profile reconstruction of high resolution\n  spectrographs",
            "updated": "2023-11-09T09:47:16Z",
            "published": "2023-11-09T09:47:16Z",
            "summary": "Knowledge of the spectrograph's instrumental profile (IP) provides important\ninformation needed for wavelength calibration and for the use in scientific\nanalyses. This work develops new methods for IP reconstruction in high\nresolution spectrographs equipped with Laser Frequency Comb calibration (LFC)\nsystems and assesses the impact that assumptions on IP shape have on achieving\naccurate spectroscopic measurements. Astronomical LFCs produce $\\approx10000$\nbright, unresolved emission lines with known wavelengths, making them excellent\nprobes of the IP. New methods based on Gaussian Process regression were\ndeveloped to extract detailed information on the IP shape from this data.\nApplying them to HARPS, an extremely stable spectrograph installed on the ESO\n3.6m telescope, we reconstructed its IP at 512 locations of the detector,\ncovering 60% of the total detector area. We found that the HARPS IP is\nasymmetric and that it varies smoothly across the detector. Empirical IP models\nprovide wavelength accuracy better than 10 ms$^{-1}$ (5 ms$^{-1}$) with 92%\n(64%) probability. In comparison, reaching the same accuracy has a probability\nof only 29% (8%) when a Gaussian IP shape is assumed. Furthermore, the Gaussian\nassumption is associated with intra-order and inter-order distortions in the\nHARPS wavelength scale as large as 60ms$^{-1}$. The spatial distribution of\nthese distortions suggests they may be related to spectrograph optics and\ntherefore may generally appear in cross-dispersed echelle spectrographs when\nGaussian IPs are used. Methods presented here can be applied to other\ninstruments equipped with LFCs, such as ESPRESSO, but also ANDES and G-CLEF in\nthe future. The empirical IPs will be crucial for obtaining objective and\nunbiased measurements of fundamental constants from high resolution spectra, as\nwell as measurements of the redshift drift, isotopic abundances, and other\nscience cases.",
            "author": [
                "Dinko Milakovi\u0107",
                "Prashin Jetwha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05240v1",
                "http://arxiv.org/pdf/2311.05240v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05231v1",
            "title": "An optimal chromatic bound for the class of $\\{P_3\\cup\n  2K_1,\\overline{P_3\\cup 2K_1}\\}$-free graphs",
            "updated": "2023-11-09T09:18:56Z",
            "published": "2023-11-09T09:18:56Z",
            "summary": "In 1987, A. Gy\\'arf\\'as in his paper ``Problems from the world surrounding\nperfect graphs'' posed the problem of determining the smallest $\\chi$-binding\nfunction for $\\mathcal{G}(F,\\overline{F})$, when $\\mathcal{G}(F)$ is\n$\\chi$-bounded. So far the problem has been attempted for only forest $F$ with\nfour or five vertices. In this paper, we address the case when $F=P_3\\cup 2K_1$\nand show that if $G$ is a $\\{P_3\\cup 2K_1,\\overline{P_3\\cup 2K_1}\\}$-free graph\nwith $\\omega(G)\\neq 3$, then it admits $\\omega(G)+1$ as a $\\chi$-binding\nfunction. Moreover, we also construct examples to show that this bound is tight\nfor all values of $\\omega\\neq 3$.",
            "author": [
                "Athmakoori Prashant",
                "S. Francis Raj"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05231v1",
                "http://arxiv.org/pdf/2311.05231v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05229v2",
            "title": "Mean Field Games in a Stackelberg problem with an informed major player",
            "updated": "2023-11-25T06:13:14Z",
            "published": "2023-11-09T09:16:45Z",
            "summary": "We investigate a stochastic differential game in which a major player has a\nprivate information (the knowledge of a random variable), which she discloses\nthrough her control to a population of small players playing in a Nash Mean\nField Game equilibrium. The major player's cost depends on the distribution of\nthe population, while the cost of the population depends on the random variable\nknown by the major player. We show that the game has a relaxed solution and\nthat the optimal control of the major player is approximatively optimal in\ngames with a large but finite number of small players.",
            "author": [
                "Philippe Bergault",
                "Pierre Cardaliaguet",
                "Catherine Rainer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05229v2",
                "http://arxiv.org/pdf/2311.05229v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05199v1",
            "title": "BrainNetDiff: Generative AI Empowers Brain Network Generation via\n  Multimodal Diffusion Model",
            "updated": "2023-11-09T08:27:12Z",
            "published": "2023-11-09T08:27:12Z",
            "summary": "Brain network analysis has emerged as pivotal method for gaining a deeper\nunderstanding of brain functions and disease mechanisms. Despite the existence\nof various network construction approaches, shortcomings persist in the\nlearning of correlations between structural and functional brain imaging data.\nIn light of this, we introduce a novel method called BrainNetDiff, which\ncombines a multi-head Transformer encoder to extract relevant features from\nfMRI time series and integrates a conditional latent diffusion model for brain\nnetwork generation. Leveraging a conditional prompt and a fusion attention\nmechanism, this method significantly improves the accuracy and stability of\nbrain network generation. To the best of our knowledge, this represents the\nfirst framework that employs diffusion for the fusion of the multimodal brain\nimaging and brain network generation from images to graphs. We validate\napplicability of this framework in the construction of brain network across\nhealthy and neurologically impaired cohorts using the authentic dataset.\nExperimental results vividly demonstrate the significant effectiveness of the\nproposed method across the downstream disease classification tasks. These\nfindings convincingly emphasize the prospective value in the field of brain\nnetwork research, particularly its key significance in neuroimaging analysis\nand disease diagnosis. This research provides a valuable reference for the\nprocessing of multimodal brain imaging data and introduces a novel, efficient\nsolution to the field of neuroimaging.",
            "author": [
                "Yongcheng Zong",
                "Shuqiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05199v1",
                "http://arxiv.org/pdf/2311.05199v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05196v1",
            "title": "Solving Combinatorial Optimization Problems on Fujitsu Digital Annealer",
            "updated": "2023-11-09T08:20:50Z",
            "published": "2023-11-09T08:20:50Z",
            "summary": "Combinatorial optimization problems are ubiquitous in various disciplines and\napplications. Many heuristic algorithms have been devoted to solve these types\nof problems. In order to increase the efficiency for finding the optimal\nsolutions, an application-specific hardware, called digital annealer (DA) has\nbeen developed for solving combinatorial optimization problems using quadratic\nunconstrained binary optimization (QUBO) formulations. In this study, we\nformulated the number partitioning problem and the graph partitioning problem\ninto QUBO forms and solved such problems with the DA developed by Fujitsu Ltd.\nThe QUBO formulation of the number partitioning problem is fully connected. The\nDA found the overall runtime for the optimal solution to be less than 30\nseconds for 6500 binary variables. For the graph partitioning problem, we\nadopted modularity as the metric for determining the quality of the partitions.\nFor Zachary's Karate Club graph, the modularity obtained was 0.445, a 6%\nincrease against D-wave Quantum Annealer and Simulated Annealing. Moreover, to\nexplore the DA's potential applications to real-world problems, we used the\nsearch for communities or virtual microgrids in a power distribution network as\nan example. The problem was formulated into graph partitioning. It is shown\nthat the DA effectively identified community structures in the IEEE 33-bus and\nIEEE 118-bus network.",
            "author": [
                "Yu-Ting Kao",
                "Jia-Le Liao",
                "Hsiu-Chuan Hsu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05196v1",
                "http://arxiv.org/pdf/2311.05196v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05194v1",
            "title": "A Weighted-Graph Curvature Calculator and Whether the Discrete Curvature\n  Senses the Smooth One",
            "updated": "2023-11-09T08:13:45Z",
            "published": "2023-11-09T08:13:45Z",
            "summary": "We investigate whether there is a relationship between the discrete\nBakry-\\'{E}mery curvature of a graph and the smooth curvature of an ambient\nsurface into which the graph is embedded geodesically. As we used weighted\ngraphs as test objects, we developed a program for the calculation of the\ndiscrete curvature and with the help of this calculator, we observed some\nindications of such a relationship.",
            "author": [
                "G\u00f6k\u00e7e \u00c7akmak",
                "Ali Deniz",
                "\u015eahin Ko\u00e7ak",
                "Murat Limoncu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05194v1",
                "http://arxiv.org/pdf/2311.05194v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05185v1",
            "title": "Mixture of Weak & Strong Experts on Graphs",
            "updated": "2023-11-09T07:45:05Z",
            "published": "2023-11-09T07:45:05Z",
            "summary": "Realistic graphs contain both rich self-features of nodes and informative\nstructures of neighborhoods, jointly handled by a GNN in the typical setup. We\npropose to decouple the two modalities by mixture of weak and strong experts\n(Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP),\nand the strong expert is an off-the-shelf Graph Neural Network (GNN). To adapt\nthe experts' collaboration to different target nodes, we propose a \"confidence\"\nmechanism based on the dispersion of the weak expert's prediction logits. The\nstrong expert is conditionally activated when either the node's classification\nrelies on neighborhood information, or the weak expert has low model quality.\nWe reveal interesting training dynamics by analyzing the influence of the\nconfidence function on loss: our training algorithm encourages the\nspecialization of each expert by effectively generating soft splitting of the\ngraph. In addition, our \"confidence\" design imposes a desirable bias toward the\nstrong expert to benefit from GNN's better generalization capability. Mowst is\neasy to optimize and achieves strong expressive power, with a computation cost\ncomparable to a single GNN. Empirically, Mowst shows significant accuracy\nimprovement on 6 standard node classification benchmarks (including both\nhomophilous and heterophilous graphs).",
            "author": [
                "Hanqing Zeng",
                "Hanjia Lyu",
                "Diyi Hu",
                "Yinglong Xia",
                "Jiebo Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05185v1",
                "http://arxiv.org/pdf/2311.05185v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05155v1",
            "title": "Weakly-supervised Deep Cognate Detection Framework for Low-Resourced\n  Languages Using Morphological Knowledge of Closely-Related Languages",
            "updated": "2023-11-09T05:46:41Z",
            "published": "2023-11-09T05:46:41Z",
            "summary": "Exploiting cognates for transfer learning in under-resourced languages is an\nexciting opportunity for language understanding tasks, including unsupervised\nmachine translation, named entity recognition and information retrieval.\nPrevious approaches mainly focused on supervised cognate detection tasks based\non orthographic, phonetic or state-of-the-art contextual language models, which\nunder-perform for most under-resourced languages. This paper proposes a novel\nlanguage-agnostic weakly-supervised deep cognate detection framework for\nunder-resourced languages using morphological knowledge from closely related\nlanguages. We train an encoder to gain morphological knowledge of a language\nand transfer the knowledge to perform unsupervised and weakly-supervised\ncognate detection tasks with and without the pivot language for the\nclosely-related languages. While unsupervised, it overcomes the need for\nhand-crafted annotation of cognates. We performed experiments on different\npublished cognate detection datasets across language families and observed not\nonly significant improvement over the state-of-the-art but also our method\noutperformed the state-of-the-art supervised and unsupervised methods. Our\nmodel can be extended to a wide range of languages from any language family as\nit overcomes the requirement of the annotation of the cognate pairs for\ntraining. The code and dataset building scripts can be found at\nhttps://github.com/koustavagoswami/Weakly_supervised-Cognate_Detection",
            "author": [
                "Koustava Goswami",
                "Priya Rani",
                "Theodorus Fransen",
                "John P. McCrae"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05155v1",
                "http://arxiv.org/pdf/2311.05155v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05147v1",
            "title": "Dynamic Association Learning of Self-Attention and Convolution in Image\n  Restoration",
            "updated": "2023-11-09T05:11:24Z",
            "published": "2023-11-09T05:11:24Z",
            "summary": "CNNs and Self attention have achieved great success in multimedia\napplications for dynamic association learning of self-attention and convolution\nin image restoration. However, CNNs have at least two shortcomings: 1) limited\nreceptive field; 2) static weight of sliding window at inference, unable to\ncope with the content diversity.In view of the advantages and disadvantages of\nCNNs and Self attention, this paper proposes an association learning method to\nutilize the advantages and suppress their shortcomings, so as to achieve\nhigh-quality and efficient inpainting. We regard rain distribution reflects the\ndegradation location and degree, in addition to the rain distribution\nprediction. Thus, we propose to refine background textures with the predicted\ndegradation prior in an association learning manner. As a result, we accomplish\nimage deraining by associating rain streak removal and background recovery,\nwhere an image deraining network and a background recovery network are designed\nfor two subtasks. The key part of association learning is a novel multi-input\nattention module. It generates the degradation prior and produces the\ndegradation mask according to the predicted rainy distribution. Benefited from\nthe global correlation calculation of SA, MAM can extract the informative\ncomplementary components from the rainy input with the degradation mask, and\nthen help accurate texture restoration. Meanwhile, SA tends to aggregate\nfeature maps with self-attention importance, but convolution diversifies them\nto focus on the local textures. A hybrid fusion network involves one residual\nTransformer branch and one encoder-decoder branch. The former takes a few\nlearnable tokens as input and stacks multi-head attention and feed-forward\nnetworks to encode global features of the image. The latter, conversely,\nleverages the multi-scale encoder-decoder to represent contexture knowledge.",
            "author": [
                "Kui Jiang",
                "Xuemei Jia",
                "Wenxin Huang",
                "Wenbin Wang",
                "Zheng Wang",
                "Junjun Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05147v1",
                "http://arxiv.org/pdf/2311.05147v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10101v1",
            "title": "Gaussian Differential Privacy on Riemannian Manifolds",
            "updated": "2023-11-09T04:46:27Z",
            "published": "2023-11-09T04:46:27Z",
            "summary": "We develop an advanced approach for extending Gaussian Differential Privacy\n(GDP) to general Riemannian manifolds. The concept of GDP stands out as a\nprominent privacy definition that strongly warrants extension to manifold\nsettings, due to its central limit properties. By harnessing the power of the\nrenowned Bishop-Gromov theorem in geometric analysis, we propose a Riemannian\nGaussian distribution that integrates the Riemannian distance, allowing us to\nachieve GDP in Riemannian manifolds with bounded Ricci curvature. To the best\nof our knowledge, this work marks the first instance of extending the GDP\nframework to accommodate general Riemannian manifolds, encompassing curved\nspaces, and circumventing the reliance on tangent space summaries. We provide a\nsimple algorithm to evaluate the privacy budget $\\mu$ on any one-dimensional\nmanifold and introduce a versatile Markov Chain Monte Carlo (MCMC)-based\nalgorithm to calculate $\\mu$ on any Riemannian manifold with constant\ncurvature. Through simulations on one of the most prevalent manifolds in\nstatistics, the unit sphere $S^d$, we demonstrate the superior utility of our\nRiemannian Gaussian mechanism in comparison to the previously proposed\nRiemannian Laplace mechanism for implementing GDP.",
            "author": [
                "Yangdi Jiang",
                "Xiaotian Chang",
                "Yi Liu",
                "Lei Ding",
                "Linglong Kong",
                "Bei Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10101v1",
                "http://arxiv.org/pdf/2311.10101v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DS",
                "cs.LG",
                "stat.ML",
                "stat.OT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05133v1",
            "title": "Materials Properties Prediction (MAPP): Empowering the prediction of\n  material properties solely based on chemical formulas",
            "updated": "2023-11-09T04:08:47Z",
            "published": "2023-11-09T04:08:47Z",
            "summary": "Predicting material properties has always been a challenging task in\nmaterials science. With the emergence of machine learning methodologies, new\navenues have opened up. In this study, we build upon our recently developed\nGraph Neural Network (GNN) approach to construct models that predict four\ndistinct material properties. Our graph model represents materials as element\ngraphs, with chemical formula serving as the only input. This approach ensures\npermutation invariance, offering a robust solution to prior limitations. By\nemploying bootstrap methods to train on this individual GNN, we further enhance\nthe reliability and accuracy of our predictions. With multi-task learning, we\nharness the power of extensive datasets to boost the performance of smaller\nones. We introduce the inaugural version of the Materials Properties Prediction\n(MAPP) framework, empowering the prediction of material properties solely based\non chemical formulas.",
            "author": [
                "Si-Da Xue",
                "Qi-Jun Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05133v1",
                "http://arxiv.org/pdf/2311.05133v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05132v3",
            "title": "The non-perturbative stringy interaction between NS-brane \\& Dp brane",
            "updated": "2023-11-26T07:11:10Z",
            "published": "2023-11-09T04:00:24Z",
            "summary": "To our best knowledge, the leading non-perturbative stringy interaction\nbetween an NS brane and a Dp brane remains unknown. We here present the\nnon-perturbative stringy amplitudes for a system of an F-string and a Dp brane\nand a system of an NS 5 brane and a Dp brane for $0 \\le p \\le 6$. In either\ncase, the F or NS5 and the Dp are placed parallel at a separation. We obtain\nthe respective amplitudes, starting from the amplitude for a system of a D1 and\na D3 for the former and that for a system of a D5 and a D3 system for the\nlatter, based on the IIB S-duality and various T-dualities plus the consistency\nof both, along with the respective known long-range amplitudes. We would like\nto point out that the amplitude for the D1/D3 or D3/D5 computed from the usual\nD-brane technique does not take into consideration of the non-perturbative\ncontribution due to the exchange of virtual closed D-string emitted by the D3.\nAs such the resulting amplitudes obtained from this one via the S-duality and\nfollowed by various T-dualities are not consistent with the IIB S-duality. We\nresolve this issue and obtain the corresponding consistent amplitudes. The\nimplications of so obtained amplitudes are also discussed.",
            "author": [
                "J. X. Lu",
                "Nan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05132v3",
                "http://arxiv.org/pdf/2311.05132v3"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05113v1",
            "title": "Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset",
            "updated": "2023-11-09T02:58:17Z",
            "published": "2023-11-09T02:58:17Z",
            "summary": "Mathematical understanding and reasoning are crucial tasks for assessing the\ncapabilities of artificial intelligence (AI). However, existing benchmarks\neither require just a few steps of reasoning, or only contain a small amount of\ndata in one specific topic, making it hard to analyse AI's behaviour with\nreference to different problems within a specific topic in detail. In this\nwork, we propose Conic10K, a challenging math problem dataset on conic sections\nin Chinese senior high school education. Our dataset contains various problems\nwith different reasoning depths, while only the knowledge from conic sections\nis required. Since the dataset only involves a narrow range of knowledge, it is\neasy to separately analyse the knowledge a model possesses and the reasoning\nability it has. For each problem, we provide a high-quality formal\nrepresentation, the reasoning steps, and the final solution. Experiments show\nthat existing large language models, including GPT-4, exhibit weak performance\non complex reasoning. We hope that our findings could inspire more advanced\ntechniques for precise natural language understanding and reasoning. Our\ndataset and codes are available at https://github.com/whyNLP/Conic10K.",
            "author": [
                "Haoyi Wu",
                "Wenyang Hui",
                "Yezeng Chen",
                "Weiqi Wu",
                "Kewei Tu",
                "Yi Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05113v1",
                "http://arxiv.org/pdf/2311.05113v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09240v1",
            "title": "Devil in the Landscapes: Inferring Epidemic Exposure Risks from Street\n  View Imagery",
            "updated": "2023-11-09T02:52:00Z",
            "published": "2023-11-09T02:52:00Z",
            "summary": "Built environment supports all the daily activities and shapes our health.\nLeveraging informative street view imagery, previous research has established\nthe profound correlation between the built environment and chronic,\nnon-communicable diseases; however, predicting the exposure risk of infectious\ndiseases remains largely unexplored. The person-to-person contacts and\ninteractions contribute to the complexity of infectious disease, which is\ninherently different from non-communicable diseases. Besides, the complex\nrelationships between street view imagery and epidemic exposure also hinder\naccurate predictions. To address these problems, we construct a regional\nmobility graph informed by the gravity model, based on which we propose a\ntransmission-aware graph convolutional network (GCN) to capture disease\ntransmission patterns arising from human mobility. Experiments show that the\nproposed model significantly outperforms baseline models by 8.54% in weighted\nF1, shedding light on a low-cost, scalable approach to assess epidemic exposure\nrisks from street view imagery.",
            "author": [
                "Zhenyu Han",
                "Yanxin Xi",
                "Tong Xia",
                "Yu Liu",
                "Yong Li"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3589132.3625596",
                "http://arxiv.org/abs/2311.09240v1",
                "http://arxiv.org/pdf/2311.09240v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05103v1",
            "title": "PID-inspired Continuous-time Distributed Optimization",
            "updated": "2023-11-09T02:32:00Z",
            "published": "2023-11-09T02:32:00Z",
            "summary": "This paper proposes two novel distributed continuous-time algorithms inspired\nby PID control to solve distributed optimization problems. The algorithms are\nreferred to as first-order and second-order, respectively, depend on the\nintrinsic dynamics of the agents in the network. Sufficient conditions are\nderived so that both algorithms converge exponentially over undirected\nconnected graphs. Finally, numerical simulations illustrate the effectiveness\nand efficiency of the proposed algorithms.",
            "author": [
                "Meng Tao",
                "Dongdong Yue",
                "Jinde Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05103v1",
                "http://arxiv.org/pdf/2311.05103v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05085v1",
            "title": "Characterizing Large Language Models as Rationalizers of\n  Knowledge-intensive Tasks",
            "updated": "2023-11-09T01:04:44Z",
            "published": "2023-11-09T01:04:44Z",
            "summary": "Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.",
            "author": [
                "Aditi Mishra",
                "Sajjadur Rahman",
                "Hannah Kim",
                "Kushan Mitra",
                "Estevam Hruschka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05085v1",
                "http://arxiv.org/pdf/2311.05085v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05084v1",
            "title": "Signal Temporal Logic-Guided Apprenticeship Learning",
            "updated": "2023-11-09T00:59:28Z",
            "published": "2023-11-09T00:59:28Z",
            "summary": "Apprenticeship learning crucially depends on effectively learning rewards,\nand hence control policies from user demonstrations. Of particular difficulty\nis the setting where the desired task consists of a number of sub-goals with\ntemporal dependencies. The quality of inferred rewards and hence policies are\ntypically limited by the quality of demonstrations, and poor inference of these\ncan lead to undesirable outcomes. In this letter, we show how temporal logic\nspecifications that describe high level task objectives, are encoded in a graph\nto define a temporal-based metric that reasons about behaviors of demonstrators\nand the learner agent to improve the quality of inferred rewards and policies.\nThrough experiments on a diverse set of robot manipulator simulations, we show\nhow our framework overcomes the drawbacks of prior literature by drastically\nimproving the number of demonstrations required to learn a control policy.",
            "author": [
                "Aniruddh G. Puranic",
                "Jyotirmoy V. Deshmukh",
                "Stefanos Nikolaidis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05084v1",
                "http://arxiv.org/pdf/2311.05084v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05656v1",
            "title": "Combating Misinformation in the Age of LLMs: Opportunities and\n  Challenges",
            "updated": "2023-11-09T00:05:27Z",
            "published": "2023-11-09T00:05:27Z",
            "summary": "Misinformation such as fake news and rumors is a serious threat on\ninformation ecosystems and public trust. The emergence of Large Language Models\n(LLMs) has great potential to reshape the landscape of combating\nmisinformation. Generally, LLMs can be a double-edged sword in the fight. On\nthe one hand, LLMs bring promising opportunities for combating misinformation\ndue to their profound world knowledge and strong reasoning abilities. Thus, one\nemergent question is: how to utilize LLMs to combat misinformation? On the\nother hand, the critical challenge is that LLMs can be easily leveraged to\ngenerate deceptive misinformation at scale. Then, another important question\nis: how to combat LLM-generated misinformation? In this paper, we first\nsystematically review the history of combating misinformation before the advent\nof LLMs. Then we illustrate the current efforts and present an outlook for\nthese two fundamental questions respectively. The goal of this survey paper is\nto facilitate the progress of utilizing LLMs for fighting misinformation and\ncall for interdisciplinary efforts from different stakeholders for combating\nLLM-generated misinformation.",
            "author": [
                "Canyu Chen",
                "Kai Shu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05656v1",
                "http://arxiv.org/pdf/2311.05656v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05066v2",
            "title": "Induced subgraphs and tree decompositions XIII. Basic obstructions in\n  $\\mathcal{H}$-free graphs for finite $\\mathcal{H}$",
            "updated": "2023-11-26T18:16:54Z",
            "published": "2023-11-09T00:04:35Z",
            "summary": "Unlike minors, the induced subgraph obstructions to bounded treewidth come in\na large variety, including, for every $t\\geq 1$, the $t$-basic obstructions:\nthe graphs $K_{t+1}$ and $K_{t,t}$, along with the subdivisions of the\n$t$-by-$t$ wall and their line graphs. But this list is far from complete. The\nsimplest example of a ''non-basic'' obstruction is due to Pohoata and Davies\n(independently). For every $n \\geq 1$, they construct certain graphs of\ntreewidth $n$ and with no $3$-basic obstruction as an induced subgraph, which\nwe call $n$-arrays.\n  Let us say a graph class $\\mathcal{G}$ is clean if the only obstructions to\nbounded treewidth in $\\mathcal{G}$ are in fact the basic ones. It follows that\na full description of the induced subgraph obstructions to bounded treewidth is\nequivalent to a characterization of all families $\\mathcal{H}$ of graphs for\nwhich the class of all $\\mathcal{H}$-free graphs is clean (a graph $G$ is\n$\\mathcal{H}$-free if no induced subgraph of $G$ is isomorphic to any graph in\n$\\mathcal{H}$).\n  This remains elusive, but there is an immediate necessary condition: if\n$\\mathcal{H}$-free graphs are clean, then there are only finitely many integers\n$n\\geq 1$ such that there is an $n$-array which is $\\mathcal{H}$-free. The\nabove necessary condition is not sufficient in general. However, the situation\nturns out to be different if $\\mathcal{H}$ is finite: we prove that for every\nfinite set $\\mathcal{H}$ of graphs, the class of all $\\mathcal{H}$-free graphs\nis clean if and only if there is no $\\mathcal{H}$-free $n$-array except\npossibly for finitely many values of $n$.",
            "author": [
                "Bogdan Alecu",
                "Maria Chudnovsky",
                "Sepehr Hajebi",
                "Sophie Spirkl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05066v2",
                "http://arxiv.org/pdf/2311.05066v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05060v1",
            "title": "Small bodies global gravity inversion via the level-set method",
            "updated": "2023-11-08T23:53:50Z",
            "published": "2023-11-08T23:53:50Z",
            "summary": "We propose an approach to infer large-scale heterogeneities within a small\ncelestial body from measurements of its gravitational potential, provided for\ninstance by spacecraft radio-tracking. The non-uniqueness of the gravity\ninversion is here mitigated by limiting the solutions to piecewise-constant\ndensity distributions, thus composed of multiple regions of uniform density\n(mass anomalies) dispersed in a background medium. The boundary of each anomaly\nis defined implicitly as the 0-level surface of a scalar field (called the\nlevel-set function), so that by modifying this field the shape and location of\nthe anomaly are varied. The gravitational potential associated with a density\ndistribution is here computed via a line-integral polyhedron method, yielding\nthe coefficients of its spherical harmonics expansion. The density distribution\nis then adjusted via an iterative least-squares approach with Tikhonov\nregularization, estimating at every iteration corrections to the level-set\nfunction, the density contrast of each anomaly, and the background density, in\norder to minimize the residuals between the predicted gravity coefficients and\nthose measured. Given the non-convexity of the problem and the lack of prior\nknowledge assumed (save for the shape of the body), the estimation process is\nrepeated for several random initial distributions, and the resulting solutions\nare clustered based on global properties independent of the input measurements.\nThis provides families of candidate interior models in agreement with the data,\nand the spread of the local density values across each family is used to assess\nthe uncertainties associated with the estimated distributions.",
            "author": [
                "Alfonso Caldiero",
                "S\u00e9bastien Le Maistre"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05060v1",
                "http://arxiv.org/pdf/2311.05060v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06406v1",
            "title": "CGC/saturation approach: Impact-parameter dependent model in\n  next-to-leading order and combined HERA data",
            "updated": "2023-11-08T21:55:39Z",
            "published": "2023-11-08T21:55:39Z",
            "summary": "In this paper we confront the next-to-leading order (NLO) CGC/saturation\napproach of Ref. [1] with the experimental combined HERA data and obtain its\nparameters. The model includes two features that are in accordance with our\ntheoretical knowledge of deep inelastic scattering. These consist of: $i$) the\nuse of analytical solution for the non-linear Balitsky-Kovchegov (BK) evolution\nequation and $ii$) the exponential behavior of the saturation momentum on the\nimpact parameter $b$-dependence, characterized by $Q_s$ $\\propto\\exp( -m b )$\nwhich reproduce the correct behaviour of the scattering amplitude at large $b$\nin accord with Froissart theorem. The model results are then compared to data\nat small-x for the structure function of the proton $F_{2}$, the longitudinal\nstructure function $F_{L}$, the charm structure function $F_2^{c\\bar{c}}$, the\nexclusive vector meson ($J/\\psi,\\phi,\\rho$) production and Deeply Virtual\nCompton Scattering (DVCS). We obtain a good agreement for the processes in a\nwide kinematic range of $Q^2$ at small $x$. Our results provide a strong guide\nfor finding an approach, based on Color Glass Condensate/saturation effective\ntheory for high energy QCD, to make reliable predictions from first principles\nas well as for forthcoming experiments like the Electron-Ion Collider and the\nLHeC.",
            "author": [
                "Michael Sanhueza",
                "Jos\u00e9 Garrido M.",
                "Miguel Guevara"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06406v1",
                "http://arxiv.org/pdf/2311.06406v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07588v1",
            "title": "NLQxform: A Language Model-based Question to SPARQL Transformer",
            "updated": "2023-11-08T21:41:45Z",
            "published": "2023-11-08T21:41:45Z",
            "summary": "In recent years, scholarly data has grown dramatically in terms of both scale\nand complexity. It becomes increasingly challenging to retrieve information\nfrom scholarly knowledge graphs that include large-scale heterogeneous\nrelationships, such as authorship, affiliation, and citation, between various\ntypes of entities, e.g., scholars, papers, and organizations. As part of the\nScholarly QALD Challenge, this paper presents a question-answering (QA) system\ncalled NLQxform, which provides an easy-to-use natural language interface to\nfacilitate accessing scholarly knowledge graphs. NLQxform allows users to\nexpress their complex query intentions in natural language questions. A\ntransformer-based language model, i.e., BART, is employed to translate\nquestions into standard SPARQL queries, which can be evaluated to retrieve the\nrequired information. According to the public leaderboard of the Scholarly QALD\nChallenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering\nover DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA\ntask, demonstrating the competitiveness of the system.",
            "author": [
                "Ruijie Wang",
                "Zhiruo Zhang",
                "Luca Rossetto",
                "Florian Ruosch",
                "Abraham Bernstein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07588v1",
                "http://arxiv.org/pdf/2311.07588v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04991v1",
            "title": "Effective Restoration of Source Knowledge in Continual Test Time\n  Adaptation",
            "updated": "2023-11-08T19:21:48Z",
            "published": "2023-11-08T19:21:48Z",
            "summary": "Traditional test-time adaptation (TTA) methods face significant challenges in\nadapting to dynamic environments characterized by continuously changing\nlong-term target distributions. These challenges primarily stem from two\nfactors: catastrophic forgetting of previously learned valuable source\nknowledge and gradual error accumulation caused by miscalibrated pseudo labels.\nTo address these issues, this paper introduces an unsupervised domain change\ndetection method that is capable of identifying domain shifts in dynamic\nenvironments and subsequently resets the model parameters to the original\nsource pre-trained values. By restoring the knowledge from the source, it\neffectively corrects the negative consequences arising from the gradual\ndeterioration of model parameters caused by ongoing shifts in the domain. Our\nmethod involves progressive estimation of global batch-norm statistics specific\nto each domain, while keeping track of changes in the statistics triggered by\ndomain shifts. Importantly, our method is agnostic to the specific adaptation\ntechnique employed and thus, can be incorporated to existing TTA methods to\nenhance their performance in dynamic environments. We perform extensive\nexperiments on benchmark datasets to demonstrate the superior performance of\nour method compared to state-of-the-art adaptation methods.",
            "author": [
                "Fahim Faisal Niloy",
                "Sk Miraj Ahmed",
                "Dripta S. Raychaudhuri",
                "Samet Oymak",
                "Amit K. Roy-Chowdhury"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04991v1",
                "http://arxiv.org/pdf/2311.04991v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04980v2",
            "title": "MaxEVA: Maximizing the Efficiency of Matrix Multiplication on Versal AI\n  Engine",
            "updated": "2023-11-14T00:42:17Z",
            "published": "2023-11-08T19:02:05Z",
            "summary": "The increasing computational and memory requirements of Deep Learning (DL)\nworkloads has led to outstanding innovations in hardware architectures. An\narchetype of such architectures is the novel Versal AI Engine (AIE) by\nAMD/Xilinx. The AIE comprises multiple programmable processors optimized for\nvector-based algorithms. An AIE array consisting of 400 processor cores,\noperating at 1.25 GHz is able to deliver a peak throughput of 8 TFLOPs for\n32-bit floating-point (fp32), and 128 TOPs for 8-bit integer (int8) precision.\nIn this work, we propose MaxEVA: a novel framework to efficiently map Matrix\nMultiplication (MatMul) workloads on Versal AIE devices. Our framework\nmaximizes the performance and energy efficiency of MatMul applications by\nefficiently exploiting features of the AIE architecture and resolving\nperformance bottlenecks from multiple angles. When demonstrating on the VC1902\ndevice of the VCK190 board, MaxEVA accomplishes up to 5.44 TFLOPs and 77.01\nTOPs throughput for fp32 and int8 precisions, respectively. In terms of energy\nefficiency, MaxEVA attains up to 124.16 GFLOPs/W for fp32, and 1.16 TOPs/W for\nint8. Our proposed method substantially outperforms the state-of-the-art\napproach by exhibiting up to 2.19x throughput gain and 20.4% higher energy\nefficiency. The MaxEVA framework provides notable insights to fill the\nknowledge gap in effectively designing MatMul-based DL workloads on the new\nVersal AIE devices.",
            "author": [
                "Endri Taka",
                "Aman Arora",
                "Kai-Chiang Wu",
                "Diana Marculescu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04980v2",
                "http://arxiv.org/pdf/2311.04980v2"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04900v1",
            "title": "How Abstract Is Linguistic Generalization in Large Language Models?\n  Experiments with Argument Structure",
            "updated": "2023-11-08T18:58:43Z",
            "published": "2023-11-08T18:58:43Z",
            "summary": "Language models are typically evaluated on their success at predicting the\ndistribution of specific words in specific contexts. Yet linguistic knowledge\nalso encodes relationships between contexts, allowing inferences between word\ndistributions. We investigate the degree to which pre-trained Transformer-based\nlarge language models (LLMs) represent such relationships, focusing on the\ndomain of argument structure. We find that LLMs perform well in generalizing\nthe distribution of a novel noun argument between related contexts that were\nseen during pre-training (e.g., the active object and passive subject of the\nverb spray), succeeding by making use of the semantically-organized structure\nof the embedding space for word embeddings. However, LLMs fail at\ngeneralizations between related contexts that have not been observed during\npre-training, but which instantiate more abstract, but well-attested structural\ngeneralizations (e.g., between the active object and passive subject of an\narbitrary verb). Instead, in this case, LLMs show a bias to generalize based on\nlinear order. This finding points to a limitation with current models and\npoints to a reason for which their training is data-intensive.s reported here\nare available at https://github.com/clay-lab/structural-alternations.",
            "author": [
                "Michael Wilson",
                "Jackson Petty",
                "Robert Frank"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04900v1",
                "http://arxiv.org/pdf/2311.04900v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04896v1",
            "title": "Optimized measurements of chaotic dynamical systems via the information\n  bottleneck",
            "updated": "2023-11-08T18:56:29Z",
            "published": "2023-11-08T18:56:29Z",
            "summary": "Deterministic chaos permits a precise notion of a \"perfect measurement\" as\none that, when obtained repeatedly, captures all of the information created by\nthe system's evolution with minimal redundancy. Finding an optimal measurement\nis challenging, and has generally required intimate knowledge of the dynamics\nin the few cases where it has been done. We establish an equivalence between a\nperfect measurement and a variant of the information bottleneck. As a\nconsequence, we can employ machine learning to optimize measurement processes\nthat efficiently extract information from trajectory data. We obtain\napproximately optimal measurements for multiple chaotic maps and lay the\nnecessary groundwork for efficient information extraction from general time\nseries.",
            "author": [
                "Kieran A. Murphy",
                "Dani S. Bassett"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04896v1",
                "http://arxiv.org/pdf/2311.04896v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "math.IT",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04894v1",
            "title": "DAMEX: Dataset-aware Mixture-of-Experts for visual understanding of\n  mixture-of-datasets",
            "updated": "2023-11-08T18:55:24Z",
            "published": "2023-11-08T18:55:24Z",
            "summary": "Construction of a universal detector poses a crucial question: How can we\nmost effectively train a model on a large mixture of datasets? The answer lies\nin learning dataset-specific features and ensembling their knowledge but do all\nthis in a single model. Previous methods achieve this by having separate\ndetection heads on a common backbone but that results in a significant increase\nin parameters. In this work, we present Mixture-of-Experts as a solution,\nhighlighting that MoEs are much more than a scalability tool. We propose\nDataset-Aware Mixture-of-Experts, DAMEX where we train the experts to become an\n`expert' of a dataset by learning to route each dataset tokens to its mapped\nexpert. Experiments on Universal Object-Detection Benchmark show that we\noutperform the existing state-of-the-art by average +10.2 AP score and improve\nover our non-MoE baseline by average +2.0 AP score. We also observe consistent\ngains while mixing datasets with (1) limited availability, (2) disparate\ndomains and (3) divergent label sets. Further, we qualitatively show that DAMEX\nis robust against expert representation collapse.",
            "author": [
                "Yash Jain",
                "Harkirat Behl",
                "Zsolt Kira",
                "Vibhav Vineet"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04894v1",
                "http://arxiv.org/pdf/2311.04894v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04892v1",
            "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
            "updated": "2023-11-08T18:52:17Z",
            "published": "2023-11-08T18:52:17Z",
            "summary": "Recent works have showcased the ability of large-scale language models (LLMs)\nto embody diverse personas in their responses, exemplified by prompts like 'You\nare Yoda. Explain the Theory of Relativity.' While this ability allows\npersonalization of LLMs and enables human behavior simulation, its effect on\nLLMs' capabilities remain unclear. To fill this gap, we present the first\nextensive study of the unintended side-effects of persona assignment on the\nability of LLMs, specifically ChatGPT, to perform basic reasoning tasks. Our\nstudy covers 24 reasoning datasets and 16 diverse personas spanning 5\nsocio-demographic groups: race, gender, religion, disability, and political\naffiliation. Our experiments unveil that ChatGPT carries deep rooted bias\nagainst various socio-demographics underneath a veneer of fairness. While it\novertly rejects stereotypes when explicitly asked ('Are Black people less\nskilled at mathematics?'), it manifests stereotypical and often erroneous\npresumptions when prompted to answer questions while taking on a persona. These\ncan be observed as abstentions in the model responses, e.g., 'As a Black\nperson, I am unable to answer this question as it requires math knowledge', and\ngenerally result in a substantial drop in performance on reasoning tasks. We\nfind that this inherent deep bias is ubiquitous - 80% of our personas\ndemonstrated bias; it is significant - certain datasets had relative drops in\nperformance of 70%+; and can be especially harmful for certain groups - certain\npersonas had stat. sign. drops on more than 80% of the datasets. Further\nanalysis shows that these persona-induced errors can be hard-to-discern and\nhard-to-avoid. Our findings serve as a cautionary tale that the practice of\nassigning personas to LLMs - a trend on the rise - can surface their\ndeep-rooted biases and have unforeseeable and detrimental side-effects.",
            "author": [
                "Shashank Gupta",
                "Vaishnavi Shrivastava",
                "Ameet Deshpande",
                "Ashwin Kalyan",
                "Peter Clark",
                "Ashish Sabharwal",
                "Tushar Khot"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04892v1",
                "http://arxiv.org/pdf/2311.04892v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04891v1",
            "title": "Quantum automorphism groups of trees",
            "updated": "2023-11-08T18:51:34Z",
            "published": "2023-11-08T18:51:34Z",
            "summary": "We give a characterisation of quantum automorphism groups of trees. In\nparticular, for every tree, we show how to iteratively construct its quantum\nautomorphism group using free products and free wreath products. This can be\nconsidered a quantum version of Jordan's theorem for the automorphism groups of\ntrees. This is one of the first characterisations of quantum automorphism\ngroups of a natural class of graphs with quantum symmetry.",
            "author": [
                "Josse van Dobben de Bruyn",
                "Prem Nigam Kar",
                "David E. Roberson",
                "Simon Schmidt",
                "Peter Zeman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04891v1",
                "http://arxiv.org/pdf/2311.04891v1"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04889v1",
            "title": "Graphs with quantum symmetry and trivial automorphism group",
            "updated": "2023-11-08T18:50:26Z",
            "published": "2023-11-08T18:50:26Z",
            "summary": "We present a sequence of finite graphs with trivial automorphism group and\nnon-trivial quantum automorphism group, which are the first known examples of\ngraphs with this property.",
            "author": [
                "Josse van Dobben de Bruyn",
                "David E. Roberson",
                "Simon Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04889v1",
                "http://arxiv.org/pdf/2311.04889v1"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04865v1",
            "title": "Computing the $5$-Edge-Connected Components in Linear Time",
            "updated": "2023-11-08T18:05:30Z",
            "published": "2023-11-08T18:05:30Z",
            "summary": "We provide a deterministic algorithm for computing the $5$-edge-connected\ncomponents of an undirected multigraph in linear time. There were probably good\nindications that this computation can be performed in linear time, but no such\nalgorithm was actually known prior to this work. Thus, our paper answers a\ntheoretical question, and sheds light on the possibility that a solution may\nexist for general $k$. A key component in our algorithm is an oracle for\nanswering connectivity queries for pairs of vertices in the presence of at most\nfour edge-failures. Specifically, the oracle has size $O(n)$, it can be\nconstructed in linear time, and it answers connectivity queries in the presence\nof at most four edge-failures in worst-case constant time, where $n$ denotes\nthe number of vertices of the graph. We note that this is a result of\nindependent interest. Our paper can be considered as a follow-up of recent work\non computing the $4$-edge-connected components in linear time. However, in\ndealing with the computation of the $5$-edge-connected components, we are faced\nwith unique challenges that do not appear when dealing with lower connectivity.\nThe problem is that the $4$-edge cuts in $3$-edge-connected graphs are\nentangled in various complicated ways, that make it difficult to organize them\nin a compact way. Here we provide a novel analysis of those cuts, that reveals\nthe existence of various interesting structures. These can be exploited so that\nwe can disentangle and collect only those cuts that are essential in computing\nthe $5$-edge-connected components. This analysis may provide a clue for a\ngeneral solution for the $k$-edge-connected components, or other related graph\nconnectivity problems.",
            "author": [
                "Evangelos Kosinas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04865v1",
                "http://arxiv.org/pdf/2311.04865v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04858v1",
            "title": "Scalable Fault-Tolerant Quantum Technologies with Silicon Colour Centres",
            "updated": "2023-11-08T17:52:57Z",
            "published": "2023-11-08T17:52:57Z",
            "summary": "The scaling barriers currently faced by both quantum networking and quantum\ncomputing technologies ultimately amount to the same core challenge of\ndistributing high-quality entanglement at scale. In this Perspective, a novel\nquantum information processing architecture based on optically active spins in\nsilicon is proposed that offers a combined single technological platform for\nscalable fault-tolerant quantum computing and networking. The architecture is\noptimized for overall entanglement distribution and leverages colour centre\nspins in silicon (T centres) for their manufacturability, photonic interface,\nand high fidelity information processing properties. Silicon nanophotonic\noptical circuits allow for photonic links between T centres, which are\nnetworked via telecom-band optical photons in a highly-connected graph. This\nhigh connectivity unlocks the use of low-overhead quantum error correction\ncodes, significantly accelerating the timeline for modular, scalable\nfault-tolerant quantum repeaters and quantum processors.",
            "author": [
                "Stephanie Simmons"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04858v1",
                "http://arxiv.org/pdf/2311.04858v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04846v1",
            "title": "Incorporating temporal dynamics of mutations to enhance the prediction\n  capability of antiretroviral therapy's outcome for HIV-1",
            "updated": "2023-11-08T17:29:41Z",
            "published": "2023-11-08T17:29:41Z",
            "summary": "Motivation: In predicting HIV therapy outcomes, a critical clinical question\nis whether using historical information can enhance predictive capabilities\ncompared with current or latest available data analysis. This study analyses\nwhether historical knowledge, which includes viral mutations detected in all\ngenotypic tests before therapy, their temporal occurrence, and concomitant\nviral load measurements, can bring improvements. We introduce a method to weigh\nmutations, considering the previously enumerated factors and the reference\nmutation-drug Stanford resistance tables. We compare a model encompassing\nhistory (H) with one not using it (NH). Results: The H-model demonstrates\nsuperior discriminative ability, with a higher ROC-AUC score (76.34%) than the\nNH-model (74.98%). Significant Wilcoxon test results confirm that incorporating\nhistorical information improves consistently predictive accuracy for treatment\noutcomes. The better performance of the H-model might be attributed to its\nconsideration of latent HIV reservoirs, probably obtained when leveraging\nhistorical information. The findings emphasize the importance of temporal\ndynamics in mutations, offering insights into HIV infection complexities.\nHowever, our result also shows that prediction accuracy remains relatively high\neven when no historical information is available. Supplementary information:\nSupplementary material is available.",
            "author": [
                "Giulia Di Teodoro",
                "Martin Pirkl",
                "Francesca Incardona",
                "Ilaria Vicenti",
                "Anders S\u00f6nnerborg",
                "Rolf Kaiser",
                "Laura Palagi",
                "Maurizio Zazzi",
                "Thomas Lengauer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04846v1",
                "http://arxiv.org/pdf/2311.04846v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04843v1",
            "title": "Bridging Dimensions: Confident Reachability for High-Dimensional\n  Controllers",
            "updated": "2023-11-08T17:26:38Z",
            "published": "2023-11-08T17:26:38Z",
            "summary": "Autonomous systems are increasingly implemented using end-end-end trained\ncontrollers. Such controllers make decisions that are executed on the real\nsystem with images as one of the primary sensing modalities. Deep neural\nnetworks form a fundamental building block of such controllers. Unfortunately,\nthe existing neural-network verification tools do not scale to inputs with\nthousands of dimensions. Especially when the individual inputs (such as pixels)\nare devoid of clear physical meaning. This paper takes a step towards\nconnecting exhaustive closed-loop verification with high-dimensional\ncontrollers. Our key insight is that the behavior of a high-dimensional\ncontroller can be approximated with several low-dimensional controllers in\ndifferent regions of the state space. To balance approximation and\nverifiability, we leverage the latest verification-aware knowledge\ndistillation. Then, if low-dimensional reachability results are inflated with\nstatistical approximation errors, they yield a high-confidence reachability\nguarantee for the high-dimensional controller. We investigate two inflation\ntechniques -- based on trajectories and actions -- both of which show\nconvincing performance in two OpenAI gym benchmarks.",
            "author": [
                "Yuang Geng",
                "Souradeep Dutta",
                "Ivan Ruchkin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04843v1",
                "http://arxiv.org/pdf/2311.04843v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04837v1",
            "title": "Identifying Semantic Component for Robust Molecular Property Prediction",
            "updated": "2023-11-08T17:01:35Z",
            "published": "2023-11-08T17:01:35Z",
            "summary": "Although graph neural networks have achieved great success in the task of\nmolecular property prediction in recent years, their generalization ability\nunder out-of-distribution (OOD) settings is still under-explored. Different\nfrom existing methods that learn discriminative representations for prediction,\nwe propose a generative model with semantic-components identifiability, named\nSCI. We demonstrate that the latent variables in this generative model can be\nexplicitly identified into semantic-relevant (SR) and semantic-irrelevant (SI)\ncomponents, which contributes to better OOD generalization by involving minimal\nchange properties of causal mechanisms. Specifically, we first formulate the\ndata generation process from the atom level to the molecular level, where the\nlatent space is split into SI substructures, SR substructures, and SR atom\nvariables. Sequentially, to reduce misidentification, we restrict the minimal\nchanges of the SR atom variables and add a semantic latent substructure\nregularization to mitigate the variance of the SR substructure under augmented\ndomain changes. Under mild assumptions, we prove the block-wise identifiability\nof the SR substructure and the comment-wise identifiability of SR atom\nvariables. Experimental studies achieve state-of-the-art performance and show\ngeneral improvement on 21 datasets in 3 mainstream benchmarks. Moreover, the\nvisualization results of the proposed SCI method provide insightful case\nstudies and explanations for the prediction results. The code is available at:\nhttps://github.com/DMIRLAB-Group/SCI.",
            "author": [
                "Zijian Li",
                "Zunhong Xu",
                "Ruichu Cai",
                "Zhenhui Yang",
                "Yuguang Yan",
                "Zhifeng Hao",
                "Guangyi Chen",
                "Kun Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04837v1",
                "http://arxiv.org/pdf/2311.04837v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05653v1",
            "title": "Minimal Input Structural Modifications for Strongly Structural\n  Controllability",
            "updated": "2023-11-08T16:53:32Z",
            "published": "2023-11-08T16:53:32Z",
            "summary": "This paper studies the problem of modifying the input matrix of a structured\nsystem to make the system strongly structurally controllable. We focus on the\ngeneralized structured systems that rely on zero/nonzero/arbitrary structure,\ni.e., some entries of system matrices are zeros, some are nonzero, and the\nremaining entries can be zero or nonzero (arbitrary). Our first approach to\nfinding minimal changes is a greedy heuristic, which is simple to implement and\nfast. However, we also show that the greedy algorithm can give arbitrarily poor\nsolutions for some special systems. The second approach is a randomized Markov\nchain Monte Carlo-based algorithm. Unlike the greedy algorithm, this algorithm\nis always guaranteed to converge to an optimal solution. Finally, we\nnumerically evaluate the algorithms' performances on random graphs to show that\nthe algorithms perform well.",
            "author": [
                "Geethu Joseph",
                "Shana Moothedath",
                "Jiabin Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05653v1",
                "http://arxiv.org/pdf/2311.05653v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04952v1",
            "title": "The High Energy X-ray Probe (HEX-P): Supernova remnants, pulsar wind\n  nebulae, and nuclear astrophysics",
            "updated": "2023-11-08T16:42:59Z",
            "published": "2023-11-08T16:42:59Z",
            "summary": "HEX-P is a probe-class mission concept that will combine high spatial\nresolution X-ray imaging ($<10\"$ full width at half maximum) and broad spectral\ncoverage (0.2--80 keV) with an effective area far superior to current\nfacilities (including XMM-Newton and NuSTAR) to enable revolutionary new\ninsights into a variety of important astrophysical problems. HEX-P is ideally\nsuited to address important problems in the physics and astrophysics of\nsupernova remnants (SNRs) and pulsar-wind nebulae (PWNe). For shell SNRs, HEX-P\ncan greatly improve our understanding via more accurate spectral\ncharacterization and localization of non-thermal X-ray emission from both\nnon-thermal-dominated SNRs and those containing both thermal and non-thermal\ncomponents, and can discover previously unknown non-thermal components in SNRs.\nMulti-epoch HEX-P observations of several young SNRs (e.g., Cas A and Tycho)\nare expected to detect year-scale variabilities of X-ray filaments and knots,\nthus enabling us to determine fundamental parameters related to diffusive shock\nacceleration, such as local magnetic field strengths and maximum electron\nenergies. For PWNe, HEX-P will provide spatially-resolved, broadband X-ray\nspectral data separately from their pulsar emission, allowing us to study how\nparticle acceleration, cooling, and propagation operate in different evolution\nstages of PWNe. HEX-P is also poised to make unique and significant\ncontributions to nuclear astrophysics of Galactic radioactive sources by\nimproving detections of, or limits on, $^{44}$Ti in the youngest SNRs and by\npotentially discovering rare nuclear lines as evidence of double neutron star\nmergers. Throughout the paper, we present simulations of each class of objects,\ndemonstrating the power of both the imaging and spectral capabilities of HEX-P\nto advance our knowledge of SNRs, PWNe, and nuclear astrophysics.",
            "author": [
                "Stephen Reynolds",
                "Hongjun An",
                "Moaz Abdelmaguid",
                "Jason Alford",
                "Chris L. Fryer",
                "Kaya Mori",
                "Melania Nynka",
                "Jaegeun Park",
                "Yukikatsu Terada",
                "Jooyun Woo",
                "Aya Bamba",
                "Priyadarshini Bangale",
                "Rebecca Diesing",
                "Jordan Eagle",
                "Stefano Gabici",
                "Joseph Gelfand",
                "Brian Grefenstette",
                "Javier Garcia",
                "Chanho Kim",
                "Sajan Kumar",
                "Brydyn Mac Intyre",
                "Kristin Madsen",
                "Silvia Manconi",
                "Yugo Motogami",
                "Hayato Ohsumi",
                "Barbara Olmi",
                "Toshiki Sato",
                "Ruo-Yu Shang",
                "Daniel Stern",
                "Naomi Tsuji",
                "George Younes",
                "Andreas Zoglauer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04952v1",
                "http://arxiv.org/pdf/2311.04952v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04818v2",
            "title": "Cross-Silo Federated Learning Across Divergent Domains with Iterative\n  Parameter Alignment",
            "updated": "2023-11-14T21:59:36Z",
            "published": "2023-11-08T16:42:14Z",
            "summary": "Learning from the collective knowledge of data dispersed across private\nsources can provide neural networks with enhanced generalization capabilities.\nFederated learning, a method for collaboratively training a machine learning\nmodel across remote clients, achieves this by combining client models via the\norchestration of a central server. However, current approaches face two\ncritical limitations: i) they struggle to converge when client domains are\nsufficiently different, and ii) current aggregation techniques produce an\nidentical global model for each client. In this work, we address these issues\nby reformulating the typical federated learning setup: rather than learning a\nsingle global model, we learn N models each optimized for a common objective.\nTo achieve this, we apply a weighted distance minimization to model parameters\nshared in a peer-to-peer topology. The resulting framework, Iterative Parameter\nAlignment, applies naturally to the cross-silo setting, and has the following\nproperties: (i) a unique solution for each participant, with the option to\nglobally converge each model in the federation, and (ii) an optional\nearly-stopping mechanism to elicit fairness among peers in collaborative\nlearning settings. These characteristics jointly provide a flexible new\nframework for iteratively learning from peer models trained on disparate\ndatasets. We find that the technique achieves competitive results on a variety\nof data partitions compared to state-of-the-art approaches. Further, we show\nthat the method is robust to divergent domains (i.e. disjoint classes across\npeers) where existing approaches struggle.",
            "author": [
                "Matt Gorbett",
                "Hossein Shirazi",
                "Indrakshi Ray"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04818v2",
                "http://arxiv.org/pdf/2311.04818v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04816v1",
            "title": "MTGER: Multi-view Temporal Graph Enhanced Temporal Reasoning over\n  Time-Involved Document",
            "updated": "2023-11-08T16:41:37Z",
            "published": "2023-11-08T16:41:37Z",
            "summary": "The facts and time in the document are intricately intertwined, making\ntemporal reasoning over documents challenging. Previous work models time\nimplicitly, making it difficult to handle such complex relationships. To\naddress this issue, we propose MTGER, a novel Multi-view Temporal Graph\nEnhanced Temporal Reasoning framework for temporal reasoning over time-involved\ndocuments. Concretely, MTGER explicitly models the temporal relationships among\nfacts by multi-view temporal graphs. On the one hand, the heterogeneous\ntemporal graphs explicitly model the temporal and discourse relationships among\nfacts; on the other hand, the multi-view mechanism captures both time-focused\nand fact-focused information, allowing the two views to complement each other\nthrough adaptive fusion. To further improve the implicit reasoning capability\nof the model, we design a self-supervised time-comparing objective. Extensive\nexperimental results demonstrate the effectiveness of our method on the TimeQA\nand SituatedQA datasets. Furthermore, MTGER gives more consistent answers under\nquestion perturbations.",
            "author": [
                "Zheng Chu",
                "Zekun Wang",
                "Jiafeng Liang",
                "Ming Liu",
                "Bing Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04816v1",
                "http://arxiv.org/pdf/2311.04816v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04800v1",
            "title": "The minimum degree of $(K_s, K_t)$-co-critical graphs",
            "updated": "2023-11-08T16:19:08Z",
            "published": "2023-11-08T16:19:08Z",
            "summary": "Given graphs $G, H_1, H_2$, we write $G \\rightarrow ({H}_1, H_2)$ if every\n\\{red, blue\\}-coloring of the edges of $G$ contains a red copy of $H_1$ or a\nblue copy of $H_2$. A non-complete graph $G$ is $(H_1, H_2)$-co-critical if $G\n\\nrightarrow ({H}_1, H_2)$ and $G+e\\rightarrow ({H}_1, H_2)$ for every edge $e$\nin the complement of $G$. The notion of co-critical graphs was initiated by\nNe$\\check{s}$et$\\check{r}$il in 1986. Galluccio, Simonovits and Simonyi in 1992\nproved that every $(K_3, K_3)$-co-critical graph on $n\\ge6$ vertices has\nminimum degree at least four, and the bound is sharp for all $n\\ge 6$. In this\npaper, we first extend the aforementioned result to all $(K_s,\nK_t)$-co-critical graphs by showing that every $(K_s, K_t)$-co-critical graph\nhas minimum degree at least $2t+s-5$, where $t\\ge s\\ge 3$. We then prove that\nevery $(K_3, K_4)$-co-critical graph on $n\\ge9$ vertices has minimum degree at\nleast seven, and the bound is sharp for all $n\\ge 9$. This answers a question\nof the third author in the positive for the case $s=3$ and $t=4$.",
            "author": [
                "Ivan Casas-Rocha",
                "Benjamin Snyder",
                "Zi-Xia Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04800v1",
                "http://arxiv.org/pdf/2311.04800v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C55, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04778v1",
            "title": "On the Multiple Roles of Ontologies in Explainable AI",
            "updated": "2023-11-08T15:57:26Z",
            "published": "2023-11-08T15:57:26Z",
            "summary": "This paper discusses the different roles that explicit knowledge, in\nparticular ontologies, can play in Explainable AI and in the development of\nhuman-centric explainable systems and intelligible explanations. We consider\nthree main perspectives in which ontologies can contribute significantly,\nnamely reference modelling, common-sense reasoning, and knowledge refinement\nand complexity management. We overview some of the existing approaches in the\nliterature, and we position them according to these three proposed\nperspectives. The paper concludes by discussing what challenges still need to\nbe addressed to enable ontology-based approaches to explanation and to evaluate\ntheir human-understandability and effectiveness.",
            "author": [
                "Roberto Confalonieri",
                "Giancarlo Guizzardi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04778v1",
                "http://arxiv.org/pdf/2311.04778v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04772v1",
            "title": "GCS-ICHNet: Assessment of Intracerebral Hemorrhage Prognosis using\n  Self-Attention with Domain Knowledge Integration",
            "updated": "2023-11-08T15:51:12Z",
            "published": "2023-11-08T15:51:12Z",
            "summary": "Intracerebral Hemorrhage (ICH) is a severe condition resulting from damaged\nbrain blood vessel ruptures, often leading to complications and fatalities.\nTimely and accurate prognosis and management are essential due to its high\nmortality rate. However, conventional methods heavily rely on subjective\nclinician expertise, which can lead to inaccurate diagnoses and delays in\ntreatment. Artificial intelligence (AI) models have been explored to assist\nclinicians, but many prior studies focused on model modification without\nconsidering domain knowledge. This paper introduces a novel deep learning\nalgorithm, GCS-ICHNet, which integrates multimodal brain CT image data and the\nGlasgow Coma Scale (GCS) score to improve ICH prognosis. The algorithm utilizes\na transformer-based fusion module for assessment. GCS-ICHNet demonstrates high\nsensitivity 81.03% and specificity 91.59%, outperforming average clinicians and\nother state-of-the-art methods.",
            "author": [
                "Xuhao Shan",
                "Xinyang Li",
                "Ruiquan Ge",
                "Shibin Wu",
                "Ahmed Elazab",
                "Jichao Zhu",
                "Lingyan Zhang",
                "Gangyong Jia",
                "Qingying Xiao",
                "Xiang Wan",
                "Changmiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04772v1",
                "http://arxiv.org/pdf/2311.04772v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04761v1",
            "title": "FAIR Knowledge Graphs with Semantic Units: a Prototype",
            "updated": "2023-11-08T15:34:15Z",
            "published": "2023-11-08T15:34:15Z",
            "summary": "Knowledge graphs and ontologies are becoming increasingly important in the\ncontext of making data and metadata findable, accessible, interoperable, and\nreusable (FAIR). We introduce the concept of Semantic Units for organizing\nKnowledge Graphs into identifiable and semantically meaningful subgraphs. Each\nSemantic Unit is represented in the graph by its own resource that instantiates\na Semantic Unit class. Different types of Semantic Units are distinguished, and\ntogether they can organize a Knowledge Graph into different levels of\nrepresentational granularity with partially overlapping, partially enclosed\nsubgraphs that users of Knowledge Graphs can refer to for making statements\nabout statements. The use of Semantic Units in Knowledge Graphs supports making\nthem FAIR and increases the human-reader-actionability of their data and\nmetadata by increasing the graph's cognitive interoperability by increasing its\nexplorability for a human reader. We introduce a minimal prototype web\napplication for a user-driven FAIR Knowledge Graph that is based on Semantic\nUnits.",
            "author": [
                "Lars Vogt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04761v1",
                "http://arxiv.org/pdf/2311.04761v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04749v1",
            "title": "Online Min Cost Circulation for Multi-Object Tracking on Fragments",
            "updated": "2023-11-08T15:18:09Z",
            "published": "2023-11-08T15:18:09Z",
            "summary": "Multi-object tracking (MOT) or global data association problem is commonly\napproached as a minimum-cost-flow or minimum-cost-circulation problem on a\ngraph. While there have been numerous studies aimed at enhancing algorithm\nefficiency, most of them focus on the batch problem, where all the data must be\navailable simultaneously to construct a static graph. However, with the growing\nnumber of applications that generate streaming data, an efficient online\nalgorithm is required to handle the streaming nature of the input. In this\npaper, we present an online extension of the well-known negative cycle\ncanceling algorithm for solving the multi-object tracking problem with\nstreaming fragmented data. We provide a proof of correctness for the proposed\nalgorithm and demonstrate its efficiency through numerical experiments.",
            "author": [
                "Yanbing Wang",
                "Junyi Ji",
                "William Barbour",
                "Daniel B. Work"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04749v1",
                "http://arxiv.org/pdf/2311.04749v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04743v1",
            "title": "Sharper analysis of the random graph $d$-process via a balls-in-bins\n  model",
            "updated": "2023-11-08T15:12:17Z",
            "published": "2023-11-08T15:12:17Z",
            "summary": "A graph $d$-process starts with an empty graph on $n$ vertices, and adds one\nedge at each time step, chosen uniformly at random from those pairs which are\nnot yet edges and whose both vertices have current degree less than $d$. If, in\nthe final graph, at most one vertex has degree $d-1$ and all other have degree\n$d$, we call the process saturated. We present a new approach to analysing this\nprocess based on random allocation of balls in bins. This allows us to get\nimproved results on the degree distribution throughout the process and,\nconsequently, to determine the asymptotic probability of non-saturation of the\nprocess.",
            "author": [
                "Andrzej Rucinski",
                "Nick Wormald"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04743v1",
                "http://arxiv.org/pdf/2311.04743v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04707v1",
            "title": "Where Do We Meet? Key Factors Influencing Collaboration Across Meeting\n  Spaces",
            "updated": "2023-11-08T14:27:56Z",
            "published": "2023-11-08T14:27:56Z",
            "summary": "Over the past years, there has been a shift towards online and hybrid meeting\nforms in workplace environments, partly as a consequence of various COVID-19\nrestrictions. However, the decision-making process on how to best collaborate\nwith team members is predominantly driven by practical concerns. While there is\na significant body of literature about where to best meet, this knowledge is\nfragmented across various disciplines and hard to use in novel meeting\nsolutions. We present the Cross-Space Collaboration model which identifies the\nmain factors that drive the features of in-person collaboration and the meeting\naspects that influence these factors such as cognitive load. We designed the\nmodel to give guidance to teams and individuals on how to meet in order to have\na higher collaboration effectiveness. Finally, we outline how the model can\nbring added value within new meeting solutions, next generation virtual reality\nmeeting spaces and educational settings.",
            "author": [
                "Isaac Valadez",
                "Sandra Trullemans",
                "Beat Signer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04707v1",
                "http://arxiv.org/pdf/2311.04707v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "H.5.3; H.5.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04689v1",
            "title": "Extremal Polynomial Norms of Graphs",
            "updated": "2023-11-08T13:55:31Z",
            "published": "2023-11-08T13:55:31Z",
            "summary": "Recent work shows that a new family of norms on Hermitian matrices arise by\nevaluating the even degree complete homogeneous symmetric (CHS) polynomials on\nthe eigenvalues of a Hermitian matrix. The CHS norm of a graph is then defined\nby evaluating the even degree CHS polynomials on the eigenvalues of the\nadjacency matrix of a graph. The fact that these norms are defined in terms of\neigenvalues (as opposed to singular values) ensures they can distinguish\nbetween graphs that other norms cannot. In addition, we prove that the CHS\nnorms are minimized over all connected graphs by the path and maximized over\nall connected graphs by the complete graph. Finally, we prove that the CHS\nnorms are minimized over all trees by the path and maximized over all trees by\nthe star. Our paper is intended for a wide mathematical audience and we assume\nno prior knowledge about graphs or symmetric polynomials.",
            "author": [
                "\u00c1ngel Ch\u00e1vez",
                "Sarah Fullerton",
                "Matilda LaFortune",
                "Keyron Linarez",
                "Nethmin Liyanage",
                "Justin Son",
                "Tyler Ting"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04689v1",
                "http://arxiv.org/pdf/2311.04689v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04679v1",
            "title": "The High Energy X-ray Probe (HEX-P): Probing the physics of the X-ray\n  corona in active galactic nuclei",
            "updated": "2023-11-08T13:37:26Z",
            "published": "2023-11-08T13:37:26Z",
            "summary": "The hard X-ray emission in active galactic nuclei (AGN) and black hole X-ray\nbinaries is thought to be produced by a hot cloud of electrons referred to as\nthe corona. This emission, commonly described by a power law with a high-energy\ncutoff, is suggestive of Comptonization by thermal electrons. While several\nhypotheses have been proposed to explain the origin, geometry, and composition\nof the corona, we still lack a clear understanding of this fundamental\ncomponent. NuSTAR has been playing a key role improving our knowledge of X-ray\ncoronae thanks to its unprecedented sensitivity above 10 keV. However, these\nconstraints are limited to bright, nearby sources. The High Energy X-ray Probe\n(HEX-P) is a probe-class mission concept combining high spatial resolution\nX-ray imaging and broad spectral coverage (0.2-80 keV) with a sensitivity\nsuperior to current facilities. In this paper, we highlight the major role that\nHEX-P will play in further advancing our insights of X-ray coronae, notably in\nAGN. We demonstrate how HEX-P will measure key properties and track the\ntemporal evolution of coronae in unobscured AGN. This will allow us to\ndetermine their electron distribution and test the dominant emission\nmechanisms. Furthermore, we show how HEX-P will accurately estimate the coronal\nproperties of obscured AGN in the local Universe, helping address fundamental\nquestions about AGN unification. In addition, HEX-P will characterize coronae\nin a large sample of luminous quasars at cosmological redshifts for the first\ntime and track the evolution of coronae in transient systems in real time. We\nalso demonstrate how HEX-P will enable estimating the coronal geometry using\nspectral-timing techniques. HEX-P will thus be essential to understand the\nevolution and growth of black holes over a broad range of mass, distance, and\nluminosity, and will help uncover the black holes' role in shaping the\nUniverse.",
            "author": [
                "E. Kammoun",
                "A. M. Lohfink",
                "M. Masterson",
                "D. R. Wilkins",
                "X. Zhao",
                "M. Balokovi\u0107",
                "P. G. Boorman",
                "R. M. T. Connors",
                "P. Coppi",
                "A. C. Fabian",
                "J. A. Garc\u00eda",
                "K. K. Madsen",
                "N. Rodriguez Cavero",
                "N. Sridhar",
                "D. Stern",
                "J. Tomsick",
                "T. Wevers",
                "D. J. Walton",
                "S. Bianchi",
                "J. Buchner",
                "F. Civano",
                "G. Lanzuisi",
                "L. Mallick",
                "G. Matt",
                "A. Merloni",
                "E. Nardini",
                "J. M. Piotrowska",
                "C. Ricci",
                "K. -W. Wong",
                "A. Zoghbi",
                "the HEX-P Collaboration"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04679v1",
                "http://arxiv.org/pdf/2311.04679v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04673v1",
            "title": "Compressive Recovery of Sparse Precision Matrices",
            "updated": "2023-11-08T13:29:08Z",
            "published": "2023-11-08T13:29:08Z",
            "summary": "We consider the problem of learning a graph modeling the statistical\nrelations of the $d$ variables of a dataset with $n$ samples $X \\in\n\\mathbb{R}^{n \\times d}$. Standard approaches amount to searching for a\nprecision matrix $\\Theta$ representative of a Gaussian graphical model that\nadequately explains the data. However, most maximum likelihood-based estimators\nusually require storing the $d^{2}$ values of the empirical covariance matrix,\nwhich can become prohibitive in a high-dimensional setting. In this work, we\nadopt a compressive viewpoint and aim to estimate a sparse $\\Theta$ from a\nsketch of the data, i.e. a low-dimensional vector of size $m \\ll d^{2}$\ncarefully designed from $X$ using nonlinear random features. Under certain\nassumptions on the spectrum of $\\Theta$ (or its condition number), we show that\nit is possible to estimate it from a sketch of size $m=\\Omega((d+2k)\\log(d))$\nwhere $k$ is the maximal number of edges of the underlying graph. These\ninformation-theoretic guarantees are inspired by compressed sensing theory and\ninvolve restricted isometry properties and instance optimal decoders. We\ninvestigate the possibility of achieving practical recovery with an iterative\nalgorithm based on the graphical lasso, viewed as a specific denoiser. We\ncompare our approach and graphical lasso on synthetic datasets, demonstrating\nits favorable performance even when the dataset is compressed.",
            "author": [
                "Titouan Vayer",
                "Etienne Lasalle",
                "R\u00e9mi Gribonval",
                "Paulo Gon\u00e7alves"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04673v1",
                "http://arxiv.org/pdf/2311.04673v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04665v1",
            "title": "A Complex Network Analysis on The Eigenvalue Spectra of Random Spin\n  Systems",
            "updated": "2023-11-08T13:12:55Z",
            "published": "2023-11-08T13:12:55Z",
            "summary": "Recent works have established a novel viewpoint that treats the eigenvalue\nspectra of disordered quantum systems as time-series, and corresponding\nalgorithms such as singular-value-decomposition has proven its advantage in\nstudying subtle physical quantities like Thouless energy and non-ergodic\nextended regime. On the other hand, algorithms from complex networks have long\nbeen known as a powerful tool to study highly nonlinear time-series. In this\nwork, we combine these two ideas together. Using the particular algorithm\ncalled visibility graph (VG) that transforms the eigenvalue spectra of a random\nspin system into complex networks, it's shown the degree distribution of the\nresulting network is capable of signaturing the eigenvalue evolution during the\nthermal to many-body localization transition, and the networks in the thermal\nphase have a small-world structure. We further show these results are robust\neven when the eigenvalues are incomplete with missing levels, which reveals the\nadvantage of the VG algorithm.",
            "author": [
                "Qiaomu Xue",
                "Wenjia Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04665v1",
                "http://arxiv.org/pdf/2311.04665v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04661v2",
            "title": "Massive Editing for Large Language Models via Meta Learning",
            "updated": "2023-11-09T11:07:15Z",
            "published": "2023-11-08T13:03:06Z",
            "summary": "While large language models (LLMs) have enabled learning knowledge from the\npre-training corpora, the acquired knowledge may be fundamentally incorrect or\noutdated over time, which necessitates rectifying the knowledge of the language\nmodel (LM) after the training. A promising approach involves employing a\nhyper-network to generate parameter shift, whereas existing hyper-networks\nsuffer from inferior scalability in synchronous editing operation amount. To\nmitigate the problem, we propose the MAssive Language Model Editing Network\n(MALMEN), which formulates the parameter shift aggregation as the least square\nproblem, subsequently updating the LM parameters using the normal equation. To\naccommodate editing multiple facts simultaneously with limited memory budgets,\nwe separate the computation on the hyper-network and LM, enabling arbitrary\nbatch size on both neural networks. Our method is evaluated by editing up to\nthousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2,\nT5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks,\ni.e., closed book fact-checking and question answering. Remarkably, MALMEN is\ncapable of editing hundreds of times more facts than strong baselines with the\nidentical hyper-network architecture and outperforms editor specifically\ndesigned for GPT. Our code is available at\nhttps://github.com/ChenmienTan/malmen.",
            "author": [
                "Chenmien Tan",
                "Ge Zhang",
                "Jie Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04661v2",
                "http://arxiv.org/pdf/2311.04661v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04656v1",
            "title": "Computing pivot-minors",
            "updated": "2023-11-08T12:55:32Z",
            "published": "2023-11-08T12:55:32Z",
            "summary": "A graph $G$ contains a graph $H$ as a pivot-minor if $H$ can be obtained from\n$G$ by applying a sequence of vertex deletions and edge pivots. Pivot-minors\nplay an important role in the study of rank-width. Pivot-minors have mainly\nbeen studied from a structural perspective. In this paper we perform the first\nsystematic computational complexity study of pivot-minors. We first prove that\nthe Pivot-Minor problem, which asks if a given graph $G$ contains a pivot-minor\nisomorphic to a given graph $H$, is NP-complete. If $H$ is not part of the\ninput, we denote the problem by $H$-Pivot-Minor. We give a certifying\npolynomial-time algorithm for $H$-Pivot-Minor when (1) $H$ is an induced\nsubgraph of $P_3+tP_1$ for some integer $t\\geq 0$, (2) $H=K_{1,t}$ for some\ninteger $t\\geq 1$, or (3) $|V(H)|\\leq 4$ except when $H \\in \\{K_4,C_3+ P_1\\}$.\nLet ${\\cal F}_H$ be the set of induced-subgraph-minimal graphs that contain a\npivot-minor isomorphic to $H$. To prove the above statement, we either show\nthat there is an integer $c_H$ such that all graphs in ${\\cal F}_H$ have at\nmost $c_H$ vertices, or we determine ${\\cal F}_H$ precisely, for each of the\nabove cases.",
            "author": [
                "Konrad K. Dabrowski",
                "Fran\u00e7ois Dross",
                "Jisu Jeong",
                "Mamadou Moustapha Kant\u00e9",
                "O-joung Kwon",
                "Sang-il Oum",
                "Dani\u00ebl Paulusma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04656v1",
                "http://arxiv.org/pdf/2311.04656v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04655v1",
            "title": "Two new algorithms for solving M\u00fcller games and their applications",
            "updated": "2023-11-08T12:54:23Z",
            "published": "2023-11-08T12:54:23Z",
            "summary": "M\\\"uller games form a well-established class of games for model checking and\nverification. These games are played on directed graphs $\\mathcal G$ where\nPlayer 0 and Player 1 play by generating an infinite path through the graph.\nThe winner is determined by the set $X$ consisting of all vertices in the path\nthat occur infinitely often. If $X$ belongs to $\\Omega$, a specified collection\nof subsets of $\\mathcal G$, then Player 0 wins. Otherwise, Player 1 claims the\nwin. These games are determined, enabling the partitioning of $\\mathcal G$ into\ntwo sets $W_0$ and $W_1$ of winning positions for Player 0 and Player 1,\nrespectively. Numerous algorithms exist that decide M\\\"uller games $\\mathcal G$\nby computing the sets $W_0$ and $W_1$. In this paper, we introduce two novel\nalgorithms that outperform all previously known methods for deciding explicitly\ngiven M\\\"uller games, especially in the worst-case scenarios. The previously\nknown algorithms either reduce M\\\"uller games to other known games (e.g. safety\ngames) or recursively change the underlying graph $\\mathcal G$ and the\ncollection of sets in $\\Omega$. In contrast, our approach does not employ these\ntechniques but instead leverages subgames, the sets within $\\Omega$, and their\ninteractions. This distinct methodology sets our algorithms apart from prior\napproaches for deciding M\\\"uller games. Additionally, our algorithms offer\nenhanced clarity and ease of comprehension. Importantly, our techniques are\napplicable not only to M\\\"uller games but also to improving the performance of\nexisting algorithms that handle other game classes, including coloured M\\\"uller\ngames, McNaughton games, Rabin games, and Streett games.",
            "author": [
                "Zihui Liang",
                "Bakh Khoussainov",
                "Mingyu Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04655v1",
                "http://arxiv.org/pdf/2311.04655v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04653v1",
            "title": "Hybrid Focal and Full-Range Attention Based Graph Transformers",
            "updated": "2023-11-08T12:53:07Z",
            "published": "2023-11-08T12:53:07Z",
            "summary": "The paradigm of Transformers using the self-attention mechanism has\nmanifested its advantage in learning graph-structured data. Yet, Graph\nTransformers are capable of modeling full range dependencies but are often\ndeficient in extracting information from locality. A common practice is to\nutilize Message Passing Neural Networks (MPNNs) as an auxiliary to capture\nlocal information, which however are still inadequate for comprehending\nsubstructures. In this paper, we present a purely attention-based architecture,\nnamely Focal and Full-Range Graph Transformer (FFGT), which can mitigate the\nloss of local information in learning global correlations. The core component\nof FFGT is a new mechanism of compound attention, which combines the\nconventional full-range attention with K-hop focal attention on ego-nets to\naggregate both global and local information. Beyond the scope of canonical\nTransformers, the FFGT has the merit of being more substructure-aware. Our\napproach enhances the performance of existing Graph Transformers on various\nopen datasets, while achieves compatible SOTA performance on several Long-Range\nGraph Benchmark (LRGB) datasets even with a vanilla transformer. We further\nexamine influential factors on the optimal focal length of attention via\nintroducing a novel synthetic dataset based on SBM-PATTERN.",
            "author": [
                "Minhong Zhu",
                "Zhenhao Zhao",
                "Weiran Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04653v1",
                "http://arxiv.org/pdf/2311.04653v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04635v1",
            "title": "Towards Deeper, Lighter and Interpretable Cross Network for CTR\n  Prediction",
            "updated": "2023-11-08T12:29:16Z",
            "published": "2023-11-08T12:29:16Z",
            "summary": "Click Through Rate (CTR) prediction plays an essential role in recommender\nsystems and online advertising. It is crucial to effectively model feature\ninteractions to improve the prediction performance of CTR models. However,\nexisting methods face three significant challenges. First, while most methods\ncan automatically capture high-order feature interactions, their performance\ntends to diminish as the order of feature interactions increases. Second,\nexisting methods lack the ability to provide convincing interpretations of the\nprediction results, especially for high-order feature interactions, which\nlimits the trustworthiness of their predictions. Third, many methods suffer\nfrom the presence of redundant parameters, particularly in the embedding layer.\nThis paper proposes a novel method called Gated Deep Cross Network (GDCN) and a\nField-level Dimension Optimization (FDO) approach to address these challenges.\nAs the core structure of GDCN, Gated Cross Network (GCN) captures explicit\nhigh-order feature interactions and dynamically filters important interactions\nwith an information gate in each order. Additionally, we use the FDO approach\nto learn condensed dimensions for each field based on their importance.\nComprehensive experiments on five datasets demonstrate the effectiveness,\nsuperiority and interpretability of GDCN. Moreover, we verify the effectiveness\nof FDO in learning various dimensions and reducing model parameters. The code\nis available on \\url{https://github.com/anonctr/GDCN}.",
            "author": [
                "Fangye Wang",
                "Hansu Gu",
                "Dongsheng Li",
                "Tun Lu",
                "Peng Zhang",
                "Ning Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04635v1",
                "http://arxiv.org/pdf/2311.04635v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04617v1",
            "title": "Image Patch-Matching with Graph-Based Learning in Street Scenes",
            "updated": "2023-11-08T11:35:43Z",
            "published": "2023-11-08T11:35:43Z",
            "summary": "Matching landmark patches from a real-time image captured by an on-vehicle\ncamera with landmark patches in an image database plays an important role in\nvarious computer perception tasks for autonomous driving. Current methods focus\non local matching for regions of interest and do not take into account spatial\nneighborhood relationships among the image patches, which typically correspond\nto objects in the environment. In this paper, we construct a spatial graph with\nthe graph vertices corresponding to patches and edges capturing the spatial\nneighborhood information. We propose a joint feature and metric learning model\nwith graph-based learning. We provide a theoretical basis for the graph-based\nloss by showing that the information distance between the distributions\nconditioned on matched and unmatched pairs is maximized under our framework. We\nevaluate our model using several street-scene datasets and demonstrate that our\napproach achieves state-of-the-art matching results.",
            "author": [
                "Rui She",
                "Qiyu Kang",
                "Sijie Wang",
                "Wee Peng Tay",
                "Yong Liang Guan",
                "Diego Navarro Navarro",
                "Andreas Hartmannsgruber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04617v1",
                "http://arxiv.org/pdf/2311.04617v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04598v1",
            "title": "Robust Portfolio Optimization under Ambiguous Chance Constraints",
            "updated": "2023-11-08T10:59:43Z",
            "published": "2023-11-08T10:59:43Z",
            "summary": "In this paper, we discuss the ambiguous chance constrained based portfolio\noptimization problems, in which the perturbations associated with the input\nparameters are stochastic in nature, but their distributions are not known\nprecisely. We consider two different families of perturbation distributions --\none is when only upper & lower bounds of mean values are known to us, and the\nsecond one is along with the mean bounds, we also have the knowledge of the\nstandard deviations of the perturbations. We derive the safe convex\napproximations of such chance constrained portfolio problems by using some\nsuitable generating functions such as a piecewise linear function, an\nexponential function, and a piecewise quadratic function. These safe\napproximations are the robust counterparts to our ambiguous chance constrained\nproblem and they are computationally tractable due to the convex nature of\nthese approximations.",
            "author": [
                "Pulak Swain",
                "Akshay Kumar Ojha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04598v1",
                "http://arxiv.org/pdf/2311.04598v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04574v1",
            "title": "Simple and Asymptotically Optimal Online Bipartite Edge Coloring",
            "updated": "2023-11-08T10:14:49Z",
            "published": "2023-11-08T10:14:49Z",
            "summary": "We provide a simple online $\\Delta(1+o(1))$-edge-coloring algorithm for\nbipartite graphs of maximum degree $\\Delta=\\omega(\\log n)$ under adversarial\nvertex arrivals on one side of the graph. Our algorithm slightly improves the\nresult of (Cohen, Peng and Wajc, FOCS19), which was the first, and currently\nonly, to obtain an asymptotically optimal $\\Delta(1+o(1))$ guarantee for an\nadversarial arrival model. More importantly, our algorithm provides a new,\nsimpler approach for tackling online edge coloring.",
            "author": [
                "Joakim Blikstad",
                "Ola Svensson",
                "Radu Vintan",
                "David Wajc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04574v1",
                "http://arxiv.org/pdf/2311.04574v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04561v1",
            "title": "Information-Theoretic Generalization Bounds for Transductive Learning\n  and its Applications",
            "updated": "2023-11-08T09:48:42Z",
            "published": "2023-11-08T09:48:42Z",
            "summary": "In this paper, we develop data-dependent and algorithm-dependent\ngeneralization bounds for transductive learning algorithms in the context of\ninformation theory for the first time. We show that the generalization gap of\ntransductive learning algorithms can be bounded by the mutual information\nbetween training labels and hypothesis. By innovatively proposing the concept\nof transductive supersamples, we go beyond the inductive learning setting and\nestablish upper bounds in terms of various information measures. Furthermore,\nwe derive novel PAC-Bayesian bounds and build the connection between\ngeneralization and loss landscape flatness under the transductive learning\nsetting. Finally, we present the upper bounds for adaptive optimization\nalgorithms and demonstrate the applications of results on semi-supervised\nlearning and graph learning scenarios. Our theoretic results are validated on\nboth synthetic and real-world datasets.",
            "author": [
                "Huayi Tang",
                "Yong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04561v1",
                "http://arxiv.org/pdf/2311.04561v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04549v1",
            "title": "From Input to Output: A Multi-layer Knowledge Distillation Framework for\n  Compressing Recommendation Models",
            "updated": "2023-11-08T09:31:48Z",
            "published": "2023-11-08T09:31:48Z",
            "summary": "To reduce the size of recommendation models, there have been many studies on\ncompressing recommendation models using knowledge distillation. In this paper,\nwe decompose recommendation models into three layers, i.e., the input layer,\nthe intermediate layer, and the output layer, and address deficiencies layer by\nlayer. First, previous methods focus only on two layers, neglecting the input\nlayer. Second, in the intermediate layer, existing methods ignore the\ninconsistency of user preferences induced by the projectors. Third, in the\noutput layer, existing methods use only hard labels rather than soft labels\nfrom the teacher. To address these deficiencies, we propose\n\\textbf{M}ulti-layer \\textbf{K}nowledge \\textbf{D}istillation (MKD), which\nconsists of three components: 1) Distillation with Neighbor-based Knowledge\n(NKD) utilizes the teacher's knowledge about entities with similar\ncharacteristics in the input layer to enable the student to learn robust\nrepresentations. 2) Distillation with Consistent Preference (CPD) reduces the\ninconsistency of user preferences caused by projectors in the intermediate\nlayer by two regularization terms. 3) Distillation with Soft Labels (SLD)\nconstructs soft labels in the output layer by considering the predictions of\nboth the teacher and the student. Our extensive experiments show that MKD even\noutperforms the teacher with one-tenth of the model size.",
            "author": [
                "Zhangchi Zhu",
                "Wei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04549v1",
                "http://arxiv.org/pdf/2311.04549v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04537v1",
            "title": "Deep Learning Assisted Multiuser MIMO Load Modulated Systems for\n  Enhanced Downlink mmWave Communications",
            "updated": "2023-11-08T08:54:56Z",
            "published": "2023-11-08T08:54:56Z",
            "summary": "This paper is focused on multiuser load modulation arrays (MU-LMAs) which are\nattractive due to their low system complexity and reduced cost for millimeter\nwave (mmWave) multi-input multi-output (MIMO) systems. The existing precoding\nalgorithm for downlink MU-LMA relies on a sub-array structured (SAS)\ntransmitter which may suffer from decreased degrees of freedom and complex\nsystem configuration. Furthermore, a conventional LMA codebook with codewords\nuniformly distributed on a hypersphere may not be channel-adaptive and may lead\nto increased signal detection complexity. In this paper, we conceive an MU-LMA\nsystem employing a full-array structured (FAS) transmitter and propose two\nalgorithms accordingly. The proposed FAS-based system addresses the SAS\nstructural problems and can support larger numbers of users. For LMA-imposed\nconstant-power downlink precoding, we propose an FAS-based normalized block\ndiagonalization (FAS-NBD) algorithm. However, the forced normalization may\nresult in performance degradation. This degradation, together with the\naforementioned codebook design problems, is difficult to solve analytically.\nThis motivates us to propose a Deep Learning-enhanced (FAS-DL-NBD) algorithm\nfor adaptive codebook design and codebook-independent decoding. It is shown\nthat the proposed algorithms are robust to imperfect knowledge of channel state\ninformation and yield excellent error performance. Moreover, the FAS-DL-NBD\nalgorithm enables signal detection with low complexity as the number of bits\nper codeword increases.",
            "author": [
                "Ercong Yu",
                "Jinle Zhu",
                "Qiang Li",
                "Zilong Liu",
                "Hongyang Chen",
                "Shlomo Shamai",
                "H. Vincent Poor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04537v1",
                "http://arxiv.org/pdf/2311.04537v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04536v1",
            "title": "Distributed Uniform Partitioning of a Region using Opaque ASYNC Luminous\n  Mobile Robots",
            "updated": "2023-11-08T08:50:02Z",
            "published": "2023-11-08T08:50:02Z",
            "summary": "We are given $N$ autonomous mobile robots inside a bounded region. The robots\nare opaque which means that three collinear robots are unable to see each other\nas one of the robots acts as an obstruction for the other two. They operate in\nclassical \\emph{Look-Compute-Move} (LCM) activation cycles. Moreover, the\nrobots are oblivious except for a persistent light (which is why they are\ncalled \\emph{Luminous robots}) that can determine a color from a fixed color\nset. Obliviousness does not allow the robots to remember any information from\npast activation cycles. The Uniform Partitioning problem requires the robots to\npartition the whole region into sub-regions of equal area, each of which\ncontains exactly one robot. Due to application-oriented motivation, we, in this\npaper consider the region to be well-known geometric shapes such as rectangle,\nsquare and circle. We investigate the problem in \\emph{asynchronous} setting\nwhere there is no notion of common time and any robot gets activated at any\ntime with a fair assumption that every robot needs to get activated infinitely\noften. To the best of our knowledge, this is the first attempt to study the\nUniform Partitioning problem using oblivious opaque robots working under\nasynchronous settings. We propose three algorithms considering three different\nregions: rectangle, square and circle. Robots partition the region in a\ndistributed way and reach their respective positions in the partitions. The\nalgorithms proposed for rectangular and square regions run in $O(N)$ epochs\nwhereas the algorithm for circular regions runs in $O(N^2)$ epochs, where an\nepoch is the smallest unit of time in which all robots are activated at least\nonce and execute their LCM cycles.",
            "author": [
                "Subhajit Pramanick",
                "Saswata Jana",
                "Adri Bhattacharya",
                "Partha Sarathi Mandal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04536v1",
                "http://arxiv.org/pdf/2311.04536v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04533v1",
            "title": "Improved Approximations for Ultrametric Violation Distance",
            "updated": "2023-11-08T08:42:47Z",
            "published": "2023-11-08T08:42:47Z",
            "summary": "We study the Ultrametric Violation Distance problem introduced by\nCohen-Addad, Fan, Lee, and Mesmay [FOCS, 2022]. Given pairwise distances $x\\in\n\\mathbb{R}_{>0}^{\\binom{[n]}{2}}$ as input, the goal is to modify the minimum\nnumber of distances so as to make it a valid ultrametric. In other words, this\nis the problem of fitting an ultrametric to given data, where the quality of\nthe fit is measured by the $\\ell_0$ norm of the error; variants of the problem\nfor the $\\ell_\\infty$ and $\\ell_1$ norms are well-studied in the literature.\n  Our main result is a 5-approximation algorithm for Ultrametric Violation\nDistance, improving the previous best large constant factor ($\\geq 1000$)\napproximation algorithm. We give an $O(\\min\\{L,\\log n\\})$-approximation for\nweighted Ultrametric Violation Distance where the weights satisfy triangle\ninequality and $L$ is the number of distinct values in the input. We also give\na $16$-approximation for the problem on $k$-partite graphs, where the input is\nspecified on pairs of vertices that form a complete $k$-partite graph. All our\nresults use a unified algorithmic framework with small modifications for the\nthree cases.",
            "author": [
                "Moses Charikar",
                "Ruiquan Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04533v1",
                "http://arxiv.org/pdf/2311.04533v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04524v2",
            "title": "Validating ChatGPT Facts through RDF Knowledge Graphs and Sentence\n  Similarity",
            "updated": "2023-11-17T13:24:17Z",
            "published": "2023-11-08T08:27:11Z",
            "summary": "Since ChatGPT offers detailed responses without justifications, and erroneous\nfacts even for popular persons, events and places, in this paper we present a\nnovel pipeline that retrieves the response of ChatGPT in RDF and tries to\nvalidate the ChatGPT facts using one or more RDF Knowledge Graphs (KGs). To\nthis end we leverage DBpedia and LODsyndesis (an aggregated Knowledge Graph\nthat contains 2 billion triples from 400 RDF KGs of many domains) and short\nsentence embeddings, and introduce an algorithm that returns the more relevant\ntriple(s) accompanied by their provenance and a confidence score. This enables\nthe validation of ChatGPT responses and their enrichment with justifications\nand provenance. To evaluate this service (such services in general), we create\nan evaluation benchmark that includes 2,000 ChatGPT facts; specifically 1,000\nfacts for famous Greek Persons, 500 facts for popular Greek Places, and 500\nfacts for Events related to Greece. The facts were manually labelled\n(approximately 73% of ChatGPT facts were correct and 27% of facts were\nerroneous). The results are promising; indicatively for the whole benchmark, we\nmanaged to verify the 85.3% of the correct facts of ChatGPT and to find the\ncorrect answer for the 58% of the erroneous ChatGPT facts.",
            "author": [
                "Michalis Mountantonakis",
                "Yannis Tzitzikas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04524v2",
                "http://arxiv.org/pdf/2311.04524v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04521v1",
            "title": "Learning Robust Multi-Scale Representation for Neural Radiance Fields\n  from Unposed Images",
            "updated": "2023-11-08T08:18:23Z",
            "published": "2023-11-08T08:18:23Z",
            "summary": "We introduce an improved solution to the neural image-based rendering problem\nin computer vision. Given a set of images taken from a freely moving camera at\ntrain time, the proposed approach could synthesize a realistic image of the\nscene from a novel viewpoint at test time. The key ideas presented in this\npaper are (i) Recovering accurate camera parameters via a robust pipeline from\nunposed day-to-day images is equally crucial in neural novel view synthesis\nproblem; (ii) It is rather more practical to model object's content at\ndifferent resolutions since dramatic camera motion is highly likely in\nday-to-day unposed images. To incorporate the key ideas, we leverage the\nfundamentals of scene rigidity, multi-scale neural scene representation, and\nsingle-image depth prediction. Concretely, the proposed approach makes the\ncamera parameters as learnable in a neural fields-based modeling framework. By\nassuming per view depth prediction is given up to scale, we constrain the\nrelative pose between successive frames. From the relative poses, absolute\ncamera pose estimation is modeled via a graph-neural network-based multiple\nmotion averaging within the multi-scale neural-fields network, leading to a\nsingle loss function. Optimizing the introduced loss function provides camera\nintrinsic, extrinsic, and image rendering from unposed images. We demonstrate,\nwith examples, that for a unified framework to accurately model multiscale\nneural scene representation from day-to-day acquired unposed multi-view images,\nit is equally essential to have precise camera-pose estimates within the scene\nrepresentation framework. Without considering robustness measures in the camera\npose estimation pipeline, modeling for multi-scale aliasing artifacts can be\ncounterproductive. We present extensive experiments on several benchmark\ndatasets to demonstrate the suitability of our approach.",
            "author": [
                "Nishant Jain",
                "Suryansh Kumar",
                "Luc Van Gool"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04521v1",
                "http://arxiv.org/pdf/2311.04521v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04507v1",
            "title": "Conversation Understanding using Relational Temporal Graph Neural\n  Networks with Auxiliary Cross-Modality Interaction",
            "updated": "2023-11-08T07:46:25Z",
            "published": "2023-11-08T07:46:25Z",
            "summary": "Emotion recognition is a crucial task for human conversation understanding.\nIt becomes more challenging with the notion of multimodal data, e.g., language,\nvoice, and facial expressions. As a typical solution, the global- and the local\ncontext information are exploited to predict the emotional label for every\nsingle sentence, i.e., utterance, in the dialogue. Specifically, the global\nrepresentation could be captured via modeling of cross-modal interactions at\nthe conversation level. The local one is often inferred using the temporal\ninformation of speakers or emotional shifts, which neglects vital factors at\nthe utterance level. Additionally, most existing approaches take fused features\nof multiple modalities in an unified input without leveraging modality-specific\nrepresentations. Motivating from these problems, we propose the Relational\nTemporal Graph Neural Network with Auxiliary Cross-Modality Interaction\n(CORECT), an novel neural network framework that effectively captures\nconversation-level cross-modality interactions and utterance-level temporal\ndependencies with the modality-specific manner for conversation understanding.\nExtensive experiments demonstrate the effectiveness of CORECT via its\nstate-of-the-art results on the IEMOCAP and CMU-MOSEI datasets for the\nmultimodal ERC task.",
            "author": [
                "Cam-Van Thi Nguyen",
                "Anh-Tuan Mai",
                "The-Son Le",
                "Hai-Dang Kieu",
                "Duc-Trong Le"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04507v1",
                "http://arxiv.org/pdf/2311.04507v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04503v1",
            "title": "Constrained Adaptive Attacks: Realistic Evaluation of Adversarial\n  Examples and Robust Training of Deep Neural Networks for Tabular Data",
            "updated": "2023-11-08T07:35:28Z",
            "published": "2023-11-08T07:35:28Z",
            "summary": "State-of-the-art deep learning models for tabular data have recently achieved\nacceptable performance to be deployed in industrial settings. However, the\nrobustness of these models remains scarcely explored. Contrary to computer\nvision, there is to date no realistic protocol to properly evaluate the\nadversarial robustness of deep tabular models due to intrinsic properties of\ntabular data such as categorical features, immutability, and feature\nrelationship constraints. To fill this gap, we propose CAA, the first efficient\nevasion attack for constrained tabular deep learning models. CAA is an\niterative parameter-free attack that combines gradient and search attacks to\ngenerate adversarial examples under constraints. We leverage CAA to build a\nbenchmark of deep tabular models across three popular use cases: credit\nscoring, phishing and botnet attacks detection. Our benchmark supports ten\nthreat models with increasing capabilities of the attacker, and reflects\nreal-world attack scenarios for each use case. Overall, our results demonstrate\nhow domain knowledge, adversarial training, and attack budgets impact the\nrobustness assessment of deep tabular models and provide security practitioners\nwith a set of recommendations to improve the robustness of deep tabular models\nagainst various evasion attack scenarios.",
            "author": [
                "Thibault Simonetto",
                "Salah Ghamizi",
                "Antoine Desjardins",
                "Maxime Cordy",
                "Yves Le Traon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04503v1",
                "http://arxiv.org/pdf/2311.04503v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04495v1",
            "title": "Multi-label and Multi-target Sampling of Machine Annotation for\n  Computational Stance Detection",
            "updated": "2023-11-08T06:54:34Z",
            "published": "2023-11-08T06:54:34Z",
            "summary": "Data collection from manual labeling provides domain-specific and\ntask-aligned supervision for data-driven approaches, and a critical mass of\nwell-annotated resources is required to achieve reasonable performance in\nnatural language processing tasks. However, manual annotations are often\nchallenging to scale up in terms of time and budget, especially when domain\nknowledge, capturing subtle semantic features, and reasoning steps are needed.\nIn this paper, we investigate the efficacy of leveraging large language models\non automated labeling for computational stance detection. We empirically\nobserve that while large language models show strong potential as an\nalternative to human annotators, their sensitivity to task-specific\ninstructions and their intrinsic biases pose intriguing yet unique challenges\nin machine annotation. We introduce a multi-label and multi-target sampling\nstrategy to optimize the annotation quality. Experimental results on the\nbenchmark stance detection corpora show that our method can significantly\nimprove performance and learning efficacy.",
            "author": [
                "Zhengyuan Liu",
                "Hai Leong Chieu",
                "Nancy F. Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04495v1",
                "http://arxiv.org/pdf/2311.04495v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04486v1",
            "title": "On the diameter of Engel graphs",
            "updated": "2023-11-08T06:37:57Z",
            "published": "2023-11-08T06:37:57Z",
            "summary": "Given a finite group $G$, the Engel graph of $G$ is a directed graph\n$\\Gamma(G)$ encoding pairs of elements satisfying some Engel word. Namely,\n$\\Gamma(G)$ is the directed graph, where the vertices are the non-hypercentral\nelements of $G$ and where there is an arc from $x$ to $y$ if and only if $[x,_\nn y] = 1$ for some $n \\in \\mathbb{N}$. From previous work, it is known that,\nexcept for a few exceptions, $\\Gamma(G)$ is strongly connected. In this paper,\nwe give an absolute upper bound on the diameter of $\\Gamma(G)$, when\n$\\Gamma(G)$ is strongly connected.",
            "author": [
                "Andrea Lucchini",
                "Pablo Spiga"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04486v1",
                "http://arxiv.org/pdf/2311.04486v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04483v1",
            "title": "Cross-Domain Waveform Design for 6G Integrated Sensing and Communication",
            "updated": "2023-11-08T06:24:11Z",
            "published": "2023-11-08T06:24:11Z",
            "summary": "Orthogonal frequency division multiplexing (OFDM) is one of the\nrepresentative integrated sensing and communication (ISAC) waveforms, where\nsensing and communications tend to be assigned with different resource elements\n(REs) due to their diverse design requirements. This motivates optimization of\nresource allocation/waveform design across time, frequency, power and\ndelay-Doppler domains. Therefore, this article proposes two cross-domain\nwaveform optimization strategies for OFDM-based ISAC systems, following\ncommunication-centric and sensing-centric criteria, respectively. For the\ncommunication-centric design, to maximize the achievable data rate, a fraction\nof REs are optimally allocated for communications according to prior knowledge\nof the communication channel. The remaining REs are then employed for sensing,\nwhere the sidelobe level and peak to average power ratio are suppressed by\noptimizing its power-frequency and phase-frequency characteristics. For the\nsensing-centric design, a `locally' perfect auto-correlation property is\nensured by adjusting the unit cells of the ambiguity function within its region\nof interest (RoI). Afterwards, the irrelevant cells beyond RoI, which can\nreadily determine the sensing power allocation, are optimized with the\ncommunication power allocation to enhance the achievable data rate. Numerical\nresults demonstrate the superiority of the proposed communication-centric and\nsensing-centric waveform designs for ISAC applications.",
            "author": [
                "Fan Zhang",
                "Tianqi Mao",
                "Ruiqi Liu",
                "Zhu Han",
                "Octavia A. Dobre",
                "Sheng Chen",
                "Zhaocheng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04483v1",
                "http://arxiv.org/pdf/2311.04483v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04478v1",
            "title": "Mixed causality graphs for continuous-time state space models and\n  orthogonal projections",
            "updated": "2023-11-08T06:11:02Z",
            "published": "2023-11-08T06:11:02Z",
            "summary": "In this paper we derive (local) causality graphs for the popular\ncontinuous-time state space models, including in particular multivariate\ncontinuous-time ARMA (MCARMA) processes. In these (local) causality graphs,\nvertices represent the components of the process, directed edges between the\nvertices indicate causal influences, and undirected edges indicate\ncontemporaneous uncorrelatednesses between the component processes. We present\nsufficient criteria for state space models to satisfy the assumptions of\nFasen-Hartmann and Schenk (2023) so that the (local) causality graphs are well\ndefined and various causal Markov properties hold. Both directed and undirected\nedges in these graphs are characterised by orthogonal projections on\nwell-defined linear spaces. To compute these orthogonal projections, we use the\nunique controller canonical form of a state space model, which exists under\nmild assumptions, to recover the input process from the output process. We are\nthen able to derive some alternative representations of the output process and\nits highest derivative. Finally, we apply these representations to calculate\nthe necessary orthogonal projections, which culminate in the characterisations\nof the edges in the (local) causality graph. These characterisations are\ninterpretatively meaningful and are given by the parameters of the controller\ncanonical form and the covariance matrix of the driving L\\'evy process.",
            "author": [
                "Vicky Fasen-Hartmann",
                "Lea Schenk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04478v1",
                "http://arxiv.org/pdf/2311.04478v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "Primary 62H22, 62M20, Secondary 62M10, 60G25",
                "G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04467v1",
            "title": "RDGCN: Reinforced Dependency Graph Convolutional Network for\n  Aspect-based Sentiment Analysis",
            "updated": "2023-11-08T05:37:49Z",
            "published": "2023-11-08T05:37:49Z",
            "summary": "Aspect-based sentiment analysis (ABSA) is dedicated to forecasting the\nsentiment polarity of aspect terms within sentences. Employing graph neural\nnetworks to capture structural patterns from syntactic dependency parsing has\nbeen confirmed as an effective approach for boosting ABSA. In most works, the\ntopology of dependency trees or dependency-based attention coefficients is\noften loosely regarded as edges between aspects and opinions, which can result\nin insufficient and ambiguous syntactic utilization. To address these problems,\nwe propose a new reinforced dependency graph convolutional network (RDGCN) that\nimproves the importance calculation of dependencies in both distance and type\nviews. Initially, we propose an importance calculation criterion for the\nminimum distances over dependency trees. Under the criterion, we design a\ndistance-importance function that leverages reinforcement learning for weight\ndistribution search and dissimilarity control. Since dependency types often do\nnot have explicit syntax like tree distances, we use global attention and mask\nmechanisms to design type-importance functions. Finally, we merge these weights\nand implement feature aggregation and classification. Comprehensive experiments\non three popular datasets demonstrate the effectiveness of the criterion and\nimportance functions. RDGCN outperforms state-of-the-art GNN-based baselines in\nall validations.",
            "author": [
                "Xusheng Zhao",
                "Hao Peng",
                "Qiong Dai",
                "Xu Bai",
                "Huailiang Peng",
                "Yanbing Liu",
                "Qinglang Guo",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04467v1",
                "http://arxiv.org/pdf/2311.04467v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04464v3",
            "title": "Enhancing Few-shot CLIP with Semantic-Aware Fine-Tuning",
            "updated": "2023-12-07T04:35:24Z",
            "published": "2023-11-08T05:18:57Z",
            "summary": "Learning generalized representations from limited training samples is crucial\nfor applying deep neural networks in low-resource scenarios. Recently, methods\nbased on Contrastive Language-Image Pre-training (CLIP) have exhibited\npromising performance in few-shot adaptation tasks. To avoid catastrophic\nforgetting and overfitting caused by few-shot fine-tuning, existing works\nusually freeze the parameters of CLIP pre-trained on large-scale datasets,\noverlooking the possibility that some parameters might not be suitable for\ndownstream tasks. To this end, we revisit CLIP's visual encoder with a specific\nfocus on its distinctive attention pooling layer, which performs a spatial\nweighted-sum of the dense feature maps. Given that dense feature maps contain\nmeaningful semantic information, and different semantics hold varying\nimportance for diverse downstream tasks (such as prioritizing semantics like\nears and eyes in pet classification tasks rather than side mirrors), using the\nsame weighted-sum operation for dense features across different few-shot tasks\nmight not be appropriate. Hence, we propose fine-tuning the parameters of the\nattention pooling layer during the training process to encourage the model to\nfocus on task-specific semantics. In the inference process, we perform residual\nblending between the features pooled by the fine-tuned and the original\nattention pooling layers to incorporate both the few-shot knowledge and the\npre-trained CLIP's prior knowledge. We term this method as Semantic-Aware\nFinE-tuning (SAFE). SAFE is effective in enhancing the conventional few-shot\nCLIP and is compatible with the existing adapter approach (termed SAFE-A).",
            "author": [
                "Yao Zhu",
                "Yuefeng Chen",
                "Wei Wang",
                "Xiaofeng Mao",
                "Xiu Yan",
                "Yue Wang",
                "Zhigang Li",
                "Wang lu",
                "Jindong Wang",
                "Xiangyang Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04464v3",
                "http://arxiv.org/pdf/2311.04464v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04455v1",
            "title": "Vector-Valued Gossip over $w$-Holonomic Networks",
            "updated": "2023-11-08T04:42:37Z",
            "published": "2023-11-08T04:42:37Z",
            "summary": "We study the weighted average consensus problem for a gossip network of\nagents with vector-valued states. For a given matrix-weighted graph, the gossip\nprocess is described by a sequence of pairs of adjacent agents communicating\nand updating their states based on the edge matrix weight. Our key contribution\nis providing conditions for the convergence of this non-homogeneous Markov\nprocess as well as the characterization of its limit set. To this end, we\nintroduce the notion of \"$w$-holonomy\" of a set of stochastic matrices, which\nenables the characterization of sequences of gossiping pairs resulting in\nreaching a desired consensus in a decentralized manner. Stated otherwise, our\nresult characterizes the limiting behavior of infinite products of\n(non-commuting, possibly with absorbing states) stochastic matrices.",
            "author": [
                "Erkan Bayram",
                "Mohamed-Ali Belabbas",
                "Tamer Ba\u015far"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04455v1",
                "http://arxiv.org/pdf/2311.04455v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04451v1",
            "title": "Pseduo-Random and de Bruijn Array Codes",
            "updated": "2023-11-08T04:21:51Z",
            "published": "2023-11-08T04:21:51Z",
            "summary": "Pseudo-random arrays and perfect maps are the two-dimensional analogs of\nM-sequences and de Bruijn sequences, respectively. We modify the definitions to\nbe applied to codes. These codes are also the two-dimensional analogs of\ncertain factors in the de Bruijn graph. These factors are called zero factors\nand perfect factors in the de Bruijn graph. We apply a folding technique to\nconstruct pseudo-random array codes and examine the minimum distance of the\nconstructed codes. The folding is applied on sequences generated from\nirreducible polynomials or a product of irreducible polynomials with the same\ndegree and the same exponent. Direct and recursive constructions for de Bruijn\narray codes are presented and discussed.",
            "author": [
                "Tuvi Etzion"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04451v1",
                "http://arxiv.org/pdf/2311.04451v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04448v1",
            "title": "Boosting Static Resource Leak Detection via LLM-based Resource-Oriented\n  Intention Inference",
            "updated": "2023-11-08T04:19:28Z",
            "published": "2023-11-08T04:19:28Z",
            "summary": "Resource leaks, caused by resources not being released after acquisition,\noften lead to performance issues and system crashes. Existing static detection\ntechniques rely on mechanical matching of predefined resource\nacquisition/release APIs, posing challenges to their effectiveness, including\ncompleteness of predefined APIs, identification of reachability validation, and\nanalysis complexity. To overcome these challenges, we propose InferROI, a novel\napproach that leverages large language models (LLMs) to directly infer\nresource-oriented intentions (acquisition, release, and reachability\nvalidation) in code, based on resource management knowledge and code context\nunderstanding, rather than mechanical API matching. InferROI uses a prompt to\ninstruct the LLM in inferring involved intentions from a given code snippet,\nwhich are then translated into formal expressions. By aggregating these\ninferred intentions, InferROI utilizes a lightweight static-analysis based\nalgorithm to analyze control-flow paths extracted from the code, thereby\ndetecting resource leaks.\n  We evaluate InferROI on Java program and investigate its effectiveness in\nboth resource-oriented intention inference and resource leak detection.\nExperimental results demonstrate that InferROI achieves a precision of 74.6%\nand a recall of 81.8% in intention inference on 172 code snippets from the\nDroidLeaks dataset. Additionally, InferROI covers a significant portion of\nconcerned Android resources listed in the dataset. When applied to 86 bugs from\nthe DroidLeaks dataset, InferROI exhibits a high bug detection rate (53.5%) and\na low false alarm rate (8.1%) compared to eight baseline detectors. Moreover,\nwe apply InferROI to resource leak detection in 100 methods from real-world\nopen-source projects, where it identifies 12 unknown resource leak bugs, with 7\nof them being confirmed by developers.",
            "author": [
                "Chong Wang",
                "Jianan Liu",
                "Xin Peng",
                "Yang Liu",
                "Yiling Lou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04448v1",
                "http://arxiv.org/pdf/2311.04448v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04434v1",
            "title": "A Hierarchical Spatial Transformer for Massive Point Samples in\n  Continuous Space",
            "updated": "2023-11-08T02:54:19Z",
            "published": "2023-11-08T02:54:19Z",
            "summary": "Transformers are widely used deep learning architectures. Existing\ntransformers are mostly designed for sequences (texts or time series), images\nor videos, and graphs. This paper proposes a novel transformer model for\nmassive (up to a million) point samples in continuous space. Such data are\nubiquitous in environment sciences (e.g., sensor observations), numerical\nsimulations (e.g., particle-laden flow, astrophysics), and location-based\nservices (e.g., POIs and trajectories). However, designing a transformer for\nmassive spatial points is non-trivial due to several challenges, including\nimplicit long-range and multi-scale dependency on irregular points in\ncontinuous space, a non-uniform point distribution, the potential high\ncomputational costs of calculating all-pair attention across massive points,\nand the risks of over-confident predictions due to varying point density. To\naddress these challenges, we propose a new hierarchical spatial transformer\nmodel, which includes multi-resolution representation learning within a\nquad-tree hierarchy and efficient spatial attention via coarse approximation.\nWe also design an uncertainty quantification branch to estimate prediction\nconfidence related to input feature noise and point sparsity. We provide a\ntheoretical analysis of computational time complexity and memory costs.\nExtensive experiments on both real-world and synthetic datasets show that our\nmethod outperforms multiple baselines in prediction accuracy and our model can\nscale up to one million points on one NVIDIA A100 GPU. The code is available at\n\\url{https://github.com/spatialdatasciencegroup/HST}.",
            "author": [
                "Wenchong He",
                "Zhe Jiang",
                "Tingsong Xiao",
                "Zelin Xu",
                "Shigang Chen",
                "Ronald Fick",
                "Miles Medina",
                "Christine Angelini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04434v1",
                "http://arxiv.org/pdf/2311.04434v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04429v1",
            "title": "Quasi-Transitive Mixed Graphs and Undirected Squares of Oriented Graphs",
            "updated": "2023-11-08T02:12:23Z",
            "published": "2023-11-08T02:12:23Z",
            "summary": "We consider the problem of classifying those graphs that arise as an\nundirected square of an oriented graph by generalising the notion of\nquasi-transitive directed graphs to mixed graphs. We fully classify those\ngraphs of maximum degree three and those graphs of girth at least four that\narise an undirected square of an oriented graph. In contrast to the recognition\nproblem for graphs that admit a quasi-transitive orientation, we find it is\nNP-complete to decide if a graph admits a partial orientation as a\nquasi-transitive mixed graph. We prove the problem is Polynomial when\nrestricted to inputs of maximum degree three, but remains NP-complete when\nrestricted to inputs with maximum degree at least five. Our proof further\nimplies that for fixed $k \\geq 3$, it is NP-complete to decide if a graph\narises as an undirected square of an orientation of a graph with $\\Delta = k$.",
            "author": [
                "Christopher Duffy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04429v1",
                "http://arxiv.org/pdf/2311.04429v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C12, 05C20, 05C75, 05C85, 68Q25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04419v1",
            "title": "PepLand: a large-scale pre-trained peptide representation model for a\n  comprehensive landscape of both canonical and non-canonical amino acids",
            "updated": "2023-11-08T01:18:32Z",
            "published": "2023-11-08T01:18:32Z",
            "summary": "In recent years, the scientific community has become increasingly interested\non peptides with non-canonical amino acids due to their superior stability and\nresistance to proteolytic degradation. These peptides present promising\nmodifications to biological, pharmacological, and physiochemical attributes in\nboth endogenous and engineered peptides. Notwithstanding their considerable\nadvantages, the scientific community exhibits a conspicuous absence of an\neffective pre-trained model adept at distilling feature representations from\nsuch complex peptide sequences. We herein propose PepLand, a novel pre-training\narchitecture for representation and property analysis of peptides spanning both\ncanonical and non-canonical amino acids. In essence, PepLand leverages a\ncomprehensive multi-view heterogeneous graph neural network tailored to unveil\nthe subtle structural representations of peptides. Empirical validations\nunderscore PepLand's effectiveness across an array of peptide property\npredictions, encompassing protein-protein interactions, permeability,\nsolubility, and synthesizability. The rigorous evaluation confirms PepLand's\nunparalleled capability in capturing salient synthetic peptide features,\nthereby laying a robust foundation for transformative advances in\npeptide-centric research domains. We have made all the source code utilized in\nthis study publicly accessible via GitHub at\nhttps://github.com/zhangruochi/pepland",
            "author": [
                "Ruochi Zhang",
                "Haoran Wu",
                "Yuting Xiu",
                "Kewei Li",
                "Ningning Chen",
                "Yu Wang",
                "Yan Wang",
                "Xin Gao",
                "Fengfeng Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04419v1",
                "http://arxiv.org/pdf/2311.04419v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04418v2",
            "title": "AI-accelerated Discovery of Altermagnetic Materials",
            "updated": "2023-11-13T02:53:04Z",
            "published": "2023-11-08T01:06:48Z",
            "summary": "Altermagnetism, a new magnetic phase, has been theoretically proposed and\nexperimentally verified to be distinct from ferromagnetism and\nantiferromagnetism. Although altermagnets have been found to possess many\nexotic physical properties, the very limited availability of known\naltermagnetic materials (e.g., 14 confirmed materials) hinders the study of\nsuch properties. Hence, discovering more types of altermagnetic materials is\ncrucial for a comprehensive understanding of altermagnetism and thus\nfacilitating new applications in the next-generation information technologies,\ne.g., storage devices and high-sensitivity sensors. Here, we report 25 new\naltermagnetic materials that cover metals, semiconductors, and insulators,\ndiscovered by an AI search engine unifying symmetry analysis, graph neural\nnetwork pre-training, optimal transport theory, and first-principles electronic\nstructure calculation. The wide range of electronic structural characteristics\nreveals that various novel physical properties manifest in these newly\ndiscovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr\neffect, and topological property. Noteworthy, we discovered 8 i-wave\naltermagnetic materials for the first time. Overall, the AI search engine\nperforms much better than human experts and suggests a set of new altermagnetic\nmaterials with unique properties, outlining its potential for accelerated\ndiscovery of the materials with targeting properties.",
            "author": [
                "Ze-Feng Gao",
                "Shuai Qu",
                "Bocheng Zeng",
                "Yang Liu",
                "Ji-Rong Wen",
                "Hao Sun",
                "Peng-Jie Guo",
                "Zhong-Yi Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04418v2",
                "http://arxiv.org/pdf/2311.04418v2"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.AI",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04405v1",
            "title": "Toward Computing Bounds for Ramsey Numbers Using Quantum Annealing",
            "updated": "2023-11-08T00:16:46Z",
            "published": "2023-11-08T00:16:46Z",
            "summary": "Quantum annealing is a powerful tool for solving and approximating\ncombinatorial optimization problems such as graph partitioning, community\ndetection, centrality, routing problems, and more. In this paper we explore the\nuse of quantum annealing as a tool for use in exploring combinatorial\nmathematics research problems. We consider the monochromatic triangle problem\nand the Ramsey number problem, both examples of graph coloring. Conversion to\nquadratic unconstrained binary optimization (QUBO) form is required to run on\nquantum hardware. While the monochromatic triangle problem is quadratic by\nnature, the Ramsey number problem requires the use of order reduction methods\nfor a quadratic formulation. We discuss implementations, limitations, and\nresults when running on the D-Wave Advantage quantum annealer.",
            "author": [
                "Joel E. Pion",
                "Susan M. Mniszewski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04405v1",
                "http://arxiv.org/pdf/2311.04405v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04401v1",
            "title": "The edge-girth-regularity of Wenger graphs",
            "updated": "2023-11-08T00:04:40Z",
            "published": "2023-11-08T00:04:40Z",
            "summary": "Let $n\\ge 1$ be an integer and $\\mathbb{F}_q$ be a finite field of\ncharacteristic $p$ with $q$ elements. In this paper, it is proved that the\nWenger graph $W_n(q)$ and linearized Wenger graph $L_m(q)$ are\nedge-girth-regular $(v,k,g,\\lambda)$-graphs, and the parameter $\\lambda$ of\ngraphs $W_n(q)$ and $L_m(q)$ is completely determined. Here, an\nedge-girth-regular graph $egr(v,k,g,\\lambda)$ means a $k$-regular graph of\norder $v$ and girth $g$ satisfying that any edge is contained in $\\lambda$\ndistinct $g$-cycles. As a direct corollary, we obtain the number of girth\ncycles of graph $W_n(q)$, and the lower bounds on the generalized Tur\\'an\nnumbers $ex(n, C_{6}, \\mathscr{C}_{5})$ and $ex(n, C_{8}, \\mathscr{C}_{7})$,\nwhere $C_k$ is the cycle of length $k$ and $\\mathscr{C}_k = \\{C_3, C_4, \\dots ,\nC_k\\}$.Moreover, there exist a family of $egr(2q^3,q,8,(q-1)^3(q-2))$-graphs\nfor $q$ odd, and the order of graph $W_2(q)$ and extremal\n$egr(v,q,8,(q-1)^3(q-2))$-graph have same asymptotic order for $q$ odd.",
            "author": [
                "Fuyuan Yang",
                "Qiang Sun",
                "Chao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04401v1",
                "http://arxiv.org/pdf/2311.04401v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C25, 05C35, 05E15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04389v1",
            "title": "Structural Balance of Complex Weighted Graphs and Multi-partite\n  Consensus",
            "updated": "2023-11-07T23:37:19Z",
            "published": "2023-11-07T23:37:19Z",
            "summary": "The structural balance of a signed graph is known to be necessary and\nsufficient to obtain a bipartite consensus among agents with friend-foe\nrelationships. In the real world, relationships are multifarious, and the\ncoexistence of different opinions is ubiquitous. We are therefore motivated to\nstudy the multi-partite consensus problem of multi-agent systems, for which we\nextend the concept of structural balance to graphs with complex edge weights.\nIt is shown that the generalized structural balance property is necessary and\nsufficient for achieving multi-partite consensus.",
            "author": [
                "Honghui Wu",
                "Ahmet Taha Koru",
                "Guanxuan Wu",
                "Frank L. Lewis",
                "Hai Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04389v1",
                "http://arxiv.org/pdf/2311.04389v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04376v2",
            "title": "Planetary Radio Interferometry and Doppler Experiment (PRIDE) of the\n  JUICE mission",
            "updated": "2023-11-28T18:59:02Z",
            "published": "2023-11-07T22:40:32Z",
            "summary": "Planetary Radio Interferometry and Doppler Experiment (PRIDE) is a\nmulti-purpose experimental technique aimed at enhancing the science return of\nplanetary missions. The technique exploits the science payload and spacecraft\nservice systems without requiring a dedicated onboard instrumentation or\nimposing on the existing instrumentation any special for PRIDE requirements.\nPRIDE is based on the near-field phase-referencing Very Long Baseline\nInterferometry (VLBI) and evaluation of the Doppler shift of the radio signal\ntransmitted by spacecraft by observing it with multiple Earth-based radio\ntelescopes. The methodology of PRIDE has been developed initially at the Joint\nInstitute for VLBI ERIC (JIVE) for tracking the ESA's Huygens Probe during its\ndescent in the atmosphere of Titan in 2005. From that point on, the technique\nhas been demonstrated for various planetary and other space science missions.\nThe estimates of lateral position of the target spacecraft are done using the\nphase-referencing VLBI technique. Together with radial Doppler estimates, these\nobservables can be used for a variety of applications, including improving the\nknowledge of the spacecraft state vector. The PRIDE measurements can be applied\nto a broad scope of research fields including studies of atmospheres through\nthe use of radio occultations, the improvement of planetary and satellite\nephemerides, as well as gravity field parameters and other geodetic properties\nof interest, and estimations of interplanetary plasma properties. This paper\npresents the implementation of PRIDE as a component of the ESA's Jupiter Icy\nMoons Explorer (JUICE) mission.",
            "author": [
                "Leonid I. Gurvits",
                "Giuseppe Cimo",
                "Dominic Dirkx",
                "Vidhya Pallichadath",
                "Alexander Akins",
                "Nicolas Altobelli",
                "Tatiana M. Bocanegra-Bahamon",
                "Stephanie M. Cazaux",
                "Patrick Charlot",
                "Dmitry A. Duev",
                "Marie S. Fayolle",
                "Judit Fogasy",
                "Sandor Frey",
                "Valery Lainey",
                "Guifre Molera Calves",
                "Krisztina Perger",
                "Sergey V. Pogrebenko",
                "N. Masdiana Md Said",
                "Claire Vallat",
                "Bert L. A. Vermeersen",
                "Pieter N. A. M. Visser",
                "Kuo-Nung Wang",
                "Konrad Willner"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s11214-023-01026-1",
                "http://arxiv.org/abs/2311.04376v2",
                "http://arxiv.org/pdf/2311.04376v2"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04374v1",
            "title": "Common Knowledge, Regained",
            "updated": "2023-11-07T22:38:16Z",
            "published": "2023-11-07T22:38:16Z",
            "summary": "Formally, for common knowledge to arise in a dynamic setting, knowledge that\nit has arisen must be simultaneously attained by all players. As a result, new\ncommon knowledge is unattainable in many realistic settings, due to timing\nfrictions. This unintuitive phenomenon, observed by Halpern and Moses (1990),\nwas discussed by Arrow et al. (1987) and by Aumann (1989), was called a paradox\nby Morris (2014), and has evaded satisfactory resolution for four decades. We\nresolve this paradox by proposing a new definition for common knowledge, which\ncoincides with the traditional one in static settings but generalizes it in\ndynamic settings. Under our definition, common knowledge can arise without\nsimultaneity, particularly in canonical examples of the Haplern-Moses paradox.\nWe demonstrate its usefulness by deriving for it an agreement theorem \\`a la\nAumann (1976), and showing that it arises in the setting of Geanakoplos and\nPolemarchakis (1982) with timing frictions added.",
            "author": [
                "Yannai A. Gonczarowski",
                "Yoram Moses"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04374v1",
                "http://arxiv.org/pdf/2311.04374v1"
            ],
            "primary_category": "econ.TH",
            "category": [
                "econ.TH",
                "cs.DC",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04369v1",
            "title": "The context-specificity of virulence evolution revealed through\n  evolutionary invasion analysis",
            "updated": "2023-11-07T22:29:25Z",
            "published": "2023-11-07T22:29:25Z",
            "summary": "Models are often employed to integrate knowledge about epidemics across\nscales and simulate disease dynamics. While these approaches have played a\ncentral role in studying the mechanics underlying epidemics, we lack ways to\nreliably predict how the relationship between virulence (the harm to hosts\ncaused by an infection) and transmission will evolve in certain virus-host\ncontexts. In this study, we invoke evolutionary invasion analysis -- a method\nused to identify the evolution of uninvadable strategies in dynamical systems\n-- to examine how the virulence-transmission dichotomy can evolve in models of\nvirus infections defined by different natural histories. We reveal that\npeculiar ecologies drive different evolved relationships between virulence and\ntransmission. Specifically, we discover patterns of virulence evolution between\nepidemics of various kinds (SARS-CoV-2 and hepatitis C virus) and that varying\ndefinitions of virulence alter our predictions for how viruses will evolve. We\ndiscuss the findings in light of contemporary conversations in the public\nhealth sector around the possibility of predicting virus evolution and in more\nextensive theoretical discussions involving virulence evolution in emerging\ninfectious diseases.",
            "author": [
                "Sudam Surasinghe",
                "Ketty Kabengele",
                "Paul E. Turner",
                "C. Brandon Ogbunugafor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04369v1",
                "http://arxiv.org/pdf/2311.04369v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04361v2",
            "title": "\\textsc{DeFault}: Deep-learning-based Fault Delineation",
            "updated": "2023-11-13T18:54:52Z",
            "published": "2023-11-07T21:51:55Z",
            "summary": "The carbon capture, utilization, and storage (CCUS) framework is an essential\ncomponent in reducing greenhouse gas emissions, with its success hinging on the\ncomprehensive knowledge of subsurface geology and geomechanics. Passive seismic\nevent relocation and fault detection serve as indispensable tools, offering\nvital insights into subsurface structures and fluid migration pathways.\nAccurate identification and localization of seismic events, however, face\nsignificant challenges, including the necessity for high-quality seismic data\nand advanced computational methods. To address these challenges, we introduce a\nnovel deep learning method, DeFault, specifically designed for passive seismic\nsource relocation and fault delineating for passive seismic monitoring\nprojects. By leveraging data domain-adaptation, DeFault allows us to train a\nneural network with labeled synthetic data and apply it directly to field data.\nUsing DeFault, the passive seismic sources are automatically clustered based on\ntheir recording time and spatial locations, and subsequently, faults and\nfractures are delineated accordingly. We demonstrate the efficacy of DeFault on\na field case study involving CO2 injection related microseismic data from the\nDecatur, Illinois area. Our approach accurately and efficiently relocated\npassive seismic events, identified faults and aided in the prevention of\npotential geological hazards. Our results highlight the potential of DeFault as\na valuable tool for passive seismic monitoring, emphasizing its role in\nensuring CCUS project safety. This research bolsters the understanding of\nsubsurface characterization in CCUS, illustrating machine learning's capacity\nto refine these methods. Ultimately, our work bear significant implications for\nCCUS technology deployment, an essential strategy in combating climate change.",
            "author": [
                "Hanchen Wang",
                "Yinpeng Chen",
                "Tariq Alkhalifah",
                "Ting Chen",
                "Youzuo Lin",
                "David Alumbaugh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04361v2",
                "http://arxiv.org/pdf/2311.04361v2"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04356v1",
            "title": "Multiarc and curve graphs are hierarchically hyperbolic",
            "updated": "2023-11-07T21:37:39Z",
            "published": "2023-11-07T21:37:39Z",
            "summary": "A multiarc and curve graph is a simplicial graph whose vertices are arc and\ncurve systems on a compact, connected, orientable surface S. We show that all\nmultiarc and curve graphs preserved by the natural action of PMod(S) and whose\nadjacent vertices have bounded geometric intersection number are hierarchically\nhyperbolic spaces with respect to witness subsurface projection. This result\nextends work of Kate Vokes on twist-free multicurve graphs and confirms two\nconjectures of Saul Schleimer in a broad setting. In addition, we prove that\nthe PMod(S)-equivariant quasi-isometry type of such a graph is fully classified\nby the set of its witness subsurfaces.",
            "author": [
                "Michael Kopreski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04356v1",
                "http://arxiv.org/pdf/2311.04356v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04350v1",
            "title": "Device Sampling and Resource Optimization for Federated Learning in\n  Cooperative Edge Networks",
            "updated": "2023-11-07T21:17:59Z",
            "published": "2023-11-07T21:17:59Z",
            "summary": "The conventional federated learning (FedL) architecture distributes machine\nlearning (ML) across worker devices by having them train local models that are\nperiodically aggregated by a server. FedL ignores two important characteristics\nof contemporary wireless networks, however: (i) the network may contain\nheterogeneous communication/computation resources, and (ii) there may be\nsignificant overlaps in devices' local data distributions. In this work, we\ndevelop a novel optimization methodology that jointly accounts for these\nfactors via intelligent device sampling complemented by device-to-device (D2D)\noffloading. Our optimization methodology aims to select the best combination of\nsampled nodes and data offloading configuration to maximize FedL training\naccuracy while minimizing data processing and D2D communication resource\nconsumption subject to realistic constraints on the network topology and device\ncapabilities. Theoretical analysis of the D2D offloading subproblem leads to\nnew FedL convergence bounds and an efficient sequential convex optimizer. Using\nthese results, we develop a sampling methodology based on graph convolutional\nnetworks (GCNs) which learns the relationship between network attributes,\nsampled nodes, and D2D data offloading to maximize FedL accuracy. Through\nevaluation on popular datasets and real-world network measurements from our\nedge testbed, we find that our methodology outperforms popular device sampling\nmethodologies from literature in terms of ML model performance, data processing\noverhead, and energy consumption.",
            "author": [
                "Su Wang",
                "Roberto Morabito",
                "Seyyedali Hosseinalipour",
                "Mung Chiang",
                "Christopher G. Brinton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04350v1",
                "http://arxiv.org/pdf/2311.04350v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04348v1",
            "title": "Evaluating the Effectiveness of Retrieval-Augmented Large Language\n  Models in Scientific Document Reasoning",
            "updated": "2023-11-07T21:09:57Z",
            "published": "2023-11-07T21:09:57Z",
            "summary": "Despite the dramatic progress in Large Language Model (LLM) development, LLMs\noften provide seemingly plausible but not factual information, often referred\nto as hallucinations. Retrieval-augmented LLMs provide a non-parametric\napproach to solve these issues by retrieving relevant information from external\ndata sources and augment the training process. These models help to trace\nevidence from an externally provided knowledge base allowing the model\npredictions to be better interpreted and verified. In this work, we critically\nevaluate these models in their ability to perform in scientific document\nreasoning tasks. To this end, we tuned multiple such model variants with\nscience-focused instructions and evaluated them on a scientific document\nreasoning benchmark for the usefulness of the retrieved document passages. Our\nfindings suggest that models justify predictions in science tasks with\nfabricated evidence and leveraging scientific corpus as pretraining data does\nnot alleviate the risk of evidence fabrication.",
            "author": [
                "Sai Munikoti",
                "Anurag Acharya",
                "Sridevi Wagle",
                "Sameera Horawalavithana"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04348v1",
                "http://arxiv.org/pdf/2311.04348v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04337v1",
            "title": "Approximately Packing Dijoins via Nowhere-Zero Flows",
            "updated": "2023-11-07T20:40:16Z",
            "published": "2023-11-07T20:40:16Z",
            "summary": "In a digraph, a dicut is a cut where all the arcs cross in one direction. A\ndijoin is a subset of arcs that intersects each dicut. Woodall conjectured in\n1976 that in every digraph, the minimum size of a dicut is equal to the maximum\nnumber of disjoint dijoins. However, prior to our work, it was not even known\nwhether at least $3$ disjoint dijoins exist in a digraph whose minimum dicut\nsize is arbitrarily large. By building connections with nowhere-zero (circular)\n$k$-flows, we prove that every digraph with minimum dicut size $\\tau$ contains\n$\\frac{\\tau}{k}$ disjoint dijoins if the underlying undirected graph admits a\nnowhere-zero (circular) $k$-flow. The existence of nowhere-zero $6$-flows in\n$2$-edge-connected graphs (Seymour 1981) directly leads to the existence of\n$\\frac{\\tau}{6}$ disjoint dijoins in any digraph with minimum dicut size\n$\\tau$, which can be found in polynomial time as well. The existence of\nnowhere-zero circular $(2+\\frac{1}{p})$-flows in $6p$-edge-connected graphs\n(Lov\\'asz et al 2013) directly leads to the existence of $\\frac{\\tau p}{2p+1}$\ndisjoint dijoins in any digraph with minimum dicut size $\\tau$ whose underlying\nundirected graph is $6p$-edge-connected.",
            "author": [
                "G\u00e9rard Cornu\u00e9jols",
                "Siyue Liu",
                "R. Ravi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04337v1",
                "http://arxiv.org/pdf/2311.04337v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04333v1",
            "title": "Practical Parallel Algorithms for Near-Optimal Densest Subgraphs on\n  Massive Graphs",
            "updated": "2023-11-07T20:36:01Z",
            "published": "2023-11-07T20:36:01Z",
            "summary": "The densest subgraph problem has received significant attention, both in\ntheory and in practice, due to its applications in problems such as community\ndetection, social network analysis, and spam detection. Due to the high cost of\nobtaining exact solutions, much attention has focused on designing approximate\ndensest subgraph algorithms. However, existing approaches are not able to scale\nto massive graphs with billions of edges.\n  In this paper, we introduce a new framework that combines approximate densest\nsubgraph algorithms with a pruning optimization. We design new parallel\nvariants of the state-of-the-art sequential Greedy++ algorithm, and plug it\ninto our framework in conjunction with a parallel pruning technique based on\n$k$-core decomposition to obtain parallel $(1+\\varepsilon)$-approximate densest\nsubgraph algorithms. On a single thread, our algorithms achieve\n$2.6$--$34\\times$ speedup over Greedy++, and obtain up to $22.37\\times$ self\nrelative parallel speedup on a 30-core machine with two-way hyper-threading.\nCompared with the state-of-the-art parallel algorithm by Harb et al.\n[NeurIPS'22], we achieve up to a $114\\times$ speedup on the same machine.\nFinally, against the recent sequential algorithm of Xu et al. [PACMMOD'23], we\nachieve up to a $25.9\\times$ speedup. The scalability of our algorithms enables\nus to obtain near-optimal density statistics on the hyperlink2012 (with roughly\n113 billion edges) and clueweb (with roughly 37 billion edges) graphs for the\nfirst time in the literature.",
            "author": [
                "Pattara Sukprasert",
                "Quanquan C. Liu",
                "Laxman Dhulipala",
                "Julian Shun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04333v1",
                "http://arxiv.org/pdf/2311.04333v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16153v2",
            "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated\n  Applications",
            "updated": "2023-11-29T03:43:03Z",
            "published": "2023-11-07T20:13:05Z",
            "summary": "Large language models (LLMs) are increasingly deployed as the service backend\nfor LLM-integrated applications such as code completion and AI-powered search.\nLLM-integrated applications serve as middleware to refine users' queries with\ndomain-specific knowledge to better inform LLMs and enhance the responses.\nDespite numerous opportunities and benefits, LLM-integrated applications also\nintroduce new attack surfaces. Understanding, minimizing, and eliminating these\nemerging attack surfaces is a new area of research. In this work, we consider a\nsetup where the user and LLM interact via an LLM-integrated application in the\nmiddle. We focus on the communication rounds that begin with user's queries and\nend with LLM-integrated application returning responses to the queries, powered\nby LLMs at the service backend. For this query-response protocol, we identify\npotential vulnerabilities that can originate from the malicious application\ndeveloper or from an outsider threat initiator that is able to control the\ndatabase access, manipulate and poison data that are high-risk for the user.\nSuccessful exploits of the identified vulnerabilities result in the users\nreceiving responses tailored to the intent of a threat initiator. We assess\nsuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5\nand GPT-4. Our empirical results show that the threats can effectively bypass\nthe restrictions and moderation policies of OpenAI, resulting in users\nreceiving responses that contain bias, toxic content, privacy risk, and\ndisinformation. To mitigate those threats, we identify and define four key\nproperties, namely integrity, source identification, attack detectability, and\nutility preservation, that need to be satisfied by a safe LLM-integrated\napplication. Based on these properties, we develop a lightweight,\nthreat-agnostic defense that mitigates both insider and outsider threats.",
            "author": [
                "Fengqing Jiang",
                "Zhangchen Xu",
                "Luyao Niu",
                "Boxin Wang",
                "Jinyuan Jia",
                "Bo Li",
                "Radha Poovendran"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16153v2",
                "http://arxiv.org/pdf/2311.16153v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04319v1",
            "title": "On-The-Fly Static Analysis via Dynamic Bidirected Dyck Reachability",
            "updated": "2023-11-07T19:52:36Z",
            "published": "2023-11-07T19:52:36Z",
            "summary": "Dyck reachability is a principled, graph-based formulation of a plethora of\nstatic analyses. Bidirected graphs are used for capturing dataflow through\nmutable heap data, and are usual formalisms of demand-driven points-to and\nalias analyses. The best (offline) algorithm runs in $O(m+n\\cdot \\alpha(n))$\ntime, where $n$ is the number of nodes and $m$ is the number of edges in the\nflow graph, which becomes $O(n^2)$ in the worst case.\n  In the everyday practice of program analysis, the analyzed code is subject to\ncontinuous change, with source code being added and removed. On-the-fly static\nanalysis under such continuous updates gives rise to dynamic Dyck reachability,\nwhere reachability queries run on a dynamically changing graph, following\nprogram updates. Naturally, executing the offline algorithm in this online\nsetting is inadequate, as the time required to process a single update is\nprohibitively large.\n  In this work we develop a novel dynamic algorithm for bidirected Dyck\nreachability that has $O(n\\cdot \\alpha(n))$ worst-case performance per update,\nthus beating the $O(n^2)$ bound, and is also optimal in certain settings. We\nalso implement our algorithm and evaluate its performance on on-the-fly\ndata-dependence and alias analyses, and compare it with two best known\nalternatives, namely (i) the optimal offline algorithm, and (ii) a fully\ndynamic Datalog solver. Our experiments show that our dynamic algorithm is\nconsistently, and by far, the top performing algorithm, exhibiting speedups in\nthe order of 1000X. The running time of each update is almost always\nunnoticeable to the human eye, making it ideal for the on-the-fly analysis\nsetting.",
            "author": [
                "Shankaranarayanan Krishna",
                "Aniket Lal",
                "Andreas Pavlogiannis",
                "Omkar Tuppe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04319v1",
                "http://arxiv.org/pdf/2311.04319v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "cs.FL",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04310v1",
            "title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI\n  Integration using Retrieval-Augmented Generation",
            "updated": "2023-11-07T19:27:28Z",
            "published": "2023-11-07T19:27:28Z",
            "summary": "Academic researchers face challenges keeping up with exponentially growing\npublished findings in their field. Performing comprehensive literature reviews\nto synthesize knowledge is time-consuming and labor-intensive using manual\napproaches. Recent advances in artificial intelligence provide promising\nsolutions, yet many require coding expertise, limiting accessibility.\nKNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the\nKNIME visual programming platform to automate literature review tasks for users\nwith no coding experience. By leveraging KNIME's intuitive graphical interface,\nresearchers can create workflows to search their Zotero libraries and utilize\nOpenAI models to extract key information without coding. Users simply provide\nAPI keys and configure settings through a user-friendly interface in a locally\nstored copy of the workflow. KNIMEZoBot then allows asking natural language\nquestions via a chatbot and retrieves relevant passages from papers to generate\nsynthesized answers. This system has significant potential to expedite\nliterature reviews for researchers unfamiliar with coding by automating\nretrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot\ndemonstrates how thoughtfully designed AI tools can expand accessibility and\naccelerate knowledge building across diverse research domains.",
            "author": [
                "Suad Alshammari",
                "Lama Basalelah",
                "Walaa Abu Rukbah",
                "Ali Alsuhibani",
                "Dayanjan S. Wijesinghe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04310v1",
                "http://arxiv.org/pdf/2311.04310v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04302v2",
            "title": "How Hard is Weak-Memory Testing?",
            "updated": "2023-11-15T09:09:35Z",
            "published": "2023-11-07T19:19:03Z",
            "summary": "Weak-memory models are standard formal specifications of concurrency across\nhardware, programming languages, and distributed systems. A fundamental\ncomputational problem is consistency testing: is the observed execution of a\nconcurrent program in alignment with the specification of the underlying\nsystem? The problem has been studied extensively across Sequential Consistency\n(SC) and weak memory, and proven to be NP-complete when some aspect of the\ninput (e.g., number of threads/memory locations) is unbounded. This\nunboundedness has left a natural question open: are there efficient\nparameterized algorithms for testing?\n  The main contribution of this paper is a deep hardness result for consistency\ntesting under many popular weak-memory models: the problem remains NP-complete\neven in its bounded setting, where candidate executions contain a bounded\nnumber of threads, memory locations, and values. This hardness spreads across\nseveral Release-Acquire variants of C11, a popular variant of its Relaxed\nfragment, popular Causal Consistency models, and the POWER architecture. To our\nknowledge, this is the first result that fully exposes the hardness of\nweak-memory testing and proves that the problem admits no parameterization\nunder standard input parameters. It also yields a computational separation of\nthese models from SC, x86-TSO, PSO, and Relaxed, for which bounded consistency\ntesting is either known (for SC), or shown here (for the rest), to be in\npolynomial time.",
            "author": [
                "Soham Chakraborty",
                "Shankaranarayanan Krishna",
                "Umang Mathur",
                "Andreas Pavlogiannis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04302v2",
                "http://arxiv.org/pdf/2311.04302v2"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04935v1",
            "title": "Node-Bound Communities for Partition of Unity Interpolation on Graphs",
            "updated": "2023-11-07T19:15:26Z",
            "published": "2023-11-07T19:15:26Z",
            "summary": "Graph signal processing benefits significantly from the direct and highly\nadaptable supplementary techniques offered by partition of unity methods (PUMs)\non graphs. In our approach, we demonstrate the generation of a partition of\nunity solely based on the underlying graph structure, employing an algorithm\nthat relies exclusively on centrality measures and modularity, without\nrequiring the input of the number of subdomains. Subsequently, we integrate\nPUMs with a local graph basis function (GBF) approximation method to develop\ncost-effective global interpolation schemes. We also discuss numerical\nexperiments conducted on both synthetic and real datasets to assess the\nperformance of this presented technique.",
            "author": [
                "Roberto Cavoretto",
                "Alessandra De Rossi",
                "Sandro Lancellotti",
                "Federico Romaniello"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04935v1",
                "http://arxiv.org/pdf/2311.04935v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04299v1",
            "title": "Node-Binded Communities for Interpolation on Graphs",
            "updated": "2023-11-07T19:12:08Z",
            "published": "2023-11-07T19:12:08Z",
            "summary": "Partition of unity methods (PUMs) on graphs represent straightforward and\nremarkably adaptable auxiliary techniques for graph signal processing. By\nrelying solely on the intrinsic graph structure, we propose the generation of a\npartition of unity through centrality measures and modularity. Subsequently, we\nintegrate PUMs with a local graph basis function (GBF) approximation approach\nto achieve low-cost global interpolation schemes.",
            "author": [
                "Roberto Cavoretto",
                "Alessandra De Rossi",
                "Sandro Lancellotti",
                "Federico Romaniello"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04299v1",
                "http://arxiv.org/pdf/2311.04299v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04287v1",
            "title": "Holistic Evaluation of Text-To-Image Models",
            "updated": "2023-11-07T19:00:56Z",
            "published": "2023-11-07T19:00:56Z",
            "summary": "The stunning qualitative improvement of recent text-to-image models has led\nto their widespread attention and adoption. However, we lack a comprehensive\nquantitative understanding of their capabilities and risks. To fill this gap,\nwe introduce a new benchmark, Holistic Evaluation of Text-to-Image Models\n(HEIM). Whereas previous evaluations focus mostly on text-image alignment and\nimage quality, we identify 12 aspects, including text-image alignment, image\nquality, aesthetics, originality, reasoning, knowledge, bias, toxicity,\nfairness, robustness, multilinguality, and efficiency. We curate 62 scenarios\nencompassing these aspects and evaluate 26 state-of-the-art text-to-image\nmodels on this benchmark. Our results reveal that no single model excels in all\naspects, with different models demonstrating different strengths. We release\nthe generated images and human evaluation results for full transparency at\nhttps://crfm.stanford.edu/heim/v1.1.0 and the code at\nhttps://github.com/stanford-crfm/helm, which is integrated with the HELM\ncodebase.",
            "author": [
                "Tony Lee",
                "Michihiro Yasunaga",
                "Chenlin Meng",
                "Yifan Mai",
                "Joon Sung Park",
                "Agrim Gupta",
                "Yunzhi Zhang",
                "Deepak Narayanan",
                "Hannah Benita Teufel",
                "Marco Bellagente",
                "Minguk Kang",
                "Taesung Park",
                "Jure Leskovec",
                "Jun-Yan Zhu",
                "Li Fei-Fei",
                "Jiajun Wu",
                "Stefano Ermon",
                "Percy Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04287v1",
                "http://arxiv.org/pdf/2311.04287v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04274v1",
            "title": "Binary Systems in Massive Scalar-Tensor Theories: Next-to-Leading Order\n  Gravitational Waveform from Effective Field Theory",
            "updated": "2023-11-07T19:00:02Z",
            "published": "2023-11-07T19:00:02Z",
            "summary": "Neutron star binaries and their associated gravitational wave signal\nfacilitate precision tests of General Relativity. Any deviation of the detected\ngravitational waveform from General Relativity would therefore be a smoking gun\nsignature of new physics, in the form of additional forces, dark matter\nparticles, or extra gravitational degrees of freedom. To be able to probe new\ntheories, precise knowledge of the expected waveform is required. In our work,\nwe consider a generic setup by augmenting General Relativity with an\nadditional, massive scalar field. We then compute the inspiral dynamics of a\nbinary system by employing an effective field theoretical approach, while\ngiving a detailed introduction to the computational framework. Finally, we\nderive the modified gravitational waveform at next-to-leading order. As a\nconsequence of our model-agnostic approach, our results are readily adaptable\nto a plethora of new physics scenarios, including modified gravity theories and\nscalar dark matter models.",
            "author": [
                "Robin Fynn Diedrichs",
                "Daniel Schmitt",
                "Laura Sagunski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04274v1",
                "http://arxiv.org/pdf/2311.04274v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.CO",
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04204v2",
            "title": "Sharp Thresholds Imply Circuit Lower Bounds: from random 2-SAT to\n  Planted Clique",
            "updated": "2023-11-30T22:56:25Z",
            "published": "2023-11-07T18:43:27Z",
            "summary": "We show that sharp thresholds for Boolean functions directly imply\naverage-case circuit lower bounds. More formally we show that any Boolean\nfunction exhibiting a sharp enough threshold at \\emph{arbitrary} critical\ndensity cannot be computed by Boolean circuits of bounded depth and polynomial\nsize.\n  Our general result implies new average-case bounded depth circuit lower\nbounds in a variety of settings.\n  (a) ($k$-cliques) For $k=\\Theta(n)$, we prove that any circuit of depth $d$\ndeciding the presence of a size $k$ clique in a random graph requires\nexponential-in-$n^{\\Theta(1/d)}$ size. To the best of our knowledge, this is\nthe first average-case exponential size lower bound for bounded depth (not\nnecessarily monotone) circuits solving the fundamental $k$-clique problem (for\nany $k=k_n$).\n  (b)(random 2-SAT) We prove that any circuit of depth $d$ deciding the\nsatisfiability of a random 2-SAT formula requires\nexponential-in-$n^{\\Theta(1/d)}$ size. To the best of our knowledge, this is\nthe first bounded depth circuit lower bound for random $k$-SAT for any value of\n$k \\geq 2.$ Our results also provide the first rigorous lower bound in\nagreement with a conjectured, but debated, ``computational hardness'' of random\n$k$-SAT around its satisfiability threshold.\n  (c)(Statistical estimation -- planted $k$-clique) Over the recent years,\nmultiple statistical estimation problems have also been proven to exhibit a\n``statistical'' sharp threshold, called the All-or-Nothing (AoN) phenomenon. We\nshow that AoN also implies circuit lower bounds for statistical problems. As a\nsimple corollary of that, we prove that any circuit of depth $d$ that solves to\ninformation-theoretic optimality a ``dense'' variant of the celebrated planted\n$k$-clique problem requires exponential-in-$n^{\\Theta(1/d)}$ size.",
            "author": [
                "David Gamarnik",
                "Elchanan Mossel",
                "Ilias Zadik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04204v2",
                "http://arxiv.org/pdf/2311.04204v2"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "math.PR",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04199v1",
            "title": "Exploring Recommendation Capabilities of GPT-4V(ision): A Preliminary\n  Case Study",
            "updated": "2023-11-07T18:39:10Z",
            "published": "2023-11-07T18:39:10Z",
            "summary": "Large Multimodal Models (LMMs) have demonstrated impressive performance\nacross various vision and language tasks, yet their potential applications in\nrecommendation tasks with visual assistance remain unexplored. To bridge this\ngap, we present a preliminary case study investigating the recommendation\ncapabilities of GPT-4V(ison), a recently released LMM by OpenAI. We construct a\nseries of qualitative test samples spanning multiple domains and employ these\nsamples to assess the quality of GPT-4V's responses within recommendation\nscenarios. Evaluation results on these test samples prove that GPT-4V has\nremarkable zero-shot recommendation abilities across diverse domains, thanks to\nits robust visual-text comprehension capabilities and extensive general\nknowledge. However, we have also identified some limitations in using GPT-4V\nfor recommendations, including a tendency to provide similar responses when\ngiven similar inputs. This report concludes with an in-depth discussion of the\nchallenges and research opportunities associated with utilizing GPT-4V in\nrecommendation scenarios. Our objective is to explore the potential of\nextending LMMs from vision and language tasks to recommendation tasks. We hope\nto inspire further research into next-generation multimodal generative\nrecommendation models, which can enhance user experiences by offering greater\ndiversity and interactivity. All images and prompts used in this report will be\naccessible at https://github.com/PALIN2018/Evaluate_GPT-4V_Rec.",
            "author": [
                "Peilin Zhou",
                "Meng Cao",
                "You-Liang Huang",
                "Qichen Ye",
                "Peiyan Zhang",
                "Junling Liu",
                "Yueqi Xie",
                "Yining Hua",
                "Jaeboum Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04199v1",
                "http://arxiv.org/pdf/2311.04199v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04193v1",
            "title": "Selective Visual Representations Improve Convergence and Generalization\n  for Embodied AI",
            "updated": "2023-11-07T18:34:02Z",
            "published": "2023-11-07T18:34:02Z",
            "summary": "Embodied AI models often employ off the shelf vision backbones like CLIP to\nencode their visual observations. Although such general purpose representations\nencode rich syntactic and semantic information about the scene, much of this\ninformation is often irrelevant to the specific task at hand. This introduces\nnoise within the learning process and distracts the agent's focus from\ntask-relevant visual cues. Inspired by selective attention in humans-the\nprocess through which people filter their perception based on their\nexperiences, knowledge, and the task at hand-we introduce a parameter-efficient\napproach to filter visual stimuli for embodied AI. Our approach induces a\ntask-conditioned bottleneck using a small learnable codebook module. This\ncodebook is trained jointly to optimize task reward and acts as a\ntask-conditioned selective filter over the visual observation. Our experiments\nshowcase state-of-the-art performance for object goal navigation and object\ndisplacement across 5 benchmarks, ProcTHOR, ArchitecTHOR, RoboTHOR, AI2-iTHOR,\nand ManipulaTHOR. The filtered representations produced by the codebook are\nalso able generalize better and converge faster when adapted to other\nsimulation environments such as Habitat. Our qualitative analyses show that\nagents explore their environments more effectively and their representations\nretain task-relevant information like target object recognition while ignoring\nsuperfluous information about other objects. Code and pretrained models are\navailable at our project website: https://embodied-codebook.github.io.",
            "author": [
                "Ainaz Eftekhar",
                "Kuo-Hao Zeng",
                "Jiafei Duan",
                "Ali Farhadi",
                "Ani Kembhavi",
                "Ranjay Krishna"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04193v1",
                "http://arxiv.org/pdf/2311.04193v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04192v1",
            "title": "JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures\n  for Image Captioning Models",
            "updated": "2023-11-07T18:33:34Z",
            "published": "2023-11-07T18:33:34Z",
            "summary": "Image captioning studies heavily rely on automatic evaluation metrics such as\nBLEU and METEOR. However, such n-gram-based metrics have been shown to\ncorrelate poorly with human evaluation, leading to the proposal of alternative\nmetrics such as SPICE for English; however, no equivalent metrics have been\nestablished for other languages. Therefore, in this study, we propose an\nautomatic evaluation metric called JaSPICE, which evaluates Japanese captions\nbased on scene graphs. The proposed method generates a scene graph from\ndependencies and the predicate-argument structure, and extends the graph using\nsynonyms. We conducted experiments employing 10 image captioning models trained\non STAIR Captions and PFN-PIC and constructed the Shichimi dataset, which\ncontains 103,170 human evaluations. The results showed that our metric\noutperformed the baseline metrics for the correlation coefficient with the\nhuman evaluation.",
            "author": [
                "Yuiga Wada",
                "Kanta Kaneda",
                "Komei Sugiura"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04192v1",
                "http://arxiv.org/pdf/2311.04192v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04190v1",
            "title": "Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality\n  Monitoring of the Hadron Calorimeter",
            "updated": "2023-11-07T18:33:08Z",
            "published": "2023-11-07T18:33:08Z",
            "summary": "The compact muon solenoid (CMS) experiment is a general-purpose detector for\nhigh-energy collision at the large hadron collider (LHC) at CERN. It employs an\nonline data quality monitoring (DQM) system to promptly spot and diagnose\nparticle data acquisition problems to avoid data quality loss. In this study,\nwe present semi-supervised spatio-temporal anomaly detection (AD) monitoring\nfor the physics particle reading channels of the hadronic calorimeter (HCAL) of\nthe CMS using three-dimensional digi-occupancy map data of the DQM. We propose\nthe GraphSTAD system, which employs convolutional and graph neural networks to\nlearn local spatial characteristics induced by particles traversing the\ndetector, and global behavior owing to shared backend circuit connections and\nhousing boxes of the channels, respectively. Recurrent neural networks capture\nthe temporal evolution of the extracted spatial features. We have validated the\naccuracy of the proposed AD system in capturing diverse channel fault types\nusing the LHC Run-2 collision data sets. The GraphSTAD system has achieved\nproduction-level accuracy and is being integrated into the CMS core production\nsystem--for real-time monitoring of the HCAL. We have also provided a\nquantitative performance comparison with alternative benchmark models to\ndemonstrate the promising leverage of the presented system.",
            "author": [
                "Mulugeta Weldezgina Asres",
                "Christian Walter Omlin",
                "Long Wang",
                "David Yu",
                "Pavel Parygin",
                "Jay Dittmann",
                "Georgia Karapostoli",
                "Markus Seidel",
                "Rosamaria Venditti",
                "Luka Lambrecht",
                "Emanuele Usai",
                "Muhammad Ahmad",
                "Javier Fernandez Menendez",
                "Kaori Maeshima",
                "the CMS-HCAL Collaboration"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04190v1",
                "http://arxiv.org/pdf/2311.04190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04184v1",
            "title": "Approximation of Subgraph Counts in the Uniform Attachment Model",
            "updated": "2023-11-07T18:26:02Z",
            "published": "2023-11-07T18:26:02Z",
            "summary": "We use Stein's method to obtain distributional approximations of subgraph\ncounts in the uniform attachment model or random directed acyclic graph; we\nprovide also estimates of rates of convergence. In particular, we give uni- and\nmulti-variate Poisson approximations to the counts of cycles, and normal\napproximations to the counts of unicyclic subgraphs; we also give a partial\nresult for the counts of trees. We further find a class of multicyclic graphs\nwhose subgraph counts are a.s. bounded as $n\\to\\infty$.",
            "author": [
                "Johan Bj\u00f6rklund",
                "Cecilia Holmgren",
                "Svante Janson",
                "Tiffany Y. Y. Lo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04184v1",
                "http://arxiv.org/pdf/2311.04184v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60C05, 05C05, 60F05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04180v1",
            "title": "Polyregular functions on unordered trees of bounded height",
            "updated": "2023-11-07T18:10:37Z",
            "published": "2023-11-07T18:10:37Z",
            "summary": "We consider injective first-order interpretations that input and output trees\nof bounded height. The corresponding functions have polynomial output size,\nsince a first-order interpretation can use a k-tuple of input nodes to\nrepresent a single output node. We prove that the equivalence problem for such\nfunctions is decidable, i.e. given two such interpretations, one can decide\nwhether, for every input tree, the two output trees are isomorphic.\n  We also give a calculus of typed functions and combinators which derives\nexactly injective first-order interpretations for unordered trees of bounded\nheight. The calculus is based on a type system, where the type constructors are\nproducts, coproducts and a monad of multisets. Thanks to our results about\ntree-to-tree interpretations, the equivalence problem is decidable for this\ncalculus.\n  As an application, we show that the equivalence problem is decidable for\nfirst-order interpretations between classes of graphs that have bounded\ntree-depth. In all cases studied in this paper, first-order logic and MSO have\nthe same expressive power, and hence all results apply also to MSO\ninterpretations.",
            "author": [
                "Miko\u0142aj Boja\u0144czyk",
                "Bartek Klin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04180v1",
                "http://arxiv.org/pdf/2311.04180v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04177v1",
            "title": "Enhancing LLM Intelligence with ARM-RAG: Auxiliary Rationale Memory for\n  Retrieval Augmented Generation",
            "updated": "2023-11-07T18:03:23Z",
            "published": "2023-11-07T18:03:23Z",
            "summary": "Large Language Models (LLMs) are smart but forgetful. Recent studies, (e.g.,\n(Bubeck et al., 2023)) on modern LLMs have shown that they are capable of\nperforming amazing tasks typically necessitating human-level intelligence.\nHowever, unlike humans, frozen LLMs do not improve over time; they neither\nacquire new knowledge nor learn from their successes or failures. Some\napproaches to improving the intelligence of LLMs include fine-tuning models\nbased on problem-solving performance (Zelikman et al., 2022), and building\nbigger and more sophisticated models (Bubeck et al., 2023). However, these\nmethods have the drawback of requiring substantial data and computational\nresources to retrain existing models. In this paper, we explore the use of\nRetrieval Augmented Generation, also known as RAG (Lewis et al., 2021) to\nimprove problem-solving performance. We propose ARM-RAG (Auxiliary Rationale\nMemory for Retrieval Augmented Generation), a system that learns from its\nsuccesses without incurring high training costs. We demonstrate that the\nstorage and subsequent retrieval of reasoning chains have a positive influence\non performance in grade-school math problems.",
            "author": [
                "Eric Melz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04177v1",
                "http://arxiv.org/pdf/2311.04177v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04142v1",
            "title": "What is Lost in Knowledge Distillation?",
            "updated": "2023-11-07T17:13:40Z",
            "published": "2023-11-07T17:13:40Z",
            "summary": "Deep neural networks (DNNs) have improved NLP tasks significantly, but\ntraining and maintaining such networks could be costly. Model compression\ntechniques, such as, knowledge distillation (KD), have been proposed to address\nthe issue; however, the compression process could be lossy. Motivated by this,\nour work investigates how a distilled student model differs from its teacher,\nif the distillation process causes any information losses, and if the loss\nfollows a specific pattern. Our experiments aim to shed light on the type of\ntasks might be less or more sensitive to KD by reporting data points on the\ncontribution of different factors, such as the number of layers or attention\nheads. Results such as ours could be utilized when determining effective and\nefficient configurations to achieve optimal information transfers between\nlarger (teacher) and smaller (student) models.",
            "author": [
                "Manas Mohanty",
                "Tanya Roosta",
                "Peyman Passban"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04142v1",
                "http://arxiv.org/pdf/2311.04142v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04140v2",
            "title": "A Nearly Linear-Time Distributed Algorithm for Exact Maximum Matching",
            "updated": "2023-11-14T05:03:25Z",
            "published": "2023-11-07T17:12:58Z",
            "summary": "In this paper, we propose a randomized $\\tilde{O}(\\mu(G))$-round algorithm\nfor the maximum cardinality matching problem in the CONGEST model, where\n$\\mu(G)$ means the maximum size of a matching of the input graph $G$. The\nproposed algorithm substantially improves the current best worst-case running\ntime. The key technical ingredient is a new randomized algorithm of finding an\naugmenting path of length $\\ell$ with high probability within $\\tilde{O}(\\ell)$\nrounds, which positively settles an open problem left in the prior work by\nAhmadi and Kuhn [DISC'20].\n  The idea of our augmenting path algorithm is based on a recent result by\nKitamura and Izumi [IEICE Trans.'22], which efficiently identifies a sparse\nsubstructure of the input graph containing an augmenting path, following a new\nconcept called \\emph{alternating base trees}. Their algorithm, however, resorts\nto a centralized approach of collecting the entire information of the\nsubstructure into a single vertex for constructing an augmenting path. The\ntechnical highlight of this paper is to provide a fully-decentralized\ncounterpart of such a centralized method. To develop the algorithm, we prove\nseveral new structural properties of alternating base trees, which are of\nindependent interest.",
            "author": [
                "Taisuke Izumi",
                "Naoki Kitamura",
                "Yutaro Yamaguchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04140v2",
                "http://arxiv.org/pdf/2311.04140v2"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04136v1",
            "title": "On the $\\mathrm{v}$-number of Gorenstein ideals and Frobenius powers",
            "updated": "2023-11-07T17:07:00Z",
            "published": "2023-11-07T17:07:00Z",
            "summary": "In this paper, we show the equality of the (local) $\\mathrm{v}$-number and\nCastelnuovo-Mumford regularity of certain classes of Gorenstein algebras,\nincluding the class of Gorenstein monomial algebras. Also, for the same classes\nof algebras with the assumption of level, we show that the (local)\n$\\mathrm{v}$-number serves as an upper bound for the regularity. Moreover, we\ninvestigate the $\\mathrm{v}$-number of Frobenius powers of graded ideals in\nprime characteristic setup. In this study, we demonstrate that the\n$\\mathrm{v}$-numbers of Frobenius powers of graded ideals have an\nasymptotically linear behaviour. In the case of unmixed monomial ideals, we\nprovide a method for computing the $\\mathrm{v}$-number without prior knowledge\nof the associated primes.",
            "author": [
                "Nirmal Kotal",
                "Kamalesh Saha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04136v1",
                "http://arxiv.org/pdf/2311.04136v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO",
                "13H10, 13A35, 13F20, 05E40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04126v2",
            "title": "From Diagram to Deployment: Translating BPMN Collaborations into X-Klaim\n  for Efficient Multi-Robot System Programming",
            "updated": "2023-11-14T16:55:26Z",
            "published": "2023-11-07T16:53:34Z",
            "summary": "This paper introduces a novel method for translating Business Process Model\nand Notation (BPMN) diagrams into executable X-Klaim code for Multi-Robot\nSystems (MRSs). Merging the clarity of BPMN with the operational strength of\nX-Klaim, we enable the design and execution of complex robotic interactions\nwithout requiring in-depth knowledge of the underlying programming language\nfrom the users. Our approach maintains the BPMN model's core design principles\nand logic in the translation to X-Klaim, thus enhancing the readability and\nmaintainability of MRS applications. We offer a series of translated examples,\naddress optimization strategies, and introduce the B2XKLAIM tool, which\nautomates the conversion process. This method aims to streamline MRS\nprogramming and improve collaboration between roboticists and domain experts\nthroughout the design and implementation stages.",
            "author": [
                "Khalid Bourr",
                "Francesco Tiezzi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04126v2",
                "http://arxiv.org/pdf/2311.04126v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04100v1",
            "title": "Graph-controlled Permutation Mixers in QAOA for the Flexible Job-Shop\n  Problem",
            "updated": "2023-11-07T16:16:52Z",
            "published": "2023-11-07T16:16:52Z",
            "summary": "One of the most promising attempts towards solving optimization problems with\nquantum computers in the noisy intermediate scale era of quantum computing are\nvariational quantum algorithms. The Quantum Alternating Operator Ansatz\nprovides an algorithmic framework for constrained, combinatorial optimization\nproblems. As opposed to the better known standard QAOA protocol, the\nconstraints of the optimization problem are built into the mixing layers of the\nansatz circuit, thereby limiting the search to the much smaller Hilbert space\nof feasible solutions. In this work we develop mixing operators for a wide\nrange of scheduling problems including the flexible job shop problem. These\nmixing operators are based on a special control scheme defined by a constraint\ngraph model. After describing an explicit construction of those mixing\noperators, they are proven to be feasibility preserving, as well as exploring\nthe feasible subspace.",
            "author": [
                "Lilly Palackal",
                "Leonhard Richter",
                "Maximilian Hess"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04100v1",
                "http://arxiv.org/pdf/2311.04100v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04078v1",
            "title": "A Lightweight and Secure PUF-Based Authentication and Key-exchange\n  Protocol for IoT Devices",
            "updated": "2023-11-07T15:42:14Z",
            "published": "2023-11-07T15:42:14Z",
            "summary": "The Internet of Things (IoT) has improved people's lives by seamlessly\nintegrating into many facets of modern life and facilitating information\nsharing across platforms. Device Authentication and Key exchange are major\nchallenges for the IoT. High computational resource requirements for\ncryptographic primitives and message transmission during Authentication make\nthe existing methods like PKI and IBE not suitable for these resource\nconstrained devices. PUF appears to offer a practical and economical security\nmechanism in place of typically sophisticated cryptosystems like PKI and IBE.\nPUF provides an unclonable and tamper sensitive unique signature based on the\nPUF chip by using manufacturing process variability. Therefore, in this study,\nwe use lightweight bitwise XOR, hash function, and PUF to Authenticate IoT\ndevices. Despite several studies employing the PUF to authenticate\ncommunication between IoT devices, to the authors' knowledge, existing\nsolutions require intermediary gateway and internet capabilities by the IoT\ndevice to directly interact with a Server for Authentication and hence, are not\nscalable when the IoT device works on different technologies like BLE, Zigbee,\netc. To address the aforementioned issue, we present a system in which the IoT\ndevice does not require a continuous active internet connection to communicate\nwith the server in order to Authenticate itself. The results of a thorough\nsecurity study are validated against adversarial attacks and PUF modeling\nattacks. For formal security validation, the AVISPA verification tool is also\nused. Performance study recommends this protocol's lightweight characteristics.\nThe proposed protocol's acceptability and defenses against adversarial assaults\nare supported by a prototype developed with ESP32.",
            "author": [
                "Chandranshu Gupta",
                "Gaurav Varshney"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04078v1",
                "http://arxiv.org/pdf/2311.04078v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04060v1",
            "title": "Estimator-Coupled Reinforcement Learning for Robust Purely Tactile\n  In-Hand Manipulation",
            "updated": "2023-11-07T15:19:50Z",
            "published": "2023-11-07T15:19:50Z",
            "summary": "This paper identifies and addresses the problems with naively combining\n(reinforcement) learning-based controllers and state estimators for robotic\nin-hand manipulation. Specifically, we tackle the challenging task of purely\ntactile, goal-conditioned, dextrous in-hand reorientation with the hand\npointing downwards. Due to the limited sensing available, many control\nstrategies that are feasible in simulation when having full knowledge of the\nobject's state do not allow for accurate state estimation. Hence, separately\ntraining the controller and the estimator and combining the two at test time\nleads to poor performance. We solve this problem by coupling the control policy\nto the state estimator already during training in simulation. This approach\nleads to more robust state estimation and overall higher performance on the\ntask while maintaining an interpretability advantage over end-to-end policy\nlearning. With our GPU-accelerated implementation, learning from scratch takes\na median training time of only 6.5 hours on a single, low-cost GPU. In\nsimulation experiments with the DLR-Hand II and for four significantly\ndifferent object shapes, we provide an in-depth analysis of the performance of\nour approach. We demonstrate the successful sim2real transfer by rotating the\nfour objects to all 24 orientations in the $\\pi/2$ discretization of SO(3),\nwhich has never been achieved for such a diverse set of shapes. Finally, our\nmethod can reorient a cube consecutively to nine goals (median), which was\nbeyond the reach of previous methods in this challenging setting.",
            "author": [
                "Lennart R\u00f6stel",
                "Johannes Pitz",
                "Leon Sievers",
                "Berthold B\u00e4uml"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04060v1",
                "http://arxiv.org/pdf/2311.04060v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04040v1",
            "title": "Data exploitation: multi-task learning of object detection and semantic\n  segmentation on partially annotated data",
            "updated": "2023-11-07T14:49:54Z",
            "published": "2023-11-07T14:49:54Z",
            "summary": "Multi-task partially annotated data where each data point is annotated for\nonly a single task are potentially helpful for data scarcity if a network can\nleverage the inter-task relationship. In this paper, we study the joint\nlearning of object detection and semantic segmentation, the two most popular\nvision problems, from multi-task data with partial annotations. Extensive\nexperiments are performed to evaluate each task performance and explore their\ncomplementarity when a multi-task network cannot optimize both tasks\nsimultaneously. We propose employing knowledge distillation to leverage\njoint-task optimization. The experimental results show favorable results for\nmulti-task learning and knowledge distillation over single-task learning and\neven full supervision scenario. All code and data splits are available at\nhttps://github.com/lhoangan/multas",
            "author": [
                "Ho\u00e0ng-\u00c2n L\u00ea",
                "Minh-Tan Pham"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04040v1",
                "http://arxiv.org/pdf/2311.04040v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04031v1",
            "title": "Ramsey Quantifiers in Linear Arithmetics",
            "updated": "2023-11-07T14:33:29Z",
            "published": "2023-11-07T14:33:29Z",
            "summary": "We study Satisfiability Modulo Theories (SMT) enriched with the so-called\nRamsey quantifiers, which assert the existence of cliques (complete graphs) in\nthe graph induced by some formulas. The extended framework is known to have\napplications in proving program termination (in particular, whether a\ntransitive binary predicate is well-founded), and monadic decomposability of\nSMT formulas. Our main result is a new algorithm for eliminating Ramsey\nquantifiers from three common SMT theories: Linear Integer Arithmetic (LIA),\nLinear Real Arithmetic (LRA), and Linear Integer Real Arithmetic (LIRA). In\nparticular, if we work only with existentially quantified formulas, then our\nalgorithm runs in polynomial time and produces a formula of linear size. One\nimmediate consequence is that checking well-foundedness of a given formula in\nthe aforementioned theory defining a transitive predicate can be\nstraightforwardly handled by highly optimized SMT-solvers. We show also how\nthis provides a uniform semi-algorithm for verifying termination and liveness\nwith completeness guarantee (in fact, with an optimal computational complexity)\nfor several well-known classes of infinite-state systems, which include\nsuccinct timed systems, one-counter systems, and monotonic counter systems.\nAnother immediate consequence is a solution to an open problem on checking\nmonadic decomposability of a given relation in quantifier-free fragments of LRA\nand LIRA, which is an important problem in automated reasoning and constraint\ndatabases. Our result immediately implies decidability of this problem with an\noptimal complexity (coNP-complete) and enables exploitation of SMT-solvers. It\nalso provides a termination guarantee for the generic monadic decomposition\nalgorithm of Veanes et al. for LIA, LRA, and LIRA. We report encouraging\nexperimental results on a prototype implementation of our algorithms on\nmicro-benchmarks.",
            "author": [
                "Pascal Bergstr\u00e4\u00dfer",
                "Moses Ganardi",
                "Anthony W. Lin",
                "Georg Zetzsche"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04031v1",
                "http://arxiv.org/pdf/2311.04031v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04030v1",
            "title": "A Biologically-Inspired Computational Model of Time Perception",
            "updated": "2023-11-07T14:32:51Z",
            "published": "2023-11-07T14:32:51Z",
            "summary": "Time perception - how humans and animals perceive the passage of time - forms\nthe basis for important cognitive skills such as decision-making, planning, and\ncommunication. In this work, we propose a framework for examining the\nmechanisms responsible for time perception. We first model neural time\nperception as a combination of two known timing sources: internal neuronal\nmechanisms and external (environmental) stimuli, and design a decision-making\nframework to replicate them. We then implement this framework in a simulated\nrobot. We measure the agent's success on a temporal discrimination task\noriginally conducted by mice to evaluate its capacity to exploit temporal\nknowledge. We conclude that the agent is able to perceive time similarly to\nanimals when it comes to their intrinsic mechanisms of interpreting time and\nperforming time-aware actions. Next, by analysing the behaviour of agents\nequipped with the framework, we propose an estimator to infer characteristics\nof the timing mechanisms intrinsic to the agents. In particular, we show that\nfrom their empirical action probability distribution we are able to estimate\nparameters used for perceiving time. Overall, our work shows promising results\nwhen it comes to drawing conclusions regarding some of the characteristics\npresent in biological timing mechanisms.",
            "author": [
                "In\u00eas Louren\u00e7o",
                "Robert Mattila",
                "Rodrigo Ventura",
                "Bo Wahlberg"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TCDS.2021.3120301",
                "http://arxiv.org/abs/2311.04030v1",
                "http://arxiv.org/pdf/2311.04030v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04028v1",
            "title": "Chaotic transport in symplectic maps: Applications in plasma",
            "updated": "2023-11-07T14:27:39Z",
            "published": "2023-11-07T14:27:39Z",
            "summary": "Chaotic transport is related to the complex dynamical evolution of chaotic\ntrajectories in Hamiltonian systems, which models various physical processes.\nIn magnetically confined plasma, it is possible to qualitatively describe the\nconfiguration of the magnetic field via the phase space of suitable symplectic\nmaps. These phase spaces are of mixed type, where chaos coexists with regular\nmotion, and the complete understanding of the chaotic transport is a challenge\nthat, when overcomed, may provide further knowledge into the behaviour of\nconfined fusion plasma. In this research, we focus our investigation on\nproperties of chaotic transport in mixed phase spaces of two symplectic maps\nthat model the magnetic field lines of tokamaks under distinct configurations.\nThe single-null divertor map, or Boozer map, models the field lines of tokamaks\nwith poloidal divertors. The ergodic magnetic limiter map, or Ullmann map,\nmodels the magnetic configuration of tokamaks assembled with ergodic magnetic\nlimiters. We propose two numerical methods developed to study and illustrate\nthe differences between the transient behaviour of open field lines in both\nmodels while considering induced magnetic configurations that either enhance or\nrestrain the escaping field lines. The first analysis shows that the spatial\norganisation of invariant manifolds creates fitting transport channels for the\nopen field lines, influencing the average dynamical evolution in the selected\nmagnetic configurations. The second identifies trajectories that widely differs\nfrom the average chaotic behaviour, specifically detecting the stickiness\nphenomenon, which can be related to additional confinement regions in the\nnearest surroundings of magnetic islands in the plasma edge. These analyses\nmay, ultimately, assist in selecting optimal experimental parameters to achieve\nspecific goals in tokamak discharges.",
            "author": [
                "Matheus Silva Palmero"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04028v1",
                "http://arxiv.org/pdf/2311.04028v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04023v1",
            "title": "Only long edges can erase the subcritical annulus-crossing phase of\n  weight-dependent random connection models",
            "updated": "2023-11-07T14:21:53Z",
            "published": "2023-11-07T14:21:53Z",
            "summary": "This short note aims at complementing the results of the recent work\narXiv:2302.05396, where Jahnel and L\\\"uchtrath investigate the question of\nexistence of a subcritical percolation phase for the annulus-crossing\nprobabilities in a large class of continuum percolation models, namely the\nclassical or generalized weight-dependent random connection models. Their work\nrelates the absence of a subcritical phase to the occurrence of long edges in\nthe graph, through a somewhat indirect criterium, that stays inconclusive in\nsome models of interest. We provide a more direct and arguably simpler\ncriterium, that is always conclusive when considering classical\nweight-dependent random connection models.",
            "author": [
                "Emmanuel Jacob"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04023v1",
                "http://arxiv.org/pdf/2311.04023v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60K35, 82B43"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06302v1",
            "title": "Knowledge-Based Support for Adhesive Selection: Will it Stick?",
            "updated": "2023-11-07T14:02:32Z",
            "published": "2023-11-07T14:02:32Z",
            "summary": "As the popularity of adhesive joints in industry increases, so does the need\nfor tools to support the process of selecting a suitable adhesive. While some\nsuch tools already exist, they are either too limited in scope, or offer too\nlittle flexibility in use. This work presents a more advanced tool, that was\ndeveloped together with a team of adhesive experts. We first extract the\nexperts' knowledge about this domain and formalize it in a Knowledge Base (KB).\nThe IDP-Z3 reasoning system can then be used to derive the necessary\nfunctionality from this KB. Together with a user-friendly interactive\ninterface, this creates an easy-to-use tool capable of assisting the adhesive\nexperts. To validate our approach, we performed user testing in the form of\nqualitative interviews. The experts are very positive about the tool, stating\nthat, among others, it will help save time and find more suitable adhesives.\nUnder consideration in Theory and Practice of Logic Programming (TPLP).",
            "author": [
                "Simon Vandevelde",
                "Jeroen Jordens",
                "Bart Van Doninck",
                "Maarten Witters",
                "Joost Vennekens"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06302v1",
                "http://arxiv.org/pdf/2311.06302v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04255v1",
            "title": "Quantum stabilizer formalism for any composite system",
            "updated": "2023-11-07T13:47:57Z",
            "published": "2023-11-07T13:47:57Z",
            "summary": "The quantum stabilizer formalism was originally introduced to describe\nquantum error correction codes more conveniently and now are also playing an\nimportant role in many other fields, e.g., quantum computing and quantum\nfoundation. In this dissertation, we first introduce relevant background and\nnecessary basic knowledge, then introduce the definition of quantum stabilizer\nand its application in quantum system evolution and measurement. Finally, we\ntry to extend the quantum stabilizer formalism to qubit-qutrit and\nqubit-ququart systems which not defined before, and further define quantum\nstabilizers of arbitrary composite systems.",
            "author": [
                "Zhelin Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04255v1",
                "http://arxiv.org/pdf/2311.04255v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03976v1",
            "title": "Its All Graph To Me: Foundational Topology Models with Contrastive\n  Learning on Multiple Domains",
            "updated": "2023-11-07T13:24:01Z",
            "published": "2023-11-07T13:24:01Z",
            "summary": "Representations and embeddings of graph data have been essential in many\ndomains of research.\n  The principle benefit of learning such representations is that the\npre-trained model can be fine-tuned on smaller datasets where data or labels\nare scarse.\n  Existing models, however, are domain specific; for example a model trained on\nmolecular graphs is fine-tuned on other molecular graphs.\n  This means that in many application cases the choice of pre-trained model can\nbe arbitrary, and novel domains may lack an appropriate pre-trained model.\n  This is of particular issue where data is scarse, precluding traditional\nsupervised methods.\n  In this work we use adversarial contrastive learning to present a \\method, a\nmodel pre-trained on many graph domains.\n  We train the model only on topologies but include node labels in evaluation.\n  We evaluate the efficacy of its learnt representations on various downstream\ntasks.\n  Against baseline models pre-trained on single domains, as well as un-trained\nmodels and non-transferred models, we show that performance is equal or better\nusing our single model.\n  This includes when node labels are used in evaluation, where performance is\nconsistently superior to single-domain or non-pre-trained models.",
            "author": [
                "Alex O. Davies",
                "Riku W. Green",
                "Nirav S. Ajmeri",
                "Telmo M. Silva Filho"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03976v1",
                "http://arxiv.org/pdf/2311.03976v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03943v2",
            "title": "CLIP Guided Image-perceptive Prompt Learning for Image Enhancement",
            "updated": "2023-11-22T07:52:06Z",
            "published": "2023-11-07T12:36:20Z",
            "summary": "Image enhancement is a significant research area in the fields of computer\nvision and image processing. In recent years, many learning-based methods for\nimage enhancement have been developed, where the Look-up-table (LUT) has proven\nto be an effective tool. In this paper, we delve into the potential of\nContrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning,\nproposing a simple structure called CLIP-LUT for image enhancement. We found\nthat the prior knowledge of CLIP can effectively discern the quality of\ndegraded images, which can provide reliable guidance. To be specific, We\ninitially learn image-perceptive prompts to distinguish between original and\ntarget images using CLIP model, in the meanwhile, we introduce a very simple\nnetwork by incorporating a simple baseline to predict the weights of three\ndifferent LUT as enhancement network. The obtained prompts are used to steer\nthe enhancement network like a loss function and improve the performance of\nmodel. We demonstrate that by simply combining a straightforward method with\nCLIP, we can obtain satisfactory results.",
            "author": [
                "Weiwen Chen",
                "Qiuhong Ke",
                "Zinuo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03943v2",
                "http://arxiv.org/pdf/2311.03943v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03942v1",
            "title": "The Music Meta Ontology: a flexible semantic model for the\n  interoperability of music metadata",
            "updated": "2023-11-07T12:35:15Z",
            "published": "2023-11-07T12:35:15Z",
            "summary": "The semantic description of music metadata is a key requirement for the\ncreation of music datasets that can be aligned, integrated, and accessed for\ninformation retrieval and knowledge discovery. It is nonetheless an open\nchallenge due to the complexity of musical concepts arising from different\ngenres, styles, and periods -- standing to benefit from a lingua franca to\naccommodate various stakeholders (musicologists, librarians, data engineers,\netc.). To initiate this transition, we introduce the Music Meta ontology, a\nrich and flexible semantic model to describe music metadata related to artists,\ncompositions, performances, recordings, and links. We follow eXtreme Design\nmethodologies and best practices for data engineering, to reflect the\nperspectives and the requirements of various stakeholders into the design of\nthe model, while leveraging ontology design patterns and accounting for\nprovenance at different levels (claims, links). After presenting the main\nfeatures of Music Meta, we provide a first evaluation of the model, alignments\nto other schema (Music Ontology, DOREMUS, Wikidata), and support for data\ntransformation.",
            "author": [
                "Jacopo de Berardinis",
                "Valentina Anita Carriero",
                "Albert Mero\u00f1o-Pe\u00f1uela",
                "Andrea Poltronieri",
                "Valentina Presutti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03942v1",
                "http://arxiv.org/pdf/2311.03942v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.MM",
                "68T30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03941v1",
            "title": "Dimension-independent weak value estimation via controlled SWAP\n  operations",
            "updated": "2023-11-07T12:31:27Z",
            "published": "2023-11-07T12:31:27Z",
            "summary": "Weak values of quantum observables are a powerful tool for investigating a\nbroad spectrum of quantum phenomena. For this reason, several methods to\nmeasure them in the laboratory have been proposed. Some of these methods\nrequire weak interactions and postselection, while others are deterministic,\nbut require statistics over a number of experiments growing exponentially with\nthe number of measured particles. Here we propose a deterministic\ndimension-independent scheme for estimating weak values of arbitrary\nobservables. The scheme, based on coherently controlled SWAP operations, does\nnot require prior knowledge of the initial and final states, nor of the\nmeasured observables, and therefore can work with uncharacterized preparation\nand measurement devices. As a byproduct, our scheme provides an alternative\nexpression for two-time states, that is, states describing quantum systems\nsubject to pre and post-selections. Using this expression, we show that the\ncontrolled-SWAP scheme can be used to estimate weak values for a class of\ntwo-time states associated to bipartite quantum states with positive partial\ntranspose.",
            "author": [
                "Giulio Chiribella",
                "Kyrylo Simonov",
                "Xuanqiang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03941v1",
                "http://arxiv.org/pdf/2311.03941v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04254v2",
            "title": "Everything of Thoughts: Defying the Law of Penrose Triangle for Thought\n  Generation",
            "updated": "2023-11-12T15:09:52Z",
            "published": "2023-11-07T12:30:36Z",
            "summary": "Recent advancements in Large Language Models (LLMs) have revolutionized\ndecision-making by breaking down complex problems into more manageable language\nsequences referred to as ``thoughts''. An effective thought design should\nconsider three key perspectives: performance, efficiency, and flexibility.\nHowever, existing thought can at most exhibit two of these attributes. To\naddress these limitations, we introduce a novel thought prompting approach\ncalled ``Everything of Thoughts'' (XoT) to defy the law of ``Penrose triangle\nof existing thought paradigms. XoT leverages pretrained reinforcement learning\nand Monte Carlo Tree Search (MCTS) to incorporate external domain knowledge\ninto thoughts, thereby enhancing LLMs' capabilities and enabling them to\ngeneralize to unseen problems efficiently. Through the utilization of the\nMCTS-LLM collaborative thought revision framework, this approach autonomously\nproduces high-quality comprehensive cognitive mappings with minimal LLM\ninteractions. Additionally, XoT empowers LLMs to engage in unconstrained\nthinking, allowing for flexible cognitive mappings for problems with multiple\nsolutions. We evaluate XoT on several challenging multi-solution\nproblem-solving tasks, including Game of 24, 8-Puzzle, and Pocket Cube. Our\nresults demonstrate that XoT significantly outperforms existing approaches.\nNotably, XoT can yield multiple solutions with just one LLM call, showcasing\nits remarkable proficiency in addressing complex problems across diverse\ndomains.",
            "author": [
                "Ruomeng Ding",
                "Chaoyun Zhang",
                "Lu Wang",
                "Yong Xu",
                "Minghua Ma",
                "Wei Zhang",
                "Si Qin",
                "Saravan Rajmohan",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04254v2",
                "http://arxiv.org/pdf/2311.04254v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04242v1",
            "title": "Surgery Exact Triangles in Instanton Theory",
            "updated": "2023-11-07T12:24:16Z",
            "published": "2023-11-07T12:24:16Z",
            "summary": "We prove an exact triangle relating knot instanton Floer homology to the\ninstanton homology of surgeries along the knot. To the author's knowledge, this\nis the first such result in instanton homology with integer coefficients and\nhas no analogue in Heegaard Floer homology. To illustrate the latter claim, we\nderive as a consequence of this triangle, building on previous computations in\nthe literature, that the Poincar\\'e Homology Sphere is not an instanton\n$L$-space with $\\mathbb{Z}/2$-coefficients.",
            "author": [
                "Deeparaj Bhat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04242v1",
                "http://arxiv.org/pdf/2311.04242v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57R58"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03934v1",
            "title": "Planet formation around Intermediate-mass stars I: Different disc\n  evolutionary pathways as a function of stellar mass",
            "updated": "2023-11-07T12:15:15Z",
            "published": "2023-11-07T12:15:15Z",
            "summary": "The study of protoplanetary disc evolution and planet formation has mainly\nconcentrated on solar (and low) mass stars since they host the majority of the\nconfirmed exoplanets. Nevertheless, the numerous planets found orbiting stars\nup to $\\sim3M_\\odot$ has sparked interest in understanding how they form and\nhow their hosting discs evolve. Our goal is to improve our knowledge on the gas\ndisc evolution around intermediate mass stars for future planet formation\nstudies. We study the long-term evolution of protoplanetary discs affected by\nviscous accretion, X-ray and FUV photoevaporation from the central star around\nstars between $1 - 3M_\\odot$ considering the effects of stellar evolution. We\nexplore different values of the viscosity parameter and the initial mass of the\ndisc. We find that the evolutionary pathway of disc dispersal depends on the\nstellar mass. Our simulations reveal four distinct evolutionary pathways for\nthe gas disc not reported before that are a consequence of stellar evolution,\nand which will likely impact dust evolution and planet formation. As the\nstellar mass grows from 1 to $\\sim2M_\\odot$, the disc evolution changes from\nthe conventional inside-out clearing to a homogeneous disc evolution scenario\nwhere both inner and outer discs, formed after photoevaporation opened a gap,\nvanish over a similar timescale. As the stellar mass continues to increase,\nreaching $\\sim 3M_\\odot$, we have identified a distinct pathway that we refer\nto as revenant disc evolution, where the inner and outer discs reconnect after\nthe gap opened. For the largest masses, we observe outside-in disc dispersal,\nin which the outer disc dissipates first due to the strong FUV\nphotoevaporation. Revenant disc evolution stands out as it is capable of\nextending the disc lifespan. Otherwise, the disc dispersal time scale decreases\nwith increasing stellar mass except for low viscosity discs.",
            "author": [
                "Mar\u00eda Paula Ronco",
                "Matthias R. Schreiber",
                "Eva Villaver",
                "Octavio M. Guilera",
                "Marcelo M. Miller Bertolami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03934v1",
                "http://arxiv.org/pdf/2311.03934v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03932v2",
            "title": "TempoGRAPHer: Aggregation Based Temporal Graph Exploration",
            "updated": "2023-11-08T13:50:33Z",
            "published": "2023-11-07T12:14:34Z",
            "summary": "Graphs offer a generic abstraction for modeling entities, and the\ninteractions and relationships between them. Most real world graphs, such as\nsocial and cooperation networks evolve over time, and exploring their evolution\nmay reveal important information. In this paper, we present TempoGRAPHer, a\nsystem for visualizing and analyzing the evolution of a temporal attributed\ngraph. TempoGRAPHer supports both temporal and attribute aggregation. It also\nallows graph exploration by identifying periods of significant growth,\nshrinkage, or stability. Temporal exploration is supported by two complementary\nstrategies, namely skyline and interaction-based exploration. Skyline-based\nexploration provides insights on the overall trends in the evolution, while\ninteraction-based exploration offers a closer look at specific parts of the\ngraph evolution history where significant changes appeared. We showcase the\nusefulness of TempoGRAPHer in understanding graph evolution by presenting a\ndetailed scenario that explores the evolution of a contact network between\nprimary school students.",
            "author": [
                "Evangelia Tsoukanara",
                "Georgia Koloniari",
                "Evaggelia Pitoura"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03932v2",
                "http://arxiv.org/pdf/2311.03932v2"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04252v2",
            "title": "CNN-Based Structural Damage Detection using Time-Series Sensor Data",
            "updated": "2023-11-09T04:06:08Z",
            "published": "2023-11-07T11:57:33Z",
            "summary": "Structural Health Monitoring (SHM) is vital for evaluating structural\ncondition, aiming to detect damage through sensor data analysis. It aligns with\npredictive maintenance in modern industry, minimizing downtime and costs by\naddressing potential structural issues. Various machine learning techniques\nhave been used to extract valuable information from vibration data, often\nrelying on prior structural knowledge. This research introduces an innovative\napproach to structural damage detection, utilizing a new Convolutional Neural\nNetwork (CNN) algorithm. In order to extract deep spatial features from time\nseries data, CNNs are taught to recognize long-term temporal connections. This\nmethodology combines spatial and temporal features, enhancing discrimination\ncapabilities when compared to methods solely reliant on deep spatial features.\nTime series data are divided into two categories using the proposed neural\nnetwork: undamaged and damaged. To validate its efficacy, the method's accuracy\nwas tested using a benchmark dataset derived from a three-floor structure at\nLos Alamos National Laboratory (LANL). The outcomes show that the new CNN\nalgorithm is very accurate in spotting structural degradation in the examined\nstructure.",
            "author": [
                "Ishan Pathak",
                "Ishan Jha",
                "Aditya Sadana",
                "Basuraj Bhowmik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04252v2",
                "http://arxiv.org/pdf/2311.04252v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03904v1",
            "title": "RobustMat: Neural Diffusion for Street Landmark Patch Matching under\n  Challenging Environments",
            "updated": "2023-11-07T11:37:20Z",
            "published": "2023-11-07T11:37:20Z",
            "summary": "For autonomous vehicles (AVs), visual perception techniques based on sensors\nlike cameras play crucial roles in information acquisition and processing. In\nvarious computer perception tasks for AVs, it may be helpful to match landmark\npatches taken by an onboard camera with other landmark patches captured at a\ndifferent time or saved in a street scene image database. To perform matching\nunder challenging driving environments caused by changing seasons, weather, and\nillumination, we utilize the spatial neighborhood information of each patch. We\npropose an approach, named RobustMat, which derives its robustness to\nperturbations from neural differential equations. A convolutional neural ODE\ndiffusion module is used to learn the feature representation for the landmark\npatches. A graph neural PDE diffusion module then aggregates information from\nneighboring landmark patches in the street scene. Finally, feature similarity\nlearning outputs the final matching score. Our approach is evaluated on several\nstreet scene datasets and demonstrated to achieve state-of-the-art matching\nresults under environmental perturbations.",
            "author": [
                "Rui She",
                "Qiyu Kang",
                "Sijie Wang",
                "Yuan-Rui Yang",
                "Kai Zhao",
                "Yang Song",
                "Wee Peng Tay"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03904v1",
                "http://arxiv.org/pdf/2311.03904v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03901v1",
            "title": "Parikh's Theorem Made Symbolic",
            "updated": "2023-11-07T11:31:13Z",
            "published": "2023-11-07T11:31:13Z",
            "summary": "Parikh's Theorem is a fundamental result in automata theory with numerous\napplications in computer science: software verification (e.g. infinite-state\nverification, string constraints, and theory of arrays), verification of\ncryptographic protocols (e.g. using Horn clauses modulo equational theories)\nand database querying (e.g. evaluating path-queries in graph databases).\nParikh's Theorem states that the letter-counting abstraction of a language\nrecognized by finite automata or context-free grammars is definable in\nPresburger Arithmetic. Unfortunately, real-world applications typically require\nlarge alphabets - which are well-known to be not amenable to explicit treatment\nof the alphabets.\n  Symbolic automata have proven in the last decade to be an effective\nalgorithmic framework for handling large finite or even infinite alphabets. A\nsymbolic automaton employs an effective boolean algebra, which offers a\nsymbolic representation of character sets and often lends itself to an\nexponentially more succinct representation of a language. Instead of\nletter-counting, Parikh's Theorem for symbolic automata amounts to counting the\nnumber of times different predicates are satisfied by an input sequence.\nUnfortunately, naively applying Parikh's Theorem from classical automata theory\nto symbolic automata yields existential Presburger formulas of exponential\nsize. We provide a new construction for Parikh's Theorem for symbolic automata\nand grammars, which avoids this exponential blowup: our algorithm computes an\nexistential formula in polynomial-time over (quantifier-free) Presburger and\nthe base theory. In fact, our algorithm extends to the model of parametric\nsymbolic grammars, which are one of the most expressive models of languages\nover infinite alphabets. We have implemented our algorithm and show it can be\nused to solve string constraints that are difficult to solve by existing\nsolvers.",
            "author": [
                "Matthew Hague",
                "Artur Je\u017c",
                "Anthony W. Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03901v1",
                "http://arxiv.org/pdf/2311.03901v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03900v1",
            "title": "Why Fair Automated Hiring Systems Breach EU Non-Discrimination Law",
            "updated": "2023-11-07T11:31:00Z",
            "published": "2023-11-07T11:31:00Z",
            "summary": "Employment selection processes that use automated hiring systems based on\nmachine learning are becoming increasingly commonplace. Meanwhile, concerns\nabout algorithmic direct and indirect discrimination that result from such\nsystems are front-and-center, and the technical solutions provided by the\nresearch community often systematically deviate from the principle of equal\ntreatment to combat disparate or adverse impacts on groups based on protected\nattributes. Those technical solutions are now being used in commercially\navailable automated hiring systems, potentially engaging in real-world\ndiscrimination. Algorithmic fairness and algorithmic non-discrimination are not\nthe same. This article examines a conflict between the two: whether such hiring\nsystems are compliant with EU non-discrimination law.",
            "author": [
                "Robert Lee Poe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03900v1",
                "http://arxiv.org/pdf/2311.03900v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03897v1",
            "title": "Temporal Graph Representation Learning with Adaptive Augmentation\n  Contrastive",
            "updated": "2023-11-07T11:21:16Z",
            "published": "2023-11-07T11:21:16Z",
            "summary": "Temporal graph representation learning aims to generate low-dimensional\ndynamic node embeddings to capture temporal information as well as structural\nand property information. Current representation learning methods for temporal\nnetworks often focus on capturing fine-grained information, which may lead to\nthe model capturing random noise instead of essential semantic information.\nWhile graph contrastive learning has shown promise in dealing with noise, it\nonly applies to static graphs or snapshots and may not be suitable for handling\ntime-dependent noise. To alleviate the above challenge, we propose a novel\nTemporal Graph representation learning with Adaptive augmentation Contrastive\n(TGAC) model. The adaptive augmentation on the temporal graph is made by\ncombining prior knowledge with temporal information, and the contrastive\nobjective function is constructed by defining the augmented inter-view contrast\nand intra-view contrast. To complement TGAC, we propose three adaptive\naugmentation strategies that modify topological features to reduce noise from\nthe network. Our extensive experiments on various real networks demonstrate\nthat the proposed model outperforms other temporal graph representation\nlearning methods.",
            "author": [
                "Hongjiang Chen",
                "Pengfei Jiao",
                "Huijun Tang",
                "Huaming Wu"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43415-0_40",
                "http://arxiv.org/abs/2311.03897v1",
                "http://arxiv.org/pdf/2311.03897v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04250v1",
            "title": "Unifying Structure and Language Semantic for Efficient Contrastive\n  Knowledge Graph Completion with Structured Entity Anchors",
            "updated": "2023-11-07T11:17:55Z",
            "published": "2023-11-07T11:17:55Z",
            "summary": "The goal of knowledge graph completion (KGC) is to predict missing links in a\nKG using trained facts that are already known. In recent, pre-trained language\nmodel (PLM) based methods that utilize both textual and structural information\nare emerging, but their performances lag behind state-of-the-art (SOTA)\nstructure-based methods or some methods lose their inductive inference\ncapabilities in the process of fusing structure embedding to text encoder. In\nthis paper, we propose a novel method to effectively unify structure\ninformation and language semantics without losing the power of inductive\nreasoning. We adopt entity anchors and these anchors and textual description of\nKG elements are fed together into the PLM-based encoder to learn unified\nrepresentations. In addition, the proposed method utilizes additional random\nnegative samples which can be reused in the each mini-batch during contrastive\nlearning to learn a generalized entity representations. We verify the\neffectiveness of the our proposed method through various experiments and\nanalysis. The experimental results on standard benchmark widely used in link\nprediction task show that the proposed model outperforms existing the SOTA KGC\nmodels. Especially, our method show the largest performance improvement on\nFB15K-237, which is competitive to the SOTA of structure-based KGC methods.",
            "author": [
                "Sang-Hyun Je",
                "Wontae Choi",
                "Kwangjin Oh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04250v1",
                "http://arxiv.org/pdf/2311.04250v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03895v1",
            "title": "Improved Deterministic Streaming Algorithms for Non-monotone Submodular\n  Maximization",
            "updated": "2023-11-07T11:17:32Z",
            "published": "2023-11-07T11:17:32Z",
            "summary": "Submodular maximization is one of the central topics in combinatorial\noptimization. It has found numerous applications in the real world. Streaming\nalgorithms for submodule maximization have gained attention in recent years,\nallowing for real-time processing of large data sets by looking at each piece\nof data only once. However, most of the state-of-the-art algorithms are subject\nto monotone cardinality constraint. There remain non-negligible gaps with\nrespect to approximation ratios between cardinality and other constraints like\n$d$-knapsack in non-monotone submodular maximization.\n  In this paper, we propose deterministic algorithms with improved\napproximation ratios for non-monotone submodular maximization. Specifically,\nfor the cardinality constraint, we provide a deterministic $1/6-\\epsilon$\napproximation algorithm with $O(\\frac{k\\log k}{\\epsilon})$ memory and sublinear\nquery time, while the previous best algorithm is randomized with a $0.1921$\napproximation ratio. To the best of our knowledge, this is the first\ndeterministic streaming algorithm for the cardinality constraint. For the\n$d$-knapsack constraint, we provide a deterministic $\\frac{1}{4(d+1)}-\\epsilon$\napproximation algorithm with $O(\\frac{b\\log b}{\\epsilon})$ memory and\n$O(\\frac{\\log b}{\\epsilon})$ query time per element. To the best of our\nknowledge, there is currently no streaming algorithm for this constraint.",
            "author": [
                "Xiaoming Sun",
                "Jialin Zhang",
                "Shuo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03895v1",
                "http://arxiv.org/pdf/2311.03895v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03868v1",
            "title": "The matroid of a graphing",
            "updated": "2023-11-07T10:34:29Z",
            "published": "2023-11-07T10:34:29Z",
            "summary": "Graphings serve as limit objects for bounded-degree graphs. We define the\n``cycle matroid'' of a graphing as a submodular setfunction, with values in\n[0,1], which generalizes (up to normalization) the cycle matroid of finite\ngraphs. We prove that for a Benjamini--Schramm convergent sequence of graphs,\nthe total rank, normalized by the number of nodes, converges to the total rank\nof the limit graphing.",
            "author": [
                "L\u00e1szl\u00f3 Lov\u00e1sz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03868v1",
                "http://arxiv.org/pdf/2311.03868v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C63, 05B35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03867v1",
            "title": "A Comparative Study of Knowledge Transfer Methods for Misaligned Urban\n  Building Labels",
            "updated": "2023-11-07T10:31:41Z",
            "published": "2023-11-07T10:31:41Z",
            "summary": "Misalignment in Earth observation (EO) images and building labels impact the\ntraining of accurate convolutional neural networks (CNNs) for semantic\nsegmentation of building footprints. Recently, three Teacher-Student knowledge\ntransfer methods have been introduced to address this issue: supervised domain\nadaptation (SDA), knowledge distillation (KD), and deep mutual learning (DML).\nHowever, these methods are merely studied for different urban buildings\n(low-rise, mid-rise, high-rise, and skyscrapers), where misalignment increases\nwith building height and spatial resolution. In this study, we present a\nworkflow for the systematic comparative study of the three methods. The\nworkflow first identifies the best (with the highest evaluation scores)\nhyperparameters, lightweight CNNs for the Student (among 43 CNNs from Computer\nVision), and encoder-decoder networks (EDNs) for both Teachers and Students.\nSecondly, three building footprint datasets are developed to train and evaluate\nthe identified Teachers and Students in the three transfer methods. The results\nshow that U-Net with VGG19 (U-VGG19) is the best Teacher, and\nU-EfficientNetv2B3 and U-EfficientNet-lite0 are among the best Students. With\nthese Teacher-Student pairs, SDA could yield upto 0.943, 0.868, 0.912, and\n0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers\nrespectively. KD and DML provide model compression of upto 82%, despite\nmarginal loss in performance. This new comparison concludes that SDA is the\nmost effective method to address the misalignment problem, while KD and DML can\nefficiently compress network size without significant loss in performance. The\n158 experiments and datasets developed in this study will be valuable to\nminimise the misaligned labels.",
            "author": [
                "Bipul Neupane",
                "Jagannath Aryal",
                "Abbas Rajabifard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03867v1",
                "http://arxiv.org/pdf/2311.03867v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03866v1",
            "title": "SCONE-GAN: Semantic Contrastive learning-based Generative Adversarial\n  Network for an end-to-end image translation",
            "updated": "2023-11-07T10:29:16Z",
            "published": "2023-11-07T10:29:16Z",
            "summary": "SCONE-GAN presents an end-to-end image translation, which is shown to be\neffective for learning to generate realistic and diverse scenery images. Most\ncurrent image-to-image translation approaches are devised as two mappings: a\ntranslation from the source to target domain and another to represent its\ninverse. While successful in many applications, these approaches may suffer\nfrom generating trivial solutions with limited diversity. That is because these\nmethods learn more frequent associations rather than the scene structures. To\nmitigate the problem, we propose SCONE-GAN that utilises graph convolutional\nnetworks to learn the objects dependencies, maintain the image structure and\npreserve its semantics while transferring images into the target domain. For\nmore realistic and diverse image generation we introduce style reference image.\nWe enforce the model to maximize the mutual information between the style image\nand output. The proposed method explicitly maximizes the mutual information\nbetween the related patches, thus encouraging the generator to produce more\ndiverse images. We validate the proposed algorithm for image-to-image\ntranslation and stylizing outdoor images. Both qualitative and quantitative\nresults demonstrate the effectiveness of our approach on four dataset.",
            "author": [
                "Iman Abbasnejad",
                "Fabio Zambetta",
                "Flora Salim",
                "Timothy Wiley",
                "Jeffrey Chan",
                "Russell Gallagher",
                "Ehsan Abbasnejad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03866v1",
                "http://arxiv.org/pdf/2311.03866v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07585v2",
            "title": "Input Reconstruction Attack against Vertical Federated Large Language\n  Models",
            "updated": "2023-11-24T07:46:23Z",
            "published": "2023-11-07T09:39:22Z",
            "summary": "Recently, large language models (LLMs) have drawn extensive attention from\nacademia and the public, due to the advent of the ChatGPT. While LLMs show\ntheir astonishing ability in text generation for various tasks, privacy\nconcerns limit their usage in real-life businesses. More specifically, either\nthe user's inputs (the user sends the query to the model-hosting server) or the\nmodel (the user downloads the complete model) itself will be revealed during\nthe usage. Vertical federated learning (VFL) is a promising solution to this\nkind of problem. It protects both the user's input and the knowledge of the\nmodel by splitting the model into a bottom part and a top part, which is\nmaintained by the user and the model provider, respectively. However, in this\npaper, we demonstrate that in LLMs, VFL fails to protect the user input since\nit is simple and cheap to reconstruct the input from the intermediate\nembeddings. Experiments show that even with a commercial GPU, the input\nsentence can be reconstructed in only one second. We also discuss several\npossible solutions to enhance the privacy of vertical federated LLMs.",
            "author": [
                "Fei Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07585v2",
                "http://arxiv.org/pdf/2311.07585v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03837v1",
            "title": "OLaLa: Ontology Matching with Large Language Models",
            "updated": "2023-11-07T09:34:20Z",
            "published": "2023-11-07T09:34:20Z",
            "summary": "Ontology (and more generally: Knowledge Graph) Matching is a challenging task\nwhere information in natural language is one of the most important signals to\nprocess. With the rise of Large Language Models, it is possible to incorporate\nthis knowledge in a better way into the matching pipeline. A number of\ndecisions still need to be taken, e.g., how to generate a prompt that is useful\nto the model, how information in the KG can be formulated in prompts, which\nLarge Language Model to choose, how to provide existing correspondences to the\nmodel, how to generate candidates, etc. In this paper, we present a prototype\nthat explores these questions by applying zero-shot and few-shot prompting with\nmultiple open Large Language Models to different tasks of the Ontology\nAlignment Evaluation Initiative (OAEI). We show that with only a handful of\nexamples and a well-designed prompt, it is possible to achieve results that are\nen par with supervised matching systems which use a much larger portion of the\nground truth.",
            "author": [
                "Sven Hertling",
                "Heiko Paulheim"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3587259.3627571",
                "http://arxiv.org/abs/2311.03837v1",
                "http://arxiv.org/pdf/2311.03837v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03830v1",
            "title": "Reducing Spatial Fitting Error in Distillation of Denoising Diffusion\n  Models",
            "updated": "2023-11-07T09:19:28Z",
            "published": "2023-11-07T09:19:28Z",
            "summary": "Denoising Diffusion models have exhibited remarkable capabilities in image\ngeneration. However, generating high-quality samples requires a large number of\niterations. Knowledge distillation for diffusion models is an effective method\nto address this limitation with a shortened sampling process but causes\ndegraded generative quality. Based on our analysis with bias-variance\ndecomposition and experimental observations, we attribute the degradation to\nthe spatial fitting error occurring in the training of both the teacher and\nstudent model. Accordingly, we propose $\\textbf{S}$patial\n$\\textbf{F}$itting-$\\textbf{E}$rror $\\textbf{R}$eduction\n$\\textbf{D}$istillation model ($\\textbf{SFERD}$). SFERD utilizes attention\nguidance from the teacher model and a designed semantic gradient predictor to\nreduce the student's fitting error. Empirically, our proposed model facilitates\nhigh-quality sample generation in a few function evaluations. We achieve an FID\nof 5.31 on CIFAR-10 and 9.39 on ImageNet 64$\\times$64 with only one step,\noutperforming existing diffusion methods. Our study provides a new perspective\non diffusion distillation by highlighting the intrinsic denoising ability of\nmodels.",
            "author": [
                "Shengzhe Zhou",
                "Zejian Lee",
                "Shengyuan Zhang",
                "Lefan Hou",
                "Changyuan Yang",
                "Guang Yang",
                "Lingyun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03830v1",
                "http://arxiv.org/pdf/2311.03830v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03828v2",
            "title": "Multi-view Information Integration and Propagation for Occluded Person\n  Re-identification",
            "updated": "2023-11-09T07:18:54Z",
            "published": "2023-11-07T09:17:56Z",
            "summary": "Occluded person re-identification (re-ID) presents a challenging task due to\nocclusion perturbations. Although great efforts have been made to prevent the\nmodel from being disturbed by occlusion noise, most current solutions only\ncapture information from a single image, disregarding the rich complementary\ninformation available in multiple images depicting the same pedestrian. In this\npaper, we propose a novel framework called Multi-view Information Integration\nand Propagation (MVI$^{2}$P). Specifically, realizing the potential of\nmulti-view images in effectively characterizing the occluded target pedestrian,\nwe integrate feature maps of which to create a comprehensive representation.\nDuring this process, to avoid introducing occlusion noise, we develop a\nCAMs-aware Localization module that selectively integrates information\ncontributing to the identification. Additionally, considering the divergence in\nthe discriminative nature of different images, we design a probability-aware\nQuantification module to emphatically integrate highly reliable information.\nMoreover, as multiple images with the same identity are not accessible in the\ntesting stage, we devise an Information Propagation (IP) mechanism to distill\nknowledge from the comprehensive representation to that of a single occluded\nimage. Extensive experiments and analyses have unequivocally demonstrated the\neffectiveness and superiority of the proposed MVI$^{2}$P. The code will be\nreleased at \\url{https://github.com/nengdong96/MVIIP}.",
            "author": [
                "Neng Dong",
                "Shuanglin Yan",
                "Hao Tang",
                "Jinhui Tang",
                "Liyan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03828v2",
                "http://arxiv.org/pdf/2311.03828v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03826v1",
            "title": "Accelerating Unstructured SpGEMM using Structured In-situ Computing",
            "updated": "2023-11-07T09:15:12Z",
            "published": "2023-11-07T09:15:12Z",
            "summary": "Sparse matrix-matrix multiplication (SpGEMM) is a critical kernel widely\nemployed in machine learning and graph algorithms. However, real-world\nmatrices' high sparsity makes SpGEMM memory-intensive. In-situ computing offers\nthe potential to accelerate memory-intensive applications through high\nbandwidth and parallelism. Nevertheless, the irregular distribution of\nnon-zeros renders SpGEMM a typical unstructured software. In contrast, in-situ\ncomputing platforms follow a fixed calculation manner, making them structured\nhardware. The mismatch between unstructured software and structured hardware\nleads to sub-optimal performance of current solutions.\n  In this paper, we propose SPLIM, a novel in-situ computing SpGEMM\naccelerator. SPLIM involves two innovations. First, we present a novel\ncomputation paradigm that converts SpGEMM into structured in-situ\nmultiplication and unstructured accumulation. Second, we develop a unique\ncoordinates alignment method utilizing in-situ search operations, effectively\ntransforming unstructured accumulation into high parallel searching operations.\nOur experimental results demonstrate that SPLIM achieves 275.74$\\times$\nperformance improvement and 687.19$\\times$ energy saving compared to NVIDIA RTX\nA6000 GPU.",
            "author": [
                "Huize Li",
                "Tulika Mitra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03826v1",
                "http://arxiv.org/pdf/2311.03826v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03812v1",
            "title": "Conversations in Galician: a Large Language Model for an\n  Underrepresented Language",
            "updated": "2023-11-07T08:52:28Z",
            "published": "2023-11-07T08:52:28Z",
            "summary": "The recent proliferation of Large Conversation Language Models has\nhighlighted the economic significance of widespread access to this type of AI\ntechnologies in the current information age. Nevertheless, prevailing models\nhave primarily been trained on corpora consisting of documents written in\npopular languages. The dearth of such cutting-edge tools for low-resource\nlanguages further exacerbates their underrepresentation in the current economic\nlandscape, thereby impacting their native speakers. This paper introduces two\nnovel resources designed to enhance Natural Language Processing (NLP) for the\nGalician language. We present a Galician adaptation of the Alpaca dataset,\ncomprising 52,000 instructions and demonstrations. This dataset proves\ninvaluable for enhancing language models by fine-tuning them to more accurately\nadhere to provided instructions. Additionally, as a demonstration of the\ndataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,\na language not originally supported by the model, by following the Alpaca\nformat. This work contributes to the research on multilingual models tailored\nfor low-resource settings, a crucial endeavor in ensuring the inclusion of all\nlinguistic communities in the development of Large Language Models. Another\nnoteworthy aspect of this research is the exploration of how knowledge of a\nclosely related language, in this case, Portuguese, can assist in generating\ncoherent text when training resources are scarce. Both the Galician Alpaca\ndataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we\nhave made the source code available to facilitate replication of this\nexperiment and encourage further advancements for underrepresented languages.",
            "author": [
                "Eliseo Bao",
                "Anxo P\u00e9rez",
                "Javier Parapar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03812v1",
                "http://arxiv.org/pdf/2311.03812v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03809v1",
            "title": "SoK: Security Below the OS -- A Security Analysis of UEFI",
            "updated": "2023-11-07T08:45:39Z",
            "published": "2023-11-07T08:45:39Z",
            "summary": "The Unified Extensible Firmware Interface (UEFI) is a linchpin of modern\ncomputing systems, governing secure system initialization and booting. This\npaper is urgently needed because of the surge in UEFI-related attacks and\nvulnerabilities in recent years. Motivated by this urgent concern, we undertake\nan extensive exploration of the UEFI landscape, dissecting its distribution\nsupply chain, booting process, and security features. We carefully study a\nspectrum of UEFI-targeted attacks and proofs of concept (PoCs) for exploiting\nUEFI-related vulnerabilities. Building upon these insights, we construct a\ncomprehensive attack threat model encompassing threat actors, attack vectors,\nattack types, vulnerabilities, attack capabilities, and attacker objectives.\nDrawing inspiration from the MITRE ATT&CK framework, we present a MITRE\nATT&CK-like taxonomy delineating tactics, techniques, and sub-techniques in the\ncontext of UEFI attacks. This taxonomy can provide a road map for identifying\nexisting gaps and developing new techniques for rootkit prevention, detection,\nand removal. Finally, the paper discusses existing countermeasures against UEFI\nattacks including a variety of technical and operational measures that can be\nimplemented to lower the risk of UEFI attacks to an acceptable level. This\npaper seeks to clarify the complexities of UEFI and equip the cybersecurity\ncommunity with the necessary knowledge to strengthen the security of this\ncritical component against a growing threat landscape.",
            "author": [
                "Priyanka Prakash Surve",
                "Oleg Brodt",
                "Mark Yampolskiy",
                "Yuval Elovici",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03809v1",
                "http://arxiv.org/pdf/2311.03809v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03808v2",
            "title": "An operad structure on the free commutative monoid over a positive\n  operad",
            "updated": "2023-11-24T19:17:41Z",
            "published": "2023-11-07T08:40:27Z",
            "summary": "We give the explicit description of an operad structure on the free\ncommutative monoid E o q generated by a given positive operad q. This\nconstruction, new up to our knowledge, does not seem to be reachable through a\ndistributive law.",
            "author": [
                "Dominique Manchon",
                "Hedi Regeiba",
                "Imen Rjaiba",
                "Yannic Vargas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03808v2",
                "http://arxiv.org/pdf/2311.03808v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12036v1",
            "title": "32x100 GHz WDM filter based on ultra-compact silicon rings with a high\n  thermal tuning efficiency of 5.85 mW/pi",
            "updated": "2023-11-07T08:32:51Z",
            "published": "2023-11-07T08:32:51Z",
            "summary": "To the best of our knowledge, this paper has achieved the lowest thermal\ntuning power (5.85 mW/pi) for silicon rings with FSR>=3.2 THz, and the first\nsilicon ring-based WDM-32x100 GHz filter.",
            "author": [
                "Qingzhong Deng",
                "Ahmed H. El-Saeed",
                "Alaa Elshazly",
                "Guy Lepage",
                "Chiara Marchese",
                "Hakim Kobbi",
                "Rafal Magdziak",
                "Jeroen De Coster",
                "Neha Singh",
                "Marko Ersek Filipcic",
                "Kristof Croes",
                "Dimitrios Velenis",
                "Maumita Chakrabarti",
                "Peter De Heyn",
                "Peter Verheyen",
                "Philippe Absil",
                "Filippo Ferraro",
                "Yoojin Ban",
                "Joris Van Campenhout"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12036v1",
                "http://arxiv.org/pdf/2311.12036v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03796v1",
            "title": "Port-Hamiltonian modeling of multidimensional flexible mechanical\n  structures defined by linear elastic relations $\\star$",
            "updated": "2023-11-07T08:25:52Z",
            "published": "2023-11-07T08:25:52Z",
            "summary": "This article presents a systematic methodology for modeling a class of\nflexible multidimensional mechanical structures defined by linear elastic\nrelations that directly allows to obtain their infinite-dimensional\nport-Hamiltonian representation. The approach is restricted to systems based on\na certain class of kinematic assumptions. However this class encompasses a wide\nrange of models currently available in the literature, such as\n${\\ell}$-dimensional elasticity models (with ${\\ell}$ = 1,2,3), vibrating\nstrings, torsion in circular bars, classical beam and plate models, among\nothers. The methodology is based on Hamilton's principle for a continuum medium\nwhich allows defining the energy variables of the port-Hamiltonian system, and\nalso on a generalization of the integration by parts theorem, which allows\ncharacterizing the skew-adjoint differential operator and boundary inputs and\nboundary outputs variables. To illustrate this method, the plate modeling\nprocess based on Reddy's third-order shear deformation theory is presented as\nan example. To the best of our knowledge, this is the first time that an\ninfinite-dimensional port-Hamiltonian representation of this system is\npresented in the literature.",
            "author": [
                "Cristobal Ponce",
                "Yongxin Wu",
                "Yann Le Gorrec",
                "Hector Ramirez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03796v1",
                "http://arxiv.org/pdf/2311.03796v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "physics.class-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03790v2",
            "title": "Back to the present: A general treatment for the tidal field from the\n  wake of dynamical friction",
            "updated": "2023-11-15T14:24:59Z",
            "published": "2023-11-07T08:19:30Z",
            "summary": "Dynamical friction can be a valuable tool for inferring dark matter\nproperties that are difficult to constrain by other methods. Most applications\nof dynamical friction calculations are concerned with the long-term angular\nmomentum loss and orbital decay of the perturber within its host. This,\nhowever, assumes knowledge of the unknown initial conditions of the system. We\nadvance an alternative methodology to infer the host properties from the\nperturber's shape distortions induced by the tides of the wake of dynamical\nfriction, which we refer to as the tidal dynamical friction. As the shape\ndistortions rely on the tidal field that has a predominantly local origin, we\npresent a strategy to find the local wake by integrating the stellar orbits\nback in time along with the perturber, then removing the perturber's potential\nand re-integrating them back to the present. This provides perturbed and\nunperturbed coordinates and hence a change in coordinates, density, and\nacceleration fields, which yields the back-reaction experienced by the\nperturber. The method successfully recovers the tidal field of the wake based\non a comparison with N-body simulations. We show that similar to the tidal\nfield itself, the noise and randomness of the dynamical friction force due to\nthe finite number of stars is also dominated by regions close to the perturber.\nStars near the perturber influence it more but are smaller in number, causing a\nhigh variance in the acceleration field. These fluctuations are intrinsic to\ndynamical friction. We show that a stellar density of $0.0014 {\\rm M_\\odot\\,\nkpc^{-3}}$ yields an inherent variance of 10% to the dynamical friction. The\ncurrent method extends the family of dynamical friction methods that allow for\nthe inference of host properties from tidal forces of the wake. It can be\napplied to specific galaxies, such as Magellanic Clouds, with Gaia data.",
            "author": [
                "Rain Kipper",
                "Peeter Tenjes",
                "Mar\u00eda Benito",
                "Punyakoti Ganeshaiah Veena",
                "Aikaterini Niovi Triantafyllaki",
                "Indrek Vurm",
                "Moorits Mihkel Muru",
                "Maret Einasto",
                "Elmo Tempel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03790v2",
                "http://arxiv.org/pdf/2311.03790v2"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03788v1",
            "title": "Language Representation Projection: Can We Transfer Factual Knowledge\n  across Languages in Multilingual Language Models?",
            "updated": "2023-11-07T08:16:16Z",
            "published": "2023-11-07T08:16:16Z",
            "summary": "Multilingual pretrained language models serve as repositories of multilingual\nfactual knowledge. Nevertheless, a substantial performance gap of factual\nknowledge probing exists between high-resource languages and low-resource\nlanguages, suggesting limited implicit factual knowledge transfer across\nlanguages in multilingual pretrained language models. This paper investigates\nthe feasibility of explicitly transferring relatively rich factual knowledge\nfrom English to non-English languages. To accomplish this, we propose two\nparameter-free $\\textbf{L}$anguage $\\textbf{R}$epresentation\n$\\textbf{P}$rojection modules (LRP2). The first module converts non-English\nrepresentations into English-like equivalents, while the second module reverts\nEnglish-like representations back into representations of the corresponding\nnon-English language. Experimental results on the mLAMA dataset demonstrate\nthat LRP2 significantly improves factual knowledge retrieval accuracy and\nfacilitates knowledge transferability across diverse non-English languages. We\nfurther investigate the working mechanism of LRP2 from the perspectives of\nrepresentation space and cross-lingual knowledge neuron.",
            "author": [
                "Shaoyang Xu",
                "Junzhuo Li",
                "Deyi Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03788v1",
                "http://arxiv.org/pdf/2311.03788v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03783v1",
            "title": "Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI",
            "updated": "2023-11-07T08:06:27Z",
            "published": "2023-11-07T08:06:27Z",
            "summary": "Embodied AI is one of the most popular studies in artificial intelligence and\nrobotics, which can effectively improve the intelligence of real-world agents\n(i.e. robots) serving human beings. Scene knowledge is important for an agent\nto understand the surroundings and make correct decisions in the varied open\nworld. Currently, knowledge base for embodied tasks is missing and most\nexisting work use general knowledge base or pre-trained models to enhance the\nintelligence of an agent. For conventional knowledge base, it is sparse,\ninsufficient in capacity and cost in data collection. For pre-trained models,\nthey face the uncertainty of knowledge and hard maintenance. To overcome the\nchallenges of scene knowledge, we propose a scene-driven multimodal knowledge\ngraph (Scene-MMKG) construction method combining conventional knowledge\nengineering and large language models. A unified scene knowledge injection\nframework is introduced for knowledge representation. To evaluate the\nadvantages of our proposed method, we instantiate Scene-MMKG considering\ntypical indoor robotic functionalities (Manipulation and Mobility), named\nManipMob-MMKG. Comparisons in characteristics indicate our instantiated\nManipMob-MMKG has broad superiority in data-collection efficiency and knowledge\nquality. Experimental results on typical embodied tasks show that\nknowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the\nperformance obviously without re-designing model structures complexly. Our\nproject can be found at https://sites.google.com/view/manipmob-mmkg",
            "author": [
                "Song Yaoxian",
                "Sun Penglei",
                "Liu Haoyu",
                "Li Zhixu",
                "Song Wei",
                "Xiao Yanghua",
                "Zhou Xiaofang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03783v1",
                "http://arxiv.org/pdf/2311.03783v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.RO",
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03780v1",
            "title": "Ensembling Textual and Structure-Based Models for Knowledge Graph\n  Completion",
            "updated": "2023-11-07T07:53:06Z",
            "published": "2023-11-07T07:53:06Z",
            "summary": "We consider two popular approaches to Knowledge Graph Completion (KGC):\ntextual models that rely on textual entity descriptions, and structure-based\nmodels that exploit the connectivity structure of the Knowledge Graph (KG).\nPreliminary experiments show that these approaches have complementary\nstrengths: structure-based models perform well when the gold answer is easily\nreachable from the query head in the KG, while textual models exploit\ndescriptions to give good performance even when the gold answer is not\nreachable. In response, we explore ensembling as a way of combining the best of\nboth approaches. We propose a novel method for learning query-dependent\nensemble weights by using the distributions of scores assigned by individual\nmodels to all candidate entities. Our ensemble baseline achieves\nstate-of-the-art results on three standard KGC datasets, with up to 6.8 pt MRR\nand 8.3 pt Hits@1 gains over best individual models.",
            "author": [
                "Ananjan Nandi",
                "Navdeep Kaur",
                "Parag Singla",
                "Mausam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03780v1",
                "http://arxiv.org/pdf/2311.03780v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03778v1",
            "title": "Bridging the Information Gap Between Domain-Specific Model and General\n  LLM for Personalized Recommendation",
            "updated": "2023-11-07T07:40:09Z",
            "published": "2023-11-07T07:40:09Z",
            "summary": "Generative large language models(LLMs) are proficient in solving general\nproblems but often struggle to handle domain-specific tasks. This is because\nmost of domain-specific tasks, such as personalized recommendation, rely on\ntask-related information for optimal performance. Current methods attempt to\nsupplement task-related information to LLMs by designing appropriate prompts or\nemploying supervised fine-tuning techniques. Nevertheless, these methods\nencounter the certain issue that information such as community behavior pattern\nin RS domain is challenging to express in natural language, which limits the\ncapability of LLMs to surpass state-of-the-art domain-specific models. On the\nother hand, domain-specific models for personalized recommendation which mainly\nrely on user interactions are susceptible to data sparsity due to their limited\ncommon knowledge capabilities. To address these issues, we proposes a method to\nbridge the information gap between the domain-specific models and the general\nlarge language models. Specifically, we propose an information sharing module\nwhich serves as an information storage mechanism and also acts as a bridge for\ncollaborative training between the LLMs and domain-specific models. By doing\nso, we can improve the performance of LLM-based recommendation with the help of\nuser behavior pattern information mined by domain-specific models. On the other\nhand, the recommendation performance of domain-specific models can also be\nimproved with the help of common knowledge learned by LLMs. Experimental\nresults on three real-world datasets have demonstrated the effectiveness of the\nproposed method.",
            "author": [
                "Wenxuan Zhang",
                "Hongzhi Liu",
                "Yingpeng Du",
                "Chen Zhu",
                "Yang Song",
                "Hengshu Zhu",
                "Zhonghai Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03778v1",
                "http://arxiv.org/pdf/2311.03778v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03776v1",
            "title": "Filtered Partial Differential Equations: a robust surrogate constraint\n  in physics-informed deep learning framework",
            "updated": "2023-11-07T07:38:23Z",
            "published": "2023-11-07T07:38:23Z",
            "summary": "Embedding physical knowledge into neural network (NN) training has been a hot\ntopic. However, when facing the complex real-world, most of the existing\nmethods still strongly rely on the quantity and quality of observation data.\nFurthermore, the neural networks often struggle to converge when the solution\nto the real equation is very complex. Inspired by large eddy simulation in\ncomputational fluid dynamics, we propose an improved method based on filtering.\nWe analyzed the causes of the difficulties in physics informed machine\nlearning, and proposed a surrogate constraint (filtered PDE, FPDE in short) of\nthe original physical equations to reduce the influence of noisy and sparse\nobservation data. In the noise and sparsity experiment, the proposed FPDE\nmodels (which are optimized by FPDE constraints) have better robustness than\nthe conventional PDE models. Experiments demonstrate that the FPDE model can\nobtain the same quality solution with 100% higher noise and 12% quantity of\nobservation data of the baseline. Besides, two groups of real measurement data\nare used to show the FPDE improvements in real cases. The final results show\nthat FPDE still gives more physically reasonable solutions when facing the\nincomplete equation problem and the extremely sparse and high-noise conditions.\nFor combining real-world experiment data into physics-informed training, the\nproposed FPDE constraint is useful and performs well in two real-world\nexperiments: modeling the blood velocity in vessels and cell migration in\nscratches.",
            "author": [
                "Dashan Zhang",
                "Yuntian Chen",
                "Shiyi Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03776v1",
                "http://arxiv.org/pdf/2311.03776v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03756v1",
            "title": "Learning Decentralized Traffic Signal Controllers with Multi-Agent Graph\n  Reinforcement Learning",
            "updated": "2023-11-07T06:43:15Z",
            "published": "2023-11-07T06:43:15Z",
            "summary": "This paper considers optimal traffic signal control in smart cities, which\nhas been taken as a complex networked system control problem. Given the\ninteracting dynamics among traffic lights and road networks, attaining\ncontroller adaptivity and scalability stands out as a primary challenge.\nCapturing the spatial-temporal correlation among traffic lights under the\nframework of Multi-Agent Reinforcement Learning (MARL) is a promising solution.\nNevertheless, existing MARL algorithms ignore effective information aggregation\nwhich is fundamental for improving the learning capacity of decentralized\nagents. In this paper, we design a new decentralized control architecture with\nimproved environmental observability to capture the spatial-temporal\ncorrelation. Specifically, we first develop a topology-aware information\naggregation strategy to extract correlation-related information from\nunstructured data gathered in the road network. Particularly, we transfer the\nroad network topology into a graph shift operator by forming a diffusion\nprocess on the topology, which subsequently facilitates the construction of\ngraph signals. A diffusion convolution module is developed, forming a new MARL\nalgorithm, which endows agents with the capabilities of graph learning.\nExtensive experiments based on both synthetic and real-world datasets verify\nthat our proposal outperforms existing decentralized algorithms.",
            "author": [
                "Yao Zhang",
                "Zhiwen Yu",
                "Jun Zhang",
                "Liang Wang",
                "Tom H. Luan",
                "Bin Guo",
                "Chau Yuen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03756v1",
                "http://arxiv.org/pdf/2311.03756v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03748v1",
            "title": "Unified Low-Resource Sequence Labeling by Sample-Aware Dynamic Sparse\n  Finetuning",
            "updated": "2023-11-07T06:19:37Z",
            "published": "2023-11-07T06:19:37Z",
            "summary": "Unified Sequence Labeling that articulates different sequence labeling\nproblems such as Named Entity Recognition, Relation Extraction, Semantic Role\nLabeling, etc. in a generalized sequence-to-sequence format opens up the\nopportunity to make the maximum utilization of large language model knowledge\ntoward structured prediction. Unfortunately, this requires formatting them into\nspecialized augmented format unknown to the base pretrained language model\n(PLMs) necessitating finetuning to the target format. This significantly bounds\nits usefulness in data-limited settings where finetuning large models cannot\nproperly generalize to the target format. To address this challenge and\nleverage PLM knowledge effectively, we propose FISH-DIP, a sample-aware dynamic\nsparse finetuning strategy that selectively focuses on a fraction of\nparameters, informed by feedback from highly regressing examples, during the\nfine-tuning process. By leveraging the dynamism of sparsity, our approach\nmitigates the impact of well-learned samples and prioritizes underperforming\ninstances for improvement in generalization. Across five tasks of sequence\nlabeling, we demonstrate that FISH-DIP can smoothly optimize the model in low\nresource settings offering upto 40% performance improvements over full\nfine-tuning depending on target evaluation settings. Also, compared to\nin-context learning and other parameter-efficient fine-tuning approaches,\nFISH-DIP performs comparably or better, notably in extreme low-resource\nsettings.",
            "author": [
                "Sarkar Snigdha Sarathi Das",
                "Ranran Haoran Zhang",
                "Peng Shi",
                "Wenpeng Yin",
                "Rui Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03748v1",
                "http://arxiv.org/pdf/2311.03748v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03734v1",
            "title": "Leveraging Structured Information for Explainable Multi-hop Question\n  Answering and Reasoning",
            "updated": "2023-11-07T05:32:39Z",
            "published": "2023-11-07T05:32:39Z",
            "summary": "Neural models, including large language models (LLMs), achieve superior\nperformance on multi-hop question-answering. To elicit reasoning capabilities\nfrom LLMs, recent works propose using the chain-of-thought (CoT) mechanism to\ngenerate both the reasoning chain and the answer, which enhances the model's\ncapabilities in conducting multi-hop reasoning. However, several challenges\nstill remain: such as struggling with inaccurate reasoning, hallucinations, and\nlack of interpretability. On the other hand, information extraction (IE)\nidentifies entities, relations, and events grounded to the text. The extracted\nstructured information can be easily interpreted by humans and machines\n(Grishman, 2019). In this work, we investigate constructing and leveraging\nextracted semantic structures (graphs) for multi-hop question answering,\nespecially the reasoning process. Empirical results and human evaluations show\nthat our framework: generates more faithful reasoning chains and substantially\nimproves the QA performance on two benchmark datasets. Moreover, the extracted\nstructures themselves naturally provide grounded explanations that are\npreferred by humans, as compared to the generated reasoning chains and\nsaliency-based explanations.",
            "author": [
                "Ruosen Li",
                "Xinya Du"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03734v1",
                "http://arxiv.org/pdf/2311.03734v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03731v1",
            "title": "A Survey of Large Language Models Attribution",
            "updated": "2023-11-07T05:20:09Z",
            "published": "2023-11-07T05:20:09Z",
            "summary": "Open-domain generative systems have gained significant attention in the field\nof conversational AI (e.g., generative search engines). This paper presents a\ncomprehensive review of the attribution mechanisms employed by these systems,\nparticularly large language models. Though attribution or citation improve the\nfactuality and verifiability, issues like ambiguous knowledge reservoirs,\ninherent biases, and the drawbacks of excessive attribution can hinder the\neffectiveness of these systems. The aim of this survey is to provide valuable\ninsights for researchers, aiding in the refinement of attribution methodologies\nto enhance the reliability and veracity of responses generated by open-domain\ngenerative systems. We believe that this field is still in its early stages;\nhence, we maintain a repository to keep track of ongoing studies at\nhttps://github.com/HITsz-TMG/awesome-llm-attributions.",
            "author": [
                "Dongfang Li",
                "Zetian Sun",
                "Xinshuo Hu",
                "Zhenyu Liu",
                "Ziyang Chen",
                "Baotian Hu",
                "Aiguo Wu",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03731v1",
                "http://arxiv.org/pdf/2311.03731v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03730v1",
            "title": "Graphs and groups with unique geodesics",
            "updated": "2023-11-07T05:16:23Z",
            "published": "2023-11-07T05:16:23Z",
            "summary": "A graph is called \\emph{geodetic} if there is a unique geodesic between each\npair of vertices. Although natural objects, geodetic graphs remain mysterious\nand do not yet admit a satisfactory classification. In this paper we tackle the\ncase of infinite graphs. We prove that if a locally finite quasi-transitive\ngraph is geodetic, then it is quasi-isometric to a tree. From the group\ntheoretic perspective, this implies that a finitely generated group that admits\na geodetic Cayley graph is virtually free. Our main tool is to define a\nboundary and understand how local behaviour influences it. Our results unify,\nand represent significant progress on, programs of research initiated by Ore,\nShapiro, and Madlener and Otto.",
            "author": [
                "Murray Elder",
                "Giles Gardam",
                "Adam Piggott",
                "Davide Spriano",
                "Kane Townsend"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03730v1",
                "http://arxiv.org/pdf/2311.03730v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "05C75, 05C12, 20F65, 20F67, 68Q42"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03728v1",
            "title": "Stable manifolds for periodically perturbed maps",
            "updated": "2023-11-07T05:03:03Z",
            "published": "2023-11-07T05:03:03Z",
            "summary": "We prove that if a certain entry in the map of the Hadamard-Perron theorem is\n$T$-periodic in one of the variables, then the stable manifold guaranteed by\nthe Hadamard-Perron theorem is a graph of a $T$-periodic function. As an\napplication, we extend the classical Levinson's result about the occurrence of\nan attracting closed invariant curve near a stable cycle of a system of\nautonomous equations under periodic perturbations to hybrid differential\nequations.",
            "author": [
                "Matthew Williams",
                "Oleg Makarenkov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03728v1",
                "http://arxiv.org/pdf/2311.03728v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "37D10, 34A38"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03713v1",
            "title": "Multimodal deep representation learning for quantum cross-platform\n  verification",
            "updated": "2023-11-07T04:35:03Z",
            "published": "2023-11-07T04:35:03Z",
            "summary": "Cross-platform verification, a critical undertaking in the realm of\nearly-stage quantum computing, endeavors to characterize the similarity of two\nimperfect quantum devices executing identical algorithms, utilizing minimal\nmeasurements. While the random measurement approach has been instrumental in\nthis context, the quasi-exponential computational demand with increasing qubit\ncount hurdles its feasibility in large-qubit scenarios. To bridge this\nknowledge gap, here we introduce an innovative multimodal learning approach,\nrecognizing that the formalism of data in this task embodies two distinct\nmodalities: measurement outcomes and classical description of compiled circuits\non explored quantum devices, both enriched with unique information. Building\nupon this insight, we devise a multimodal neural network to independently\nextract knowledge from these modalities, followed by a fusion operation to\ncreate a comprehensive data representation. The learned representation can\neffectively characterize the similarity between the explored quantum devices\nwhen executing new quantum algorithms not present in the training data. We\nevaluate our proposal on platforms featuring diverse noise models, encompassing\nsystem sizes up to 50 qubits. The achieved results demonstrate a\nthree-orders-of-magnitude improvement in prediction accuracy compared to the\nrandom measurements and offer compelling evidence of the complementary roles\nplayed by each modality in cross-platform verification. These findings pave the\nway for harnessing the power of multimodal learning to overcome challenges in\nwider quantum system learning tasks.",
            "author": [
                "Yang Qian",
                "Yuxuan Du",
                "Zhenliang He",
                "Min-hsiu Hsieh",
                "Dacheng Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03713v1",
                "http://arxiv.org/pdf/2311.03713v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03706v1",
            "title": "Parallelized Conflict Graph Cut Generation",
            "updated": "2023-11-07T04:12:54Z",
            "published": "2023-11-07T04:12:54Z",
            "summary": "A conflict graph represents logical relations between binary variables, and\neffective use of the graph can significantly accelerate branch-and-cut solvers\nfor mixed-integer programming (MIP). In this paper we develop efficient\nparallel algorithms for the various components of conflict graph management:\nconflict detection; maximal clique generation; clique extension; and clique\nmerging. We leverage parallel computing in order to intensify computational\neffort on the conflict graph, thereby generating a much larger pool of cutting\nplanes than what can be practically achieved in serial. Computational\nexperiments demonstrate near-linear (i.e. ideal) speedups using up to 64 cores\nwhen there is high computational load from the conflict graph. Moreover, the\nexpanded pool of cuts enabled by parallel computing lead to substantial\nreductions in total MIP solve time, especially for more challenging cases.",
            "author": [
                "Yongzheng Dai",
                "Chen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03706v1",
                "http://arxiv.org/pdf/2311.03706v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "90C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03703v1",
            "title": "Pipeline Parallelism for DNN Inference with Practical Performance\n  Guarantees",
            "updated": "2023-11-07T03:55:39Z",
            "published": "2023-11-07T03:55:39Z",
            "summary": "We optimize pipeline parallelism for deep neural network (DNN) inference by\npartitioning model graphs into $k$ stages and minimizing the running time of\nthe bottleneck stage, including communication. We design practical algorithms\nfor this NP-hard problem and show that they are nearly optimal in practice by\ncomparing against strong lower bounds obtained via novel mixed-integer\nprogramming (MIP) formulations. We apply these algorithms and lower-bound\nmethods to production models to achieve substantially improved approximation\nguarantees compared to standard combinatorial lower bounds. For example,\nevaluated via geometric means across production data with $k=16$ pipeline\nstages, our MIP formulations more than double the lower bounds, improving the\napproximation ratio from $2.175$ to $1.058$. This work shows that while\nmax-throughput partitioning is theoretically hard, we have a handle on the\nalgorithmic side of the problem in practice and much of the remaining challenge\nis in developing more accurate cost models to feed into the partitioning\nalgorithms.",
            "author": [
                "Aaron Archer",
                "Matthew Fahrbach",
                "Kuikui Liu",
                "Prakash Prabhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03703v1",
                "http://arxiv.org/pdf/2311.03703v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03668v1",
            "title": "On arithmetical structures on K9",
            "updated": "2023-11-07T02:38:33Z",
            "published": "2023-11-07T02:38:33Z",
            "summary": "We study the arithmetical structures on the complete graph $K_9$. Our method\nis based on studying the solutions to writing the unit as a sum of 9 unit\nfractions. We work from the perspective of the Diophantine equation and use\nsome elementary properties on the $p$-adic valuations. The proofs are assisted\nby trees and automata.",
            "author": [
                "Claire Levaillant"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03668v1",
                "http://arxiv.org/pdf/2311.03668v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT",
                "math.CO",
                "11A67, 11B75, 11D85, 11D72, 05C05, 05C30",
                "D.1; E.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04245v1",
            "title": "GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks",
            "updated": "2023-11-07T02:36:24Z",
            "published": "2023-11-07T02:36:24Z",
            "summary": "In recent years, there has been a rapid development of spatio-temporal\nprediction techniques in response to the increasing demands of traffic\nmanagement and travel planning. While advanced end-to-end models have achieved\nnotable success in improving predictive performance, their integration and\nexpansion pose significant challenges. This work aims to address these\nchallenges by introducing a spatio-temporal pre-training framework that\nseamlessly integrates with downstream baselines and enhances their performance.\nThe framework is built upon two key designs: (i) We propose a spatio-temporal\nmask autoencoder as a pre-training model for learning spatio-temporal\ndependencies. The model incorporates customized parameter learners and\nhierarchical spatial pattern encoding networks. These modules are specifically\ndesigned to capture spatio-temporal customized representations and intra- and\ninter-cluster region semantic relationships, which have often been neglected in\nexisting approaches. (ii) We introduce an adaptive mask strategy as part of the\npre-training mechanism. This strategy guides the mask autoencoder in learning\nrobust spatio-temporal representations and facilitates the modeling of\ndifferent relationships, ranging from intra-cluster to inter-cluster, in an\neasy-to-hard training manner. Extensive experiments conducted on representative\nbenchmarks demonstrate the effectiveness of our proposed method. We have made\nour model implementation publicly available at https://github.com/HKUDS/GPT-ST.",
            "author": [
                "Zhonghang Li",
                "Lianghao Xia",
                "Yong Xu",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04245v1",
                "http://arxiv.org/pdf/2311.04245v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03665v1",
            "title": "Faster Algorithms for Cycle Hitting Problems on Disk Graphs",
            "updated": "2023-11-07T02:21:35Z",
            "published": "2023-11-07T02:21:35Z",
            "summary": "In this paper, we consider three hitting problems on a disk intersection\ngraph: Triangle Hitting Set, Feedback Vertex Set, and Odd Cycle Transversal.\nGiven a disk intersection graph $G$, our goal is to compute a set of vertices\nhitting all triangles, all cycles, or all odd cycles, respectively. Our\nalgorithms run in time $2^{\\tilde O(k^{4/5})}n^{O(1)}$, $2^{\\tilde\nO(k^{9/10})}n^{O(1)}$, and $2^{\\tilde O(k^{19/20})}n^{O(1)}$, respectively,\nwhere $n$ denotes the number of vertices of $G$. These do not require a\ngeometric representation of a disk graph. If a geometric representation of a\ndisk graph is given as input, we can solve these problems more efficiently. In\nthis way, we improve the algorithms for those three problem by Lokshtanov et\nal. [SODA 2022].",
            "author": [
                "Shinwoo An",
                "Kyungjin Cho",
                "Eunjin Oh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03665v1",
                "http://arxiv.org/pdf/2311.03665v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03661v1",
            "title": "Graph Neural Networks for Power Grid Operational Risk Assessment",
            "updated": "2023-11-07T02:16:50Z",
            "published": "2023-11-07T02:16:50Z",
            "summary": "In this article, the utility of graph neural network (GNN) surrogates for\nMonte Carlo (MC) sampling-based risk quantification in daily operations of\npower grid is investigated. The MC simulation process necessitates solving a\nlarge number of optimal power flow (OPF) problems corresponding to the sample\nvalues of stochastic grid variables (power demand and renewable generation),\nwhich is computationally prohibitive. Computationally inexpensive surrogates of\nthe OPF problem provide an attractive alternative for expedited MC simulation.\nGNN surrogates are especially suitable due to their superior ability to handle\ngraph-structured data. Therefore, GNN surrogates of OPF problem are trained\nusing supervised learning. They are then used to obtain Monte Carlo (MC)\nsamples of the quantities of interest (operating reserve, transmission line\nflow) given the (hours-ahead) probabilistic wind generation and load forecast.\nThe utility of GNN surrogates is evaluated by comparing OPF-based and GNN-based\ngrid reliability and risk for IEEE Case118 synthetic grid. It is shown that the\nGNN surrogates are sufficiently accurate for predicting the (bus-level,\nbranch-level and system-level) grid state and enable fast as well as accurate\noperational risk quantification for power grids. The article thus develops\nvarious tools for fast reliability and risk quantification for real-world power\ngrids using GNNs.",
            "author": [
                "Yadong Zhang",
                "Pranav M Karve",
                "Sankaran Mahadevan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03661v1",
                "http://arxiv.org/pdf/2311.03661v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03659v1",
            "title": "GNN-Based Beamforming for Sum-Rate Maximization in MU-MISO Networks",
            "updated": "2023-11-07T02:08:25Z",
            "published": "2023-11-07T02:08:25Z",
            "summary": "The advantages of graph neural networks (GNNs) in leveraging the graph\ntopology of wireless networks have drawn increasing attentions. This paper\nstudies the GNN-based learning approach for the sum-rate maximization in\nmultiple-user multiple-input single-output (MU-MISO) networks subject to the\nusers' individual data rate requirements and the power budget of the base\nstation. By modeling the MU-MISO network as a graph, a GNN-based architecture\nnamed CRGAT is proposed to directly map the channel state information to the\nbeamforming vectors. The attention-enabled aggregation and the\nresidual-assisted combination are adopted to enhance the learning capability\nand avoid the oversmoothing issue. Furthermore, a novel activation function is\nproposed for the constraint due to the limited power budget at the base\nstation. The CRGAT is trained in an unsupervised learning manner with two\nproposed loss functions. An evaluation method is proposed for the\nlearning-based approach, based on which the effectiveness of the proposed CRGAT\nis validated in comparison with several convex optimization and learning based\napproaches. Numerical results are provided to reveal the advantages of the\nCRGAT including the millisecond-level response with limited optimality\nperformance loss, the scalability to different number of users and power\nbudgets, and the adaptability to different system settings.",
            "author": [
                "Yuhang Li",
                "Yang Lu",
                "Bo Ai",
                "Octavia A. Dobre",
                "Zhiguo Ding",
                "Dusit Niyato"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03659v1",
                "http://arxiv.org/pdf/2311.03659v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04244v1",
            "title": "HKTGNN: Hierarchical Knowledge Transferable Graph Neural Network-based\n  Supply Chain Risk Assessment",
            "updated": "2023-11-07T00:54:04Z",
            "published": "2023-11-07T00:54:04Z",
            "summary": "The strength of a supply chain is an important measure of a country's or\nregion's technical advancement and overall competitiveness. Establishing supply\nchain risk assessment models for effective management and mitigation of\npotential risks has become increasingly crucial. As the number of businesses\ngrows, the important relationships become more complicated and difficult to\nmeasure. This emphasizes the need of extracting relevant information from graph\ndata. Previously, academics mostly employed knowledge inference to increase the\nvisibility of links between nodes in the supply chain. However, they have not\nsolved the data hunger problem of single node feature characteristics. We\npropose a hierarchical knowledge transferable graph neural network-based\n(HKTGNN) supply chain risk assessment model to address these issues. Our\napproach is based on current graph embedding methods for assessing corporate\ninvestment risk assessment. We embed the supply chain network corresponding to\nindividual goods in the supply chain using the graph embedding module,\nresulting in a directed homogeneous graph with just product nodes. This reduces\nthe complicated supply chain network into a basic product network. It addresses\ndifficulties using the domain difference knowledge transferable module based on\ncentrality, which is presented by the premise that supply chain feature\ncharacteristics may be biased in the actual world. Meanwhile, the feature\ncomplement and message passing will alleviate the data hunger problem, which is\ndriven by domain differences. Our model outperforms in experiments on a\nreal-world supply chain dataset. We will give an equation to prove that our\ncomparative experiment is both effective and fair.",
            "author": [
                "Zhanting Zhou",
                "Kejun Bi",
                "Yuyanzhen Zhong",
                "Chao Tang",
                "Dongfen Li",
                "Shi Ying",
                "Ruijin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04244v1",
                "http://arxiv.org/pdf/2311.04244v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03631v1",
            "title": "Novel data structures for label based queries specifically efficient for\n  billion+ property graph networks using Kinetica-Graph",
            "updated": "2023-11-07T00:38:04Z",
            "published": "2023-11-07T00:38:04Z",
            "summary": "This paper discusses a novel data structure that efficiently implements label\nbased graph queries particularly for very large graphs. The major issues in\nlarge graph databases is the memory foot-print of label based property\nassociations to graph entities and subsequent query speeds. To this end, unlike\nthe available graph databases, that use key-value pairs using map like\nassociative containers, we have devised a novel data structure that is superior\nin its memory foot-print as well as its fast search characteristics without any\ncompromise on the number of labels that can be associated to graph nodes and\nedges. We will demonstrate the power of this novel unconventional data\nstructure over billion plus graphs within the context.",
            "author": [
                "Bilge Kaan Karamete",
                "Eli Glaser"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03631v1",
                "http://arxiv.org/pdf/2311.03631v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03623v1",
            "title": "Stochastic convergence of regularized solutions for backward heat\n  conduction problems",
            "updated": "2023-11-07T00:18:34Z",
            "published": "2023-11-07T00:18:34Z",
            "summary": "In this paper, we study the stochastic convergence of regularized solutions\nfor backward heat conduction problems. These problems are recognized as\nill-posed due to the exponential decay of eigenvalues associated with the\nforward problems. We derive an error estimate for the least-squares regularized\nminimization problem within the framework of stochastic convergence. Our\nanalysis reveals that the optimal error of the Tikhonov-type least-squares\noptimization problem depends on the noise level, the number of sensors, and the\nunderlying ground truth. Moreover, we propose a self-adaptive algorithm to\nidentify the optimal regularization parameter for the optimization problem\nwithout requiring knowledge of the noise level or any other prior information,\nwhich will be very practical in applications. We present numerical examples to\ndemonstrate the accuracy and efficiency of our proposed method. These numerical\nresults show that our method is efficient in solving backward heat conduction\nproblems.",
            "author": [
                "Zhongjian Wang",
                "Wenlong Zhang",
                "Zhiwen Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03623v1",
                "http://arxiv.org/pdf/2311.03623v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03608v1",
            "title": "Implicit Knowledge in Unawareness Structures",
            "updated": "2023-11-06T23:26:21Z",
            "published": "2023-11-06T23:26:21Z",
            "summary": "Awareness structures by Fagin and Halpern (1988) (FH) feature a syntactic\nawareness correspondence and accessibility relations modeling implicit\nknowledge. They are a flexible model of unawareness, and best interpreted from\na outside modeler's perspective. Unawareness structures by Heifetz, Meier, and\nSchipper (2006, 2008) (HMS) model awareness by a lattice of state spaces and\nexplicit knowledge via possibility correspondences. Sublattices thereof can be\ninterpreted as subjective views of agents. Open questions include (1) how\nimplicit knowledge can be defined in HMS structures, and (2) in which way FH\nstructures can be extended to model the agents' subjective views. In this\npaper, we address (1) by defining implicit knowledge such that it is consistent\nwith explicit knowledge in HMS models. We also introduce a variant of HMS\nmodels that instead of explicit knowledge, takes implicit knowledge and\nawareness as primitives. Further, we address (2) by introducing a category of\nFH models that are modally equivalent relative to sublanguages and can be\ninterpreted as agents' subjective views depending on their awareness. These\nconstructions allow us to show an equivalence between HMS and FH models. As a\ncorollary, we obtain soundness and completeness of HMS models with respect to\nthe Logic of Propositional Awareness, based on a language featuring both\nimplicit and explicit knowledge.",
            "author": [
                "Gaia Belardinelli",
                "Burkhard C. Schipper"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03608v1",
                "http://arxiv.org/pdf/2311.03608v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.GT",
                "F.4.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03606v1",
            "title": "Multimodal Stress Detection Using Facial Landmarks and Biometric Signals",
            "updated": "2023-11-06T23:20:30Z",
            "published": "2023-11-06T23:20:30Z",
            "summary": "The development of various sensing technologies is improving measurements of\nstress and the well-being of individuals. Although progress has been made with\nsingle signal modalities like wearables and facial emotion recognition,\nintegrating multiple modalities provides a more comprehensive understanding of\nstress, given that stress manifests differently across different people.\nMulti-modal learning aims to capitalize on the strength of each modality rather\nthan relying on a single signal. Given the complexity of processing and\nintegrating high-dimensional data from limited subjects, more research is\nneeded. Numerous research efforts have been focused on fusing stress and\nemotion signals at an early stage, e.g., feature-level fusion using basic\nmachine learning methods and 1D-CNN Methods. This paper proposes a multi-modal\nlearning approach for stress detection that integrates facial landmarks and\nbiometric signals. We test this multi-modal integration with various\nearly-fusion and late-fusion techniques to integrate the 1D-CNN model from\nbiometric signals and 2-D CNN using facial landmarks. We evaluate these\narchitectures using a rigorous test of models' generalizability using the\nleave-one-subject-out mechanism, i.e., all samples related to a single subject\nare left out to train the model. Our findings show that late-fusion achieved\n94.39\\% accuracy, and early-fusion surpassed it with a 98.38\\% accuracy rate.\nThis research contributes valuable insights into enhancing stress detection\nthrough a multi-modal approach. The proposed research offers important\nknowledge in improving stress detection using a multi-modal approach.",
            "author": [
                "Majid Hosseini",
                "Morteza Bodaghi",
                "Ravi Teja Bhupatiraju",
                "Anthony Maida",
                "Raju Gottumukkala"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03606v1",
                "http://arxiv.org/pdf/2311.03606v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03603v1",
            "title": "The steady state of the boundary-driven multiparticle asymmetric\n  diffusion model",
            "updated": "2023-11-06T23:15:52Z",
            "published": "2023-11-06T23:15:52Z",
            "summary": "We consider the multiparticle asymmetric diffusion model (MADM) introduced by\nSasamoto and Wadati with integrability preserving reservoirs at the boundaries.\nIn contrast to the open asymmetric simple exclusion process (ASEP) the number\nof particles allowed per site is unbounded in the MADM. Taking inspiration from\nthe stationary measure in the symmetric case, i.e. the rational limit, we first\nobtain the length 1 solution and then show that the steady state can be\nexpressed as an iterated product of Jackson q-integrals. In the proof of the\nstationarity condition, we observe a cancellation mechanism that closely\nresembles the one of the matrix product ansatz. To our knowledge, the\noccupation probabilities in the steady state of the boundary-driven MADM were\nnot available before.",
            "author": [
                "Rouven Frassek",
                "Istv\u00e1n M. Sz\u00e9cs\u00e9nyi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03603v1",
                "http://arxiv.org/pdf/2311.03603v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "cond-mat.stat-mech",
                "hep-th",
                "math.MP",
                "math.PR",
                "nlin.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03593v1",
            "title": "Identifying Markov chain models from time-to-event data: an algebraic\n  approach",
            "updated": "2023-11-06T22:52:30Z",
            "published": "2023-11-06T22:52:30Z",
            "summary": "In many fields, including biology, medicine, physics, chemistry, economy and\nactuarial science, data can be represented as the time-to-event of a finite\nstate Markov chain model. The distribution of time intervals between successive\nrecorded events is known as a phase-type distribution. We demonstrate that in\ncases where the eigenvalues of the Markov chain transition matrix are distinct\n(non-degenerate), the phase-type distribution is multi-exponential. We then\npose and solve an inverse problem: given knowledge of the phase-type\ndistribution, can we determine the transition rate parameters of the underlying\nMarkov chain? To tackle this challenge, we initially convert the inverse\nproblem into a computer algebraic task, involving the solution of a system of\npolynomial equations. These equations are symmetrized with respect to the\nparameters of the phase-type distribution. For a specific subset of Markov\nmodels that we refer to as \"solvable,\" the inverse problem yields a unique\nsolution, up to transformations by finite symmetries. We outline a recursive\napproach to compute these solutions for specific families of models, regardless\nof their number of states. Additionally, we use the Thomas decomposition\ntechnique to calculate solutions for models possessing 2, 3, or 4 states.\nInterestingly, we show that models having the same number of states but\ndifferent transition graphs can yield identical phase-distributions. In such\n\"Rashomon effect\" scenarios, time-to-event data permits the formulation of\nmultiple models and interpretations, all of which are consistent with the\nobserved experimental data. To differentiate among these models, we propose\nadditional distinguishing properties beyond just the time until the next event.\nIn order to prove its applicability, we discuss how this method can be used to\ninfer models of transcription regulation from transcriptional bursting data.",
            "author": [
                "Ovidiu Radulescu",
                "Dima Grigoriev",
                "Matthias Seiss",
                "Maria Douaihy",
                "Mounia Lagha",
                "Edouard Bertrand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03593v1",
                "http://arxiv.org/pdf/2311.03593v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "q-bio.QM",
                "60J28"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03583v1",
            "title": "Finding Increasingly Large Extremal Graphs with AlphaZero and Tabu\n  Search",
            "updated": "2023-11-06T22:29:55Z",
            "published": "2023-11-06T22:29:55Z",
            "summary": "This work studies a central extremal graph theory problem inspired by a 1975\nconjecture of Erd\\H{o}s, which aims to find graphs with a given size (number of\nnodes) that maximize the number of edges without having 3- or 4-cycles. We\nformulate this problem as a sequential decision-making problem and compare\nAlphaZero, a neural network-guided tree search, with tabu search, a heuristic\nlocal search method. Using either method, by introducing a curriculum --\njump-starting the search for larger graphs using good graphs found at smaller\nsizes -- we improve the state-of-the-art lower bounds for several sizes. We\nalso propose a flexible graph-generation environment and a\npermutation-invariant network architecture for learning to search in the space\nof graphs.",
            "author": [
                "Abbas Mehrabian",
                "Ankit Anand",
                "Hyunjik Kim",
                "Nicolas Sonnerat",
                "Matej Balog",
                "Gheorghe Comanici",
                "Tudor Berariu",
                "Andrew Lee",
                "Anian Ruoss",
                "Anna Bulanova",
                "Daniel Toyama",
                "Sam Blackwell",
                "Bernardino Romera Paredes",
                "Petar Veli\u010dkovi\u0107",
                "Laurent Orseau",
                "Joonkyung Lee",
                "Anurag Murty Naredla",
                "Doina Precup",
                "Adam Zsolt Wagner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03583v1",
                "http://arxiv.org/pdf/2311.03583v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03565v1",
            "title": "MIRAGE: Multi-Binary Image Risk Assessment with Attack Graph Employment",
            "updated": "2023-11-06T22:07:04Z",
            "published": "2023-11-06T22:07:04Z",
            "summary": "Attackers can exploit known vulnerabilities to infiltrate a device's firmware\nand the communication between firmware binaries, in order to pass between them.\nTo improve cybersecurity, organizations must identify and mitigate the risks of\nthe firmware they use. An attack graph (AG) can be used to assess and visually\ndisplay firmware's risks by organizing the identified vulnerabilities into\nattack paths composed of sequences of actions attackers may perform to\ncompromise firmware images. In this paper, we utilize AGs for firmware risk\nassessment. We propose MIRAGE (Multi-binary Image Risk Assessment with Attack\nGraph Employment), a framework for identifying potential attack vectors and\nvulnerable interactions between firmware binaries; MIRAGE accomplishes this by\ngenerating AGs for firmware inter-binary communication. The use cases of the\nproposed firmware AG generation framework include the identification of risky\nexternal interactions, supply chain risk assessment, and security analysis with\ndigital twins. To evaluate the MIRAGE framework, we collected a dataset of 703\nfirmware images. We also propose a model for examining the risks of firmware\nbinaries, demonstrate the model's implementation on the dataset of firmware\nimages, and list the riskiest binaries.",
            "author": [
                "David Tayouri",
                "Telem Nachum",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03565v1",
                "http://arxiv.org/pdf/2311.03565v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03559v1",
            "title": "Algebraic Conditions on One-Step Breadth-First Search",
            "updated": "2023-11-06T22:01:15Z",
            "published": "2023-11-06T22:01:15Z",
            "summary": "The GraphBLAS community has demonstrated the power of linear\nalgebra-leveraged graph algorithms, such as matrix-vector products for\nbreadth-first search (BFS) traversals. This paper investigates the algebraic\nconditions needed for such computations when working with directed hypergraphs,\nrepresented by incidence arrays with entries from an arbitrary value set with\nbinary addition and multiplication operations. Our results show the one-step\nBFS traversal is equivalent to requiring specific algebraic properties of those\noperations. Assuming identity elements 0, 1 for operations, we show that the\ntwo operations must be zero-sum-free, zero-divisor-free, and 0 must be an\nannihilator under multiplication. Additionally, associativity and commutativity\nare shown to be necessary and sufficient for independence of the one-step BFS\ncomputation from several arbitrary conventions. These results aid in\napplication and algorithm development by determining the efficacy of a value\nset in computations.",
            "author": [
                "Emma Fu",
                "Hayden Jananthan",
                "Jeremy Kepner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03559v1",
                "http://arxiv.org/pdf/2311.03559v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03550v1",
            "title": "United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure\n  Learning from Videos",
            "updated": "2023-11-06T21:33:56Z",
            "published": "2023-11-06T21:33:56Z",
            "summary": "Given multiple videos of the same task, procedure learning addresses\nidentifying the key-steps and determining their order to perform the task. For\nthis purpose, existing approaches use the signal generated from a pair of\nvideos. This makes key-steps discovery challenging as the algorithms lack\ninter-videos perspective. Instead, we propose an unsupervised Graph-based\nProcedure Learning (GPL) framework. GPL consists of the novel UnityGraph that\nrepresents all the videos of a task as a graph to obtain both intra-video and\ninter-videos context. Further, to obtain similar embeddings for the same\nkey-steps, the embeddings of UnityGraph are updated in an unsupervised manner\nusing the Node2Vec algorithm. Finally, to identify the key-steps, we cluster\nthe embeddings using KMeans. We test GPL on benchmark ProceL, CrossTask, and\nEgoProceL datasets and achieve an average improvement of 2% on third-person\ndatasets and 3.6% on EgoProceL over the state-of-the-art.",
            "author": [
                "Siddhant Bansal",
                "Chetan Arora",
                "C. V. Jawahar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03550v1",
                "http://arxiv.org/pdf/2311.03550v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03547v1",
            "title": "InterVLS: Interactive Model Understanding and Improvement with\n  Vision-Language Surrogates",
            "updated": "2023-11-06T21:30:59Z",
            "published": "2023-11-06T21:30:59Z",
            "summary": "Deep learning models are widely used in critical applications, highlighting\nthe need for pre-deployment model understanding and improvement. Visual\nconcept-based methods, while increasingly used for this purpose, face\nchallenges: (1) most concepts lack interpretability, (2) existing methods\nrequire model knowledge, often unavailable at run time. Additionally, (3) there\nlacks a no-code method for post-understanding model improvement. Addressing\nthese, we present InterVLS. The system facilitates model understanding by\ndiscovering text-aligned concepts, measuring their influence with\nmodel-agnostic linear surrogates. Employing visual analytics, InterVLS offers\nconcept-based explanations and performance insights. It enables users to adjust\nconcept influences to update a model, facilitating no-code model improvement.\nWe evaluate InterVLS in a user study, illustrating its functionality with two\nscenarios. Results indicates that InterVLS is effective to help users identify\ninfluential concepts to a model, gain insights and adjust concept influence to\nimprove the model. We conclude with a discussion based on our study results.",
            "author": [
                "Jinbin Huang",
                "Wenbin He",
                "Liang Gou",
                "Liu Ren",
                "Chris Bryan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03547v1",
                "http://arxiv.org/pdf/2311.03547v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03543v1",
            "title": "Enabling Dynamic Selection of Implementation Variants in Component-Based\n  Parallel Programming for Heterogeneous Systems",
            "updated": "2023-11-06T21:28:20Z",
            "published": "2023-11-06T21:28:20Z",
            "summary": "Heterogeneous systems, consisting of CPUs and GPUs, offer the capability to\naddress the demands of compute- and data-intensive applications. However,\nprogramming such systems is challenging, requiring knowledge of various\nparallel programming frameworks. This paper introduces COMPAR, a\ncomponent-based parallel programming framework that enables the exposure and\nselection of multiple implementation variants of components at runtime. The\nframework leverages compiler directive-based language extensions to annotate\nthe source code and generate the necessary glue code for the StarPU runtime\nsystem. COMPAR provides a unified view of implementation variants and allows\nfor intelligent selection based on runtime context. Our evaluation demonstrates\nthe effectiveness of COMPAR through benchmark applications. The proposed\napproach simplifies heterogeneous parallel programming and promotes code reuse\nwhile achieving optimal performance.",
            "author": [
                "Suejb Memeti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03543v1",
                "http://arxiv.org/pdf/2311.03543v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03542v1",
            "title": "Indexing Techniques for Graph Reachability Queries",
            "updated": "2023-11-06T21:27:32Z",
            "published": "2023-11-06T21:27:32Z",
            "summary": "We survey graph reachability indexing techniques for efficient processing of\ngraph reachability queries in two types of popular graph models: plain graphs\nand edge-labeled graphs. Reachability queries are fundamental in graph\nprocessing, and reachability indexes are specialized data structures tailored\nfor speeding up such queries. Work on this topic goes back four decades -- we\ninclude 33 of the proposed techniques. Plain graphs contain only vertices and\nedges, with reachability queries checking path existence between a source and\ntarget vertex. Edge-labeled graphs, in contrast, augment plain graphs by adding\nedge labels. Reachability queries in edge-labeled graphs incorporate path\nconstraints based on edge labels, assessing both path existence and compliance\nwith constraints.\n  We categorize techniques in both plain and edge-labeled graphs and discuss\nthe approaches according to this classification, using existing techniques as\nexemplars. We discuss the main challenges within each class and how these might\nbe addressed in other approaches. We conclude with a discussion of the open\nresearch challenges and future research directions, along the lines of\nintegrating reachability indexes into graph data management systems. This\nsurvey serves as a comprehensive resource for researchers and practitioners\ninterested in the advancements, techniques, and challenges on reachability\nindexing in graph analytics.",
            "author": [
                "Chao Zhang",
                "Angela Bonifati",
                "M. Tamer \u00d6zsu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03542v1",
                "http://arxiv.org/pdf/2311.03542v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03524v1",
            "title": "A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised\n  Learning",
            "updated": "2023-11-06T21:15:09Z",
            "published": "2023-11-06T21:15:09Z",
            "summary": "Open-world semi-supervised learning aims at inferring both known and novel\nclasses in unlabeled data, by harnessing prior knowledge from a labeled set\nwith known classes. Despite its importance, there is a lack of theoretical\nfoundations for this problem. This paper bridges the gap by formalizing a\ngraph-theoretic framework tailored for the open-world setting, where the\nclustering can be theoretically characterized by graph factorization. Our\ngraph-theoretic framework illuminates practical algorithms and provides\nguarantees. In particular, based on our graph formulation, we apply the\nalgorithm called Spectral Open-world Representation Learning (SORL), and show\nthat minimizing our loss is equivalent to performing spectral decomposition on\nthe graph. Such equivalence allows us to derive a provable error bound on the\nclustering performance for both known and novel classes, and analyze rigorously\nwhen labeled data helps. Empirically, SORL can match or outperform several\nstrong baselines on common benchmark datasets, which is appealing for practical\nusage while enjoying theoretical guarantees.",
            "author": [
                "Yiyou Sun",
                "Zhenmei Shi",
                "Yixuan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03524v1",
                "http://arxiv.org/pdf/2311.03524v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03520v1",
            "title": "Brain Networks and Intelligence: A Graph Neural Network Based Approach\n  to Resting State fMRI Data",
            "updated": "2023-11-06T20:58:07Z",
            "published": "2023-11-06T20:58:07Z",
            "summary": "Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful\ntool for investigating the relationship between brain function and cognitive\nprocesses as it allows for the functional organization of the brain to be\ncaptured without relying on a specific task or stimuli. In this paper, we\npresent a novel modeling architecture called BrainRGIN for predicting\nintelligence (fluid, crystallized, and total intelligence) using graph neural\nnetworks on rsfMRI derived static functional network connectivity matrices.\nExtending from the existing graph convolution networks, our approach\nincorporates a clustering-based embedding and graph isomorphism network in the\ngraph convolutional layer to reflect the nature of the brain sub-network\norganization and efficient network expression, in combination with TopK pooling\nand attention-based readout functions. We evaluated our proposed architecture\non a large dataset, specifically the Adolescent Brain Cognitive Development\nDataset, and demonstrated its effectiveness in predicting individual\ndifferences in intelligence. Our model achieved lower mean squared errors and\nhigher correlation scores than existing relevant graph architectures and other\ntraditional machine learning models for all of the intelligence prediction\ntasks. The middle frontal gyrus exhibited a significant contribution to both\nfluid and crystallized intelligence, suggesting their pivotal role in these\ncognitive processes. Total composite scores identified a diverse set of brain\nregions to be relevant which underscores the complex nature of total\nintelligence.",
            "author": [
                "Bishal Thapaliya",
                "Esra Akbas",
                "Jiayu Chen",
                "Raam Sapkota",
                "Bhaskar Ray",
                "Pranav Suresh",
                "Vince Calhoun",
                "Jingyu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03520v1",
                "http://arxiv.org/pdf/2311.03520v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03510v1",
            "title": "Spoken Dialogue System for Medical Prescription Acquisition on\n  Smartphone: Development, Corpus and Evaluation",
            "updated": "2023-11-06T20:36:55Z",
            "published": "2023-11-06T20:36:55Z",
            "summary": "Hospital information systems (HIS) have become an essential part of\nhealthcare institutions and now incorporate prescribing support software.\nPrescription support software allows for structured information capture, which\nimproves the safety, appropriateness and efficiency of prescriptions and\nreduces the number of adverse drug events (ADEs). However, such a system\nincreases the amount of time physicians spend at a computer entering\ninformation instead of providing medical care. In addition, any new visiting\nclinician must learn to manage complex interfaces since each HIS has its own\ninterfaces. In this paper, we present a natural language interface for\ne-prescribing software in the form of a spoken dialogue system accessible on a\nsmartphone. This system allows prescribers to record their prescriptions\nverbally, a form of interaction closer to their usual practice. The system\nextracts the formal representation of the prescription ready to be checked by\nthe prescribing software and uses the dialogue to request mandatory\ninformation, correct errors or warn of particular situations. Since, to the\nbest of our knowledge, there is no existing voice-based prescription dialogue\nsystem, we present the system developed in a low-resource environment, focusing\non dialogue modeling, semantic extraction and data augmentation. The system was\nevaluated in the wild with 55 participants. This evaluation showed that our\nsystem has an average prescription time of 66.15 seconds for physicians and\n35.64 seconds for other experts, and a task success rate of 76\\% for physicians\nand 72\\% for other experts. All evaluation data were recorded and annotated to\nform PxCorpus, the first spoken drug prescription corpus that has been made\nfully available to the community\n(\\url{https://doi.org/10.5281/zenodo.6524162}).",
            "author": [
                "Ali Can Kocabiyikoglu",
                "Fran\u00e7ois Portet",
                "Jean-Marc Babouchkine",
                "Prudence Gibert",
                "Herv\u00e9 Blanchon",
                "Ga\u00ebtan Gavazzi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03510v1",
                "http://arxiv.org/pdf/2311.03510v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03496v1",
            "title": "Asynchronous Local Computations in Distributed Bayesian Learning",
            "updated": "2023-11-06T20:11:41Z",
            "published": "2023-11-06T20:11:41Z",
            "summary": "Due to the expanding scope of machine learning (ML) to the fields of sensor\nnetworking, cooperative robotics and many other multi-agent systems,\ndistributed deployment of inference algorithms has received a lot of attention.\nThese algorithms involve collaboratively learning unknown parameters from\ndispersed data collected by multiple agents. There are two competing aspects in\nsuch algorithms, namely, intra-agent computation and inter-agent communication.\nTraditionally, algorithms are designed to perform both synchronously. However,\ncertain circumstances need frugal use of communication channels as they are\neither unreliable, time-consuming, or resource-expensive. In this paper, we\npropose gossip-based asynchronous communication to leverage fast computations\nand reduce communication overhead simultaneously. We analyze the effects of\nmultiple (local) intra-agent computations by the active agents between\nsuccessive inter-agent communications. For local computations, Bayesian\nsampling via unadjusted Langevin algorithm (ULA) MCMC is utilized. The\ncommunication is assumed to be over a connected graph (e.g., as in\ndecentralized learning), however, the results can be extended to coordinated\ncommunication where there is a central server (e.g., federated learning). We\ntheoretically quantify the convergence rates in the process. To demonstrate the\nefficacy of the proposed algorithm, we present simulations on a toy problem as\nwell as on real world data sets to train ML models to perform classification\ntasks. We observe faster initial convergence and improved performance accuracy,\nespecially in the low data range. We achieve on average 78% and over 90%\nclassification accuracy respectively on the Gamma Telescope and mHealth data\nsets from the UCI ML repository.",
            "author": [
                "Kinjal Bhar",
                "He Bai",
                "Jemin George",
                "Carl Busart"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03496v1",
                "http://arxiv.org/pdf/2311.03496v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03475v1",
            "title": "Fluid limit of a model for distributed ledger with random delay",
            "updated": "2023-11-06T19:26:42Z",
            "published": "2023-11-06T19:26:42Z",
            "summary": "Blockchain and other decentralized databases, known as distributed ledgers,\nare designed to store information online where all trusted network members can\nupdate the data with transparency. The dynamics of ledger's development can be\nmathematically represented by a directed acyclic graph (DAG). In this paper, we\nstudy a DAG model which considers batch arrivals and random delay of\nattachment. We analyze the asymptotic behavior of this model by letting the\narrival rate goes to infinity and the inter arrival time goes to zero. We\nestablish that the number of leaves in the DAG and various random variables\ncharacterizing the vertices in the DAG can be approximated by its fluid limit,\nrepresented as delayed partial differential equations. Furthermore, we\nestablish the stable state of this fluid limit and validate our findings\nthrough simulations.",
            "author": [
                "Jiewei Feng",
                "Christopher King"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03475v1",
                "http://arxiv.org/pdf/2311.03475v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03456v1",
            "title": "A Topological Data Analysis of the CHIME/FRB Catalogues",
            "updated": "2023-11-06T19:00:29Z",
            "published": "2023-11-06T19:00:29Z",
            "summary": "In this paper, we use Topological Data Analysis (TDA), a mathematical\napproach for studying data shape, to analyse Fast Radio Bursts (FRBs). Applying\nthe Mapper algorithm, we visualise the topological structure of a large FRB\nsample. Our findings reveal three distinct FRB populations based on their\ninferred source properties, and show a robust structure indicating their\nmorphology and energy. We also identify potential non-repeating FRBs that might\nbecome repeaters based on proximity in the Mapper graph. This work showcases\nTDA's promise in unraveling the origin and nature of FRBs.",
            "author": [
                "Shruti Bhatporia",
                "Anthony Walters",
                "Jeff Murugan",
                "Amanda Weltman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03456v1",
                "http://arxiv.org/pdf/2311.03456v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03342v1",
            "title": "A Classification of Graphs through Quadratic Embedding Constants and\n  Clique Graph Insights",
            "updated": "2023-11-06T18:49:05Z",
            "published": "2023-11-06T18:49:05Z",
            "summary": "The quadratic embedding constant (QE constant for short) of a graph $G$ is a\nnew numeric invariant, which is defined in terms of the distance matrix and is\ndenoted by $\\mathrm{QEC}(G)$. By observing graph structure of the maximal\ncliques (clique graph), we show that a graph $G$ with $\\mathrm{QEC}(G)<-1/2$\nadmits a \"cactus-like\" structure. We derive a formula for the QE constant of a\ngraph consisting of two maximal cliques. As an application we discuss\ncharacterization of graphs along the increasing sequence of\n$\\mathrm{QEC}(P_d)$, in particular, graphs $G$ satisfying\n$\\mathrm{QEC}(G)<\\mathrm{QEC}(P_5)$.",
            "author": [
                "Edy Tri Baskoro",
                "Nobuaki Obata"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03342v1",
                "http://arxiv.org/pdf/2311.03342v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "primary 05C50, secondary 05C12, 05C76"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03341v2",
            "title": "On polynomial degree-boundedness",
            "updated": "2023-11-27T06:10:13Z",
            "published": "2023-11-06T18:47:44Z",
            "summary": "We prove a conjecture of Bonamy, Bousquet, Pilipczuk, Rz\\k{a}\\.zewski,\nThomass\\'e, and Walczak, that for every graph $H$, there is a polynomial $p$\nsuch that for every positive integer $s$, every graph of average degree at\nleast $p(s)$ contains either $K_{s,s}$ as a subgraph or contains an induced\nsubdivision of $H$. This improves upon a result of K\\\"uhn and Osthus from 2004\nwho proved it for graphs whose average degree is at least triply exponential in\n$s$ and a recent result of Du, Gir\\~{a}o, Hunter, McCarty and Scott for graphs\nwith average degree at least singly exponential in $s$. As an application, we\nprove that the class of graphs that do not contain an induced subdivision of\n$K_{s,t}$ is polynomially $\\chi$-bounded. In the case of $K_{2,3}$, this is the\nclass of theta-free graphs, and answers a question of Davies. Along the way, we\nalso answer a recent question of McCarty, by showing that if $\\mathcal{G}$ is a\nhereditary class of graphs for which there is a polynomial $p$ such that every\nbipartite $K_{s,s}$-free graph in $\\mathcal{G}$ has average degree at most\n$p(s)$, then more generally, there is a polynomial $p'$ such that every\n$K_{s,s}$-free graph in $\\mathcal{G}$ has average degree at most $p'(s)$. Our\nmain new tool is an induced variant of the K\\H{o}v\\'ari-S\\'os-Tur\\'an theorem,\nwhich we find to be of independent interest.",
            "author": [
                "Romain Bourneuf",
                "Matija Buci\u0107",
                "Linda Cook",
                "James Davies"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03341v2",
                "http://arxiv.org/pdf/2311.03341v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03340v2",
            "title": "Multitask Kernel-based Learning with First-Order Logic Constraints",
            "updated": "2023-11-08T18:03:48Z",
            "published": "2023-11-06T18:44:55Z",
            "summary": "In this paper we propose a general framework to integrate supervised and\nunsupervised examples with background knowledge expressed by a collection of\nfirst-order logic clauses into kernel machines. In particular, we consider a\nmulti-task learning scheme where multiple predicates defined on a set of\nobjects are to be jointly learned from examples, enforcing a set of FOL\nconstraints on the admissible configurations of their values. The predicates\nare defined on the feature spaces, in which the input objects are represented,\nand can be either known a priori or approximated by an appropriate kernel-based\nlearner. A general approach is presented to convert the FOL clauses into a\ncontinuous implementation that can deal with the outputs computed by the\nkernel-based predicates. The learning problem is formulated as a\nsemi-supervised task that requires the optimization in the primal of a loss\nfunction that combines a fitting loss measure on the supervised examples, a\nregularization term, and a penalty term that enforces the constraints on both\nthe supervised and unsupervised examples. Unfortunately, the penalty term is\nnot convex and it can hinder the optimization process. However, it is possible\nto avoid poor solutions by using a two stage learning schema, in which the\nsupervised examples are learned first and then the constraints are enforced.",
            "author": [
                "Michelangelo Diligenti",
                "Marco Gori",
                "Marco Maggini",
                "Leonardo Rigutini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03340v2",
                "http://arxiv.org/pdf/2311.03340v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03335v1",
            "title": "Cross-Image Attention for Zero-Shot Appearance Transfer",
            "updated": "2023-11-06T18:33:24Z",
            "published": "2023-11-06T18:33:24Z",
            "summary": "Recent advancements in text-to-image generative models have demonstrated a\nremarkable ability to capture a deep semantic understanding of images. In this\nwork, we leverage this semantic knowledge to transfer the visual appearance\nbetween objects that share similar semantics but may differ significantly in\nshape. To achieve this, we build upon the self-attention layers of these\ngenerative models and introduce a cross-image attention mechanism that\nimplicitly establishes semantic correspondences across images. Specifically,\ngiven a pair of images -- one depicting the target structure and the other\nspecifying the desired appearance -- our cross-image attention combines the\nqueries corresponding to the structure image with the keys and values of the\nappearance image. This operation, when applied during the denoising process,\nleverages the established semantic correspondences to generate an image\ncombining the desired structure and appearance. In addition, to improve the\noutput image quality, we harness three mechanisms that either manipulate the\nnoisy latent codes or the model's internal representations throughout the\ndenoising process. Importantly, our approach is zero-shot, requiring no\noptimization or training. Experiments show that our method is effective across\na wide range of object categories and is robust to variations in shape, size,\nand viewpoint between the two input images.",
            "author": [
                "Yuval Alaluf",
                "Daniel Garibi",
                "Or Patashnik",
                "Hadar Averbuch-Elor",
                "Daniel Cohen-Or"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03335v1",
                "http://arxiv.org/pdf/2311.03335v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03332v1",
            "title": "Learning Hard-Constrained Models with One Sample",
            "updated": "2023-11-06T18:29:57Z",
            "published": "2023-11-06T18:29:57Z",
            "summary": "We consider the problem of estimating the parameters of a Markov Random Field\nwith hard-constraints using a single sample. As our main running examples, we\nuse the $k$-SAT and the proper coloring models, as well as general $H$-coloring\nmodels; for all of these we obtain both positive and negative results. In\ncontrast to the soft-constrained case, we show in particular that single-sample\nestimation is not always possible, and that the existence of an estimator is\nrelated to the existence of non-satisfiable instances.\n  Our algorithms are based on the pseudo-likelihood estimator. We show variance\nbounds for this estimator using coupling techniques inspired, in the case of\n$k$-SAT, by Moitra's sampling algorithm (JACM, 2019); our positive results for\ncolorings build on this new coupling approach. For $q$-colorings on graphs with\nmaximum degree $d$, we give a linear-time estimator when $q>d+1$, whereas the\nproblem is non-identifiable when $q\\leq d+1$. For general $H$-colorings, we\nshow that standard conditions that guarantee sampling, such as Dobrushin's\ncondition, are insufficient for one-sample learning; on the positive side, we\nprovide a general condition that is sufficient to guarantee linear-time\nlearning and obtain applications for proper colorings and permissive models.\nFor the $k$-SAT model on formulas with maximum degree $d$, we provide a\nlinear-time estimator when $k\\gtrsim 6.45\\log d$, whereas the problem becomes\nnon-identifiable when $k\\lesssim \\log d$.",
            "author": [
                "Andreas Galanis",
                "Alkis Kalavasis",
                "Anthimos Vardis Kandiros"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03332v1",
                "http://arxiv.org/pdf/2311.03332v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DS",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03321v1",
            "title": "Exact Shortest Paths with Rational Weights on the Word RAM",
            "updated": "2023-11-06T18:17:53Z",
            "published": "2023-11-06T18:17:53Z",
            "summary": "Exact computation of shortest paths in weighted graphs has been traditionally\nstudied in one of two settings. First, one can assume that the edge weights are\nreal numbers and all the performed operations on reals (typically comparisons\nand additions) take constant time. Classical Dijkstra's and Bellman-Ford\nalgorithms have been described in this setting. More efficient exact shortest\npaths algorithms have been obtained for integer-weighted graphs. Integrality\nassumption not only enables faster algorithms but also allows implementing the\naforementioned algorithms in a much more realistic word RAM model where only\narithmetic operations on $O(\\log{n})$-bit integers are performed in constant\ntime. On the word RAM one can as efficiently exactly encode even\n\\emph{rational-weighted} instances with $O(\\log{n})$-bit numerators and\ndenominators. However, the known exact real-weighted shortest paths algorithms,\nrun on such a rational input, can easily encounter intermediate values of\n$\\Theta(n)$ bits if represented exactly. This leads to a factor-$\\Omega(n)$\nslowdown on the word RAM. At the same time, the scaling algorithms suited for\ninteger weights do not produce exact solutions for rational inputs without\ndramatically increasing their accuracy.\n  In this paper, we design randomized exact single-source shortest paths\nalgorithms for rational-weighted graphs on the word RAM. Most importantly, in\nthe non-negative case, we obtain a near-linear time algorithm matching\nDijkstra's algorithm running time up to polylogarithmic factors. In presence of\nnegative weights, we give an $\\tilde{O}(n^{2.5})$-time algorithm breaking\nthrough the best known strongly polynomial bound attained by Bellman-Ford for\nsufficiently dense graphs.",
            "author": [
                "Adam Karczmarz",
                "Wojciech Nadara",
                "Marek Soko\u0142owski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03321v1",
                "http://arxiv.org/pdf/2311.03321v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03318v1",
            "title": "A Foundation Model for Music Informatics",
            "updated": "2023-11-06T18:12:27Z",
            "published": "2023-11-06T18:12:27Z",
            "summary": "This paper investigates foundation models tailored for music informatics, a\ndomain currently challenged by the scarcity of labeled data and generalization\nissues. To this end, we conduct an in-depth comparative study among various\nfoundation model variants, examining key determinants such as model\narchitectures, tokenization methods, temporal resolution, data, and model\nscalability. This research aims to bridge the existing knowledge gap by\nelucidating how these individual factors contribute to the success of\nfoundation models in music informatics. Employing a careful evaluation\nframework, we assess the performance of these models across diverse downstream\ntasks in music information retrieval, with a particular focus on token-level\nand sequence-level classification. Our results reveal that our model\ndemonstrates robust performance, surpassing existing models in specific key\nmetrics. These findings contribute to the understanding of self-supervised\nlearning in music informatics and pave the way for developing more effective\nand versatile foundation models in the field. A pretrained version of our model\nis publicly available to foster reproducibility and future research.",
            "author": [
                "Minz Won",
                "Yun-Ning Hung",
                "Duc Le"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03318v1",
                "http://arxiv.org/pdf/2311.03318v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.IR",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03316v1",
            "title": "Improving Collaborative Filtering Recommendation via Graph Learning",
            "updated": "2023-11-06T18:08:37Z",
            "published": "2023-11-06T18:08:37Z",
            "summary": "Recommendation systems are designed to provide personalized predictions for\nitems that are most appealing to individual customers. Among various types of\nrecommendation algorithms, k-nearest neighbor based collaborative filtering\nalgorithm attracts tremendous attention and are widely used in practice.\nHowever, the k-nearest neighbor scheme can only capture the local relationship\namong users and the uniform neighborhood size is also not suitable to represent\nthe underlying data structure. In this paper, we leverage emerging graph signal\nprocessing (GSP) theory to construct sparse yet high quality graph to enhance\nthe solution quality and efficiency of collaborative filtering algorithm.\nExperimental results show that our method outperforms k-NN based collaborative\nfiltering algorithm by a large margin on the benchmark data set.",
            "author": [
                "Yongyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03316v1",
                "http://arxiv.org/pdf/2311.03316v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03294v1",
            "title": "Indirect Quantum Approximate Optimization Algorithms: application to the\n  TSP",
            "updated": "2023-11-06T17:39:14Z",
            "published": "2023-11-06T17:39:14Z",
            "summary": "We propose an Indirect Quantum Approximate Optimization Algorithm (referred\nto as IQAOA) where the Quantum Alternating Operator Ansatz takes into\nconsideration a general parameterized family of unitary operators to\nefficiently model the Hamiltonian describing the set of string vectors. This\nalgorithm creates an efficient alternative to QAOA, where: 1) a Quantum\nparametrized circuit executed on a quantum machine models the set of string\nvectors; 2) a Classical meta-optimization loop executed on a classical machine;\n3) an estimation of the average cost of each string vector computing, using a\nwell know algorithm coming from the OR community that is problem dependent. The\nindirect encoding defined by dimensional string vector is mapped into a\nsolution by an efficient coding/decoding mechanism. The main advantage is to\nobtain a quantum circuit with a strongly limited number of gates that could be\nexecuted on the noisy current quantum machines. The numerical experiments\nachieved with IQAOA permits to solve 8-customer instances TSP using the IBM\nsimulator which are to the best of our knowledge the largest TSP ever solved\nusing a QAOA based approach.",
            "author": [
                "Eric Bourreau",
                "Gerard Fleury",
                "Philippe Lacomme"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03294v1",
                "http://arxiv.org/pdf/2311.03294v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03283v1",
            "title": "Risk of Transfer Learning and its Applications in Finance",
            "updated": "2023-11-06T17:23:54Z",
            "published": "2023-11-06T17:23:54Z",
            "summary": "Transfer learning is an emerging and popular paradigm for utilizing existing\nknowledge from previous learning tasks to improve the performance of new ones.\nIn this paper, we propose a novel concept of transfer risk and and analyze its\nproperties to evaluate transferability of transfer learning. We apply transfer\nlearning techniques and this concept of transfer risk to stock return\nprediction and portfolio optimization problems. Numerical results demonstrate a\nstrong correlation between transfer risk and overall transfer learning\nperformance, where transfer risk provides a computationally efficient way to\nidentify appropriate source tasks in transfer learning, including\ncross-continent, cross-sector, and cross-frequency transfer for portfolio\noptimization.",
            "author": [
                "Haoyang Cao",
                "Haotian Gu",
                "Xin Guo",
                "Mathieu Rosenbaum"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03283v1",
                "http://arxiv.org/pdf/2311.03283v1"
            ],
            "primary_category": "q-fin.MF",
            "category": [
                "q-fin.MF",
                "cs.LG",
                "91-08, 91-10, 91G10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03275v1",
            "title": "Exploiting Latent Attribute Interaction with Transformer on\n  Heterogeneous Information Networks",
            "updated": "2023-11-06T17:18:37Z",
            "published": "2023-11-06T17:18:37Z",
            "summary": "Heterogeneous graph neural networks (HGNNs) have recently shown impressive\ncapability in modeling heterogeneous graphs that are ubiquitous in real-world\napplications. Due to the diversity of attributes of nodes in different types,\nmost existing models first align nodes by mapping them into the same\nlow-dimensional space. However, in this way, they lose the type information of\nnodes. In addition, most of them only consider the interactions between nodes\nwhile neglecting the high-order information behind the latent interactions\namong different node features. To address these problems, in this paper, we\npropose a novel heterogeneous graph model MULAN, including two major\ncomponents, i.e., a type-aware encoder and a dimension-aware encoder.\nSpecifically, the type-aware encoder compensates for the loss of node type\ninformation and better leverages graph heterogeneity in learning node\nrepresentations. Built upon transformer architecture, the dimension-aware\nencoder is capable of capturing the latent interactions among the diverse node\nfeatures. With these components, the information of graph heterogeneity, node\nfeatures and graph structure can be comprehensively encoded in node\nrepresentations. We conduct extensive experiments on six heterogeneous\nbenchmark datasets, which demonstrates the superiority of MULAN over other\nstate-of-the-art competitors and also shows that MULAN is efficient.",
            "author": [
                "Zeyuan Zhao",
                "Qingqing Ge",
                "Anfeng Cheng",
                "Yiding Liu",
                "Xiang Li",
                "Shuaiqiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03275v1",
                "http://arxiv.org/pdf/2311.03275v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03267v1",
            "title": "Nibbling at Long Cycles: Dynamic (and Static) Edge Coloring in Optimal\n  Time",
            "updated": "2023-11-06T16:59:29Z",
            "published": "2023-11-06T16:59:29Z",
            "summary": "We consider the problem of maintaining a $(1+\\epsilon)\\Delta$-edge coloring\nin a dynamic graph $G$ with $n$ nodes and maximum degree at most $\\Delta$. The\nstate-of-the-art update time is $O_\\epsilon(\\text{polylog}(n))$, by Duan, He\nand Zhang [SODA'19] and by Christiansen [STOC'23], and more precisely $O(\\log^7\nn/\\epsilon^2)$, where $\\Delta = \\Omega(\\log^2 n / \\epsilon^2)$.\n  The following natural question arises: What is the best possible update time\nof an algorithm for this task? More specifically, \\textbf{ can we bring it all\nthe way down to some constant} (for constant $\\epsilon$)? This question\ncoincides with the \\emph{static} time barrier for the problem: Even for\n$(2\\Delta-1)$-coloring, there is only a naive $O(m \\log \\Delta)$-time\nalgorithm.\n  We answer this fundamental question in the affirmative, by presenting a\ndynamic $(1+\\epsilon)\\Delta$-edge coloring algorithm with $O(\\log^4\n(1/\\epsilon)/\\epsilon^9)$ update time, provided $\\Delta =\n\\Omega_\\epsilon(\\text{polylog}(n))$. As a corollary, we also get the first\nlinear time (for constant $\\epsilon$) \\emph{static} algorithm for\n$(1+\\epsilon)\\Delta$-edge coloring; in particular, we achieve a running time of\n$O(m \\log (1/\\epsilon)/\\epsilon^2)$.\n  We obtain our results by carefully combining a variant of the \\textsc{Nibble}\nalgorithm from Bhattacharya, Grandoni and Wajc [SODA'21] with the subsampling\ntechnique of Kulkarni, Liu, Sah, Sawhney and Tarnawski [STOC'22].",
            "author": [
                "Sayan Bhattacharya",
                "Mart\u00edn Costa",
                "Nadav Panski",
                "Shay Solomon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03267v1",
                "http://arxiv.org/pdf/2311.03267v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03262v1",
            "title": "On Finding Optimal (Dynamic) Arborescences",
            "updated": "2023-11-06T16:48:49Z",
            "published": "2023-11-06T16:48:49Z",
            "summary": "Let G = (V, E) be a directed and weighted graph with vertex set V of size n\nand edge set E of size m, such that each edge (u, v) \\in E has a real-valued\nweight w(u, c). An arborescence in G is a subgraph T = (V, E') such that for a\nvertex u \\in V, the root, there is a unique path in T from u to any other\nvertex v \\in V. The weight of T is the sum of the weights of its edges. In this\npaper, given G, we are interested in finding an arborescence in G with minimum\nweight, i.e., an optimal arborescence. Furthermore, when G is subject to\nchanges, namely edge insertions and deletions, we are interested in efficiently\nmaintaining a dynamic arborescence in G. This is a well known problem with\napplications in several domains such as network design optimization and in\nphylogenetic inference. In this paper we revisit algorithmic ideas proposed by\nseveral authors for this problem, we provide detailed pseudo-code as well as\nimplementation details, and we present experimental results on large scale-free\nnetworks and on phylogenetic inference. Our implementation is publicly\navailable at \\url{https://gitlab.com/espadas/optimal-arborescences}.",
            "author": [
                "Joaquim Espada",
                "Alexandre P. Francisco",
                "Tatiana Rocher",
                "Lu\u00eds M. S. Russo",
                "C\u00e1tia Vaz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03262v1",
                "http://arxiv.org/pdf/2311.03262v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03260v1",
            "title": "From Coupled Oscillators to Graph Neural Networks: Reducing\n  Over-smoothing via a Kuramoto Model-based Approach",
            "updated": "2023-11-06T16:47:17Z",
            "published": "2023-11-06T16:47:17Z",
            "summary": "We propose the Kuramoto Graph Neural Network (KuramotoGNN), a novel class of\ncontinuous-depth graph neural networks (GNNs) that employs the Kuramoto model\nto mitigate the over-smoothing phenomenon, in which node features in GNNs\nbecome indistinguishable as the number of layers increases. The Kuramoto model\ncaptures the synchronization behavior of non-linear coupled oscillators. Under\nthe view of coupled oscillators, we first show the connection between Kuramoto\nmodel and basic GNN and then over-smoothing phenomenon in GNNs can be\ninterpreted as phase synchronization in Kuramoto model. The KuramotoGNN\nreplaces this phase synchronization with frequency synchronization to prevent\nthe node features from converging into each other while allowing the system to\nreach a stable synchronized state. We experimentally verify the advantages of\nthe KuramotoGNN over the baseline GNNs and existing methods in reducing\nover-smoothing on various graph deep learning benchmark tasks.",
            "author": [
                "Tuan Nguyen",
                "Tan M. Nguyen",
                "Hirotada Honda",
                "Takashi Sano",
                "Vinh Nguyen",
                "Shugo Nakamura"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03260v1",
                "http://arxiv.org/pdf/2311.03260v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03252v1",
            "title": "Parameter-Agnostic Optimization under Relaxed Smoothness",
            "updated": "2023-11-06T16:39:53Z",
            "published": "2023-11-06T16:39:53Z",
            "summary": "Tuning hyperparameters, such as the stepsize, presents a major challenge of\ntraining machine learning models. To address this challenge, numerous adaptive\noptimization algorithms have been developed that achieve near-optimal\ncomplexities, even when stepsizes are independent of problem-specific\nparameters, provided that the loss function is $L$-smooth. However, as the\nassumption is relaxed to the more realistic $(L_0, L_1)$-smoothness, all\nexisting convergence results still necessitate tuning of the stepsize. In this\nstudy, we demonstrate that Normalized Stochastic Gradient Descent with Momentum\n(NSGD-M) can achieve a (nearly) rate-optimal complexity without prior knowledge\nof any problem parameter, though this comes at the cost of introducing an\nexponential term dependent on $L_1$ in the complexity. We further establish\nthat this exponential term is inevitable to such schemes by introducing a\ntheoretical framework of lower bounds tailored explicitly for\nparameter-agnostic algorithms. Interestingly, in deterministic settings, the\nexponential factor can be neutralized by employing Gradient Descent with a\nBacktracking Line Search. To the best of our knowledge, these findings\nrepresent the first parameter-agnostic convergence results under the\ngeneralized smoothness condition. Our empirical experiments further confirm our\ntheoretical insights.",
            "author": [
                "Florian H\u00fcbler",
                "Junchi Yang",
                "Xiang Li",
                "Niao He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03252v1",
                "http://arxiv.org/pdf/2311.03252v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03250v1",
            "title": "Instructed Language Models with Retrievers Are Powerful Entity Linkers",
            "updated": "2023-11-06T16:38:51Z",
            "published": "2023-11-06T16:38:51Z",
            "summary": "Generative approaches powered by large language models (LLMs) have\ndemonstrated emergent abilities in tasks that require complex reasoning\nabilities. Yet the generative nature still makes the generated content suffer\nfrom hallucinations, thus unsuitable for entity-centric tasks like entity\nlinking (EL) requiring precise entity predictions over a large knowledge base.\nWe present Instructed Generative Entity Linker (INSGENEL), the first approach\nthat enables casual language models to perform entity linking over knowledge\nbases. Several methods to equip language models with EL capability were\nproposed in this work, including (i) a sequence-to-sequence training EL\nobjective with instruction-tuning, (ii) a novel generative EL framework based\non a light-weight potential mention retriever that frees the model from heavy\nand non-parallelizable decoding, achieving 4$\\times$ speedup without compromise\non linking metrics. INSGENEL outperforms previous generative alternatives with\n+6.8 F1 points gain on average, also with a huge advantage in training data\nefficiency and training compute consumption. In addition, our skillfully\nengineered in-context learning (ICL) framework for EL still lags behind\nINSGENEL significantly, reaffirming that the EL task remains a persistent\nhurdle for general LLMs.",
            "author": [
                "Zilin Xiao",
                "Ming Gong",
                "Jie Wu",
                "Xingyao Zhang",
                "Linjun Shou",
                "Jian Pei",
                "Daxin Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03250v1",
                "http://arxiv.org/pdf/2311.03250v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03249v1",
            "title": "Note on multicolour Erd\u0151s-Hajnal conjecture",
            "updated": "2023-11-06T16:38:45Z",
            "published": "2023-11-06T16:38:45Z",
            "summary": "Informally, the Erd\\H{o}s-Hajnal conjecture (shortly EH-conjecture) asserts\nthat if a sufficiently large host clique on $n$ vertices is edge-coloured\navoiding a copy of some fixed edge-coloured clique, then there is a large\nhomogeneous set of size $n^\\beta$ for some positive $\\beta$, where a set of\nvertices is homogeneous if it does not induce all the colours. This conjecture,\nif true, claims that imposing local conditions on edge-partitions of cliques\nresults in a global structural consequence such as a large homogeneous set, a\nset avoiding all edges of some part. While this conjecture attracted a lot of\nattention, it is still open even for two colours. In this note, we reduce the\nmulticolour EH-conjecture to the case when the number of colours used in a host\nclique is either the same as in the forbidden pattern or one more. We exhibit a\nnon-monotonicity behaviour of homogeneous sets in coloured cliques with\nforbidden patterns by showing that allowing an extra colour in the host graph\ncould actually decrease the size of a largest homogeneous set.",
            "author": [
                "Maria Axenovich",
                "Alex Riasanovsky",
                "Lea Weber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03249v1",
                "http://arxiv.org/pdf/2311.03249v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03245v1",
            "title": "Error analysis of the Lie splitting for semilinear wave equations with\n  finite-energy solutions",
            "updated": "2023-11-06T16:33:40Z",
            "published": "2023-11-06T16:33:40Z",
            "summary": "We study time integration schemes for $\\dot H^1$-solutions to the\nenergy-(sub)critical semilinear wave equation on $\\mathbb{R}^3$. We show\nfirst-order convergence in $L^2$ for the Lie splitting and convergence order\n$3/2$ for a corrected Lie splitting. To our knowledge this includes the first\nerror analysis performed for scaling-critical dispersive problems. Our approach\nis based on discrete-time Strichartz estimates, including one (with a\nlogarithmic correction) for the case of the forbidden endpoint. Our schemes and\nthe Strichartz estimates contain frequency cut-offs.",
            "author": [
                "Maximilian Ruff",
                "Roland Schnaubelt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03245v1",
                "http://arxiv.org/pdf/2311.03245v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "math.AP",
                "65M15 (Primary) 35B33, 35L71, 65M12 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03235v1",
            "title": "p-Laplacian Transformer",
            "updated": "2023-11-06T16:25:56Z",
            "published": "2023-11-06T16:25:56Z",
            "summary": "$p$-Laplacian regularization, rooted in graph and image signal processing,\nintroduces a parameter $p$ to control the regularization effect on these data.\nSmaller values of $p$ promote sparsity and interpretability, while larger\nvalues encourage smoother solutions. In this paper, we first show that the\nself-attention mechanism obtains the minimal Laplacian regularization ($p=2$)\nand encourages the smoothness in the architecture. However, the smoothness is\nnot suitable for the heterophilic structure of self-attention in transformers\nwhere attention weights between tokens that are in close proximity and\nnon-close ones are assigned indistinguishably. From that insight, we then\npropose a novel class of transformers, namely the $p$-Laplacian Transformer\n(p-LaT), which leverages $p$-Laplacian regularization framework to harness the\nheterophilic features within self-attention layers. In particular, low $p$\nvalues will effectively assign higher attention weights to tokens that are in\nclose proximity to the current token being processed. We empirically\ndemonstrate the advantages of p-LaT over the baseline transformers on a wide\nrange of benchmark datasets.",
            "author": [
                "Tuan Nguyen",
                "Tam Nguyen",
                "Vinh Nguyen",
                "Tan M. Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03235v1",
                "http://arxiv.org/pdf/2311.03235v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07584v1",
            "title": "Performance Prediction of Data-Driven Knowledge summarization of High\n  Entropy Alloys (HEAs) literature implementing Natural Language Processing\n  algorithms",
            "updated": "2023-11-06T16:22:32Z",
            "published": "2023-11-06T16:22:32Z",
            "summary": "The ability to interpret spoken language is connected to natural language\nprocessing. It involves teaching the AI how words relate to one another, how\nthey are meant to be used, and in what settings. The goal of natural language\nprocessing (NLP) is to get a machine intelligence to process words the same way\na human brain does. This enables machine intelligence to interpret, arrange,\nand comprehend textual data by processing the natural language. The technology\ncan comprehend what is communicated, whether it be through speech or writing\nbecause AI pro-cesses language more quickly than humans can. In the present\nstudy, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic\nAnalysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the\nfirst time for the knowledge summarization purpose of the High Entropy Alloys\n(HEAs). The performance prediction of these algorithms is made by using the\nBLEU score and ROUGE score. The results showed that the Luhn algorithm has the\nhighest accuracy score for the knowledge summarization tasks compared to the\nother used algorithms.",
            "author": [
                "Akshansh Mishra",
                "Vijaykumar S Jatti",
                "Vaishnavi More",
                "Anish Dasgupta",
                "Devarrishi Dixit",
                "Eyob Messele Sefene"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07584v1",
                "http://arxiv.org/pdf/2311.07584v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03225v1",
            "title": "Dichotomies for Tree Minor Containment with Structural Parameters",
            "updated": "2023-11-06T16:11:37Z",
            "published": "2023-11-06T16:11:37Z",
            "summary": "The problem of determining whether a graph $G$ contains another graph $H$ as\na minor, referred to as the minor containment problem, is a fundamental problem\nin the field of graph algorithms. While it is NP-complete when $G$ and $H$ are\ngeneral graphs, it is sometimes tractable on more restricted graph classes.\nThis study focuses on the case where both $G$ and $H$ are trees, known as the\ntree minor containment problem. Even in this case, the problem is known to be\nNP-complete. In contrast, polynomial-time algorithms are known for the case\nwhen both trees are caterpillars or when the maximum degree of $H$ is a\nconstant. Our research aims to clarify the boundary of tractability and\nintractability for the tree minor containment problem. Specifically, we provide\ndichotomies for the computational complexities of the problem based on three\nstructural parameters: the diameter, pathwidth, and path eccentricity.",
            "author": [
                "Tatsuya Gima",
                "Soh Kumabe",
                "Kazuhiro Kurita",
                "Yuto Okada",
                "Yota Otachi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03225v1",
                "http://arxiv.org/pdf/2311.03225v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03224v1",
            "title": "Risk Analysis in the Selection of Project Managers Based on ANP and FMEA",
            "updated": "2023-11-06T16:08:10Z",
            "published": "2023-11-06T16:08:10Z",
            "summary": "Project managers play a crucial role in the success of projects. The\nselection of an appropriate project manager is a primary concern for senior\nmanagers in firms. Typically, this process involves candidate interviews and\nassessments of their abilities. There are various criteria for selecting a\nproject manager, and the importance of each criterion depends on the project\ntype, its conditions, and the risks associated with their absence in the chosen\ncandidate. Often, senior managers in engineering companies lack awareness of\nthe significance of these criteria and the potential risks linked to their\nabsence. This research aims to identify these risks in selecting project\nmanagers for civil engineering projects, utilizing a combined ANP-FMEA\napproach. Through a comprehensive literature review, five risk categories have\nbeen identified: individual skills, power-related issues, knowledge and\nexpertise, experience, and personality traits. Subsequently, these risks, along\nwith their respective sub-criteria and internal relationships, were analysed\nusing the combined ANP-FMEA technique. The results highlighted that the lack of\npolitical influence, absence of construction experience, and deficiency in\nproject management expertise represent the most substantial risks in selecting\na project manager. Moreover, upon comparison with the traditional FMEA\napproach, this study demonstrates the superior ability of the ANP-FMEA model in\ndifferentiating risks and pinpointing factors with elevated risk levels.",
            "author": [
                "Armin Asaadi",
                "Armita Atrian",
                "Hesam Nik Hoseini",
                "Mohammad Mahdi Movahedi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03224v1",
                "http://arxiv.org/pdf/2311.03224v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "q-fin.RM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03221v1",
            "title": "Segmentation of Drone Collision Hazards in Airborne RADAR Point Clouds\n  Using PointNet",
            "updated": "2023-11-06T16:04:58Z",
            "published": "2023-11-06T16:04:58Z",
            "summary": "The integration of unmanned aerial vehicles (UAVs) into shared airspace for\nbeyond visual line of sight (BVLOS) operations presents significant challenges\nbut holds transformative potential for sectors like transportation,\nconstruction, energy and defense. A critical prerequisite for this integration\nis equipping UAVs with enhanced situational awareness to ensure safe\noperations. Current approaches mainly target single object detection or\nclassification, or simpler sensing outputs that offer limited perceptual\nunderstanding and lack the rapid end-to-end processing needed to convert sensor\ndata into safety-critical insights. In contrast, our study leverages radar\ntechnology for novel end-to-end semantic segmentation of aerial point clouds to\nsimultaneously identify multiple collision hazards. By adapting and optimizing\nthe PointNet architecture and integrating aerial domain insights, our framework\ndistinguishes five distinct classes: mobile drones (DJI M300 and DJI Mini) and\nairplanes (Ikarus C42), and static returns (ground and infrastructure) which\nresults in enhanced situational awareness for UAVs. To our knowledge, this is\nthe first approach addressing simultaneous identification of multiple collision\nthreats in an aerial setting, achieving a robust 94% accuracy. This work\nhighlights the potential of radar technology to advance situational awareness\nin UAVs, facilitating safe and efficient BVLOS operations.",
            "author": [
                "Hector Arroyo",
                "Paul Kier",
                "Dylan Angus",
                "Santiago Matalonga",
                "Svetlozar Georgiev",
                "Mehdi Goli",
                "Gerard Dooly",
                "James Riordan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03221v1",
                "http://arxiv.org/pdf/2311.03221v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03215v1",
            "title": "Quantum speedups for linear programming via interior point methods",
            "updated": "2023-11-06T16:00:07Z",
            "published": "2023-11-06T16:00:07Z",
            "summary": "We describe a quantum algorithm based on an interior point method for solving\na linear program with $n$ inequality constraints on $d$ variables. The\nalgorithm explicitly returns a feasible solution that is $\\epsilon$-close to\noptimal, and runs in time $\\sqrt{n}\\,\n\\mathrm{poly}(d,\\log(n),\\log(1/\\varepsilon))$ which is sublinear for tall\nlinear programs (i.e., $n \\gg d$). Our algorithm speeds up the Newton step in\nthe state-of-the-art interior point method of Lee and Sidford [FOCS '14]. This\nrequires us to efficiently approximate the Hessian and gradient of the barrier\nfunction, and these are our main contributions.\n  To approximate the Hessian, we describe a quantum algorithm for the spectral\napproximation of $A^T A$ for a tall matrix $A \\in \\mathbb R^{n \\times d}$. The\nalgorithm uses leverage score sampling in combination with Grover search, and\nreturns a $\\delta$-approximation by making $O(\\sqrt{nd}/\\delta)$ row queries to\n$A$. This generalizes an earlier quantum speedup for graph sparsification by\nApers and de Wolf [FOCS '20]. To approximate the gradient, we use a recent\nquantum algorithm for multivariate mean estimation by Cornelissen, Hamoudi and\nJerbi [STOC '22]. While a naive implementation introduces a dependence on the\ncondition number of the Hessian, we avoid this by pre-conditioning our random\nvariable using our quantum algorithm for spectral approximation.",
            "author": [
                "Simon Apers",
                "Sander Gribling"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03215v1",
                "http://arxiv.org/pdf/2311.03215v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DS",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03199v1",
            "title": "Quantifying Interstellar Extinction at High Galactic Latitudes I:\n  Bayesian Model Validation",
            "updated": "2023-11-06T15:39:49Z",
            "published": "2023-11-06T15:39:49Z",
            "summary": "Accurate knowledge of the interstellar medium (ISM) at high-Galactic\nlatitudes is crucial for future cosmic microwave background (CMB) polarization\nexperiments due to extinction, albeit being low, being a foreground larger than\nthe anticipated signal in these regions. We develop a Bayesian model to\nidentify a region of the Hertzsprung-Russell (HR) diagram and an associated\ndataset suited to constrain the single-star extinction accurately at\nhigh-Galactic latitudes. Using photometry from Gaia, 2MASS and ALLWISE,\nparallax from Gaia and stellar parameters derived from the Gaia low-resolution\nBP/RP (XP) spectra as input data, we employ nested sampling to fit the model to\nthe data and analyse samples from the extinction posterior. Charting low\nvariations in extinction is complex due to both systematic errors and\ndegeneracies between extinction and other stellar parameters. The systematics\ncan be minimised by restricting our data to a region of the HR diagram where\nthe stellar models are most accurate. Moreover, the degeneracies can be\nsignificantly reduced by including spectroscopic estimates of the effective\ntemperature as data. We show that underestimating the measurement error on the\ndata is detrimental to recovering an accurate extinction distribution. We\ndemonstrate that a full posterior solution is necessary to understand the\nextinction parameter and find fine variation in the ISM. However, by only using\nthe mean extinction and a prior assumption of spatial correlation, we can\nproduce a dust map similar to other benchmark maps.",
            "author": [
                "Matthew O'Callaghan",
                "Gerry Gilmore",
                "Kaisey S. Mandel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03199v1",
                "http://arxiv.org/pdf/2311.03199v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03185v1",
            "title": "Spanning trees in pseudorandom graphs via sorting networks",
            "updated": "2023-11-06T15:25:13Z",
            "published": "2023-11-06T15:25:13Z",
            "summary": "We show that $(n,d,\\lambda)$-graphs with $\\lambda=O(d/\\log^3 n)$ are\nuniversal with respect to all bounded degree spanning trees. This significantly\nimproves upon the previous best bound due to Han and Yang of the form\n$\\lambda=d/\\exp{(O(\\sqrt{\\log n}))}$, and makes progress towards a problem of\nAlon, Krivelevich, and Sudakov from 2007. Our proof relies on the existence of\nsorting networks of logarithmic depth, as given by a celebrated construction of\nAjtai, Koml\\'os and Szemer\\'edi. Using this construction, we show that the\nclassical vertex-disjoint paths problem can be solved for a set of vertices\nfixed in advance.",
            "author": [
                "Joseph Hyde",
                "Natasha Morrison",
                "Alp M\u00fcyesser",
                "Mat\u00edas Pavez-Sign\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03185v1",
                "http://arxiv.org/pdf/2311.03185v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05D40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03174v1",
            "title": "Incremental Approximate Maximum Flow on Undirected Graphs in\n  Subpolynomial Update Time",
            "updated": "2023-11-06T15:09:22Z",
            "published": "2023-11-06T15:09:22Z",
            "summary": "We provide an algorithm which, with high probability, maintains a\n$(1-\\epsilon)$-approximate maximum flow on an undirected graph undergoing\n$m$-edge additions in amortized $m^{o(1)} \\epsilon^{-3}$ time per update. To\nobtain this result, we provide a more general algorithm that solves what we\ncall the incremental, thresholded $p$-norm flow problem that asks to determine\nthe first edge-insertion in an undirected graph that causes the minimum\n$\\ell_p$-norm flow to decrease below a given threshold in value. Since we solve\nthis thresholded problem, our data structure succeeds against an adaptive\nadversary that can only see the data structure's output. Furthermore, since our\nalgorithm holds for $p = 2$, we obtain improved algorithms for dynamically\nmaintaining the effective resistance between a pair of vertices in an\nundirected graph undergoing edge insertions.\n  Our algorithm builds upon previous dynamic algorithms for approximately\nsolving the minimum-ratio cycle problem that underlie previous advances on the\nmaximum flow problem [Chen-Kyng-Liu-Peng-Probst Gutenberg-Sachdeva, FOCS '22]\nas well as recent dynamic maximum flow algorithms [v.d.Brand-Liu-Sidford, STOC\n'23]. Instead of using interior point methods, which were a key component of\nthese recent advances, our algorithm uses an optimization method based on\n$\\ell_p$-norm iterative refinement and the multiplicative weight update method.\nThis ensures a monotonicity property in the minimum-ratio cycle subproblems\nthat allows us to apply known data structures and bypass issues arising from\nadaptive queries.",
            "author": [
                "Jan van den Brand",
                "Li Chen",
                "Rasmus Kyng",
                "Yang P. Liu",
                "Richard Peng",
                "Maximilian Probst Gutenberg",
                "Sushant Sachdeva",
                "Aaron Sidford"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03174v1",
                "http://arxiv.org/pdf/2311.03174v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03157v1",
            "title": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian\n  Optimization",
            "updated": "2023-11-06T14:52:30Z",
            "published": "2023-11-06T14:52:30Z",
            "summary": "Modern database management systems (DBMS) expose hundreds of configurable\nknobs to control system behaviours. Determining the appropriate values for\nthese knobs to improve DBMS performance is a long-standing problem in the\ndatabase community. As there is an increasing number of knobs to tune and each\nknob could be in continuous or categorical values, manual tuning becomes\nimpractical. Recently, automatic tuning systems using machine learning methods\nhave shown great potentials. However, existing approaches still incur\nsignificant tuning costs or only yield sub-optimal performance. This is because\nthey either ignore the extensive domain knowledge available (e.g., DBMS manuals\nand forum discussions) and only rely on the runtime feedback of benchmark\nevaluations to guide the optimization, or they utilize the domain knowledge in\na limited way. Hence, we propose GPTuner, a manual-reading database tuning\nsystem. Firstly, we develop a Large Language Model (LLM)-based pipeline to\ncollect and refine heterogeneous knowledge, and propose a prompt ensemble\nalgorithm to unify a structured view of the refined knowledge. Secondly, using\nthe structured knowledge, we (1) design a workload-aware and training-free knob\nselection strategy, (2) develop a search space optimization technique\nconsidering the value range of each knob, and (3) propose a Coarse-to-Fine\nBayesian Optimization Framework to explore the optimized space. Finally, we\nevaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics\n(throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to\nthe state-of-the-art approaches, GPTuner identifies better configurations in\n16x less time on average. Moreover, GPTuner achieves up to 30% performance\nimprovement (higher throughput or lower latency) over the best-performing\nalternative.",
            "author": [
                "Jiale Lao",
                "Yibo Wang",
                "Yufei Li",
                "Jianping Wang",
                "Yunjia Zhang",
                "Zhiyuan Cheng",
                "Wanghu Chen",
                "Mingjie Tang",
                "Jianguo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03157v1",
                "http://arxiv.org/pdf/2311.03157v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03155v1",
            "title": "Comparison of symbolic and ordinary powers of parity binomial edge\n  ideals",
            "updated": "2023-11-06T14:49:12Z",
            "published": "2023-11-06T14:49:12Z",
            "summary": "In this paper, we investigate when symbolic and ordinary powers of the parity\nbinomial edge ideal of a graph fail to be equal. It turns out that if\n$\\mathcal{I}_{G}$ is the parity binomial edge ideal of a graph $G$, then in\neach of the following cases the symbolic power $\\mathcal{I}_{G}^{(t)}$ and the\nordinary power $\\mathcal{I}_{G}^t$ are not equal for some $t$: (i) the clique\nnumber of $G$ is greater than 3; (ii) $G$ has a net; or (iii) $G$ has a PT as\nan induced subgraph.",
            "author": [
                "Nadia Taghipour",
                "Shamila Bayati",
                "Farhad Rahmati"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s00605-023-01912-4",
                "http://arxiv.org/abs/2311.03155v1",
                "http://arxiv.org/pdf/2311.03155v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03149v1",
            "title": "Asymmetric Masked Distillation for Pre-Training Small Foundation Models",
            "updated": "2023-11-06T14:44:34Z",
            "published": "2023-11-06T14:44:34Z",
            "summary": "Self-supervised foundation models have shown great potential in computer\nvision thanks to the pre-training paradigm of masked autoencoding. Scale is a\nprimary factor influencing the performance of these foundation models. However,\nthese large foundation models often result in high computational cost that\nmight limit their deployment. This paper focuses on pre-training relatively\nsmall vision transformer models that could be efficiently adapted to downstream\ntasks. Specifically, taking inspiration from knowledge distillation in model\ncompression, we propose a new asymmetric masked distillation(AMD) framework for\npre-training relatively small models with autoencoding. The core of AMD is to\ndevise an asymmetric masking strategy, where the teacher model is enabled to\nsee more context information with a lower masking ratio, while the student\nmodel still with high masking ratio to the original masked pre-training. We\ndesign customized multi-layer feature alignment between the teacher encoder and\nstudent encoder to regularize the pre-training of student MAE. To demonstrate\nthe effectiveness and versatility of AMD, we apply it to both ImageMAE and\nVideoMAE for pre-training relatively small ViT models. AMD achieved 84.6%\nclassification accuracy on IN1K using the ViT-B model. And AMD achieves 73.3%\nclassification accuracy using the ViT-B model on the Something-in-Something V2\ndataset, a 3.7% improvement over the original ViT-B model from VideoMAE. We\nalso transfer AMD pre-trained models to downstream tasks and obtain consistent\nperformance improvement over the standard pre-training.",
            "author": [
                "Zhiyu Zhao",
                "Bingkun Huang",
                "Sen Xing",
                "Gangshan Wu",
                "Yu Qiao",
                "Limin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03149v1",
                "http://arxiv.org/pdf/2311.03149v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03147v1",
            "title": "On Sharp Bounds of Local Fractional Metric Dimension for Certain\n  Symmetrical Algebraic Structure Graphs",
            "updated": "2023-11-06T14:43:50Z",
            "published": "2023-11-06T14:43:50Z",
            "summary": "The smallest set of vertices needed to differentiate or categorize every\nother vertex in a graph is referred to as the graph's metric dimension. Finding\nthe class of graphs for a particular given metric dimension is an NP-hard\nproblem. This concept has applications in many different domains, including\ngraph theory, network architecture, and facility location problems. A graph $G$\nwith order $n$ is known as a Toeplitz graph over the subset $S$ of consecutive\ncollections of integers from one to $n$, and two vertices will be adjacent to\neach other if their absolute difference is a member of $S$. A graph\n$G(\\mathbb{Z}_{n})$ is called a zero-divisor graph over the zero divisors of a\ncommutative ring $\\mathbb{Z}_{n}$, in which two vertices will be adjacent to\neach other if their product will leave the remainder zero under modulo $n$.\nSince the local fractional metric dimension problem is NP-hard, it is\ncomputationally difficult to identify an optimal solution or to precisely\ndetermine the minimal size of a local resolving set; in the worst case, the\nprocess takes exponential time. Different upper bound sequences of local\nfractional metric dimension are suggested in this article, along with a\ncomparison analysis for certain families of Toeplitz and zero-divisor graphs.\nFurthermore, we note that the analyzed local fractional metric dimension upper\nbounds fall into three metric families: constant, limited, and unbounded.",
            "author": [
                "Amal S. Alali",
                "Shahbaz Ali",
                "Muhammad Adnan",
                "Delfim F. M. Torres"
            ],
            "link": [
                "http://dx.doi.org/10.3390/sym15101911",
                "http://arxiv.org/abs/2311.03147v1",
                "http://arxiv.org/pdf/2311.03147v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35, 05C72, 90C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03139v1",
            "title": "End-to-end Material Thermal Conductivity Prediction through Machine\n  Learning",
            "updated": "2023-11-06T14:34:30Z",
            "published": "2023-11-06T14:34:30Z",
            "summary": "We investigated the accelerated prediction of the thermal conductivity of\nmaterials through end- to-end structure-based approaches employing machine\nlearning methods. Due to the non-availability of high-quality thermal\nconductivity data, we first performed high-throughput calculations based on\nfirst principles and the Boltzmann transport equation for 225 materials,\neffectively more than doubling the size of the existing dataset. We assessed\nthe performance of state-of-the-art machine learning models for thermal\nconductivity prediction on this expanded dataset and observed that all these\nmodels suffered from overfitting. To address this issue, we introduced a novel\ngraph-based neural network model, which demonstrated more consistent and\nregularized performance across all evaluated datasets. Nevertheless, the best\nmean absolute percentage error achieved on the test dataset remained in the\nrange of 50-60%. This suggests that while these models are valuable for\nexpediting material screening, their current accuracy is still limited.",
            "author": [
                "Yagyank Srivastava",
                "Ankit Jain"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03139v1",
                "http://arxiv.org/pdf/2311.03139v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03132v1",
            "title": "Solution to a problem of Gr\u00fcnbaum on the edge density of $4$-critical\n  planar graphs",
            "updated": "2023-11-06T14:28:33Z",
            "published": "2023-11-06T14:28:33Z",
            "summary": "We show that $\\limsup |E(G)|/|V(G)| = 2.5$ over all $4$-critical planar\ngraphs $G$, answering a question of Gr\\\"unbaum from 1988.",
            "author": [
                "Zden\u011bk Dvo\u0159\u00e1k",
                "Carl Feghali"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03132v1",
                "http://arxiv.org/pdf/2311.03132v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03118v1",
            "title": "Algebraic Dynamical Systems in Machine Learning",
            "updated": "2023-11-06T14:10:40Z",
            "published": "2023-11-06T14:10:40Z",
            "summary": "We introduce an algebraic analogue of dynamical systems, based on term\nrewriting. We show that a recursive function applied to the output of an\niterated rewriting system defines a formal class of models into which all the\nmain architectures for dynamic machine learning models (including recurrent\nneural networks, graph neural networks, and diffusion models) can be embedded.\nConsidered in category theory, we also show that these algebraic models are a\nnatural language for describing the compositionality of dynamic models.\nFurthermore, we propose that these models provide a template for the\ngeneralisation of the above dynamic models to learning problems on structured\nor non-numerical data, including 'hybrid symbolic-numeric' models.",
            "author": [
                "Iolo Jones",
                "Jerry Swan",
                "Jeffrey Giansiracusa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03118v1",
                "http://arxiv.org/pdf/2311.03118v1"
            ],
            "primary_category": "math.CT",
            "category": [
                "math.CT",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03111v1",
            "title": "List colorings of $k$-partite $k$-graphs",
            "updated": "2023-11-06T14:02:52Z",
            "published": "2023-11-06T14:02:52Z",
            "summary": "A $k$-uniform hypergraph (or $k$-graph) $H = (V, E)$ is $k$-partite if $V$\ncan be partitioned into $k$ sets $V_1, \\ldots, V_k$ such that each edge in $E$\ncontains precisely one vertex from each $V_i$. In this note, we consider list\ncolorings for such hypergraphs. We show that for any $\\epsilon > 0$ if each\nvertex $v \\in V(H)$ is assigned a list of size $|L(v)| \\geq\n\\left((k-1+\\epsilon)\\Delta/\\log \\Delta\\right)^{1/(k-1)}$, then $H$ admits a\nproper $L$-coloring, provided $\\Delta$ is sufficiently large. Up to a constant\nfactor, this matches the bound on the chromatic number of simple $k$-graphs\nshown by Frieze and Mubayi, and that on the list chromatic number of triangle\nfree $k$-graphs shown by Li and Postle. Our results hold in the more general\nsetting of \"color-degree\" as has been considered for graphs. Furthermore, we\nestablish a number of asymmetric statements matching results of Alon, Cambie,\nand Kang for bipartite graphs.",
            "author": [
                "Abhishek Dhawan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03111v1",
                "http://arxiv.org/pdf/2311.03111v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03094v1",
            "title": "Equivariance Is Not All You Need: Characterizing the Utility of\n  Equivariant Graph Neural Networks for Particle Physics Tasks",
            "updated": "2023-11-06T13:37:00Z",
            "published": "2023-11-06T13:37:00Z",
            "summary": "Incorporating inductive biases into ML models is an active area of ML\nresearch, especially when ML models are applied to data about the physical\nworld. Equivariant Graph Neural Networks (GNNs) have recently become a popular\nmethod for learning from physics data because they directly incorporate the\nsymmetries of the underlying physical system. Drawing from the relevant\nliterature around group equivariant networks, this paper presents a\ncomprehensive evaluation of the proposed benefits of equivariant GNNs by using\nreal-world particle physics reconstruction tasks as an evaluation test-bed. We\ndemonstrate that many of the theoretical benefits generally associated with\nequivariant networks may not hold for realistic systems and introduce\ncompelling directions for future research that will benefit both the scientific\ntheory of ML and physics applications.",
            "author": [
                "Savannah Thais",
                "Daniel Murnane"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03094v1",
                "http://arxiv.org/pdf/2311.03094v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03087v1",
            "title": "Persistent homology for high-dimensional data based on spectral methods",
            "updated": "2023-11-06T13:18:08Z",
            "published": "2023-11-06T13:18:08Z",
            "summary": "Persistent homology is a popular computational tool for detecting non-trivial\ntopology of point clouds, such as the presence of loops or voids. However, many\nreal-world datasets with low intrinsic dimensionality reside in an ambient\nspace of much higher dimensionality. We show that in this case vanilla\npersistent homology becomes very sensitive to noise and fails to detect the\ncorrect topology. The same holds true for most existing refinements of\npersistent homology. As a remedy, we find that spectral distances on the\n$k$-nearest-neighbor graph of the data, such as diffusion distance and\neffective resistance, allow persistent homology to detect the correct topology\neven in the presence of high-dimensional noise. Furthermore, we derive a novel\nclosed-form expression for effective resistance in terms of the\neigendecomposition of the graph Laplacian, and describe its relation to\ndiffusion distances. Finally, we apply these methods to several\nhigh-dimensional single-cell RNA-sequencing datasets and show that spectral\ndistances on the $k$-nearest-neighbor graph allow robust detection of cell\ncycle loops.",
            "author": [
                "Sebastian Damrich",
                "Philipp Berens",
                "Dmitry Kobak"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03087v1",
                "http://arxiv.org/pdf/2311.03087v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03057v1",
            "title": "GLEN: Generative Retrieval via Lexical Index Learning",
            "updated": "2023-11-06T12:35:06Z",
            "published": "2023-11-06T12:35:06Z",
            "summary": "Generative retrieval shed light on a new paradigm of document retrieval,\naiming to directly generate the identifier of a relevant document for a query.\nWhile it takes advantage of bypassing the construction of auxiliary index\nstructures, existing studies face two significant challenges: (i) the\ndiscrepancy between the knowledge of pre-trained language models and\nidentifiers and (ii) the gap between training and inference that poses\ndifficulty in learning to rank. To overcome these challenges, we propose a\nnovel generative retrieval method, namely Generative retrieval via LExical\niNdex learning (GLEN). For training, GLEN effectively exploits a dynamic\nlexical identifier using a two-phase index learning strategy, enabling it to\nlearn meaningful lexical identifiers and relevance signals between queries and\ndocuments. For inference, GLEN utilizes collision-free inference, using\nidentifier weights to rank documents without additional overhead. Experimental\nresults prove that GLEN achieves state-of-the-art or competitive performance\nagainst existing generative retrieval methods on various benchmark datasets,\ne.g., NQ320k, MS MARCO, and BEIR. The code is available at\nhttps://github.com/skleee/GLEN.",
            "author": [
                "Sunkyung Lee",
                "Minjin Choi",
                "Jongwuk Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03057v1",
                "http://arxiv.org/pdf/2311.03057v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03054v3",
            "title": "AnyText: Multilingual Visual Text Generation And Editing",
            "updated": "2023-11-30T12:54:05Z",
            "published": "2023-11-06T12:10:43Z",
            "summary": "Diffusion model based Text-to-Image has achieved impressive achievements\nrecently. Although current technology for synthesizing images is highly\nadvanced and capable of generating images with high fidelity, it is still\npossible to give the show away when focusing on the text area in the generated\nimage. To address this issue, we introduce AnyText, a diffusion-based\nmultilingual visual text generation and editing model, that focuses on\nrendering accurate and coherent text in the image. AnyText comprises a\ndiffusion pipeline with two primary elements: an auxiliary latent module and a\ntext embedding module. The former uses inputs like text glyph, position, and\nmasked image to generate latent features for text generation or editing. The\nlatter employs an OCR model for encoding stroke data as embeddings, which blend\nwith image caption embeddings from the tokenizer to generate texts that\nseamlessly integrate with the background. We employed text-control diffusion\nloss and text perceptual loss for training to further enhance writing accuracy.\nAnyText can write characters in multiple languages, to the best of our\nknowledge, this is the first work to address multilingual visual text\ngeneration. It is worth mentioning that AnyText can be plugged into existing\ndiffusion models from the community for rendering or editing text accurately.\nAfter conducting extensive evaluation experiments, our method has outperformed\nall other approaches by a significant margin. Additionally, we contribute the\nfirst large-scale multilingual text images dataset, AnyWord-3M, containing 3\nmillion image-text pairs with OCR annotations in multiple languages. Based on\nAnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual\ntext generation accuracy and quality. Our project will be open-sourced on\nhttps://github.com/tyxsspa/AnyText to improve and promote the development of\ntext generation technology.",
            "author": [
                "Yuxiang Tuo",
                "Wangmeng Xiang",
                "Jun-Yan He",
                "Yifeng Geng",
                "Xuansong Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03054v3",
                "http://arxiv.org/pdf/2311.03054v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03049v1",
            "title": "Arboricity and Acyclic Chromatic Number",
            "updated": "2023-11-06T11:48:06Z",
            "published": "2023-11-06T11:48:06Z",
            "summary": "A theorem of Hakimi, Mitchem and Schmeichel from 1996 states that the edge\narboricity arb(G) of a graph is bounded above by the acyclic chromatic number\nacy(G). We can improve this HMS inequality by 1, if acy(G) is even. We review\nalso results about acyclic chromatic numbers in the context of a Gr\\\"unbaum\nconjecture from 1973.",
            "author": [
                "Oliver Knill"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03049v1",
                "http://arxiv.org/pdf/2311.03049v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C10, 05C15, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03045v1",
            "title": "Lipschitz cutset for fractal graphs and applications to the spread of\n  infections",
            "updated": "2023-11-06T11:40:42Z",
            "published": "2023-11-06T11:40:42Z",
            "summary": "We consider the fractal Sierpi\\'{n}ski gasket or carpet graph in dimension\n$d\\geq 2,$ denoted by $G$. At time $0$, we place a Poisson point process of\nparticles onto the graph and let them perform independent simple random walks,\nwhich in this setting exhibit sub-diffusive behaviour. We generalise the\nconcept of particle process dependent Lipschitz percolation to the (coarse\ngraining of the) space-time graph $G\\times \\mathbb{R}$, where the opened/closed\nstate of space-time cells is measurable with respect to the particle process\ninside the cell. We then provide an application of this generalised framework\nand prove the following: if particles can spread an infection when they share a\nsite of $G$, and if they recover independently at some rate $\\gamma>0$, then if\n$\\gamma$ is sufficiently small, the infection started with a single infected\nparticle survives indefinitely with positive probability.",
            "author": [
                "Alexander Drewitz",
                "Gioele Gallo",
                "Peter Gracar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03045v1",
                "http://arxiv.org/pdf/2311.03045v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "Primary 60K35, secondary 60G55"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03042v1",
            "title": "Progress in the Partial-Wave Analysis Methods at COMPASS",
            "updated": "2023-11-06T11:32:49Z",
            "published": "2023-11-06T11:32:49Z",
            "summary": "We study the excitation spectrum of light and strange mesons in diffractive\nscattering. We identify different hadron resonances through partial wave\nanalysis, which inherently relies on analysis models. Besides statistical\nuncertainties, the model dependence of the analysis introduces dominant\nsystematic uncertainties. We discuss several of their sources for the\n$\\pi^-\\pi^-\\pi^+$ and $K^0_S K^-$ final states and present methods to reduce\nthem. We have developed a new approach exploiting a-priori knowledge of signal\ncontinuity over adjacent final-state-mass bins to stably fit a large pool of\npartial-waves to our data, allowing a clean identification of very small\nsignals in our large data sets. For two-body final states of scalar particles,\nsuch as $K^0_S K^-$, mathematical ambiguities in the partial-wave decomposition\nlead to the same intensity distribution for different combinations of amplitude\nvalues. We will discuss these ambiguities and present solutions to resolve or\nat least reduce the number of possible solutions. Resolving these issues will\nallow for a complementary analysis of the $a_J$-like resonance sector in these\ntwo final states.",
            "author": [
                "Florian Markus Kaspar",
                "Julien Beckers",
                "Jakob Knollm\u00fcller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03042v1",
                "http://arxiv.org/pdf/2311.03042v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03035v1",
            "title": "GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation",
            "updated": "2023-11-06T11:14:19Z",
            "published": "2023-11-06T11:14:19Z",
            "summary": "Vision Transformers (ViTs) have revolutionized the field of computer vision,\nyet their deployments on resource-constrained devices remain challenging due to\nhigh computational demands. To expedite pre-trained ViTs, token pruning and\ntoken merging approaches have been developed, which aim at reducing the number\nof tokens involved in the computation. However, these methods still have some\nlimitations, such as image information loss from pruned tokens and inefficiency\nin the token-matching process. In this paper, we introduce a novel Graph-based\nToken Propagation (GTP) method to resolve the challenge of balancing model\nefficiency and information preservation for efficient ViTs. Inspired by graph\nsummarization algorithms, GTP meticulously propagates less significant tokens'\ninformation to spatially and semantically connected tokens that are of greater\nimportance. Consequently, the remaining few tokens serve as a summarization of\nthe entire token graph, allowing the method to reduce computational complexity\nwhile preserving essential information of eliminated tokens. Combined with an\ninnovative token selection strategy, GTP can efficiently identify image tokens\nto be propagated. Extensive experiments have validated GTP's effectiveness,\ndemonstrating both efficiency and performance improvements. Specifically, GTP\ndecreases the computational complexity of both DeiT-S and DeiT-B by up to 26%\nwith only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and\nremarkably surpasses the state-of-the-art token merging method on various\nbackbones at an even faster inference speed. The source code is available at\nhttps://github.com/Ackesnal/GTP-ViT.",
            "author": [
                "Xuwei Xu",
                "Sen Wang",
                "Yudong Chen",
                "Yanping Zheng",
                "Zhewei Wei",
                "Jiajun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03035v1",
                "http://arxiv.org/pdf/2311.03035v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03034v1",
            "title": "Byzantine Consensus in Abstract MAC Layer",
            "updated": "2023-11-06T11:14:01Z",
            "published": "2023-11-06T11:14:01Z",
            "summary": "This paper studies the design of Byzantine consensus algorithms in an\n\\textit{asynchronous }single-hop network equipped with the \"abstract MAC layer\"\n[DISC09], which captures core properties of modern wireless MAC protocols.\nNewport [PODC14], Newport and Robinson [DISC18], and Tseng and Zhang [PODC22]\nstudy crash-tolerant consensus in the model. In our setting, a Byzantine faulty\nnode may behave arbitrarily, but it cannot break the guarantees provided by the\nunderlying abstract MAC layer. To our knowledge, we are the first to study\nByzantine faults in this model.\n  We harness the power of the abstract MAC layer to develop a Byzantine\napproximate consensus algorithm and a Byzantine randomized binary consensus\nalgorithm. Both of our algorithms require \\textit{only} the knowledge of the\nupper bound on the number of faulty nodes $f$, and do \\textit{not} require the\nknowledge of the number of nodes $n$. This demonstrates the \"power\" of the\nabstract MAC layer, as consensus algorithms in traditional message-passing\nmodels require the knowledge of \\textit{both} $n$ and $f$. Additionally, we\nshow that it is necessary to know $f$ in order to reach consensus. Hence, from\nthis perspective, our algorithms require the minimal knowledge.\n  The lack of knowledge of $n$ brings the challenge of identifying a quorum\nexplicitly, which is a common technique in traditional message-passing\nalgorithms. A key technical novelty of our algorithms is to identify \"implicit\nquorums\" which have the necessary information for reaching consensus. The\nquorums are implicit because nodes do not know the identity of the quorums --\nsuch notion is only used in the analysis.",
            "author": [
                "Lewis Tseng",
                "Callie Sardina"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03034v1",
                "http://arxiv.org/pdf/2311.03034v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03019v1",
            "title": "Optimal Control of Linear Cost Networks",
            "updated": "2023-11-06T10:58:03Z",
            "published": "2023-11-06T10:58:03Z",
            "summary": "We present a method for optimal control with respect to a linear cost\nfunction for positive linear systems with coupled input constraints. We show\nthat the optimal cost function and resulting sparse state feedback for these\nsystems can be computed by linear programming. Our framework admits a range of\nnetwork routing problems with underlying linear dynamics. These dynamics can be\nused to model traditional graph-theoretical problems like shortest path as a\nspecial case, but can also capture more complex behaviors. We provide an\nasynchronous and distributed value iteration algorithm for obtaining the\noptimal cost function and control law.",
            "author": [
                "David Ohlin",
                "Emma Tegling",
                "Anders Rantzer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03019v1",
                "http://arxiv.org/pdf/2311.03019v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03007v1",
            "title": "Exploiting spatial group error and synchrony for a unicycle tracking\n  controller",
            "updated": "2023-11-06T10:22:27Z",
            "published": "2023-11-06T10:22:27Z",
            "summary": "Trajectory tracking for the kinematic unicycle has been heavily studied for\nseveral decades. The unicycle admits a natural $\\SE(2)$ symmetry, a key\nstructure exploited in many of the most successful nonlinear controllers in the\nliterature. To the author's knowledge however, all prior work has used a\nbody-fixed, or left-invariant, group error formulation for the study of the\ntracking problem. In this paper, we consider the spatial, or right-invariant,\ngroup error in the design of a tracking controller for the kinematic unicycle.\nWe provide a physical interpretation of the right-invariant error and go on to\nshow that the associated error dynamics are drift-free, a property that is not\ntrue for the body-fixed error. We exploit this property to propose a simple\nnonlinear control scheme for the kinematic unicycle and prove almost-global\nasymptotic stability of this control scheme for a class of persistently\nexciting trajectories. We also verify performance of this control scheme in\nsimulation for an example trajectory.",
            "author": [
                "Matthew Hampsey",
                "Pieter van Goor",
                "Robert Mahony"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03007v1",
                "http://arxiv.org/pdf/2311.03007v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02999v1",
            "title": "Bayesian Time-Lapse Full Waveform Inversion using Hamiltonian Monte\n  Carlo",
            "updated": "2023-11-06T10:06:11Z",
            "published": "2023-11-06T10:06:11Z",
            "summary": "Time-lapse images carry out important information about dynamic changes in\nEarth's interior which can be inferred using different Full Waveform Inversion\n(FWI) schemes. The estimation process is performed by manipulating more than\none seismic dataset, associated with the baseline and monitors surveys. The\ntime-lapse variations can be so minute and localised that quantifying the\nuncertainties becomes fundamental to assessing the reliability of the results.\nThe Bayesian formulation of the FWI problem naturally provides levels of\nconfidence in the solution, but evaluating the uncertainty of time-lapse\nseismic inversion remains a challenge due to the ill-posedness and high\ndimensionality of the problem. The Hamiltonian Monte Carlo (HMC) can be used to\neffectively sample over high dimensional distributions with affordable\ncomputational efforts. In this context, we propose a probabilistic Bayesian\nsequential approach for time-lapse FWI using the HMC method. Our approach\nrelies on the integration of the baseline survey information as prior knowledge\nin the monitor estimation. We compare the proposed methodology with a parallel\nscheme in perfect and perturbed acquisition geometry scenarios. We also\ninvestigate the correlation effect between baseline and monitor samples in the\npropagated uncertainties. The results show that our strategy provides accurate\ntimes-lapse estimates with errors of similar magnitude to the parallel\nmethodology.",
            "author": [
                "Paulo Douglas S. de Lima",
                "Mauro S. Ferreira",
                "Gilberto Corso",
                "Jo\u00e3o M. de Ara\u00fajo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02999v1",
                "http://arxiv.org/pdf/2311.02999v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph",
                "cond-mat.stat-mech",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02996v1",
            "title": "Visual-information-driven model for crowd simulation using temporal\n  convolutional network",
            "updated": "2023-11-06T09:58:04Z",
            "published": "2023-11-06T09:58:04Z",
            "summary": "Crowd simulations play a pivotal role in building design, influencing both\nuser experience and public safety. While traditional knowledge-driven models\nhave their merits, data-driven crowd simulation models promise to bring a new\ndimension of realism to these simulations. However, most of the existing\ndata-driven models are designed for specific geometries, leading to poor\nadaptability and applicability. A promising strategy for enhancing the\nadaptability and realism of data-driven crowd simulation models is to\nincorporate visual information, including the scenario geometry and pedestrian\nlocomotion. Consequently, this paper proposes a novel visual-information-driven\n(VID) crowd simulation model. The VID model predicts the pedestrian velocity at\nthe next time step based on the prior social-visual information and motion data\nof an individual. A radar-geometry-locomotion method is established to extract\nthe visual information of pedestrians. Moreover, a temporal convolutional\nnetwork (TCN)-based deep learning model, named social-visual TCN, is developed\nfor velocity prediction. The VID model is tested on three public pedestrian\nmotion datasets with distinct geometries, i.e., corridor, corner, and\nT-junction. Both qualitative and quantitative metrics are employed to evaluate\nthe VID model, and the results highlight the improved adaptability of the model\nacross all three geometric scenarios. Overall, the proposed method demonstrates\neffectiveness in enhancing the adaptability of data-driven crowd models.",
            "author": [
                "Xuanwen Liang",
                "Eric Wai Ming Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02996v1",
                "http://arxiv.org/pdf/2311.02996v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02993v1",
            "title": "The nonlinear stationary differential equations with fractional order\n  $1<\u03b1<2$ on metric star graphs",
            "updated": "2023-11-06T09:56:22Z",
            "published": "2023-11-06T09:56:22Z",
            "summary": "In this paper we consider nonlinear stationary fractional-in-space\ndifferential equations with order $1<\\alpha<2$ on the metric star graph with\nthree finite bonds. At the branched point of the star graph we put the weight\ncontinuity and the generalized Kirchhoff rule. It is found the exact solutions\nof nonlinear stationary fractional equations on the star graph. These can be\nextended to the star graphs with any number of bonds.",
            "author": [
                "K. K. Sabirov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02993v1",
                "http://arxiv.org/pdf/2311.02993v1"
            ],
            "primary_category": "math.CA",
            "category": [
                "math.CA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03415v1",
            "title": "PowerFlowNet: Leveraging Message Passing GNNs for Improved Power Flow\n  Approximation",
            "updated": "2023-11-06T09:44:00Z",
            "published": "2023-11-06T09:44:00Z",
            "summary": "Accurate and efficient power flow (PF) analysis is crucial in modern\nelectrical networks' efficient operation and planning. Therefore, there is a\nneed for scalable algorithms capable of handling large-scale power networks\nthat can provide accurate and fast solutions. Graph Neural Networks (GNNs) have\nemerged as a promising approach for enhancing the speed of PF approximations by\nleveraging their ability to capture distinctive features from the underlying\npower network graph. In this study, we introduce PowerFlowNet, a novel GNN\narchitecture for PF approximation that showcases similar performance with the\ntraditional Newton-Raphson method but achieves it 4 times faster in the simple\nIEEE 14-bus system and 145 times faster in the realistic case of the French\nhigh voltage network (6470rte). Meanwhile, it significantly outperforms other\ntraditional approximation methods, such as the DC relaxation method, in terms\nof performance and execution time; therefore, making PowerFlowNet a highly\npromising solution for real-world PF analysis. Furthermore, we verify the\nefficacy of our approach by conducting an in-depth experimental evaluation,\nthoroughly examining the performance, scalability, interpretability, and\narchitectural dependability of PowerFlowNet. The evaluation provides insights\ninto the behavior and potential applications of GNNs in power system analysis.",
            "author": [
                "Nan Lin",
                "Stavros Orfanoudakis",
                "Nathan Ordonez Cardenas",
                "Juan S. Giraldo",
                "Pedro P. Vergara"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03415v1",
                "http://arxiv.org/pdf/2311.03415v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02969v1",
            "title": "Planar graphs without $5^{-}$-cycles at distance less than $3$ are\n  $(\\mathcal{I}, \\mathcal{F})$-colorable",
            "updated": "2023-11-06T09:14:43Z",
            "published": "2023-11-06T09:14:43Z",
            "summary": "A graph is $(\\mathcal{I}, \\mathcal{F})$-colorable if its vertex set can be\npartitioned into two subsets, one of which is an independent set, and the other\ninduces a forest. In this paper, we prove that every planar graph without\n$5^{-}$-cycles at distance less than $3$ is $(\\mathcal{I},\n\\mathcal{F})$-colorable.",
            "author": [
                "Zhen He",
                "Tao Wang",
                "Xiaojing Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02969v1",
                "http://arxiv.org/pdf/2311.02969v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02968v1",
            "title": "A new approach for constructing graph being determined by their\n  generalized $Q$-spectrum",
            "updated": "2023-11-06T09:14:28Z",
            "published": "2023-11-06T09:14:28Z",
            "summary": "Given a graph $G$, we have the adjacency matrix $A(G)$ and degree diagonal\nmatrix $D(G)$. The $Q$-spectrum is the all eigenvalues of $Q$-matrix\n$Q(G)=A(G)+D(G)$. A class of graphs is determined by their generalized\n$Q$-spectrum (DGQS for short) if any two graphs among the class have the same\n$Q$-spectrum and so do their complement imply that they are isomorphic. In\n[11], the authors provides a new way to construct $DGQS$ graphs by considering\nthe rooted product graphs $G\\circ P_{k}$ and they prove when $k=2,3$, $G\\circ\nP_{k}$ is $DGQS$ for a special graph $G$. In this paper, we will prove that\nunder the same conditions for $G$, the conclusion is true for any positive\ninteger $k$.",
            "author": [
                "Liwen Gao",
                "Xuejun Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02968v1",
                "http://arxiv.org/pdf/2311.02968v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02967v1",
            "title": "Non-intrusive model combination for learning dynamical systems",
            "updated": "2023-11-06T09:11:23Z",
            "published": "2023-11-06T09:11:23Z",
            "summary": "In data-driven modelling of complex dynamic processes, it is often desirable\nto combine different classes of models to enhance performance. Examples include\ncoupled models of different fidelities, or hybrid models based on physical\nknowledge and data-driven strategies. A key limitation of the broad adoption of\nmodel combination in applications is intrusiveness: training combined models\ntypically requires significant modifications to the learning algorithm\nimplementations, which may often be already well-developed and optimized for\nindividual model spaces. In this work, we propose an iterative, non-intrusive\nmethodology to combine two model spaces to learn dynamics from data. We show\nthat this can be understood, at least in the linear setting, as finding the\noptimal solution in the direct sum of the two hypothesis spaces, while\nleveraging only the projection operators in each individual space. Hence, the\nproposed algorithm can be viewed as iterative projections, for which we can\nobtain estimates on its convergence properties. To highlight the extensive\napplicability of our framework, we conduct numerical experiments in various\nproblem settings, with particular emphasis on various hybrid models based on\nthe Koopman operator approach.",
            "author": [
                "Shiqi Wu",
                "Ludovic Chamoin",
                "Qianxiao Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02967v1",
                "http://arxiv.org/pdf/2311.02967v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02963v1",
            "title": "Cyclic structure, vertex degree and number of linear vertices in Minimal\n  Strong Digraphs",
            "updated": "2023-11-06T09:05:27Z",
            "published": "2023-11-06T09:05:27Z",
            "summary": "Minimal Strong Digraphs (MSDs) can be regarded as a generalization of the\nconcept of tree to directed graphs. Their cyclic structure and some spectral\nproperties have been studied in several articles. In this work, we further\nstudy some properties of MSDs that have to do with bounding the length of the\nlongest cycle (regarding the number of linear vertices, or the maximal in- or\noutdegree of vertices); studying whatever consequences from the spectral point\nof view; and giving some insight about the circumstances in which an efficient\nalgorithm to find the longest cycle contained in an MSD can be formulated.\nAmong other properties, we show that the number of linear vertices contained in\nan MSD is greater or equal to the maximal (resp. minimal) in- or outdegree of\nany vertex of the MSD and that the maximal length of a cycle contained in an\nMSD is lesser or equal to 2n-m, where n,m are the order and the size of the MSD\nrespectively; we have found a bound for the coefficients of the characteristic\npolynomial of an MSD, extending the result in a previous work of Lacalle,\nMarijuan and Pozo, and finally, we prove that computing the longest cycle\ncontained in an MSD is an NP-Hard problem.",
            "author": [
                "Miguel Arcos Argudo",
                "Jesus Lacalle",
                "Luis Miguel Pozo Coronado"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02963v1",
                "http://arxiv.org/pdf/2311.02963v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C75 (Primary), 05C20, 05C7, 05C38, 05C40, 68Q25 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02962v1",
            "title": "Retrieval-Augmented Code Generation for Universal Information Extraction",
            "updated": "2023-11-06T09:03:21Z",
            "published": "2023-11-06T09:03:21Z",
            "summary": "Information Extraction (IE) aims to extract structural knowledge (e.g.,\nentities, relations, events) from natural language texts, which brings\nchallenges to existing methods due to task-specific schemas and complex text\nexpressions. Code, as a typical kind of formalized language, is capable of\ndescribing structural knowledge under various schemas in a universal way. On\nthe other hand, Large Language Models (LLMs) trained on both codes and texts\nhave demonstrated powerful capabilities of transforming texts into codes, which\nprovides a feasible solution to IE tasks. Therefore, in this paper, we propose\na universal retrieval-augmented code generation framework based on LLMs, called\nCode4UIE, for IE tasks. Specifically, Code4UIE adopts Python classes to define\ntask-specific schemas of various structural knowledge in a universal way. By so\ndoing, extracting knowledge under these schemas can be transformed into\ngenerating codes that instantiate the predefined Python classes with the\ninformation in texts. To generate these codes more precisely, Code4UIE adopts\nthe in-context learning mechanism to instruct LLMs with examples. In order to\nobtain appropriate examples for different tasks, Code4UIE explores several\nexample retrieval strategies, which can retrieve examples semantically similar\nto the given texts. Extensive experiments on five representative IE tasks\nacross nine datasets demonstrate the effectiveness of the Code4UIE framework.",
            "author": [
                "Yucan Guo",
                "Zixuan Li",
                "Xiaolong Jin",
                "Yantao Liu",
                "Yutao Zeng",
                "Wenxuan Liu",
                "Xiang Li",
                "Pan Yang",
                "Long Bai",
                "Jiafeng Guo",
                "Xueqi Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02962v1",
                "http://arxiv.org/pdf/2311.02962v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02960v1",
            "title": "Understanding Deep Representation Learning via Layerwise Feature\n  Compression and Discrimination",
            "updated": "2023-11-06T09:00:38Z",
            "published": "2023-11-06T09:00:38Z",
            "summary": "Over the past decade, deep learning has proven to be a highly effective tool\nfor learning meaningful features from raw data. However, it remains an open\nquestion how deep networks perform hierarchical feature learning across layers.\nIn this work, we attempt to unveil this mystery by investigating the structures\nof intermediate features. Motivated by our empirical findings that linear\nlayers mimic the roles of deep layers in nonlinear networks for feature\nlearning, we explore how deep linear networks transform input data into output\nby investigating the output (i.e., features) of each layer after training in\nthe context of multi-class classification problems. Toward this goal, we first\ndefine metrics to measure within-class compression and between-class\ndiscrimination of intermediate features, respectively. Through theoretical\nanalysis of these two metrics, we show that the evolution of features follows a\nsimple and quantitative pattern from shallow to deep layers when the input data\nis nearly orthogonal and the network weights are minimum-norm, balanced, and\napproximate low-rank: Each layer of the linear network progressively compresses\nwithin-class features at a geometric rate and discriminates between-class\nfeatures at a linear rate with respect to the number of layers that data have\npassed through. To the best of our knowledge, this is the first quantitative\ncharacterization of feature evolution in hierarchical representations of deep\nlinear networks. Empirically, our extensive experiments not only validate our\ntheoretical results numerically but also reveal a similar pattern in deep\nnonlinear networks which aligns well with recent empirical studies. Moreover,\nwe demonstrate the practical implications of our results in transfer learning.\nOur code is available at \\url{https://github.com/Heimine/PNC_DLN}.",
            "author": [
                "Peng Wang",
                "Xiao Li",
                "Can Yaras",
                "Zhihui Zhu",
                "Laura Balzano",
                "Wei Hu",
                "Qing Qu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02960v1",
                "http://arxiv.org/pdf/2311.02960v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02956v1",
            "title": "In-Context Learning for Knowledge Base Question Answering for Unmanned\n  Systems based on Large Language Models",
            "updated": "2023-11-06T08:52:11Z",
            "published": "2023-11-06T08:52:11Z",
            "summary": "Knowledge Base Question Answering (KBQA) aims to answer factoid questions\nbased on knowledge bases. However, generating the most appropriate knowledge\nbase query code based on Natural Language Questions (NLQ) poses a significant\nchallenge in KBQA. In this work, we focus on the CCKS2023 Competition of\nQuestion Answering with Knowledge Graph Inference for Unmanned Systems.\nInspired by the recent success of large language models (LLMs) like ChatGPT and\nGPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL)\ngeneration framework to generate the most appropriate CQL based on the given\nNLQ. Our generative framework contains six parts: an auxiliary model predicting\nthe syntax-related information of CQL based on the given NLQ, a proper noun\nmatcher extracting proper nouns from the given NLQ, a demonstration example\nselector retrieving similar examples of the input sample, a prompt constructor\ndesigning the input template of ChatGPT, a ChatGPT-based generation model\ngenerating the CQL, and an ensemble model to obtain the final answers from\ndiversified outputs. With our ChatGPT-based CQL generation framework, we\nachieved the second place in the CCKS 2023 Question Answering with Knowledge\nGraph Inference for Unmanned Systems competition, achieving an F1-score of\n0.92676.",
            "author": [
                "Yunlong Chen",
                "Yaming Zhang",
                "Jianfei Yu",
                "Li Yang",
                "Rui Xia"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-7224-1_26",
                "http://arxiv.org/abs/2311.02956v1",
                "http://arxiv.org/pdf/2311.02956v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02938v1",
            "title": "Contrastive Multi-Level Graph Neural Networks for Session-based\n  Recommendation",
            "updated": "2023-11-06T08:11:32Z",
            "published": "2023-11-06T08:11:32Z",
            "summary": "Session-based recommendation (SBR) aims to predict the next item at a certain\ntime point based on anonymous user behavior sequences. Existing methods\ntypically model session representation based on simple item transition\ninformation. However, since session-based data consists of limited users'\nshort-term interactions, modeling session representation by capturing fixed\nitem transition information from a single dimension suffers from data sparsity.\nIn this paper, we propose a novel contrastive multi-level graph neural networks\n(CM-GNN) to better exploit complex and high-order item transition information.\nSpecifically, CM-GNN applies local-level graph convolutional network (L-GCN)\nand global-level network (G-GCN) on the current session and all the sessions\nrespectively, to effectively capture pairwise relations over all the sessions\nby aggregation strategy. Meanwhile, CM-GNN applies hyper-level graph\nconvolutional network (H-GCN) to capture high-order information among all the\nitem transitions. CM-GNN further introduces an attention-based fusion module to\nlearn pairwise relation-based session representation by fusing the item\nrepresentations generated by L-GCN and G-GCN. CM-GNN averages the item\nrepresentations obtained by H-GCN to obtain high-order relation-based session\nrepresentation. Moreover, to convert the high-order item transition information\ninto the pairwise relation-based session representation, CM-GNN maximizes the\nmutual information between the representations derived from the fusion module\nand the average pool layer by contrastive learning paradigm. We conduct\nextensive experiments on multiple widely used benchmark datasets to validate\nthe efficacy of the proposed method. The encouraging results demonstrate that\nour proposed method outperforms the state-of-the-art SBR techniques.",
            "author": [
                "Fuyun Wang",
                "Xingyu Gao",
                "Zhenyu Chen",
                "Lei Lyu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMM.2023.3250087",
                "http://arxiv.org/abs/2311.02938v1",
                "http://arxiv.org/pdf/2311.02938v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02921v3",
            "title": "Edge2Node: Reducing Edge Prediction to Node Classification",
            "updated": "2023-11-22T17:26:08Z",
            "published": "2023-11-06T07:28:16Z",
            "summary": "Despite the success of graph neural network models in node classification,\nedge prediction (the task of predicting missing or potential links between\nnodes in a graph) remains a challenging problem for these models. A common\napproach for edge prediction is to first obtain the embeddings of two nodes,\nand then a predefined scoring function is used to predict the existence of an\nedge between the two nodes. Here, we introduce a preliminary idea called\nEdge2Node which suggests to directly obtain an embedding for each edge, without\nthe need for a scoring function. This idea wants to create a new graph H based\non the graph G given for the edge prediction task, and then suggests reducing\nthe edge prediction task on G to a node classification task on H. We anticipate\nthat this introductory method could stimulate further investigations for edge\nprediction task.",
            "author": [
                "Zahed Rahmati"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02921v3",
                "http://arxiv.org/pdf/2311.02921v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02914v1",
            "title": "Tight upper bound on the clique size in the square of 2-degenerate\n  graphs",
            "updated": "2023-11-06T07:04:33Z",
            "published": "2023-11-06T07:04:33Z",
            "summary": "The {\\em square} of a graph $G$, denoted $G^2$, has the same vertex set as\n$G$ and has an edge between two vertices if the distance between them in $G$ is\nat most $2$. In general, $\\Delta(G) + 1 \\leq \\chi(G^2) \\leq \\Delta(G)^2 +1$ for\nevery graph $G$. Charpentier [1] asked whether $\\chi(G^2) \\leq 2 \\Delta(G)$ if\n$mad(G) < 4$. But Hocquard, Kim, and Pierron [6] answered his question\nnegatively. For every even value of $\\Delta(G)$, they constructed a\n2-degenerate graph $G$ such that $\\omega(G^2) = \\frac{5}{2} \\Delta(G)$. Note\nthat if $G$ is a 2-degenerate graph, then $mad(G) < 4$. Thus, we have that \\[\n{\\displaystyle \\frac{5}{2} \\Delta(G) \\leq \\max \\{\\chi(G^2) : G \\mbox{ is a\n2-degenerate graph} \\} \\leq 3 \\Delta(G) +1}. \\] So, it was naturally asked\nwhether there exists a constant $D_0$ such that $\\chi(G^2) \\leq \\frac{5}{2}\n\\Delta(G)$ if $G$ is a 2-degenerate graph with $\\Delta(G) \\geq D_0$. Recently\nCranston and Yu [3] showed that $\\omega(G^2) \\leq \\frac{5}{2} \\Delta(G)+72$ if\n$G$ is a 2-degenerate graph, and $\\omega(G^2) \\leq \\frac{5}{2} \\Delta(G)+60$ if\n$G$ is a 2-degenerate graph with $\\Delta(G) \\geq 1729$. We show that there\nexists a constant $D_0$ such that $\\omega(G^2) \\leq \\frac{5}{2} \\Delta(G)$ if\n$G$ is a 2-degenerate graph with $\\Delta(G) \\geq D_0$. This upper bound on\n$\\omega(G^2)$ is tight by the construction in [6].",
            "author": [
                "Seog-Jin Kim",
                "Xiaopan Lian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02914v1",
                "http://arxiv.org/pdf/2311.02914v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02909v1",
            "title": "Distributed Matrix-Based Sampling for Graph Neural Network Training",
            "updated": "2023-11-06T06:40:43Z",
            "published": "2023-11-06T06:40:43Z",
            "summary": "The primary contribution of this paper is new methods for reducing\ncommunication in the sampling step for distributed GNN training. Here, we\npropose a matrix-based bulk sampling approach that expresses sampling as a\nsparse matrix multiplication (SpGEMM) and samples multiple minibatches at once.\nWhen the input graph topology does not fit on a single device, our method\ndistributes the graph and use communication-avoiding SpGEMM algorithms to scale\nGNN minibatch sampling, enabling GNN training on much larger graphs than those\nthat can fit into a single device memory. When the input graph topology (but\nnot the embeddings) fits in the memory of one GPU, our approach (1) performs\nsampling without communication, (2) amortizes the overheads of sampling a\nminibatch, and (3) can represent multiple sampling algorithms by simply using\ndifferent matrix constructions. In addition to new methods for sampling, we\nshow that judiciously replicating feature data with a simple all-to-all\nexchange can outperform current methods for the feature extraction step in\ndistributed GNN training. We provide experimental results on the largest Open\nGraph Benchmark (OGB) datasets on $128$ GPUs, and show that our pipeline is\n$2.5\\times$ faster Quiver (a distributed extension to PyTorch-Geometric) on a\n$3$-layer GraphSAGE network. On datasets outside of OGB, we show a $8.46\\times$\nspeedup on $128$ GPUs in-per epoch time. Finally, we show scaling when the\ngraph is distributed across GPUs and scaling for both node-wise and layer-wise\nsampling algorithms",
            "author": [
                "Alok Tripathy",
                "Katherine Yelick",
                "Aydin Buluc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02909v1",
                "http://arxiv.org/pdf/2311.02909v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02903v1",
            "title": "HDGL: A hierarchical dynamic graph representation learning model for\n  brain disorder classification",
            "updated": "2023-11-06T06:29:23Z",
            "published": "2023-11-06T06:29:23Z",
            "summary": "The human brain can be considered as complex networks, composed of various\nregions that continuously exchange their information with each other, forming\nthe brain network graph, from which nodes and edges are extracted using\nresting-state functional magnetic resonance imaging (rs-fMRI). Therefore, this\ngraph can potentially depict abnormal patterns that have emerged under the\ninfluence of brain disorders. So far, numerous studies have attempted to find\nembeddings for brain network graphs and subsequently classify samples with\nbrain disorders from healthy ones, which include limitations such as: not\nconsidering the relationship between samples, not utilizing phenotype\ninformation, lack of temporal analysis, using static functional connectivity\n(FC) instead of dynamic ones and using a fixed graph structure. We propose a\nhierarchical dynamic graph representation learning (HDGL) model, which is the\nfirst model designed to address all the aforementioned challenges. HDGL\nconsists of two levels, where at the first level, it constructs brain network\ngraphs and learns their spatial and temporal embeddings, and at the second\nlevel, it forms population graphs and performs classification after embedding\nlearning. Furthermore, based on how these two levels are trained, four methods\nhave been introduced, some of which are suggested for reducing memory\ncomplexity. We evaluated the performance of the proposed model on the ABIDE and\nADHD-200 datasets, and the results indicate the improvement of this model\ncompared to several state-of-the-art models in terms of various evaluation\nmetrics.",
            "author": [
                "Parniyan Jalali",
                "Mehran Safayani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02903v1",
                "http://arxiv.org/pdf/2311.02903v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02896v1",
            "title": "Recasting the Hazrat Conjecture: Relating Shift Equivalence to Graded\n  Morita Equivalence",
            "updated": "2023-11-06T06:03:40Z",
            "published": "2023-11-06T06:03:40Z",
            "summary": "Let $E$ and $F$ be finite graphs with no sinks, and $k$ any field. We show\nthat shift equivalence of the adjacency matrices $A_E$ and $A_F$, together with\nan additional compatibility condition, implies that the Leavitt path algebras\n$L_k(E)$ and $L_k(F)$ are graded Morita equivalent. Along the way, we build a\nnew type of $L_k(E)$--$L_k(F)$-bimodule (a bridging bimodule), which we use to\nestablish the graded equivalence.",
            "author": [
                "Gene Abrams",
                "Efren Ruiz",
                "Mark Tomforde"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02896v1",
                "http://arxiv.org/pdf/2311.02896v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "16S88, 46L35, 37B10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03411v1",
            "title": "ViDa: Visualizing DNA hybridization trajectories with\n  biophysics-informed deep graph embeddings",
            "updated": "2023-11-06T05:27:29Z",
            "published": "2023-11-06T05:27:29Z",
            "summary": "Visualization tools can help synthetic biologists and molecular programmers\nunderstand the complex reactive pathways of nucleic acid reactions, which can\nbe designed for many potential applications and can be modelled using a\ncontinuous-time Markov chain (CTMC). Here we present ViDa, a new visualization\napproach for DNA reaction trajectories that uses a 2D embedding of the\nsecondary structure state space underlying the CTMC model. To this end, we\nintegrate a scattering transform of the secondary structure adjacency, a\nvariational autoencoder, and a nonlinear dimensionality reduction method. We\naugment the training loss with domain-specific supervised terms that capture\nboth thermodynamic and kinetic features. We assess ViDa on two well-studied DNA\nhybridization reactions. Our results demonstrate that the domain-specific\nfeatures lead to significant quality improvements over the state-of-the-art in\nDNA state space visualization, successfully separating different folding\npathways and thus providing useful insights into dominant reaction mechanisms.",
            "author": [
                "Chenwei Zhang",
                "Jordan Lovrod",
                "Boyan Beronov",
                "Khanh Dao Duc",
                "Anne Condon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03411v1",
                "http://arxiv.org/pdf/2311.03411v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.AI",
                "cs.HC",
                "cs.LG",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02884v1",
            "title": "Deep Learning-Empowered Semantic Communication Systems with a Shared\n  Knowledge Base",
            "updated": "2023-11-06T05:25:31Z",
            "published": "2023-11-06T05:25:31Z",
            "summary": "Deep learning-empowered semantic communication is regarded as a promising\ncandidate for future 6G networks. Although existing semantic communication\nsystems have achieved superior performance compared to traditional methods, the\nend-to-end architecture adopted by most semantic communication systems is\nregarded as a black box, leading to the lack of explainability. To tackle this\nissue, in this paper, a novel semantic communication system with a shared\nknowledge base is proposed for text transmissions. Specifically, a textual\nknowledge base constructed by inherently readable sentences is introduced into\nour system. With the aid of the shared knowledge base, the proposed system\nintegrates the message and corresponding knowledge from the shared knowledge\nbase to obtain the residual information, which enables the system to transmit\nfewer symbols without semantic performance degradation. In order to make the\nproposed system more reliable, the semantic self-information and the source\nentropy are mathematically defined based on the knowledge base. Furthermore,\nthe knowledge base construction algorithm is developed based on a\nsimilarity-comparison method, in which a pre-configured threshold can be\nleveraged to control the size of the knowledge base. Moreover, the simulation\nresults have demonstrated that the proposed approach outperforms existing\nbaseline methods in terms of transmitted data size and sentence similarity.",
            "author": [
                "Peng Yi",
                "Yang Cao",
                "Xin Kang",
                "Ying-Chang Liang"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TWC.2023.3330744",
                "http://arxiv.org/abs/2311.02884v1",
                "http://arxiv.org/pdf/2311.02884v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02880v1",
            "title": "MultiSPANS: A Multi-range Spatial-Temporal Transformer Network for\n  Traffic Forecast via Structural Entropy Optimization",
            "updated": "2023-11-06T05:19:06Z",
            "published": "2023-11-06T05:19:06Z",
            "summary": "Traffic forecasting is a complex multivariate time-series regression task of\nparamount importance for traffic management and planning. However, existing\napproaches often struggle to model complex multi-range dependencies using local\nspatiotemporal features and road network hierarchical knowledge. To address\nthis, we propose MultiSPANS. First, considering that an individual recording\npoint cannot reflect critical spatiotemporal local patterns, we design\nmulti-filter convolution modules for generating informative ST-token embeddings\nto facilitate attention computation. Then, based on ST-token and\nspatial-temporal position encoding, we employ the Transformers to capture\nlong-range temporal and spatial dependencies. Furthermore, we introduce\nstructural entropy theory to optimize the spatial attention mechanism.\nSpecifically, The structural entropy minimization algorithm is used to generate\noptimal road network hierarchies, i.e., encoding trees. Based on this, we\npropose a relative structural entropy-based position encoding and a multi-head\nattention masking scheme based on multi-layer encoding trees. Extensive\nexperiments demonstrate the superiority of the presented framework over several\nstate-of-the-art methods in real-world traffic datasets, and the longer\nhistorical windows are effectively utilized. The code is available at\nhttps://github.com/SELGroup/MultiSPANS.",
            "author": [
                "Dongcheng Zou",
                "Senzhang Wang",
                "Xuefeng Li",
                "Hao Peng",
                "Yuandong Wang",
                "Chunyang Liu",
                "Kehua Sheng",
                "Bo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02880v1",
                "http://arxiv.org/pdf/2311.02880v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14690v1",
            "title": "Evolutionary City: Towards a Flexible, Agile and Symbiotic System",
            "updated": "2023-11-06T05:10:33Z",
            "published": "2023-11-06T05:10:33Z",
            "summary": "Urban growth sometimes leads to rigid infrastructure that struggles to adapt\nto changing demand. This paper introduces a novel approach, aiming to enable\ncities to evolve and respond more effectively to such dynamic demand. It\nidentifies the limitations arising from the complexity and inflexibility of\nexisting urban systems. A framework is presented for enhancing the city's\nadaptability perception through advanced sensing technologies, conducting\nparallel simulation via graph-based techniques, and facilitating autonomous\ndecision-making across domains through decentralized and autonomous\norganization and operation. Notably, a symbiotic mechanism is employed to\nimplement these technologies practically, thereby making urban management more\nagile and responsive. In the case study, we explore how this approach can\noptimize traffic flow by adjusting lane allocations. This case not only\nenhances traffic efficiency but also reduces emissions. The proposed\nevolutionary city offers a new perspective on sustainable urban development,\nhighliting the importance of integrated intelligence within urban systems.",
            "author": [
                "Xi Chen",
                "Wei Hu",
                "Jingru Yu",
                "Ding Wang",
                "Shengyue Yao",
                "Yilun Lin",
                "Fei-Yue Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14690v1",
                "http://arxiv.org/pdf/2311.14690v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03409v1",
            "title": "Visualizing DNA reaction trajectories with deep graph embedding\n  approaches",
            "updated": "2023-11-06T05:06:35Z",
            "published": "2023-11-06T05:06:35Z",
            "summary": "Synthetic biologists and molecular programmers design novel nucleic acid\nreactions, with many potential applications. Good visualization tools are\nneeded to help domain experts make sense of the complex outputs of folding\npathway simulations of such reactions. Here we present ViDa, a new approach for\nvisualizing DNA reaction folding trajectories over the energy landscape of\nsecondary structures. We integrate a deep graph embedding model with common\ndimensionality reduction approaches, to map high-dimensional data onto 2D\nEuclidean space. We assess ViDa on two well-studied and contrasting DNA\nhybridization reactions. Our preliminary results suggest that ViDa's\nvisualization successfully separates trajectories with different folding\nmechanisms, thereby providing useful insight to users, and is a big improvement\nover the current state-of-the-art in DNA kinetics visualization.",
            "author": [
                "Chenwei Zhang",
                "Khanh Dao Duc",
                "Anne Condon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03409v1",
                "http://arxiv.org/pdf/2311.03409v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02869v4",
            "title": "Lightweight equivariant interaction graph neural network for accurate\n  and efficient interatomic potential and force predictions",
            "updated": "2023-11-17T15:36:42Z",
            "published": "2023-11-06T04:49:09Z",
            "summary": "In modern computational materials science, deep learning has shown the\ncapability to predict interatomic potentials, thereby supporting and\naccelerating conventional simulations. However, existing models typically\nsacrifice either accuracy or efficiency. Moreover, lightweight models are\nhighly demanded for offering simulating systems on a considerably larger scale\nat reduced computational costs. A century ago, Felix Bloch demonstrated how\nleveraging the equivariance of the translation operation on a crystal lattice\n(with geometric symmetry) could significantly reduce the computational cost of\ndetermining wavefunctions and accurately calculate material properties. Here,\nwe introduce a lightweight equivariant interaction graph neural network\n(LEIGNN) that can enable accurate and efficient interatomic potential and force\npredictions in crystals. Rather than relying on higher-order representations,\nLEIGNN employs a scalar-vector dual representation to encode equivariant\nfeatures. By extracting both local and global structures from vector\nrepresentations and learning geometric symmetry information, our model remains\nlightweight while ensuring prediction accuracy and robustness through the\nequivariance. Our results show that LEIGNN consistently outperforms the\nprediction performance of the representative baselines and achieves significant\nefficiency across diverse datasets, which include catalysts, molecules, and\norganic isomers. Finally, to further validate the predicted interatomic\npotentials from our model, we conduct classical molecular dynamics (MD) and ab\ninitio MD simulation across various systems, including solid, liquid, and gas.\nIt is found that LEIGNN can achieve the accuracy of ab initio MD and retain the\ncomputational efficiency of classical MD across all examined systems,\ndemonstrating its accuracy, efficiency, and universality.",
            "author": [
                "Ziduo Yang",
                "Xian Wang",
                "Yifan Li",
                "Qiujie Lv",
                "Calvin Yu-Chian Chen",
                "Lei Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02869v4",
                "http://arxiv.org/pdf/2311.02869v4"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02868v1",
            "title": "Sample Complexity Bounds for Estimating Probability Divergences under\n  Invariances",
            "updated": "2023-11-06T04:45:21Z",
            "published": "2023-11-06T04:45:21Z",
            "summary": "Group-invariant probability distributions appear in many data-generative\nmodels in machine learning, such as graphs, point clouds, and images. In\npractice, one often needs to estimate divergences between such distributions.\nIn this work, we study how the inherent invariances, with respect to any smooth\naction of a Lie group on a manifold, improve sample complexity when estimating\nthe Wasserstein distance, the Sobolev Integral Probability Metrics (Sobolev\nIPMs), the Maximum Mean Discrepancy (MMD), and also the complexity of the\ndensity estimation problem (in the $L^2$ and $L^\\infty$ distance). Our results\nindicate a two-fold gain: (1) reducing the sample complexity by a\nmultiplicative factor corresponding to the group size (for finite groups) or\nthe normalized volume of the quotient space (for groups of positive dimension);\n(2) improving the exponent in the convergence rate (for groups of positive\ndimension). These results are completely new for groups of positive dimension\nand extend recent bounds for finite group actions.",
            "author": [
                "Behrooz Tahmasebi",
                "Stefanie Jegelka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02868v1",
                "http://arxiv.org/pdf/2311.02868v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02859v1",
            "title": "Microphysical Parameter Variation in GRB Stratified Afterglows and\n  Closure Relations: from sub-GeV to TeV Observations",
            "updated": "2023-11-06T04:19:57Z",
            "published": "2023-11-06T04:19:57Z",
            "summary": "Gamma-ray bursts (GRBs) are one of the most exciting sources that offer\nvaluable opportunities for investigating the evolution of energy fraction given\nto magnetic fields and particles through microphysical parameters during\nrelativistic shocks. The delayed onset of GeV-TeV radiation from bursts\ndetected by the \\textit{Fermi} Large Area Telescope (\\textit{Fermi}-LAT) and\nCherenkov Telescopes provide crucial information in favor of the external-shock\nmodel. Derivation of the closure relations (CRs) and the light curves in\nexternal shocks requires knowledge of GRB afterglow physics. In this\nmanuscript, we derive the CRs and light curves in a stratified medium with\nvariations of microphysical parameters of the synchrotron and SSC afterglow\nmodel radiated by an electron distribution with a hard and soft spectral index.\nUsing Markov Chain Monte Carlo simulations, we apply the current model to\ninvestigate the evolution of the spectral and temporal indexes of those GRBs\nreported in the Second Gamma-ray Burst Catalog (2FLGC), which comprises 29\nbursts with photon energies above 10 GeV and of those bursts (GRB 180720B,\n190114C, 190829A and 221009A) with energetic photons above 100 GeV, which can\nhardly be modeled with the CRs of the standard synchrotron scenario. The\nanalysis shows that i) the most likely afterglow model using synchrotron and\nSSC emission on the 2FLGC corresponds to the constant-density scenario, and ii)\nvariations of spectral (temporal) index keeping the temporal (spectral) index\nconstant could be associated with the evolution of microphysical parameters, as\nexhibited in GRB 190829A and GRB 221009A.",
            "author": [
                "Nissim Fraija",
                "Maria G. Dainotti",
                "Boris Betancourt Kamenetskaia",
                "Antonio Galv\u00e1n-G\u00e1mez",
                "Edilberto Aguilar-Ruiz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02859v1",
                "http://arxiv.org/pdf/2311.02859v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02849v2",
            "title": "Co-training and Co-distillation for Quality Improvement and Compression\n  of Language Models",
            "updated": "2023-11-07T18:41:55Z",
            "published": "2023-11-06T03:29:00Z",
            "summary": "Knowledge Distillation (KD) compresses computationally expensive pre-trained\nlanguage models (PLMs) by transferring their knowledge to smaller models,\nallowing their use in resource-constrained or real-time settings. However, most\nsmaller models fail to surpass the performance of the original larger model,\nresulting in sacrificing performance to improve inference speed. To address\nthis issue, we propose Co-Training and Co-Distillation (CTCD), a novel\nframework that improves performance and inference speed together by co-training\ntwo models while mutually distilling knowledge. The CTCD framework successfully\nachieves this based on two significant findings: 1) Distilling knowledge from\nthe smaller model to the larger model during co-training improves the\nperformance of the larger model. 2) The enhanced performance of the larger\nmodel further boosts the performance of the smaller model. The CTCD framework\nshows promise as it can be combined with existing techniques like architecture\ndesign or data augmentation, replacing one-way KD methods, to achieve further\nperformance improvement. Extensive ablation studies demonstrate the\neffectiveness of CTCD, and the small model distilled by CTCD outperforms the\noriginal larger model by a significant margin of 1.66 on the GLUE benchmark.",
            "author": [
                "Hayeon Lee",
                "Rui Hou",
                "Jongpil Kim",
                "Davis Liang",
                "Hongbo Zhang",
                "Sung Ju Hwang",
                "Alexander Min"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02849v2",
                "http://arxiv.org/pdf/2311.02849v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02847v2",
            "title": "Kinematic-aware Prompting for Generalizable Articulated Object\n  Manipulation with LLMs",
            "updated": "2023-11-08T06:12:36Z",
            "published": "2023-11-06T03:26:41Z",
            "summary": "Generalizable articulated object manipulation is essential for home-assistant\nrobots. Recent efforts focus on imitation learning from demonstrations or\nreinforcement learning in simulation, however, due to the prohibitive costs of\nreal-world data collection and precise object simulation, it still remains\nchallenging for these works to achieve broad adaptability across diverse\narticulated objects. Recently, many works have tried to utilize the strong\nin-context learning ability of Large Language Models (LLMs) to achieve\ngeneralizable robotic manipulation, but most of these researches focus on\nhigh-level task planning, sidelining low-level robotic control. In this work,\nbuilding on the idea that the kinematic structure of the object determines how\nwe can manipulate it, we propose a kinematic-aware prompting framework that\nprompts LLMs with kinematic knowledge of objects to generate low-level motion\ntrajectory waypoints, supporting various object manipulation. To effectively\nprompt LLMs with the kinematic structure of different objects, we design a\nunified kinematic knowledge parser, which represents various articulated\nobjects as a unified textual description containing kinematic joints and\ncontact location. Building upon this unified description, a kinematic-aware\nplanner model is proposed to generate precise 3D manipulation waypoints via a\ndesigned kinematic-aware chain-of-thoughts prompting method. Our evaluation\nspanned 48 instances across 16 distinct categories, revealing that our\nframework not only outperforms traditional methods on 8 seen categories but\nalso shows a powerful zero-shot capability for 8 unseen articulated object\ncategories. Moreover, the real-world experiments on 7 different object\ncategories prove our framework's adaptability in practical scenarios. Code is\nreleased at\n\\href{https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main}{here}.",
            "author": [
                "Wenke Xia",
                "Dong Wang",
                "Xincheng Pang",
                "Zhigang Wang",
                "Bin Zhao",
                "Di Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02847v2",
                "http://arxiv.org/pdf/2311.02847v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02843v1",
            "title": "Non-uniform Mixing of Quantum Walks on the Symmetric Group",
            "updated": "2023-11-06T03:17:36Z",
            "published": "2023-11-06T03:17:36Z",
            "summary": "It is well-known that classical random walks on regular graphs converge to\nthe uniform distribution. Quantum walks, in their various forms, are\nquantizations of their corresponding classical random walk processes. Gerhardt\nand Watrous (2003) demonstrated that continuous-time quantum walks do not\nconverge to the uniform distribution on certain Cayley graphs of the Symmetric\ngroup, which by definition are all regular. In this paper, we demonstrate that\ndiscrete-time quantum walks, in the sense of quantized Markov chains as\nintroduced by Szegedy (2004), also do not converge to the uniform distribution.\nWe analyze the spectra of the Szegedy walk operators using the representation\ntheory of the symmetric group. In the discrete setting, the analysis is\ncomplicated by the fact that we work within a Hilbert space of a higher\ndimension than the continuous case, spanned by pairs of vertices. Our\ntechniques are general, and we believe they can be applied to derive similar\nanalytical results for other non-commutative groups using the characters of\ntheir irreducible representation.",
            "author": [
                "Avah Banerjee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02843v1",
                "http://arxiv.org/pdf/2311.02843v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02839v1",
            "title": "Cell-Probe Lower Bound for Accessible Interval Graphs",
            "updated": "2023-11-06T02:59:44Z",
            "published": "2023-11-06T02:59:44Z",
            "summary": "We spot a hole in the area of succinct data structures for graph classes from\na universe of size at most $n^n$. Very often, the input graph is labeled by the\nuser in an arbitrary and easy-to-use way, and the data structure for the graph\nrelabels the input graph in some way. For any access, the user needs to store\nthese labels or compute the new labels in an online manner. This might require\nmore bits than the information-theoretic minimum of the original graph class,\nhence, defeating the purpose of succinctness. Given this, the data structure\ndesigner must allow the user to access the data structure with the original\nlabels, i.e., relabeling is not allowed. We call such a graph data structure\n``accessible''. In this paper, we study the complexity of such accessible data\nstructures for interval graphs, a graph class with information-theoretic\nminimum less than $n\\log n$ bits.\n  - We formalize the concept of \"accessibility\" (which was implicitly assumed),\nand propose the \"universal interval representation\", for interval graphs.\n  - Any data structure for interval graphs in universal interval\nrepresentation, which supports both adjacency and degree query simultaneously\nwith time cost $t_1$ and $t_2$ respectively, must consume at least\n$\\log_2(n!)+n/(\\log n)^{O(t_1+t_2)}$ bits of space. This is also the first\nlower bound for graph classes with information-theoretic minimum less than\n$n\\log_2n$ bits.\n  - We provide efficient succinct data structures for interval graphs in\nuniversal interval representation supporting adjacency query and degree query\nindividually in constant time and space costs. Therefore, two upper bounds\ntogether with the lower bound show that the two elementary queries for interval\ngraphs are incompatible with each other in the context of succinct data\nstructure. To the best of our knowledge, this is the first proof of such\nincompatibility phenomenon.",
            "author": [
                "Sankardeep Chakraborty",
                "Christian Engels",
                "Seungbum Jo",
                "Mingmou Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02839v1",
                "http://arxiv.org/pdf/2311.02839v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "68P05, 68P30, 68Q17",
                "E.1; F.1.3; F.2.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02838v1",
            "title": "Barron Space for Graph Convolution Neural Networks",
            "updated": "2023-11-06T02:58:05Z",
            "published": "2023-11-06T02:58:05Z",
            "summary": "Graph convolutional neural network (GCNN) operates on graph domain and it has\nachieved a superior performance to accomplish a wide range of tasks. In this\npaper, we introduce a Barron space of functions on a compact domain of graph\nsignals. We prove that the proposed Barron space is a reproducing kernel Banach\nspace, it can be decomposed into the union of a family of reproducing kernel\nHilbert spaces with neuron kernels, and it could be dense in the space of\ncontinuous functions on the domain. Approximation property is one of the main\nprinciples to design neural networks. In this paper, we show that outputs of\nGCNNs are contained in the Barron space and functions in the Barron space can\nbe well approximated by outputs of some GCNNs in the integrated square and\nuniform measurements. We also estimate the Rademacher complexity of functions\nwith bounded Barron norm and conclude that functions in the Barron space could\nbe learnt from their random samples efficiently.",
            "author": [
                "Seok-Young Chung",
                "Qiyu Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02838v1",
                "http://arxiv.org/pdf/2311.02838v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02835v1",
            "title": "Flexible Multi-Generator Model with Fused Spatiotemporal Graph for\n  Trajectory Prediction",
            "updated": "2023-11-06T02:46:05Z",
            "published": "2023-11-06T02:46:05Z",
            "summary": "Trajectory prediction plays a vital role in automotive radar systems,\nfacilitating precise tracking and decision-making in autonomous driving.\nGenerative adversarial networks with the ability to learn a distribution over\nfuture trajectories tend to predict out-of-distribution samples, which\ntypically occurs when the distribution of forthcoming paths comprises a blend\nof various manifolds that may be disconnected. To address this issue, we\npropose a trajectory prediction framework, which can capture the social\ninteraction variations and model disconnected manifolds of pedestrian\ntrajectories. Our framework is based on a fused spatiotemporal graph to better\nmodel the complex interactions of pedestrians in a scene, and a multi-generator\narchitecture that incorporates a flexible generator selector network on\ngenerated trajectories to learn a distribution over multiple generators. We\nshow that our framework achieves state-of-the-art performance compared with\nseveral baselines on different challenging datasets.",
            "author": [
                "Peiyuan Zhu",
                "Fengxia Han",
                "Hao Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02835v1",
                "http://arxiv.org/pdf/2311.02835v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02832v1",
            "title": "Prioritized Propagation in Graph Neural Networks",
            "updated": "2023-11-06T02:38:35Z",
            "published": "2023-11-06T02:38:35Z",
            "summary": "Graph neural networks (GNNs) have recently received significant attention.\nLearning node-wise message propagation in GNNs aims to set personalized\npropagation steps for different nodes in the graph. Despite the success,\nexisting methods ignore node priority that can be reflected by node influence\nand heterophily. In this paper, we propose a versatile framework PPro, which\ncan be integrated with most existing GNN models and aim to learn prioritized\nnode-wise message propagation in GNNs. Specifically, the framework consists of\nthree components: a backbone GNN model, a propagation controller to determine\nthe optimal propagation steps for nodes, and a weight controller to compute the\npriority scores for nodes. We design a mutually enhanced mechanism to compute\nnode priority, optimal propagation step and label prediction. We also propose\nan alternative optimization strategy to learn the parameters in the backbone\nGNN model and two parametric controllers. We conduct extensive experiments to\ncompare our framework with other 11 state-of-the-art competitors on 8 benchmark\ndatasets. Experimental results show that our framework can lead to superior\nperformance in terms of propagation strategies and node representations.",
            "author": [
                "Yao Cheng",
                "Minjie Chen",
                "Xiang Li",
                "Caihua Shan",
                "Ming Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02832v1",
                "http://arxiv.org/pdf/2311.02832v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02831v3",
            "title": "SemanticTopoLoop: Semantic Loop Closure With 3D Topological Graph Based\n  on Quadric-Level Object Map",
            "updated": "2023-11-09T12:55:34Z",
            "published": "2023-11-06T02:30:30Z",
            "summary": "Loop closure, as one of the crucial components in SLAM, plays an essential\nrole in correcting the accumulated errors. Traditional appearance-based\nmethods, such as bag-of-words models, are often limited by local 2D features\nand the volume of training data, making them less versatile and robust in\nreal-world scenarios, leading to missed detections or false positives\ndetections in loop closure. To address these issues, we first propose a\nobject-level data association method based on multi-level verification, which\ncan associate 2D semantic features of current frame with 3D objects landmarks\nof map. Next, taking advantage of these association relations, we introduce a\nsemantic loop closure method based on quadric-level object map topology, which\nrepresents scenes through the topological graph of objects and achieves\naccurate loop closure at a wide field of view by comparing differences in the\ntopological graphs. Finally, we integrate these two methods into a complete\nobject-aware SLAM system. Qualitative experiments and ablation studies\ndemonstrate the effectiveness and robustness of the proposed object-level data\nassociation algorithm. Quantitative experiments show that our semantic loop\nclosure method outperforms existing state-of-the-art methods in terms of\nprecision, recall and localization accuracy metrics.",
            "author": [
                "Zhenzhong Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02831v3",
                "http://arxiv.org/pdf/2311.02831v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02816v1",
            "title": "APGL4SR: A Generic Framework with Adaptive and Personalized Global\n  Collaborative Information in Sequential Recommendation",
            "updated": "2023-11-06T01:33:24Z",
            "published": "2023-11-06T01:33:24Z",
            "summary": "The sequential recommendation system has been widely studied for its\npromising effectiveness in capturing dynamic preferences buried in users'\nsequential behaviors. Despite the considerable achievements, existing methods\nusually focus on intra-sequence modeling while overlooking exploiting global\ncollaborative information by inter-sequence modeling, resulting in inferior\nrecommendation performance. Therefore, previous works attempt to tackle this\nproblem with a global collaborative item graph constructed by pre-defined\nrules. However, these methods neglect two crucial properties when capturing\nglobal collaborative information, i.e., adaptiveness and personalization,\nyielding sub-optimal user representations. To this end, we propose a\ngraph-driven framework, named Adaptive and Personalized Graph Learning for\nSequential Recommendation (APGL4SR), that incorporates adaptive and\npersonalized global collaborative information into sequential recommendation\nsystems. Specifically, we first learn an adaptive global graph among all items\nand capture global collaborative information with it in a self-supervised\nfashion, whose computational burden can be further alleviated by the proposed\nSVD-based accelerator. Furthermore, based on the graph, we propose to extract\nand utilize personalized item correlations in the form of relative positional\nencoding, which is a highly compatible manner of personalizing the utilization\nof global collaborative information. Finally, the entire framework is optimized\nin a multi-task learning paradigm, thus each part of APGL4SR can be mutually\nreinforced. As a generic framework, APGL4SR can outperform other baselines with\nsignificant margins. The code is available at\nhttps://github.com/Graph-Team/APGL4SR.",
            "author": [
                "Mingjia Yin",
                "Hao Wang",
                "Xiang Xu",
                "Likang Wu",
                "Sirui Zhao",
                "Wei Guo",
                "Yong Liu",
                "Ruiming Tang",
                "Defu Lian",
                "Enhong Chen"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3583780.3614781",
                "http://arxiv.org/abs/2311.02816v1",
                "http://arxiv.org/pdf/2311.02816v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02814v2",
            "title": "A Novel Catalyst Scheme for Stochastic Minimax Optimization",
            "updated": "2023-11-08T03:59:13Z",
            "published": "2023-11-06T01:19:27Z",
            "summary": "This paper presents a proximal-point-based catalyst scheme for simple\nfirst-order methods applied to convex minimization and convex-concave minimax\nproblems. In particular, for smooth and (strongly)-convex minimization\nproblems, the proposed catalyst scheme, instantiated with a simple variant of\nstochastic gradient method, attains the optimal rate of convergence in terms of\nboth deterministic and stochastic errors. For smooth and\nstrongly-convex-strongly-concave minimax problems, the catalyst scheme attains\nthe optimal rate of convergence for deterministic and stochastic errors up to a\nlogarithmic factor. To the best of our knowledge, this reported convergence\nseems to be attained for the first time by stochastic first-order methods in\nthe literature. We obtain this result by designing and catalyzing a novel\nvariant of stochastic extragradient method for solving smooth and\nstrongly-monotone variational inequality, which may be of independent interest.",
            "author": [
                "Guanghui Lan",
                "Yan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02814v2",
                "http://arxiv.org/pdf/2311.02814v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02811v2",
            "title": "Contour Algorithm for Connectivity",
            "updated": "2023-11-07T02:42:27Z",
            "published": "2023-11-06T01:02:44Z",
            "summary": "Finding connected components in a graph is a fundamental problem in graph\nanalysis. In this work, we present a novel minimum-mapping based Contour\nalgorithm to efficiently solve the connectivity problem. We prove that the\nContour algorithm with two or higher order operators can identify all connected\ncomponents of an undirected graph within $\\mathcal{O}(\\log d_{max})$\niterations, with each iteration involving $\\mathcal{O}(m)$ work, where\n$d_{max}$ represents the largest diameter among all components in the given\ngraph, and $m$ is the total number of edges in the graph. Importantly, each\niteration is highly parallelizable, making use of the efficient minimum-mapping\noperator applied to all edges. To further enhance its practical performance, we\noptimize the Contour algorithm through asynchronous updates, early convergence\nchecking, eliminating atomic operations, and choosing more efficient mapping\noperators. Our implementation of the Contour algorithm has been integrated into\nthe open-source framework Arachne. Arachne extends Arkouda for large-scale\ninteractive graph analytics, providing a Python API powered by the\nhigh-productivity parallel language Chapel. Experimental results on both\nreal-world and synthetic graphs demonstrate the superior performance of our\nproposed Contour algorithm compared to state-of-the-art large-scale parallel\nalgorithm FastSV and the fastest shared memory algorithm ConnectIt. On average,\nContour achieves a speedup of 7.3x and 1.4x compared to FastSV and ConnectIt,\nrespectively. All code for the Contour algorithm and the Arachne framework is\npublicly available on GitHub ( https://github.com/Bears-R-Us/arkouda-njit ),\nensuring transparency and reproducibility of our work.",
            "author": [
                "Zhihui Du",
                "Oliver Alvarado Rodriguez",
                "Fuhuan Li",
                "Mohammad Dindoost",
                "David A. Bader"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02811v2",
                "http://arxiv.org/pdf/2311.02811v2"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02798v1",
            "title": "From molecules to scaffolds to functional groups: building\n  context-dependent molecular representation via multi-channel learning",
            "updated": "2023-11-05T23:47:52Z",
            "published": "2023-11-05T23:47:52Z",
            "summary": "Reliable molecular property prediction is essential for various scientific\nendeavors and industrial applications, such as drug discovery. However, the\nscarcity of data, combined with the highly non-linear causal relationships\nbetween physicochemical and biological properties and conventional molecular\nfeaturization schemes, complicates the development of robust molecular machine\nlearning models. Self-supervised learning (SSL) has emerged as a popular\nsolution, utilizing large-scale, unannotated molecular data to learn a\nfoundational representation of chemical space that might be advantageous for\ndownstream tasks. Yet, existing molecular SSL methods largely overlook\ndomain-specific knowledge, such as molecular similarity and scaffold\nimportance, as well as the context of the target application when operating\nover the large chemical space. This paper introduces a novel learning framework\nthat leverages the knowledge of structural hierarchies within molecular\nstructures, embeds them through separate pre-training tasks over distinct\nchannels, and employs a task-specific channel selection to compose a\ncontext-dependent representation. Our approach demonstrates competitive\nperformance across various molecular property benchmarks and establishes some\nstate-of-the-art results. It further offers unprecedented advantages in\nparticularly challenging yet ubiquitous scenarios like activity cliffs with\nenhanced robustness and generalizability compared to other baselines.",
            "author": [
                "Yue Wan",
                "Jialu Wu",
                "Tingjun Hou",
                "Chang-Yu Hsieh",
                "Xiaowei Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02798v1",
                "http://arxiv.org/pdf/2311.02798v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.chem-ph",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02792v1",
            "title": "Signed graphs and inverses of their incidence matrices",
            "updated": "2023-11-05T23:35:51Z",
            "published": "2023-11-05T23:35:51Z",
            "summary": "The Laplacian matrix $L$ of a signed graph $G$ may or may not be invertible.\nWe present a combinatorial formula of the Moore-Penrose inverse of $L$. This is\nachieved by finding a combinatorial formula for the Moore-Penrose inverse of an\nincidence matrix of $G$. This work generalizes related known results on\nincidence and Laplacian matrices of an unsigned graph. Several examples are\nprovided to show the usefulness of these combinatorial formulas.",
            "author": [
                "Abdullah Alazemi",
                "Milica Andelic",
                "Sudipta Mallik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02792v1",
                "http://arxiv.org/pdf/2311.02792v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 15A09"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02791v1",
            "title": "MirrorCalib: Utilizing Human Pose Information for Mirror-based Virtual\n  Camera Calibration",
            "updated": "2023-11-05T23:25:21Z",
            "published": "2023-11-05T23:25:21Z",
            "summary": "In this paper, we present the novel task of estimating the extrinsic\nparameters of a virtual camera with respect to a real camera with one single\nfixed planar mirror. This task poses a significant challenge in cases where\nobjects captured lack overlapping views from both real and mirrored cameras. To\naddress this issue, prior knowledge of a human body and 2D joint locations are\nutilized to estimate the camera extrinsic parameters when a person is in front\nof a mirror. We devise a modified eight-point algorithm to obtain an initial\nestimation from 2D joint locations. The 2D joint locations are then refined\nsubject to human body constraints. Finally, a RANSAC algorithm is employed to\nremove outliers by comparing their epipolar distances to a predetermined\nthreshold. MirrorCalib is evaluated on both synthetic and real datasets and\nachieves a rotation error of 0.62{\\deg}/1.82{\\deg} and a translation error of\n37.33/69.51 mm on the synthetic/real dataset, which outperforms the\nstate-of-art method.",
            "author": [
                "Longyun Liao",
                "Andrew Mitchell",
                "Rong Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02791v1",
                "http://arxiv.org/pdf/2311.02791v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02778v1",
            "title": "MuSHRoom: Multi-Sensor Hybrid Room Dataset for Joint 3D Reconstruction\n  and Novel View Synthesis",
            "updated": "2023-11-05T21:46:12Z",
            "published": "2023-11-05T21:46:12Z",
            "summary": "Metaverse technologies demand accurate, real-time, and immersive modeling on\nconsumer-grade hardware for both non-human perception (e.g.,\ndrone/robot/autonomous car navigation) and immersive technologies like AR/VR,\nrequiring both structural accuracy and photorealism. However, there exists a\nknowledge gap in how to apply geometric reconstruction and photorealism\nmodeling (novel view synthesis) in a unified framework.\n  To address this gap and promote the development of robust and immersive\nmodeling and rendering with consumer-grade devices, first, we propose a\nreal-world Multi-Sensor Hybrid Room Dataset (MuSHRoom). Our dataset presents\nexciting challenges and requires state-of-the-art methods to be cost-effective,\nrobust to noisy data and devices, and can jointly learn 3D reconstruction and\nnovel view synthesis, instead of treating them as separate tasks, making them\nideal for real-world applications. Second, we benchmark several famous\npipelines on our dataset for joint 3D mesh reconstruction and novel view\nsynthesis. Finally, in order to further improve the overall performance, we\npropose a new method that achieves a good trade-off between the two tasks. Our\ndataset and benchmark show great potential in promoting the improvements for\nfusing 3D reconstruction and high-quality rendering in a robust and\ncomputationally efficient end-to-end fashion.",
            "author": [
                "Xuqian Ren",
                "Wenjia Wang",
                "Dingding Cai",
                "Tuuli Tuominen",
                "Juho Kannala",
                "Esa Rahtu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02778v1",
                "http://arxiv.org/pdf/2311.02778v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02767v1",
            "title": "Understanding Structured Knowledge Production: A Case Study of\n  Wikidata's Representation Injustice",
            "updated": "2023-11-05T20:52:27Z",
            "published": "2023-11-05T20:52:27Z",
            "summary": "Wikidata is a multi-language knowledge base that is being edited and\nmaintained by editors from different language communities. Due to the\nstructured nature of its content, the contributions are in various forms,\nincluding manual edit, tool-assisted edits, automated edits, etc, with the\nmajority of edits being the import from wiki-internal or external datasets. Due\nto the outstanding power of bots and tools reflecting from their large volume\nof edits, knowledge contributions within Wikidata can easily cause epistemic\ninjustice due to internal and external reasons. In this case study, we compared\nthe coverage and edit history of human pages in two countries. By shedding\nlight on these disparities and offering actionable solutions, our study aims to\nenhance the fairness and inclusivity of knowledge representation within\nWikidata, ultimately contributing to a more equitable and comprehensive global\nknowledge base.",
            "author": [
                "Jeffrey Jun-jie Ma",
                "Charles Chuankai Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02767v1",
                "http://arxiv.org/pdf/2311.02767v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02765v1",
            "title": "Rule Learning as Machine Translation using the Atomic Knowledge Bank",
            "updated": "2023-11-05T20:48:54Z",
            "published": "2023-11-05T20:48:54Z",
            "summary": "Machine learning models, and in particular language models, are being applied\nto various tasks that require reasoning. While such models are good at\ncapturing patterns their ability to reason in a trustable and controlled manner\nis frequently questioned. On the other hand, logic-based rule systems allow for\ncontrolled inspection and already established verification methods. However it\nis well-known that creating such systems manually is time-consuming and prone\nto errors. We explore the capability of transformers to translate sentences\nexpressing rules in natural language into logical rules. We see reasoners as\nthe most reliable tools for performing logical reasoning and focus on\ntranslating language into the format expected by such tools. We perform\nexperiments using the DKET dataset from the literature and create a dataset for\nlanguage to logic translation based on the Atomic knowledge bank.",
            "author": [
                "Kristoffer \u00c6s\u00f8y",
                "Ana Ozaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02765v1",
                "http://arxiv.org/pdf/2311.02765v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02760v1",
            "title": "Causal Question Answering with Reinforcement Learning",
            "updated": "2023-11-05T20:33:18Z",
            "published": "2023-11-05T20:33:18Z",
            "summary": "Causal questions inquire about causal relationships between different events\nor phenomena. Specifically, they often aim to determine whether there is a\nrelationship between two phenomena, or to identify all causes/effects of a\nphenomenon. Causal questions are important for a variety of use cases,\nincluding virtual assistants and search engines. However, many current\napproaches to causal question answering cannot provide explanations or evidence\nfor their answers. Hence, in this paper, we aim to answer causal questions with\nCauseNet, a large-scale dataset of causal relations and their provenance data.\nInspired by recent, successful applications of reinforcement learning to\nknowledge graph tasks, such as link prediction and fact-checking, we explore\nthe application of reinforcement learning on CauseNet for causal question\nanswering. We introduce an Actor-Critic based agent which learns to search\nthrough the graph to answer causal questions. We bootstrap the agent with a\nsupervised learning procedure to deal with large action spaces and sparse\nrewards. Our evaluation shows that the agent successfully prunes the search\nspace to answer binary causal questions by visiting less than 30 nodes per\nquestion compared to over 3,000 nodes by a naive breadth-first search. Our\nablation study indicates that our supervised learning strategy provides a\nstrong foundation upon which our reinforcement learning agent improves. The\npaths returned by our agent explain the mechanisms by which a cause produces an\neffect. Moreover, for each edge on a path, CauseNet stores its original source\non the web allowing for easy verification of paths.",
            "author": [
                "Lukas Bl\u00fcbaum",
                "Stefan Heindorf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02760v1",
                "http://arxiv.org/pdf/2311.02760v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02757v1",
            "title": "ELEGANT: Certified Defense on the Fairness of Graph Neural Networks",
            "updated": "2023-11-05T20:29:40Z",
            "published": "2023-11-05T20:29:40Z",
            "summary": "Graph Neural Networks (GNNs) have emerged as a prominent graph learning model\nin various graph-based tasks over the years. Nevertheless, due to the\nvulnerabilities of GNNs, it has been empirically proved that malicious\nattackers could easily corrupt the fairness level of their predictions by\nadding perturbations to the input graph data. In this paper, we take crucial\nsteps to study a novel problem of certifiable defense on the fairness level of\nGNNs. Specifically, we propose a principled framework named ELEGANT and present\na detailed theoretical certification analysis for the fairness of GNNs. ELEGANT\ntakes any GNNs as its backbone, and the fairness level of such a backbone is\ntheoretically impossible to be corrupted under certain perturbation budgets for\nattackers. Notably, ELEGANT does not have any assumption over the GNN structure\nor parameters, and does not require re-training the GNNs to realize\ncertification. Hence it can serve as a plug-and-play framework for any\noptimized GNNs ready to be deployed. We verify the satisfactory effectiveness\nof ELEGANT in practice through extensive experiments on real-world datasets\nacross different backbones of GNNs, where ELEGANT is also demonstrated to be\nbeneficial for GNN debiasing. Open-source code can be found at\nhttps://github.com/yushundong/ELEGANT.",
            "author": [
                "Yushun Dong",
                "Binchi Zhang",
                "Hanghang Tong",
                "Jundong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02757v1",
                "http://arxiv.org/pdf/2311.02757v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02753v1",
            "title": "On Why the 10-TeV Cosmic Ray Bump Originates in the Local Interstellar\n  Medium",
            "updated": "2023-11-05T20:13:12Z",
            "published": "2023-11-05T20:13:12Z",
            "summary": "Recent measurements of primary and secondary CR spectra, their arrival\ndirections, and our improved knowledge of the magnetic field geometry around\nthe heliosphere allow us to set a bound on the distance beyond which a puzzling\n10-TeV \"bump\" cannot originate. The sharpness of the spectral breaks associated\nwith the bump, the abrupt change of the CR intensity across the local magnetic\nequator ($90^{\\circ}$ pitch angle), and the similarity between the primary and\nsecondary CR spectral patterns point to a local reacceleration of the bump\nparticles out of the background CRs. We argue that a nearby shock may generate\nsuch a bump by increasing the rigidity of the preexisting CRs below 50 TV by a\nmere factor of ~1.5. Reaccelerated particles below ~0.5 TV are convected with\nthe interstellar medium flow and do not reach the Sun, thus creating the bump.\nThis single universal process is responsible for the observed spectra of all CR\nspecies in the rigidity range below 100 TV. We propose that one viable\ncandidate is the system of shocks associated with Epsilon Eridani star at 3.2\npc of the Sun, which is well aligned with the direction of the local magnetic\nfield. Other shocks, such as old supernova shells, may produce a similar\neffect. We provide a simple formula that reproduces the spectra of all CR\nspecies with only three parameters uniquely derived from the CR proton data. We\nshow how our formalism predicts helium and carbon spectra and the B/C ratio.",
            "author": [
                "Mikhail Malkov",
                "Patrick Diamond",
                "Mingyun Cao",
                "Igor Moskalenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02753v1",
                "http://arxiv.org/pdf/2311.02753v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02719v1",
            "title": "Uncertainty Estimation for Safety-critical Scene Segmentation via\n  Fine-grained Reward Maximization",
            "updated": "2023-11-05T17:43:37Z",
            "published": "2023-11-05T17:43:37Z",
            "summary": "Uncertainty estimation plays an important role for future reliable deployment\nof deep segmentation models in safety-critical scenarios such as medical\napplications. However, existing methods for uncertainty estimation have been\nlimited by the lack of explicit guidance for calibrating the prediction risk\nand model confidence. In this work, we propose a novel fine-grained reward\nmaximization (FGRM) framework, to address uncertainty estimation by directly\nutilizing an uncertainty metric related reward function with a reinforcement\nlearning based model tuning algorithm. This would benefit the model uncertainty\nestimation through direct optimization guidance for model calibration.\nSpecifically, our method designs a new uncertainty estimation reward function\nusing the calibration metric, which is maximized to fine-tune an evidential\nlearning pre-trained segmentation model for calibrating prediction risk.\nImportantly, we innovate an effective fine-grained parameter update scheme,\nwhich imposes fine-grained reward-weighting of each network parameter according\nto the parameter importance quantified by the fisher information matrix. To the\nbest of our knowledge, this is the first work exploring reward optimization for\nmodel uncertainty estimation in safety-critical vision tasks. The effectiveness\nof our method is demonstrated on two large safety-critical surgical scene\nsegmentation datasets under two different uncertainty estimation settings. With\nreal-time one forward pass at inference, our method outperforms\nstate-of-the-art methods by a clear margin on all the calibration metrics of\nuncertainty estimation, while maintaining a high task accuracy for the\nsegmentation results. Code is available at\n\\url{https://github.com/med-air/FGRM}.",
            "author": [
                "Hongzheng Yang",
                "Cheng Chen",
                "Yueyao Chen",
                "Markus Scheppach",
                "Hon Chi Yip",
                "Qi Dou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02719v1",
                "http://arxiv.org/pdf/2311.02719v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02708v1",
            "title": "Highly Connected Steiner Subgraph -- Parameterized Algorithms and\n  Applications to Hitting Set Problems",
            "updated": "2023-11-05T16:54:23Z",
            "published": "2023-11-05T16:54:23Z",
            "summary": "Given a simple connected undirected graph G = (V, E), a set X \\subseteq V(G),\nand integers k and p, STEINER SUBGRAPH EXTENSION problem asks if there exists a\nset S \\supseteq X with at most k vertices such that G[S] is p-edge-connected.\nThis is a natural generalization of a well-studied problem STEINER TREE (set\np=1 and X as the set of all terminals). In this paper, we initiate the study of\nSTEINER SUBGRAPH EXTENSION from the perspective of parameterized complexity and\ngive a fixed-parameter algorithm parameterized by k and p on graphs of bounded\ndegeneracy. In case we remove the assumption of the input graph being bounded\ndegenerate, then the STEINER SUBGRAPH EXTENSION problem becomes W[1]-hard.\nBesides being an independent advance on the parameterized complexity of network\ndesign problems, our result has natural applications. In particular, we use our\nresult to obtain singly exponential-time FPT algorithms for several vertex\ndeletion problem studied in the literature, where the goal is to delete a\nsmallest set of vertices such that (i) the resulting graph belongs to a\nspecific hereditary graph class, and (ii) the deleted set of vertices induces a\np-edge-connected subgraph of the input graph.",
            "author": [
                "Eduard Eiben",
                "Diptapriyo Majumdar",
                "M. S. Ramanujan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02708v1",
                "http://arxiv.org/pdf/2311.02708v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "F.2.2; G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02702v1",
            "title": "Extraction of Atypical Aspects from Customer Reviews: Datasets and\n  Experiments with Language Models",
            "updated": "2023-11-05T16:15:50Z",
            "published": "2023-11-05T16:15:50Z",
            "summary": "A restaurant dinner may become a memorable experience due to an unexpected\naspect enjoyed by the customer, such as an origami-making station in the\nwaiting area. If aspects that are atypical for a restaurant experience were\nknown in advance, they could be leveraged to make recommendations that have the\npotential to engender serendipitous experiences, further increasing user\nsatisfaction. Although relatively rare, whenever encountered, atypical aspects\noften end up being mentioned in reviews due to their memorable quality.\nCorrespondingly, in this paper we introduce the task of detecting atypical\naspects in customer reviews. To facilitate the development of extraction\nmodels, we manually annotate benchmark datasets of reviews in three domains -\nrestaurants, hotels, and hair salons, which we use to evaluate a number of\nlanguage models, ranging from fine-tuning the instruction-based text-to-text\ntransformer Flan-T5 to zero-shot and few-shot prompting of GPT-3.5.",
            "author": [
                "Smita Nannaware",
                "Erfan Al-Hossami",
                "Razvan Bunescu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02702v1",
                "http://arxiv.org/pdf/2311.02702v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02687v1",
            "title": "Architecture Matters: Uncovering Implicit Mechanisms in Graph\n  Contrastive Learning",
            "updated": "2023-11-05T15:54:17Z",
            "published": "2023-11-05T15:54:17Z",
            "summary": "With the prosperity of contrastive learning for visual representation\nlearning (VCL), it is also adapted to the graph domain and yields promising\nperformance. However, through a systematic study of various graph contrastive\nlearning (GCL) methods, we observe that some common phenomena among existing\nGCL methods that are quite different from the original VCL methods, including\n1) positive samples are not a must for GCL; 2) negative samples are not\nnecessary for graph classification, neither for node classification when\nadopting specific normalization modules; 3) data augmentations have much less\ninfluence on GCL, as simple domain-agnostic augmentations (e.g., Gaussian\nnoise) can also attain fairly good performance. By uncovering how the implicit\ninductive bias of GNNs works in contrastive learning, we theoretically provide\ninsights into the above intriguing properties of GCL. Rather than directly\nporting existing VCL methods to GCL, we advocate for more attention toward the\nunique architecture of graph learning and consider its implicit influence when\ndesigning GCL methods. Code is available at https:\n//github.com/PKU-ML/ArchitectureMattersGCL.",
            "author": [
                "Xiaojun Guo",
                "Yifei Wang",
                "Zeming Wei",
                "Yisen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02687v1",
                "http://arxiv.org/pdf/2311.02687v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02682v1",
            "title": "Integral invariants for framed 3-manifolds associated to trivalent\n  graphs possibly with self-loops",
            "updated": "2023-11-05T15:45:47Z",
            "published": "2023-11-05T15:45:47Z",
            "summary": "Bott--Cattaneo's theory defines the integral invariants of framed rational\nhomology 3-spheres with acyclic orthogonal local systems associated to graph\ncocycles without self-loops. The 2-loop term of their invariants is associated\nwith the Theta graph. Their invariants can be defined when a cohomological\ncondition holds. Cattaneo--Shimizu gave a refinement of the 2-loop term of\nBott--Cattaneo invariants by removing this cohomological condition, their\n2-loop term is associated with a linear combination of the Theta graph and the\ndumbbell graph that is the only 2-loop trivalent graph with self-loops. In this\narticle, when an acyclic local system is given by the adjoint representation of\na semi-simple Lie group composed with a representation of the fundamental group\nof a closed 3-manifold, we show that the associated integral of dumbbell graph\ncan be vanished by a cohomological reason. Based on this idea, we construct a\ntheory of graph complexes and cocycles, so that higher-loop invariants can be\ndefined using both the graph cocycles without self-loop, as by Bott--Cattaneo,\nand with self-loops, as by Cattaneo--Shimizu. As a consequence, we prove that\nthe generating series from Chern--Simons perturbation theory gives rise to\ntopological invariants for framed 3-manifolds in our setting, which admits a\nformula in terms of only trivalent graphs without self-loop.",
            "author": [
                "Hisatoshi Kodani",
                "Bingxiao Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02682v1",
                "http://arxiv.org/pdf/2311.02682v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math-ph",
                "math.DG",
                "math.MP",
                "math.QA",
                "57R56, 57K31, 58J28, 81Q30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02665v1",
            "title": "Digital Typhoon: Long-term Satellite Image Dataset for the\n  Spatio-Temporal Modeling of Tropical Cyclones",
            "updated": "2023-11-05T14:22:13Z",
            "published": "2023-11-05T14:22:13Z",
            "summary": "This paper presents the official release of the Digital Typhoon dataset, the\nlongest typhoon satellite image dataset for 40+ years aimed at benchmarking\nmachine learning models for long-term spatio-temporal data. To build the\ndataset, we developed a workflow to create an infrared typhoon-centered image\nfor cropping using Lambert azimuthal equal-area projection referring to the\nbest track data. We also address data quality issues such as inter-satellite\ncalibration to create a homogeneous dataset. To take advantage of the dataset,\nwe organized machine learning tasks by the types and targets of inference, with\nother tasks for meteorological analysis, societal impact, and climate change.\nThe benchmarking results on the analysis, forecasting, and reanalysis for the\nintensity suggest that the dataset is challenging for recent deep learning\nmodels, due to many choices that affect the performance of various models. This\ndataset reduces the barrier for machine learning researchers to meet\nlarge-scale real-world events called tropical cyclones and develop machine\nlearning models that may contribute to advancing scientific knowledge on\ntropical cyclones as well as solving societal and sustainability issues such as\ndisaster reduction and climate change. The dataset is publicly available at\nhttp://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and\nhttps://github.com/kitamoto-lab/digital-typhoon/.",
            "author": [
                "Asanobu Kitamoto",
                "Jared Hwang",
                "Bastien Vuillod",
                "Lucas Gautier",
                "Yingtao Tian",
                "Tarin Clanuwat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02665v1",
                "http://arxiv.org/pdf/2311.02665v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02657v1",
            "title": "GSC: Generalizable Service Coordination",
            "updated": "2023-11-05T14:04:09Z",
            "published": "2023-11-05T14:04:09Z",
            "summary": "Services with distributed and interdependent components are becoming a\npopular option for harnessing dispersed resources available on cloud and edge\nnetworks. However, effective deployment and management of these services,\nnamely service coordination, is a challenging task. Service coordination\ncomprises the placement and scalability of components and scheduling incoming\ntraffic requesting for services between deployed instances. Due to the online\nnature of the problem and the success of Deep Reinforcement Learning (DRL)\nmethods, previous works considered DRL agents for solving service coordination\nproblems, yet these solutions have to be retrained for every unseen scenario.\nOther works have tried to tackle this shortcoming by incorporating Graph Neural\nNetworks (GNN) into their solutions, but they often focus on specific aspects\n(and disregard others) or cannot operate in dynamic and practical situations\nwhere there is no labeled dataset and feedback from the network might be\ndelayed. In response to these challenges, we present GSC, a generalizable\nservice coordinator that jointly considers service placement, scaling, and\ntraffic scheduling. GSC can operate in unseen situations without significant\nperformance degradation and outperforms existing state-of-the-art solutions by\n40%, as determined by simulating real-world network situations.",
            "author": [
                "Farzad Mohammadi",
                "Vahid Shah-Mansouri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02657v1",
                "http://arxiv.org/pdf/2311.02657v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02642v1",
            "title": "An Approach for Multi-Object Tracking with Two-Stage Min-Cost Flow",
            "updated": "2023-11-05T13:01:11Z",
            "published": "2023-11-05T13:01:11Z",
            "summary": "The minimum network flow algorithm is widely used in multi-target tracking.\nHowever, the majority of the present methods concentrate exclusively on\nminimizing cost functions whose values may not indicate accurate solutions\nunder occlusions. In this paper, by exploiting the properties of tracklets\nintersections and low-confidence detections, we develop a two-stage tracking\npipeline with an intersection mask that can accurately locate inaccurate\ntracklets which are corrected in the second stage. Specifically, we employ the\nminimum network flow algorithm with high-confidence detections as input in the\nfirst stage to obtain the candidate tracklets that need correction. Then we\nleverage the intersection mask to accurately locate the inaccurate parts of\ncandidate tracklets. The second stage utilizes low-confidence detections that\nmay be attributed to occlusions for correcting inaccurate tracklets. This\nprocess constructs a graph of nodes in inaccurate tracklets and low-confidence\nnodes and uses it for the second round of minimum network flow calculation. We\nperform sufficient experiments on popular MOT benchmark datasets and achieve\n78.4 MOTA on the test set of MOT16, 79.2 on MOT17, and 76.4 on MOT20, which\nshows that the proposed method is effective.",
            "author": [
                "Huining Li",
                "Yalong Jiang",
                "Xianlin Zeng",
                "Feng Li",
                "Zhipeng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02642v1",
                "http://arxiv.org/pdf/2311.02642v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02631v1",
            "title": "A Critical Perceptual Pre-trained Model for Complex Trajectory Recovery",
            "updated": "2023-11-05T12:20:39Z",
            "published": "2023-11-05T12:20:39Z",
            "summary": "The trajectory on the road traffic is commonly collected at a low sampling\nrate, and trajectory recovery aims to recover a complete and continuous\ntrajectory from the sparse and discrete inputs. Recently, sequential language\nmodels have been innovatively adopted for trajectory recovery in a pre-trained\nmanner: it learns road segment representation vectors, which will be used in\nthe downstream tasks. However, existing methods are incapable of handling\ncomplex trajectories: when the trajectory crosses remote road segments or makes\nseveral turns, which we call critical nodes, the quality of learned\nrepresentations deteriorates, and the recovered trajectories skip the critical\nnodes. This work is dedicated to offering a more robust trajectory recovery for\ncomplex trajectories. Firstly, we define the trajectory complexity based on the\ndetour score and entropy score and construct the complexity-aware semantic\ngraphs correspondingly. Then, we propose a Multi-view Graph and Complexity\nAware Transformer (MGCAT) model to encode these semantics in trajectory\npre-training from two aspects: 1) adaptively aggregate the multi-view graph\nfeatures considering trajectory pattern, and 2) higher attention to critical\nnodes in a complex trajectory. Such that, our MGCAT is perceptual when handling\nthe critical scenario of complex trajectories. Extensive experiments are\nconducted on large-scale datasets. The results prove that our method learns\nbetter representations for trajectory recovery, with 5.22% higher F1-score\noverall and 8.16% higher F1-score for complex trajectories particularly. The\ncode is available at https://github.com/bonaldli/ComplexTraj.",
            "author": [
                "Dedong Li",
                "Ziyue Li",
                "Zhishuai Li",
                "Lei Bai",
                "Qingyuan Gong",
                "Lijun Sun",
                "Wolfgang Ketter",
                "Rui Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02631v1",
                "http://arxiv.org/pdf/2311.02631v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02624v1",
            "title": "Failure detection for transport processes on networks",
            "updated": "2023-11-05T11:35:11Z",
            "published": "2023-11-05T11:35:11Z",
            "summary": "Diffusion on complex networks is a convenient framework to describe a great\nvariety of transport systems. Failure phenomena in a link of the network may\nsimulate the presence of a break or a congestion effect in the system. A real\ntime detection of failures can mitigate their effect and allow to optimize the\ncontrol procedures on the transport network. The main objective of this work is\nto provide a dimensionality reduction technique for a transport network where a\ndiffusive dynamics takes place, to detect presence of a failure by a limited\nnumber of observations. Our approach is based on the susceptibility response of\nthe network state under random perturbations of the link weights. The\ncorrelations among the nodes fluctuations is exploited in order to provide the\nclustering procedure. The network dimensionality is therefore reduced\nintroducing `representative nodes' for each cluster and generating a reduced\nnetwork model, whose dynamical state is detected by the observations. We\nrealize a failure identification procedure for the whole network, studying the\ndynamics of the coarse-grained network. The localization efficiency of the\nproposed clustering algorithm, averaging over all possible single-edge\nfailures, is compared with traditional structure-based clustering using\ndifferent graph configurations. We show that the proposed clustering algorithm\nis more sensitive than traditional clustering techniques to detect link failure\nwith higher stationary fluxes.",
            "author": [
                "Edoardo Rolando",
                "Armando Bazzani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02624v1",
                "http://arxiv.org/pdf/2311.02624v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "nlin.AO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02609v1",
            "title": "Dock Assignment and Truck Scheduling Problem; Consideration of Multiple\n  Scenarios with Resource Allocation Constraints",
            "updated": "2023-11-05T09:32:18Z",
            "published": "2023-11-05T09:32:18Z",
            "summary": "The notion of 'resource' plays an important role in the overall efficiency\nand performance of most cross-docks. The processing time can often be described\nin terms of the resources allocated to different trucks. Conversely, for a\ngiven processing time, different combinations of resources can be prescribed.\nWe study the problem of truck scheduling and dock assignment in the presence of\nresource constraints. In the absence of a closed-form (or well-defined) linear\nformulation describing the processing times as a function of resources, expert'\nknowledge has been mobilised to enable modelling of the problem as an integer\nlinear model. Two cases are taken into account: In the first one, the expert\nbelieves in his/her estimation of the processing time for every truck and only\nproposes a different combination of resources for his/her estimation, while in\nthe second one the expert proposes a limited number of resource deployment\nscenarios for serving trucks, each of which has a different combination of\nresources and different processing times. We propose a novel compact integer\nprogramming formulation for the problem, which is particularly designed with an\nembedded structure that can be exploited in dual decomposition techniques with\na remarkably computationally efficient column generation approach in this case.\nThe case in which a scenario with invariant processing time is considered and\nmodelled as a special case of the proposed model. Since a direct application of\ncommercial solvers such as CPLEX to solve instances of this problem is not\nrealistic, we propose a branch-and-price framework and, moreover, several\nclasses of valid inequalities. Our extensive computational experiments confirm\nthat the proposed exact solution framework is very efficient and viable in\nsolving real-size instances of the practice and in a reasonable amount of time.",
            "author": [
                "Rahimeh Neamatian Monemia",
                "Shahin Gelareh"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.cor.2022.106074",
                "http://arxiv.org/abs/2311.02609v1",
                "http://arxiv.org/pdf/2311.02609v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02598v1",
            "title": "Automated Camera Calibration via Homography Estimation with GNNs",
            "updated": "2023-11-05T08:45:26Z",
            "published": "2023-11-05T08:45:26Z",
            "summary": "Over the past few decades, a significant rise of camera-based applications\nfor traffic monitoring has occurred. Governments and local administrations are\nincreasingly relying on the data collected from these cameras to enhance road\nsafety and optimize traffic conditions. However, for effective data\nutilization, it is imperative to ensure accurate and automated calibration of\nthe involved cameras. This paper proposes a novel approach to address this\nchallenge by leveraging the topological structure of intersections. We propose\na framework involving the generation of a set of synthetic intersection\nviewpoint images from a bird's-eye-view image, framed as a graph of virtual\ncameras to model these images. Using the capabilities of Graph Neural Networks,\nwe effectively learn the relationships within this graph, thereby facilitating\nthe estimation of a homography matrix. This estimation leverages the\nneighbourhood representation for any real-world camera and is enhanced by\nexploiting multiple images instead of a single match. In turn, the homography\nmatrix allows the retrieval of extrinsic calibration parameters. As a result,\nthe proposed framework demonstrates superior performance on both synthetic\ndatasets and real-world cameras, setting a new state-of-the-art benchmark.",
            "author": [
                "Giacomo D'Amicantonio",
                "Egor Bondarev",
                "Peter H. N. De With"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02598v1",
                "http://arxiv.org/pdf/2311.02598v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02597v1",
            "title": "FloodBrain: Flood Disaster Reporting by Web-based Retrieval Augmented\n  Generation with an LLM",
            "updated": "2023-11-05T08:34:26Z",
            "published": "2023-11-05T08:34:26Z",
            "summary": "Fast disaster impact reporting is crucial in planning humanitarian\nassistance. Large Language Models (LLMs) are well known for their ability to\nwrite coherent text and fulfill a variety of tasks relevant to impact\nreporting, such as question answering or text summarization. However, LLMs are\nconstrained by the knowledge within their training data and are prone to\ngenerating inaccurate, or \"hallucinated\", information. To address this, we\nintroduce a sophisticated pipeline embodied in our tool FloodBrain\n(floodbrain.com), specialized in generating flood disaster impact reports by\nextracting and curating information from the web. Our pipeline assimilates\ninformation from web search results to produce detailed and accurate reports on\nflood events. We test different LLMs as backbones in our tool and compare their\ngenerated reports to human-written reports on different metrics. Similar to\nother studies, we find a notable correlation between the scores assigned by\nGPT-4 and the scores given by human evaluators when comparing our generated\nreports to human-authored ones. Additionally, we conduct an ablation study to\ntest our single pipeline components and their relevancy for the final reports.\nWith our tool, we aim to advance the use of LLMs for disaster impact reporting\nand reduce the time for coordination of humanitarian efforts in the wake of\nflood disasters.",
            "author": [
                "Grace Colverd",
                "Paul Darm",
                "Leonard Silverberg",
                "Noah Kasmanoff"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02597v1",
                "http://arxiv.org/pdf/2311.02597v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03396v1",
            "title": "Differentially Private Pre-Trained Model Fusion using Decentralized\n  Federated Graph Matching",
            "updated": "2023-11-05T07:56:00Z",
            "published": "2023-11-05T07:56:00Z",
            "summary": "Model fusion is becoming a crucial component in the context of\nmodel-as-a-service scenarios, enabling the delivery of high-quality model\nservices to local users. However, this approach introduces privacy risks and\nimposes certain limitations on its applications. Ensuring secure model exchange\nand knowledge fusion among users becomes a significant challenge in this\nsetting. To tackle this issue, we propose PrivFusion, a novel architecture that\npreserves privacy while facilitating model fusion under the constraints of\nlocal differential privacy. PrivFusion leverages a graph-based structure,\nenabling the fusion of models from multiple parties without necessitating\nretraining. By employing randomized mechanisms, PrivFusion ensures privacy\nguarantees throughout the fusion process. To enhance model privacy, our\napproach incorporates a hybrid local differentially private mechanism and\ndecentralized federated graph matching, effectively protecting both activation\nvalues and weights. Additionally, we introduce a perturbation filter adapter to\nalleviate the impact of randomized noise, thereby preserving the utility of the\nfused model. Through extensive experiments conducted on diverse image datasets\nand real-world healthcare applications, we provide empirical evidence\nshowcasing the effectiveness of PrivFusion in maintaining model performance\nwhile preserving privacy. Our contributions offer valuable insights and\npractical solutions for secure and collaborative data analysis within the\ndomain of privacy-preserving model fusion.",
            "author": [
                "Qian Chen",
                "Yiqiang Chen",
                "Xinlong Jiang",
                "Teng Zhang",
                "Weiwei Dai",
                "Wuliang Huang",
                "Zhen Yan",
                "Bo Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03396v1",
                "http://arxiv.org/pdf/2311.03396v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02575v1",
            "title": "On Stanley-Reisner rings with linear resolution",
            "updated": "2023-11-05T06:37:49Z",
            "published": "2023-11-05T06:37:49Z",
            "summary": "For a graph $G$, Bayer-Denker-Milutinovi\\'c-Rowlands-Sundaram-Xue study in\n\\cite{B-D-M-R-S-X} a new graph complex $\\Delta_k^t(G)$, namely the simplicial\ncomplex with facets that are complements to independent sets of size $k$ in\n$G$. They are interested in topological properties such as shellability, vertex\ndecomposability, homotopy type, and homology of these complexes. In this paper\nwe study more algebraic properties, such as Cohen-Macaulayness, Betti numbers,\nand linear resolutions of the Stanley-Reisner ring of these complexes and their\nAlexander duals.",
            "author": [
                "Ralf Froberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02575v1",
                "http://arxiv.org/pdf/2311.02575v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "13C70 13D02"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02571v1",
            "title": "Link residual closeness of graphs with fixed parameters",
            "updated": "2023-11-05T06:02:51Z",
            "published": "2023-11-05T06:02:51Z",
            "summary": "Link residual closeness is a newly proposed measure for network\nvulnerability. In this model, vertices are perfectly reliable and the links\nfail independently of each other. It measures the vulnerability even when the\nremoval of links does not disconnect the graph. In this paper, we characterize\nthose graphs that maximize the link residual closeness over the connected\ngraphs with fixed order and one parameters such as connectivity, edge\nconnectivity, bipartiteness, independence number, matching number, chromatic\nnumber, number of vertices and number of cut edges.",
            "author": [
                "Leyou Xu",
                "Chengli Li",
                "Bo Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02571v1",
                "http://arxiv.org/pdf/2311.02571v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02565v1",
            "title": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy",
            "updated": "2023-11-05T04:43:48Z",
            "published": "2023-11-05T04:43:48Z",
            "summary": "Sensors are commonly deployed to perceive the environment. However, due to\nthe high cost, sensors are usually sparsely deployed. Kriging is the tailored\ntask to infer the unobserved nodes (without sensors) using the observed source\nnodes (with sensors). The essence of kriging task is transferability. Recently,\nseveral inductive spatio-temporal kriging methods have been proposed based on\ngraph neural networks, being trained based on a graph built on top of observed\nnodes via pretext tasks such as masking nodes out and reconstructing them.\nHowever, the graph in training is inevitably much sparser than the graph in\ninference that includes all the observed and unobserved nodes. The learned\npattern cannot be well generalized for inference, denoted as graph gap. To\naddress this issue, we first present a novel Increment training strategy:\ninstead of masking nodes (and reconstructing them), we add virtual nodes into\nthe training graph so as to mitigate the graph gap issue naturally.\nNevertheless, the empty-shell virtual nodes without labels could have\nbad-learned features and lack supervision signals. To solve these issues, we\npair each virtual node with its most similar observed node and fuse their\nfeatures together; to enhance the supervision signal, we construct reliable\npseudo labels for virtual nodes. As a result, the learned pattern of virtual\nnodes could be safely transferred to real unobserved nodes for reliable\nkriging. We name our new Kriging model with Increment Training Strategy as\nKITS. Extensive experiments demonstrate that KITS consistently outperforms\nexisting kriging methods by large margins, e.g., the improvement over MAE score\ncould be as high as 18.33%.",
            "author": [
                "Qianxiong Xu",
                "Cheng Long",
                "Ziyue Li",
                "Sijie Ruan",
                "Rui Zhao",
                "Zhishuai Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02565v1",
                "http://arxiv.org/pdf/2311.02565v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02561v1",
            "title": "Ego-Network Transformer for Subsequence Classification in Time Series\n  Data",
            "updated": "2023-11-05T04:21:42Z",
            "published": "2023-11-05T04:21:42Z",
            "summary": "Time series classification is a widely studied problem in the field of time\nseries data mining. Previous research has predominantly focused on scenarios\nwhere relevant or foreground subsequences have already been extracted, with\neach subsequence corresponding to a single label. However, real-world time\nseries data often contain foreground subsequences that are intertwined with\nbackground subsequences. Successfully classifying these relevant subsequences\nrequires not only distinguishing between different classes but also accurately\nidentifying the foreground subsequences amidst the background. To address this\nchallenge, we propose a novel subsequence classification method that represents\neach subsequence as an ego-network, providing crucial nearest neighbor\ninformation to the model. The ego-networks of all subsequences collectively\nform a time series subsequence graph, and we introduce an algorithm to\nefficiently construct this graph. Furthermore, we have demonstrated the\nsignificance of enforcing temporal consistency in the prediction of adjacent\nsubsequences for the subsequence classification problem. To evaluate the\neffectiveness of our approach, we conducted experiments using 128 univariate\nand 30 multivariate time series datasets. The experimental results demonstrate\nthe superior performance of our method compared to alternative approaches.\nSpecifically, our method outperforms the baseline on 104 out of 158 datasets.",
            "author": [
                "Chin-Chia Michael Yeh",
                "Huiyuan Chen",
                "Yujie Fan",
                "Xin Dai",
                "Yan Zheng",
                "Vivian Lai",
                "Junpeng Wang",
                "Zhongfang Zhuang",
                "Liang Wang",
                "Wei Zhang",
                "Eamonn Keogh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02561v1",
                "http://arxiv.org/pdf/2311.02561v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07582v1",
            "title": "Evaluating the Potential of Leading Large Language Models in Reasoning\n  Biology Questions",
            "updated": "2023-11-05T03:34:17Z",
            "published": "2023-11-05T03:34:17Z",
            "summary": "Recent advances in Large Language Models (LLMs) have presented new\nopportunities for integrating Artificial General Intelligence (AGI) into\nbiological research and education. This study evaluated the capabilities of\nleading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in\nanswering conceptual biology questions. The models were tested on a\n108-question multiple-choice exam covering biology topics in molecular biology,\nbiological techniques, metabolic engineering, and synthetic biology. Among the\nmodels, GPT-4 achieved the highest average score of 90 and demonstrated the\ngreatest consistency across trials with different prompts. The results\nindicated GPT-4's proficiency in logical reasoning and its potential to aid\nbiology research through capabilities like data analysis, hypothesis\ngeneration, and knowledge integration. However, further development and\nvalidation are still required before the promise of LLMs in accelerating\nbiological discovery can be realized.",
            "author": [
                "Xinyu Gong",
                "Jason Holmes",
                "Yiwei Li",
                "Zhengliang Liu",
                "Qi Gan",
                "Zihao Wu",
                "Jianli Zhang",
                "Yusong Zou",
                "Yuxi Teng",
                "Tian Jiang",
                "Hongtu Zhu",
                "Wei Liu",
                "Tianming Liu",
                "Yajun Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07582v1",
                "http://arxiv.org/pdf/2311.07582v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02533v1",
            "title": "Precision ground-state energy calculation for the water molecule on a\n  superconducting quantum processor",
            "updated": "2023-11-05T01:05:58Z",
            "published": "2023-11-05T01:05:58Z",
            "summary": "The accurate computation of properties of large molecular systems is\nclassically infeasible and is one of the applications in which it is hoped that\nquantum computers will demonstrate an advantage over classical devices.\nHowever, due to the limitations of present-day quantum hardware,\nvariational-hybrid algorithms introduced to tackle these problems struggle to\nmeet the accuracy and precision requirements of chemical applications. Here, we\napply the Quantum Computed Moments (QCM) approach combined with a variety of\nnoise-mitigation techniques to an 8 qubit/spin-orbital representation of the\nwater molecule (H$_2$O). A noise-stable improvement on the variational result\nfor a 4-excitation trial-state (circuit depth 25, 22 CNOTs) was obtained, with\nthe ground-state energy computed to be within $1.4\\pm1.2$ mHa of exact\ndiagonalisation in the 14 spin-orbital basis. Thus, the QCM approach, despite\nan increased number of measurements and noisy quantum hardware (CNOT error\nrates c.1% corresponding to expected error rates on the trial-state circuit of\norder 20%), is able to determine the ground-state energy of a non-trivial\nmolecular system at the required accuracy (c.0.1%). To the best of our\nknowledge, these results are the largest calculations performed on a physical\nquantum computer to date in terms of encoding individual spin-orbitals\nproducing chemically relevant accuracy, and a promising indicator of how such\nhybrid approaches might scale to problems of interest in the\nlow-error/fault-tolerant regimes as quantum computers develop.",
            "author": [
                "Michael A. Jones",
                "Harish J. Vallury",
                "Lloyd C. L. Hollenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02533v1",
                "http://arxiv.org/pdf/2311.02533v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02525v1",
            "title": "QOCO: A QoE-Oriented Computation Offloading Algorithm based on Deep\n  Reinforcement Learning for Mobile Edge Computing",
            "updated": "2023-11-04T23:22:42Z",
            "published": "2023-11-04T23:22:42Z",
            "summary": "In the realm of mobile edge computing (MEC), efficient computation task\noffloading plays a pivotal role in ensuring a seamless quality of experience\n(QoE) for users. Maintaining a high QoE is paramount in today's interconnected\nworld, where users demand responsive and reliable services. This challenge\nstands as one of the most primary key factors contributing to handling dynamic\nand uncertain mobile environment. In this study, we delve into computation\noffloading in MEC systems, where strict task processing deadlines and energy\nconstraints can adversely affect the system performance. We formulate the\ncomputation task offloading problem as a Markov decision process (MDP) to\nmaximize the long-term QoE of each user individually. We propose a\ndecentralized QoE-oriented computation offloading (QOCO) algorithm based on\ndeep reinforcement learning (DRL) that empowers mobile devices to make their\noffloading decisions without requiring knowledge of decisions made by other\ndevices. Through numerical studies, we evaluate the performance of QOCO.\nSimulation results validate that the QOCO algorithm efficiently exploits the\ncomputational resources of edge nodes. Consequently, it can complete 14% more\ntasks and reduce task delay and energy consumption by 9% and 6%, respectively.\nThese together contribute to a significant improvement of at least 37% in\naverage QoE compared to an existing algorithm.",
            "author": [
                "Iman Rahmati",
                "Hamed Shah-Mansouri",
                "Ali Movaghar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02525v1",
                "http://arxiv.org/pdf/2311.02525v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02520v2",
            "title": "Single-Source Shortest Paths with Negative Real Weights in\n  $\\tilde{O}(mn^{8/9})$ Time",
            "updated": "2023-11-13T15:54:07Z",
            "published": "2023-11-04T22:35:39Z",
            "summary": "This paper presents a randomized algorithm for the problem of single-source\nshortest paths on directed graphs with real (both positive and negative) edge\nweights. Given an input graph with $n$ vertices and $m$ edges, the algorithm\ncompletes in $\\tilde{O}(mn^{8/9})$ time with high probability. For\nreal-weighted graphs, this result constitutes the first asymptotic improvement\nover the classic $O(mn)$-time algorithm variously attributed to Shimbel,\nBellman, Ford, and Moore.",
            "author": [
                "Jeremy T. Fineman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02520v2",
                "http://arxiv.org/pdf/2311.02520v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02505v1",
            "title": "On the Aldous-Caputo Spectral Gap Conjecture for Hypergraphs",
            "updated": "2023-11-04T20:51:01Z",
            "published": "2023-11-04T20:51:01Z",
            "summary": "In their celebrated paper (arXiv:0906.1238), Caputo, Liggett and Richthammer\nproved Aldous' conjecture and showed that for an arbitrary finite graph, the\nspectral gap of the interchange process is equal to the spectral gap of the\nunderlying random walk. A crucial ingredient in the proof was the Octopus\nInequality - a certain inequality of operators in the group ring\n$\\mathbb{R}[S_n]$ of the symmetric group. Here we generalize the Octopus\nInequality and apply it to generalize the Caputo-Liggett-Richthammer Theorem to\ncertain hypergraphs, proving some cases of a conjecture of Caputo.",
            "author": [
                "Gil Alon",
                "Gady Kozma",
                "Doron Puder"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02505v1",
                "http://arxiv.org/pdf/2311.02505v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "math.PR",
                "math.RT",
                "20c30 (Primary) 05c81, 05c65, 20B30, 05c50, 60b15, 60j10,\n  60k35(Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02495v2",
            "title": "Uncertainty Quantification in Multivariable Regression for Material\n  Property Prediction with Bayesian Neural Networks",
            "updated": "2023-12-05T18:00:59Z",
            "published": "2023-11-04T19:40:16Z",
            "summary": "With the increased use of data-driven approaches and machine learning-based\nmethods in material science, the importance of reliable uncertainty\nquantification (UQ) of the predicted variables for informed decision-making\ncannot be overstated. UQ in material property prediction poses unique\nchallenges, including the multi-scale and multi-physics nature of advanced\nmaterials, intricate interactions between numerous factors, limited\navailability of large curated datasets for model training, etc. Recently,\nBayesian Neural Networks (BNNs) have emerged as a promising approach for UQ,\noffering a probabilistic framework for capturing uncertainties within neural\nnetworks. In this work, we introduce an approach for UQ within physics-informed\nBNNs, which integrates knowledge from governing laws in material modeling to\nguide the models toward physically consistent predictions. To evaluate the\neffectiveness of this approach, we present case studies for predicting the\ncreep rupture life of steel alloys. Experimental validation with three datasets\nof collected measurements from creep tests demonstrates the ability of BNNs to\nproduce accurate point and uncertainty estimates that are competitive or exceed\nthe performance of the conventional method of Gaussian Process Regression.\nSimilarly, we evaluated the suitability of BNNs for UQ in an active learning\napplication and reported competitive performance. The most promising framework\nfor creep life prediction is BNNs based on Markov Chain Monte Carlo\napproximation of the posterior distribution of network parameters, as it\nprovided more reliable results in comparison to BNNs based on variational\ninference approximation or related NNs with probabilistic outputs. The codes\nare available at:\nhttps://github.com/avakanski/Creep-uncertainty-quantification.",
            "author": [
                "Longze Li",
                "Jiang Chang",
                "Aleksandar Vakanski",
                "Yachun Wang",
                "Tiankai Yao",
                "Min Xian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02495v2",
                "http://arxiv.org/pdf/2311.02495v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02479v1",
            "title": "Accurate and precise measurement of the wavelength of\n  4d$^4D_{7/2}\\rightarrow$5p$^4P^\\circ_{5/2}$ transition of Kr II",
            "updated": "2023-11-04T18:39:39Z",
            "published": "2023-11-04T18:39:39Z",
            "summary": "Electric propulsion requires exhaustive ground test campaigns to obtain an\naccurate characterization of the propulsion devices, or thrusters, used by the\nspacecraft. Among the many plasma parameters, accurately measured during the\ntests, that of the ion velocity is key, and can be measured using non-intrusive\ntools such as Laser-Induced Fluorescence (LIF) diagnostics. The ion velocity is\ninferred by Doppler shift measurements that presupposes a precise and accurate\nknowledge of the wavelength of excitation of the ions at rest. Today electric\npropulsion is moving towards the use of Krypton as a propellant, due to the\ndramatic increase in cost of the more advantageous Xenon gas propellant,\ncommonly used until now. The utilization of Krypton implies that LIF diagnostic\ntool used be adapted accordingly. In this work an accurate measurement of the\nwavelength of 4d$^4D_{7/2}\\rightarrow$5p$^4P^\\circ_{5/2}$ transition of Kr II\n(one of the most advantageous transitions for LIF diagnostics) is performed in\nthe plasma of a hollow cathode acting as a neutralizer. These measurements will\nsignificantly improve accuracy in the determination of the ion velocity.",
            "author": [
                "Y. Dancheva",
                "P. Coniglio",
                "D. Pagano",
                "A. Garde",
                "F. Scortecci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02479v1",
                "http://arxiv.org/pdf/2311.02479v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "physics.atom-ph",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02460v1",
            "title": "Extracting Network Structures from Corporate Organization Charts Using\n  Heuristic Image Processing",
            "updated": "2023-11-04T17:32:50Z",
            "published": "2023-11-04T17:32:50Z",
            "summary": "Organizational structure of corporations has potential to provide\nimplications for dynamics and performance of corporate operations. However,\nthis subject has remained unexplored because of the lack of readily available\norganization network datasets. To overcome the this gap, we developed a new\nheuristic image-processing method to extract and reconstruct organization\nnetwork data from published organization charts. Our method analyzes a PDF file\nof a corporate organization chart and detects text labels, boxes, connecting\nlines, and other objects through multiple steps of heuristically implemented\nimage processing. The detected components are reorganized together into a\nPython's NetworkX Graph object for visualization, validation and further\nnetwork analysis. We applied the developed method to the organization charts of\nall the listed firms in Japan shown in the ``Organization Chart/System Diagram\nHandbook'' published by Diamond, Inc., from 2008 to 2011. Out of the 10,008\norganization chart PDF files, our method was able to reconstruct 4,606\norganization networks (data acquisition success rate: 46%). For each\nreconstructed organization network, we measured several network diagnostics,\nwhich will be used for further statistical analysis to investigate their\npotential correlations with corporate behavior and performance.",
            "author": [
                "Hiroki Sayama",
                "Junichi Yamanoi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02460v1",
                "http://arxiv.org/pdf/2311.02460v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CV",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02431v1",
            "title": "The contribution of US broadband infrastructure subsidy and investment\n  programs to GDP using input-output modeling",
            "updated": "2023-11-04T15:21:55Z",
            "published": "2023-11-04T15:21:55Z",
            "summary": "More than one-fifth of the US population does not subscribe to a fixed\nbroadband service despite being a recognized merit good. For example, less than\n4% of citizens earning more than US \\$70k annually do not have broadband,\ncompared to 26% of those earning below US \\$20k annually. To address this, the\nBiden Administration has undertaken one of the largest broadband investment\nprograms ever via The Bipartisan Infrastructure Law, with the aim of addressing\nthis disparity and expanding broadband connectivity to all citizens. Proponents\nstate this will reduce the US digital divide once-and-for-all. However,\ndetractors say the program leads to unprecedented borrowing at a late stage of\nthe economic cycle, leaving little fiscal headroom. Subsequently, in this\npaper, we examine broadband availability, adoption, and need and then construct\nan input-output model to explore the macroeconomic impacts of broadband\nspending in Gross Domestic Product (GDP) terms. Finally, we quantify\ninter-sectoral macroeconomic supply chain linkages from this investment. The\nresults indicate that federal broadband investment of US \\$42 billion has the\npotential to increase GDP by up to US \\$216 billion, equating to 0.2% of annual\nUS GDP over the next five years, with an estimated Keynesian investment\nmultiplier of 2.89. To our knowledge, we contribute the first economic impact\nassessment of the US Bipartisan Infrastructure Law to the literature.",
            "author": [
                "Matthew Sprintson",
                "Edward Oughton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02431v1",
                "http://arxiv.org/pdf/2311.02431v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02430v1",
            "title": "Fr\u00f6berg's Theorem, vertex splittability and higher independence\n  complexes",
            "updated": "2023-11-04T15:17:23Z",
            "published": "2023-11-04T15:17:23Z",
            "summary": "A celebrated theorem of Fr\\\"oberg gives a complete combinatorial\nclassification of quadratic square-free monomial ideals with a linear\nresolution. A generalization of this theorem to higher degree square-free\nmonomial ideals is an active area of research. The existence of a linear\nresolution of such ideals often depends on the field over which the polynomial\nring is defined. Hence, it is too much to expect that in the higher degree case\na linear resolution can be identified purely using a combinatorial feature of\nan associated combinatorial structure. However, some classes of ideals having\nlinear resolutions have been identified using combinatorial structures. In the\npresent paper, we use the notion of $r$-independence to construct an\n$r$-uniform hypergraph from the given graph. We then show that when the\nunderlying graph is co-chordal, the corresponding edge ideal is vertex\nsplittable, a condition stronger than having a linear resolution. We use this\nresult to explicitly compute graded Betti numbers for various graph classes.\nFinally, we give a different proof for the existence of a linear resolution\nusing the topological notion of $r$-collapsibility.",
            "author": [
                "Priyavrat Deshpande",
                "Amit Roy",
                "Anurag Singh",
                "Adam Van Tuyl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02430v1",
                "http://arxiv.org/pdf/2311.02430v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO",
                "13F55, 05E45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02427v1",
            "title": "Succinct Data Structure for Graphs with $d$-Dimensional\n  $t$-Representation",
            "updated": "2023-11-04T15:11:48Z",
            "published": "2023-11-04T15:11:48Z",
            "summary": "Erd\\H{o}s and West (Discrete Mathematics'85) considered the class of $n$\nvertex intersection graphs which have a {\\em $d$-dimensional} {\\em\n$t$-representation}, that is, each vertex of a graph in the class has an\nassociated set consisting of at most $t$ $d$-dimensional axis-parallel boxes.\nIn particular, for a graph $G$ and for each $d \\geq 1$, they consider $i_d(G)$\nto be the minimum $t$ for which $G$ has such a representation. For fixed $t$\nand $d$, they consider the class of $n$ vertex labeled graphs for which $i_d(G)\n\\leq t$, and prove an upper bound of $(2nt+\\frac{1}{2})d \\log n - (n -\n\\frac{1}{2})d \\log(4\\pi t)$ on the logarithm of size of the class.\n  In this work, for fixed $t$ and $d$ we consider the class of $n$ vertex\nunlabeled graphs which have a {\\em $d$-dimensional $t$-representation}, denoted\nby $\\mathcal{G}_{t,d}$. We address the problem of designing a succinct data\nstructure for the class $\\mathcal{G}_{t,d}$ in an attempt to generalize the\nrelatively recent results on succinct data structures for interval graphs\n(Algorithmica'21). To this end, for each $n$ such that $td^2$ is in $o(n / \\log\nn)$, we first prove a lower bound of $(2dt-1)n \\log n - O(ndt \\log \\log\nn)$-bits on the size of any data structure for encoding an arbitrary graph that\nbelongs to $\\mathcal{G}_{t,d}$.\n  We then present a $((2dt-1)n \\log n + dt\\log t + o(ndt \\log n))$-bit data\nstructure for $\\mathcal{G}_{t,d}$ that supports navigational queries\nefficiently. Contrasting this data structure with our lower bound argument, we\nshow that for each fixed $t$ and $d$, and for all $n \\geq 0$ when $td^2$ is in\n$o(n/\\log n)$ our data structure for $\\mathcal{G}_{t,d}$ is succinct.\n  As a byproduct, we also obtain succinct data structures for graphs of bounded\nboxicity (denoted by $d$ and $t = 1$) and graphs of bounded interval number\n(denoted by $t$ and $d=1$) when $td^2$ is in $o(n/\\log n)$.",
            "author": [
                "Girish Balakrishnan",
                "Sankardeep Chakraborty",
                "Seungbum Jo",
                "N S Narayanaswamy",
                "Kunihiko Sadakane"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02427v1",
                "http://arxiv.org/pdf/2311.02427v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02423v1",
            "title": "Payoff-based learning with matrix multiplicative weights in quantum\n  games",
            "updated": "2023-11-04T14:56:17Z",
            "published": "2023-11-04T14:56:17Z",
            "summary": "In this paper, we study the problem of learning in quantum games - and other\nclasses of semidefinite games - with scalar, payoff-based feedback. For\nconcreteness, we focus on the widely used matrix multiplicative weights (MMW)\nalgorithm and, instead of requiring players to have full knowledge of the game\n(and/or each other's chosen states), we introduce a suite of\nminimal-information matrix multiplicative weights (3MW) methods tailored to\ndifferent information frameworks. The main difficulty to attaining convergence\nin this setting is that, in contrast to classical finite games, quantum games\nhave an infinite continuum of pure states (the quantum equivalent of pure\nstrategies), so standard importance-weighting techniques for estimating payoff\nvectors cannot be employed. Instead, we borrow ideas from bandit convex\noptimization and we design a zeroth-order gradient sampler adapted to the\nsemidefinite geometry of the problem at hand. As a first result, we show that\nthe 3MW method with deterministic payoff feedback retains the\n$\\mathcal{O}(1/\\sqrt{T})$ convergence rate of the vanilla, full information MMW\nalgorithm in quantum min-max games, even though the players only observe a\nsingle scalar. Subsequently, we relax the algorithm's information requirements\neven further and we provide a 3MW method that only requires players to observe\na random realization of their payoff observable, and converges to equilibrium\nat an $\\mathcal{O}(T^{-1/4})$ rate. Finally, going beyond zero-sum games, we\nshow that a regularized variant of the proposed 3MW method guarantees local\nconvergence with high probability to all equilibria that satisfy a certain\nfirst-order stability condition.",
            "author": [
                "Kyriakos Lotidis",
                "Panayotis Mertikopoulos",
                "Nicholas Bambos",
                "Jose Blanchet"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02423v1",
                "http://arxiv.org/pdf/2311.02423v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.LG",
                "math.OC",
                "quant-ph",
                "Primary 91A10, 91A26, 37N40, secondary 68Q32, 81Q93"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02416v2",
            "title": "Expressive Power of Hypergraph Lambek Grammars",
            "updated": "2023-11-29T08:08:23Z",
            "published": "2023-11-04T14:39:20Z",
            "summary": "Hypergraph Lambek grammars (HL-grammars) is a novel logical approach to\ngenerating graph languages based on the hypergraph Lambek calculus. In this\npaper, we establish a precise relation between HL-grammars and hypergraph\ngrammars based on the double pushout (DPO) approach: we prove that HL-grammars\ngenerate the same class of languages as DPO grammars with the linear\nrestriction on lengths of derivations. This can be viewed as a complete\ndescription of the expressive power of HL-grammars and also as an analogue of\nthe Pentus theorem, which states that Lambek grammars generate the same class\nof languages as context-free grammars. As a corollary, we prove that\nHL-grammars subsume contextual hyperedge replacement grammars.",
            "author": [
                "Tikhon Pshenitsyn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02416v2",
                "http://arxiv.org/pdf/2311.02416v2"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16140v1",
            "title": "Adapting Segment Anything Model (SAM) through Prompt-based Learning for\n  Enhanced Protein Identification in Cryo-EM Micrographs",
            "updated": "2023-11-04T14:20:08Z",
            "published": "2023-11-04T14:20:08Z",
            "summary": "Cryo-electron microscopy (cryo-EM) remains pivotal in structural biology, yet\nthe task of protein particle picking, integral for 3D protein structure\nconstruction, is laden with manual inefficiencies. While recent AI tools such\nas Topaz and crYOLO are advancing the field, they do not fully address the\nchallenges of cryo-EM images, including low contrast, complex shapes, and\nheterogeneous conformations. This study explored prompt-based learning to adapt\nthe state-of-the-art image segmentation foundation model Segment Anything Model\n(SAM) for cryo-EM. This focus was driven by the desire to optimize model\nperformance with a small number of labeled data without altering pre-trained\nparameters, aiming for a balance between adaptability and foundational\nknowledge retention. Through trials with three prompt-based learning\nstrategies, namely head prompt, prefix prompt, and encoder prompt, we observed\nenhanced performance and reduced computational requirements compared to the\nfine-tuning approach. This work not only highlights the potential of prompting\nSAM in protein identification from cryo-EM micrographs but also suggests its\nbroader promise in biomedical image segmentation and object detection.",
            "author": [
                "Fei He",
                "Zhiyuan Yang",
                "Mingyue Gao",
                "Biplab Poudel",
                "Newgin Sam Ebin Sam Dhas",
                "Rajan Gyawali",
                "Ashwin Dhakal",
                "Jianlin Cheng",
                "Dong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16140v1",
                "http://arxiv.org/pdf/2311.16140v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02399v1",
            "title": "Entropy Aware Training for Fast and Accurate Distributed GNN",
            "updated": "2023-11-04T13:11:49Z",
            "published": "2023-11-04T13:11:49Z",
            "summary": "Several distributed frameworks have been developed to scale Graph Neural\nNetworks (GNNs) on billion-size graphs. On several benchmarks, we observe that\nthe graph partitions generated by these frameworks have heterogeneous data\ndistributions and class imbalance, affecting convergence, and resulting in\nlower performance than centralized implementations. We holistically address\nthese challenges and develop techniques that reduce training time and improve\naccuracy. We develop an Edge-Weighted partitioning technique to improve the\nmicro average F1 score (accuracy) by minimizing the total entropy. Furthermore,\nwe add an asynchronous personalization phase that adapts each compute-host's\nmodel to its local data distribution. We design a class-balanced sampler that\nconsiderably speeds up convergence. We implemented our algorithms on the\nDistDGL framework and observed that our training techniques scale much better\nthan the existing training approach. We achieved a (2-3x) speedup in training\ntime and 4\\% improvement on average in micro-F1 scores on 5 large graph\nbenchmarks compared to the standard baselines.",
            "author": [
                "Dhruv Deshmukh",
                "Gagan Raj Gupta",
                "Manisha Chawla",
                "Vishwesh Jatala",
                "Anirban Haldar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02399v1",
                "http://arxiv.org/pdf/2311.02399v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "I.5.1; I.5.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02398v1",
            "title": "CDR-Adapter: Learning Adapters to Dig Out More Transferring Ability for\n  Cross-Domain Recommendation Models",
            "updated": "2023-11-04T13:03:24Z",
            "published": "2023-11-04T13:03:24Z",
            "summary": "Data sparsity and cold-start problems are persistent challenges in\nrecommendation systems. Cross-domain recommendation (CDR) is a promising\nsolution that utilizes knowledge from the source domain to improve the\nrecommendation performance in the target domain. Previous CDR approaches have\nmainly followed the Embedding and Mapping (EMCDR) framework, which involves\nlearning a mapping function to facilitate knowledge transfer. However, these\napproaches necessitate re-engineering and re-training the network structure to\nincorporate transferrable knowledge, which can be computationally expensive and\nmay result in catastrophic forgetting of the original knowledge. In this paper,\nwe present a scalable and efficient paradigm to address data sparsity and\ncold-start issues in CDR, named CDR-Adapter, by decoupling the original\nrecommendation model from the mapping function, without requiring\nre-engineering the network structure. Specifically, CDR-Adapter is a novel\nplug-and-play module that employs adapter modules to align feature\nrepresentations, allowing for flexible knowledge transfer across different\ndomains and efficient fine-tuning with minimal training costs. We conducted\nextensive experiments on the benchmark dataset, which demonstrated the\neffectiveness of our approach over several state-of-the-art CDR approaches.",
            "author": [
                "Yanyu Chen",
                "Yao Yao",
                "Wai Kin Victor Chan",
                "Li Xiao",
                "Kai Zhang",
                "Liang Zhang",
                "Yun Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02398v1",
                "http://arxiv.org/pdf/2311.02398v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02393v1",
            "title": "Continual Learning of Unsupervised Monocular Depth from Videos",
            "updated": "2023-11-04T12:36:07Z",
            "published": "2023-11-04T12:36:07Z",
            "summary": "Spatial scene understanding, including monocular depth estimation, is an\nimportant problem in various applications, such as robotics and autonomous\ndriving. While improvements in unsupervised monocular depth estimation have\npotentially allowed models to be trained on diverse crowdsourced videos, this\nremains underexplored as most methods utilize the standard training protocol,\nwherein the models are trained from scratch on all data after new data is\ncollected. Instead, continual training of models on sequentially collected data\nwould significantly reduce computational and memory costs. Nevertheless, naive\ncontinual training leads to catastrophic forgetting, where the model\nperformance deteriorates on older domains as it learns on newer domains,\nhighlighting the trade-off between model stability and plasticity. While\nseveral techniques have been proposed to address this issue in image\nclassification, the high-dimensional and spatiotemporally correlated outputs of\ndepth estimation make it a distinct challenge. To the best of our knowledge, no\nframework or method currently exists focusing on the problem of continual\nlearning in depth estimation. Thus, we introduce a framework that captures the\nchallenges of continual unsupervised depth estimation (CUDE), and define the\nnecessary metrics to evaluate model performance. We propose a rehearsal-based\ndual-memory method, MonoDepthCL, which utilizes spatiotemporal consistency for\ncontinual learning in depth estimation, even when the camera intrinsics are\nunknown.",
            "author": [
                "Hemang Chawla",
                "Arnav Varma",
                "Elahe Arani",
                "Bahram Zonooz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02393v1",
                "http://arxiv.org/pdf/2311.02393v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02392v1",
            "title": "Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot\n  Classification",
            "updated": "2023-11-04T12:28:04Z",
            "published": "2023-11-04T12:28:04Z",
            "summary": "The conventional few-shot classification aims at learning a model on a large\nlabeled base dataset and rapidly adapting to a target dataset that is from the\nsame distribution as the base dataset. However, in practice, the base and the\ntarget datasets of few-shot classification are usually from different domains,\nwhich is the problem of cross-domain few-shot classification. We tackle this\nproblem by making a small proportion of unlabeled images in the target domain\naccessible in the training stage. In this setup, even though the base data are\nsufficient and labeled, the large domain shift still makes transferring the\nknowledge from the base dataset difficult. We meticulously design a cross-level\nknowledge distillation method, which can strengthen the ability of the model to\nextract more discriminative features in the target dataset by guiding the\nnetwork's shallow layers to learn higher-level information. Furthermore, in\norder to alleviate the overfitting in the evaluation stage, we propose a\nfeature denoising operation which can reduce the feature redundancy and\nmitigate overfitting. Our approach can surpass the previous state-of-the-art\nmethod, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot\nclassification tasks on average in the BSCD-FSL benchmark. The implementation\ncode will be available at https://github.com/jarucezh/cldfd.",
            "author": [
                "Hao Zheng",
                "Runqi Wang",
                "Jianzhuang Liu",
                "Asako Kanezaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02392v1",
                "http://arxiv.org/pdf/2311.02392v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02388v1",
            "title": "A theoretical expansion of the Sprout game",
            "updated": "2023-11-04T12:09:36Z",
            "published": "2023-11-04T12:09:36Z",
            "summary": "Sprout is a two-player pen and paper game which starts with $n$ vertices, and\nthe players take turns to join two pre-existing dots by a subdivided edge while\nkeeping the graph sub-cubic planar at all times. The first player not being\nable to move loses. A major conjecture claims that Player 1 has a winning\nstrategy if and only if $n \\equiv 3,4,5$ ($\\bmod~6$). The conjecture is\nverified until $44$, and a few isolated values of $n$, usually with the help of\na computer. However, to the best of our understanding, not too much progress\ncould be made towards finding a theoretical proof of the conjecture till now.\n  In this article, we try to take a bottom-up approach and start building a\ntheory around the problem. We start by expanding a related game called Brussels\nSprout (where dots are replaced by crosses) introduced by Conway, possibly to\nhelp the understanding of Sprout. In particular, we introduce and study a\ngeneralized version of Brussels Sprout where crosses are replaced by a dot\nhaving an arbitrary number of ``partial edges'' (say, general cross) coming\nout, and planar graphs are replaced by any (pre-decided) hereditary class of\ngraphs. We study the game for forests, graphs on surfaces, and sparse planar\ngraphs. We also do a nimber characterization of the game when the hereditary\nclass is taken to be triangle-free planar graphs, and we have started the game\nwith two arbitrary generalized crosses. Moreover, while studying this\nparticular case, we naturally stumble upon a circular version of the same game\nand solve a difficult nimber characterization using the method of structural\ninduction. The above mentioned proof may potentially be one approach to solving\nthe Sprout conjecture.",
            "author": [
                "Soura Sena Das",
                "Zin Mar Myint",
                "Soumen Nandi",
                "Sagnik Sen",
                "\u00c9ric Sopena"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02388v1",
                "http://arxiv.org/pdf/2311.02388v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02367v1",
            "title": "Quantum Communications",
            "updated": "2023-11-04T10:34:39Z",
            "published": "2023-11-04T10:34:39Z",
            "summary": "The second quantum revolution has been picking up momentum over the last\ndecade. Quantum technologies are starting to attract more attention from\ngovernments, private companies, investors, and public. The ability to control\nindividual quantum systems for the purpose of information processing and\ncommunication is no longer a theoretical dream, but is steadily becoming\nroutine in laboratories and startups around the world. With this comes the need\nto educate the future generation of quantum engineers. This textbook is a\ncompanion to our video lectures on Overview of Quantum Communications from the\nQ-Leap Education project known as Quantum Academy of Science and Technology. It\nis a gentle introduction to quantum networks, and is suitable for use as a\ntextbook for undergraduate students of diverse background. No prior knowledge\nof quantum physics or quantum information is assumed. Exercises are included in\neach chapter.",
            "author": [
                "Michal Hajdu\u0161ek",
                "Rodney Van Meter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02367v1",
                "http://arxiv.org/pdf/2311.02367v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02357v1",
            "title": "Contrastive Deep Nonnegative Matrix Factorization for Community\n  Detection",
            "updated": "2023-11-04T09:50:37Z",
            "published": "2023-11-04T09:50:37Z",
            "summary": "Recently, nonnegative matrix factorization (NMF) has been widely adopted for\ncommunity detection, because of its better interpretability. However, the\nexisting NMF-based methods have the following three problems: 1) they directly\ntransform the original network into community membership space, so it is\ndifficult for them to capture the hierarchical information; 2) they often only\npay attention to the topology of the network and ignore its node attributes; 3)\nit is hard for them to learn the global structure information necessary for\ncommunity detection. Therefore, we propose a new community detection algorithm,\nnamed Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we\ndeepen NMF to strengthen its capacity for information extraction. Subsequently,\ninspired by contrastive learning, our algorithm creatively constructs network\ntopology and node attributes as two contrasting views. Furthermore, we utilize\na debiased negative sampling layer and learn node similarity at the community\nlevel, thereby enhancing the suitability of our model for community detection.\nWe conduct experiments on three public real graph datasets and the proposed\nmodel has achieved better results than state-of-the-art methods. Code available\nat https://github.com/6lyc/CDNMF.git.",
            "author": [
                "Yuecheng Li",
                "Jialong Chen",
                "Chuan Chen",
                "Lei Yang",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02357v1",
                "http://arxiv.org/pdf/2311.02357v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02356v1",
            "title": "MATA*: Combining Learnable Node Matching with A* Algorithm for\n  Approximate Graph Edit Distance Computation",
            "updated": "2023-11-04T09:33:08Z",
            "published": "2023-11-04T09:33:08Z",
            "summary": "Graph Edit Distance (GED) is a general and domain-agnostic metric to measure\ngraph similarity, widely used in graph search or retrieving tasks. However, the\nexact GED computation is known to be NP-complete. For instance, the widely used\nA* algorithms explore the entire search space to find the optimal solution\nwhich inevitably suffers scalability issues. Learning-based methods apply graph\nrepresentation techniques to learn the GED by formulating a regression task,\nwhich can not recover the edit path and lead to inaccurate GED approximation\n(i.e., the predicted GED is smaller than the exact). To this end, in this work,\nwe present a data-driven hybrid approach MATA* for approximate GED computation\nbased on Graph Neural Networks (GNNs) and A* algorithms, which models from the\nperspective of learning to match nodes instead of directly regressing GED.\nSpecifically, aware of the structure-dominant operations (i.e.,node and edge\ninsertion/deletion) property in GED computation, a structure-enhanced GNN is\nfirstly designed to jointly learn local and high-order structural information\nfor node embeddings for node matchings. Second, top-k candidate nodes are\nproduced via a differentiable top-k operation to enable the training for node\nmatchings, which is adhering to another property of GED, i.e., multiple optimal\nnode matchings. Third, benefiting from the candidate nodes, MATA* only performs\non the promising search directions, reaching the solution efficiently. Finally,\nextensive experiments show the superiority of MATA* as it significantly\noutperforms the combinatorial search-based, learning-based and hybrid methods\nand scales well to large-size graphs.",
            "author": [
                "Junfeng Liu",
                "Min Zhou",
                "Shuai Ma",
                "Lujia Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02356v1",
                "http://arxiv.org/pdf/2311.02356v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02331v1",
            "title": "NODLINK: An Online System for Fine-Grained APT Attack Detection and\n  Investigation",
            "updated": "2023-11-04T05:36:59Z",
            "published": "2023-11-04T05:36:59Z",
            "summary": "Advanced Persistent Threats (APT) attacks have plagued modern enterprises,\ncausing significant financial losses. To counter these attacks, researchers\npropose techniques that capture the complex and stealthy scenarios of APT\nattacks by using provenance graphs to model system entities and their\ndependencies. Particularly, to accelerate attack detection and reduce financial\nlosses, online provenance-based detection systems that detect and investigate\nAPT attacks under the constraints of timeliness and limited resources are in\ndire need. Unfortunately, existing online systems usually sacrifice detection\ngranularity to reduce computational complexity and produce provenance graphs\nwith more than 100,000 nodes, posing challenges for security admins to\ninterpret the detection results. In this paper, we design and implement\nNodLink, the first online detection system that maintains high detection\naccuracy without sacrificing detection granularity. Our insight is that the APT\nattack detection process in online provenance-based detection systems can be\nmodeled as a Steiner Tree Problem (STP), which has efficient online\napproximation algorithms that recover concise attack-related provenance graphs\nwith a theoretically bounded error. To utilize STP approximation algorithm\nframeworks for APT attack detection, we propose a novel design of in-memory\ncache, an efficient attack screening method, and a new STP approximation\nalgorithm that is more efficient than the conventional one in APT attack\ndetection while maintaining the same complexity. We evaluate NodLink in a\nproduction environment. The open-world experiment shows that NodLink\noutperforms two state-of-the-art (SOTA) online provenance analysis systems by\nachieving magnitudes higher detection and investigation accuracy while having\nthe same or higher throughput.",
            "author": [
                "Shaofei Li",
                "Feng Dong",
                "Xusheng Xiao",
                "Haoyu Wang",
                "Fei Shao",
                "Jiedong Chen",
                "Yao Guo",
                "Xiangqun Chen",
                "Ding Li"
            ],
            "link": [
                "http://dx.doi.org/10.14722/ndss.2024.23204",
                "http://arxiv.org/abs/2311.02331v1",
                "http://arxiv.org/pdf/2311.02331v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02329v2",
            "title": "Complex Organ Mask Guided Radiology Report Generation",
            "updated": "2023-11-10T04:24:35Z",
            "published": "2023-11-04T05:34:24Z",
            "summary": "The goal of automatic report generation is to generate a clinically accurate\nand coherent phrase from a single given X-ray image, which could alleviate the\nworkload of traditional radiology reporting. However, in a real-world scenario,\nradiologists frequently face the challenge of producing extensive reports\nderived from numerous medical images, thereby medical report generation from\nmulti-image perspective is needed. In this paper, we propose the Complex Organ\nMask Guided (termed as COMG) report generation model, which incorporates masks\nfrom multiple organs (e.g., bones, lungs, heart, and mediastinum), to provide\nmore detailed information and guide the model's attention to these crucial body\nregions. Specifically, we leverage prior knowledge of the disease corresponding\nto each organ in the fusion process to enhance the disease identification phase\nduring the report generation process. Additionally, cosine similarity loss is\nintroduced as target function to ensure the convergence of cross-modal\nconsistency and facilitate model optimization.Experimental results on two\npublic datasets show that COMG achieves a 11.4% and 9.7% improvement in terms\nof BLEU@4 scores over the SOTA model KiUT on IU-Xray and MIMIC, respectively.\nThe code is publicly available at https://github.com/GaryGuTC/COMG_model.",
            "author": [
                "Tiancheng Gu",
                "Dongnan Liu",
                "Zhiyuan Li",
                "Weidong Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02329v2",
                "http://arxiv.org/pdf/2311.02329v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02319v1",
            "title": "On the Robustness, Connectivity and Giant Component Size of Random K-out\n  Graphs",
            "updated": "2023-11-04T04:07:49Z",
            "published": "2023-11-04T04:07:49Z",
            "summary": "Random K-out graphs are garnering interest in designing distributed systems\nincluding secure sensor networks, anonymous crypto-currency networks, and\ndifferentially-private decentralized learning. In these security-critical\napplications, it is important to model and analyze the resilience of the\nnetwork to node failures and adversarial captures. Motivated by this, we\nanalyze how the connectivity properties of random K-out graphs vary with the\nnetwork parameters $K$, the number of nodes ($n$), and the number of nodes that\nget failed or compromised ($\\gamma_n$). In particular, we study the conditions\nfor achieving \\emph{connectivity} {with high probability} and for the existence\nof a \\emph{giant component} with formal guarantees on the size of the largest\nconnected component in terms of the parameters $n,~K$, and $\\gamma_n$. Next, we\nanalyze the property of \\emph{$r$-robustness} which is a stronger property than\nconnectivity and leads to resilient consensus in the presence of malicious\nnodes. We derive conditions on $K$ and $n$ under which the random K-out graph\nachieves r-robustness with high probability. We also provide extensive\nnumerical simulations and compare our results on random K-out graphs with known\nresults on Erd\\H{o}s-R\\'enyi (ER) graphs.",
            "author": [
                "Eray Can Elumar",
                "Mansi Sood",
                "Osman Ya\u011fan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02319v1",
                "http://arxiv.org/pdf/2311.02319v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02302v1",
            "title": "Proactively incremental-learning QAOA",
            "updated": "2023-11-04T02:15:26Z",
            "published": "2023-11-04T02:15:26Z",
            "summary": "Solving optimization problems with high performance is the target of existing\nworks of Quantum Approximate Optimization Algorithm (QAOA). With this\nintention, we propose an advanced QAOA based on incremental learning, where the\ntraining trajectory is proactively segmented into incremental phases. Taking\nthe MaxCut problem as our example, we randomly select a small subgraph from the\nwhole graph and train the quantum circuit to get optimized parameters for the\nMaxCut of the subgraph in the first phase. Then in each subsequent incremental\nphase, a portion of the remaining nodes and edges are added to the current\nsubgraph, and the circuit is retrained to get new optimized parameters. The\nabove operation is repeated until the MaxCut problem on the whole graph is\nsolved. The key point is that the optimized parameters of the previous phase\nwill be reused in the initial parameters of the current phase. Numerous\nsimulation experiments show our method has superior performance on\nApproximation Ratio (AR) and training time compared to prevalent works of QAOA.\nSpecifically, the AR is higher than standard QAOA by 13.17% on weighted random\ngraphs.",
            "author": [
                "Lingxiao Li",
                "Jing Li",
                "Yanqi Song",
                "Sujuan Qin",
                "Qiaoyan Wen",
                "Fei Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02302v1",
                "http://arxiv.org/pdf/2311.02302v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02271v2",
            "title": "FaMeSumm: Investigating and Improving Faithfulness of Medical\n  Summarization",
            "updated": "2023-11-08T22:54:33Z",
            "published": "2023-11-03T23:25:53Z",
            "summary": "Summaries of medical text shall be faithful by being consistent and factual\nwith source inputs, which is an important but understudied topic for safety and\nefficiency in healthcare. In this paper, we investigate and improve\nfaithfulness in summarization on a broad range of medical summarization tasks.\nOur investigation reveals that current summarization models often produce\nunfaithful outputs for medical input text. We then introduce FaMeSumm, a\nframework to improve faithfulness by fine-tuning pre-trained language models\nbased on medical knowledge. FaMeSumm performs contrastive learning on designed\nsets of faithful and unfaithful summaries, and it incorporates medical terms\nand their contexts to encourage faithful generation of medical terms. We\nconduct comprehensive experiments on three datasets in two languages: health\nquestion and radiology report summarization datasets in English, and a\npatient-doctor dialogue dataset in Chinese. Results demonstrate that FaMeSumm\nis flexible and effective by delivering consistent improvements over mainstream\nlanguage models such as BART, T5, mT5, and PEGASUS, yielding state-of-the-art\nperformances on metrics for faithfulness and general quality. Human evaluation\nby doctors also shows that FaMeSumm generates more faithful outputs. Our code\nis available at https://github.com/psunlpgroup/FaMeSumm .",
            "author": [
                "Nan Zhang",
                "Yusen Zhang",
                "Wu Guo",
                "Prasenjit Mitra",
                "Rui Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02271v2",
                "http://arxiv.org/pdf/2311.02271v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02263v1",
            "title": "List Decoding of Tanner and Expander Amplified Codes from Distance\n  Certificates",
            "updated": "2023-11-03T23:02:32Z",
            "published": "2023-11-03T23:02:32Z",
            "summary": "We develop new list decoding algorithms for Tanner codes and\ndistance-amplified codes based on bipartite spectral expanders. We show that\nproofs exhibiting lower bounds on the minimum distance of these codes can be\nused as certificates discoverable by relaxations in the Sum-of-Squares (SoS)\nsemidefinite programming hierarchy. Combining these certificates with certain\nentropic proxies to ensure that the solutions to the relaxations cover the\nentire list, then leads to algorithms for list decoding several families of\ncodes up to the Johnson bound.\n  We prove the following:\n  - We show that the LDPC Tanner codes of Sipser-Spielman [IEEE Trans. Inf.\nTheory 1996] and Z\\'{e}mor [IEEE Trans. Inf. Theory 2001] with alphabet size\n$q$, block-length $n$ and distance $\\delta$, based on an expander graph with\ndegree $d$, can be list-decoded up to distance $\\mathcal{J}_q(\\delta) -\n\\epsilon$ in time $n^{O_{d,q}(1/\\epsilon^4)}$, where $\\mathcal{J}_q(\\delta)$\ndenotes the Johnson bound.\n  - We show that the codes obtained via the expander-based distance\namplification procedure of Alon, Edmonds and Luby [FOCS 1995] can be\nlist-decoded close to the Johnson bound using the SoS hierarchy, by reducing\nthe list decoding problem to unique decoding of the base code. In particular,\nstarting from \\emph{any} base code unique-decodable up to distance $\\delta$,\none can obtain near-MDS codes with rate $R$ and distance $1-R - \\epsilon$,\nlist-decodable up to the Johnson bound in time $n^{O_{\\epsilon, \\delta}(1)}$.\n  - We show that the locally testable codes of Dinur et al. [STOC 2022] with\nalphabet size $q$, block-length $n$ and distance $\\delta$ based on a square\nCayley complex with generator sets of size $d$, can be list-decoded up to\ndistance $\\mathcal{J}_q(\\delta) - \\epsilon$ in time\n$n^{O_{d,q}(1/\\epsilon^{4})}$, where $\\mathcal{J}_q(\\delta)$ denotes the\nJohnson bound.",
            "author": [
                "Fernando Granha Jeronimo",
                "Shashank Srivastava",
                "Madhur Tulsiani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02263v1",
                "http://arxiv.org/pdf/2311.02263v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02262v1",
            "title": "Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs",
            "updated": "2023-11-03T22:56:43Z",
            "published": "2023-11-03T22:56:43Z",
            "summary": "In human-written articles, we often leverage the subtleties of text style,\nsuch as bold and italics, to guide the attention of readers. These textual\nemphases are vital for the readers to grasp the conveyed information. When\ninteracting with large language models (LLMs), we have a similar need -\nsteering the model to pay closer attention to user-specified information, e.g.,\nan instruction. Existing methods, however, are constrained to process plain\ntext and do not support such a mechanism. This motivates us to introduce PASTA\n- Post-hoc Attention STeering Approach, a method that allows LLMs to read text\nwith user-specified emphasis marks. To this end, PASTA identifies a small\nsubset of attention heads and applies precise attention reweighting on them,\ndirecting the model attention to user-specified parts. Like prompting, PASTA is\napplied at inference time and does not require changing any model parameters.\nExperiments demonstrate that PASTA can substantially enhance an LLM's ability\nto follow user instructions or integrate new knowledge from user inputs,\nleading to a significant performance improvement on a variety of tasks, e.g.,\nan average accuracy improvement of 22% for LLAMA-7B. Our code is publicly\navailable at https://github.com/QingruZhang/PASTA .",
            "author": [
                "Qingru Zhang",
                "Chandan Singh",
                "Liyuan Liu",
                "Xiaodong Liu",
                "Bin Yu",
                "Jianfeng Gao",
                "Tuo Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02262v1",
                "http://arxiv.org/pdf/2311.02262v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02253v1",
            "title": "Comparative Knowledge Distillation",
            "updated": "2023-11-03T21:55:33Z",
            "published": "2023-11-03T21:55:33Z",
            "summary": "In the era of large scale pretrained models, Knowledge Distillation (KD)\nserves an important role in transferring the wisdom of computationally heavy\nteacher models to lightweight, efficient student models while preserving\nperformance. Traditional KD paradigms, however, assume readily available access\nto teacher models for frequent inference -- a notion increasingly at odds with\nthe realities of costly, often proprietary, large scale models. Addressing this\ngap, our paper considers how to minimize the dependency on teacher model\ninferences in KD in a setting we term Few Teacher Inference Knowledge\nDistillation (FTI KD). We observe that prevalent KD techniques and state of the\nart data augmentation strategies fall short in this constrained setting.\nDrawing inspiration from educational principles that emphasize learning through\ncomparison, we propose Comparative Knowledge Distillation (CKD), which\nencourages student models to understand the nuanced differences in a teacher\nmodel's interpretations of samples. Critically, CKD provides additional\nlearning signals to the student without making additional teacher calls. We\nalso extend the principle of CKD to groups of samples, enabling even more\nefficient learning from limited teacher calls. Empirical evaluation across\nvaried experimental settings indicates that CKD consistently outperforms state\nof the art data augmentation and KD techniques.",
            "author": [
                "Alex Wilf",
                "Alex Tianyi Xu",
                "Paul Pu Liang",
                "Alexander Obolenskiy",
                "Daniel Fried",
                "Louis-Philippe Morency"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02253v1",
                "http://arxiv.org/pdf/2311.02253v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02247v1",
            "title": "PRISM: Progressive Restoration for Scene Graph-based Image Manipulation",
            "updated": "2023-11-03T21:30:34Z",
            "published": "2023-11-03T21:30:34Z",
            "summary": "Scene graphs have emerged as accurate descriptive priors for image generation\nand manipulation tasks, however, their complexity and diversity of the shapes\nand relations of objects in data make it challenging to incorporate them into\nthe models and generate high-quality results. To address these challenges, we\npropose PRISM, a novel progressive multi-head image manipulation approach to\nimprove the accuracy and quality of the manipulated regions in the scene. Our\nimage manipulation framework is trained using an end-to-end denoising masked\nreconstruction proxy task, where the masked regions are progressively unmasked\nfrom the outer regions to the inner part. We take advantage of the outer part\nof the masked area as they have a direct correlation with the context of the\nscene. Moreover, our multi-head architecture simultaneously generates detailed\nobject-specific regions in addition to the entire image to produce\nhigher-quality images. Our model outperforms the state-of-the-art methods in\nthe semantic image manipulation task on the CLEVR and Visual Genome datasets.\nOur results demonstrate the potential of our approach for enhancing the quality\nand precision of scene graph-based image manipulation.",
            "author": [
                "Pavel Jahoda",
                "Azade Farshad",
                "Yousef Yeganeh",
                "Ehsan Adeli",
                "Nassir Navab"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02247v1",
                "http://arxiv.org/pdf/2311.02247v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02236v1",
            "title": "Robust Fine-Tuning of Vision-Language Models for Domain Generalization",
            "updated": "2023-11-03T20:50:40Z",
            "published": "2023-11-03T20:50:40Z",
            "summary": "Transfer learning enables the sharing of common knowledge among models for a\nvariety of downstream tasks, but traditional methods suffer in limited training\ndata settings and produce narrow models incapable of effectively generalizing\nunder distribution shifts. Foundation models have recently demonstrated\nimpressive zero-shot inference capabilities and robustness under distribution\nshifts. However, zero-shot evaluation for these models has been predominantly\nconfined to benchmarks with simple distribution shifts, limiting our\nunderstanding of their effectiveness under the more realistic shifts found in\npractice. Moreover, common fine-tuning methods for these models have yet to be\nevaluated against vision models in few-shot scenarios where training data is\nlimited. To address these gaps, we present a new recipe for few-shot\nfine-tuning of the popular vision-language foundation model CLIP and evaluate\nits performance on challenging benchmark datasets with realistic distribution\nshifts from the WILDS collection. Our experimentation demonstrates that, while\nzero-shot CLIP fails to match performance of trained vision models on more\ncomplex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only\ncounterparts in terms of in-distribution and out-of-distribution accuracy at\nall levels of training data availability. This provides a strong incentive for\nadoption of foundation models within few-shot learning applications operating\nwith real-world data. Code is available at\nhttps://github.com/mit-ll/robust-vision-language-finetuning",
            "author": [
                "Kevin Vogt-Lowell",
                "Noah Lee",
                "Theodoros Tsiligkaridis",
                "Marc Vaillant"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02236v1",
                "http://arxiv.org/pdf/2311.02236v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02234v1",
            "title": "Synchronous Observer Design for Inertial Navigation Systems with\n  Almost-Global Convergence",
            "updated": "2023-11-03T20:46:46Z",
            "published": "2023-11-03T20:46:46Z",
            "summary": "An Inertial Navigation System (INS) is a system that integrates acceleration\nand angular velocity readings from an Inertial Measurement Unit (IMU), along\nwith other sensors such as GNSS position, GNSS velocity, and magnetometer, to\nestimate the attitude, velocity, and position of a vehicle. This paper shows\nthat the INS problem can be analysed using the automorphism group of the\nextended special Euclidean group: a group we term the extended similarity\ngroup. By exploiting this novel geometric framework, we propose an observer\narchitecture with synchronous error dynamics; that is, the error is stationary\nif the observer correction terms are set to zero. In turn, this enables us to\nderive a modular, or plug-and-play, observer design for INS that allows\ndifferent sensors to be added or removed depending on what is available in the\nvehicle sensor suite. We prove both almost-global asymptotic and local\nexponential stability of the error dynamics for the common scenario of at least\nIMU and GNSS position. To the authors' knowledge, this is the first non-linear\nobserver design with almost global convergence guarantees or with plug-and-play\nmodular capability. A simulation with extreme initial error demonstrates the\nalmost-global robustness of the system. Real-world capability is demonstrated\non data from a fixed-wing UAV, and the solution is compared to the\nstate-of-the-art ArduPilot INS.",
            "author": [
                "Pieter van Goor",
                "Tarek Hamel",
                "Robert Mahony"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02234v1",
                "http://arxiv.org/pdf/2311.02234v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02227v1",
            "title": "State-wise Safe Reinforcement Learning With Pixel Observations",
            "updated": "2023-11-03T20:32:30Z",
            "published": "2023-11-03T20:32:30Z",
            "summary": "Reinforcement Learning(RL) in the context of safe exploration has long\ngrappled with the challenges of the delicate balance between maximizing rewards\nand minimizing safety violations, the complexities arising from contact-rich or\nnon-smooth environments, and high-dimensional pixel observations. Furthermore,\nincorporating state-wise safety constraints in the exploration and learning\nprocess, where the agent is prohibited from accessing unsafe regions without\nprior knowledge, adds an additional layer of complexity. In this paper, we\npropose a novel pixel-observation safe RL algorithm that efficiently encodes\nstate-wise safety constraints with unknown hazard regions through the\nintroduction of a latent barrier function learning mechanism. As a joint\nlearning framework, our approach first involves constructing a latent dynamics\nmodel with low-dimensional latent spaces derived from pixel observations.\nSubsequently, we build and learn a latent barrier function on top of the latent\ndynamics and conduct policy optimization simultaneously, thereby improving both\nsafety and the total expected return. Experimental evaluations on the\nsafety-gym benchmark suite demonstrate that our proposed method significantly\nreduces safety violations throughout the training process and demonstrates\nfaster safety convergence compared to existing methods while achieving\ncompetitive results in reward return.",
            "author": [
                "Simon Sinong Zhan",
                "Yixuan Wang",
                "Qingyuan Wu",
                "Ruochen Jiao",
                "Chao Huang",
                "Qi Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02227v1",
                "http://arxiv.org/pdf/2311.02227v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16139v1",
            "title": "GNNBleed: Inference Attacks to Unveil Private Edges in Graphs with\n  Realistic Access to GNN Models",
            "updated": "2023-11-03T20:26:03Z",
            "published": "2023-11-03T20:26:03Z",
            "summary": "Graph Neural Networks (GNNs) have increasingly become an indispensable tool\nin learning from graph-structured data, catering to various applications\nincluding social network analysis, recommendation systems, etc. At the heart of\nthese networks are the edges which are crucial in guiding GNN models'\npredictions. In many scenarios, these edges represent sensitive information,\nsuch as personal associations or financial dealings -- thus requiring privacy\nassurance. However, their contributions to GNN model predictions may in turn be\nexploited by the adversary to compromise their privacy. Motivated by these\nconflicting requirements, this paper investigates edge privacy in contexts\nwhere adversaries possess black-box GNN model access, restricted further by\naccess controls, preventing direct insights into arbitrary node outputs. In\nthis context, we introduce a series of privacy attacks grounded on the\nmessage-passing mechanism of GNNs. These strategies allow adversaries to deduce\nconnections between two nodes not by directly analyzing the model's output for\nthese pairs but by analyzing the output for nodes linked to them. Our\nevaluation with seven real-life datasets and four GNN architectures underlines\na significant vulnerability: even in systems fortified with access control\nmechanisms, an adaptive adversary can decipher private connections between\nnodes, thereby revealing potentially sensitive relationships and compromising\nthe confidentiality of the graph.",
            "author": [
                "Zeyu Song",
                "Ehsanul Kabir",
                "Shagufta Mehnaz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16139v1",
                "http://arxiv.org/pdf/2311.16139v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02215v1",
            "title": "Towards model-free RL algorithms that scale well with unstructured data",
            "updated": "2023-11-03T20:03:54Z",
            "published": "2023-11-03T20:03:54Z",
            "summary": "Conventional reinforcement learning (RL) algorithms exhibit broad generality\nin their theoretical formulation and high performance on several challenging\ndomains when combined with powerful function approximation. However, developing\nRL algorithms that perform well across problems with unstructured observations\nat scale remains challenging because most function approximation methods rely\non externally provisioned knowledge about the structure of the input for good\nperformance (e.g. convolutional networks, graph neural networks, tile-coding).\nA common practice in RL is to evaluate algorithms on a single problem, or on\nproblems with limited variation in the observation scale. RL practitioners lack\na systematic way to study how well a single RL algorithm performs when\ninstantiated across a range of problem scales, and they lack function\napproximation techniques that scale well with unstructured observations.\n  We address these limitations by providing environments and algorithms to\nstudy scaling for unstructured observation vectors and flat action spaces. We\nintroduce a family of combinatorial RL problems with an exponentially large\nstate space and high-dimensional dynamics but where linear computation is\nsufficient to learn a (nonlinear) value function estimate for performant\ncontrol. We provide an algorithm that constructs reward-relevant general value\nfunction (GVF) questions to find and exploit predictive structure directly from\nthe experience stream. In an empirical evaluation of the approach on synthetic\nproblems, we observe a sample complexity that scales linearly with the\nobservation size. The proposed algorithm reliably outperforms a conventional\ndeep RL algorithm on these scaling problems, and they exhibit several desirable\nauxiliary properties. These results suggest new algorithmic mechanisms by which\nalgorithms can learn at scale from unstructured data.",
            "author": [
                "Joseph Modayil",
                "Zaheer Abbas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02215v1",
                "http://arxiv.org/pdf/2311.02215v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02204v1",
            "title": "Active risk aversion in SIS epidemics on networks",
            "updated": "2023-11-03T19:26:58Z",
            "published": "2023-11-03T19:26:58Z",
            "summary": "We present and analyze an actively controlled\nSusceptible-Infected-Susceptible (actSIS) model of interconnected populations\nto study how risk aversion strategies, such as social distancing, affect\nnetwork epidemics. A population using a risk aversion strategy reduces its\ncontact rate with other populations when it perceives an increase in infection\nrisk. The network actSIS model relies on two distinct networks. One is a\nphysical contact network that defines which populations come into contact with\nwhich other populations and thus how infection spreads. The other is a\ncommunication network, such as an online social network, that defines which\npopulations observe the infection level of which other populations and thus how\ninformation spreads. We prove that the model, with these two networks and\npopulations using risk aversion strategies, exhibits a transcritical\nbifurcation in which an endemic equilibrium emerges. For regular graphs, we\nprove that the endemic infection level is uniform across populations and\nreduced by the risk aversion strategy, relative to the network SIS endemic\nlevel. We show that when communication is sufficiently sparse, this initially\nstable equilibrium loses stability in a secondary bifurcation. Simulations show\nthat a new stable solution emerges with nonuniform infection levels.",
            "author": [
                "Anastasia Bizyaeva",
                "Marcela Ordorica Arango",
                "Yunxiu Zhou",
                "Simon Levin",
                "Naomi Ehrich Leonard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02204v1",
                "http://arxiv.org/pdf/2311.02204v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "cs.SY",
                "eess.SY",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02201v1",
            "title": "On 2-Distance ($\u0394+4$)-coloring of planar graphs with girth at least\n  five",
            "updated": "2023-11-03T19:06:55Z",
            "published": "2023-11-03T19:06:55Z",
            "summary": "A vertex coloring of a graph $G$ is called a 2-distance coloring if any two\nvertices at distance at most $2$ from each other receive different colors. Let\n$G$ be a planar graph with girth at least $5$. We prove that $G$ admits a\n$2$-distance coloring with $\\Delta+4$ colors if $\\Delta\\geq 22$.",
            "author": [
                "Zakir Deniz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02201v1",
                "http://arxiv.org/pdf/2311.02201v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02193v1",
            "title": "An effective self-supervised learning method for various seismic noise\n  attenuation",
            "updated": "2023-11-03T18:51:50Z",
            "published": "2023-11-03T18:51:50Z",
            "summary": "Faced with the scarcity of clean label data in real scenarios, seismic\ndenoising methods based on supervised learning (SL) often encounter performance\nlimitations. Specifically, when a model trained on synthetic data is directly\napplied to field data, its performance would drastically decline due to\nsignificant differences in feature distributions between the two. To address\nthis challenge, we develop an effective self-supervised strategy. This\nstrategy, while relying on a single denoising network model, adeptly attenuates\nvarious types of seismic noise. The strategy comprises two main phases: 1. The\nwarm-up phase. By using prior knowledge or extracting information from real\ndata, we introduce additional noise to the original noisy data, constructing a\nnoisier data with intensified noise. This data serves as the input, with the\noriginal noisy data acting as pseudo-labels. This facilitates rapid\npre-training of the network to capture a certain noise characteristics and\nboosts network stability, setting the stage for the subsequent phase. 2.\nIterative data refinement (IDR) phase. During this phase, we use the\npredictions of the original noisy data from the network trained in the previous\nepoch as the pseudo-labels. We continue to add noise to the predictions,\ncreating a new noisier-noisy dataset for the current epoch of network training.\nThrough this iterative process, we progressively reduce the discrepancy between\nthe original noisy data and the desired clean data. Ultimately, the network's\npredictions on the original noisy data become our denoised results. Validations\nunder scenarios with random noise, backscattered noise, and blending noise\nreveal that our method not only matches the traditional SL techniques on\nsynthetic data but significantly outperforms them on field data.",
            "author": [
                "Shijun Cheng",
                "Zhiyao Cheng",
                "Chao Jiang",
                "Weijian Mao",
                "Qingchen Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02193v1",
                "http://arxiv.org/pdf/2311.02193v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02192v1",
            "title": "Automating Governing Knowledge Commons and Contextual Integrity (GKC-CI)\n  Privacy Policy Annotations with Large Language Models",
            "updated": "2023-11-03T18:49:05Z",
            "published": "2023-11-03T18:49:05Z",
            "summary": "Identifying contextual integrity (CI) and governing knowledge commons (GKC)\nparameters in privacy policy texts can facilitate normative privacy analysis.\nHowever, GKC-CI annotation has heretofore required manual or crowdsourced\neffort. This paper demonstrates that high-accuracy GKC-CI parameter annotation\nof privacy policies can be performed automatically using large language models.\nWe fine-tune 18 open-source and proprietary models on 21,588 GKC-CI annotations\nfrom 16 ground truth privacy policies. Our best-performing model (fine-tuned\nGPT-3.5 Turbo with prompt engineering) has an accuracy of 86%, exceeding the\nperformance of prior crowdsourcing approaches despite the complexity of privacy\npolicy texts and the nuance of the GKC-CI annotation task. We apply our\nbest-performing model to privacy policies from 164 popular online services,\ndemonstrating the effectiveness of scaling GKC-CI annotation for data\nexploration. We make all annotated policies as well as the training data and\nscripts needed to fine-tune our best-performing model publicly available for\nfuture research.",
            "author": [
                "Jake Chanenson",
                "Madison Pickering",
                "Noah Apthorpe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02192v1",
                "http://arxiv.org/pdf/2311.02192v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02152v1",
            "title": "Gas and star formation in satellites of Milky Way analogs",
            "updated": "2023-11-03T18:00:00Z",
            "published": "2023-11-03T18:00:00Z",
            "summary": "We have imaged the entirety of eight (plus one partial) Milky Way-like\nsatellite systems, a total of 42 (45) satellites, from the Satellites Around\nGalactic Analogs (SAGA) II catalog in both H$\\alpha$ and HI with the\nCanada-France-Hawaii Telescope and the Jansky Very Large Array. In these eight\nsystems we have identified four cases where a satellite appears to be currently\nundergoing ram pressure stripping (RPS) as its HI gas collides with the\ncircumgalactic medium (CGM) of its host. We also see a clear suppression of gas\nfraction ($M_\\mathrm{HI}/M_\\ast$) with decreasing (projected) satellite--host\nseparation; to our knowledge, the first time this has been observed in a sample\nof Milky Way-like systems. Comparisons to the Auriga, APOSTLE, and TNG-50\ncosmological zoom-in simulations show consistent global behavior, but they\nsystematically under-predict gas fractions across all satellites by roughly 0.5\ndex. Using a simplistic RPS model we estimate the average peak CGM density that\nsatellites in these systems have encountered to be $\\log\n\\rho_\\mathrm{cgm}/\\mathrm{g\\,cm^{-3}} \\approx -27.3$. Furthermore, we see\ntentative evidence that these satellites are following a specific star\nformation rate-to-gas fraction relation that is distinct from field galaxies.\nFinally, we detect one new gas-rich satellite in the UGC903 system with an\noptical size and surface brightness meeting the standard criteria to be\nconsidered an ultra-diffuse galaxy.",
            "author": [
                "Michael G. Jones",
                "David J. Sand",
                "Ananthan Karunakaran",
                "Kristine Spekkens",
                "Kyle A. Oman",
                "Paul Bennet",
                "Gurtina Besla",
                "Denija Crnojevic",
                "Jean-Charles Cuillandre",
                "Catherine E. Fielder",
                "Stephen Gwyn",
                "Burcin Mutlu-Pakdil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02152v1",
                "http://arxiv.org/pdf/2311.02152v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02075v1",
            "title": "Envy-Free Cake-Cutting for Four Agents",
            "updated": "2023-11-03T17:58:14Z",
            "published": "2023-11-03T17:58:14Z",
            "summary": "In the envy-free cake-cutting problem we are given a resource, usually called\na cake and represented as the $[0,1]$ interval, and a set of $n$ agents with\nheterogeneous preferences over pieces of the cake. The goal is to divide the\ncake among the $n$ agents such that no agent is envious of any other agent.\nEven under a very general preferences model, this fundamental fair division\nproblem is known to always admit an exact solution where each agent obtains a\nconnected piece of the cake; we study the complexity of finding an approximate\nsolution, i.e., a connected $\\varepsilon$-envy-free allocation.\n  For monotone valuations of cake pieces, Deng, Qi, and Saberi (2012) gave an\nefficient ($\\textsf{poly}(\\log(1/\\varepsilon))$ queries) algorithm for three\nagents and posed the open problem of four (or more) monotone agents. Even for\nthe special case of additive valuations, Br\\^anzei and Nisan (2022) conjectured\nan $\\Omega(1/\\varepsilon)$ lower bound on the number of queries for four\nagents. We provide the first efficient algorithm for finding a connected\n$\\varepsilon$-envy-free allocation with four monotone agents.\n  We also prove that as soon as valuations are allowed to be non-monotone, the\nproblem becomes hard: it becomes PPAD-hard, requires\n$\\textsf{poly}(1/\\varepsilon)$ queries in the black-box model, and even\n$\\textsf{poly}(1/\\varepsilon)$ communication complexity. This constitutes, to\nthe best of our knowledge, the first intractability result for any version of\nthe cake-cutting problem in the communication complexity model.",
            "author": [
                "Alexandros Hollender",
                "Aviad Rubinstein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02075v1",
                "http://arxiv.org/pdf/2311.02075v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02073v1",
            "title": "Streaming Algorithms for Weighted $k$-Disjoint Matchings",
            "updated": "2023-11-03T17:55:57Z",
            "published": "2023-11-03T17:55:57Z",
            "summary": "We design and implement two single-pass semi-streaming algorithms for the\nmaximum weight $k$-disjoint matching ($k$-DM) problem. Given an integer $k$,\nthe $k$-DM problem is to find $k$ pairwise edge-disjoint matchings such that\nthe sum of the weights of the matchings is maximized. For $k \\geq 2$, this\nproblem is NP-hard. Our first algorithm is based on the primal-dual framework\nof a linear programming relaxation of the problem and is\n$\\frac{1}{3+\\varepsilon}$-approximate. We also develop an approximation\npreserving reduction from $k$-DM to the maximum weight $b$-matching problem.\nLeveraging this reduction and an existing semi-streaming $b$-matching\nalgorithm, we design a $\\frac{k}{(2+\\varepsilon)(k+1)}$-approximate\nsemi-streaming algorithm for $k$-DM. For any constant $\\varepsilon > 0$, both\nof these algorithms require $O(nk \\log_{1+\\varepsilon}^2 n)$ bits of space. To\nthe best of our knowledge, this is the first study of semi-streaming algorithms\nfor the $k$-DM problem.\n  We compare our two algorithms to state-of-the-art offline algorithms on 82\nreal-world and synthetic test problems. On the smaller instances, our streaming\nalgorithms used significantly less memory (ranging from 6$\\times$ to\n114$\\times$ less) and were faster in runtime than the offline algorithms. Our\nsolutions were often within 5\\% of the best weights from the offline\nalgorithms. On a collection of six large graphs with a memory limit of 1 TB and\nwith $k=8$, the offline algorithms terminated only on one graph\n(mycielskian20). The best offline algorithm on this instance required 640 GB of\nmemory and 20 minutes to complete. In contrast, our slowest streaming algorithm\nfor this instance took under four minutes and produced a matching that was 18\\%\nbetter in weight, using only 1.4 GB of memory.",
            "author": [
                "S M Ferdous",
                "Bhargav Samineni",
                "Alex Pothen",
                "Mahantesh Halappanavar",
                "Bala Krishnamoorthy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02073v1",
                "http://arxiv.org/pdf/2311.02073v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02146v1",
            "title": "Bayesian Optimization of Function Networks with Partial Evaluations",
            "updated": "2023-11-03T17:55:08Z",
            "published": "2023-11-03T17:55:08Z",
            "summary": "Bayesian optimization is a framework for optimizing functions that are costly\nor time-consuming to evaluate. Recent work has considered Bayesian optimization\nof function networks (BOFN), where the objective function is computed via a\nnetwork of functions, each taking as input the output of previous nodes in the\nnetwork and additional parameters. Exploiting this network structure has been\nshown to yield significant performance improvements. Existing BOFN algorithms\nfor general-purpose networks are required to evaluate the full network at each\niteration. However, many real-world applications allow evaluating nodes\nindividually. To take advantage of this opportunity, we propose a novel\nknowledge gradient acquisition function for BOFN that chooses which node to\nevaluate as well as the inputs for that node in a cost-aware fashion. This\napproach can dramatically reduce query costs by allowing the evaluation of part\nof the network at a lower cost relative to evaluating the entire network. We\nprovide an efficient approach to optimizing our acquisition function and show\nit outperforms existing BOFN methods and other benchmarks across several\nsynthetic and real-world problems. Our acquisition function is the first to\nenable cost-aware optimization of a broad class of function networks.",
            "author": [
                "Poompol Buathong",
                "Jiayue Wan",
                "Samuel Daulton",
                "Raul Astudillo",
                "Maximilian Balandat",
                "Peter I. Frazier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02146v1",
                "http://arxiv.org/pdf/2311.02146v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02070v2",
            "title": "Positive discrepancy, MaxCut, and eigenvalues of graphs",
            "updated": "2023-11-20T12:55:00Z",
            "published": "2023-11-03T17:54:27Z",
            "summary": "The positive discrepancy of a graph $G$ of edge density\n$p=e(G)/\\binom{v(G)}{2}$ is defined as\n  $$\\mbox{disc}^{+}(G)=\\max_{U\\subset V(G)}e(G[U])-p\\binom{|U|}{2}.$$\n  In 1993, Alon proved (using the equivalent terminology of minimum bisections)\nthat if $G$ is $d$-regular on $n$ vertices, and $d=O(n^{1/9})$, then\n$\\mbox{disc}^{+}(G)=\\Omega(d^{1/2}n)$. We greatly extend this by showing that\nif $G$ has average degree $d$, then\n$\\mbox{disc}^{+}(G)=\\Omega(d^{\\frac{1}{2}}n)$ if $d\\in [0,n^{\\frac{2}{3}}]$,\n$\\Omega(n^2/d)$ if $d\\in [n^{\\frac{2}{3}},n^{\\frac{4}{5}}]$, and\n$\\Omega(d^{\\frac{1}{4}}n/\\log n)$ if $d\\in\n\\left[n^{\\frac{4}{5}},(\\frac{1}{2}-\\varepsilon)n\\right]$. These bounds are best\npossible if $d\\ll n^{3/4}$, and the complete bipartite graph shows that\n$\\mbox{disc}^{+}(G)=\\Omega(n)$ cannot be improved if $d\\approx n/2$. Our proofs\nare based on semidefinite programming and linear algebraic techniques.\n  An interesting corollary of our results is that every $d$-regular graph on\n$n$ vertices with ${\\frac{1}{2}+\\varepsilon\\leq \\frac{d}{n}\\leq 1-\\varepsilon}$\nhas a cut of size $\\frac{nd}{4}+\\Omega(n^{5/4}/\\log n)$. This is not\nnecessarily true without the assumption of regularity, or the bounds on $d$.\n  The positive discrepancy of regular graphs is controlled by the second\neigenvalue $\\lambda_2$, as $\\mbox{disc}^{+}(G)\\leq \\frac{\\lambda_2}{2} n+d$. As\na byproduct of our arguments, we present lower bounds on $\\lambda_2$ for\nregular graphs, extending the celebrated Alon-Boppana theorem in the dense\nregime.",
            "author": [
                "Eero R\u00e4ty",
                "Benny Sudakov",
                "Istv\u00e1n Tomon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02070v2",
                "http://arxiv.org/pdf/2311.02070v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02062v2",
            "title": "GroomGen: A High-Quality Generative Hair Model Using Hierarchical Latent\n  Representations",
            "updated": "2023-11-16T20:15:37Z",
            "published": "2023-11-03T17:46:29Z",
            "summary": "Despite recent successes in hair acquisition that fits a high-dimensional\nhair model to a specific input subject, generative hair models, which establish\ngeneral embedding spaces for encoding, editing, and sampling diverse\nhairstyles, are way less explored. In this paper, we present GroomGen, the\nfirst generative model designed for hair geometry composed of highly-detailed\ndense strands. Our approach is motivated by two key ideas. First, we construct\nhair latent spaces covering both individual strands and hairstyles. The latent\nspaces are compact, expressive, and well-constrained for high-quality and\ndiverse sampling. Second, we adopt a hierarchical hair representation that\nparameterizes a complete hair model to three levels: single strands, sparse\nguide hairs, and complete dense hairs. This representation is critical to the\ncompactness of latent spaces, the robustness of training, and the efficiency of\ninference. Based on this hierarchical latent representation, our proposed\npipeline consists of a strand-VAE and a hairstyle-VAE that encode an individual\nstrand and a set of guide hairs to their respective latent spaces, and a hybrid\ndensification step that populates sparse guide hairs to a dense hair model.\nGroomGen not only enables novel hairstyle sampling and plausible hairstyle\ninterpolation, but also supports interactive editing of complex hairstyles, or\ncan serve as strong data-driven prior for hairstyle reconstruction from images.\nWe demonstrate the superiority of our approach with qualitative examples of\ndiverse sampled hairstyles and quantitative evaluation of generation quality\nregarding every single component and the entire pipeline.",
            "author": [
                "Yuxiao Zhou",
                "Menglei Chai",
                "Alessandro Pepe",
                "Markus Gross",
                "Thabo Beeler"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3618309",
                "http://arxiv.org/abs/2311.02062v2",
                "http://arxiv.org/pdf/2311.02062v2"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02058v2",
            "title": "LOTUS: Continual Imitation Learning for Robot Manipulation Through\n  Unsupervised Skill Discovery",
            "updated": "2023-11-17T08:26:16Z",
            "published": "2023-11-03T17:38:35Z",
            "summary": "We introduce LOTUS, a continual imitation learning algorithm that empowers a\nphysical robot to continuously and efficiently learn to solve new manipulation\ntasks throughout its lifespan. The core idea behind LOTUS is constructing an\never-growing skill library from a sequence of new tasks with a small number of\nhuman demonstrations. LOTUS starts with a continual skill discovery process\nusing an open-vocabulary vision model, which extracts skills as recurring\npatterns presented in unsegmented demonstrations. Continual skill discovery\nupdates existing skills to avoid catastrophic forgetting of previous tasks and\nadds new skills to solve novel tasks. LOTUS trains a meta-controller that\nflexibly composes various skills to tackle vision-based manipulation tasks in\nthe lifelong learning process. Our comprehensive experiments show that LOTUS\noutperforms state-of-the-art baselines by over 11% in success rate, showing its\nsuperior knowledge transfer ability compared to prior methods. More results and\nvideos can be found on the project website:\nhttps://ut-austin-rpl.github.io/Lotus/.",
            "author": [
                "Weikang Wan",
                "Yifeng Zhu",
                "Rutav Shah",
                "Yuke Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02058v2",
                "http://arxiv.org/pdf/2311.02058v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02143v2",
            "title": "Pairing-based graph neural network for simulating quantum materials",
            "updated": "2023-11-21T15:54:28Z",
            "published": "2023-11-03T17:12:29Z",
            "summary": "We develop a pairing-based graph neural network for simulating quantum\nmany-body systems. Our architecture augments a BCS-type geminal wavefunction\nwith a generalized pair amplitude parameterized by a graph neural network.\nVariational Monte Carlo with our neural network simultaneously provides an\naccurate, flexible, and scalable method for simulating many-electron systems.\nWe apply this method to two-dimensional semiconductor electron-hole bilayers\nand obtain accurate results on a variety of interaction-induced phases,\nincluding the exciton Bose-Einstein condensate, electron-hole superconductor,\nand bilayer Wigner crystal. Our study demonstrates the potential of\nphysically-motivated neural network wavefunctions for quantum materials\nsimulations.",
            "author": [
                "Di Luo",
                "David D. Dai",
                "Liang Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02143v2",
                "http://arxiv.org/pdf/2311.02143v2"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.dis-nn",
                "cs.LG",
                "physics.comp-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16138v1",
            "title": "After-Stroke Arm Paresis Detection using Kinematic Data",
            "updated": "2023-11-03T16:56:02Z",
            "published": "2023-11-03T16:56:02Z",
            "summary": "This paper presents an approach for detecting unilateral arm\nparalysis/weakness using kinematic data. Our method employs temporal\nconvolution networks and recurrent neural networks, guided by knowledge\ndistillation, where we use inertial measurement units attached to the body to\ncapture kinematic information such as acceleration, rotation, and flexion of\nbody joints during an action. This information is then analyzed to recognize\nbody actions and patterns. Our proposed network achieves a high paretic\ndetection accuracy of 97.99\\%, with an action classification accuracy of\n77.69\\%, through knowledge sharing. Furthermore, by incorporating causal\nreasoning, we can gain additional insights into the patient's condition, such\nas their Fugl-Meyer assessment score or impairment level based on the machine\nlearning result. Overall, our approach demonstrates the potential of using\nkinematic data and machine learning for detecting arm paralysis/weakness. The\nresults suggest that our method could be a useful tool for clinicians and\nhealthcare professionals working with patients with this condition.",
            "author": [
                "Kenneth Lai",
                "Mohammed Almekhlafi",
                "Svetlana Yanushkevich"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16138v1",
                "http://arxiv.org/pdf/2311.16138v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02142v1",
            "title": "Sparse Training of Discrete Diffusion Models for Graph Generation",
            "updated": "2023-11-03T16:50:26Z",
            "published": "2023-11-03T16:50:26Z",
            "summary": "Generative models for graphs often encounter scalability challenges due to\nthe inherent need to predict interactions for every node pair. Despite the\nsparsity often exhibited by real-world graphs, the unpredictable sparsity\npatterns of their adjacency matrices, stemming from their unordered nature,\nleads to quadratic computational complexity. In this work, we introduce\nSparseDiff, a denoising diffusion model for graph generation that is able to\nexploit sparsity during its training phase. At the core of SparseDiff is a\nmessage-passing neural network tailored to predict only a subset of edges\nduring each forward pass. When combined with a sparsity-preserving noise model,\nthis model can efficiently work with edge lists representations of graphs,\npaving the way for scalability to much larger structures. During the sampling\nphase, SparseDiff iteratively populates the adjacency matrix from its prior\nstate, ensuring prediction of the full graph while controlling memory\nutilization. Experimental results show that SparseDiff simultaneously matches\nstate-of-the-art in generation performance on both small and large graphs,\nhighlighting the versatility of our method.",
            "author": [
                "Yiming Qin",
                "Clement Vignac",
                "Pascal Frossard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02142v1",
                "http://arxiv.org/pdf/2311.02142v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02018v1",
            "title": "Active Reasoning in an Open-World Environment",
            "updated": "2023-11-03T16:24:34Z",
            "published": "2023-11-03T16:24:34Z",
            "summary": "Recent advances in vision-language learning have achieved notable success on\ncomplete-information question-answering datasets through the integration of\nextensive world knowledge. Yet, most models operate passively, responding to\nquestions based on pre-stored knowledge. In stark contrast, humans possess the\nability to actively explore, accumulate, and reason using both newfound and\nexisting information to tackle incomplete-information questions. In response to\nthis gap, we introduce $Conan$, an interactive open-world environment devised\nfor the assessment of active reasoning. $Conan$ facilitates active exploration\nand promotes multi-round abductive inference, reminiscent of rich, open-world\nsettings like Minecraft. Diverging from previous works that lean primarily on\nsingle-round deduction via instruction following, $Conan$ compels agents to\nactively interact with their surroundings, amalgamating new evidence with prior\nknowledge to elucidate events from incomplete observations. Our analysis on\n$Conan$ underscores the shortcomings of contemporary state-of-the-art models in\nactive exploration and understanding complex scenarios. Additionally, we\nexplore Abduction from Deduction, where agents harness Bayesian rules to recast\nthe challenge of abduction as a deductive process. Through $Conan$, we aim to\ngalvanize advancements in active reasoning and set the stage for the next\ngeneration of artificial intelligence agents adept at dynamically engaging in\nenvironments.",
            "author": [
                "Manjie Xu",
                "Guangyuan Jiang",
                "Wei Liang",
                "Chi Zhang",
                "Yixin Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02018v1",
                "http://arxiv.org/pdf/2311.02018v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02010v1",
            "title": "A cast of thousands: How the IDEAS Productivity project has advanced\n  software productivity and sustainability",
            "updated": "2023-11-03T16:14:17Z",
            "published": "2023-11-03T16:14:17Z",
            "summary": "Computational and data-enabled science and engineering are revolutionizing\nadvances throughout science and society, at all scales of computing. For\nexample, teams in the U.S. DOE Exascale Computing Project have been tackling\nnew frontiers in modeling, simulation, and analysis by exploiting unprecedented\nexascale computing capabilities-building an advanced software ecosystem that\nsupports next-generation applications and addresses disruptive changes in\ncomputer architectures. However, concerns are growing about the productivity of\nthe developers of scientific software, its sustainability, and the\ntrustworthiness of the results that it produces. Members of the IDEAS project\nserve as catalysts to address these challenges through fostering software\ncommunities, incubating and curating methodologies and resources, and\ndisseminating knowledge to advance developer productivity and software\nsustainability. This paper discusses how these synergistic activities are\nadvancing scientific discovery-mitigating technical risks by building a firmer\nfoundation for reproducible, sustainable science at all scales of computing,\nfrom laptops to clusters to exascale and beyond.",
            "author": [
                "Lois Curfman McInnes",
                "Michael Heroux",
                "David E. Bernholdt",
                "Anshu Dubey",
                "Elsa Gonsiorowski",
                "Rinku Gupta",
                "Osni Marques",
                "J. David Moulton",
                "Hai Ah Nam",
                "Boyana Norris",
                "Elaine M. Raybourn",
                "Jim Willenbring",
                "Ann Almgren",
                "Ross Bartlett",
                "Kita Cranfill",
                "Stephen Fickas",
                "Don Frederick",
                "William Godoy",
                "Patricia Grubel",
                "Rebecca Hartman-Baker",
                "Axel Huebl",
                "Rose Lynch",
                "Addi Malviya Thakur",
                "Reed Milewicz",
                "Mark C. Miller",
                "Miranda Mundt",
                "Erik Palmer",
                "Suzanne Parete-Koon",
                "Megan Phinney",
                "Katherine Riley",
                "David M. Rogers",
                "Ben Sims",
                "Deborah Stevens",
                "Gregory R. Watson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02010v1",
                "http://arxiv.org/pdf/2311.02010v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02008v1",
            "title": "Sharp Global Well-posedness and Scattering of the Boltzmann Equation",
            "updated": "2023-11-03T16:13:31Z",
            "published": "2023-11-03T16:13:31Z",
            "summary": "We consider the 3D Boltzmann equation for the Maxwellian particle and soft\npotential with an angular cutoff. We prove sharp global well-posedness with\ninitial data small in the scaling-critical space. The solution also remains in\n$L^{1}$ if the initial datum is in $L^{1}$, even at such low regularity. The\nkey to existence, uniqueness and regularity criteria is the new bilinear\nspacetime estimates for the gain term, the proof of which is based on novel\ntechniques from nonlinear dispersive PDEs including the atomic $U$-$V$ spaces,\nmulti-linear frequency analysis, dispersive estimates, etc. To our knowledge,\nthis is the first 3D sharp global result for the Boltzmann equation.",
            "author": [
                "Xuwen Chen",
                "Shunlin Shen",
                "Zhifei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02008v1",
                "http://arxiv.org/pdf/2311.02008v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02006v1",
            "title": "Eigenvalue spectra of finely structured random matrices",
            "updated": "2023-11-03T16:11:04Z",
            "published": "2023-11-03T16:11:04Z",
            "summary": "Random matrix theory allows for the deduction of stability criteria for\ncomplex systems using only a summary knowledge of the statistics of the\ninteractions between components. As such, results like the well-known\nelliptical law are applicable in a myriad of different contexts. However, it is\noften assumed that all components of the complex system in question are\nstatistically equivalent, which is unrealistic in many applications. Here, we\nintroduce the concept of a finely-structured random matrix. These are random\nmatrices with element-specific statistics, which can be used to model systems\nin which the individual components are statistically distinct. By supposing\nthat the degree of `fine structure' in the matrix is small, we arrive at a\nsuccinct `modified' elliptical law. We demonstrate the direct applicability of\nour results to the niche and cascade models in theoretical ecology, as well as\na model of a neural network, and a directed network with arbitrary degree\ndistribution. The simple closed form of our central results allow us to draw\nbroad qualitative conclusions about the effect of fine structure on stability.",
            "author": [
                "Lyle Poley",
                "Tobias Galla",
                "Joseph W. Baron"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02006v1",
                "http://arxiv.org/pdf/2311.02006v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02139v1",
            "title": "Broadband ptychographic imaging of biological samples using a\n  deconvolution algorithm",
            "updated": "2023-11-03T16:06:28Z",
            "published": "2023-11-03T16:06:28Z",
            "summary": "Ptychography is an attractive advance of coherent diffraction imaging (CDI),\nwhich can provide high lateral resolution and wide field of view. The\ntheoretical resolution of ptychography is dose-limited, therefore making\nptychography workable with a broadband source will be highly beneficial.\nHowever, broad spectra of light source conflict with the high coherence\nassumption in CDI that the current reconstruction algorithm were built upon. In\nthis paper, we demonstrated that incorporation of a blind deconvolution in the\nreconstruction algorithm can improve the image quality of ptychography with\nbroadband source. This broadband reconstruction algorithm can obtain\nhigh-quality amplitude and phase images of complex-valued samples requiring no\nknowledge of the illumination spectrum. Optical experiments using biological\nsamples demonstrate the effectiveness of our method. The significant\nimprovement in low coherence tolerance by our approach can pave the way for\nimplementing ultrafast imaging with femtosecond or attosecond lasers or\nhigh-flux ptychographic imaging with laboratory EUV or X-ray sources.",
            "author": [
                "Huixiang Lin",
                "Fucai Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02139v1",
                "http://arxiv.org/pdf/2311.02139v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "eess.IV",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02000v1",
            "title": "High Probability Convergence of Adam Under Unbounded Gradients and\n  Affine Variance Noise",
            "updated": "2023-11-03T15:55:53Z",
            "published": "2023-11-03T15:55:53Z",
            "summary": "In this paper, we study the convergence of the Adaptive Moment Estimation\n(Adam) algorithm under unconstrained non-convex smooth stochastic\noptimizations. Despite the widespread usage in machine learning areas, its\ntheoretical properties remain limited. Prior researches primarily investigated\nAdam's convergence from an expectation view, often necessitating strong\nassumptions like uniformly stochastic bounded gradients or problem-dependent\nknowledge in prior. As a result, the applicability of these findings in\npractical real-world scenarios has been constrained. To overcome these\nlimitations, we provide a deep analysis and show that Adam could converge to\nthe stationary point in high probability with a rate of $\\mathcal{O}\\left({\\rm\npoly}(\\log T)/\\sqrt{T}\\right)$ under coordinate-wise \"affine\" variance noise,\nnot requiring any bounded gradient assumption and any problem-dependent\nknowledge in prior to tune hyper-parameters. Additionally, it is revealed that\nAdam confines its gradients' magnitudes within an order of\n$\\mathcal{O}\\left({\\rm poly}(\\log T)\\right)$. Finally, we also investigate a\nsimplified version of Adam without one of the corrective terms and obtain a\nconvergence rate that is adaptive to the noise level.",
            "author": [
                "Yusu Hong",
                "Junhong Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02000v1",
                "http://arxiv.org/pdf/2311.02000v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01999v1",
            "title": "Model selection for Markov random fields on graphs under a mixing\n  condition",
            "updated": "2023-11-03T15:54:16Z",
            "published": "2023-11-03T15:54:16Z",
            "summary": "In this work, we propose a global model selection criterion to estimate the\ngraph of conditional dependencies of a random vector based on a finite sample.\nBy global criterion, we mean optimizing a function over the entire set of\npossible graphs, eliminating the need to estimate the individual neighborhoods\nand subsequently combine them to estimate the graph. We prove the almost sure\nconvergence of the graph estimator. This convergence holds provided the data is\na realization of a multivariate stochastic process that satisfies a mixing\ncondition. To the best of our knowledge, these are the first results to show\nthe consistency of a model selection criterion for Markov random fields on\ngraphs under non-independent data.",
            "author": [
                "Florencia Leonardi",
                "Magno T. F Severino"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01999v1",
                "http://arxiv.org/pdf/2311.01999v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.PR",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16137v1",
            "title": "A Graph-to-Text Approach to Knowledge-Grounded Response Generation in\n  Human-Robot Interaction",
            "updated": "2023-11-03T15:44:28Z",
            "published": "2023-11-03T15:44:28Z",
            "summary": "Knowledge graphs are often used to represent structured information in a\nflexible and efficient manner, but their use in situated dialogue remains\nunder-explored. This paper presents a novel conversational model for\nhuman--robot interaction that rests upon a graph-based representation of the\ndialogue state. The knowledge graph representing the dialogue state is\ncontinuously updated with new observations from the robot sensors, including\nlinguistic, situated and multimodal inputs, and is further enriched by other\nmodules, in particular for spatial understanding. The neural conversational\nmodel employed to respond to user utterances relies on a simple but effective\ngraph-to-text mechanism that traverses the dialogue state graph and converts\nthe traversals into a natural language form. This conversion of the state graph\ninto text is performed using a set of parameterized functions, and the values\nfor those parameters are optimized based on a small set of Wizard-of-Oz\ninteractions. After this conversion, the text representation of the dialogue\nstate graph is included as part of the prompt of a large language model used to\ndecode the agent response. The proposed approach is empirically evaluated\nthrough a user study with a humanoid robot that acts as conversation partner to\nevaluate the impact of the graph-to-text mechanism on the response generation.\nAfter moving a robot along a tour of an indoor environment, participants\ninteracted with the robot using spoken dialogue and evaluated how well the\nrobot was able to answer questions about what the robot observed during the\ntour. User scores show a statistically significant improvement in the perceived\nfactuality of the robot responses when the graph-to-text approach is employed,\ncompared to a baseline using inputs structured as semantic triples.",
            "author": [
                "Nicholas Thomas Walker",
                "Stefan Ultes",
                "Pierre Lison"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16137v1",
                "http://arxiv.org/pdf/2311.16137v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01989v2",
            "title": "Leveraging Large-Scale Pretrained Vision Foundation Models for\n  Label-Efficient 3D Point Cloud Segmentation",
            "updated": "2023-11-06T08:18:26Z",
            "published": "2023-11-03T15:41:15Z",
            "summary": "Recently, large-scale pre-trained models such as Segment-Anything Model (SAM)\nand Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable\nsuccess and revolutionized the field of computer vision. These foundation\nvision models effectively capture knowledge from a large-scale broad data with\ntheir vast model parameters, enabling them to perform zero-shot segmentation on\npreviously unseen data without additional training. While they showcase\ncompetence in 2D tasks, their potential for enhancing 3D scene understanding\nremains relatively unexplored. To this end, we present a novel framework that\nadapts various foundational models for the 3D point cloud segmentation task.\nOur approach involves making initial predictions of 2D semantic masks using\ndifferent large vision models. We then project these mask predictions from\nvarious frames of RGB-D video sequences into 3D space. To generate robust 3D\nsemantic pseudo labels, we introduce a semantic label fusion strategy that\neffectively combines all the results via voting. We examine diverse scenarios,\nlike zero-shot learning and limited guidance from sparse 2D point labels, to\nassess the pros and cons of different vision foundation models. Our approach is\nexperimented on ScanNet dataset for 3D indoor scenes, and the results\ndemonstrate the effectiveness of adopting general 2D foundation models on\nsolving 3D point cloud segmentation tasks.",
            "author": [
                "Shichao Dong",
                "Fayao Liu",
                "Guosheng Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01989v2",
                "http://arxiv.org/pdf/2311.01989v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01987v1",
            "title": "Generalization of Graph-Based Active Learning Relaxation Strategies\n  Across Materials",
            "updated": "2023-11-03T15:40:20Z",
            "published": "2023-11-03T15:40:20Z",
            "summary": "Although density functional theory (DFT) has aided in accelerating the\ndiscovery of new materials, such calculations are computationally expensive,\nespecially for high-throughput efforts. This has prompted an explosion in\nexploration of machine learning assisted techniques to improve the\ncomputational efficiency of DFT. In this study, we present a comprehensive\ninvestigation of the broader application of Finetuna, an active learning\nframework to accelerate structural relaxation in DFT with prior information\nfrom Open Catalyst Project pretrained graph neural networks. We explore the\nchallenges associated with out-of-domain systems: alcohol ($C_{>2}$) on metal\nsurfaces as larger adsorbates, metal-oxides with spin polarization, and\nthree-dimensional (3D) structures like zeolites and metal-organic-frameworks.\nBy pre-training machine learning models on large datasets and fine-tuning the\nmodel along the simulation, we demonstrate the framework's ability to conduct\nrelaxations with fewer DFT calculations. Depending on the similarity of the\ntest systems to the training systems, a more conservative querying strategy is\napplied. Our best-performing Finetuna strategy reduces the number of DFT\nsingle-point calculations by 80% for alcohols and 3D structures, and 42% for\noxide systems.",
            "author": [
                "Xiaoxiao Wang",
                "Joseph Musielewicz",
                "Richard Tran",
                "Sudheesh Kumar Ethirajan",
                "Xiaoyan Fu",
                "Hilda Mera",
                "John R. Kitchin",
                "Rachel C. Kurchin",
                "Zachary W. Ulissi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01987v1",
                "http://arxiv.org/pdf/2311.01987v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02135v2",
            "title": "Transitive subtournaments of $k$-th power Paley digraphs and improved\n  lower bounds for Ramsey numbers",
            "updated": "2023-11-08T05:42:29Z",
            "published": "2023-11-03T15:19:55Z",
            "summary": "Let $k \\geq 2$ be an even integer. Let $q$ be a prime power such that $q\n\\equiv k+1 \\pmod {2k}$. We define the $\\textit{k-th power Paley digraph}$ of\norder $q$, $G_k(q)$, as the graph with vertex set $\\mathbb{F}_q$ where $a \\to\nb$ is an edge if and only if $b-a$ is a $k$-th power residue. This generalizes\nthe (k=2) Paley Tournament. We provide a formula, in terms of finite field\nhypergeometric functions, for the number of transitive subtournaments of order\nfour contained in $G_k(q)$, $\\mathcal{K}_4(G_k(q))$, which holds for all $k$.\nWe also provide a formula, in terms of Jacobi sums, for the number of\ntransitive subtournaments of order three contained in $G_k(q)$,\n$\\mathcal{K}_3(G_k(q))$. In both cases, we give explicit determinations of\nthese formulae for small $k$. We show that zero values of\n$\\mathcal{K}_4(G_k(q))$ (resp. $\\mathcal{K}_3(G_k(q))$) yield lower bounds for\nthe multicolor directed Ramsey numbers $R_{\\frac{k}{2}}(4)=R(4,4,\\cdots,4)$\n(resp. $R_{\\frac{k}{2}}(3)$). We state explicitly these lower bounds for $k\\leq\n10$ and compare to known bounds, showing improvement for $R_2(4)$ and $R_3(3)$.\nCombining with known multiplicative relations we give improved lower bounds for\n$R_{t}(4)$, for all $t\\geq 2$, and for $R_{t}(3)$, for all $t \\geq 3$.",
            "author": [
                "Dermot McCarthy",
                "Mason Springfield"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02135v2",
                "http://arxiv.org/pdf/2311.02135v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01957v1",
            "title": "Dynamic Regret and Cumulative Constraint Violation Analysis for\n  Distributed Online Constrained Convex Optimization with Event-Triggered\n  Communication",
            "updated": "2023-11-03T14:53:13Z",
            "published": "2023-11-03T14:53:13Z",
            "summary": "This paper focuses on the distributed online convex optimization problem with\ntime-varying inequality constraints over a network of agents, where each agent\ncollaborates with its neighboring agents to minimize the cumulative\nnetwork-wide loss over time. To reduce communication overhead between the\nagents, we propose a distributed event-triggered online primal-dual algorithm\nover a time-varying directed graph. Dynamic network regret and network\ncumulative constraint violation are leveraged to measure the performance of the\nalgorithm. Based on the natural decreasing parameter sequences, we establish\nsublinear dynamic network regret and network cumulative constraint violation\nbounds. The theoretical results broaden the applicability of event-triggered\nonline convex optimization to the regime with inequality constraints. Finally,\na numerical simulation example is provided to verify the theoretical results.",
            "author": [
                "Kunpeng Zhang",
                "Xinlei Yi",
                "Yuzhe Li",
                "Ming Cao",
                "Tianyou Chai",
                "Tao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01957v1",
                "http://arxiv.org/pdf/2311.01957v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01955v1",
            "title": "Too Much Information: Keeping Training Simple for BabyLMs",
            "updated": "2023-11-03T14:50:00Z",
            "published": "2023-11-03T14:50:00Z",
            "summary": "This paper details the work of the University of Groningen for the BabyLM\nChallenge. We follow the idea that, like babies, language models should be\nintroduced to simpler concepts first and build off of that knowledge to\nunderstand more complex concepts. We examine this strategy of\nsimple-then-complex through a variety of lenses, namely context size,\nvocabulary, and overall linguistic complexity of the data. We find that only\none, context size, is truly beneficial to training a language model. However\nthis simple change to context size gives us improvements of 2 points on average\non (Super)GLUE tasks, 1 point on MSGS tasks, and 12\\% on average on BLiMP\ntasks. Our context-limited model outperforms the baseline that was trained on\n10$\\times$ the amount of data.",
            "author": [
                "Lukas Edman",
                "Lisa Bylinina"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01955v1",
                "http://arxiv.org/pdf/2311.01955v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01949v1",
            "title": "Hint-enhanced In-Context Learning wakes Large Language Models up for\n  knowledge-intensive tasks",
            "updated": "2023-11-03T14:39:20Z",
            "published": "2023-11-03T14:39:20Z",
            "summary": "In-context learning (ICL) ability has emerged with the increasing scale of\nlarge language models (LLMs), enabling them to learn input-label mappings from\ndemonstrations and perform well on downstream tasks. However, under the\nstandard ICL setting, LLMs may sometimes neglect query-related information in\ndemonstrations, leading to incorrect predictions. To address this limitation,\nwe propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to\nexplore the power of ICL in open-domain question answering, an important form\nin knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract\nquery-related knowledge from demonstrations, then concatenates the knowledge to\nprompt LLMs in a more explicit way. Furthermore, we track the source of this\nknowledge to identify specific examples, and introduce a Hint-related Example\nRetriever (HER) to select informative examples for enhanced demonstrations. We\nevaluate HICL with HER on 3 open-domain QA benchmarks, and observe average\nperformance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM\nscore and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.",
            "author": [
                "Yifan Wang",
                "Qingyan Guo",
                "Xinzhe Ni",
                "Chufan Shi",
                "Lemao Liu",
                "Haiyun Jiang",
                "Yujiu Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01949v1",
                "http://arxiv.org/pdf/2311.01949v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01945v1",
            "title": "Closure property of contraction-depth of matroids",
            "updated": "2023-11-03T14:35:59Z",
            "published": "2023-11-03T14:35:59Z",
            "summary": "The contraction$^*$-depth is the matroid depth parameter analogous to\ntree-depth of graphs. We establish the matroid analogue of the classical graph\ntheory result asserting that the tree-depth of a graph $G$ is the minimum\nheight of a rooted forest whose closure contains $G$ by proving the following\nfor every matroid $M$ (except the trivial case when $M$ consists of loops and\nbridges only): the contraction$^*$-depth of $M$ plus one is equal to the\nminimum contraction-depth of a matroid containing $M$ as a restriction.",
            "author": [
                "Marcin Brianski",
                "Daniel Kral",
                "Ander Lamaison"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01945v1",
                "http://arxiv.org/pdf/2311.01945v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01940v2",
            "title": "Balanced independent sets and colorings of hypergraphs",
            "updated": "2023-11-07T18:52:29Z",
            "published": "2023-11-03T14:26:54Z",
            "summary": "A $k$-uniform hypergraph $H = (V, E)$ is $k$-partite if $V$ can be\npartitioned into $k$ sets $V_1, \\ldots, V_k$ such that every edge in $E$\ncontains precisely one vertex from each $V_i$. We call such a graph\n$n$-balanced if $|V_i| = n$ for each $i$. An independent set $I$ in $H$ is\nbalanced if $|I\\cap V_i| = |I|/k$ for each $i$, and a coloring is balanced if\neach color class induces a balanced independent set in $H$. In this paper, we\nprovide a lower bound on the balanced independence number $\\alpha_b(H)$ in\nterms of the average degree $D = |E|/n$, and an upper bound on the balanced\nchromatic number $\\chi_b(H)$ in terms of the maximum degree $\\Delta$. Our\nresults match those of recent work of Chakraborti for $k = 2$.",
            "author": [
                "Abhishek Dhawan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01940v2",
                "http://arxiv.org/pdf/2311.01940v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01939v1",
            "title": "A Quantitative Autonomy Quantification Framework for Fully Autonomous\n  Robotic Systems",
            "updated": "2023-11-03T14:26:53Z",
            "published": "2023-11-03T14:26:53Z",
            "summary": "Although autonomous functioning facilitates deployment of robotic systems in\ndomains that admit limited human oversight on our planet and beyond, finding\ncorrespondence between task requirements and autonomous capability is still an\nopen challenge. Consequently, a number of methods for quantifying autonomy have\nbeen proposed over the last three decades, but to our knowledge all these have\nno discernment of sub-mode features of variation of autonomy and some are based\non metrics that violet the Goodhart's law. This paper focuses on the full\nautonomous mode and proposes a task-requirements based autonomy assessment\nframework. The framework starts by establishing robot task characteristics from\nwhich three autonomy metrics, namely requisite capability, reliability and\nresponsiveness, and functions for determining autonomy as a two-part measure,\nnamely of level of autonomy and degree of autonomy are derived. These\ncharacteristics are founded on the realization that robots ultimately replace\nhuman skilled workers, to find a mapping between human job and robot task\ncharacteristics. The distinction between level and degree of autonomy stemmed\nfrom the acknowledgment that autonomy is not just a question of existence, but\nalso one of performance of requisite capability. When continuously monitored,\nthe proposed metrics provide a means of monitoring the integrity of a system.\nThe framework has been demonstrated on two case studies, namely autonomous\nvehicle at an on-road dynamic driving task and the DARPA subT challenge rules\nanalysis. The framework provides not only a tool for quantifying autonomy, but\nalso a regulatory interface and common language for autonomous systems\ndevelopers and users.",
            "author": [
                "Nasser Gyagenda",
                "Hubert Roth"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01939v1",
                "http://arxiv.org/pdf/2311.01939v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02133v1",
            "title": "Safe Online Dynamics Learning with Initially Unknown Models and\n  Infeasible Safety Certificates",
            "updated": "2023-11-03T14:23:57Z",
            "published": "2023-11-03T14:23:57Z",
            "summary": "Safety-critical control tasks with high levels of uncertainty are becoming\nincreasingly common. Typically, techniques that guarantee safety during\nlearning and control utilize constraint-based safety certificates, which can be\nleveraged to compute safe control inputs. However, excessive model uncertainty\ncan render robust safety certification methods or infeasible, meaning no\ncontrol input satisfies the constraints imposed by the safety certificate. This\npaper considers a learning-based setting with a robust safety certificate based\non a control barrier function (CBF) second-order cone program. If the control\nbarrier function certificate is feasible, our approach leverages it to\nguarantee safety. Otherwise, our method explores the system dynamics to collect\ndata and recover the feasibility of the control barrier function constraint. To\nthis end, we employ a method inspired by well-established tools from Bayesian\noptimization. We show that if the sampling frequency is high enough, we recover\nthe feasibility of the robust CBF certificate, guaranteeing safety. Our\napproach requires no prior model and corresponds, to the best of our knowledge,\nto the first algorithm that guarantees safety in settings with occasionally\ninfeasible safety certificates without requiring a backup non-learning-based\ncontroller.",
            "author": [
                "Alexandre Capone",
                "Ryan Cosner",
                "Aaron Ames",
                "Sandra Hirche"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02133v1",
                "http://arxiv.org/pdf/2311.02133v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01936v1",
            "title": "Permutation Tutte polynomial",
            "updated": "2023-11-03T14:20:42Z",
            "published": "2023-11-03T14:20:42Z",
            "summary": "The classical Tutte polynomial is a two-variate polynomial $T_G(x,y)$\nassociated to graphs or more generally, matroids. In this paper, we introduce a\npolynomial $\\widetilde{T}_H(x,y)$ associated to a bipartite graph $H$ that we\ncall the permutation Tutte polynomial of the graph $H$. It turns out that\n$T_G(x,y)$ and $\\widetilde{T}_H(x,y)$ share many properties, and the\npermutation Tutte polynomial serves as a tool to study the classical Tutte\npolynomial. We discuss the analogs of Brylawsi's identities and\nConde--Merino--Welsh type inequalities. In particular, we will show that if $H$\ndoes not contain isolated vertices, then\n$$\\widetilde{T}_H(3,0)\\widetilde{T}_H(0,3)\\geq \\widetilde{T}_H(1,1)^2,$$ which\ngives a short proof to the analogous result of Jackson: $$T_G(3,0)T_G(0,3)\\geq\nT_G(1,1)^2$$ for graphs without loops and bridges. We also improve on the\nconstant $3$ in this statement by showing that one can replace it with\n$2.9243$.",
            "author": [
                "Csongor Beke",
                "Gergely K\u00e1l Cs\u00e1ji",
                "P\u00e9ter Csikv\u00e1ri",
                "S\u00e1ra Pituk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01936v1",
                "http://arxiv.org/pdf/2311.01936v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01928v1",
            "title": "Constructing Temporal Dynamic Knowledge Graphs from Interactive\n  Text-based Games",
            "updated": "2023-11-03T14:09:31Z",
            "published": "2023-11-03T14:09:31Z",
            "summary": "In natural language processing, interactive text-based games serve as a test\nbed for interactive AI systems. Prior work has proposed to play text-based\ngames by acting based on discrete knowledge graphs constructed by the Discrete\nGraph Updater (DGU) to represent the game state from the natural language\ndescription. While DGU has shown promising results with high interpretability,\nit suffers from lower knowledge graph accuracy due to its lack of temporality\nand limited generalizability to complex environments with objects with the same\nlabel. In order to address DGU's weaknesses while preserving its high\ninterpretability, we propose the Temporal Discrete Graph Updater (TDGU), a\nnovel neural network model that represents dynamic knowledge graphs as a\nsequence of timestamped graph events and models them using a temporal point\nbased graph neural network. Through experiments on the dataset collected from a\ntext-based game TextWorld, we show that TDGU outperforms the baseline DGU. We\nfurther show the importance of temporal information for TDGU's performance\nthrough an ablation study and demonstrate that TDGU has the ability to\ngeneralize to more complex environments with objects with the same label. All\nthe relevant code can be found at\n\\url{https://github.com/yukw777/temporal-discrete-graph-updater}.",
            "author": [
                "Keunwoo Peter Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01928v1",
                "http://arxiv.org/pdf/2311.01928v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01922v1",
            "title": "Welded graphs, Wirtinger groups and knotted punctured spheres",
            "updated": "2023-11-03T14:01:00Z",
            "published": "2023-11-03T14:01:00Z",
            "summary": "We develop a general diagrammatic theory of welded graphs, and provide an\nextension of Satoh's Tube map from welded graphs to ribbon surface-links. As a\ntopological application, we obtain a complete link-homotopy classification of\nso-called \\emph{knotted punctured spheres} in $4$--space, by means of the\n$4$--dimensional Milnor invariants introduced by the authors in \\cite{cutAMY}.\nOn the algebraic side, we show that the theory of welded graphs can be\nreinterpreted as a theory of Wirtinger group presentations, up to a natural set\nof transformations; these groups arise as the fundamental group of the exterior\nof the surface-link obtained from the given welded graph by the extended Tube\nmap. Finally, we address the injectivity question for the Tube map, identifying\na new family of local moves on welded links, called $\\Upsilon$ moves, under\nwhich the (non extended) Tube map is invariant.",
            "author": [
                "Benjamin Audoux",
                "Jean-Baptiste Meilhan",
                "Akira Yasuhara"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01922v1",
                "http://arxiv.org/pdf/2311.01922v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57K12, 57K45, 57M05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01920v1",
            "title": "ChartGPT: Leveraging LLMs to Generate Charts from Abstract Natural\n  Language",
            "updated": "2023-11-03T13:58:13Z",
            "published": "2023-11-03T13:58:13Z",
            "summary": "The use of natural language interfaces (NLIs) for the creation of charts is\nbecoming increasingly popular due to the intuitiveness of natural language\ninteractions. One key challenge in this approach is to accurately capture user\nintents and transform them to proper chart specifications. This obstructs the\nwide use of NLI in chart generation, as users' natural language inputs are\ngenerally abstract (i.e., ambiguous or under-specified), without a clear\nspecification of visual encodings. Recently, pre-trained large language models\n(LLMs) have exhibited superior performance in understanding and generating\nnatural language, demonstrating great potential for downstream tasks. Inspired\nby this major trend, we propose ChartGPT, generating charts from abstract\nnatural language inputs. However, LLMs are struggling to address complex logic\nproblems. To enable the model to accurately specify the complex parameters and\nperform operations in chart generation, we decompose the generation process\ninto a step-by-step reasoning pipeline, so that the model only needs to reason\na single and specific sub-task during each run. Moreover, LLMs are pre-trained\non general datasets, which might be biased for the task of chart generation. To\nprovide adequate visualization knowledge, we create a dataset consisting of\nabstract utterances and charts and improve model performance through\nfine-tuning. We further design an interactive interface for ChartGPT that\nallows users to check and modify the intermediate outputs of each step. The\neffectiveness of the proposed system is evaluated through quantitative\nevaluations and a user study.",
            "author": [
                "Yuan Tian",
                "Weiwei Cui",
                "Dazhen Deng",
                "Xinjing Yi",
                "Yurun Yang",
                "Haidong Zhang",
                "Yingcai Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01920v1",
                "http://arxiv.org/pdf/2311.01920v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01918v1",
            "title": "Large Language Models Illuminate a Progressive Pathway to Artificial\n  Healthcare Assistant: A Review",
            "updated": "2023-11-03T13:51:36Z",
            "published": "2023-11-03T13:51:36Z",
            "summary": "With the rapid development of artificial intelligence, large language models\n(LLMs) have shown promising capabilities in mimicking human-level language\ncomprehension and reasoning. This has sparked significant interest in applying\nLLMs to enhance various aspects of healthcare, ranging from medical education\nto clinical decision support. However, medicine involves multifaceted data\nmodalities and nuanced reasoning skills, presenting challenges for integrating\nLLMs. This paper provides a comprehensive review on the applications and\nimplications of LLMs in medicine. It begins by examining the fundamental\napplications of general-purpose and specialized LLMs, demonstrating their\nutilities in knowledge retrieval, research support, clinical workflow\nautomation, and diagnostic assistance. Recognizing the inherent multimodality\nof medicine, the review then focuses on multimodal LLMs, investigating their\nability to process diverse data types like medical imaging and EHRs to augment\ndiagnostic accuracy. To address LLMs' limitations regarding personalization and\ncomplex clinical reasoning, the paper explores the emerging development of\nLLM-powered autonomous agents for healthcare. Furthermore, it summarizes the\nevaluation methodologies for assessing LLMs' reliability and safety in medical\ncontexts. Overall, this review offers an extensive analysis on the\ntransformative potential of LLMs in modern medicine. It also highlights the\npivotal need for continuous optimizations and ethical oversight before these\nmodels can be effectively integrated into clinical practice. Visit\nhttps://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying\nGitHub repository containing latest papers.",
            "author": [
                "Mingze Yuan",
                "Peng Bao",
                "Jiajia Yuan",
                "Yunhao Shen",
                "Zifan Chen",
                "Yi Xie",
                "Jie Zhao",
                "Yang Chen",
                "Li Zhang",
                "Lin Shen",
                "Bin Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01918v1",
                "http://arxiv.org/pdf/2311.01918v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01915v1",
            "title": "Discrete infinity Laplace equations on graphs and tug-of-war games",
            "updated": "2023-11-03T13:48:10Z",
            "published": "2023-11-03T13:48:10Z",
            "summary": "We study the Dirichlet problem of the following discrete infinity Laplace\nequation on a subgraph with finite width $$\\Delta_{\\infty} u(x) = \\inf_{y \\sim\nx}u(y)+\\sup_{y \\sim x}u(y)-2u(x) = f(x).$$ We say that a subgraph has finite\nwidth if the distances from all vertices to the boundary are uniformly bounded.\nBy Perron's method, we show the existence of bounded solutions. We also prove\nthe uniqueness if $f$ is nonnegative by establishing a comparison result, and\nhence obtain the existence of game values for corresponding tug-of-war games\nintroduced by Peres, Schramm, Sheffield, and Wilson (2009). As an application\nwe show a strong Liouville property for infinity harmonic functions. By an\nargument of Arzela-Ascoli, we prove the convergence of solutions of\n$\\varepsilon$-tug-of-war games as $\\varepsilon$ tends to 0. Correspondingly, we\nobtain the existence of bounded solutions to normalized infinity Laplace\nequations on Euclidean domains with finite width.",
            "author": [
                "Fengwen Han",
                "Tao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01915v1",
                "http://arxiv.org/pdf/2311.01915v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01908v2",
            "title": "LLM-driven Multimodal Target Volume Contouring in Radiation Oncology",
            "updated": "2023-11-27T15:23:27Z",
            "published": "2023-11-03T13:38:42Z",
            "summary": "Target volume contouring for radiation therapy is considered significantly\nmore challenging than the normal organ segmentation tasks as it necessitates\nthe utilization of both image and text-based clinical information. Inspired by\nthe recent advancement of large language models (LLMs) that can facilitate the\nintegration of the textural information and images, here we present a novel\nLLM-driven multi-modal AI that utilizes the clinical text information and is\napplicable to the challenging task of target volume contouring for radiation\ntherapy, and validate it within the context of breast cancer radiation therapy\ntarget volume contouring. Using external validation and data-insufficient\nenvironments, which attributes highly conducive to real-world applications, we\ndemonstrate that the proposed model exhibits markedly improved performance\ncompared to conventional vision-only AI models, particularly exhibiting robust\ngeneralization performance and data-efficiency. To our best knowledge, this is\nthe first LLM-driven multimodal AI model that integrates the clinical text\ninformation into target volume delineation for radiation oncology.",
            "author": [
                "Yujin Oh",
                "Sangjoon Park",
                "Hwa Kyung Byun",
                "Jin Sung Kim",
                "Jong Chul Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01908v2",
                "http://arxiv.org/pdf/2311.01908v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01900v1",
            "title": "Online non-parametric likelihood-ratio estimation by Pearson-divergence\n  functional minimization",
            "updated": "2023-11-03T13:20:11Z",
            "published": "2023-11-03T13:20:11Z",
            "summary": "Quantifying the difference between two probability density functions, $p$ and\n$q$, using available data, is a fundamental problem in Statistics and Machine\nLearning. A usual approach for addressing this problem is the likelihood-ratio\nestimation (LRE) between $p$ and $q$, which -- to our best knowledge -- has\nbeen investigated mainly for the offline case. This paper contributes by\nintroducing a new framework for online non-parametric LRE (OLRE) for the\nsetting where pairs of iid observations $(x_t \\sim p, x'_t \\sim q)$ are\nobserved over time. The non-parametric nature of our approach has the advantage\nof being agnostic to the forms of $p$ and $q$. Moreover, we capitalize on the\nrecent advances in Kernel Methods and functional minimization to develop an\nestimator that can be efficiently updated online. We provide theoretical\nguarantees for the performance of the OLRE method along with empirical\nvalidation in synthetic experiments.",
            "author": [
                "Alejandro de la Concha",
                "Nicolas Vayatis",
                "Argyris Kalogeratos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01900v1",
                "http://arxiv.org/pdf/2311.01900v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01888v1",
            "title": "Learning Sparse Codes with Entropy-Based ELBOs",
            "updated": "2023-11-03T13:03:41Z",
            "published": "2023-11-03T13:03:41Z",
            "summary": "Standard probabilistic sparse coding assumes a Laplace prior, a linear\nmapping from latents to observables, and Gaussian observable distributions. We\nhere derive a solely entropy-based learning objective for the parameters of\nstandard sparse coding. The novel variational objective has the following\nfeatures: (A) unlike MAP approximations, it uses non-trivial posterior\napproximations for probabilistic inference; (B) unlike for previous non-trivial\napproximations, the novel objective is fully analytical; and (C) the objective\nallows for a novel principled form of annealing. The objective is derived by\nfirst showing that the standard ELBO objective converges to a sum of entropies,\nwhich matches similar recent results for generative models with Gaussian\npriors. The conditions under which the ELBO becomes equal to entropies are then\nshown to have analytical solutions, which leads to the fully analytical\nobjective. Numerical experiments are used to demonstrate the feasibility of\nlearning with such entropy-based ELBOs. We investigate different posterior\napproximations including Gaussians with correlated latents and deep amortized\napproximations. Furthermore, we numerically investigate entropy-based annealing\nwhich results in improved learning. Our main contributions are theoretical,\nhowever, and they are twofold: (1) for non-trivial posterior approximations, we\nprovide the (to the knowledge of the authors) first analytical ELBO objective\nfor standard probabilistic sparse coding; and (2) we provide the first\ndemonstration on how a recently shown convergence of the ELBO to entropy sums\ncan be used for learning.",
            "author": [
                "Dmytro Velychko",
                "Simon Damm",
                "Asja Fischer",
                "J\u00f6rg L\u00fccke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01888v1",
                "http://arxiv.org/pdf/2311.01888v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01887v1",
            "title": "Graphs with arbitrary Ramsey number and connectivity",
            "updated": "2023-11-03T13:03:05Z",
            "published": "2023-11-03T13:03:05Z",
            "summary": "The Ramsey number $r(G)$ of a graph $G$ is the minimum number $N$ such that\nany red-blue colouring of the edges of $K_N$ contains a monochromatic copy of\n$G$. Pavez-Sign\\'e, Piga and Sanhueza-Matamala proved that for any function\n$n\\leq f(n) \\leq r(K_n)$, there is a sequence of connected graphs $(G_n)_{n\\in\n\\mathbb{N}}$ with $|V(G_n)|=n$ such that $r(G_n)=\\Theta(f(n))$ and conjectured\nthat $G_n$ can additionally have arbitrarily large connectivity. In this note\nwe prove their conjecture.",
            "author": [
                "Isabel Ahme",
                "Alex Scott"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01887v1",
                "http://arxiv.org/pdf/2311.01887v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01884v1",
            "title": "A note on median eigenvalues of subcubic graphs",
            "updated": "2023-11-03T12:50:28Z",
            "published": "2023-11-03T12:50:28Z",
            "summary": "Let $G$ be an simple graph of order $n$ whose adjacency eigenvalues are\n$\\lambda_1\\ge\\dots\\ge\\lambda_n$. The HL--index of $G$ is defined to be $R(G)=\n\\max\\{|\\lambda_{h}|, |\\lambda_{l}|\\}$ with\n$h=\\left\\lfloor\\frac{n+1}{2}\\right\\rfloor$ and $\nl=\\left\\lceil\\frac{n+1}{2}\\right\\rceil.$ Mohar conjectured that $R(G)\\le 1$ for\nevery planar subcubic graph $G$. In this note, we prove that Mohar's Conjecture\nholds for every $K_4$-minor-free subcubic graph. Note that a $K_4$-minor-free\ngraph is also called a series--parallel graph. In addition, $R(G)\\le 1$ for\nevery subcubic graph $G$ which contains a subgraph $K_{2,3}$.",
            "author": [
                "Yuzhenni Wang",
                "Xiao-Dong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01884v1",
                "http://arxiv.org/pdf/2311.01884v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01862v1",
            "title": "$R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and\n  Hallucinations Mitigation",
            "updated": "2023-11-03T12:11:12Z",
            "published": "2023-11-03T12:11:12Z",
            "summary": "While current NL2SQL tasks constructed using Foundation Models have achieved\ncommendable results, their direct application to Natural Language to Graph\nQuery Language (NL2GQL) tasks poses challenges due to the significant\ndifferences between GQL and SQL expressions, as well as the numerous types of\nGQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation\nModels demonstrate superior cross-schema generalization abilities, while\nsmaller Foundation Models struggle to improve their GQL generation capabilities\nthrough fine-tuning. However, after fine-tuning, smaller models exhibit better\nintent comprehension and higher grammatical accuracy. Diverging from rule-based\nand slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller\nand larger Foundation Models as reranker, rewriter and refiner. The approach\nharnesses the comprehension ability of smaller models for information reranker\nand rewriter, and the exceptional generalization and generation capabilities of\nlarger models to transform input natural language queries and code structure\nschema into any form of GQLs. Recognizing the lack of established datasets in\nthis nascent domain, we have created a bilingual dataset derived from graph\ndatabase documentation and some open-source Knowledge Graphs (KGs). We tested\nour approach on this dataset and the experimental results showed that delivers\npromising performance and robustness.Our code and dataset is available at\nhttps://github.com/zhiqix/NL2GQL",
            "author": [
                "Yuhang Zhou",
                "He Yu",
                "Siyu Tian",
                "Dan Chen",
                "Liuzhi Zhou",
                "Xinlin Yu",
                "Chuanjun Ji",
                "Sen Liu",
                "Guangnan Ye",
                "Hongfeng Chai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01862v1",
                "http://arxiv.org/pdf/2311.01862v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01844v1",
            "title": "A three-year comparative study of dominant misconceptions among\n  first-year physics students at a South African university",
            "updated": "2023-11-03T11:17:47Z",
            "published": "2023-11-03T11:17:47Z",
            "summary": "This article discusses a three-year study (2020 - 2022) of dominant\nmisconceptions (DMs) for a large cohort of first-year physics course students\nat the University of Johannesburg (UJ), South Africa. Our study considered\npre-test scores on the Force Concept Inventory using a graphical method, where\nwe found statistical differences between the mean DM scores for the 2020\ncohort, as compared to the 2021 and 2022 cohort; possibly due to the onset of\nCOVID lockdowns. We also compared our data from South Africa with cohorts based\nin Spain and the Kingdom of Saudi Arabia, where the method of DMs was also\napplied. From this comparison, we found some differences in the preconception\nknowledge of the cohorts. Furthermore, we included an analysis of DMs through\nthe `gender lens' for the South African cohort, finding no statistically\nsignificant difference between the means for DM scores of students who identify\nas male or female. Finally, given the diverse language backgrounds and levels\nof matriculation preparation for university level physics courses, we have also\nshown how quickly responding to student misconceptions can be efficiently\naddressed using the method of DMs.",
            "author": [
                "Anna Chrysostomou",
                "Alan S. Cornell",
                "Wade Naylor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01844v1",
                "http://arxiv.org/pdf/2311.01844v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01840v1",
            "title": "Spectral Clustering of Attributed Multi-relational Graphs",
            "updated": "2023-11-03T11:05:29Z",
            "published": "2023-11-03T11:05:29Z",
            "summary": "Graph clustering aims at discovering a natural grouping of the nodes such\nthat similar nodes are assigned to a common cluster. Many different algorithms\nhave been proposed in the literature: for simple graphs, for graphs with\nattributes associated to nodes, and for graphs where edges represent different\ntypes of relations among nodes. However, complex data in many domains can be\nrepresented as both attributed and multi-relational networks.\n  In this paper, we propose SpectralMix, a joint dimensionality reduction\ntechnique for multi-relational graphs with categorical node attributes.\nSpectralMix integrates all information available from the attributes, the\ndifferent types of relations, and the graph structure to enable a sound\ninterpretation of the clustering results. Moreover, it generalizes existing\ntechniques: it reduces to spectral embedding and clustering when only applied\nto a single graph and to homogeneity analysis when applied to categorical data.\nExperiments conducted on several real-world datasets enable us to detect\ndependencies between graph structure and categorical attributes, moreover, they\nexhibit the superiority of SpectralMix over existing methods.",
            "author": [
                "Ylli Sadikaj",
                "Yllka Velaj",
                "Sahar Behzadi",
                "Claudia Plant"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3447548.3467381",
                "http://arxiv.org/abs/2311.01840v1",
                "http://arxiv.org/pdf/2311.01840v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01828v1",
            "title": "Unbiased Offline Evaluation for Learning to Rank with Business Rules",
            "updated": "2023-11-03T10:34:06Z",
            "published": "2023-11-03T10:34:06Z",
            "summary": "For industrial learning-to-rank (LTR) systems, it is common that the output\nof a ranking model is modified, either as a results of post-processing logic\nthat enforces business requirements, or as a result of unforeseen design flaws\nor bugs present in real-world production systems. This poses a challenge for\ndeploying off-policy learning and evaluation methods, as these often rely on\nthe assumption that rankings implied by the model's scores coincide with\ndisplayed items to the users. Further requirements for reliable offline\nevaluation are proper randomization and correct estimation of the propensities\nof displaying each item in any given position of the ranking, which are also\nimpacted by the aforementioned post-processing. We investigate empirically how\nthese scenarios impair off-policy evaluation for learning-to-rank models. We\nthen propose a novel correction method based on the Birkhoff-von-Neumann\ndecomposition that is robust to this type of post-processing. We obtain more\naccurate off-policy estimates in offline experiments, overcoming the problem of\npost-processed rankings. To the best of our knowledge this is the first study\non the impact of real-world business rules on offline evaluation of LTR models.",
            "author": [
                "Matej Jakimov",
                "Alexander Buchholz",
                "Yannik Stein",
                "Thorsten Joachims"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01828v1",
                "http://arxiv.org/pdf/2311.01828v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02127v1",
            "title": "A Systematic Review of Deep Graph Neural Networks: Challenges,\n  Classification, Architectures, Applications & Potential Utility in\n  Bioinformatics",
            "updated": "2023-11-03T10:25:47Z",
            "published": "2023-11-03T10:25:47Z",
            "summary": "In recent years, tasks of machine learning ranging from image processing &\naudio/video analysis to natural language understanding have been transformed by\ndeep learning. The data content in all these scenarios are expressed via\nEuclidean space. However, a considerable amount of application data is\nstructured in non-Euclidean space and is expressed as graphs, e.g. dealing with\ncomplicated interactions & object interdependencies. Modelling physical\nsystems, learning molecular signatures, identifying protein interactions and\npredicting diseases involve utilising a model that can adapt from graph data.\nGraph neural networks (GNNs), specified as artificial-neural models, employ\nmessage transmission between graph nodes to represent graph dependencies and\nare primarily used in the non-Euclidean domain. Variants of GNN like Graph\nRecurrent Networks (GRN), Graph Auto Encoder (GAE), Graph Convolution Networks\n(GCN), Graph Adversarial Methods & Graph Reinforcement learning have exhibited\nbreakthrough productivity on a wide range of tasks, especially in the field of\nbioinformatics, in recent years as a result of the rapid collection of\nbiological network data. Apart from presenting all existing GNN models,\nmathematical analysis and comparison of the variants of all types of GNN have\nbeen highlighted in this survey. Graph neural networks are investigated for\ntheir potential real-world applications in various fields, focusing on\nBioinformatics. Furthermore, resources for evaluating graph neural network\nmodels and accessing open-source code & benchmark data sets are included.\nUltimately, we provide some (seven) proposals for future research in this\nrapidly evolving domain. GNNs have the potential to be an excellent tool for\nsolving a wide range of biological challenges in bioinformatics research, as\nthey are best represented as connected complex graphs.",
            "author": [
                "Adil Mudasir Malla",
                "Asif Ali Banka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02127v1",
                "http://arxiv.org/pdf/2311.02127v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01808v1",
            "title": "Diameter of uniform spanning trees on random weighted graphs",
            "updated": "2023-11-03T09:36:04Z",
            "published": "2023-11-03T09:36:04Z",
            "summary": "For any edge weight distribution, we consider the uniform spanning tree (UST)\non finite graphs with i.i.d. random edge weights. We show that, for bounded\ndegree expander graphs and finite boxes of ${\\mathbb Z}^d$, the diameter of the\nUST is of order $n^{1/2+o(1)}$ with high probability, where $n$ is the number\nof vertices.",
            "author": [
                "Luca Makowiec",
                "Michele Salvi",
                "Rongfeng Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01808v1",
                "http://arxiv.org/pdf/2311.01808v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01806v1",
            "title": "Sketching for Convex and Nonconvex Regularized Least Squares with Sharp\n  Guarantees",
            "updated": "2023-11-03T09:35:01Z",
            "published": "2023-11-03T09:35:01Z",
            "summary": "Randomized algorithms are important for solving large-scale optimization\nproblems. In this paper, we propose a fast sketching algorithm for least square\nproblems regularized by convex or nonconvex regularization functions, Sketching\nfor Regularized Optimization (SRO). Our SRO algorithm first generates a sketch\nof the original data matrix, then solves the sketched problem. Different from\nexisting randomized algorithms, our algorithm handles general Frechet\nsubdifferentiable regularization functions in an unified framework. We present\ngeneral theoretical result for the approximation error between the optimization\nresults of the original problem and the sketched problem for regularized least\nsquare problems which can be convex or nonconvex. For arbitrary convex\nregularizer, relative-error bound is proved for the approximation error.\nImportantly, minimax rates for sparse signal estimation by solving the sketched\nsparse convex or nonconvex learning problems are also obtained using our\ngeneral theoretical result under mild conditions. To the best of our knowledge,\nour results are among the first to demonstrate minimax rates for convex or\nnonconvex sparse learning problem by sketching under a unified theoretical\nframework. We further propose an iterative sketching algorithm which reduces\nthe approximation error exponentially by iteratively invoking the sketching\nalgorithm. Experimental results demonstrate the effectiveness of the proposed\nSRO and Iterative SRO algorithms.",
            "author": [
                "Yingzhen Yang",
                "Ping Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01806v1",
                "http://arxiv.org/pdf/2311.01806v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01796v1",
            "title": "Learning to Augment Distributions for Out-of-Distribution Detection",
            "updated": "2023-11-03T09:19:33Z",
            "published": "2023-11-03T09:19:33Z",
            "summary": "Open-world classification systems should discern out-of-distribution (OOD)\ndata whose labels deviate from those of in-distribution (ID) cases, motivating\nrecent studies in OOD detection. Advanced works, despite their promising\nprogress, may still fail in the open world, owing to the lack of knowledge\nabout unseen OOD data in advance. Although one can access auxiliary OOD data\n(distinct from unseen ones) for model training, it remains to analyze how such\nauxiliary data will work in the open world. To this end, we delve into such a\nproblem from a learning theory perspective, finding that the distribution\ndiscrepancy between the auxiliary and the unseen real OOD data is the key to\naffecting the open-world detection performance. Accordingly, we propose\nDistributional-Augmented OOD Learning (DAL), alleviating the OOD distribution\ndiscrepancy by crafting an OOD distribution set that contains all distributions\nin a Wasserstein ball centered on the auxiliary OOD distribution. We justify\nthat the predictor trained over the worst OOD data in the ball can shrink the\nOOD distribution discrepancy, thus improving the open-world detection\nperformance given only the auxiliary OOD data. We conduct extensive evaluations\nacross representative OOD detection setups, demonstrating the superiority of\nour DAL over its advanced counterparts.",
            "author": [
                "Qizhou Wang",
                "Zhen Fang",
                "Yonggang Zhang",
                "Feng Liu",
                "Yixuan Li",
                "Bo Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01796v1",
                "http://arxiv.org/pdf/2311.01796v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01786v1",
            "title": "TCM-GPT: Efficient Pre-training of Large Language Models for Domain\n  Adaptation in Traditional Chinese Medicine",
            "updated": "2023-11-03T08:54:50Z",
            "published": "2023-11-03T08:54:50Z",
            "summary": "Pre-training and fine-tuning have emerged as a promising paradigm across\nvarious natural language processing (NLP) tasks. The effectiveness of\npretrained large language models (LLM) has witnessed further enhancement,\nholding potential for applications in the field of medicine, particularly in\nthe context of Traditional Chinese Medicine (TCM). However, the application of\nthese general models to specific domains often yields suboptimal results,\nprimarily due to challenges like lack of domain knowledge, unique objectives,\nand computational efficiency. Furthermore, their effectiveness in specialized\ndomains, such as Traditional Chinese Medicine, requires comprehensive\nevaluation. To address the above issues, we propose a novel domain specific\nTCMDA (TCM Domain Adaptation) approach, efficient pre-training with\ndomain-specific corpus. Specifically, we first construct a large TCM-specific\ncorpus, TCM-Corpus-1B, by identifying domain keywords and retreving from\ngeneral corpus. Then, our TCMDA leverages the LoRA which freezes the pretrained\nmodel's weights and uses rank decomposition matrices to efficiently train\nspecific dense layers for pre-training and fine-tuning, efficiently aligning\nthe model with TCM-related tasks, namely TCM-GPT-7B. We further conducted\nextensive experiments on two TCM tasks, including TCM examination and TCM\ndiagnosis. TCM-GPT-7B archived the best performance across both datasets,\noutperforming other models by relative increments of 17% and 12% in accuracy,\nrespectively. To the best of our knowledge, our study represents the pioneering\nvalidation of domain adaptation of a large language model with 7 billion\nparameters in TCM domain. We will release both TCMCorpus-1B and TCM-GPT-7B\nmodel once accepted to facilitate interdisciplinary development in TCM and NLP,\nserving as the foundation for further study.",
            "author": [
                "Guoxing Yang",
                "Jianyu Shi",
                "Zan Wang",
                "Xiaohong Liu",
                "Guangyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01786v1",
                "http://arxiv.org/pdf/2311.01786v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01759v1",
            "title": "TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices",
            "updated": "2023-11-03T07:34:47Z",
            "published": "2023-11-03T07:34:47Z",
            "summary": "Developing deep learning models on tiny devices (e.g. Microcontroller units,\nMCUs) has attracted much attention in various embedded IoT applications.\nHowever, it is challenging to efficiently design and deploy recent advanced\nmodels (e.g. transformers) on tiny devices due to their severe hardware\nresource constraints. In this work, we propose TinyFormer, a framework\nspecifically designed to develop and deploy resource-efficient transformers on\nMCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine.\nSeparately, SuperNAS aims to search for an appropriate supernet from a vast\nsearch space. SparseNAS evaluates the best sparse single-path model including\ntransformer architecture from the identified supernet. Finally, SparseEngine\nefficiently deploys the searched sparse models onto MCUs. To the best of our\nknowledge, SparseEngine is the first deployment framework capable of performing\ninference of sparse models with transformer on MCUs. Evaluation results on the\nCIFAR-10 dataset demonstrate that TinyFormer can develop efficient transformers\nwith an accuracy of $96.1\\%$ while adhering to hardware constraints of $1$MB\nstorage and $320$KB memory. Additionally, TinyFormer achieves significant\nspeedups in sparse inference, up to $12.2\\times$, when compared to the CMSIS-NN\nlibrary. TinyFormer is believed to bring powerful transformers into TinyML\nscenarios and greatly expand the scope of deep learning applications.",
            "author": [
                "Jianlei Yang",
                "Jiacheng Liao",
                "Fanding Lei",
                "Meichen Liu",
                "Junyi Chen",
                "Lingkun Long",
                "Han Wan",
                "Bei Yu",
                "Weisheng Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01759v1",
                "http://arxiv.org/pdf/2311.01759v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01755v1",
            "title": "Towards a Unified Transformer-based Framework for Scene Graph Generation\n  and Human-object Interaction Detection",
            "updated": "2023-11-03T07:25:57Z",
            "published": "2023-11-03T07:25:57Z",
            "summary": "Scene graph generation (SGG) and human-object interaction (HOI) detection are\ntwo important visual tasks aiming at localising and recognising relationships\nbetween objects, and interactions between humans and objects, respectively.\n  Prevailing works treat these tasks as distinct tasks, leading to the\ndevelopment of task-specific models tailored to individual datasets. However,\nwe posit that the presence of visual relationships can furnish crucial\ncontextual and intricate relational cues that significantly augment the\ninference of human-object interactions. This motivates us to think if there is\na natural intrinsic relationship between the two tasks, where scene graphs can\nserve as a source for inferring human-object interactions. In light of this, we\nintroduce SG2HOI+, a unified one-step model based on the Transformer\narchitecture. Our approach employs two interactive hierarchical Transformers to\nseamlessly unify the tasks of SGG and HOI detection. Concretely, we initiate a\nrelation Transformer tasked with generating relation triples from a suite of\nvisual features. Subsequently, we employ another transformer-based decoder to\npredict human-object interactions based on the generated relation triples. A\ncomprehensive series of experiments conducted across established benchmark\ndatasets including Visual Genome, V-COCO, and HICO-DET demonstrates the\ncompelling performance of our SG2HOI+ model in comparison to prevalent\none-stage SGG models. Remarkably, our approach achieves competitive performance\nwhen compared to state-of-the-art HOI methods. Additionally, we observe that\nour SG2HOI+ jointly trained on both SGG and HOI tasks in an end-to-end manner\nyields substantial improvements for both tasks compared to individualized\ntraining paradigms.",
            "author": [
                "Tao He",
                "Lianli Gao",
                "Jingkuan Song",
                "Yuan-Fang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01755v1",
                "http://arxiv.org/pdf/2311.01755v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01750v1",
            "title": "Ramsey properties of randomly perturbed hypergraphs",
            "updated": "2023-11-03T07:06:34Z",
            "published": "2023-11-03T07:06:34Z",
            "summary": "Let $\\tilde K_t$ denote the $3$-uniform linear clique of order $t$. Given an\neven integer $t \\geq 4$, let $M$ denote the asymmetric maximal density of\n$\\tilde K_t$ and $\\tilde K_{t/2}$. We prove that there exists a constant $C>0$\nsuch that, if $(H_n)_{n \\in \\mathbb{N}}$ is any sequence of dense $3$-uniform\nhypergraphs and $p := p(n) \\geq Cn^{-1/M}$, then $$ \\lim_{n \\to \\infty} \\Pr[H_n\n\\cup R_n \\to (\\tilde K_t)_2] =1 $$ holds, whenever $R_n \\sim \\mathbb{H}(n,p)$,\nwhere the latter denotes the binomial $3$-uniform random hypergraph\ndistribution. We conjecture that this result uncovers the threshold for the\nproperty in question for $t \\geq 6$.\n  The key tools of our proof are a new variant of the Strong Hypergraph\nRegularity Lemma along with a new Tuple Lemma accompanying it. Our variant\nincorporates both the strong and the weak hypergraph regularity lemmas into a\nsingle lemma; allowing for graph-like applications of the strong lemma to be\ncarried out in $3$-uniform hypergraphs.",
            "author": [
                "Elad Aigner-Horev",
                "Dan Hefetz",
                "Mathias Schacht"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01750v1",
                "http://arxiv.org/pdf/2311.01750v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01733v1",
            "title": "Independent domination stability in graphs",
            "updated": "2023-11-03T05:59:21Z",
            "published": "2023-11-03T05:59:21Z",
            "summary": "A non-empty set $S\\subseteq V (G)$ of the simple graph $G=(V(G),E(G))$ is an\nindependent dominating set of $G$ if every vertex not in $S$ is adjacent with\nsome vertex in $S$ and the vertices of $S$ are pairwise non-adjacent. The\nindependent domination number of $G$, denoted by $\\gamma_i(G)$, is the minimum\nsize of all independent dominating sets of $G$. The independent domination\nstability, or simply $id$-stability of $G$ is the minimum number of vertices\nwhose removal changes the independent domination number of $G$. In this paper,\nwe investigate properties of independent domination stability in graphs. In\nparticular, we obtain several bounds and obtain the independent domination\nstability of some operations of two graphs.",
            "author": [
                "Saeid Alikhani",
                "Mazharodin Mehraban",
                "Alexei Zakharov",
                "Hamidreza Golmohammadi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01733v1",
                "http://arxiv.org/pdf/2311.01733v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C05, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01729v2",
            "title": "CDGraph: Dual Conditional Social Graph Synthesizing via Diffusion Model",
            "updated": "2023-11-06T02:19:09Z",
            "published": "2023-11-03T05:54:07Z",
            "summary": "The social graphs synthesized by the generative models are increasingly in\ndemand due to data scarcity and concerns over user privacy. One of the key\nperformance criteria for generating social networks is the fidelity to\nspecified conditionals, such as users with certain membership and financial\nstatus. While recent diffusion models have shown remarkable performance in\ngenerating images, their effectiveness in synthesizing graphs has not yet been\nexplored in the context of conditional social graphs. In this paper, we propose\nthe first kind of conditional diffusion model for social networks, CDGraph,\nwhich trains and synthesizes graphs based on two specified conditions. We\npropose the co-evolution dependency in the denoising process of CDGraph to\ncapture the mutual dependencies between the dual conditions and further\nincorporate social homophily and social contagion to preserve the connectivity\nbetween nodes while satisfying the specified conditions. Moreover, we introduce\na novel classifier loss, which guides the training of the diffusion process\nthrough the mutual dependency of dual conditions. We evaluate CDGraph against\nfour existing graph generative methods, i.e., SPECTRE, GSM, EDGE, and DiGress,\non four datasets. Our results show that the generated graphs from CDGraph\nachieve much higher dual-conditional validity and lower discrepancy in various\nsocial network metrics than the baselines, thus demonstrating its proficiency\nin generating dual-conditional social graphs.",
            "author": [
                "Jui-Yi Tsai",
                "Ya-Wen Teng",
                "Ho Chiok Yew",
                "De-Nian Yang",
                "Lydia Y. Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01729v2",
                "http://arxiv.org/pdf/2311.01729v2"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01727v1",
            "title": "Flexible Error Mitigation of Quantum Processes with Data Augmentation\n  Empowered Neural Model",
            "updated": "2023-11-03T05:52:14Z",
            "published": "2023-11-03T05:52:14Z",
            "summary": "Neural networks have shown their effectiveness in various tasks in the realm\nof quantum computing. However, their application in quantum error mitigation, a\ncrucial step towards realizing practical quantum advancements, has been\nrestricted by reliance on noise-free statistics. To tackle this critical\nchallenge, we propose a data augmentation empowered neural model for error\nmitigation (DAEM). Our model does not require any prior knowledge about the\nspecific noise type and measurement settings and can estimate noise-free\nstatistics solely from the noisy measurement results of the target quantum\nprocess, rendering it highly suitable for practical implementation. In\nnumerical experiments, we show the model's superior performance in mitigating\nvarious types of noise, including Markovian noise and Non-Markovian noise,\ncompared with previous error mitigation methods. We further demonstrate its\nversatility by employing the model to mitigate errors in diverse types of\nquantum processes, including those involving large-scale quantum systems and\ncontinuous-variable quantum states. This powerful data augmentation-empowered\nneural model for error mitigation establishes a solid foundation for realizing\nmore reliable and robust quantum technologies in practical applications.",
            "author": [
                "Manwen Liao",
                "Yan Zhu",
                "Giulio Chiribella",
                "Yuxiang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01727v1",
                "http://arxiv.org/pdf/2311.01727v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01712v1",
            "title": "A New Korean Text Classification Benchmark for Recognizing the Political\n  Intents in Online Newspapers",
            "updated": "2023-11-03T04:59:55Z",
            "published": "2023-11-03T04:59:55Z",
            "summary": "Many users reading online articles in various magazines may suffer\nconsiderable difficulty in distinguishing the implicit intents in texts. In\nthis work, we focus on automatically recognizing the political intents of a\ngiven online newspaper by understanding the context of the text. To solve this\ntask, we present a novel Korean text classification dataset that contains\nvarious articles. We also provide deep-learning-based text classification\nbaseline models trained on the proposed dataset. Our dataset contains 12,000\nnews articles that may contain political intentions, from the politics section\nof six of the most representative newspaper organizations in South Korea. All\nthe text samples are labeled simultaneously in two aspects (1) the level of\npolitical orientation and (2) the level of pro-government. To the best of our\nknowledge, our paper is the most large-scale Korean news dataset that contains\nlong text and addresses multi-task classification problems. We also train\nrecent state-of-the-art (SOTA) language models that are based on transformer\narchitectures and demonstrate that the trained models show decent text\nclassification performance. All the codes, datasets, and trained models are\navailable at https://github.com/Kdavid2355/KoPolitic-Benchmark-Dataset.",
            "author": [
                "Beomjune Kim",
                "Eunsun Lee",
                "Dongbin Na"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01712v1",
                "http://arxiv.org/pdf/2311.01712v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01703v2",
            "title": "Taking a PEEK into YOLOv5 for Satellite Component Recognition via\n  Entropy-based Visual Explanations",
            "updated": "2023-11-25T20:22:24Z",
            "published": "2023-11-03T04:21:27Z",
            "summary": "The escalating risk of collisions and the accumulation of space debris in Low\nEarth Orbit (LEO) has reached critical concern due to the ever increasing\nnumber of spacecraft. Addressing this crisis, especially in dealing with\nnon-cooperative and unidentified space debris, is of paramount importance. This\npaper contributes to efforts in enabling autonomous swarms of small chaser\nsatellites for target geometry determination and safe flight trajectory\nplanning for proximity operations in LEO. Our research explores on-orbit use of\nthe You Only Look Once v5 (YOLOv5) object detection model trained to detect\nsatellite components. While this model has shown promise, its inherent lack of\ninterpretability hinders human understanding, a critical aspect of validating\nalgorithms for use in safety-critical missions. To analyze the decision\nprocesses, we introduce Probabilistic Explanations for Entropic Knowledge\nextraction (PEEK), a method that utilizes information theoretic analysis of the\nlatent representations within the hidden layers of the model. Through both\nsynthetic in hardware-in-the-loop experiments, PEEK illuminates the\ndecision-making processes of the model, helping identify its strengths,\nlimitations and biases.",
            "author": [
                "Mackenzie J. Meni",
                "Trupti Mahendrakar",
                "Olivia D. M. Raney",
                "Ryan T. White",
                "Michael L. Mayo",
                "Kevin Pilkiewicz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01703v2",
                "http://arxiv.org/pdf/2311.01703v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01702v1",
            "title": "Medical Image Segmentation with Domain Adaptation: A Survey",
            "updated": "2023-11-03T04:17:06Z",
            "published": "2023-11-03T04:17:06Z",
            "summary": "Deep learning (DL) has shown remarkable success in various medical imaging\ndata analysis applications. However, it remains challenging for DL models to\nachieve good generalization, especially when the training and testing datasets\nare collected at sites with different scanners, due to domain shift caused by\ndifferences in data distributions. Domain adaptation has emerged as an\neffective means to address this challenge by mitigating domain gaps in medical\nimaging applications. In this review, we specifically focus on domain\nadaptation approaches for DL-based medical image segmentation. We first present\nthe motivation and background knowledge underlying domain adaptations, then\nprovide a comprehensive review of domain adaptation applications in medical\nimage segmentations, and finally discuss the challenges, limitations, and\nfuture research trends in the field to promote the methodology development of\ndomain adaptation in the context of medical image segmentation. Our goal was to\nprovide researchers with up-to-date references on the applications of domain\nadaptation in medical image segmentation studies.",
            "author": [
                "Yuemeng Li",
                "Yong Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01702v1",
                "http://arxiv.org/pdf/2311.01702v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01689v1",
            "title": "Data-Free Distillation of Language Model by Text-to-Text Transfer",
            "updated": "2023-11-03T03:31:47Z",
            "published": "2023-11-03T03:31:47Z",
            "summary": "Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the\nmodel when original training data is unavailable. Previous works for DFKD in\nNLP mainly focus on distilling encoder-only structures like BERT on\nclassification tasks, which overlook the notable progress of generative\nlanguage modeling. In this work, we propose a novel DFKD framework, namely\nDFKD-T$^{3}$, where the pretrained generative language model can also serve as\na controllable data generator for model compression. This novel framework\nDFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to\ntransform the general domain corpus to compression-friendly task data,\ntargeting to improve both the \\textit{specificity} and \\textit{diversity}.\nExtensive experiments show that our method can boost the distillation\nperformance in various downstream tasks such as sentiment analysis, linguistic\nacceptability, and information extraction. Furthermore, we show that the\ngenerated texts can be directly used for distilling other language models and\noutperform the SOTA methods, making our method more appealing in a general DFKD\nsetting. Our code is available at\nhttps://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\\_T3.",
            "author": [
                "Zheyuan Bai",
                "Xinduo Liu",
                "Hailin Hu",
                "Tianyu Guo",
                "Qinghua Zhang",
                "Yunhe Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01689v1",
                "http://arxiv.org/pdf/2311.01689v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01684v1",
            "title": "CASE: Commonsense-Augmented Score with an Expanded Answer Space",
            "updated": "2023-11-03T03:15:26Z",
            "published": "2023-11-03T03:15:26Z",
            "summary": "LLMs have demonstrated impressive zero-shot performance on NLP tasks thanks\nto the knowledge they acquired in their training. In multiple-choice QA tasks,\nthe LM probabilities are used as an imperfect measure of the plausibility of\neach answer choice. One of the major limitations of the basic score is that it\ntreats all words as equally important. We propose CASE, a Commonsense-Augmented\nScore with an Expanded Answer Space. CASE addresses this limitation by\nassigning importance weights for individual words based on their semantic\nrelations to other words in the input. The dynamic weighting approach\noutperforms basic LM scores, not only because it reduces noise from unimportant\nwords, but also because it informs the model of implicit commonsense knowledge\nthat may be useful for answering the question. We then also follow prior work\nin expanding the answer space by generating lexically-divergent answers that\nare conceptually-similar to the choices. When combined with answer space\nexpansion, our method outperforms strong baselines on 5 commonsense benchmarks.\nWe further show these two approaches are complementary and may be especially\nbeneficial when using smaller LMs.",
            "author": [
                "Wenkai Chen",
                "Sahithya Ravi",
                "Vered Shwartz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01684v1",
                "http://arxiv.org/pdf/2311.01684v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02117v2",
            "title": "Cooperative Network Learning for Large-Scale and Decentralized Graphs",
            "updated": "2023-11-07T08:50:24Z",
            "published": "2023-11-03T02:56:01Z",
            "summary": "Graph research, the systematic study of interconnected data points\nrepresented as graphs, plays a vital role in capturing intricate relationships\nwithin networked systems. However, in the real world, as graphs scale up,\nconcerns about data security among different data-owning agencies arise,\nhindering information sharing and, ultimately, the utilization of graph data.\nTherefore, establishing a mutual trust mechanism among graph agencies is\ncrucial for unlocking the full potential of graphs. Here, we introduce a\nCooperative Network Learning (CNL) framework to ensure secure graph computing\nfor various graph tasks. Essentially, this CNL framework unifies the local and\nglobal perspectives of GNN computing with distributed data for an agency by\nvirtually connecting all participating agencies as a global graph without a\nfixed central coordinator. Inter-agency computing is protected by various\ntechnologies inherent in our framework, including homomorphic encryption and\nsecure transmission. Moreover, each agency has a fair right to design or employ\nvarious graph learning models from its local or global perspective. Thus, CNL\ncan collaboratively train GNN models based on decentralized graphs inferred\nfrom local and global graphs. Experiments on contagion dynamics prediction and\ntraditional graph tasks (i.e., node classification and link prediction)\ndemonstrate that our CNL architecture outperforms state-of-the-art GNNs\ndeveloped at individual sites, revealing that CNL can provide a reliable, fair,\nsecure, privacy-preserving, and global perspective to build effective and\npersonalized models for network applications. We hope this framework will\naddress privacy concerns in graph-related research and integrate decentralized\ngraph data structures to benefit the network research community in cooperation\nand innovation.",
            "author": [
                "Qiang Wu",
                "Yiming Huang",
                "Yujie Zeng",
                "Yijie Teng",
                "Fang Zhou",
                "Linyuan L\u00fc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02117v2",
                "http://arxiv.org/pdf/2311.02117v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02116v1",
            "title": "Resist Label Noise with PGM for Graph Neural Networks",
            "updated": "2023-11-03T02:47:06Z",
            "published": "2023-11-03T02:47:06Z",
            "summary": "While robust graph neural networks (GNNs) have been widely studied for graph\nperturbation and attack, those for label noise have received significantly less\nattention. Most existing methods heavily rely on the label smoothness\nassumption to correct noisy labels, which adversely affects their performance\non heterophilous graphs. Further, they generally perform poorly in high\nnoise-rate scenarios. To address these problems, in this paper, we propose a\nnovel probabilistic graphical model (PGM) based framework LNP. Given a noisy\nlabel set and a clean label set, our goal is to maximize the likelihood of\nlabels in the clean set. We first present LNP-v1, which generates clean labels\nbased on graphs only in the Bayesian network. To further leverage the\ninformation of clean labels in the noisy label set, we put forward LNP-v2,\nwhich incorporates the noisy label set into the Bayesian network to generate\nclean labels. The generative process can then be used to predict labels for\nunlabeled nodes. We conduct extensive experiments to show the robustness of LNP\non varying noise types and rates, and also on graphs with different\nheterophilies. In particular, we show that LNP can lead to inspiring\nperformance in high noise-rate situations.",
            "author": [
                "Qingqing Ge",
                "Jianxiang Yu",
                "Zeyuan Zhao",
                "Xiang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02116v1",
                "http://arxiv.org/pdf/2311.02116v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01672v1",
            "title": "Counterexamples to Negami's Conjecture have ply at least 14",
            "updated": "2023-11-03T02:37:38Z",
            "published": "2023-11-03T02:37:38Z",
            "summary": "S. Negami conjectured in $1988$ that a connected graph has a finite planar\ncover if and only if it embeds in the projective plane. It follows from the\nworks of D. Archdeacon, M. Fellows, P. Hlin\\v{e}n\\'{y}, and S. Negami that the\nconjecture is true if the graph $K_{2, 2, 2, 1}$ has no finite planar cover. We\nprove that $K_{2, 2, 2, 1}$ has no planar cover of ply less than $14$ and\nconsequently, any counterexample to Negami's conjecture has ply at least 14 and\nat least $98$ vertices.",
            "author": [
                "Dickson Y. B. Annor",
                "Yuri Nikolayevsky",
                "Michael S. Payne"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01672v1",
                "http://arxiv.org/pdf/2311.01672v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10 (Primary), 57M15 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01657v1",
            "title": "Simulating Heavy-Hex Transverse Field Ising Model Magnetization Dynamics\n  Using Programmable Quantum Annealers",
            "updated": "2023-11-03T01:33:24Z",
            "published": "2023-11-03T01:33:24Z",
            "summary": "Recently, a Hamiltonian dynamics simulation was performed on a kicked\nferromagnetic 2D transverse field Ising model with a connectivity graph native\nto the 127 qubit heavy-hex IBM Quantum architecture using ZNE quantum error\nmitigation. We demonstrate that one of the observables in this Trotterized\nHamiltonian dynamics simulation, namely magnetization, can be efficiently\nsimulated on current superconducting qubit-based programmable quantum annealing\ncomputers. We show this using two distinct methods: reverse quantum annealing\nand h-gain state encoding. This simulation is possible because the 127 qubit\nheavy-hex connectivity graph can be natively embedded onto the D-Wave Pegasus\nquantum annealer hardware graph and because there exists a direct equivalence\nbetween the energy scales of the two types of quantum computers. We derive\nequivalent anneal pauses in order to simulate the Trotterized quantum circuit\ndynamics for varying Rx rotations $\\theta_h \\in (0, \\frac{\\pi}{2}]$, using\nquantum annealing processors. Multiple disjoint instances of the Ising model of\ninterest can be embedded onto the D-Wave Pegasus hardware graph, allowing for\nparallel quantum annealing. We report equivalent magnetization dynamics using\nquantum annealing for time steps of 20, 50 up to 10,000, which we find are\nconsistent with exact classical 27 qubit heavy-hex Trotterized circuit\nmagnetization dynamics, and we observe reasonable, albeit noisy, agreement with\nthe existing simulations for single site magnetization at 20 Trotter steps. The\nquantum annealers are able to simulate equivalent magnetization dynamics for\nthousands of time steps, significantly out of the computational reach of the\ndigital quantum computers on which the original Hamiltonian dynamics\nsimulations were performed.",
            "author": [
                "Elijah Pelofske",
                "Andreas B\u00e4rtschi",
                "Stephan Eidenbenz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01657v1",
                "http://arxiv.org/pdf/2311.01657v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01647v1",
            "title": "Calibrate and Boost Logical Expressiveness of GNN Over Multi-Relational\n  and Temporal Graphs",
            "updated": "2023-11-03T00:33:24Z",
            "published": "2023-11-03T00:33:24Z",
            "summary": "As a powerful framework for graph representation learning, Graph Neural\nNetworks (GNNs) have garnered significant attention in recent years. However,\nto the best of our knowledge, there has been no formal analysis of the logical\nexpressiveness of GNNs as Boolean node classifiers over multi-relational\ngraphs, where each edge carries a specific relation type. In this paper, we\ninvestigate $\\mathcal{FOC}_2$, a fragment of first-order logic with two\nvariables and counting quantifiers. On the negative side, we demonstrate that\nthe R$^2$-GNN architecture, which extends the local message passing GNN by\nincorporating global readout, fails to capture $\\mathcal{FOC}_2$ classifiers in\nthe general case. Nevertheless, on the positive side, we establish that\nR$^2$-GNNs models are equivalent to $\\mathcal{FOC}_2$ classifiers under certain\nrestricted yet reasonable scenarios. To address the limitations of R$^2$-GNNs\nregarding expressiveness, we propose a simple graph transformation technique,\nakin to a preprocessing step, which can be executed in linear time. This\ntransformation enables R$^2$-GNNs to effectively capture any $\\mathcal{FOC}_2$\nclassifiers when applied to the \"transformed\" input graph. Moreover, we extend\nour analysis of expressiveness and graph transformation to temporal graphs,\nexploring several temporal GNN architectures and providing an expressiveness\nhierarchy for them. To validate our findings, we implement R$^2$-GNNs and the\ngraph transformation technique and conduct empirical tests in node\nclassification tasks against various well-known GNN architectures that support\nmulti-relational or temporal graphs. Our experimental results consistently\ndemonstrate that R$^2$-GNN with the graph transformation outperforms the\nbaseline methods on both synthetic and real-world datasets",
            "author": [
                "Yeyuan Chen",
                "Dingmin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01647v1",
                "http://arxiv.org/pdf/2311.01647v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04921v1",
            "title": "Successor Features for Efficient Multisubject Controlled Text Generation",
            "updated": "2023-11-03T00:17:08Z",
            "published": "2023-11-03T00:17:08Z",
            "summary": "While large language models (LLMs) have achieved impressive performance in\ngenerating fluent and realistic text, controlling the generated text so that it\nexhibits properties such as safety, factuality, and non-toxicity remains\nchallenging. % such as DExperts, GeDi, and rectification Existing\ndecoding-based methods are static in terms of the dimension of control; if the\ntarget subject is changed, they require new training. Moreover, it can quickly\nbecome prohibitive to concurrently control multiple subjects. In this work, we\nintroduce SF-GEN, which is grounded in two primary concepts: successor features\n(SFs) to decouple the LLM's dynamics from task-specific rewards, and language\nmodel rectification to proportionally adjust the probability of selecting a\ntoken based on the likelihood that the finished text becomes undesired. SF-GEN\nseamlessly integrates the two to enable dynamic steering of text generation\nwith no need to alter the LLM's parameters. Thanks to the decoupling effect\ninduced by successor features, our method proves to be memory-wise and\ncomputationally efficient for training as well as decoding, especially when\ndealing with multiple target subjects. To the best of our knowledge, our\nresearch represents the first application of successor features in text\ngeneration. In addition to its computational efficiency, the resultant language\nproduced by our method is comparable to the SOTA (and outperforms baselines) in\nboth control measures as well as language quality, which we demonstrate through\na series of experiments in various controllable text generation tasks.",
            "author": [
                "Meng Cao",
                "Mehdi Fatemi",
                "Jackie Chi Kit Cheung",
                "Samira Shabanian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04921v1",
                "http://arxiv.org/pdf/2311.04921v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01630v1",
            "title": "Generalizations of Matrix Multiplication can solve the Light Bulb\n  Problem",
            "updated": "2023-11-02T22:49:41Z",
            "published": "2023-11-02T22:49:41Z",
            "summary": "In the light bulb problem, one is given uniformly random vectors $x_1,\n\\ldots, x_n, y_1, \\ldots, y_n \\in \\{-1,1\\}^d$. They are all chosen\nindependently except a planted pair $(x_{i^*}, y_{j^*})$ is chosen with\ncorrelation $\\rho>0$. The goal is to find the planted pair. This problem was\nintroduced over 30 years ago by L.~Valiant, and is known to have many\napplications in data analysis, statistics, and learning theory.\n  The naive algorithm runs in $\\Omega(n^2)$ time, and algorithms based on\nLocality-Sensitive Hashing approach quadratic time as $\\rho \\to 0$. In 2012,\nG.~Valiant gave a breakthrough algorithm using fast matrix multiplication that\nruns in time $O(n^{(5-\\omega)/(4-\\omega)}) < O(n^{1.615})$, no matter how small\n$\\rho>0$ is. This was subsequently refined by Karppa, Kaski, and Kohonen in\n2016 to $O(n^{2 \\omega / 3}) < O(n^{1.582})$.\n  In this paper, we propose a new approach which can replace matrix\nmultiplication tensor with other tensors. Those tensors can omit some terms one\nis supposed to compute, and include additional error terms. Our new approach\ncan make use of any tensors which previously had no known algorithmic\napplications, including tensors which arise naturally as intermediate steps in\nborder rank methods and in the Laser method.\n  We further show that our approach can be combined with locality-sensitive\nhashing to design an algorithm whose running time improves as $\\rho$ gets\nlarger. To our knowledge, this is the first algorithm which combines fast\nmatrix multiplication with hashing for the light bulb problem or any closest\npair problem, and it leads to faster algorithms for small $\\rho>0$.\n  We also introduce a new tensor $T_{2112}$, which has the same size of $2\n\\times 2$ matrix multiplication tensor, but runs faster than the Strassen's\nalgorithm for light bulb problem.",
            "author": [
                "Josh Alman",
                "Hengjie Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01630v1",
                "http://arxiv.org/pdf/2311.01630v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01619v2",
            "title": "InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV\n  Images",
            "updated": "2023-12-04T01:08:34Z",
            "published": "2023-11-02T22:06:23Z",
            "summary": "Power line maintenance and inspection are essential to avoid power supply\ninterruptions, reducing its high social and financial impacts yearly.\nAutomating power line visual inspections remains a relevant open problem for\nthe industry due to the lack of public real-world datasets of power line\ncomponents and their various defects to foster new research. This paper\nintroduces InsPLAD, a Power Line Asset Inspection Dataset and Benchmark\ncontaining 10,607 high-resolution Unmanned Aerial Vehicles colour images. The\ndataset contains seventeen unique power line assets captured from real-world\noperating power lines. Additionally, five of those assets present six defects:\nfour of which are corrosion, one is a broken component, and one is a bird's\nnest presence. All assets were labelled according to their condition, whether\nnormal or the defect name found on an image level. We thoroughly evaluate\nstate-of-the-art and popular methods for three image-level computer vision\ntasks covered by InsPLAD: object detection, through the AP metric; defect\nclassification, through Balanced Accuracy; and anomaly detection, through the\nAUROC metric. InsPLAD offers various vision challenges from uncontrolled\nenvironments, such as multi-scale objects, multi-size class instances, multiple\nobjects per image, intra-class variation, cluttered background, distinct\npoint-of-views, perspective distortion, occlusion, and varied lighting\nconditions. To the best of our knowledge, InsPLAD is the first large real-world\ndataset and benchmark for power line asset inspection with multiple components\nand defects for various computer vision tasks, with a potential impact to\nimprove state-of-the-art methods in the field. It will be publicly available in\nits integrity on a repository with a thorough description. It can be found at\nhttps://github.com/andreluizbvs/InsPLAD.",
            "author": [
                "Andr\u00e9 Luiz Buarque Vieira e Silva",
                "Heitor de Castro Felix",
                "Franscisco Paulo Magalh\u00e3es Sim\u00f5es",
                "Veronica Teichrieb",
                "Michel Mozinho dos Santos",
                "Hemir Santiago",
                "Virginia Sgotti",
                "Henrique Lott Neto"
            ],
            "link": [
                "http://dx.doi.org/10.1080/01431161.2023.2283900",
                "http://arxiv.org/abs/2311.01619v2",
                "http://arxiv.org/pdf/2311.01619v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01597v1",
            "title": "Vertical Decomposition in 3D and 4D with Applications to Line\n  Nearest-Neighbor Searching in 3D",
            "updated": "2023-11-02T21:07:07Z",
            "published": "2023-11-02T21:07:07Z",
            "summary": "Vertical decomposition is a widely used general technique for decomposing the\ncells of arrangements of semi-algebraic sets in $d$-space into\nconstant-complexity subcells. In this paper, we settle in the affirmative a few\nlong-standing open problems involving the vertical decomposition of\nsubstructures of arrangements for $d=3,4$: (i) Let $\\mathcal{S}$ be a\ncollection of $n$ semi-algebraic sets of constant complexity in 3D, and let\n$U(m)$ be an upper bound on the complexity of the union\n$\\mathcal{U}(\\mathcal{S}')$ of any subset $\\mathcal{S}'\\subseteq \\mathcal{S}$\nof size at most $m$. We prove that the complexity of the vertical decomposition\nof the complement of $\\mathcal{U}(\\mathcal{S})$ is $O^*(n^2+U(n))$ (where the\n$O^*(\\cdot)$ notation hides subpolynomial factors). We also show that the\ncomplexity of the vertical decomposition of the entire arrangement\n$\\mathcal{A}(\\mathcal{S})$ is $O^*(n^2+X)$, where $X$ is the number of vertices\nin $\\mathcal{A}(\\mathcal{S})$. (ii) Let $\\mathcal{F}$ be a collection of $n$\ntrivariate functions whose graphs are semi-algebraic sets of constant\ncomplexity. We show that the complexity of the vertical decomposition of the\nportion of the arrangement $\\mathcal{A}(\\mathcal{F})$ in 4D lying below the\nlower envelope of $\\mathcal{F}$ is $O^*(n^3)$.\n  These results lead to efficient algorithms for a variety of problems\ninvolving these decompositions, including algorithms for constructing the\ndecompositions themselves, and for constructing $(1/r)$-cuttings of\nsubstructures of arrangements of the kinds considered above. One additional\nalgorithm of interest is for output-sensitive point enclosure queries amid\nsemi-algebraic sets in three or four dimensions. In addition, as a main domain\nof applications, we study various proximity problems involving points and lines\nin 3D.",
            "author": [
                "Pankaj K. Agarwal",
                "Esther Ezra",
                "Micha Sharir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01597v1",
                "http://arxiv.org/pdf/2311.01597v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01591v1",
            "title": "Better Fair than Sorry: Adversarial Missing Data Imputation for Fair\n  GNNs",
            "updated": "2023-11-02T20:57:44Z",
            "published": "2023-11-02T20:57:44Z",
            "summary": "This paper addresses the problem of learning fair Graph Neural Networks\n(GNNs) under missing protected attributes. GNNs have achieved state-of-the-art\nresults in many relevant tasks where decisions might disproportionately impact\nspecific communities. However, existing work on fair GNNs assumes that either\nprotected attributes are fully-observed or that the missing data imputation is\nfair. In practice, biases in the imputation will be propagated to the model\noutcomes, leading them to overestimate the fairness of their predictions. We\naddress this challenge by proposing Better Fair than Sorry (BFtS), a fair\nmissing data imputation model for protected attributes used by fair GNNs. The\nkey design principle behind BFtS is that imputations should approximate the\nworst-case scenario for the fair GNN -- i.e. when optimizing fairness is the\nhardest. We implement this idea using a 3-player adversarial scheme where two\nadversaries collaborate against the fair GNN. Experiments using synthetic and\nreal datasets show that BFtS often achieves a better fairness $\\times$ accuracy\ntrade-off than existing alternatives.",
            "author": [
                "Debolina Halder Lina",
                "Arlei Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01591v1",
                "http://arxiv.org/pdf/2311.01591v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01588v2",
            "title": "Domain Adaptive Graph Neural Networks for Constraining Cosmological\n  Parameters Across Multiple Data Sets",
            "updated": "2023-11-21T19:16:51Z",
            "published": "2023-11-02T20:40:21Z",
            "summary": "Deep learning models have been shown to outperform methods that rely on\nsummary statistics, like the power spectrum, in extracting information from\ncomplex cosmological data sets. However, due to differences in the subgrid\nphysics implementation and numerical approximations across different simulation\nsuites, models trained on data from one cosmological simulation show a drop in\nperformance when tested on another. Similarly, models trained on any of the\nsimulations would also likely experience a drop in performance when applied to\nobservational data. Training on data from two different suites of the CAMELS\nhydrodynamic cosmological simulations, we examine the generalization\ncapabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing\nGNNs, we capitalize on their capacity to capture structured scale-free\ncosmological information from galaxy distributions. Moreover, by including\nunsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable\nour models to extract domain-invariant features. We demonstrate that DA-GNN\nachieves higher accuracy and robustness on cross-dataset tasks (up to $28\\%$\nbetter relative error and up to almost an order of magnitude better $\\chi^2$).\nUsing data visualizations, we show the effects of domain adaptation on proper\nlatent space data alignment. This shows that DA-GNNs are a promising method for\nextracting domain-independent cosmological information, a vital step toward\nrobust deep learning for real cosmic survey data.",
            "author": [
                "Andrea Roncoli",
                "Aleksandra \u0106iprijanovi\u0107",
                "Maggie Voetberg",
                "Francisco Villaescusa-Navarro",
                "Brian Nord"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01588v2",
                "http://arxiv.org/pdf/2311.01588v2"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01582v1",
            "title": "The $k$-visibility Localization Game",
            "updated": "2023-11-02T20:27:16Z",
            "published": "2023-11-02T20:27:16Z",
            "summary": "We study a variant of the Localization game in which the cops have limited\nvisibility, along with the corresponding optimization parameter, the\n$k$-visibility localization number $\\zeta_k$, where $k$ is a non-negative\ninteger. We give bounds on $k$-visibility localization numbers related to\ndomination, maximum degree, and isoperimetric inequalities. For all $k$, we\ngive a family of trees with unbounded $\\zeta_k$ values. Extending results known\nfor the localization number, we show that for $k\\geq 2$, every tree contains a\nsubdivision with $\\zeta_k = 1$. For many $n$, we give the exact value of\n$\\zeta_k$ for the $n \\times n$ Cartesian grid graphs, with the remaining cases\nbeing one of two values as long as $n$ is sufficiently large. These examples\nalso illustrate that $\\zeta_i \\neq \\zeta_j$ for all distinct choices of $i$ and\n$j.$",
            "author": [
                "Anthony Bonato",
                "Trent G. Marbach",
                "John Marcoux",
                "JD Nir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01582v1",
                "http://arxiv.org/pdf/2311.01582v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01579v1",
            "title": "Generalized regular Tur\u00e1n numbers",
            "updated": "2023-11-02T20:17:03Z",
            "published": "2023-11-02T20:17:03Z",
            "summary": "We combine two generalizations of ordinary Tur\\'an problems. Given graphs $H$\nand $F$ and a positive integer $n$, we study $rex(n, H, F )$, which is the\nlargest number of copies of $H$ in $F$-free regular $n$-vertex graphs.",
            "author": [
                "D\u00e1niel Gerbner",
                "Hilal Hama Karim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01579v1",
                "http://arxiv.org/pdf/2311.01579v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01573v1",
            "title": "Improving Fairness using Vision-Language Driven Image Augmentation",
            "updated": "2023-11-02T19:51:10Z",
            "published": "2023-11-02T19:51:10Z",
            "summary": "Fairness is crucial when training a deep-learning discriminative model,\nespecially in the facial domain. Models tend to correlate specific\ncharacteristics (such as age and skin color) with unrelated attributes\n(downstream tasks), resulting in biases which do not correspond to reality. It\nis common knowledge that these correlations are present in the data and are\nthen transferred to the models during training. This paper proposes a method to\nmitigate these correlations to improve fairness. To do so, we learn\ninterpretable and meaningful paths lying in the semantic space of a pre-trained\ndiffusion model (DiffAE) -- such paths being supervised by contrastive text\ndipoles. That is, we learn to edit protected characteristics (age and skin\ncolor). These paths are then applied to augment images to improve the fairness\nof a given dataset. We test the proposed method on CelebA-HQ and UTKFace on\nseveral downstream tasks with age and skin color as protected characteristics.\nAs a proxy for fairness, we compute the difference in accuracy with respect to\nthe protected characteristics. Quantitative results show how the augmented\nimages help the model improve the overall accuracy, the aforementioned metric,\nand the disparity of equal opportunity. Code is available at:\nhttps://github.com/Moreno98/Vision-Language-Bias-Control.",
            "author": [
                "Moreno D'Inc\u00e0",
                "Christos Tzelepis",
                "Ioannis Patras",
                "Nicu Sebe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01573v1",
                "http://arxiv.org/pdf/2311.01573v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01571v1",
            "title": "Preserving the knowledge of long clinical texts using aggregated\n  ensembles of large language models",
            "updated": "2023-11-02T19:50:02Z",
            "published": "2023-11-02T19:50:02Z",
            "summary": "Clinical texts, such as admission notes, discharge summaries, and progress\nnotes, contain rich and valuable information that can be used for various\nclinical outcome prediction tasks. However, applying large language models,\nsuch as BERT-based models, to clinical texts poses two major challenges: the\nlimitation of input length and the diversity of data sources. This paper\nproposes a novel method to preserve the knowledge of long clinical texts using\naggregated ensembles of large language models. Unlike previous studies which\nuse model ensembling or text aggregation methods separately, we combine\nensemble learning with text aggregation and train multiple large language\nmodels on two clinical outcome tasks: mortality prediction and length of stay\nprediction. We show that our method can achieve better results than baselines,\nensembling, and aggregation individually, and can improve the performance of\nlarge language models while handling long inputs and diverse datasets. We\nconduct extensive experiments on the admission notes from the MIMIC-III\nclinical database by combining multiple unstructured and high-dimensional\ndatasets, demonstrating our method's effectiveness and superiority over\nexisting approaches. We also provide a comprehensive analysis and discussion of\nour results, highlighting our method's applications and limitations for future\nresearch in the domain of clinical healthcare. The results and analysis of this\nstudy is supportive of our method assisting in clinical healthcare systems by\nenabling clinical decision-making with robust performance overcoming the\nchallenges of long text inputs and varied datasets.",
            "author": [
                "Mohammad Junayed Hasan",
                "Suhra Noor",
                "Mohammad Ashrafuzzaman Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01571v1",
                "http://arxiv.org/pdf/2311.01571v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01570v1",
            "title": "Sequential Subset Matching for Dataset Distillation",
            "updated": "2023-11-02T19:49:11Z",
            "published": "2023-11-02T19:49:11Z",
            "summary": "Dataset distillation is a newly emerging task that synthesizes a small-size\ndataset used in training deep neural networks (DNNs) for reducing data storage\nand model training costs. The synthetic datasets are expected to capture the\nessence of the knowledge contained in real-world datasets such that the former\nyields a similar performance as the latter. Recent advancements in distillation\nmethods have produced notable improvements in generating synthetic datasets.\nHowever, current state-of-the-art methods treat the entire synthetic dataset as\na unified entity and optimize each synthetic instance equally. This static\noptimization approach may lead to performance degradation in dataset\ndistillation. Specifically, we argue that static optimization can give rise to\na coupling issue within the synthetic data, particularly when a larger amount\nof synthetic data is being optimized. This coupling issue, in turn, leads to\nthe failure of the distilled dataset to extract the high-level features learned\nby the deep neural network (DNN) in the latter epochs.\n  In this study, we propose a new dataset distillation strategy called\nSequential Subset Matching (SeqMatch), which tackles this problem by adaptively\noptimizing the synthetic data to encourage sequential acquisition of knowledge\nduring dataset distillation. Our analysis indicates that SeqMatch effectively\naddresses the coupling issue by sequentially generating the synthetic\ninstances, thereby enhancing its performance significantly. Our proposed\nSeqMatch outperforms state-of-the-art methods in various datasets, including\nSVNH, CIFAR-10, CIFAR-100, and Tiny ImageNet. Our code is available at\nhttps://github.com/shqii1j/seqmatch.",
            "author": [
                "Jiawei Du",
                "Qin Shi",
                "Joey Tianyi Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01570v1",
                "http://arxiv.org/pdf/2311.01570v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01540v1",
            "title": "Open-Set Object Recognition Using Mechanical Properties During\n  Interaction",
            "updated": "2023-11-02T18:50:10Z",
            "published": "2023-11-02T18:50:10Z",
            "summary": "while most of the tactile robots are operated in close-set conditions, it is\nchallenging for them to operate in open-set conditions where test objects are\nbeyond the robots' knowledge. We proposed an open-set recognition framework\nusing mechanical properties to recongise known objects and incrementally label\nnovel objects. The main contribution is a clustering algorithm that exploits\nknowledge of known objects to estimate cluster centre and sizes, unlike a\ntypical algorithm that randomly selects them. The framework is validated with\nthe mechanical properties estimated from a real object during interaction. The\nresults show that the framework could recognise objects better than alternative\nmethods contributed by the novelty detector. Importantly, our clustering\nalgorithm yields better clustering performance than other methods. Furthermore,\nthe hyperparameters studies show that cluster size is important to clustering\nresults and needed to be tuned properly.",
            "author": [
                "Pakorn Uttayopas",
                "Xiaoxiao Cheng",
                "Etienne Burdet"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01540v1",
                "http://arxiv.org/pdf/2311.01540v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01534v1",
            "title": "Approximate Multiagent Reinforcement Learning for On-Demand Urban\n  Mobility Problem on a Large Map (extended version)",
            "updated": "2023-11-02T18:33:32Z",
            "published": "2023-11-02T18:33:32Z",
            "summary": "In this paper, we focus on the autonomous multiagent taxi routing problem for\na large urban environment where the location and number of future ride requests\nare unknown a-priori, but follow an estimated empirical distribution. Recent\ntheory has shown that if a base policy is stable then a rollout-based algorithm\nwith such a base policy produces a near-optimal stable policy. Although,\nrollout-based approaches are well-suited for learning cooperative multiagent\npolicies with considerations for future demand, applying such methods to a\nlarge urban environment can be computationally expensive. Large environments\ntend to have a large volume of requests, and hence require a large fleet of\ntaxis to guarantee stability. In this paper, we aim to address the\ncomputational bottleneck of multiagent (one-at-a-time) rollout, where the\ncomputational complexity grows linearly in the number of agents. We propose an\napproximate one-at-a-time rollout-based two-phase algorithm that reduces the\ncomputational cost, while still achieving a stable near-optimal policy. Our\napproach partitions the graph into sectors based on the predicted demand and an\nuser-defined maximum number of agents that can be planned for using the\none-at-a-time rollout approach. The algorithm then applies instantaneous\nassignment (IA) for re-balancing taxis across sectors and a sector-wide\none-at-a-time rollout algorithm that is executed in parallel for each sector.\nWe characterize the number of taxis $m$ that is sufficient for IA base policy\nto be stable, and derive a necessary condition on $m$ as time goes to infinity.\nOur numerical results show that our approach achieves stability for an $m$ that\nsatisfies the theoretical conditions. We also empirically demonstrate that our\nproposed two-phase algorithm has comparable performance to the one-at-a-time\nrollout over the entire map, but with significantly lower runtimes.",
            "author": [
                "Daniel Garces",
                "Sushmita Bhattacharya",
                "Dimitri Bertsekas",
                "Stephanie Gil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01534v1",
                "http://arxiv.org/pdf/2311.01534v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01529v1",
            "title": "The two upper critical dimensions of the Ising and Potts models",
            "updated": "2023-11-02T18:25:17Z",
            "published": "2023-11-02T18:25:17Z",
            "summary": "We derive the exact actions of the $Q$-state Potts model valid on any graph,\nfirst for the spin degrees of freedom, and second for the Fortuin-Kasteleyn\nclusters. In both cases the field is a traceless $Q$-component scalar field\n$\\Phi^\\alpha$. For the Ising model ($Q=2$), the field theory for the spins has\nupper critical dimension $d_{\\rm c}^{\\rm spin}=4$, whereas for the clusters it\nhas $d_{\\rm c}^{\\rm cluster}=6$. As a consequence, the probability for three\npoints to be in the same cluster is not given by mean-field theory theory for\n$d$ within $4<d<6$. We estimate the associated universal structure constant as\n$C=\\sqrt{6-d}+ {\\cal O}(6-d)^{3/2}$. This shows that some observables in the\nIsing model have an upper critical dimension of 4, while others have an upper\ncritical dimension of $6$. Combining perturbative results from the\n$\\epsilon=6-d$ expansion with a non-perturbative treatment close to dimension\n$d=4$ allows us to locate the shape of the critical domain of the Potts model\nin the whole $(Q,d)$ plane.",
            "author": [
                "Kay Joerg Wiese",
                "Jesper Lykke Jacobsen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01529v1",
                "http://arxiv.org/pdf/2311.01529v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01526v1",
            "title": "ATGNN: Audio Tagging Graph Neural Network",
            "updated": "2023-11-02T18:19:26Z",
            "published": "2023-11-02T18:19:26Z",
            "summary": "Deep learning models such as CNNs and Transformers have achieved impressive\nperformance for end-to-end audio tagging. Recent works have shown that despite\nstacking multiple layers, the receptive field of CNNs remains severely limited.\nTransformers on the other hand are able to map global context through\nself-attention, but treat the spectrogram as a sequence of patches which is not\nflexible enough to capture irregular audio objects. In this work, we treat the\nspectrogram in a more flexible way by considering it as graph structure and\nprocess it with a novel graph neural architecture called ATGNN. ATGNN not only\ncombines the capability of CNNs with the global information sharing ability of\nGraph Neural Networks, but also maps semantic relationships between learnable\nclass embeddings and corresponding spectrogram regions. We evaluate ATGNN on\ntwo audio tagging tasks, where it achieves 0.585 mAP on the FSD50K dataset and\n0.335 mAP on the AudioSet-balanced dataset, achieving comparable results to\nTransformer based models with significantly lower number of learnable\nparameters.",
            "author": [
                "Shubhr Singh",
                "Christian J. Steinmetz",
                "Emmanouil Benetos",
                "Huy Phan",
                "Dan Stowell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01526v1",
                "http://arxiv.org/pdf/2311.01526v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01522v2",
            "title": "An Efficient Detection and Control System for Underwater Docking using\n  Machine Learning and Realistic Simulation: A Comprehensive Approach",
            "updated": "2023-11-06T19:34:05Z",
            "published": "2023-11-02T18:10:20Z",
            "summary": "Underwater docking is critical to enable the persistent operation of\nAutonomous Underwater Vehicles (AUVs). For this, the AUV must be capable of\ndetecting and localizing the docking station, which is complex due to the\nhighly dynamic undersea environment. Image-based solutions offer a high\nacquisition rate and versatile alternative to adapt to this environment;\nhowever, the underwater environment presents challenges such as low visibility,\nhigh turbidity, and distortion. In addition to this, field experiments to\nvalidate underwater docking capabilities can be costly and dangerous due to the\nspecialized equipment and safety considerations required to conduct the\nexperiments. This work compares different deep-learning architectures to\nperform underwater docking detection and classification. The architecture with\nthe best performance is then compressed using knowledge distillation under the\nteacher-student paradigm to reduce the network's memory footprint, allowing\nreal-time implementation. To reduce the simulation-to-reality gap, a Generative\nAdversarial Network (GAN) is used to do image-to-image translation, converting\nthe Gazebo simulation image into a realistic underwater-looking image. The\nobtained image is then processed using an underwater image formation model to\nsimulate image attenuation over distance under different water types. The\nproposed method is finally evaluated according to the AUV docking success rate\nand compared with classical vision methods. The simulation results show an\nimprovement of 20% in the high turbidity scenarios regardless of the underwater\ncurrents. Furthermore, we show the performance of the proposed approach by\nshowing experimental results on the off-the-shelf AUV Iver3.",
            "author": [
                "Jalil Chavez-Galaviz",
                "Jianwen Li",
                "Matthew Bergman",
                "Miras Mengdibayev",
                "Nina Mahmoudian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01522v2",
                "http://arxiv.org/pdf/2311.01522v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01511v1",
            "title": "Dispersion, Capacitated Nodes, and the Power of a Trusted Shepherd",
            "updated": "2023-11-02T18:00:23Z",
            "published": "2023-11-02T18:00:23Z",
            "summary": "In this paper, we look at and expand the problems of dispersion and Byzantine\ndispersion of mobile robots on a graph, introduced by Augustine and\nMoses~Jr.~[ICDCN~2018] and by Molla, Mondal, and Moses~Jr.~[ALGOSENSORS~2020],\nrespectively, to graphs where nodes have variable capacities. We use the idea\nof a single shepherd, a more powerful robot that will never act in a Byzantine\nmanner, to achieve fast Byzantine dispersion, even when other robots may be\nstrong Byzantine in nature. We also show the benefit of a shepherd for\ndispersion on capacitated graphs when no Byzantine robots are present.",
            "author": [
                "William K. Moses Jr.",
                "Amanda Redlich"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01511v1",
                "http://arxiv.org/pdf/2311.01511v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "F.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01460v1",
            "title": "Implicit Chain of Thought Reasoning via Knowledge Distillation",
            "updated": "2023-11-02T17:59:49Z",
            "published": "2023-11-02T17:59:49Z",
            "summary": "To augment language models with the ability to reason, researchers usually\nprompt or finetune them to produce chain of thought reasoning steps before\nproducing the final answer. However, although people use natural language to\nreason effectively, it may be that LMs could reason more effectively with some\nintermediate computation that is not in natural language. In this work, we\nexplore an alternative reasoning approach: instead of explicitly producing the\nchain of thought reasoning steps, we use the language model's internal hidden\nstates to perform implicit reasoning. The implicit reasoning steps are\ndistilled from a teacher model trained on explicit chain-of-thought reasoning,\nand instead of doing reasoning \"horizontally\" by producing intermediate words\none-by-one, we distill it such that the reasoning happens \"vertically\" among\nthe hidden states in different layers. We conduct experiments on a multi-digit\nmultiplication task and a grade school math problem dataset and find that this\napproach enables solving tasks previously not solvable without explicit\nchain-of-thought, at a speed comparable to no chain-of-thought.",
            "author": [
                "Yuntian Deng",
                "Kiran Prasad",
                "Roland Fernandez",
                "Paul Smolensky",
                "Vishrav Chaudhary",
                "Stuart Shieber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01460v1",
                "http://arxiv.org/pdf/2311.01460v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01455v2",
            "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning\n  via Generative Simulation",
            "updated": "2023-11-13T18:40:10Z",
            "published": "2023-11-02T17:59:21Z",
            "summary": "We present RoboGen, a generative robotic agent that automatically learns\ndiverse robotic skills at scale via generative simulation. RoboGen leverages\nthe latest advancements in foundation and generative models. Instead of\ndirectly using or adapting these models to produce policies or low-level\nactions, we advocate for a generative scheme, which uses these models to\nautomatically generate diversified tasks, scenes, and training supervisions,\nthereby scaling up robotic skill learning with minimal human supervision. Our\napproach equips a robotic agent with a self-guided propose-generate-learn\ncycle: the agent first proposes interesting tasks and skills to develop, and\nthen generates corresponding simulation environments by populating pertinent\nobjects and assets with proper spatial configurations. Afterwards, the agent\ndecomposes the proposed high-level task into sub-tasks, selects the optimal\nlearning approach (reinforcement learning, motion planning, or trajectory\noptimization), generates required training supervision, and then learns\npolicies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried\nrepeatedly, producing an endless stream of skill demonstrations associated with\ndiverse tasks and environments.",
            "author": [
                "Yufei Wang",
                "Zhou Xian",
                "Feng Chen",
                "Tsun-Hsuan Wang",
                "Yian Wang",
                "Zackory Erickson",
                "David Held",
                "Chuang Gan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01455v2",
                "http://arxiv.org/pdf/2311.01455v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01441v1",
            "title": "Distilling Out-of-Distribution Robustness from Vision-Language\n  Foundation Models",
            "updated": "2023-11-02T17:55:13Z",
            "published": "2023-11-02T17:55:13Z",
            "summary": "We propose a conceptually simple and lightweight framework for improving the\nrobustness of vision models through the combination of knowledge distillation\nand data augmentation. We address the conjecture that larger models do not make\nfor better teachers by showing strong gains in out-of-distribution robustness\nwhen distilling from pretrained foundation models. Following this finding, we\npropose Discrete Adversarial Distillation (DAD), which leverages a robust\nteacher to generate adversarial examples and a VQGAN to discretize them,\ncreating more informative samples than standard data augmentation techniques.\nWe provide a theoretical framework for the use of a robust teacher in the\nknowledge distillation with data augmentation setting and demonstrate strong\ngains in out-of-distribution robustness and clean accuracy across different\nstudent architectures. Notably, our method adds minor computational overhead\ncompared to similar techniques and can be easily combined with other data\naugmentations for further improvements.",
            "author": [
                "Andy Zhou",
                "Jindong Wang",
                "Yu-Xiong Wang",
                "Haohan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01441v1",
                "http://arxiv.org/pdf/2311.01441v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01437v1",
            "title": "Checkerboard CFT",
            "updated": "2023-11-02T17:52:12Z",
            "published": "2023-11-02T17:52:12Z",
            "summary": "The Checkerboard conformal field theory is an interesting representative of a\nlarge class of non-unitary, logarithmic Fishnet CFTs (FCFT) in arbitrary\ndimension which have been intensively studied in the last years. Its planar\nFeynman graphs have the structure of a regular square lattice with checkerboard\ncolouring. Such graphs are integrable since each coloured cell of the lattice\nis equal to an R-matrix in the principal series representations of the\nconformal group. We compute perturbatively and numerically the anomalous\ndimension of the shortest single-trace operator in two reductions of the\nCheckerboard CFT: the first one corresponds to the fishnet limit of the twisted\nABJM theory in 3D, whereas the spectrum in the second, 2D reduction contains\nthe energy of the BFKL Pomeron. We derive an analytic expression for the\nCheckerboard analogues of Basso--Dixon 4-point functions, as well as for the\nclass of Diamond-type 4-point graphs with disc topology. The properties of the\nlatter are studied in terms of OPE for operators with open indices. We prove\nthat the spectrum of the theory receives corrections only at even orders in the\nloop expansion and we conjecture such a modification of Checkerboard CFT where\nquantum corrections occur only with a given periodicity in the loop order.",
            "author": [
                "Mikhail Alfimov",
                "Gwena\u00ebl Ferrando",
                "Vladimir Kazakov",
                "Enrico Olivucci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01437v1",
                "http://arxiv.org/pdf/2311.01437v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01412v1",
            "title": "Castor: Causal Temporal Regime Structure Learning",
            "updated": "2023-11-02T17:26:49Z",
            "published": "2023-11-02T17:26:49Z",
            "summary": "The task of uncovering causal relationships among multivariate time series\ndata stands as an essential and challenging objective that cuts across a broad\narray of disciplines ranging from climate science to healthcare. Such data\nentails linear or non-linear relationships, and usually follow multiple a\npriori unknown regimes. Existing causal discovery methods can infer summary\ncausal graphs from heterogeneous data with known regimes, but they fall short\nin comprehensively learning both regimes and the corresponding causal graph. In\nthis paper, we introduce CASTOR, a novel framework designed to learn causal\nrelationships in heterogeneous time series data composed of various regimes,\neach governed by a distinct causal graph. Through the maximization of a score\nfunction via the EM algorithm, CASTOR infers the number of regimes and learns\nlinear or non-linear causal relationships in each regime. We demonstrate the\nrobust convergence properties of CASTOR, specifically highlighting its\nproficiency in accurately identifying unique regimes. Empirical evidence,\ngarnered from exhaustive synthetic experiments and two real-world benchmarks,\nconfirm CASTOR's superior performance in causal discovery compared to baseline\nmethods. By learning a full temporal causal graph for each regime, CASTOR\nestablishes itself as a distinctly interpretable method for causal discovery in\nheterogeneous time series.",
            "author": [
                "Abdellah Rahmani",
                "Pascal Frossard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01412v1",
                "http://arxiv.org/pdf/2311.01412v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01406v1",
            "title": "Analysis of Information Propagation in Ethereum Network Using Combined\n  Graph Attention Network and Reinforcement Learning to Optimize Network\n  Efficiency and Scalability",
            "updated": "2023-11-02T17:19:45Z",
            "published": "2023-11-02T17:19:45Z",
            "summary": "Blockchain technology has revolutionized the way information is propagated in\ndecentralized networks. Ethereum plays a pivotal role in facilitating smart\ncontracts and decentralized applications. Understanding information propagation\ndynamics in Ethereum is crucial for ensuring network efficiency, security, and\nscalability. In this study, we propose an innovative approach that utilizes\nGraph Convolutional Networks (GCNs) to analyze the information propagation\npatterns in the Ethereum network. The first phase of our research involves data\ncollection from the Ethereum blockchain, consisting of blocks, transactions,\nand node degrees. We construct a transaction graph representation using\nadjacency matrices to capture the node embeddings; while our major contribution\nis to develop a combined Graph Attention Network (GAT) and Reinforcement\nLearning (RL) model to optimize the network efficiency and scalability. It\nlearns the best actions to take in various network states, ultimately leading\nto improved network efficiency, throughput, and optimize gas limits for block\nprocessing. In the experimental evaluation, we analyze the performance of our\nmodel on a large-scale Ethereum dataset. We investigate effectively aggregating\ninformation from neighboring nodes capturing graph structure and updating node\nembeddings using GCN with the objective of transaction pattern prediction,\naccounting for varying network loads and number of blocks. Not only we design a\ngas limit optimization model and provide the algorithm, but also to address\nscalability, we demonstrate the use and implementation of sparse matrices in\nGraphConv, GraphSAGE, and GAT. The results indicate that our designed GAT-RL\nmodel achieves superior results compared to other GCN models in terms of\nperformance. It effectively propagates information across the network,\noptimizing gas limits for block processing and improving network efficiency.",
            "author": [
                "Stefan Kambiz Behfar",
                "Jon Crowcroft"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01406v1",
                "http://arxiv.org/pdf/2311.01406v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01405v1",
            "title": "Learning to See Physical Properties with Active Sensing Motor Policies",
            "updated": "2023-11-02T17:19:18Z",
            "published": "2023-11-02T17:19:18Z",
            "summary": "Knowledge of terrain's physical properties inferred from color images can aid\nin making efficient robotic locomotion plans. However, unlike image\nclassification, it is unintuitive for humans to label image patches with\nphysical properties. Without labeled data, building a vision system that takes\nas input the observed terrain and predicts physical properties remains\nchallenging. We present a method that overcomes this challenge by\nself-supervised labeling of images captured by robots during real-world\ntraversal with physical property estimators trained in simulation. To ensure\naccurate labeling, we introduce Active Sensing Motor Policies (ASMP), which are\ntrained to explore locomotion behaviors that increase the accuracy of\nestimating physical parameters. For instance, the quadruped robot learns to\nswipe its foot against the ground to estimate the friction coefficient\naccurately. We show that the visual system trained with a small amount of\nreal-world traversal data accurately predicts physical parameters. The trained\nsystem is robust and works even with overhead images captured by a drone\ndespite being trained on data collected by cameras attached to a quadruped\nrobot walking on the ground.",
            "author": [
                "Gabriel B. Margolis",
                "Xiang Fu",
                "Yandong Ji",
                "Pulkit Agrawal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01405v1",
                "http://arxiv.org/pdf/2311.01405v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01403v1",
            "title": "REAL: Resilience and Adaptation using Large Language Models on\n  Autonomous Aerial Robots",
            "updated": "2023-11-02T17:16:21Z",
            "published": "2023-11-02T17:16:21Z",
            "summary": "Large Language Models (LLMs) pre-trained on internet-scale datasets have\nshown impressive capabilities in code understanding, synthesis, and general\npurpose question-and-answering. Key to their performance is the substantial\nprior knowledge acquired during training and their ability to reason over\nextended sequences of symbols, often presented in natural language. In this\nwork, we aim to harness the extensive long-term reasoning, natural language\ncomprehension, and the available prior knowledge of LLMs for increased\nresilience and adaptation in autonomous mobile robots. We introduce REAL, an\napproach for REsilience and Adaptation using LLMs. REAL provides a strategy to\nemploy LLMs as a part of the mission planning and control framework of an\nautonomous robot. The LLM employed by REAL provides (i) a source of prior\nknowledge to increase resilience for challenging scenarios that the system had\nnot been explicitly designed for; (ii) a way to interpret natural-language and\nother log/diagnostic information available in the autonomy stack, for mission\nplanning; (iii) a way to adapt the control inputs using minimal user-provided\nprior knowledge about the dynamics/kinematics of the robot. We integrate REAL\nin the autonomy stack of a real multirotor, querying onboard an offboard LLM at\n0.1-1.0 Hz as part the robot's mission planning and control feedback loops. We\ndemonstrate in real-world experiments the ability of the LLM to reduce the\nposition tracking errors of a multirotor under the presence of (i) errors in\nthe parameters of the controller and (ii) unmodeled dynamics. We also show\n(iii) decision making to avoid potentially dangerous scenarios (e.g., robot\noscillates) that had not been explicitly accounted for in the initial prompt\ndesign.",
            "author": [
                "Andrea Tagliabue",
                "Kota Kondo",
                "Tong Zhao",
                "Mason Peterson",
                "Claudius T. Tewari",
                "Jonathan P. How"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01403v1",
                "http://arxiv.org/pdf/2311.01403v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01398v1",
            "title": "Server-side Rescoring of Spoken Entity-centric Knowledge Queries for\n  Virtual Assistants",
            "updated": "2023-11-02T17:07:23Z",
            "published": "2023-11-02T17:07:23Z",
            "summary": "On-device Virtual Assistants (VAs) powered by Automatic Speech Recognition\n(ASR) require effective knowledge integration for the challenging entity-rich\nquery recognition. In this paper, we conduct an empirical study of modeling\nstrategies for server-side rescoring of spoken information domain queries using\nvarious categories of Language Models (LMs) (N-gram word LMs, sub-word neural\nLMs). We investigate the combination of on-device and server-side signals, and\ndemonstrate significant WER improvements of 23%-35% on various entity-centric\nquery subpopulations by integrating various server-side LMs compared to\nperforming ASR on-device only. We also perform a comparison between LMs trained\non domain data and a GPT-3 variant offered by OpenAI as a baseline.\nFurthermore, we also show that model fusion of multiple server-side LMs trained\nfrom scratch most effectively combines complementary strengths of each model\nand integrates knowledge learned from domain-specific data to a VA ASR system.",
            "author": [
                "Youyuan Zhang",
                "Sashank Gondala",
                "Thiago Fraga-Silva",
                "Christophe Van Gysel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01398v1",
                "http://arxiv.org/pdf/2311.01398v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01373v1",
            "title": "Recognize Any Regions",
            "updated": "2023-11-02T16:31:49Z",
            "published": "2023-11-02T16:31:49Z",
            "summary": "Understanding the semantics of individual regions or patches within\nunconstrained images, such as in open-world object detection, represents a\ncritical yet challenging task in computer vision. Building on the success of\npowerful image-level vision-language (ViL) foundation models like CLIP, recent\nefforts have sought to harness their capabilities by either training a\ncontrastive model from scratch with an extensive collection of region-label\npairs or aligning the outputs of a detection model with image-level\nrepresentations of region proposals. Despite notable progress, these approaches\nare plagued by computationally intensive training requirements, susceptibility\nto data noise, and deficiency in contextual information. To address these\nlimitations, we explore the synergistic potential of off-the-shelf foundation\nmodels, leveraging their respective strengths in localization and semantics. We\nintroduce a novel, generic, and efficient region recognition architecture,\nnamed RegionSpot, designed to integrate position-aware localization knowledge\nfrom a localization foundation model (e.g., SAM) with semantic information\nextracted from a ViL model (e.g., CLIP). To fully exploit pretrained knowledge\nwhile minimizing training overhead, we keep both foundation models frozen,\nfocusing optimization efforts solely on a lightweight attention-based knowledge\nintegration module. Through extensive experiments in the context of open-world\nobject recognition, our RegionSpot demonstrates significant performance\nimprovements over prior alternatives, while also providing substantial\ncomputational savings. For instance, training our model with 3 million data in\na single day using 8 V100 GPUs. Our model outperforms GLIP by 6.5 % in mean\naverage precision (mAP), with an even larger margin by 14.8 % for more\nchallenging and rare categories.",
            "author": [
                "Haosen Yang",
                "Chuofan Ma",
                "Bin Wen",
                "Yi Jiang",
                "Zehuan Yuan",
                "Xiatian Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01373v1",
                "http://arxiv.org/pdf/2311.01373v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01372v2",
            "title": "Data-Augmented and Retrieval-Augmented Context Enrichment in Chinese\n  Media Bias Detection",
            "updated": "2023-11-18T09:45:01Z",
            "published": "2023-11-02T16:29:49Z",
            "summary": "With the increasing pursuit of objective reports, automatically understanding\nmedia bias has drawn more attention in recent research. However, most of the\nprevious work examines media bias from Western ideology, such as the left and\nright in the political spectrum, which is not applicable to Chinese outlets.\nBased on the previous lexical bias and informational bias structure, we refine\nit from the Chinese perspective and go one step further to craft data with 7\nfine-grained labels. To be specific, we first construct a dataset with Chinese\nnews reports about COVID-19 which is annotated by our newly designed system,\nand then conduct substantial experiments on it to detect media bias. However,\nthe scale of the annotated data is not enough for the latest deep-learning\ntechnology, and the cost of human annotation in media bias, which needs a lot\nof professional knowledge, is too expensive. Thus, we explore some context\nenrichment methods to automatically improve these problems. In Data-Augmented\nContext Enrichment (DACE), we enlarge the training data; while in\nRetrieval-Augmented Context Enrichment (RACE), we improve information retrieval\nmethods to select valuable information and integrate it into our models to\nbetter understand bias. Extensive experiments are conducted on both our dataset\nand an English dataset BASIL. Our results show that both methods outperform our\nbaselines, while the RACE methods are more efficient and have more potential.",
            "author": [
                "Luyang Lin",
                "Jing Li",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01372v2",
                "http://arxiv.org/pdf/2311.01372v2"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01363v1",
            "title": "Variational Methods for Computing Non-Local Quantum Strategies",
            "updated": "2023-11-02T16:17:18Z",
            "published": "2023-11-02T16:17:18Z",
            "summary": "In a nonlocal game, two noncommunicating players cooperate to convince a\nreferee that they possess a strategy that does not violate the rules of the\ngame. Quantum strategies enable players to optimally win some games by\nperforming joint measurements on a shared entangled state, but computing these\nstrategies can be challenging. We develop a variational algorithm for computing\nstrategies of nonlocal games and show that it can yield optimal strategies for\nsmall examples of both convex and non-convex games. We show that our algorithm\nreturns an optimal quantum strategy for a graph coloring game; whereas no\noptimal quantum strategy was previously known for this problem. Moreover, we\ndescribe how this technique can be run on quantum computers to discover\nshallow-depth circuits that yield optimal quantum strategies. We argue that\nsuch circuits will be useful for benchmarking quantum computers because of the\nability to verify the solutions at scale and the experiment's sensitivity to\n2-qubit gate noise. Finally, we demonstrate the use of nonlocal games as a\nbenchmarking strategy experimentally on 11 IBM quantum computers.",
            "author": [
                "Jim Furches",
                "Nathan Wiebe",
                "Carlos Ortiz Marrero"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01363v1",
                "http://arxiv.org/pdf/2311.01363v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04228v1",
            "title": "Graph Neural Networks for Topological Feature Extraction in ECG\n  Classification",
            "updated": "2023-11-02T16:14:34Z",
            "published": "2023-11-02T16:14:34Z",
            "summary": "The electrocardiogram (ECG) is a dependable instrument for assessing the\nfunction of the cardiovascular system. There has recently been much emphasis on\nprecisely classifying ECGs. While ECG situations have numerous similarities,\nlittle attention has been paid to categorizing ECGs using graph neural\nnetworks. In this study, we offer three distinct techniques for classifying\nheartbeats using deep graph neural networks to classify the ECG signals\naccurately. We suggest using different methods to extract topological features\nfrom the ECG signal and then using a branch of the graph neural network named\ngraph isomorphism network for classifying the ECGs. On the PTB Diagnostics data\nset, we tested the three proposed techniques. According to the findings, the\nthree proposed techniques are capable of making arrhythmia classification\npredictions with the accuracy of 99.38, 98.76, and 91.93 percent, respectively.",
            "author": [
                "Kamyar Zeinalipour",
                "Marco Gori"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-981-99-3592-5_2",
                "http://arxiv.org/abs/2311.04228v1",
                "http://arxiv.org/pdf/2311.04228v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01488v2",
            "title": "A note on universal graphs for spanning trees",
            "updated": "2023-11-06T16:59:16Z",
            "published": "2023-11-02T16:07:13Z",
            "summary": "Chung and Graham considered the problem of minimizing the number of edges in\nan $n$-vertex graph containing all $n$-vertex trees as a subgraph. They showed\nthat such a graph has at least $\\frac{1}{2}n \\log{n}$ edges. In this note, we\nimprove this lower estimate to $n \\log{n}$.",
            "author": [
                "Ervin Gy\u0151ri",
                "Binlong Li",
                "Nika Salia",
                "Casey Tompkins"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01488v2",
                "http://arxiv.org/pdf/2311.01488v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01344v1",
            "title": "Like an Open Book? Read Neural Network Architecture with Simple Power\n  Analysis on 32-bit Microcontrollers",
            "updated": "2023-11-02T15:55:20Z",
            "published": "2023-11-02T15:55:20Z",
            "summary": "Model extraction is a growing concern for the security of AI systems. For\ndeep neural network models, the architecture is the most important information\nan adversary aims to recover. Being a sequence of repeated computation blocks,\nneural network models deployed on edge-devices will generate distinctive\nside-channel leakages. The latter can be exploited to extract critical\ninformation when targeted platforms are physically accessible. By combining\ntheoretical knowledge about deep learning practices and analysis of a\nwidespread implementation library (ARM CMSIS-NN), our purpose is to answer this\ncritical question: how far can we extract architecture information by simply\nexamining an EM side-channel trace? For the first time, we propose an\nextraction methodology for traditional MLP and CNN models running on a high-end\n32-bit microcontroller (Cortex-M7) that relies only on simple pattern\nrecognition analysis. Despite few challenging cases, we claim that, contrary to\nparameters extraction, the complexity of the attack is relatively low and we\nhighlight the urgent need for practicable protections that could fit the strong\nmemory and latency requirements of such platforms.",
            "author": [
                "Raphael Joud",
                "Pierre-Alain Moellic",
                "Simon Pontie",
                "Jean-Baptiste Rigaud"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01344v1",
                "http://arxiv.org/pdf/2311.01344v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01343v3",
            "title": "Collaborative Large Language Model for Recommender Systems",
            "updated": "2023-11-08T05:02:06Z",
            "published": "2023-11-02T15:52:35Z",
            "summary": "Recently, there is a growing interest in developing next-generation\nrecommender systems (RSs) based on pretrained large language models (LLMs),\nfully utilizing their encoded knowledge and reasoning ability. However, the\nsemantic gap between natural language and recommendation tasks is still not\nwell addressed, leading to multiple issues such as spuriously-correlated\nuser/item descriptors, ineffective language modeling on user/item contents, and\ninefficient recommendations via auto-regression, etc. In this paper, we propose\nCLLM4Rec, the first generative RS that tightly integrates the LLM paradigm and\nID paradigm of RS, aiming to address the above challenges simultaneously. We\nfirst extend the vocabulary of pretrained LLMs with user/item ID tokens to\nfaithfully model the user/item collaborative and content semantics.\nAccordingly, in the pretraining stage, a novel soft+hard prompting strategy is\nproposed to effectively learn user/item collaborative/content token embeddings\nvia language modeling on RS-specific corpora established from user-item\ninteractions and user/item features, where each document is split into a prompt\nconsisting of heterogeneous soft (user/item) tokens and hard (vocab) tokens and\na main text consisting of homogeneous item tokens or vocab tokens that\nfacilitates stable and effective language modeling. In addition, a novel mutual\nregularization strategy is introduced to encourage the CLLM4Rec to capture\nrecommendation-oriented information from user/item contents. Finally, we\npropose a novel recommendation-oriented finetuning strategy for CLLM4Rec, where\nan item prediction head with multinomial likelihood is added to the pretrained\nCLLM4Rec backbone to predict hold-out items based on the soft+hard prompts\nestablished from masked user-item interaction history, where recommendations of\nmultiple items can be generated efficiently.",
            "author": [
                "Yaochen Zhu",
                "Liang Wu",
                "Qi Guo",
                "Liangjie Hong",
                "Jundong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01343v3",
                "http://arxiv.org/pdf/2311.01343v3"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01337v1",
            "title": "Adaptive Identification of SIS Models",
            "updated": "2023-11-02T15:47:33Z",
            "published": "2023-11-02T15:47:33Z",
            "summary": "Effective containment of spreading processes such as epidemics requires\naccurate knowledge of several key parameters that govern their dynamics. In\nthis work, we first show that the problem of identifying the underlying\nparameters of epidemiological spreading processes is often ill-conditioned and\nlacks the persistence of excitation required for the convergence of adaptive\nlearning schemes. To tackle this challenge, we leverage a relaxed property\ncalled initial excitation combined with a recursive least squares algorithm to\ndesign an online adaptive identifier to learn the parameters of the\nsusceptible-infected-susceptible (SIS) epidemic model from the knowledge of its\nstates. We prove that the iterates generated by the proposed algorithm minimize\nan auxiliary weighted least squares cost function. We illustrate the\nconvergence of the error of the estimated epidemic parameters via several\nnumerical case studies and compare it with results obtained using conventional\napproaches.",
            "author": [
                "Chi Ho Leung",
                "William E. Retnaraj",
                "Ashish R. Hota",
                "Philip E. Par\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01337v1",
                "http://arxiv.org/pdf/2311.01337v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01326v1",
            "title": "Better Together: Enhancing Generative Knowledge Graph Completion with\n  Language Models and Neighborhood Information",
            "updated": "2023-11-02T15:38:39Z",
            "published": "2023-11-02T15:38:39Z",
            "summary": "Real-world Knowledge Graphs (KGs) often suffer from incompleteness, which\nlimits their potential performance. Knowledge Graph Completion (KGC) techniques\naim to address this issue. However, traditional KGC methods are computationally\nintensive and impractical for large-scale KGs, necessitating the learning of\ndense node embeddings and computing pairwise distances. Generative\ntransformer-based language models (e.g., T5 and recent KGT5) offer a promising\nsolution as they can predict the tail nodes directly. In this study, we propose\nto include node neighborhoods as additional information to improve KGC methods\nbased on language models. We examine the effects of this imputation and show\nthat, on both inductive and transductive Wikidata subsets, our method\noutperforms KGT5 and conventional KGC approaches. We also provide an extensive\nanalysis of the impact of neighborhood on model prediction and show its\nimportance. Furthermore, we point the way to significantly improve KGC through\nmore effective neighborhood selection.",
            "author": [
                "Alla Chepurova",
                "Aydar Bulatov",
                "Yuri Kuratov",
                "Mikhail Burtsev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01326v1",
                "http://arxiv.org/pdf/2311.01326v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01317v2",
            "title": "On Graphs with Finite-Time Consensus and Their Use in Gradient Tracking",
            "updated": "2023-11-14T18:43:56Z",
            "published": "2023-11-02T15:32:37Z",
            "summary": "This paper studies sequences of graphs satisfying the finite-time consensus\nproperty (i.e., iterating through such a finite sequence is equivalent to\nperforming global or exact averaging) and their use in Gradient Tracking. We\nprovide an explicit weight matrix representation of the studied sequences and\nprove their finite-time consensus property. Moreover, we incorporate the\nstudied finite-time consensus topologies into Gradient Tracking and present a\nnew algorithmic scheme called Gradient Tracking for Finite-Time Consensus\nTopologies (GT-FT). We analyze the new scheme for nonconvex problems with\nstochastic gradient estimates. Our analysis shows that the convergence rate of\nGT-FT does not depend on the heterogeneity of the agents' functions or the\nconnectivity of any individual graph in the topology sequence. Furthermore,\nowing to the sparsity of the graphs, GT-FT requires lower communication costs\nthan Gradient Tracking using the static counterpart of the topology sequence.",
            "author": [
                "Edward Duc Hien Nguyen",
                "Xin Jiang",
                "Bicheng Ying",
                "C\u00e9sar A. Uribe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01317v2",
                "http://arxiv.org/pdf/2311.01317v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01307v1",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual\n  Consistency of Language Models",
            "updated": "2023-11-02T15:20:11Z",
            "published": "2023-11-02T15:20:11Z",
            "summary": "Large Language Models (LLMs) make natural interfaces to factual knowledge,\nbut their usefulness is limited by their tendency to deliver inconsistent\nanswers to semantically equivalent questions. For example, a model might\npredict both \"Anne Redpath passed away in Edinburgh.\" and \"Anne Redpath's life\nended in London.\" In this work, we identify potential causes of inconsistency\nand evaluate the effectiveness of two mitigation strategies: up-scaling and\naugmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas\nmodels show that both strategies reduce inconsistency while retrieval\naugmentation is considerably more efficient. We further consider and\ndisentangle the consistency contributions of different components of Atlas. For\nall LMs evaluated we find that syntactical form and other evaluation task\nartifacts impact consistency. Taken together, our results provide a better\nunderstanding of the factors affecting the factual consistency of language\nmodels.",
            "author": [
                "Lovisa Hagstr\u00f6m",
                "Denitsa Saynova",
                "Tobias Norlund",
                "Moa Johansson",
                "Richard Johansson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01307v1",
                "http://arxiv.org/pdf/2311.01307v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01299v1",
            "title": "Large traveling capillary-gravity waves for Darcy flow",
            "updated": "2023-11-02T15:14:07Z",
            "published": "2023-11-02T15:14:07Z",
            "summary": "We study capillary-gravity and capillary surface waves for fluid flows\ngoverned by Darcy's law. This includes flows in vertical Hele-Shaw cells and in\nporous media (the one-phase Muskat problem) with finite or infinite depth. The\nfree boundary is acted upon by an external pressure posited to be in traveling\nwave form with an arbitrary periodic profile and an amplitude parameter. For\nany given wave speed, we first prove that there exists a unique local curve of\nsmall periodic traveling waves corresponding to small values of the parameter.\nThen we prove that as the parameter increases but could possibly be bounded,\nthe curve belongs to a connected set $\\mathcal{C}$ of traveling waves. The set\n$\\mathcal{C}$ contains traveling waves that either have arbitrarily large\ngradients or are arbitrarily close to the rigid bottom in the finite depth\ncase. To the best of our knowledge, this is the first construction of large\ntraveling surface waves for a viscous free boundary problem.",
            "author": [
                "Huy Q. Nguyen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01299v1",
                "http://arxiv.org/pdf/2311.01299v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01292v1",
            "title": "Joint 3D Shape and Motion Estimation from Rolling Shutter Light-Field\n  Images",
            "updated": "2023-11-02T15:08:18Z",
            "published": "2023-11-02T15:08:18Z",
            "summary": "In this paper, we propose an approach to address the problem of 3D\nreconstruction of scenes from a single image captured by a light-field camera\nequipped with a rolling shutter sensor. Our method leverages the 3D information\ncues present in the light-field and the motion information provided by the\nrolling shutter effect. We present a generic model for the imaging process of\nthis sensor and a two-stage algorithm that minimizes the re-projection error\nwhile considering the position and motion of the camera in a motion-shape\nbundle adjustment estimation strategy. Thereby, we provide an instantaneous 3D\nshape-and-pose-and-velocity sensing paradigm. To the best of our knowledge,\nthis is the first study to leverage this type of sensor for this purpose. We\nalso present a new benchmark dataset composed of different light-fields showing\nrolling shutter effects, which can be used as a common base to improve the\nevaluation and tracking the progress in the field. We demonstrate the\neffectiveness and advantages of our approach through several experiments\nconducted for different scenes and types of motions. The source code and\ndataset are publicly available at: https://github.com/ICB-Vision-AI/RSLF",
            "author": [
                "Hermes McGriff",
                "Renato Martins",
                "Nicolas Andreff",
                "C\u00e9dric Demonceaux"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01292v1",
                "http://arxiv.org/pdf/2311.01292v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01283v1",
            "title": "Distilling Knowledge from CNN-Transformer Models for Enhanced Human\n  Action Recognition",
            "updated": "2023-11-02T14:57:58Z",
            "published": "2023-11-02T14:57:58Z",
            "summary": "This paper presents a study on improving human action recognition through the\nutilization of knowledge distillation, and the combination of CNN and ViT\nmodels. The research aims to enhance the performance and efficiency of smaller\nstudent models by transferring knowledge from larger teacher models. The\nproposed method employs a Transformer vision network as the student model,\nwhile a convolutional network serves as the teacher model. The teacher model\nextracts local image features, whereas the student model focuses on global\nfeatures using an attention mechanism. The Vision Transformer (ViT)\narchitecture is introduced as a robust framework for capturing global\ndependencies in images. Additionally, advanced variants of ViT, namely PVT,\nConvit, MVIT, Swin Transformer, and Twins, are discussed, highlighting their\ncontributions to computer vision tasks. The ConvNeXt model is introduced as a\nteacher model, known for its efficiency and effectiveness in computer vision.\nThe paper presents performance results for human action recognition on the\nStanford 40 dataset, comparing the accuracy and mAP of student models trained\nwith and without knowledge distillation. The findings illustrate that the\nsuggested approach significantly improves the accuracy and mAP when compared to\ntraining networks under regular settings. These findings emphasize the\npotential of combining local and global features in action recognition tasks.",
            "author": [
                "Hamid Ahmadabadi",
                "Omid Nejati Manzari",
                "Ahmad Ayatollahi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01283v1",
                "http://arxiv.org/pdf/2311.01283v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01276v2",
            "title": "Long-Range Neural Atom Learning for Molecular Graphs",
            "updated": "2023-11-27T13:02:50Z",
            "published": "2023-11-02T14:44:50Z",
            "summary": "Graph Neural Networks (GNNs) have been widely adopted for drug discovery with\nmolecular graphs. Nevertheless, current GNNs are mainly good at leveraging\nshort-range interactions (SRI) but struggle to capture long-range interactions\n(LRI), both of which are crucial for determining molecular properties. To\ntackle this issue, we propose a method that implicitly projects all original\natoms into a few Neural Atoms, which abstracts the collective information of\natomic groups within a molecule. Specifically, we explicitly exchange the\ninformation among neural atoms and project them back to the atoms'\nrepresentations as an enhancement. With this mechanism, neural atoms establish\nthe communication channels among distant nodes, effectively reducing the\ninteraction scope of arbitrary node pairs into a single hop. To provide an\ninspection of our method from a physical perspective, we reveal its connection\nwith the traditional LRI calculation method, Ewald Summation. We conduct\nextensive experiments on three long-range graph benchmarks, covering both\ngraph-level and link-level tasks on molecular graphs. We empirically justify\nthat our method can be equipped with an arbitrary GNN and help to capture LRI.",
            "author": [
                "Xuan Li",
                "Zhanke Zhou",
                "Jiangchao Yao",
                "Yu Rong",
                "Lu Zhang",
                "Bo Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01276v2",
                "http://arxiv.org/pdf/2311.01276v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01266v1",
            "title": "Let's Discover More API Relations: A Large Language Model-based AI Chain\n  for Unsupervised API Relation Inference",
            "updated": "2023-11-02T14:25:00Z",
            "published": "2023-11-02T14:25:00Z",
            "summary": "APIs have intricate relations that can be described in text and represented\nas knowledge graphs to aid software engineering tasks. Existing relation\nextraction methods have limitations, such as limited API text corpus and\naffected by the characteristics of the input text.To address these limitations,\nwe propose utilizing large language models (LLMs) (e.g., GPT-3.5) as a neural\nknowledge base for API relation inference. This approach leverages the entire\nWeb used to pre-train LLMs as a knowledge base and is insensitive to the\ncontext and complexity of input texts. To ensure accurate inference, we design\nour analytic flow as an AI Chain with three AI modules: API FQN Parser, API\nKnowledge Extractor, and API Relation Decider. The accuracy of the API FQN\nparser and API Relation Decider module are 0.81 and 0.83, respectively. Using\nthe generative capacity of the LLM and our approach's inference capability, we\nachieve an average F1 value of 0.76 under the three datasets, significantly\nhigher than the state-of-the-art method's average F1 value of 0.40. Compared to\nCoT-based method, our AI Chain design improves the inference reliability by\n67%, and the AI-crowd-intelligence strategy enhances the robustness of our\napproach by 26%.",
            "author": [
                "Qing Huang",
                "Yanbang Sun",
                "Zhenchang Xing",
                "Yuanlong Cao",
                "Jieshan Chen",
                "Xiwei Xu",
                "Huan Jin",
                "Jiaxing Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01266v1",
                "http://arxiv.org/pdf/2311.01266v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02109v1",
            "title": "On conjectures concerning the graph grabbing game",
            "updated": "2023-11-02T14:13:02Z",
            "published": "2023-11-02T14:13:02Z",
            "summary": "We consider two conjectures made in regard to the graph grabbing game, played\non a vertex weighted graph. Seacrest and Seacrest conjectured in 2012 that the\nfirst player can win the graph grabbing game on any even-order bipartite graph.\nEoh and Choi conjectured a strengthening of this in 2019, namely that the first\nplayer can win on any graph with no induced corona product of an odd cycle and\na point. We provide a family of counterexamples to the latter conjecture, and\npropose a weaker conjecture in its place. We also show that the above two\nconjectures are equivalent when the vertex weights are all $0$ or $1$.",
            "author": [
                "Lawrence Hollom"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02109v1",
                "http://arxiv.org/pdf/2311.02109v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C57"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01222v1",
            "title": "Image Reflection on LLEE Charts -- Another Proof for the Completeness of\n  an Axiomatization of 1-Free Regular Expressions Modulo Bisimilarity",
            "updated": "2023-11-02T13:22:05Z",
            "published": "2023-11-02T13:22:05Z",
            "summary": "We analyze a phenomenon called \"image reflection\" on a type of\ncharacterization graphs -- LLEE charts -- of 1-free regular expressions modulo\nbisimulation equivalence. Due to the correspondence between 1-free regular\nexpressions and the provable solutions of LEE/LLEE charts, this observation\nnaturally leads to a new proof for the completeness of the proof system\n\\MilIfree\\ for 1-free regular expressions modulo bisimulation equivalence. The\ncritical part of the previous proof is to show that bisimulation collapse,\nwhich plays the role in linking the provable solutions of two LLEE charts, is\nstill an LLEE chart. The difference of our proof, compared to the previous one,\nis that we do not rely on the graph transformations from LLEE charts into their\nbisimulation collapses by merging two bisimular nodes in each transformation\nstep. Instead, we directly show that the bisimulation collapse of an LLEE chart\npossesses an LEE/LLEE structure based on its set of images mapped through the\nbisimulation function from the LLEE chart, and the constrained relation between\nthe images and their so-called \"well-structured\" looping-back charts pre-images\non the LLEE chart. Our approach provides a novel angle to look at this problem\nand related problems, and might introduce a different way for proving the\ncompleteness problem of \\Mil\\ for regular expressions modulo bisimulation\nequivalence, which had remained open until very recently.",
            "author": [
                "Yuanrui Zhang",
                "Xinxin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01222v1",
                "http://arxiv.org/pdf/2311.01222v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01212v1",
            "title": "Multi-view Relation Learning for Cross-domain Few-shot Hyperspectral\n  Image Classification",
            "updated": "2023-11-02T13:06:03Z",
            "published": "2023-11-02T13:06:03Z",
            "summary": "Cross-domain few-shot hyperspectral image classification focuses on learning\nprior knowledge from a large number of labeled samples from source domain and\nthen transferring the knowledge to the tasks which contain only few labeled\nsamples in target domains. Following the metric-based manner, many current\nmethods first extract the features of the query and support samples, and then\ndirectly predict the classes of query samples according to their distance to\nthe support samples or prototypes. The relations between samples have not been\nfully explored and utilized. Different from current works, this paper proposes\nto learn sample relations from different views and take them into the model\nlearning process, to improve the cross-domain few-shot hyperspectral image\nclassification. Building on current DCFSL method which adopts a domain\ndiscriminator to deal with domain-level distribution difference, the proposed\nmethod applys contrastive learning to learn the class-level sample relations to\nobtain more discriminable sample features. In addition, it adopts a transformer\nbased cross-attention learning module to learn the set-level sample relations\nand acquire the attentions from query samples to support samples. Our\nexperimental results have demonstrated the contribution of the multi-view\nrelation learning mechanism for few-shot hyperspectral image classification\nwhen compared with the state of the art methods.",
            "author": [
                "Chun Liu",
                "Longwei Yang",
                "Zheng Li",
                "Wei Yang",
                "Zhigang Han",
                "Jianzhong Guo",
                "Junyong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01212v1",
                "http://arxiv.org/pdf/2311.01212v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01205v1",
            "title": "Attacking Graph Neural Networks with Bit Flips: Weisfeiler and Lehman Go\n  Indifferent",
            "updated": "2023-11-02T12:59:32Z",
            "published": "2023-11-02T12:59:32Z",
            "summary": "Prior attacks on graph neural networks have mostly focused on graph poisoning\nand evasion, neglecting the network's weights and biases. Traditional\nweight-based fault injection attacks, such as bit flip attacks used for\nconvolutional neural networks, do not consider the unique properties of graph\nneural networks. We propose the Injectivity Bit Flip Attack, the first bit flip\nattack designed specifically for graph neural networks. Our attack targets the\nlearnable neighborhood aggregation functions in quantized message passing\nneural networks, degrading their ability to distinguish graph structures and\nlosing the expressivity of the Weisfeiler-Lehman test. Our findings suggest\nthat exploiting mathematical properties specific to certain graph neural\nnetwork architectures can significantly increase their vulnerability to bit\nflip attacks. Injectivity Bit Flip Attacks can degrade the maximal expressive\nGraph Isomorphism Networks trained on various graph property prediction\ndatasets to random output by flipping only a small fraction of the network's\nbits, demonstrating its higher destructive power compared to a bit flip attack\ntransferred from convolutional neural networks. Our attack is transparent and\nmotivated by theoretical insights which are confirmed by extensive empirical\nresults.",
            "author": [
                "Lorenz Kummer",
                "Samir Moustafa",
                "Nils N. Kriege",
                "Wilfried N. Gansterer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01205v1",
                "http://arxiv.org/pdf/2311.01205v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01198v1",
            "title": "Gaussian Processes on Cellular Complexes",
            "updated": "2023-11-02T12:49:14Z",
            "published": "2023-11-02T12:49:14Z",
            "summary": "In recent years, there has been considerable interest in developing machine\nlearning models on graphs in order to account for topological inductive biases.\nIn particular, recent attention was given to Gaussian processes on such\nstructures since they can additionally account for uncertainty. However, graphs\nare limited to modelling relations between two vertices. In this paper, we go\nbeyond this dyadic setting and consider polyadic relations that include\ninteractions between vertices, edges and one of their generalisations, known as\ncells. Specifically, we propose Gaussian processes on cellular complexes, a\ngeneralisation of graphs that captures interactions between these higher-order\ncells. One of our key contributions is the derivation of two novel kernels, one\nthat generalises the graph Mat\\'ern kernel and one that additionally mixes\ninformation of different cell types.",
            "author": [
                "Mathieu Alain",
                "So Takao",
                "Brooks Paige",
                "Marc Peter Deisenroth"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01198v1",
                "http://arxiv.org/pdf/2311.01198v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01196v1",
            "title": "Combating Bilateral Edge Noise for Robust Link Prediction",
            "updated": "2023-11-02T12:47:49Z",
            "published": "2023-11-02T12:47:49Z",
            "summary": "Although link prediction on graphs has achieved great success with the\ndevelopment of graph neural networks (GNNs), the potential robustness under the\nedge noise is still less investigated. To close this gap, we first conduct an\nempirical study to disclose that the edge noise bilaterally perturbs both input\ntopology and target label, yielding severe performance degradation and\nrepresentation collapse. To address this dilemma, we propose an\ninformation-theory-guided principle, Robust Graph Information Bottleneck\n(RGIB), to extract reliable supervision signals and avoid representation\ncollapse. Different from the basic information bottleneck, RGIB further\ndecouples and balances the mutual dependence among graph topology, target\nlabels, and representation, building new learning objectives for robust\nrepresentation against the bilateral noise. Two instantiations, RGIB-SSL and\nRGIB-REP, are explored to leverage the merits of different methodologies, i.e.,\nself-supervised learning and data reparameterization, for implicit and explicit\ndata denoising, respectively. Extensive experiments on six datasets and three\nGNNs with diverse noisy scenarios verify the effectiveness of our RGIB\ninstantiations. The code is publicly available at:\nhttps://github.com/tmlr-group/RGIB.",
            "author": [
                "Zhanke Zhou",
                "Jiangchao Yao",
                "Jiaxu Liu",
                "Xiawei Guo",
                "Quanming Yao",
                "Li He",
                "Liang Wang",
                "Bo Zheng",
                "Bo Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01196v1",
                "http://arxiv.org/pdf/2311.01196v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01192v1",
            "title": "Semantic Scene Graph Generation Based on an Edge Dual Scene Graph and\n  Message Passing Neural Network",
            "updated": "2023-11-02T12:36:52Z",
            "published": "2023-11-02T12:36:52Z",
            "summary": "Along with generative AI, interest in scene graph generation (SGG), which\ncomprehensively captures the relationships and interactions between objects in\nan image and creates a structured graph-based representation, has significantly\nincreased in recent years. However, relying on object-centric and dichotomous\nrelationships, existing SGG methods have a limited ability to accurately\npredict detailed relationships. To solve these problems, a new approach to the\nmodeling multiobject relationships, called edge dual scene graph generation\n(EdgeSGG), is proposed herein. EdgeSGG is based on a edge dual scene graph and\nDual Message Passing Neural Network (DualMPNN), which can capture rich\ncontextual interactions between unconstrained objects. To facilitate the\nlearning of edge dual scene graphs with a symmetric graph structure, the\nproposed DualMPNN learns both object- and relation-centric features for more\naccurately predicting relation-aware contexts and allows fine-grained\nrelational updates between objects. A comparative experiment with\nstate-of-the-art (SoTA) methods was conducted using two public datasets for SGG\noperations and six metrics for three subtasks. Compared with SoTA approaches,\nthe proposed model exhibited substantial performance improvements across all\nSGG subtasks. Furthermore, experiment on long-tail distributions revealed that\nincorporating the relationships between objects effectively mitigates existing\nlong-tail problems.",
            "author": [
                "Hyeongjin Kim",
                "Sangwon Kim",
                "Jong Taek Lee",
                "Byoung Chul Ko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01192v1",
                "http://arxiv.org/pdf/2311.01192v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01191v1",
            "title": "VIGraph: Self-supervised Learning for Class-Imbalanced Node\n  Classification",
            "updated": "2023-11-02T12:36:19Z",
            "published": "2023-11-02T12:36:19Z",
            "summary": "Class imbalance in graph data poses significant challenges for node\nclassification. Existing methods, represented by SMOTE-based approaches,\npartially alleviate this issue but still exhibit limitations during imbalanced\nscenario construction. Self-supervised learning (SSL) offers a promising\nsolution by synthesizing minority nodes from the data itself, yet its potential\nremains unexplored. In this paper, we analyze the limitations of SMOTE-based\napproaches and introduce VIGraph, a novel SSL model based on the\nself-supervised Variational Graph Auto-Encoder (VGAE) that leverages\nVariational Inference (VI) to generate minority nodes. Specifically, VIGraph\nstrictly adheres to the concept of imbalance when constructing imbalanced\ngraphs and utilizes the generative VGAE to generate minority nodes. Moreover,\nVIGraph introduces a novel Siamese contrastive strategy at the decoding phase\nto improve the overall quality of generated nodes. VIGraph can generate\nhigh-quality nodes without reintegrating them into the original graph,\neliminating the \"Generating, Reintegrating, and Retraining\" process found in\nSMOTE-based methods. Experiments on multiple real-world datasets demonstrate\nthat VIGraph achieves promising results for class-imbalanced node\nclassification tasks.",
            "author": [
                "Yulan Hu",
                "Sheng Ouyang",
                "Zhirui Yang",
                "Yong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01191v1",
                "http://arxiv.org/pdf/2311.01191v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01190v3",
            "title": "Non-canonical maximum cliques without a design structure in the block\n  graphs of 2-designs",
            "updated": "2023-11-07T02:04:03Z",
            "published": "2023-11-02T12:36:12Z",
            "summary": "In this note we answer positively a question of Chris Godsil and Karen\nMeagher on the existence of a 2-design whose block graph has a non-canonical\nmaximum clique without a design structure.",
            "author": [
                "Sergey Goryainov",
                "Elena V. Konstantinova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01190v3",
                "http://arxiv.org/pdf/2311.01190v3"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01180v1",
            "title": "Automatic Configuration of Multi-Agent Model Predictive Controllers\n  based on Semantic Graph World Models",
            "updated": "2023-11-02T12:23:08Z",
            "published": "2023-11-02T12:23:08Z",
            "summary": "We propose a shared semantic map architecture to construct and configure\nModel Predictive Controllers (MPC) dynamically, that solve navigation problems\nfor multiple robotic agents sharing parts of the same environment. The\nnavigation task is represented as a sequence of semantically labeled areas in\nthe map, that must be traversed sequentially, i.e. a route. Each semantic label\nrepresents one or more constraints on the robots' motion behaviour in that\narea. The advantages of this approach are: (i) an MPC-based motion controller\nin each individual robot can be (re-)configured, at runtime, with the locally\nand temporally relevant parameters; (ii) the application can influence, also at\nruntime, the navigation behaviour of the robots, just by adapting the semantic\nlabels; and (iii) the robots can reason about their need for coordination,\nthrough analyzing over which horizon in time and space their routes overlap.\nThe paper provides simulations of various representative situations, showing\nthat the approach of runtime configuration of the MPC drastically decreases\ncomputation time, while retaining task execution performance similar to an\napproach in which each robot always includes all other robots in its MPC\ncomputations.",
            "author": [
                "K. de Vos",
                "E. Torta",
                "H. Bruyninckx",
                "C. A. Lopez Martinez",
                "M. J. G. van de Molengraft"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01180v1",
                "http://arxiv.org/pdf/2311.01180v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01169v1",
            "title": "Resource-aware Research on Universe and Matter: Call-to-Action in\n  Digital Transformation",
            "updated": "2023-11-02T12:07:35Z",
            "published": "2023-11-02T12:07:35Z",
            "summary": "Given the urgency to reduce fossil fuel energy production to make climate\ntipping points less likely, we call for resource-aware knowledge gain in the\nresearch areas on Universe and Matter with emphasis on the digital\ntransformation. A portfolio of measures is described in detail and then\nsummarized according to the timescales required for their implementation. The\nmeasures will both contribute to sustainable research and accelerate scientific\nprogress through increased awareness of resource usage. This work is based on a\nthree-days workshop on sustainability in digital transformation held in May\n2023.",
            "author": [
                "Ben Bruers",
                "Marilyn Cruces",
                "Markus Demleitner",
                "Guenter Duckeck",
                "Michael D\u00fcren",
                "Niclas Eich",
                "Torsten En\u00dflin",
                "Johannes Erdmann",
                "Martin Erdmann",
                "Peter Fackeldey",
                "Christian Felder",
                "Benjamin Fischer",
                "Stefan Fr\u00f6se",
                "Stefan Funk",
                "Martin Gasthuber",
                "Andrew Grimshaw",
                "Daniela Hadasch",
                "Moritz Hannemann",
                "Alexander Kappes",
                "Raphael Kleinem\u00fchl",
                "Oleksiy M. Kozlov",
                "Thomas Kuhr",
                "Michael Lupberger",
                "Simon Neuhaus",
                "Pardis Niknejadi",
                "Judith Reindl",
                "Daniel Schindler",
                "Astrid Schneidewind",
                "Frank Schreiber",
                "Markus Schumacher",
                "Kilian Schwarz",
                "Achim Streit",
                "R. Florian von Cube",
                "Rod Walker",
                "Cyrus Walther",
                "Sebastian Wozniewski",
                "Kai Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01169v1",
                "http://arxiv.org/pdf/2311.01169v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "astro-ph.IM",
                "cond-mat.mtrl-sci",
                "hep-ex",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01161v1",
            "title": "Weakly Supervised Semantic Parsing with Execution-based Spurious Program\n  Filtering",
            "updated": "2023-11-02T11:45:40Z",
            "published": "2023-11-02T11:45:40Z",
            "summary": "The problem of spurious programs is a longstanding challenge when training a\nsemantic parser from weak supervision. To eliminate such programs that have\nwrong semantics but correct denotation, existing methods focus on exploiting\nsimilarities between examples based on domain-specific knowledge. In this\npaper, we propose a domain-agnostic filtering mechanism based on program\nexecution results. Specifically, for each program obtained through the search\nprocess, we first construct a representation that captures the program's\nsemantics as execution results under various inputs. Then, we run a majority\nvote on these representations to identify and filter out programs with\nsignificantly different semantics from the other programs. In particular, our\nmethod is orthogonal to the program search process so that it can easily\naugment any of the existing weakly supervised semantic parsing frameworks.\nEmpirical evaluations on the Natural Language Visual Reasoning and\nWikiTableQuestions demonstrate that applying our method to the existing\nsemantic parsers induces significantly improved performances.",
            "author": [
                "Kang-il Lee",
                "Segwang Kim",
                "Kyomin Jung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01161v1",
                "http://arxiv.org/pdf/2311.01161v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01153v1",
            "title": "ACES: Translation Accuracy Challenge Sets at WMT 2023",
            "updated": "2023-11-02T11:29:09Z",
            "published": "2023-11-02T11:29:09Z",
            "summary": "We benchmark the performance of segmentlevel metrics submitted to WMT 2023\nusing the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists\nof 36K examples representing challenges from 68 phenomena and covering 146\nlanguage pairs. The phenomena range from simple perturbations at the\nword/character level to more complex errors based on discourse and real-world\nknowledge. For each metric, we provide a detailed profile of performance over a\nrange of error categories as well as an overall ACES-Score for quick\ncomparison. We also measure the incremental performance of the metrics\nsubmitted to both WMT 2023 and 2022. We find that 1) there is no clear winner\namong the metrics submitted to WMT 2023, and 2) performance change between the\n2023 and 2022 versions of the metrics is highly variable. Our recommendations\nare similar to those from WMT 2022. Metric developers should focus on: building\nensembles of metrics from different design families, developing metrics that\npay more attention to the source and rely less on surface-level overlap, and\ncarefully determining the influence of multilingual embeddings on MT\nevaluation.",
            "author": [
                "Chantal Amrhein",
                "Nikita Moghe",
                "Liane Guillou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01153v1",
                "http://arxiv.org/pdf/2311.01153v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01150v1",
            "title": "Revisiting the Knowledge Injection Frameworks",
            "updated": "2023-11-02T11:18:16Z",
            "published": "2023-11-02T11:18:16Z",
            "summary": "In recent years, large language models (LLMs), such as GPTs, have attained\ngreat impact worldwide. However, how to adapt these LLMs to better suit the\nvertical domain-specific tasks by utilizing external knowledge remains not\ncompletely solved. Indeed, there have emerged a few works on this line where\nmost of them rely on an alignment heuristic that is built to inject the\ncorresponding knowledge tuple into the associated text sample.\n  However, despite the promise, we identify a pivotal problem in this work\nubiquitously. Simply put, we find that injecting unaligned (i.e., random)\nknowledge tuple into the LLMs achieves comparable (and sometimes better)\nresults than the aligned knowledge being injected. We therefore take a thorough\ninvestigation of this frustrating finding on a variety of related prior work\nand further provide a chain of potential interpretations for the phenomenon.\nBased on all that, we offer a simple remediated technique. Briefly, the core of\nthis technique is rooted in an ideological emphasis on the pruning and\npurification of the external knowledge base to be injected into LLMs. At last,\nwe show that by integrating this technique into most (if not all) knowledge\ninjection frameworks and recent LLMs, it manages to overcome the aforementioned\nsanity problem and further pushes the boundary of the performance of the\ndomain-adaptive LLMs.",
            "author": [
                "Peng Fu",
                "Yiming Zhang",
                "Haobo Wang",
                "Weikang Qiu",
                "Junbo Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01150v1",
                "http://arxiv.org/pdf/2311.01150v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01123v1",
            "title": "Transitivity And Related Notions For Graph Induced Symbolic Systems",
            "updated": "2023-11-02T10:09:41Z",
            "published": "2023-11-02T10:09:41Z",
            "summary": "In this paper, we investigate the dynamical behavior of a two dimensional\nshift $X_G$ (generated by a two dimensional graph\n$G=(\\mathcal{H},\\mathcal{V})$) using the adjacency matrices of the generating\ngraph $G$. In particular, we investigate properties such as transitivity,\ndirectional transitivity, weak mixing, directional weak mixing and mixing for\nthe shift space $X_G$. We prove that if $(HV)_{ij}\\neq 0 \\Leftrightarrow\n(VH)_{ij}\\neq 0$ (for all $i,j$), while doubly transitivity (weak mixing) of\n$X_H$ (or $X_V$) ensures the same for two dimensional shift generated by the\ngraph $G$, directional transitivity (in the direction $(r,s)$) can be\ncharacterized through the block representation of $H^rV^s$. We provide\nnecessary and sufficient criteria to establish horizontal (vertical)\ntransitivity for the shift space $X_G$. We also provide examples to establish\nthe necessity of the conditions imposed. Finally, we investigate the\ndecomposability of a given graph into product of graphs with reduced\ncomplexity.",
            "author": [
                "Prashant Kumar",
                "Puneet Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01123v1",
                "http://arxiv.org/pdf/2311.01123v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "37B10, 37B20, 37B51"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01104v2",
            "title": "Projected Policy Gradient Converges in a Finite Number of Iterations",
            "updated": "2023-11-06T08:37:34Z",
            "published": "2023-11-02T09:12:44Z",
            "summary": "The convergence of the projected policy gradient (PPG) method under the\nsimplex parameterization is studied and it is shown that this method indeed\nachieves the exact convergence in a finite number of iterations for any\nconstant step size. To establish this result, we first establish the sublinear\nconvergence of PPG for an arbitrary fixed step size, which is also new, to the\nbest of knowledge. The finite iteration convergence property is also applicable\nto a preconditioned version of PPG, namely the projected Q-ascent (PQA) method.\nAdditionally, the linear convergence of PPG and its equivalence to PI are\nestablished under the non-adaptive increasing step sizes and the adaptive step\nsizes, respectively.",
            "author": [
                "Jiacai Liu",
                "Wenye Li",
                "Ke Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01104v2",
                "http://arxiv.org/pdf/2311.01104v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01096v1",
            "title": "Gapped boundaries of fermionic topological orders and higher central\n  charges",
            "updated": "2023-11-02T08:59:27Z",
            "published": "2023-11-02T08:59:27Z",
            "summary": "We develop a test for the vanishing of higher central charges of a fermionic\ntopological order, which is a necessary condition for the existence of a gapped\nboundary, purely in terms of the modular data of the super-modular tensor\ncategory. More precisely, we test whether a given super-MTC has $c = 0$ mod\n$\\frac{1}{2}$, and, if so, whether the modular extension with $c =0$ mod $8$\nhas vanishing higher central charges. The test itself does not require an\nexplicit computation of the modular extensions and is easily carried out. We\napply this test to known examples of super-modular tensor categories. Since our\ntest allows us to obtain information about the chiral central charge of a\nsuper-modular tensor category in terms of its modular data without direct\nknowledge of its modular extensions, this can also be thought of as the first\nstep towards a fermionic analogue of the Gauss-Milgram formula.",
            "author": [
                "Minyoung You"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01096v1",
                "http://arxiv.org/pdf/2311.01096v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01094v1",
            "title": "Max $s,t$-Flow Oracles and Negative Cycle Detection in Planar Digraphs",
            "updated": "2023-11-02T08:57:17Z",
            "published": "2023-11-02T08:57:17Z",
            "summary": "We study the maximum $s,t$-flow oracle problem on planar directed graphs\nwhere the goal is to design a data structure answering max $s,t$-flow value (or\nequivalently, min $s,t$-cut value) queries for arbitrary source-target pairs\n$(s,t)$. For the case of polynomially bounded integer edge capacities, we\ndescribe an exact max $s,t$-flow oracle with truly subquadratic space and\npreprocessing, and sublinear query time. Moreover, if\n$(1-\\epsilon)$-approximate answers are acceptable, we obtain a static oracle\nwith near-linear preprocessing and $\\tilde{O}(n^{3/4})$ query time and a\ndynamic oracle supporting edge capacity updates and queries in\n$\\tilde{O}(n^{6/7})$ worst-case time.\n  To the best of our knowledge, for directed planar graphs, no (approximate)\nmax $s,t$-flow oracles have been described even in the unweighted case, and\nonly trivial tradeoffs involving either no preprocessing or precomputing all\nthe $n^2$ possible answers have been known.\n  One key technical tool we develop on the way is a sublinear (in the number of\nedges) algorithm for finding a negative cycle in so-called dense distance\ngraphs. By plugging it in earlier frameworks, we obtain improved bounds for\nother fundamental problems on planar digraphs. In particular, we show: (1) a\ndeterministic $O(n\\log(nC))$ time algorithm for negatively-weighted SSSP in\nplanar digraphs with integer edge weights at least $-C$. This improves upon the\npreviously known bounds in the important case of weights polynomial in $n$, and\n(2) an improved $O(n\\log{n})$ bound on finding a perfect matching in a\nbipartite planar graph.",
            "author": [
                "Adam Karczmarz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01094v1",
                "http://arxiv.org/pdf/2311.01094v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01087v1",
            "title": "Hyperbolic isometries of the ne curve graph of higher genus surfaces",
            "updated": "2023-11-02T08:54:02Z",
            "published": "2023-11-02T08:54:02Z",
            "summary": "We prove that for a homeomorphism f that is isotopic to the identity on a\nclosed hyperbolic surface, the following are equivalent: * f acts\nhyperbolically on the fine curve graph; * f is isotopic to a pseudo-Anosov map\nrelative to a finite f-invariant set; * the ergodic homological rotation set of\nf has nonempty interior.",
            "author": [
                "Pierre-Antoine Guih\u00e9neuf",
                "Emmanuel Militon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01087v1",
                "http://arxiv.org/pdf/2311.01087v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.GR",
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.03382v1",
            "title": "Causal Structure Representation Learning of Confounders in Latent Space\n  for Recommendation",
            "updated": "2023-11-02T08:46:07Z",
            "published": "2023-11-02T08:46:07Z",
            "summary": "Inferring user preferences from the historical feedback of users is a\nvaluable problem in recommender systems. Conventional approaches often rely on\nthe assumption that user preferences in the feedback data are equivalent to the\nreal user preferences without additional noise, which simplifies the problem\nmodeling. However, there are various confounders during user-item interactions,\nsuch as weather and even the recommendation system itself. Therefore,\nneglecting the influence of confounders will result in inaccurate user\npreferences and suboptimal performance of the model. Furthermore, the\nunobservability of confounders poses a challenge in further addressing the\nproblem. To address these issues, we refine the problem and propose a more\nrational solution. Specifically, we consider the influence of confounders,\ndisentangle them from user preferences in the latent space, and employ causal\ngraphs to model their interdependencies without specific labels. By cleverly\ncombining local and global causal graphs, we capture the user-specificity of\nconfounders on user preferences. We theoretically demonstrate the\nidentifiability of the obtained causal graph. Finally, we propose our model\nbased on Variational Autoencoders, named Causal Structure representation\nlearning of Confounders in latent space (CSC). We conducted extensive\nexperiments on one synthetic dataset and five real-world datasets,\ndemonstrating the superiority of our model. Furthermore, we demonstrate that\nthe learned causal representations of confounders are controllable, potentially\noffering users fine-grained control over the objectives of their recommendation\nlists with the learned causal graphs.",
            "author": [
                "Hangtong Xu",
                "Yuanbo Xu",
                "Yongjian Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.03382v1",
                "http://arxiv.org/pdf/2311.03382v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01082v1",
            "title": "The Feasibility Problem -- the family ${\\cal F}$$(G)$ of all induced\n  $G$-free graphs",
            "updated": "2023-11-02T08:45:07Z",
            "published": "2023-11-02T08:45:07Z",
            "summary": "An infinite family of graphs ${\\cal F}$ is called feasible if for any pair of\nintegers $(n,m)$, $n \\geq 1$, $0 \\leq m \\leq \\binom{n}{2}$, there is a member\n$G \\in {\\cal F}$ such that $G$ has $n$ vertices and $m$ edges. We prove that\ngiven a graph $G$, the family ${\\cal F}$$(G)$ of all induced $G$-free graphs is\nfeasible if and only if $G$ is not $K_k$, $K_k\\backslash K_2$,\n$\\overline{K_k}$, $\\overline{K_k\\backslash K_2}$, for $k \\geq 2$.",
            "author": [
                "Yair Caro",
                "Matthew Cassar",
                "Josef Lauri",
                "Christina Zarb"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01082v1",
                "http://arxiv.org/pdf/2311.01082v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C75"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01077v1",
            "title": "On Conflict-Free Cuts: Algorithms and Complexity",
            "updated": "2023-11-02T08:42:18Z",
            "published": "2023-11-02T08:42:18Z",
            "summary": "One way to define the Matching Cut problem is: Given a graph $G$, is there an\nedge-cut $M$ of $G$ such that $M$ is an independent set in the line graph of\n$G$? We propose the more general Conflict-Free Cut problem: Together with the\ngraph $G$, we are given a so-called conflict graph $\\hat{G}$ on the edges of\n$G$, and we ask for an edge-cutset $M$ of $G$ that is independent in $\\hat{G}$.\nSince conflict-free settings are popular generalizations of classical\noptimization problems and Conflict-Free Cut was not considered in the\nliterature so far, we start the study of the problem. We show that the problem\nis $\\textsf{NP}$-complete even when the maximum degree of $G$ is 5 and\n$\\hat{G}$ is 1-regular. The same reduction implies an exponential lower bound\non the solvability based on the Exponential Time Hypothesis. We also give\nparameterized complexity results: We show that the problem is fixed-parameter\ntractable with the vertex cover number of $G$ as a parameter, and we show\n$\\textsf{W[1]}$-hardness even when $G$ has a feedback vertex set of size one,\nand the clique cover number of $\\hat{G}$ is the parameter. Since the clique\ncover number of $\\hat{G}$ is an upper bound on the independence number of\n$\\hat{G}$ and thus the solution size, this implies $\\textsf{W[1]}$-hardness\nwhen parameterized by the cut size. We list polynomial-time solvable cases and\ninteresting open problems. At last, we draw a connection to a symmetric variant\nof SAT.",
            "author": [
                "Johannes Rauch",
                "Dieter Rautenbach",
                "U\u00e9verton S. Souza"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01077v1",
                "http://arxiv.org/pdf/2311.01077v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.CO",
                "05C85, 68Q25, 68R10",
                "F.2.2; G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01074v1",
            "title": "A simple toy model for the collapse of the local group with the Shapley\n  attractor",
            "updated": "2023-11-02T08:40:35Z",
            "published": "2023-11-02T08:40:35Z",
            "summary": "A toy model is proposed for the Cosmic Dipole consisting in the Shapley\nattractor and the so-called Dipole repeller, whose action is assimilated to an\nanti-gravitational force. According to this model, the local group will\ncollapse in finite time with the Shapley attractor and, by using the available\nfigures for distances and masses, it is shown that it will happen in less than\n100 billion years. To obtain a more precise estimate, more knowledge will be\nnecessary on the equivalent negative mass of the repeller.",
            "author": [
                "Alain Haraux"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01074v1",
                "http://arxiv.org/pdf/2311.01074v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01073v2",
            "title": "Fourier Analysis of Signals on Directed Acyclic Graphs (DAG) Using Graph\n  Zero-Padding",
            "updated": "2023-11-13T09:33:24Z",
            "published": "2023-11-02T08:40:21Z",
            "summary": "Directed acyclic graphs (DAGs) are used for modeling causal relationships,\ndependencies, and flows in various systems. However, spectral analysis becomes\nimpractical in this setting because the eigen-decomposition of the adjacency\nmatrix yields all eigenvalues equal to zero. This inherent property of DAGs\nresults in an inability to differentiate between frequency components of\nsignals on such graphs. This problem can be addressed by alternating the\nFourier basis or adding edges in a DAG. However, these approaches change the\nphysics of the considered problem. To address this limitation, we propose a\ngraph zero-padding approach. This approach involves augmenting the original DAG\nwith additional vertices that are connected to the existing structure. The\nadded vertices are characterized by signal values set to zero. The proposed\ntechnique enables the spectral evaluation of system outputs on DAGs (in almost\nall cases), that is the computation of vertex-domain convolution without the\nadverse effects of aliasing due to changes in a graph structure, with the\nultimate goal of preserving the output of the system on a graph as if the\nchanges in the graph structure were not done.",
            "author": [
                "Ljubisa Stankovic",
                "Milos Dakovic",
                "Ali Bagheri Bardi",
                "Milos Brajovic",
                "Isidora Stankovic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01073v2",
                "http://arxiv.org/pdf/2311.01073v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01070v1",
            "title": "DistilWhisper: Efficient Distillation of Multi-task Speech Models via\n  Language-Specific Experts",
            "updated": "2023-11-02T08:37:30Z",
            "published": "2023-11-02T08:37:30Z",
            "summary": "Whisper is a multitask and multilingual speech model covering 99 languages.\nIt yields commendable automatic speech recognition (ASR) results in a subset of\nits covered languages, but the model still under-performs on a non-negligible\nnumber of under-represented languages, a problem exacerbated in smaller model\nversions. In this work, we propose DistilWhisper, an approach able to bridge\nthe performance gap in ASR for these languages while retaining the advantages\nof multitask and multilingual capabilities. Our approach involves two key\nstrategies: lightweight modular ASR fine-tuning of whisper-small using\nlanguage-specific experts, and knowledge distillation from whisper-large-v2.\nThis dual approach allows us to effectively boost ASR performance while keeping\nthe robustness inherited from the multitask and multilingual pre-training.\nResults demonstrate that our approach is more effective than standard\nfine-tuning or LoRA adapters, boosting performance in the targeted languages\nfor both in- and out-of-domain test sets, while introducing only a negligible\nparameter overhead at inference.",
            "author": [
                "Thomas Palmeira Ferraz",
                "Marcely Zanon Boito",
                "Caroline Brun",
                "Vassilina Nikoulina"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01070v1",
                "http://arxiv.org/pdf/2311.01070v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01069v1",
            "title": "Inverse of the Squared Distance Matrix of a Complete Multipartite Graph",
            "updated": "2023-11-02T08:36:58Z",
            "published": "2023-11-02T08:36:58Z",
            "summary": "Let $G$ be a connected graph on $n$ vertices and $d_{ij}$ be the length of\nthe shortest path between vertices $i$ and $j$ in $G$. We set $d_{ii}=0$ for\nevery vertex $i$ in $G$. The squared distance matrix $\\Delta(G)$ of $G$ is the\n$n\\times n$ matrix with $(i,j)^{th}$ entry equal to $0$ if $i = j$ and equal to\n$d_{ij}^2$ if $i \\neq j$. For a given complete $t$-partite graph\n$K_{n_1,n_2,\\cdots,n_t}$ on $n=\\sum_{i=1}^t n_i$ vertices, under some condition\nwe find the inverse $\\Delta(K_{n_1,n_2,\\cdots,n_t})^{-1}$ as a rank-one\nperturbation of a symmetric Laplacian-like matrix $\\mathcal{L}$ with\n$\\textup{rank} (\\mathcal{L})=n-1$. We also investigate the inertia of\n$\\mathcal{L}$.",
            "author": [
                "Joyentanuj Das",
                "Sumit Mohanty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01069v1",
                "http://arxiv.org/pdf/2311.01069v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C12, 05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01064v1",
            "title": "Multimodal Foundation Models for Zero-shot Animal Species Recognition in\n  Camera Trap Images",
            "updated": "2023-11-02T08:32:00Z",
            "published": "2023-11-02T08:32:00Z",
            "summary": "Due to deteriorating environmental conditions and increasing human activity,\nconservation efforts directed towards wildlife is crucial. Motion-activated\ncamera traps constitute an efficient tool for tracking and monitoring wildlife\npopulations across the globe. Supervised learning techniques have been\nsuccessfully deployed to analyze such imagery, however training such techniques\nrequires annotations from experts. Reducing the reliance on costly labelled\ndata therefore has immense potential in developing large-scale wildlife\ntracking solutions with markedly less human labor. In this work we propose\nWildMatch, a novel zero-shot species classification framework that leverages\nmultimodal foundation models. In particular, we instruction tune\nvision-language models to generate detailed visual descriptions of camera trap\nimages using similar terminology to experts. Then, we match the generated\ncaption to an external knowledge base of descriptions in order to determine the\nspecies in a zero-shot manner. We investigate techniques to build instruction\ntuning datasets for detailed animal description generation and propose a novel\nknowledge augmentation technique to enhance caption quality. We demonstrate the\nperformance of WildMatch on a new camera trap dataset collected in the\nMagdalena Medio region of Colombia.",
            "author": [
                "Zalan Fabian",
                "Zhongqi Miao",
                "Chunyuan Li",
                "Yuanhan Zhang",
                "Ziwei Liu",
                "Andr\u00e9s Hern\u00e1ndez",
                "Andr\u00e9s Montes-Rojas",
                "Rafael Escucha",
                "Laura Siabatto",
                "Andr\u00e9s Link",
                "Pablo Arbel\u00e1ez",
                "Rahul Dodhia",
                "Juan Lavista Ferres"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01064v1",
                "http://arxiv.org/pdf/2311.01064v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01061v1",
            "title": "Deep Learning for real-time neural decoding of grasp",
            "updated": "2023-11-02T08:26:29Z",
            "published": "2023-11-02T08:26:29Z",
            "summary": "Neural decoding involves correlating signals acquired from the brain to\nvariables in the physical world like limb movement or robot control in Brain\nMachine Interfaces. In this context, this work starts from a specific\npre-existing dataset of neural recordings from monkey motor cortex and presents\na Deep Learning-based approach to the decoding of neural signals for grasp type\nclassification. Specifically, we propose here an approach that exploits LSTM\nnetworks to classify time series containing neural data (i.e., spike trains)\ninto classes representing the object being grasped. The main goal of the\npresented approach is to improve over state-of-the-art decoding accuracy\nwithout relying on any prior neuroscience knowledge, and leveraging only the\ncapability of deep learning models to extract correlations from data. The paper\npresents the results achieved for the considered dataset and compares them with\nprevious works on the same dataset, showing a significant improvement in\nclassification accuracy, even if considering simulated real-time decoding.",
            "author": [
                "Paolo Viviani",
                "Ilaria Gesmundo",
                "Elios Ghinato",
                "Andres Agudelo-Toro",
                "Chiara Vercellino",
                "Giacomo Vitali",
                "Letizia Bergamasco",
                "Alberto Scionti",
                "Marco Ghislieri",
                "Valentina Agostini",
                "Olivier Terzo",
                "Hansj\u00f6rg Scherberger"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43427-3_23",
                "http://arxiv.org/abs/2311.01061v1",
                "http://arxiv.org/pdf/2311.01061v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01051v1",
            "title": "Sponsorship Disclosure in Native Advertising: A Theoretical Framework",
            "updated": "2023-11-02T07:51:28Z",
            "published": "2023-11-02T07:51:28Z",
            "summary": "Native advertising is one of the fastest growing areas of online promotion.\nAfter reviewing extant literature via EBSCOhost database, this study draws on\nPersuasion Knowledge Model and develops a theoretical framework which\nfacilitates a clearer understanding of the relationship between sponsorship\ndisclosure in native advertising and consumer outcome. The framework suggests\nthat sponsorship disclosure has a negative effect on electronic word of mouth\n(eWOM), and further proposes the interplay between the main effect with brand\nprominence and the type of device. This is highly relevant to marketer as\nregulators have been pressuring for the disclosure of native advertising. As\nthis is likely to have detrimental effect to the eWOM, marketer may employ the\nboundary conditions proposed by this framework to attenuate that negative\neffect.",
            "author": [
                "Poompak Kusawat"
            ],
            "link": [
                "http://dx.doi.org/10.14456/jcdr-hs.2021.13",
                "http://arxiv.org/abs/2311.01051v1",
                "http://arxiv.org/pdf/2311.01051v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01041v1",
            "title": "Learn to Refuse: Making Large Language Models More Controllable and\n  Reliable through Knowledge Scope Limitation and Refusal Mechanism",
            "updated": "2023-11-02T07:20:49Z",
            "published": "2023-11-02T07:20:49Z",
            "summary": "Large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, enabling them to answer a wide range\nof questions across various domains. However, these models are not flawless and\noften produce responses that contain errors or misinformation. These\ninaccuracies, commonly referred to as hallucinations, render LLMs unreliable\nand even unusable in many scenarios. In this paper, our focus is on mitigating\nthe issue of hallucination in LLMs, particularly in the context of\nquestion-answering. Instead of attempting to answer all questions, we explore a\nrefusal mechanism that instructs LLMs to refuse to answer challenging questions\nin order to avoid errors. We then propose a simple yet effective solution\ncalled Learn to Refuse (L2R), which incorporates the refusal mechanism to\nenable LLMs to recognize and refuse to answer questions that they find\ndifficult to address. To achieve this, we utilize a structured knowledge base\nto represent all the LLM's understanding of the world, enabling it to provide\ntraceable gold knowledge. This knowledge base is separate from the LLM and\ninitially empty, and it is progressively expanded with validated knowledge.\nWhen an LLM encounters questions outside its domain, the system recognizes its\nknowledge scope and determines whether it can answer the question\nindependently. Additionally, we introduce a method for automatically and\nefficiently expanding the knowledge base of LLMs. Through qualitative and\nquantitative analysis, we demonstrate that our approach enhances the\ncontrollability and reliability of LLMs.",
            "author": [
                "Lang Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01041v1",
                "http://arxiv.org/pdf/2311.01041v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01039v1",
            "title": "Cross-cultural electronic word-of-mouth: a systematic literature review",
            "updated": "2023-11-02T07:16:23Z",
            "published": "2023-11-02T07:16:23Z",
            "summary": "Purpose: Global adoption of the internet and mobile usage results in a huge\nvariation in the cultural backgrounds of consumers who generate and consume\nelectronic word-of-mouth (eWOM). Unsurprisingly, a research trend on\ncross-cultural eWOM has emerged. However, there has not been an attempt to\nsynthesize this research topic. This paper aims to bridge this gap.\n  Methodology: This research paper conducts a systematic literature review of\nthe current research findings on cross-cultural eWOM. Journal articles\npublished from 2006 to 2021 are included. This study then presents the key\nissues in the extant literature and suggests potential future research.\n  Findings: The findings show that there has been an upward trend in the number\nof publications on cross-cultural eWOM since the early 2010s, with a relatively\nsteeper increase toward 2020. The findings also synthesize cross-cultural eWOM\nresearch into four elements and suggest potential future research avenues.\n  Value: To the best of the authors' knowledge, there is currently no\nexhaustive/integrated review of cross-cultural eWOM research. This research\nfills the need to summarize the current state of cross-cultural eWOM literature\nand identifies research questions to be addressed in the future.",
            "author": [
                "Poompak Kusawat",
                "Surat Teerakapibal"
            ],
            "link": [
                "http://dx.doi.org/10.1108/SJME-06-2021-0116",
                "http://arxiv.org/abs/2311.01039v1",
                "http://arxiv.org/pdf/2311.01039v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01038v2",
            "title": "Better with Less: A Data-Active Perspective on Pre-Training Graph Neural\n  Networks",
            "updated": "2023-11-21T05:48:06Z",
            "published": "2023-11-02T07:09:59Z",
            "summary": "Pre-training on graph neural networks (GNNs) aims to learn transferable\nknowledge for downstream tasks with unlabeled data, and it has recently become\nan active research area. The success of graph pre-training models is often\nattributed to the massive amount of input data. In this paper, however, we\nidentify the curse of big data phenomenon in graph pre-training: more training\ndata do not necessarily lead to better downstream performance. Motivated by\nthis observation, we propose a better-with-less framework for graph\npre-training: fewer, but carefully chosen data are fed into a GNN model to\nenhance pre-training. The proposed pre-training pipeline is called the\ndata-active graph pre-training (APT) framework, and is composed of a graph\nselector and a pre-training model. The graph selector chooses the most\nrepresentative and instructive data points based on the inherent properties of\ngraphs as well as predictive uncertainty. The proposed predictive uncertainty,\nas feedback from the pre-training model, measures the confidence level of the\nmodel in the data. When fed with the chosen data, on the other hand, the\npre-training model grasps an initial understanding of the new, unseen data, and\nat the same time attempts to remember the knowledge learned from previous data.\nTherefore, the integration and interaction between these two components form a\nunified framework (APT), in which graph pre-training is performed in a\nprogressive and iterative way. Experiment results show that the proposed APT is\nable to obtain an efficient pre-training model with fewer training data and\nbetter downstream performance.",
            "author": [
                "Jiarong Xu",
                "Renhong Huang",
                "Xin Jiang",
                "Yuxuan Cao",
                "Carl Yang",
                "Chunping Wang",
                "Yang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01038v2",
                "http://arxiv.org/pdf/2311.01038v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01036v1",
            "title": "ATHENA: Mathematical Reasoning with Thought Expansion",
            "updated": "2023-11-02T07:03:25Z",
            "published": "2023-11-02T07:03:25Z",
            "summary": "Solving math word problems depends on how to articulate the problems, the\nlens through which models view human linguistic expressions. Real-world\nsettings count on such a method even more due to the diverse practices of the\nsame mathematical operations. Earlier works constrain available thinking\nprocesses by limited prediction strategies without considering their\nsignificance in acquiring mathematical knowledge. We introduce Attention-based\nTHought Expansion Network Architecture (ATHENA) to tackle the challenges of\nreal-world practices by mimicking human thought expansion mechanisms in the\nform of neural network propagation. A thought expansion recurrently generates\nthe candidates carrying the thoughts of possible math expressions driven from\nthe previous step and yields reasonable thoughts by selecting the valid\npathways to the goal. Our experiments show that ATHENA achieves a new\nstate-of-the-art stage toward the ideal model that is compelling in variant\nquestions even when the informativeness in training examples is restricted.",
            "author": [
                "JB. Kim",
                "Hazel Kim",
                "Joonghyuk Hahn",
                "Yo-Sub Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01036v1",
                "http://arxiv.org/pdf/2311.01036v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7; I.2.3; F.4.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01033v1",
            "title": "Non-Autoregressive Diffusion-based Temporal Point Processes for\n  Continuous-Time Long-Term Event Prediction",
            "updated": "2023-11-02T06:52:44Z",
            "published": "2023-11-02T06:52:44Z",
            "summary": "Continuous-time long-term event prediction plays an important role in many\napplication scenarios. Most existing works rely on autoregressive frameworks to\npredict event sequences, which suffer from error accumulation, thus\ncompromising prediction quality. Inspired by the success of denoising diffusion\nprobabilistic models, we propose a diffusion-based non-autoregressive temporal\npoint process model for long-term event prediction in continuous time. Instead\nof generating events one at a time in an autoregressive way, our model predicts\nthe future event sequence entirely as a whole. In order to perform diffusion\nprocesses on event sequences, we develop a bidirectional map between target\nevent sequences and the Euclidean vector space. Furthermore, we design a novel\ndenoising network to capture both sequential and contextual features for better\nsample quality. Extensive experiments are conducted to prove the superiority of\nour proposed model over state-of-the-art methods on long-term event prediction\nin continuous time. To the best of our knowledge, this is the first work to\napply diffusion methods to long-term event prediction problems.",
            "author": [
                "Wang-Tao Zhou",
                "Zhao Kang",
                "Ling Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01033v1",
                "http://arxiv.org/pdf/2311.01033v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01030v1",
            "title": "Joint Learning of Local and Global Features for Aspect-based Sentiment\n  Classification",
            "updated": "2023-11-02T06:43:50Z",
            "published": "2023-11-02T06:43:50Z",
            "summary": "Aspect-based sentiment classification (ASC) aims to judge the sentiment\npolarity conveyed by the given aspect term in a sentence. The sentiment\npolarity is not only determined by the local context but also related to the\nwords far away from the given aspect term. Most recent efforts related to the\nattention-based models can not sufficiently distinguish which words they should\npay more attention to in some cases. Meanwhile, graph-based models are coming\ninto ASC to encode syntactic dependency tree information. But these models do\nnot fully leverage syntactic dependency trees as they neglect to incorporate\ndependency relation tag information into representation learning effectively.\nIn this paper, we address these problems by effectively modeling the local and\nglobal features. Firstly, we design a local encoder containing: a Gaussian mask\nlayer and a covariance self-attention layer. The Gaussian mask layer tends to\nadjust the receptive field around aspect terms adaptively to deemphasize the\neffects of unrelated words and pay more attention to local information. The\ncovariance self-attention layer can distinguish the attention weights of\ndifferent words more obviously. Furthermore, we propose a dual-level graph\nattention network as a global encoder by fully employing dependency tag\ninformation to capture long-distance information effectively. Our model\nachieves state-of-the-art performance on both SemEval 2014 and Twitter\ndatasets.",
            "author": [
                "Hao Niu",
                "Yun Xiong",
                "Xiaosu Wang",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01030v1",
                "http://arxiv.org/pdf/2311.01030v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01026v3",
            "title": "A Constant Factor Approximation for Directed Feedback Vertex Set in\n  Graphs of Bounded Genus",
            "updated": "2023-12-06T06:15:03Z",
            "published": "2023-11-02T06:38:26Z",
            "summary": "The minimum directed feedback vertex set problem consists in finding the\nminimum set of vertices that should be removed in order to make a directed\ngraph acyclic. This is a well-known NP-hard optimization problem with\napplications in various fields, such as VLSI chip design, bioinformatics and\ntransaction processing deadlock prevention and node-weighted network design. We\nshow a constant factor approximation for the directed feedback vertex set\nproblem in graphs of bounded genus.",
            "author": [
                "Hao Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01026v3",
                "http://arxiv.org/pdf/2311.01026v3"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "68W25",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01025v1",
            "title": "Incorporating Language-Driven Appearance Knowledge Units with Visual\n  Cues in Pedestrian Detection",
            "updated": "2023-11-02T06:38:19Z",
            "published": "2023-11-02T06:38:19Z",
            "summary": "Large language models (LLMs) have shown their capability in understanding\ncontextual and semantic information regarding appearance knowledge of\ninstances. In this paper, we introduce a novel approach to utilize the strength\nof an LLM in understanding contextual appearance variations and to leverage its\nknowledge into a vision model (here, pedestrian detection). While pedestrian\ndetection is considered one of crucial tasks directly related with our safety\n(e.g., intelligent driving system), it is challenging because of varying\nappearances and poses in diverse scenes. Therefore, we propose to formulate\nlanguage-driven appearance knowledge units and incorporate them with visual\ncues in pedestrian detection. To this end, we establish description corpus\nwhich includes numerous narratives describing various appearances of\npedestrians and others. By feeding them through an LLM, we extract appearance\nknowledge sets that contain the representations of appearance variations. After\nthat, we perform a task-prompting process to obtain appearance knowledge units\nwhich are representative appearance knowledge guided to be relevant to a\ndownstream pedestrian detection task. Finally, we provide plentiful appearance\ninformation by integrating the language-driven knowledge units with visual\ncues. Through comprehensive experiments with various pedestrian detectors, we\nverify the effectiveness of our method showing noticeable performance gains and\nachieving state-of-the-art detection performance.",
            "author": [
                "Sungjune Park",
                "Hyunjun Kim",
                "Yong Man Ro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01025v1",
                "http://arxiv.org/pdf/2311.01025v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01024v1",
            "title": "Distance-Based Propagation for Efficient Knowledge Graph Reasoning",
            "updated": "2023-11-02T06:37:46Z",
            "published": "2023-11-02T06:37:46Z",
            "summary": "Knowledge graph completion (KGC) aims to predict unseen edges in knowledge\ngraphs (KGs), resulting in the discovery of new facts. A new class of methods\nhave been proposed to tackle this problem by aggregating path information.\nThese methods have shown tremendous ability in the task of KGC. However they\nare plagued by efficiency issues. Though there are a few recent attempts to\naddress this through learnable path pruning, they often sacrifice the\nperformance to gain efficiency. In this work, we identify two intrinsic\nlimitations of these methods that affect the efficiency and representation\nquality. To address the limitations, we introduce a new method, TAGNet, which\nis able to efficiently propagate information. This is achieved by only\naggregating paths in a fixed window for each source-target pair. We demonstrate\nthat the complexity of TAGNet is independent of the number of layers. Extensive\nexperiments demonstrate that TAGNet can cut down on the number of propagated\nmessages by as much as 90% while achieving competitive performance on multiple\nKG datasets. The code is available at https://github.com/HarryShomer/TAGNet.",
            "author": [
                "Harry Shomer",
                "Yao Ma",
                "Juanhui Li",
                "Bo Wu",
                "Charu C. Aggarwal",
                "Jiliang Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01024v1",
                "http://arxiv.org/pdf/2311.01024v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01018v1",
            "title": "Expanding Expressiveness of Diffusion Models with Limited Data via\n  Self-Distillation based Fine-Tuning",
            "updated": "2023-11-02T06:24:06Z",
            "published": "2023-11-02T06:24:06Z",
            "summary": "Training diffusion models on limited datasets poses challenges in terms of\nlimited generation capacity and expressiveness, leading to unsatisfactory\nresults in various downstream tasks utilizing pretrained diffusion models, such\nas domain translation and text-guided image manipulation. In this paper, we\npropose Self-Distillation for Fine-Tuning diffusion models (SDFT), a\nmethodology to address these challenges by leveraging diverse features from\ndiffusion models pretrained on large source datasets. SDFT distills more\ngeneral features (shape, colors, etc.) and less domain-specific features\n(texture, fine details, etc) from the source model, allowing successful\nknowledge transfer without disturbing the training process on target datasets.\nThe proposed method is not constrained by the specific architecture of the\nmodel and thus can be generally adopted to existing frameworks. Experimental\nresults demonstrate that SDFT enhances the expressiveness of the diffusion\nmodel with limited datasets, resulting in improved generation capabilities\nacross various downstream tasks.",
            "author": [
                "Jiwan Hur",
                "Jaehyun Choi",
                "Gyojin Han",
                "Dong-Jae Lee",
                "Junmo Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01018v1",
                "http://arxiv.org/pdf/2311.01018v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01015v1",
            "title": "Act As You Wish: Fine-Grained Control of Motion Diffusion Model with\n  Hierarchical Semantic Graphs",
            "updated": "2023-11-02T06:20:23Z",
            "published": "2023-11-02T06:20:23Z",
            "summary": "Most text-driven human motion generation methods employ sequential modeling\napproaches, e.g., transformer, to extract sentence-level text representations\nautomatically and implicitly for human motion synthesis. However, these compact\ntext representations may overemphasize the action names at the expense of other\nimportant properties and lack fine-grained details to guide the synthesis of\nsubtly distinct motion. In this paper, we propose hierarchical semantic graphs\nfor fine-grained control over motion generation. Specifically, we disentangle\nmotion descriptions into hierarchical semantic graphs including three levels of\nmotions, actions, and specifics. Such global-to-local structures facilitate a\ncomprehensive understanding of motion description and fine-grained control of\nmotion generation. Correspondingly, to leverage the coarse-to-fine topology of\nhierarchical semantic graphs, we decompose the text-to-motion diffusion process\ninto three semantic levels, which correspond to capturing the overall motion,\nlocal actions, and action specifics. Extensive experiments on two benchmark\nhuman motion datasets, including HumanML3D and KIT, with superior performances,\njustify the efficacy of our method. More encouragingly, by modifying the edge\nweights of hierarchical semantic graphs, our method can continuously refine the\ngenerated motion, which may have a far-reaching impact on the community. Code\nand pre-training weights are available at\nhttps://github.com/jpthu17/GraphMotion.",
            "author": [
                "Peng Jin",
                "Yang Wu",
                "Yanbo Fan",
                "Zhongqian Sun",
                "Yang Wei",
                "Li Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01015v1",
                "http://arxiv.org/pdf/2311.01015v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01013v1",
            "title": "Evaluation Measures of Individual Item Fairness for Recommender Systems:\n  A Critical Study",
            "updated": "2023-11-02T06:15:49Z",
            "published": "2023-11-02T06:15:49Z",
            "summary": "Fairness is an emerging and challenging topic in recommender systems. In\nrecent years, various ways of evaluating and therefore improving fairness have\nemerged. In this study, we examine existing evaluation measures of fairness in\nrecommender systems. Specifically, we focus solely on exposure-based fairness\nmeasures of individual items that aim to quantify the disparity in how\nindividual items are recommended to users, separate from item relevance to\nusers. We gather all such measures and we critically analyse their theoretical\nproperties. We identify a series of limitations in each of them, which\ncollectively may render the affected measures hard or impossible to interpret,\nto compute, or to use for comparing recommendations. We resolve these\nlimitations by redefining or correcting the affected measures, or we argue why\ncertain limitations cannot be resolved. We further perform a comprehensive\nempirical analysis of both the original and our corrected versions of these\nfairness measures, using real-world and synthetic datasets. Our analysis\nprovides novel insights into the relationship between measures based on\ndifferent fairness concepts, and different levels of measure sensitivity and\nstrictness. We conclude with practical suggestions of which fairness measures\nshould be used and when. Our code is publicly available. To our knowledge, this\nis the first critical comparison of individual item fairness measures in\nrecommender systems.",
            "author": [
                "Theresia Veronika Rampisela",
                "Maria Maistro",
                "Tuukka Ruotsalo",
                "Christina Lioma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01013v1",
                "http://arxiv.org/pdf/2311.01013v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01011v1",
            "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
            "updated": "2023-11-02T06:13:36Z",
            "published": "2023-11-02T06:13:36Z",
            "summary": "While Large Language Models (LLMs) are increasingly being used in real-world\napplications, they remain vulnerable to prompt injection attacks: malicious\nthird party prompts that subvert the intent of the system designer. To help\nresearchers study this problem, we present a dataset of over 126,000 prompt\ninjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\nall created by players of an online game called Tensor Trust. To the best of\nour knowledge, this is currently the largest dataset of human-generated\nadversarial examples for instruction-following LLMs. The attacks in our dataset\nhave a lot of easily interpretable stucture, and shed light on the weaknesses\nof LLMs. We also use the dataset to create a benchmark for resistance to two\ntypes of prompt injection, which we refer to as prompt extraction and prompt\nhijacking. Our benchmark results show that many models are vulnerable to the\nattack strategies in the Tensor Trust dataset. Furthermore, we show that some\nattack strategies from the dataset generalize to deployed LLM-based\napplications, even though they have a very different set of constraints to the\ngame. We release all data and source code at https://tensortrust.ai/paper",
            "author": [
                "Sam Toyer",
                "Olivia Watkins",
                "Ethan Adrian Mendes",
                "Justin Svegliato",
                "Luke Bailey",
                "Tiffany Wang",
                "Isaac Ong",
                "Karim Elmaaroufi",
                "Pieter Abbeel",
                "Trevor Darrell",
                "Alan Ritter",
                "Stuart Russell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01011v1",
                "http://arxiv.org/pdf/2311.01011v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01008v1",
            "title": "On Linear Complementary Pairs of Algebraic Geometry Codes over Finite\n  Fields",
            "updated": "2023-11-02T06:02:02Z",
            "published": "2023-11-02T06:02:02Z",
            "summary": "Linear complementary dual (LCD) codes and linear complementary pairs (LCP) of\ncodes have been proposed for new applications as countermeasures against\nside-channel attacks (SCA) and fault injection attacks (FIA) in the context of\ndirect sum masking (DSM). The countermeasure against FIA may lead to a\nvulnerability for SCA when the whole algorithm needs to be masked (in\nenvironments like smart cards). This led to a variant of the LCD and LCP\nproblems, where several results have been obtained intensively for LCD codes,\nbut only partial results have been derived for LCP codes. Given the gap between\nthe thin results and their particular importance, this paper aims to reduce\nthis by further studying the LCP of codes in special code families and,\nprecisely, the characterisation and construction mechanism of LCP codes of\nalgebraic geometry codes over finite fields. Notably, we propose constructing\nexplicit LCP of codes from elliptic curves. Besides, we also study the security\nparameters of the derived LCP of codes $(\\mathcal{C}, \\mathcal{D})$ (notably\nfor cyclic codes), which are given by the minimum distances $d(\\mathcal{C})$\nand $d(\\mathcal{D}^\\perp)$. Further, we show that for LCP algebraic geometry\ncodes $(\\mathcal{C},\\mathcal{D})$, the dual code $\\mathcal{C}^\\perp$ is\nequivalent to $\\mathcal{D}$ under some specific conditions we exhibit. Finally,\nwe investigate whether MDS LCP of algebraic geometry codes exist (MDS codes are\namong the most important in coding theory due to their theoretical significance\nand practical interests). Construction schemes for obtaining LCD codes from any\nalgebraic curve were given in 2018 by Mesnager, Tang and Qi in [``Complementary\ndual algebraic geometry codes\", IEEE Trans. Inform Theory, vol. 64(4),\n2390--3297, 2018]. To our knowledge, it is the first time LCP of algebraic\ngeometry codes has been studied.",
            "author": [
                "Sanjit Bhowmick",
                "Deepak Kumar Dalai",
                "Sihem Mesnager"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01008v1",
                "http://arxiv.org/pdf/2311.01008v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01478v1",
            "title": "Adversary ML Resilience in Autonomous Driving Through Human Centered\n  Perception Mechanisms",
            "updated": "2023-11-02T04:11:45Z",
            "published": "2023-11-02T04:11:45Z",
            "summary": "Physical adversarial attacks on road signs are continuously exploiting\nvulnerabilities in modern day autonomous vehicles (AVs) and impeding their\nability to correctly classify what type of road sign they encounter. Current\nmodels cannot generalize input data well, resulting in overfitting or\nunderfitting. In overfitting, the model memorizes the input data but cannot\ngeneralize to new scenarios. In underfitting, the model does not learn enough\nof the input data to accurately classify these road signs. This paper explores\nthe resilience of autonomous driving systems against three main physical\nadversarial attacks (tape, graffiti, illumination), specifically targeting\nobject classifiers. Several machine learning models were developed and\nevaluated on two distinct datasets: road signs (stop signs, speed limit signs,\ntraffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons,\ncircles, squares, and triangles). The study compared algorithm performance\nunder different conditions, including clean and adversarial training and\ntesting on these datasets. To build robustness against attacks, defense\ntechniques like adversarial training and transfer learning were implemented.\nResults demonstrated transfer learning models played a crucial role in\nperformance by allowing knowledge gained from shape training to improve\ngeneralizability of road sign classification, despite the datasets being\ncompletely different. The paper suggests future research directions, including\nhuman-in-the-loop validation, security analysis, real-world testing, and\nexplainable AI for transparency. This study aims to contribute to improving\nsecurity and robustness of object classifiers in autonomous vehicles and\nmitigating adversarial example impacts on driving systems.",
            "author": [
                "Aakriti Shah"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01478v1",
                "http://arxiv.org/pdf/2311.01478v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "68",
                "I.4.0"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04916v1",
            "title": "Explainable Identification of Hate Speech towards Islam using Graph\n  Neural Networks",
            "updated": "2023-11-02T04:01:04Z",
            "published": "2023-11-02T04:01:04Z",
            "summary": "Islamophobic language is a prevalent challenge on online social interaction\nplatforms. Identifying and eliminating such hatred is a crucial step towards a\nfuture of harmony and peace. This study presents a novel paradigm for\nidentifying and explaining hate speech towards Islam using graph neural\nnetworks. Utilizing the intrinsic ability of graph neural networks to find,\nextract, and use relationships across disparate data points, our model\nconsistently achieves outstanding performance while offering explanations for\nthe underlying correlations and causation.",
            "author": [
                "Azmine Toushik Wasi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04916v1",
                "http://arxiv.org/pdf/2311.04916v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00965v1",
            "title": "On Negative Correlation of Arboreal Gas on Some Graphs",
            "updated": "2023-11-02T03:21:10Z",
            "published": "2023-11-02T03:21:10Z",
            "summary": "Arboreal Gas is a type of (unrooted) random forest on a graph, where the\nprobability is determined by a parameter $\\beta>0$ per edge. This model is\nessentially equivalent to acyclic Bernoulli bond percolation with a parameter\n$p=\\beta/(1+\\beta)$. Additionally, Arboreal Gas can be considered as the limit\nof the $q$-states random cluster model with $p=\\beta q$ as $q\\to 0$. A natural\nquestion arises regarding the existence and performance of the weak limit of\nArboreal Gas as the graph size goes to infinity. The answer to this question\nrelies on the negative correlation of Arboreal Gas, which is still an open\nproblem. This paper primarily focuses on the negative correlation of Arboreal\nGas and provides some results for specific graphs.",
            "author": [
                "Xiangyu Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00965v1",
                "http://arxiv.org/pdf/2311.00965v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00954v1",
            "title": "On the Free Boundary Problems of 3-D Compressible Euler Equations\n  Coupled or Uncoupled With a Nonlinear Poisson Equation",
            "updated": "2023-11-02T02:46:01Z",
            "published": "2023-11-02T02:46:01Z",
            "summary": "For the problem of the non-isentropic compressible Euler Equations coupled\nwith a nonlinear Poisson equation with the electric potential satisfying the\nDirichlet boundary condition in three spatial dimensions with a general free\nboundary not restricting to a graph, we identify suitable stability conditions\non the electric potential and the pressure under which we obtain a priori\nestimates on the Sobolev norms of the fluid and electric variables and bounds\nfor geometric quantities of free surface. The stability conditions in this case\nfor a general variable entropy are that the outer normal derivative of the\nelectric potential is positive on the free surface, whereas that on the\npressure is negative. In the isentropic case, the stability condition reduces\nto a single one, the outer normal derivative of the difference of the enthalpy\nand the electric potential is negative on the free surface. For the free\nboundary problem of the non-isentropic compressible Euler equations with\nvariable entropy without coupling with the nonlinear Poisson equation, the\ncorresponding higher-order estimates are also obtained under the Taylor sign\ncondition. It is also found that one less derivative is needed to close the\nenergy estimates for the problem for the non-isentropic compressible Euler\nEquations coupled with a nonlinear Poisson equation when the electric potential\nsatisfies the Dirichlet boundary condition under the stability conditions on\nthe electric potential and the pressure, compared with the problem of the\nnon-isentropic compressible Euler equations.",
            "author": [
                "Tao Luo",
                "Konstantina Trivisa",
                "Huihui Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00954v1",
                "http://arxiv.org/pdf/2311.00954v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00953v1",
            "title": "Blending Reward Functions via Few Expert Demonstrations for Faithful and\n  Accurate Knowledge-Grounded Dialogue Generation",
            "updated": "2023-11-02T02:42:41Z",
            "published": "2023-11-02T02:42:41Z",
            "summary": "The development of trustworthy conversational information-seeking systems\nrelies on dialogue models that can generate faithful and accurate responses\nbased on relevant knowledge texts. However, two main challenges hinder this\ntask. Firstly, language models may generate hallucinations due to data biases\npresent in their pretraining corpus. Secondly, knowledge texts often contain\nredundant and irrelevant information that distracts the model's attention from\nthe relevant text span. Previous works use additional data annotations on the\nknowledge texts to learn a knowledge identification module in order to bypass\nirrelevant information, but collecting such high-quality span annotations can\nbe costly. In this work, we leverage reinforcement learning algorithms to\novercome the above challenges by introducing a novel reward function. Our\nreward function combines an accuracy metric and a faithfulness metric to\nprovide a balanced quality judgment of generated responses, which can be used\nas a cost-effective approximation to a human preference reward model when only\na few preference annotations are available. Empirical experiments on two\nconversational information-seeking datasets demonstrate that our method can\ncompete with other strong supervised learning baselines.",
            "author": [
                "Wanyu Du",
                "Yangfeng Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00953v1",
                "http://arxiv.org/pdf/2311.00953v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00950v1",
            "title": "A robust version of the multipartite Hajnal--Szemer\u00e9di theorem",
            "updated": "2023-11-02T02:33:35Z",
            "published": "2023-11-02T02:33:35Z",
            "summary": "In this note we show the following strengthening of a multipartite version of\nthe Hajnal--Szemer\\'edi theorem. For an integer $r \\ge 3$ and $\\gamma > 0$,\nthere exists a constant $C$ such that if $p\\ge Cn^{-2/r}(\\log n)^{1/{r \\choose\n2}}$ and $G$ is a balanced $r$-partite graph with each vertex class of size $n$\nand $\\delta^\\ast(G)\\ge (1-1/r+\\gamma)n$, then with high probability the random\nsubgraph $G(p)$ of $G$ contains a $K_r$-factor. We also use it to derive\ncorresponding transversal versions.",
            "author": [
                "Jie Han",
                "Jie Hu",
                "Donglei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00950v1",
                "http://arxiv.org/pdf/2311.00950v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00943v1",
            "title": "Sound Call Graph Construction for Java Object Deserialization",
            "updated": "2023-11-02T02:07:54Z",
            "published": "2023-11-02T02:07:54Z",
            "summary": "Object serialization and deserialization is widely used for storing and\npreserving objects in files, memory, or database as well as for transporting\nthem across machines, enabling remote interaction among processes and many\nmore. This mechanism relies on reflection, a dynamic language that introduces\nserious challenges for static analyses. Current state-of-the-art call graph\nconstruction algorithms does not fully support object\nserialization/deserialization, i.e., they are unable to uncover the callback\nmethods that are invoked when objects are serialized and deserialized. Since\ncall graphs are a core data structure for multiple type of analysis (e.g.,\nvulnerability detection), an appropriate analysis cannot be performed since the\ncall graph does not capture hidden (vulnerable) paths that occur via callback\nmethods. In this paper, we present Seneca, an approach for handling\nserialization with improved soundness in the context of call graph\nconstruction. Our approach relies on taint analysis and API modeling to\nconstruct sound call graphs. We evaluated our approach with respect to\nsoundness, precision, performance, and usefulness in detecting untrusted object\ndeserialization vulnerabilities. Our results show that Seneca can create sound\ncall graphs with respect to serialization features. The resulting call graphs\ndo not incur significant overhead and were shown to be useful for performing\nidentification of vulnerable paths caused by untrusted object deserialization.",
            "author": [
                "Joanna C. S. Santos",
                "Mehdi Mirakhorli",
                "Ali Shokri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00943v1",
                "http://arxiv.org/pdf/2311.00943v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00940v1",
            "title": "Dynamic Uploading Scheduling in mmWave-Based Sensor Networks via Mobile\n  Blocker Detection",
            "updated": "2023-11-02T02:04:58Z",
            "published": "2023-11-02T02:04:58Z",
            "summary": "The freshness of information, measured as Age of Information (AoI), is\ncritical for many applications in next-generation wireless sensor networks\n(WSNs). Due to its high bandwidth, millimeter wave (mmWave) communication is\nseen to be frequently exploited in WSNs to facilitate the deployment of\nbandwidth-demanding applications. However, the vulnerability of mmWave to user\nmobility typically results in link blockage and thus postponed real-time\ncommunications. In this paper, joint sampling and uploading scheduling in an\nAoI-oriented WSN working in mmWave band is considered, where a single human\nblocker is moving randomly and signal propagation paths may be blocked. The\nlocations of signal reflectors and the real-time position of the blocker can be\ndetected via wireless sensing technologies. With the knowledge of blocker\nmotion pattern, the statistics of future wireless channels can be predicted. As\na result, the AoI degradation arising from link blockage can be forecast and\nmitigated. Specifically, we formulate the long-term sampling, uplink\ntransmission time and power allocation as an infinite-horizon Markov decision\nprocess (MDP) with discounted cost. Due to the curse of dimensionality, the\noptimal solution is infeasible. A novel low-complexity solution framework with\nguaranteed performance in the worst case is proposed where the forecast of link\nblockage is exploited in a value function approximation. Simulations show that\ncompared with several heuristic benchmarks, our proposed policy, benefiting\nfrom the awareness of link blockage, can reduce average cost up to 49.6%.",
            "author": [
                "Yifei Sun",
                "Bojie Lv",
                "Rui Wang",
                "Haisheng Tan",
                "Francis C. M. Lau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00940v1",
                "http://arxiv.org/pdf/2311.00940v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00939v1",
            "title": "Accelerated Data-Driven Discovery and Screening of Two-Dimensional\n  Magnets Using Graph Neural Networks",
            "updated": "2023-11-02T02:03:30Z",
            "published": "2023-11-02T02:03:30Z",
            "summary": "Two-dimensional (2D) magnets have transformative potential in spintronics\napplications. In this study, we use Graph Neural Networks (GNNs) to accelerate\nthe discovery of novel 2D magnetic materials. Using data from the Materials\nProject database and the Computational 2D materials database (C2DB), we train\nthree GNN architectures on a dataset of 1190 magnetic monolayers with energy\nabove the convex hull $E_{\\text{hull}}$ less than 0.3 eV/atom. Our Crystal\nDiffusion Variational Auto Encoder (CDVAE) generates around 11,000 material\ncandidates. Subsequent training on two Atomistic Line Graph Neural Networks\n(ALIGNN) achieves a 93$\\%$ accuracy in predicting magnetic monolayers and a\nmean average error of 0.039 eV/atom for $E_{\\text{hull}}$ predictions. After\nnarrowing down candidates based on magnetic likelihood and predicted energy,\nand constraining the atom count in the monolayer to four or fewer, we\nidentified 158 candidates. These are validated using Density-Functional Theory\n(DFT) to confirm their magnetic and energetic favorability resulting in 150\nmaterials magnetic monolayer with $E_{\\text{hull}} < 0.3$ eV/atom. Our\nmethodology offers a way to accelerate exploring and predicting potential 2D\nmagnetic materials, contributing to the ongoing computational and experimental\nefforts aimed at the discovery of new 2D magnets.",
            "author": [
                "Ahmed Elrashidy",
                "James Della-Giustina",
                "Jia-An Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00939v1",
                "http://arxiv.org/pdf/2311.00939v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00936v1",
            "title": "SatBird: Bird Species Distribution Modeling with Remote Sensing and\n  Citizen Science Data",
            "updated": "2023-11-02T02:00:27Z",
            "published": "2023-11-02T02:00:27Z",
            "summary": "Biodiversity is declining at an unprecedented rate, impacting ecosystem\nservices necessary to ensure food, water, and human health and well-being.\nUnderstanding the distribution of species and their habitats is crucial for\nconservation policy planning. However, traditional methods in ecology for\nspecies distribution models (SDMs) generally focus either on narrow sets of\nspecies or narrow geographical areas and there remain significant knowledge\ngaps about the distribution of species. A major reason for this is the limited\navailability of data traditionally used, due to the prohibitive amount of\neffort and expertise required for traditional field monitoring. The wide\navailability of remote sensing data and the growing adoption of citizen science\ntools to collect species observations data at low cost offer an opportunity for\nimproving biodiversity monitoring and enabling the modelling of complex\necosystems. We introduce a novel task for mapping bird species to their\nhabitats by predicting species encounter rates from satellite images, and\npresent SatBird, a satellite dataset of locations in the USA with labels\nderived from presence-absence observation data from the citizen science\ndatabase eBird, considering summer (breeding) and winter seasons. We also\nprovide a dataset in Kenya representing low-data regimes. We additionally\nprovide environmental data and species range maps for each location. We\nbenchmark a set of baselines on our dataset, including SOTA models for remote\nsensing tasks. SatBird opens up possibilities for scalably modelling properties\nof ecosystems worldwide.",
            "author": [
                "M\u00e9lisande Teng",
                "Amna Elmustafa",
                "Benjamin Akera",
                "Yoshua Bengio",
                "Hager Radi Abdelwahed",
                "Hugo Larochelle",
                "David Rolnick"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00936v1",
                "http://arxiv.org/pdf/2311.00936v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00917v1",
            "title": "RPCANet: Deep Unfolding RPCA Based Infrared Small Target Detection",
            "updated": "2023-11-02T01:21:12Z",
            "published": "2023-11-02T01:21:12Z",
            "summary": "Deep learning (DL) networks have achieved remarkable performance in infrared\nsmall target detection (ISTD). However, these structures exhibit a deficiency\nin interpretability and are widely regarded as black boxes, as they disregard\ndomain knowledge in ISTD. To alleviate this issue, this work proposes an\ninterpretable deep network for detecting infrared dim targets, dubbed RPCANet.\nSpecifically, our approach formulates the ISTD task as sparse target\nextraction, low-rank background estimation, and image reconstruction in a\nrelaxed Robust Principle Component Analysis (RPCA) model. By unfolding the\niterative optimization updating steps into a deep-learning framework,\ntime-consuming and complex matrix calculations are replaced by theory-guided\nneural networks. RPCANet detects targets with clear interpretability and\npreserves the intrinsic image feature, instead of directly transforming the\ndetection task into a matrix decomposition problem. Extensive experiments\nsubstantiate the effectiveness of our deep unfolding framework and demonstrate\nits trustworthy results, surpassing baseline methods in both qualitative and\nquantitative evaluations.",
            "author": [
                "Fengyi Wu",
                "Tianfang Zhang",
                "Lei Li",
                "Yian Huang",
                "Zhenming Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00917v1",
                "http://arxiv.org/pdf/2311.00917v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00915v1",
            "title": "Task-Agnostic Low-Rank Adapters for Unseen English Dialects",
            "updated": "2023-11-02T01:17:29Z",
            "published": "2023-11-02T01:17:29Z",
            "summary": "Large Language Models (LLMs) are trained on corpora disproportionally\nweighted in favor of Standard American English. As a result, speakers of other\ndialects experience significantly more failures when interacting with these\ntechnologies. In practice, these speakers often accommodate their speech to be\nbetter understood. Our work shares the belief that language technologies should\nbe designed to accommodate the diversity in English dialects and not the other\nway around. However, prior works on dialect struggle with generalizing to\nevolving and emerging dialects in a scalable manner. To fill this gap, our\nmethod, HyperLoRA, leverages expert linguistic knowledge to enable\nresource-efficient adaptation via hypernetworks. By disentangling\ndialect-specific and cross-dialectal information, HyperLoRA improves\ngeneralization to unseen dialects in a task-agnostic fashion. Not only is\nHyperLoRA more scalable in the number of parameters, but it also achieves the\nbest or most competitive performance across 5 dialects in a zero-shot setting.\nIn this way, our approach facilitates access to language technology for\nbillions of English dialect speakers who are traditionally underrepresented.",
            "author": [
                "Zedian Xiao",
                "William Held",
                "Yanchen Liu",
                "Diyi Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00915v1",
                "http://arxiv.org/pdf/2311.00915v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00908v1",
            "title": "How Real is Incomputability in Physics?",
            "updated": "2023-11-02T00:44:40Z",
            "published": "2023-11-02T00:44:40Z",
            "summary": "A physical system is determined by a finite set of initial conditions and\nlaws represented by equations. The system is computable if we can solve the\nequations in all instances using a ``finite body of mathematical knowledge\". In\nthis case, if the laws of the system can be coded into a computer program, then\ngiven the system's initial conditions of the system, one can compute the\nsystem's evolution. This scenario is tacitly taken for granted. But is this\nreasonable? The answer is negative, and a straightforward example is when the\ninitial conditions or equations use irrational numbers, like Chaitin's Omega\nNumber: no program can deal with such numbers because of their ``infinity''.\nAre there incomputable physical systems? This question has been theoretically\nstudied in the last 30--40 years. This article presents a class of quantum\nprotocols producing quantum random bits. Theoretically, we prove that every\ninfinite sequence generated by these quantum protocols is strongly incomputable\n-- no algorithm computing any bit of such a sequence can be proved correct.\nThis theoretical result is not only more robust than the ones in the\nliterature: experimental results support and complement it.",
            "author": [
                "Jos\u00e9 Manuel Ag\u00fcero Trejo",
                "Cristian S. Calude",
                "Michael J. Dinneen",
                "Arkady Fedorov",
                "Anatoly Kulikov",
                "Rohit Navarathna",
                "Karl Svozil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00908v1",
                "http://arxiv.org/pdf/2311.00908v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.02103v1",
            "title": "Relax: Composable Abstractions for End-to-End Dynamic Machine Learning",
            "updated": "2023-11-01T23:03:59Z",
            "published": "2023-11-01T23:03:59Z",
            "summary": "Dynamic shape computations have become critical in modern machine learning\nworkloads, especially in emerging large language models. The success of these\nmodels has driven demand for deploying them to a diverse set of backend\nenvironments. In this paper, we present Relax, a compiler abstraction for\noptimizing end-to-end dynamic machine learning workloads. Relax introduces\nfirst-class symbolic shape annotations to track dynamic shape computations\nglobally across the program. It also introduces a cross-level abstraction that\nencapsulates computational graphs, loop-level tensor programs, and library\ncalls in a single representation to enable cross-level optimizations. We build\nan end-to-end compilation framework using the proposed approach to optimize\ndynamic shape models. Experimental results on large language models show that\nRelax delivers performance competitive with state-of-the-art hand-optimized\nsystems across platforms and enables deployment of emerging dynamic models to a\nbroader set of environments, including mobile phones, embedded devices, and web\nbrowsers.",
            "author": [
                "Ruihang Lai",
                "Junru Shao",
                "Siyuan Feng",
                "Steven S. Lyubomirsky",
                "Bohan Hou",
                "Wuwei Lin",
                "Zihao Ye",
                "Hongyi Jin",
                "Yuchen Jin",
                "Jiawei Liu",
                "Lesheng Jin",
                "Yaxing Cai",
                "Ziheng Jiang",
                "Yong Wu",
                "Sunghyun Park",
                "Prakalp Srivastava",
                "Jared G. Roesch",
                "Todd C. Mowry",
                "Tianqi Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.02103v1",
                "http://arxiv.org/pdf/2311.02103v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00882v2",
            "title": "Semidefinite programming and linear equations vs. homomorphism problems",
            "updated": "2023-11-17T05:02:40Z",
            "published": "2023-11-01T22:15:19Z",
            "summary": "We introduce a relaxation for homomorphism problems that combines\nsemidefinite programming with linear Diophantine equations, and propose a\nframework for the analysis of its power based on the spectral theory of\nassociation schemes. We use this framework to establish an unconditional lower\nbound against the semidefinite programming + linear equations model, by showing\nthat the relaxation does not solve the approximate graph homomorphism problem\nand thus, in particular, the approximate graph colouring problem.",
            "author": [
                "Lorenzo Ciardo",
                "Stanislav \u017divn\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00882v2",
                "http://arxiv.org/pdf/2311.00882v2"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DM",
                "cs.DS",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00874v1",
            "title": "Huber Loss-Based Penalty Approach to Problems with Linear Constraints",
            "updated": "2023-11-01T22:00:44Z",
            "published": "2023-11-01T22:00:44Z",
            "summary": "We consider a convex optimization problem with many linear inequality\nconstraints. To deal with a large number of constraints, we provide a penalty\nreformulation of the problem, where the penalty is a variant of the one-sided\nHuber loss function with two penalty parameters. We study the infeasibility\nproperties of the solutions of penalized problems for nonconvex and convex\nobjective functions, as the penalty parameters vary with time. Then, we propose\na random incremental penalty method for solving the original problem, and\ninvestigate its convergence properties for convex and strongly convex objective\nfunctions. We show that the iterates of the method converge to a solution of\nthe original problem almost surely and in expectation for suitable choices of\nthe penalty parameters and the stepsize. Also, we establish convergence rate of\nthe method in terms of the expected function values by utilizing appropriately\ndefined weighted averages of the iterates. We show $O(\\ln^{1/2+\\epsilon}\nk/{\\sqrt k})$-convergence rate when the objective function is convex and\n$O(\\ln^{\\epsilon} k/k)$-convergence rate when the objective function is\nstrongly convex, with $\\epsilon>0$ being an arbitrarily small scalar. } To the\nbest of our knowledge, these are the first results on the convergence rate for\nthe penalty-based incremental subgradient method with time-varying penalty\nparameters.",
            "author": [
                "Angelia Nedich",
                "Tatiana Tatarenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00874v1",
                "http://arxiv.org/pdf/2311.00874v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00873v1",
            "title": "Low-latency Real-time Voice Conversion on CPU",
            "updated": "2023-11-01T21:57:52Z",
            "published": "2023-11-01T21:57:52Z",
            "summary": "We adapt the architectures of previous audio manipulation and generation\nneural networks to the task of real-time any-to-one voice conversion. Our\nresulting model, LLVC ($\\textbf{L}$ow-latency $\\textbf{L}$ow-resource\n$\\textbf{V}$oice $\\textbf{C}$onversion), has a latency of under 20ms at a\nbitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU.\nLLVC uses both a generative adversarial architecture as well as knowledge\ndistillation in order to attain this performance. To our knowledge LLVC\nachieves both the lowest resource usage as well as the lowest latency of any\nopen-source voice conversion model. We provide open-source samples, code, and\npretrained model weights at https://github.com/KoeAI/LLVC.",
            "author": [
                "Konstantine Sadov",
                "Matthew Hutter",
                "Asara Near"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00873v1",
                "http://arxiv.org/pdf/2311.00873v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00869v1",
            "title": "Scaling Frustration Index and Corresponding Balanced State Discovery for\n  Real Signed Graphs",
            "updated": "2023-11-01T21:37:46Z",
            "published": "2023-11-01T21:37:46Z",
            "summary": "Structural balance modeling for signed graph networks presents how to model\nthe sources of conflicts. The state-of-the-art has focused on computing the\nfrustration index of a signed graph as a critical step toward solving problems\nin social and sensor networks and for scientific modeling. However, the\nproposed approaches do not scale to modern large, sparse signed networks. Also,\nthey do not address that there is more than one way in some networks to reach a\nconsensus with the minimum number of edge-sign switches needed. We propose an\nefficient balanced state discovery algorithm and a network frustration\ncomputation that will discover the nearest balanced state for the \\emph{any}\nsize of the graph network and compute the frustration of the network. The\nspeedup of the proposed method is around 300 times faster than the\nstate-of-the-art for signed graphs with hundreds of thousands of edges. The\ntechnique successfully scales to find the balanced states and frustration of\nthe networks with millions of nodes and edges in real time where\nstate-of-the-art fails.",
            "author": [
                "Muhieddine Shebaro",
                "Jelena Te\u0161i\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00869v1",
                "http://arxiv.org/pdf/2311.00869v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00868v1",
            "title": "Towards a complete classification of 6D supergravities",
            "updated": "2023-11-01T21:37:03Z",
            "published": "2023-11-01T21:37:03Z",
            "summary": "The constraints arising from anomaly cancellation are particular strong for\nchiral theories in six dimensions. We make progress towards a complete\nclassification of 6D supergravities with minimal supersymmetry and non-abelian\ngauge group. First, we generalize a previously known infinite class of\nanomaly-free theories which has $T\\gg 9$ to essentially any semi-simple gauge\ngroup and infinitely many choices for hypermultiplets. The construction relies\non having many decoupled sectors all selected from a list of four simple\ntheories which we identify. Second, we use ideas from graph theory to rephrase\nthe task of finding anomaly-free theories as constructing cliques in a certain\nmultigraph. A branch-and-bound type algorithm is described which can be used to\nexplicitly construct, in a $T$-independent way, anomaly-free theories with an\narbitrary number of simple factors in the gauge group. We implement these ideas\nto generate an ensemble of $\\mathcal{O}(10^7)$ irreducible cliques from which\nanomaly-free theories may be easily built, and as a special case obtain a\ncomplete list of $19,\\!847$ consistent theories for $T=0$, for which the\nmaximal gauge group rank is $24$. Modulo the new infinite families, we give a\ncomplete characterization of anomaly-free theories and show that the bound\n$T\\leq 273$ is sharp.",
            "author": [
                "Yuta Hamada",
                "Gregory J. Loges"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00868v1",
                "http://arxiv.org/pdf/2311.00868v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00860v2",
            "title": "Zero Coordinate Shift: Whetted Automatic Differentiation for\n  Physics-informed Operator Learning",
            "updated": "2023-11-23T19:41:41Z",
            "published": "2023-11-01T21:28:24Z",
            "summary": "Automatic differentiation (AD) is a critical step in physics-informed machine\nlearning, required for computing the high-order derivatives of network output\nw.r.t. coordinates of collocation points. In this paper, we present a novel and\nlightweight algorithm to conduct AD for physics-informed operator learning,\nwhich we call the trick of Zero Coordinate Shift (ZCS). Instead of making all\nsampled coordinates as leaf variables, ZCS introduces only one scalar-valued\nleaf variable for each spatial or temporal dimension, simplifying the wanted\nderivatives from \"many-roots-many-leaves\" to \"one-root-many-leaves\" whereby\nreverse-mode AD becomes directly utilisable. It has led to an outstanding\nperformance leap by avoiding the duplication of the computational graph along\nthe dimension of functions (physical parameters). ZCS is easy to implement with\ncurrent deep learning libraries; our own implementation is achieved by\nextending the DeepXDE package. We carry out a comprehensive benchmark analysis\nand several case studies, training physics-informed DeepONets to solve partial\ndifferential equations (PDEs) without data. The results show that ZCS has\npersistently reduced GPU memory consumption and wall time for training by an\norder of magnitude, and such reduction factor scales with the number of\nfunctions. As a low-level optimisation technique, ZCS imposes no restrictions\non data, physics (PDE) or network architecture and does not compromise training\nresults from any aspect.",
            "author": [
                "Kuangdai Leng",
                "Mallikarjun Shankar",
                "Jeyan Thiyagalingam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00860v2",
                "http://arxiv.org/pdf/2311.00860v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "math.NA",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00857v1",
            "title": "Asymmetric Ramsey properties of randomly perturbed graphs",
            "updated": "2023-11-01T21:22:23Z",
            "published": "2023-11-01T21:22:23Z",
            "summary": "In this note, we investigate for various pairs of graphs $(H,G)$ the question\nof how many random edges must be added to a dense graph to guarantee that any\nred-blue coloring of the edges contains a red copy of $H$ or a blue copy of\n$G$. We determine this perturbed Ramsey threshold for many new pairs of graphs\nand various ranges of densities, obtaining several generalizations of results\nobtained by Das and Treglown. In particular, we resolve the remaining cases\ntoward determining the perturbed Ramsey threshold for pairs $(K_t,K_s)$ where\n$t\\geq s\\geq 5$.",
            "author": [
                "Emily Heath",
                "Daniel McGinnis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00857v1",
                "http://arxiv.org/pdf/2311.00857v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C55, 05C80"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00854v2",
            "title": "$2$-Fault-Tolerant Strong Connectivity Oracles",
            "updated": "2023-12-02T22:00:44Z",
            "published": "2023-11-01T21:13:39Z",
            "summary": "We study the problem of efficiently answering strong connectivity queries\nunder two vertex failures. Given a directed graph $G$ with $n$ vertices, we\nprovide a data structure with $O(nh)$ space and $O(h)$ query time, where $h$ is\nthe height of a decomposition tree of $G$ into strongly connected subgraphs.\nThis immediately implies data structures with $O(n \\log{n})$ space and\n$O(\\log{n})$ query time for graphs of constant treewidth, and $O(n^{3/2})$\nspace and $O(\\sqrt{n})$ query time for planar graphs. For general directed\ngraphs, we give a refined version of our data structure that achieves\n$O(n\\sqrt{m})$ space and $O(\\sqrt{m})$ query time, where $m$ is the number of\nedges of the graph. We also provide some simple BFS-based heuristics that seem\nto work remarkably well in practice. In the experimental part, we first\nevaluate various methods to construct a decomposition tree with small height\n$h$ in practice. Then we provide efficient implementations of our data\nstructures, and evaluate their empirical performance by conducting an extensive\nexperimental study on graphs taken from real-world applications.",
            "author": [
                "Loukas Georgiadis",
                "Evangelos Kosinas",
                "Daniel Tsokaktsis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00854v2",
                "http://arxiv.org/pdf/2311.00854v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00853v1",
            "title": "An efficient tangent based topologically distinctive path finding for\n  grid maps",
            "updated": "2023-11-01T21:12:43Z",
            "published": "2023-11-01T21:12:43Z",
            "summary": "Conventional local planners frequently become trapped in a locally optimal\ntrajectory, primarily due to their inability to traverse obstacles. Having a\nlarger number of topologically distinctive paths increases the likelihood of\nfinding the optimal trajectory. It is crucial to generate a substantial number\nof topologically distinctive paths in real-time. Accordingly, we propose an\nefficient path planning approach based on tangent graphs to yield multiple\ntopologically distinctive paths. Diverging from existing algorithms, our method\neliminates the necessity of distinguishing whether two paths belong to the same\ntopology; instead, it generates multiple topologically distinctive paths based\non the locally shortest property of tangents. Additionally, we introduce a\npriority constraint for the queue during graph search, thereby averting the\nexponential expansion of queue size. To illustrate the advantages of our\nmethod, we conducted a comparative analysis with various typical algorithms\nusing a widely recognized public\ndataset\\footnote{https://movingai.com/benchmarks/grids.html}. The results\nindicate that, on average, our method generates 320 topologically distinctive\npaths within a mere 100 milliseconds. This outcome underscores a significant\nenhancement in efficiency when compared to existing methods. To foster further\nresearch within the community, we have made the source code of our proposed\nalgorithm publicly\naccessible\\footnote{https://joeyao-bit.github.io/posts/2023/09/07/}. We\nanticipate that this framework will significantly contribute to the development\nof more efficient topologically distinctive path planning, along with related\ntrajectory optimization and motion planning endeavors.",
            "author": [
                "Zhuo Yao",
                "Wei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00853v1",
                "http://arxiv.org/pdf/2311.00853v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00848v1",
            "title": "ABCD: Algorithm for Balanced Component Discovery in Signed Networks",
            "updated": "2023-11-01T20:58:14Z",
            "published": "2023-11-01T20:58:14Z",
            "summary": "The most significant balanced element in signed graphs plays a vital role in\nhelping researchers understand the fundamental structure of the graph, as it\nreveals valuable information about the complex relationships between vertices\nin the network. The challenge is an NP-hard problem; there is no current\nbaseline to evaluate state-of-the-art signed graphs derived from real networks.\nIn this paper, we propose a scalable state-of-the-art approach for the maximum\nbalanced sub-graph detection in the network of \\emph{any} size. However, it is\nstill bounded by computational capability. The proposed approach builds on the\ngraph characteristics and a scalable fundamental cycle discovery method to\nminimize the number of vertices discarded. We evaluate the proposed approach\nagainst state-of-the-art and demonstrate over two times higher graph size\nregarding the number of vertices selected of the discovered subset on an\nextensive signed network with millions of vertices and edges over the\nstate-of-art in the same time frame.",
            "author": [
                "Muhieddine Shebaro",
                "Jelena Te\u0161i\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00848v1",
                "http://arxiv.org/pdf/2311.00848v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01475v1",
            "title": "Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts",
            "updated": "2023-11-01T19:59:25Z",
            "published": "2023-11-01T19:59:25Z",
            "summary": "Unsupervised image segmentation aims at grouping different semantic patterns\nin an image without the use of human annotation. Similarly, image clustering\nsearches for groupings of images based on their semantic content without\nsupervision. Classically, both problems have captivated researchers as they\ndrew from sound mathematical concepts to produce concrete applications. With\nthe emergence of deep learning, the scientific community turned its attention\nto complex neural network-based solvers that achieved impressive results in\nthose domains but rarely leveraged the advances made by classical methods. In\nthis work, we propose a patch-based unsupervised image segmentation strategy\nthat bridges advances in unsupervised feature extraction from deep clustering\nmethods with the algorithmic help of classical graph-based methods. We show\nthat a simple convolutional neural network, trained to classify image patches\nand iteratively regularized using graph cuts, naturally leads to a\nstate-of-the-art fully-convolutional unsupervised pixel-level segmenter.\nFurthermore, we demonstrate that this is the ideal setting for leveraging the\npatch-level pairwise features generated by vision transformer models. Our\nresults on real image data demonstrate the effectiveness of our proposed\nmethodology.",
            "author": [
                "Isaac Wasserman",
                "Jeova Farias Sales Rocha Neto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01475v1",
                "http://arxiv.org/pdf/2311.01475v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00815v1",
            "title": "PIAug -- Physics Informed Augmentation for Learning Vehicle Dynamics for\n  Off-Road Navigation",
            "updated": "2023-11-01T19:56:58Z",
            "published": "2023-11-01T19:56:58Z",
            "summary": "Modeling the precise dynamics of off-road vehicles is a complex yet essential\ntask due to the challenging terrain they encounter and the need for optimal\nperformance and safety. Recently, there has been a focus on integrating nominal\nphysics-based models alongside data-driven neural networks using Physics\nInformed Neural Networks. These approaches often assume the availability of a\nwell-distributed dataset; however, this assumption may not hold due to regions\nin the physical distribution that are hard to collect, such as high-speed\nmotions and rare terrains. Therefore, we introduce a physics-informed data\naugmentation methodology called PIAug. We show an example use case of the same\nby modeling high-speed and aggressive motion predictions, given a dataset with\nonly low-speed data. During the training phase, we leverage the nominal model\nfor generating target domain (medium and high velocity) data using the\navailable source data (low velocity). Subsequently, we employ a\nphysics-inspired loss function with this augmented dataset to incorporate prior\nknowledge of physics into the neural network. Our methodology results in up to\n67% less mean error in trajectory prediction in comparison to a standalone\nnominal model, especially during aggressive maneuvers at speeds outside the\ntraining domain. In real-life navigation experiments, our model succeeds in 4x\ntighter waypoint tracking constraints than the Kinematic Bicycle Model (KBM) at\nout-of-domain velocities.",
            "author": [
                "Parv Maheshwari",
                "Wenshan Wang",
                "Samuel Triest",
                "Matthew Sivaprakasam",
                "Shubhra Aich",
                "John G. Rogers III",
                "Jason M. Gregory",
                "Sebastian Scherer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00815v1",
                "http://arxiv.org/pdf/2311.00815v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00809v1",
            "title": "Graph-Based Optimization for Technology Pathway Analysis: A Case Study\n  in Decarbonization of University Campuses",
            "updated": "2023-11-01T19:48:37Z",
            "published": "2023-11-01T19:48:37Z",
            "summary": "Industrial sectors such as urban centers, chemical companies, manufacturing\nfacilities, and microgrids are actively exploring strategies to help reduce\ntheir carbon footprint. For instance, university campuses are complex urban\ndistricts (involving collections of buildings and utility systems) that are\nseeking to reduce carbon footprints that originate from diverse activities\n(e.g., transportation operations and production of heating, cooling, and power\nutilities). This work presents an optimization framework to identify technology\npathways that enable decarbonization of complex industrial sectors. The\nframework uses a graph abstraction that compactly captures interdependencies\nbetween diverse products and technologies as well as diverse externalities\n(e.g., market, policy, and carbon prices). Duality analysis reveals that the\nformulation can be interpreted as an economy, market, or value chain that uses\ntechnologies to generate economic value (wealth) by transforming basic products\ninto higher value products. This interpretation also reveals that the\nformulation identifies pathways that maximize the profit of stakeholders, helps\nreveal the inherent value (prices) of intermediate products, and helps analyze\nthe impact of externalities and technology specifications on product values.\nOur developments are illustrated via a case study involving a prototypical\nuniversity campus that seeks to identify pathways that reduce its carbon\nfootprint (e.g., via electrification and deployment of hydrogen technologies).\nWe use the framework to determine carbon tax values, technology specifications,\nand investment budgets that activate different technology pathways and that\nachieve different levels of decarbonization.",
            "author": [
                "Blake Lopez",
                "Jiaze Ma",
                "Victor M. Zavala"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00809v1",
                "http://arxiv.org/pdf/2311.00809v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00788v1",
            "title": "Code Sparsification and its Applications",
            "updated": "2023-11-01T19:17:08Z",
            "published": "2023-11-01T19:17:08Z",
            "summary": "We introduce a notion of code sparsification that generalizes the notion of\ncut sparsification in graphs. For a (linear) code $\\mathcal{C} \\subseteq\n\\mathbb{F}_q^n$ of dimension $k$ a $(1 \\pm \\epsilon)$-sparsification of size\n$s$ is given by a weighted set $S \\subseteq [n]$ with $|S| \\leq s$ such that\nfor every codeword $c \\in \\mathcal{C}$ the projection $c|_S$ of $c$ to the set\n$S$ has (weighted) hamming weight which is a $(1 \\pm \\epsilon)$ approximation\nof the hamming weight of $c$. We show that for every code there exists a $(1\n\\pm \\epsilon)$-sparsification of size $s = \\widetilde{O}(k \\log (q) /\n\\epsilon^2)$. This immediately implies known results on graph and hypergraph\ncut sparsification up to polylogarithmic factors (with a simple unified proof).\n  One application of our result is near-linear size sparsifiers for constraint\nsatisfaction problems (CSPs) over $\\mathbb{F}_p$-valued variables whose\nunsatisfying assignments can be expressed as the zeros of a linear equation\nmodulo a prime $p$. Building on this, we obtain a complete characterization of\nternary Boolean CSPs that admit near-linear size sparsification. Finally, by\nconnections between the eigenvalues of the Laplacians of Cayley graphs over\n$\\mathbb{F}_2^k$ to the weights of codewords, we also give the first proof of\nthe existence of spectral Cayley graph sparsifiers over $\\mathbb{F}_2^k$ by\nCayley graphs, i.e., where we sparsify the set of generators to nearly-optimal\nsize.",
            "author": [
                "Sanjeev Khanna",
                "Aaron L Putterman",
                "Madhu Sudan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00788v1",
                "http://arxiv.org/pdf/2311.00788v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00784v1",
            "title": "Rational design of oxide catalysts for OER with OC22",
            "updated": "2023-11-01T19:04:55Z",
            "published": "2023-11-01T19:04:55Z",
            "summary": "The efficiency of $H_2$ production via water electrolysis is typically\nlimited to the sluggish oxygen evolution reaction (OER). As such, significant\nemphasis has been placed upon improving the rate of OER through the anode\ncatalyst. More recently, the Open Catalyst 2022 (OC22) has provided a large\ndataset of density functional theory (DFT) calculated oxide catalysts for OER.\nWhen coupled with state-of-the-art graph neural network models, total energy\npredictions can be achieved with a mean absolute error as low as of 0.22 eV. In\nthis work, we demonstrate the full utility of the OC22 framework by\nconstructing a database of the total energy predictions for all slabs and OER\nsurface intermediates for the 3,989 oxide materials in the original OC22\ndataset. This expansion includes all terminations of all facets up to a maximum\nMiller index of 1 with adsorption configurations for the three OER\nintermediates: $O^*$, $OH^*$ and a subset of $OOH^*$. We considered two\nthermodynamic models to interpret surface energy: a \"variant\" model where\nsurface energy is a function of metal concentration and an \"invariant\" model\nwhere surface energy is fixed at the equilibrium conditions for OER. We\nvalidated the predicted overpotentials of these stable surfaces with quantities\nfrom the experimental literature as well as our own DFT calculations. We then\nconstructed a screening framework to identify viable candidate anode catalysts\nfor OER by assessing the price, thermodynamic stability, resistance to\ncorrosion, surface stability, and overpotential. Finally we verified the\noverpotentials and reaction energies of the final candidate catalysts using\nDFT.",
            "author": [
                "Richard Tran",
                "Liqiang Huang",
                "Yuan Zi",
                "Shengguang Wang",
                "Benjamin M. Comer",
                "Xuqing Wu",
                "Stefan J. Raaijman",
                "Nishant K. Sinha",
                "Sajanikumari Sadasivan",
                "Shibin Thundiyil",
                "Kuldeep B. Mamtani",
                "Ganesh Iyer",
                "Lars Grabow",
                "Ligang Lu",
                "Jiefu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00784v1",
                "http://arxiv.org/pdf/2311.00784v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00779v2",
            "title": "Shortest paths on polymatroids and hypergraphic polytopes",
            "updated": "2023-11-06T10:42:17Z",
            "published": "2023-11-01T18:46:52Z",
            "summary": "Base polytopes of polymatroids, also known as generalized permutohedra, are\npolytopes whose edges are parallel to a vector of the form $\\mathbf{e}_i -\n\\mathbf{e}_j$. We consider the following computational problem: Given two\nvertices of a generalized permutohedron $P$, find a shortest path between them\non the skeleton of $P$. This captures many known flip distance problems, such\nas computing the minimum number of exchanges between two spanning trees of a\ngraph, the rotation distance between binary search trees, the flip distance\nbetween acyclic orientations of a graph, or rectangulations of a square. We\nprove that this problem is $NP$-hard, even when restricted to very simple\npolymatroids in $\\mathbb{R}^n$ defined by $O(n)$ inequalities. Assuming $P\\not=\nNP$, this rules out the existence of an efficient simplex pivoting rule that\nperforms a minimum number of nondegenerate pivoting steps to an optimal\nsolution of a linear program, even when the latter defines a polymatroid. We\nalso prove that the shortest path problem is inapproximable when the\npolymatroid is specified via an evaluation oracle for a corresponding\nsubmodular function, strengthening a recent result by Ito et al. (ICALP'23).\nMore precisely, we prove the $APX$-hardness of the shortest path problem when\nthe polymatroid is a hypergraphic polytope, whose vertices are in bijection\nwith acyclic orientations of a given hypergraph. The shortest path problem then\namounts to computing the flip distance between two acyclic orientations of a\nhypergraph. On the positive side, we provide a polynomial-time approximation\nalgorithm for the problem of computing the flip distance between two acyclic\norientations of a hypergraph, where the approximation factor is the maximum\ncodegree of the hypergraph. Our result implies an exact polynomial-time\nalgorithm for the flip distance between two acyclic orientations of any linear\nhypergraph.",
            "author": [
                "Jean Cardinal",
                "Raphael Steiner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00779v2",
                "http://arxiv.org/pdf/2311.00779v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO",
                "math.OC",
                "90C05, 90C08, 90C27, 90C35, 90C49, 90C57, 90C60, 05C50, 05C65,\n  05B35, 52B40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04912v1",
            "title": "ezBIDS: Guided standardization of neuroimaging data interoperable with\n  major data archives and platforms",
            "updated": "2023-11-01T18:31:16Z",
            "published": "2023-11-01T18:31:16Z",
            "summary": "Data standardization has become one of the leading methods neuroimaging\nresearchers rely on for data sharing and reproducibility. Data standardization\npromotes a common framework through which researchers can utilize others' data.\nYet, as of today, formatting datasets that adhere to community best practices\nrequires technical expertise involving coding and considerable knowledge of\nfile formats and standards. We describe ezBIDS, a tool for converting\nneuroimaging data and associated metadata to the Brain Imaging Data Structure\n(BIDS) standard. ezBIDS provides four unique features: (1) No installation or\nprogramming requirements. (2) Handling of both imaging and task events data and\nmetadata. (3) Automated inference and guidance for adherence to BIDS. (4)\nMultiple data management options: download BIDS data to local system, or\ntransfer to OpenNeuro.org or brainlife.io. In sum, ezBIDS requires neither\ncoding proficiency nor knowledge of BIDS and is the first BIDS tool to offer\nguided standardization, support for task events conversion, and\ninteroperability with OpenNeuro and brainlife.io.",
            "author": [
                "Daniel Levitas",
                "Soichi Hayashi",
                "Sophia Vinci-Booher",
                "Anibal Heinsfeld",
                "Dheeraj Bhatia",
                "Nicholas Lee",
                "Anthony Galassi",
                "Guiomar Niso",
                "Franco Pestilli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04912v1",
                "http://arxiv.org/pdf/2311.04912v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04911v1",
            "title": "From Text to Structure: Using Large Language Models to Support the\n  Development of Legal Expert Systems",
            "updated": "2023-11-01T18:31:02Z",
            "published": "2023-11-01T18:31:02Z",
            "summary": "Encoding legislative text in a formal representation is an important\nprerequisite to different tasks in the field of AI & Law. For example,\nrule-based expert systems focused on legislation can support laypeople in\nunderstanding how legislation applies to them and provide them with helpful\ncontext and information. However, the process of analyzing legislation and\nother sources to encode it in the desired formal representation can be\ntime-consuming and represents a bottleneck in the development of such systems.\nHere, we investigate to what degree large language models (LLMs), such as\nGPT-4, are able to automatically extract structured representations from\nlegislation. We use LLMs to create pathways from legislation, according to the\nJusticeBot methodology for legal decision support systems, evaluate the\npathways and compare them to manually created pathways. The results are\npromising, with 60% of generated pathways being rated as equivalent or better\nthan manually created ones in a blind comparison. The approach suggests a\npromising path to leverage the capabilities of LLMs to ease the costly\ndevelopment of systems based on symbolic approaches that are transparent and\nexplainable.",
            "author": [
                "Samyar Janatian",
                "Hannes Westermann",
                "Jinzhe Tan",
                "Jaromir Savelka",
                "Karim Benyekhlef"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04911v1",
                "http://arxiv.org/pdf/2311.04911v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00769v1",
            "title": "An Integrated Approach to Aerial Grasping: Combining a Bistable Gripper\n  with Adaptive Control",
            "updated": "2023-11-01T18:23:43Z",
            "published": "2023-11-01T18:23:43Z",
            "summary": "Grasping using an aerial robot can have many applications ranging from\ninfrastructure inspection and maintenance to precise agriculture. However,\naerial grasping is a challenging problem since the robot has to maintain an\naccurate position and orientation relative to the grasping object, while\nnegotiating various forms of uncertainties (e.g., contact force from the\nobject). To address such challenges, in this paper, we integrate a novel\npassive gripper design and advanced adaptive control methods to enable robust\naerial grasping. The gripper is enabled by a pre-stressed band with two stable\nstates (a flat shape and a curled shape). In this case, it can automatically\ninitiate the grasping process upon contact with an object. The gripper also\nfeatures a cable-driven system by a single DC motor to open the gripper without\nusing cumbersome pneumatics. Since the gripper is passively triggered and\ninitially has a straight shape, it can function without precisely aligning the\ngripper with the object (within an $80$ mm tolerance). Our adaptive control\nscheme eliminates the need for any a priori knowledge (nominal or upper bounds)\nof uncertainties. The closed-loop stability of the system is analyzed via\nLyapunov-based method. Combining the gripper and the adaptive control, we\nconduct comparative real-time experimental results to demonstrate the\neffectiveness of the proposed integrated system for grasping. Our integrated\napproach can pave the way to enhance aerial grasping for different\napplications.",
            "author": [
                "Rishabh Dev Yadav",
                "Brycen Jones",
                "Saksham Gupta",
                "Amitabh Sharma",
                "Jiefeng Sun",
                "Jianguo Zhao",
                "Spandan Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00769v1",
                "http://arxiv.org/pdf/2311.00769v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00768v1",
            "title": "Language Model Training Paradigms for Clinical Feature Embeddings",
            "updated": "2023-11-01T18:23:12Z",
            "published": "2023-11-01T18:23:12Z",
            "summary": "In research areas with scarce data, representation learning plays a\nsignificant role. This work aims to enhance representation learning for\nclinical time series by deriving universal embeddings for clinical features,\nsuch as heart rate and blood pressure. We use self-supervised training\nparadigms for language models to learn high-quality clinical feature\nembeddings, achieving a finer granularity than existing time-step and\npatient-level representation learning. We visualize the learnt embeddings via\nunsupervised dimension reduction techniques and observe a high degree of\nconsistency with prior clinical knowledge. We also evaluate the model\nperformance on the MIMIC-III benchmark and demonstrate the effectiveness of\nusing clinical feature embeddings. We publish our code online for replication.",
            "author": [
                "Yurong Hu",
                "Manuel Burger",
                "Gunnar R\u00e4tsch",
                "Rita Kuznetsova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00768v1",
                "http://arxiv.org/pdf/2311.00768v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00766v1",
            "title": "Quantum Pathways for Charged Track Finding in High-Energy Collisions",
            "updated": "2023-11-01T18:13:59Z",
            "published": "2023-11-01T18:13:59Z",
            "summary": "In high-energy particle collisions, charged track finding is a complex yet\ncrucial endeavour. We propose a quantum algorithm, specifically quantum\ntemplate matching, to enhance the accuracy and efficiency of track finding.\nAbstracting the Quantum Amplitude Amplification routine by introducing a data\nregister, and utilising a novel oracle construction, allows data to be parsed\nto the circuit and matched with a hit-pattern template, without prior knowledge\nof the input data. Furthermore, we address the challenges posed by missing hit\ndata, demonstrating the ability of the quantum template matching algorithm to\nsuccessfully identify charged-particle tracks from hit patterns with missing\nhits. Our findings therefore propose quantum methodologies tailored for\nreal-world applications and underline the potential of quantum computing in\ncollider physics.",
            "author": [
                "Christopher Brown",
                "Michael Spannowsky",
                "Alexander Tapper",
                "Simon Williams",
                "Ioannis Xiotidis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00766v1",
                "http://arxiv.org/pdf/2311.00766v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00690v3",
            "title": "What User Behaviors Make the Differences During the Process of Visual\n  Analytics?",
            "updated": "2023-12-04T02:58:02Z",
            "published": "2023-11-01T17:45:52Z",
            "summary": "The understanding of visual analytics process can benefit visualization\nresearchers from multiple aspects, including improving visual designs and\ndeveloping advanced interaction functions. However, the log files of user\nbehaviors are still hard to analyze due to the complexity of sensemaking and\nour lack of knowledge on the related user behaviors. This work presents a study\non a comprehensive data collection of user behaviors, and our analysis approach\nwith time-series classification methods. We have chosen a classical\nvisualization application, Covid-19 data analysis, with common analysis tasks\ncovering geo-spatial, time-series and multi-attributes. Our user study collects\nuser behaviors on a diverse set of visualization tasks with two comparable\nsystems, desktop and immersive visualizations. We summarize the classification\nresults with three time-series machine learning algorithms at two scales, and\nexplore the influences of behavior features. Our results reveal that user\nbehaviors can be distinguished during the process of visual analytics and there\nis a potentially strong association between the physical behaviors of users and\nthe visualization tasks they perform. We also demonstrate the usage of our\nmodels by interpreting open sessions of visual analytics, which provides an\nautomatic way to study sensemaking without tedious manual annotations.",
            "author": [
                "Zekun Wu",
                "Shahin Doroudian",
                "Aidong Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00690v3",
                "http://arxiv.org/pdf/2311.00690v3"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00674v1",
            "title": "Recovering Linear Causal Models with Latent Variables via Cholesky\n  Factorization of Covariance Matrix",
            "updated": "2023-11-01T17:27:49Z",
            "published": "2023-11-01T17:27:49Z",
            "summary": "Discovering the causal relationship via recovering the directed acyclic graph\n(DAG) structure from the observed data is a well-known challenging\ncombinatorial problem. When there are latent variables, the problem becomes\neven more difficult. In this paper, we first propose a DAG structure recovering\nalgorithm, which is based on the Cholesky factorization of the covariance\nmatrix of the observed data. The algorithm is fast and easy to implement and\nhas theoretical grantees for exact recovery. On synthetic and real-world\ndatasets, the algorithm is significantly faster than previous methods and\nachieves the state-of-the-art performance. Furthermore, under the equal error\nvariances assumption, we incorporate an optimization procedure into the\nCholesky factorization based algorithm to handle the DAG recovering problem\nwith latent variables. Numerical simulations show that the modified \"Cholesky +\noptimization\" algorithm is able to recover the ground truth graph in most cases\nand outperforms existing algorithms.",
            "author": [
                "Yunfeng Cai",
                "Xu Li",
                "Minging Sun",
                "Ping Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00674v1",
                "http://arxiv.org/pdf/2311.00674v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00658v1",
            "title": "Explicit Morphological Knowledge Improves Pre-training of Language\n  Models for Hebrew",
            "updated": "2023-11-01T17:02:49Z",
            "published": "2023-11-01T17:02:49Z",
            "summary": "Pre-trained language models (PLMs) have shown remarkable successes in\nacquiring a wide range of linguistic knowledge, relying solely on\nself-supervised training on text streams. Nevertheless, the effectiveness of\nthis language-agnostic approach has been frequently questioned for its\nsub-optimal performance when applied to morphologically-rich languages (MRLs).\nWe investigate the hypothesis that incorporating explicit morphological\nknowledge in the pre-training phase can improve the performance of PLMs for\nMRLs. We propose various morphologically driven tokenization methods enabling\nthe model to leverage morphological cues beyond raw text. We pre-train multiple\nlanguage models utilizing the different methods and evaluate them on Hebrew, a\nlanguage with complex and highly ambiguous morphology. Our experiments show\nthat morphologically driven tokenization demonstrates improved results compared\nto a standard language-agnostic tokenization, on a benchmark of both semantic\nand morphologic tasks. These findings suggest that incorporating morphological\nknowledge holds the potential for further improving PLMs for morphologically\nrich languages.",
            "author": [
                "Eylon Gueta",
                "Omer Goldman",
                "Reut Tsarfaty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00658v1",
                "http://arxiv.org/pdf/2311.00658v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00656v1",
            "title": "Online Signal Estimation on the Graph Edges via Line Graph\n  Transformation",
            "updated": "2023-11-01T17:02:41Z",
            "published": "2023-11-01T17:02:41Z",
            "summary": "We propose the Line Graph Normalized Least Mean Square (LGNLMS) algorithm for\nonline time-varying graph edge signals prediction. LGNLMS utilizes the Line\nGraph to transform graph edge signals into the node of its edge-to-vertex dual.\nThis enables edge signals to be processed using established GSP concepts\nwithout redefining them on graph edges.",
            "author": [
                "Yi Yan",
                "Ercan Engin Kuruoglu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00656v1",
                "http://arxiv.org/pdf/2311.00656v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00646v1",
            "title": "Understanding the Issues and Causes in WebAssembly Application\n  Development: A Mining-based Study",
            "updated": "2023-11-01T16:46:53Z",
            "published": "2023-11-01T16:46:53Z",
            "summary": "WebAssembly (Wasm) is a binary instruction format designed for secure and\nefficient execution within sandboxed environments - predominantly web apps and\nbrowsers - to facilitate performance, security, and flexibility of web\nprogramming languages. In recent years, Wasm has gained significant attention\nfrom academic research community and industrial development projects to\nengineer high-performance web applications. Despite the offered benefits,\ndevelopers encounter a multitude of issues rooted in Wasm (e.g., faults,\nerrors, failures) and are often unaware of their root-causes that impact the\ndevelopment of web applications. Wasm developers require knowledge, documented\nas empirically rooted guidelines, patterns, documents etc., that help them to\nunderstand, analyse, and resolve the issues that currently lacks in existing\nresearch and practice. To this end, we conducted an empirical study that mines\nand documents practitioners' knowledge expressed as 385 issues from 12\nopen-source Wasm projects deployed on GitHub and 354 question-answer posts via\nStack Overflow. Our study led to the first-of-its-kind taxonomies of issues\nfaced by developers and their underlying causes in Wasm-based applications.\nIssues faced by developers arise from 'Infrastructure, Integration and\nCompatibility Aspects' (28.16%), 'Language Features and Documentation Errors'\n(18.00%), along with 'Code Implementation and Build failures' (13.83%). The\nresults indicate that 'Syntactic and Semantic Errors' (25.77%), 'Configuration\nand Compatibility Constraints' (20.1%), and 'Operational Limitations' (12.98%)\nare the principal causes of these issues. The study provides a taxonomical\nclassification of issues and their causes, offering empirically derived\nguidelines, that can inform researchers and developers to systematically\ndesign, develop, and refactor Wasm-based applications.",
            "author": [
                "Muhammad Waseem",
                "Teerath Das",
                "Aakash Ahmad",
                "Peng Liang",
                "Tommi Mikkonen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00646v1",
                "http://arxiv.org/pdf/2311.00646v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00636v1",
            "title": "Kronecker-Factored Approximate Curvature for Modern Neural Network\n  Architectures",
            "updated": "2023-11-01T16:37:00Z",
            "published": "2023-11-01T16:37:00Z",
            "summary": "The core components of many modern neural network architectures, such as\ntransformers, convolutional, or graph neural networks, can be expressed as\nlinear layers with $\\textit{weight-sharing}$. Kronecker-Factored Approximate\nCurvature (K-FAC), a second-order optimisation method, has shown promise to\nspeed up neural network training and thereby reduce computational costs.\nHowever, there is currently no framework to apply it to generic architectures,\nspecifically ones with linear weight-sharing layers. In this work, we identify\ntwo different settings of linear weight-sharing layers which motivate two\nflavours of K-FAC -- $\\textit{expand}$ and $\\textit{reduce}$. We show that they\nare exact for deep linear networks with weight-sharing in their respective\nsetting. Notably, K-FAC-reduce is generally faster than K-FAC-expand, which we\nleverage to speed up automatic hyperparameter selection via optimising the\nmarginal likelihood for a Wide ResNet. Finally, we observe little difference\nbetween these two K-FAC variations when using them to train both a graph neural\nnetwork and a vision transformer. However, both variations are able to reach a\nfixed validation metric target in $50$-$75\\%$ of the number of steps of a\nfirst-order reference run, which translates into a comparable improvement in\nwall-clock time. This highlights the potential of applying K-FAC to modern\nneural network architectures.",
            "author": [
                "Runa Eschenhagen",
                "Alexander Immer",
                "Richard E. Turner",
                "Frank Schneider",
                "Philipp Hennig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00636v1",
                "http://arxiv.org/pdf/2311.00636v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00635v1",
            "title": "GATSY: Graph Attention Network for Music Artist Similarity",
            "updated": "2023-11-01T16:36:19Z",
            "published": "2023-11-01T16:36:19Z",
            "summary": "The artist similarity quest has become a crucial subject in social and\nscientific contexts. Modern research solutions facilitate music discovery\naccording to user tastes. However, defining similarity among artists may\ninvolve several aspects, even related to a subjective perspective, and it often\naffects a recommendation. This paper presents GATSY, a recommendation system\nbuilt upon graph attention networks and driven by a clusterized embedding of\nartists. The proposed framework takes advantage of a graph topology of the\ninput data to achieve outstanding performance results without relying heavily\non hand-crafted features. This flexibility allows us to introduce fictitious\nartists in a music dataset, create bridges to previously unrelated artists, and\nget recommendations conditioned by possibly heterogeneous sources. Experimental\nresults prove the effectiveness of the proposed method with respect to\nstate-of-the-art solutions.",
            "author": [
                "Andrea Giuseppe Di Francesco",
                "Giuliano Giampietro",
                "Indro Spinelli",
                "Danilo Comminiello"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00635v1",
                "http://arxiv.org/pdf/2311.00635v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00620v1",
            "title": "Sigma invariants for partial orders on nilpotent groups",
            "updated": "2023-11-01T16:14:59Z",
            "published": "2023-11-01T16:14:59Z",
            "summary": "We prove that a map onto a nilpotent group $Q$ has finitely generated kernel\nif and only if the preimage of the positive cone is coarsely connected as a\nsubset of the Cayley graph for every full archimedean partial order on $Q$. In\ncase $Q$ is abelian, we recover the classical theorem that $N$ is finitely\ngenerated if and only if $S(G,N) \\subseteq \\Sigma^1(G)$. Furthermore, we\nprovide a way to construct all such orders on nilpotent groups. A key step is\nto translate the classical setting based on characters into a language of\norders on $G$.",
            "author": [
                "Kevin Klinge"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00620v1",
                "http://arxiv.org/pdf/2311.00620v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00612v1",
            "title": "A Collaborative Filtering-Based Two Stage Model with Item Dependency for\n  Course Recommendation",
            "updated": "2023-11-01T16:01:00Z",
            "published": "2023-11-01T16:01:00Z",
            "summary": "Recommender systems have been studied for decades with numerous promising\nmodels been proposed. Among them, Collaborative Filtering (CF) models are\narguably the most successful one due to its high accuracy in recommendation and\nelimination of privacy-concerned personal meta-data from training. This paper\nextends the usage of CF-based model to the task of course recommendation. We\npoint out several challenges in applying the existing CF-models to build a\ncourse recommendation engine, including the lack of rating and meta-data, the\nimbalance of course registration distribution, and the demand of course\ndependency modeling. We then propose several ideas to address these challenges.\nEventually, we combine a two-stage CF model regularized by course dependency\nwith a graph-based recommender based on course-transition network, to achieve\nAUC as high as 0.97 with a real-world dataset.",
            "author": [
                "Eric L. Lee",
                "Tsung-Ting Kuo",
                "Shou-De Lin"
            ],
            "link": [
                "http://dx.doi.org/10.1109/DSAA.2017.18",
                "http://arxiv.org/abs/2311.00612v1",
                "http://arxiv.org/pdf/2311.00612v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00599v1",
            "title": "Structure Learning with Adaptive Random Neighborhood Informed MCMC",
            "updated": "2023-11-01T15:47:18Z",
            "published": "2023-11-01T15:47:18Z",
            "summary": "In this paper, we introduce a novel MCMC sampler, PARNI-DAG, for a\nfully-Bayesian approach to the problem of structure learning under\nobservational data. Under the assumption of causal sufficiency, the algorithm\nallows for approximate sampling directly from the posterior distribution on\nDirected Acyclic Graphs (DAGs). PARNI-DAG performs efficient sampling of DAGs\nvia locally informed, adaptive random neighborhood proposal that results in\nbetter mixing properties. In addition, to ensure better scalability with the\nnumber of nodes, we couple PARNI-DAG with a pre-tuning procedure of the\nsampler's parameters that exploits a skeleton graph derived through some\nconstraint-based or scoring-based algorithms. Thanks to these novel features,\nPARNI-DAG quickly converges to high-probability regions and is less likely to\nget stuck in local modes in the presence of high correlation between nodes in\nhigh-dimensional settings. After introducing the technical novelties in\nPARNI-DAG, we empirically demonstrate its mixing efficiency and accuracy in\nlearning DAG structures on a variety of experiments.",
            "author": [
                "Alberto Caron",
                "Xitong Liang",
                "Samuel Livingstone",
                "Jim Griffin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00599v1",
                "http://arxiv.org/pdf/2311.00599v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00595v2",
            "title": "Graph-based mutually exciting point processes for modelling event times\n  in docked bike-sharing systems",
            "updated": "2023-11-02T22:21:59Z",
            "published": "2023-11-01T15:40:30Z",
            "summary": "This paper introduces graph-based mutually exciting processes (GB-MEP) to\nmodel event times in network point processes, focusing on an application to\ndocked bike-sharing systems. GB-MEP incorporates known relationships between\nnodes in a graph within the intensity function of a node-based multivariate\nHawkes process. This approach reduces the number of parameters to a quantity\nproportional to the number of nodes in the network, resulting in significant\nadvantages for computational scalability when compared to traditional methods.\nThe model is applied on event data observed on the Santander Cycles network in\ncentral London, demonstrating that exploiting network-wide information related\nto geographical location of the stations is beneficial to improve the\nperformance of node-based models for applications in bike-sharing systems. The\nproposed GB-MEP framework is more generally applicable to any network point\nprocess where a distance function between nodes is available, demonstrating\nwider applicability.",
            "author": [
                "Francesco Sanna Passino",
                "Yining Che",
                "Carlos Cardoso Correia Perello"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00595v2",
                "http://arxiv.org/pdf/2311.00595v2"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00588v1",
            "title": "Boosting Summarization with Normalizing Flows and Aggressive Training",
            "updated": "2023-11-01T15:33:38Z",
            "published": "2023-11-01T15:33:38Z",
            "summary": "This paper presents FlowSUM, a normalizing flows-based variational\nencoder-decoder framework for Transformer-based summarization. Our approach\ntackles two primary challenges in variational summarization: insufficient\nsemantic information in latent representations and posterior collapse during\ntraining. To address these challenges, we employ normalizing flows to enable\nflexible latent posterior modeling, and we propose a controlled alternate\naggressive training (CAAT) strategy with an improved gate mechanism.\nExperimental results show that FlowSUM significantly enhances the quality of\ngenerated summaries and unleashes the potential for knowledge distillation with\nminimal impact on inference time. Furthermore, we investigate the issue of\nposterior collapse in normalizing flows and analyze how the summary quality is\naffected by the training strategy, gate initialization, and the type and number\nof normalizing flows used, offering valuable insights for future research.",
            "author": [
                "Yu Yang",
                "Xiaotong Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00588v1",
                "http://arxiv.org/pdf/2311.00588v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00560v1",
            "title": "Continuous Experimentation and Human Factors An Exploratory Study",
            "updated": "2023-11-01T14:56:33Z",
            "published": "2023-11-01T14:56:33Z",
            "summary": "In todays rapidly evolving technological landscape, the success of tools and\nsystems relies heavily on their ability to meet the needs and expectations of\nusers. User-centered design approaches, with a focus on human factors, have\ngained increasing attention as they prioritize the human element in the\ndevelopment process. With the increasing complexity of software-based systems,\ncompanies are adopting agile development methodologies and emphasizing\ncontinuous software experimentation. However, there is limited knowledge on how\nto effectively execute continuous experimentation with respect to human factors\nwithin this context. This research paper presents an exploratory qualitative\nstudy for integrating human factors in continuous experimentation, aiming to\nuncover distinctive characteristics of human factors and continuous software\nexperiments, practical challenges for integrating human factors in continuous\nsoftware experiments, and best practices associated with the management of\ncontinuous human factors experimentation.",
            "author": [
                "Amna Pir Muhammad",
                "Eric Knauss",
                "Jonas B\u00e4rgman",
                "Alessia Knauss"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00560v1",
                "http://arxiv.org/pdf/2311.00560v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00559v1",
            "title": "Learning to optimize by multi-gradient for multi-objective optimization",
            "updated": "2023-11-01T14:55:54Z",
            "published": "2023-11-01T14:55:54Z",
            "summary": "The development of artificial intelligence (AI) for science has led to the\nemergence of learning-based research paradigms, necessitating a compelling\nreevaluation of the design of multi-objective optimization (MOO) methods. The\nnew generation MOO methods should be rooted in automated learning rather than\nmanual design. In this paper, we introduce a new automatic learning paradigm\nfor optimizing MOO problems, and propose a multi-gradient learning to optimize\n(ML2O) method, which automatically learns a generator (or mappings) from\nmultiple gradients to update directions. As a learning-based method, ML2O\nacquires knowledge of local landscapes by leveraging information from the\ncurrent step and incorporates global experience extracted from historical\niteration trajectory data. By introducing a new guarding mechanism, we propose\na guarded multi-gradient learning to optimize (GML2O) method, and prove that\nthe iterative sequence generated by GML2O converges to a Pareto critical point.\nThe experimental results demonstrate that our learned optimizer outperforms\nhand-designed competitors on training multi-task learning (MTL) neural network.",
            "author": [
                "Linxi Yang",
                "Xinmin Yang",
                "Liping Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00559v1",
                "http://arxiv.org/pdf/2311.00559v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00548v3",
            "title": "Continual atlas-based segmentation of prostate MRI",
            "updated": "2023-11-06T12:34:45Z",
            "published": "2023-11-01T14:29:46Z",
            "summary": "Continual learning (CL) methods designed for natural image classification\noften fail to reach basic quality standards for medical image segmentation.\nAtlas-based segmentation, a well-established approach in medical imaging,\nincorporates domain knowledge on the region of interest, leading to\nsemantically coherent predictions. This is especially promising for CL, as it\nallows us to leverage structural information and strike an optimal balance\nbetween model rigidity and plasticity over time. When combined with\nprivacy-preserving prototypes, this process offers the advantages of\nrehearsal-based CL without compromising patient privacy. We propose Atlas\nReplay, an atlas-based segmentation approach that uses prototypes to generate\nhigh-quality segmentation masks through image registration that maintain\nconsistency even as the training distribution changes. We explore how our\nproposed method performs compared to state-of-the-art CL methods in terms of\nknowledge transferability across seven publicly available prostate segmentation\ndatasets. Prostate segmentation plays a vital role in diagnosing prostate\ncancer, however, it poses challenges due to substantial anatomical variations,\nbenign structural differences in older age groups, and fluctuating acquisition\nparameters. Our results show that Atlas Replay is both robust and generalizes\nwell to yet-unseen domains while being able to maintain knowledge, unlike\nend-to-end segmentation methods. Our code base is available under\nhttps://github.com/MECLabTUDA/Atlas-Replay.",
            "author": [
                "Amin Ranem",
                "Camila Gonz\u00e1lez",
                "Daniel Pinto dos Santos",
                "Andreas M. Bucher",
                "Ahmed E. Othman",
                "Anirban Mukhopadhyay"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00548v3",
                "http://arxiv.org/pdf/2311.00548v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00533v1",
            "title": "The Eclipse Layout Kernel",
            "updated": "2023-11-01T14:10:45Z",
            "published": "2023-11-01T14:10:45Z",
            "summary": "The Eclipse Layout Kernel (ELK) is a collection of graph drawing algorithms\nthat supports compound graph layout and ports as explicit anchor points of\nedges. It is available as open-source library under an EPL license. Since its\nbeginning, ELK has served both as a research vehicle for graph drawing\nalgorithms, and as a practical tool for solving real-world problems. ELK and\nits transpiled JavaScript cousin elkjs are now included in numerous academic\nand commercial projects.\n  Most of the algorithms realized in ELK are described in a series of\npublications. In this paper, the technical description concentrates on the key\nfeatures of the flag-ship algorithm ELK Layered, the algorithm architecture,\nand usage. However, the main purpose of this paper is to give the broader view\nthat is typically left unpublished. Specifically, we review its history, give a\nbrief overview of technical papers, discuss lessons learned over the past\nfifteen years, and present example usages. Finally, we reflect on potential\nthreats to open-source graph drawing libraries.",
            "author": [
                "S\u00f6ren Domr\u00f6s",
                "Reinhard von Hanxleden",
                "Miro Sp\u00f6nemann",
                "Ulf R\u00fcegg",
                "Christoph Daniel Schulze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00533v1",
                "http://arxiv.org/pdf/2311.00533v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00514v1",
            "title": "How Hard Is Squash? -- Towards Information Theoretic Analysis of Motor\n  Behavior in Squash",
            "updated": "2023-11-01T13:37:59Z",
            "published": "2023-11-01T13:37:59Z",
            "summary": "Fitts' law has been widely employed as a research method for analyzing tasks\nwithin the domain of Human-Computer Interaction (HCI). However, its application\nto non-computer tasks has remained limited. This study aims to extend the\napplication of Fitts' law to the realm of sports, specifically focusing on\nsquash. Squash is a high-intensity sport that requires quick movements and\nprecise shots. Our research investigates the effectiveness of utilizing Fitts'\nlaw to evaluate the task difficulty and effort level associated with executing\nand responding to various squash shots. By understanding the effort/information\nrate required for each shot, we can determine which shots are more effective in\nmaking the opponent work harder. Additionally, this knowledge can be valuable\nfor coaches in designing training programs. However, since Fitts' law was\nprimarily developed for human-computer interaction, we adapted it to fit the\nsquash scenario. This paper provides an overview of Fitts' law and its\nrelevance to sports, elucidates the motivation driving this investigation,\noutlines the methodology employed to explore this novel avenue, and presents\nthe obtained results, concluding with key insights. We conducted experiments\nwith different shots and players, collecting data on shot speed, player\nmovement time, and distance traveled. Using this data, we formulated a modified\nversion of Fitts' law specifically for squash. The results provide insights\ninto the difficulty and effectiveness of various shots, offering valuable\ninformation for both players and coaches in the sport of squash.",
            "author": [
                "Kavya Anand",
                "Pramit Saha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00514v1",
                "http://arxiv.org/pdf/2311.00514v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00513v1",
            "title": "Rule-Based Error Classification for Analyzing Differences in Frequent\n  Errors",
            "updated": "2023-11-01T13:36:20Z",
            "published": "2023-11-01T13:36:20Z",
            "summary": "Finding and fixing errors is a time-consuming task not only for novice\nprogrammers but also for expert programmers. Prior work has identified frequent\nerror patterns among various levels of programmers. However, the differences in\nthe tendencies between novices and experts have yet to be revealed. From the\nknowledge of the frequent errors in each level of programmers, instructors will\nbe able to provide helpful advice for each level of learners. In this paper, we\npropose a rule-based error classification tool to classify errors in code pairs\nconsisting of wrong and correct programs. We classify errors for 95,631 code\npairs and identify 3.47 errors on average, which are submitted by various\nlevels of programmers on an online judge system. The classified errors are used\nto analyze the differences in frequent errors between novice and expert\nprogrammers. The analyzed results show that, as for the same introductory\nproblems, errors made by novices are due to the lack of knowledge in\nprogramming, and the mistakes are considered an essential part of the learning\nprocess. On the other hand, errors made by experts are due to misunderstandings\ncaused by the carelessness of reading problems or the challenges of solving\nproblems differently than usual. The proposed tool can be used to create\nerror-labeled datasets and for further code-related educational research.",
            "author": [
                "Atsushi Shirafuji",
                "Taku Matsumoto",
                "Md Faizul Ibne Amin",
                "Yutaka Watanobe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00513v1",
                "http://arxiv.org/pdf/2311.00513v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00493v1",
            "title": "The stability of deep learning for 21cm foreground removal across\n  various sky models and frequency-dependent systematics",
            "updated": "2023-11-01T12:48:37Z",
            "published": "2023-11-01T12:48:37Z",
            "summary": "Deep learning (DL) has recently been proposed as a novel approach for 21cm\nforeground removal. Before applying DL to real observations, it is essential to\nassess its consistency with established methods, its performance across various\nsimulation models and its robustness against instrumental systematics. This\nstudy develops a commonly used U-Net and evaluates its performance for\npost-reionisation foreground removal across three distinct sky simulation\nmodels based on pure Gaussian realisations, the Lagrangian perturbation theory,\nand the Planck sky model. Stable outcomes across the models are achieved\nprovided that training and testing data align with the same model. On average,\nthe residual foreground in the U-Net reconstructed data is $\\sim$10% of the\nsignal across angular scales at the considered redshift range. Comparable\nresults are found with traditional approaches. However, blindly using a network\ntrained on one model for data from another model yields inaccurate\nreconstructions, emphasising the need for consistent training data. The study\nthen introduces frequency-dependent Gaussian beams and gain drifts to the test\ndata. The network struggles to denoise data affected by \"unexpected\"\nsystematics without prior information. However, after re-training consistently\nwith systematics-contaminated data, the network effectively restores its\nreconstruction accuracy. This highlights the importance of incorporating prior\nsystematics knowledge during training for successful denoising. Our work\nprovides critical guidelines for using DL for 21cm foreground removal, tailored\nto specific data attributes. Notably, it is the first time that DL has been\napplied to the Planck sky model being most realistic foregrounds at present.",
            "author": [
                "T. Chen",
                "M. Bianco",
                "E. Tolley",
                "M. Spinelli",
                "D. Forero-Sanchez",
                "J. P. Kneib"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00493v1",
                "http://arxiv.org/pdf/2311.00493v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00491v1",
            "title": "Bayes-enhanced Multi-view Attention Networks for Robust POI\n  Recommendation",
            "updated": "2023-11-01T12:47:38Z",
            "published": "2023-11-01T12:47:38Z",
            "summary": "POI recommendation is practically important to facilitate various\nLocation-Based Social Network services, and has attracted rising research\nattention recently. Existing works generally assume the available POI check-ins\nreported by users are the ground-truth depiction of user behaviors. However, in\nreal application scenarios, the check-in data can be rather unreliable due to\nboth subjective and objective causes including positioning error and user\nprivacy concerns, leading to significant negative impacts on the performance of\nthe POI recommendation. To this end, we investigate a novel problem of robust\nPOI recommendation by considering the uncertainty factors of the user\ncheck-ins, and proposes a Bayes-enhanced Multi-view Attention Network.\nSpecifically, we construct personal POI transition graph, the semantic-based\nPOI graph and distance-based POI graph to comprehensively model the\ndependencies among the POIs. As the personal POI transition graph is usually\nsparse and sensitive to noise, we design a Bayes-enhanced spatial dependency\nlearning module for data augmentation from the local view. A Bayesian posterior\nguided graph augmentation approach is adopted to generate a new graph with\ncollaborative signals to increase the data diversity. Then both the original\nand the augmented graphs are used for POI representation learning to counteract\nthe data uncertainty issue. Next, the POI representations of the three view\ngraphs are input into the proposed multi-view attention-based user preference\nlearning module. By incorporating the semantic and distance correlations of\nPOIs, the user preference can be effectively refined and finally robust\nrecommendation results are achieved. The results of extensive experiments show\nthat BayMAN significantly outperforms the state-of-the-art methods in POI\nrecommendation when the available check-ins are incomplete and noisy.",
            "author": [
                "Jiangnan Xia",
                "Yu Yang",
                "Senzhang Wang",
                "Hongzhi Yin",
                "Jiannong Cao",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00491v1",
                "http://arxiv.org/pdf/2311.00491v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00476v1",
            "title": "Group Distributionally Robust Knowledge Distillation",
            "updated": "2023-11-01T12:25:02Z",
            "published": "2023-11-01T12:25:02Z",
            "summary": "Knowledge distillation enables fast and effective transfer of features\nlearned from a bigger model to a smaller one. However, distillation objectives\nare susceptible to sub-population shifts, a common scenario in medical imaging\nanalysis which refers to groups/domains of data that are underrepresented in\nthe training set. For instance, training models on health data acquired from\nmultiple scanners or hospitals can yield subpar performance for minority\ngroups. In this paper, inspired by distributionally robust optimization (DRO)\ntechniques, we address this shortcoming by proposing a group-aware distillation\nloss. During optimization, a set of weights is updated based on the per-group\nlosses at a given iteration. This way, our method can dynamically focus on\ngroups that have low performance during training. We empirically validate our\nmethod, GroupDistil on two benchmark datasets (natural images and cardiac MRIs)\nand show consistent improvement in terms of worst-group accuracy.",
            "author": [
                "Konstantinos Vilouras",
                "Xiao Liu",
                "Pedro Sanchez",
                "Alison Q. O'Neil",
                "Sotirios A. Tsaftaris"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00476v1",
                "http://arxiv.org/pdf/2311.00476v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00472v1",
            "title": "The Structure of the Solar Cycle and of the Activity Cycles of Late-Type\n  Stars",
            "updated": "2023-11-01T12:12:00Z",
            "published": "2023-11-01T12:12:00Z",
            "summary": "It is shown that the description of the solar cycle that takes into account\nthe odd zonal harmonic of the solar magnetic field allows us to deepen our\nknowledge of two important aspects of the solar activity. First, to clarify and\nexpand predictions of the evolution of the cyclic activity of the Sun in the\nnear future. Second, to develop a program for monitoring the spectrophotometric\ncharacteristics of radiation of the solar-type stars aimed at obtaining new\ninformation about their magnetic fields.",
            "author": [
                "V. N. Obridko",
                "D. D. Sokoloff",
                "M. M. Katsova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00472v1",
                "http://arxiv.org/pdf/2311.00472v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00470v1",
            "title": "Universal symmetry of optimal control at the microscale",
            "updated": "2023-11-01T12:10:59Z",
            "published": "2023-11-01T12:10:59Z",
            "summary": "Optimal control is an important field in thermodynamics, mathematics and\nengineering which searches for strategies e.g. to move an object to a target\nposition within a given time with minimal energetic effort. Especially for\nmicro- and nanomachines, for which power supply is often limited, knowledge of\noptimal control protocols is crucial for their operation under realistic\nconditions. Here we investigate experimentally and theoretically the optimal\nprotocol for transporting a microscopic colloidal particle with an optical trap\nin such a way that the required work is minimal. The experiments were conducted\nin viscous and viscoelastic media, which represent typical environments for\nsynthetic and biological nanomachines. Despite marked differences between the\nprotocols in both fluids, we find that the optimal transport protocol and the\ncorresponding average particle trajectory always obeys time-reversal symmetry.\nThis symmetry, which surprisingly appears here for a class of processes far\naway from thermal equilibrium, is moreover expected to hold universally for\ne.g. active, granular and long-range correlated media in their linear regimes.\nThe uncovered symmetry provides a rigorous and versatile criterion for optimal\ncontrol which greatly facilitates the search of energy-efficient transport\nstrategies in a wide range of systems.",
            "author": [
                "Sarah A. M. Loos",
                "Samuel Monter",
                "Felix Ginot",
                "Clemens Bechinger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00470v1",
                "http://arxiv.org/pdf/2311.00470v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00465v1",
            "title": "Asynchronous SGD on Graphs: a Unified Framework for Asynchronous\n  Decentralized and Federated Optimization",
            "updated": "2023-11-01T11:58:16Z",
            "published": "2023-11-01T11:58:16Z",
            "summary": "Decentralized and asynchronous communications are two popular techniques to\nspeedup communication complexity of distributed machine learning, by\nrespectively removing the dependency over a central orchestrator and the need\nfor synchronization. Yet, combining these two techniques together still remains\na challenge. In this paper, we take a step in this direction and introduce\nAsynchronous SGD on Graphs (AGRAF SGD) -- a general algorithmic framework that\ncovers asynchronous versions of many popular algorithms including SGD,\nDecentralized SGD, Local SGD, FedBuff, thanks to its relaxed communication and\ncomputation assumptions. We provide rates of convergence under much milder\nassumptions than previous decentralized asynchronous works, while still\nrecovering or even improving over the best know results for all the algorithms\ncovered.",
            "author": [
                "Mathieu Even",
                "Anastasia Koloskova",
                "Laurent Massouli\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00465v1",
                "http://arxiv.org/pdf/2311.00465v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00452v1",
            "title": "Hessian Eigenvectors and Principal Component Analysis of Neural Network\n  Weight Matrices",
            "updated": "2023-11-01T11:38:31Z",
            "published": "2023-11-01T11:38:31Z",
            "summary": "This study delves into the intricate dynamics of trained deep neural networks\nand their relationships with network parameters. Trained networks predominantly\ncontinue training in a single direction, known as the drift mode. This drift\nmode can be explained by the quadratic potential model of the loss function,\nsuggesting a slow exponential decay towards the potential minima. We unveil a\ncorrelation between Hessian eigenvectors and network weights. This\nrelationship, hinging on the magnitude of eigenvalues, allows us to discern\nparameter directions within the network. Notably, the significance of these\ndirections relies on two defining attributes: the curvature of their potential\nwells (indicated by the magnitude of Hessian eigenvalues) and their alignment\nwith the weight vectors. Our exploration extends to the decomposition of weight\nmatrices through singular value decomposition. This approach proves practical\nin identifying critical directions within the Hessian, considering both their\nmagnitude and curvature. Furthermore, our examination showcases the\napplicability of principal component analysis in approximating the Hessian,\nwith update parameters emerging as a superior choice over weights for this\npurpose. Remarkably, our findings unveil a similarity between the largest\nHessian eigenvalues of individual layers and the entire network. Notably,\nhigher eigenvalues are concentrated more in deeper layers. Leveraging these\ninsights, we venture into addressing catastrophic forgetting, a challenge of\nneural networks when learning new tasks while retaining knowledge from previous\nones. By applying our discoveries, we formulate an effective strategy to\nmitigate catastrophic forgetting, offering a possible solution that can be\napplied to networks of varying scales, including larger architectures.",
            "author": [
                "David Haink"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00452v1",
                "http://arxiv.org/pdf/2311.00452v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00451v1",
            "title": "Discourse Relations Classification and Cross-Framework Discourse\n  Relation Classification Through the Lens of Cognitive Dimensions: An\n  Empirical Investigation",
            "updated": "2023-11-01T11:38:19Z",
            "published": "2023-11-01T11:38:19Z",
            "summary": "Existing discourse formalisms use different taxonomies of discourse\nrelations, which require expert knowledge to understand, posing a challenge for\nannotation and automatic classification. We show that discourse relations can\nbe effectively captured by some simple cognitively inspired dimensions proposed\nby Sanders et al.(2018). Our experiments on cross-framework discourse relation\nclassification (PDTB & RST) demonstrate that it is possible to transfer\nknowledge of discourse relations for one framework to another framework by\nmeans of these dimensions, in spite of differences in discourse segmentation of\nthe two frameworks. This manifests the effectiveness of these dimensions in\ncharacterizing discourse relations across frameworks. Ablation studies reveal\nthat different dimensions influence different types of discourse relations. The\npatterns can be explained by the role of dimensions in characterizing and\ndistinguishing different relations. We also report our experimental results on\nautomatic prediction of these dimensions.",
            "author": [
                "Yingxue Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00451v1",
                "http://arxiv.org/pdf/2311.00451v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00450v1",
            "title": "Nuclear PDFs After the First Decade of LHC Data",
            "updated": "2023-11-01T11:27:56Z",
            "published": "2023-11-01T11:27:56Z",
            "summary": "We present a review of the conceptual basis, present knowledge and recent\nprogress in the field of global analysis of nuclear parton distribution\nfunctions (PDFs). After introducing the theoretical foundations and\nmethodological approaches for the extraction of nuclear PDFs from experimental\ndata, we discuss how different measurements in fixed-target and collider\nexperiments provide increasingly precise constraints on various aspects of\nnuclear PDFs, including shadowing, antishadowing, the EMC effect, Fermi motion,\nflavor separation, deuteron binding, target-mass and other higher-twist\neffects. Particular emphasis is given to measurements carried out in\nproton-lead collisions at the Large Hadron Collider, which have revolutionized\nthe global analysis during the past decade. These measurements include\nelectroweak-boson, jet, light-hadron, and heavy-flavor observables. Finally, we\noutline the expected impact of the future Electron Ion Collider and discuss the\nrole and interplay of nuclear PDFs with other branches of nuclear, particle and\nastroparticle physics.",
            "author": [
                "M. Klasen",
                "H. Paukkunen"
            ],
            "link": [
                "http://dx.doi.org/10.1146/annurev-nucl-102122-022747",
                "http://arxiv.org/abs/2311.00450v1",
                "http://arxiv.org/pdf/2311.00450v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "nucl-ex",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00449v1",
            "title": "Progress in the partial-wave analysis methods at COMPASS",
            "updated": "2023-11-01T11:21:20Z",
            "published": "2023-11-01T11:21:20Z",
            "summary": "We study the excitation spectrum of light and strange mesons in diffractive\nscattering. We identify different hadron resonances through partial wave\nanalysis, which inherently relies on analysis models. Besides statistical\nuncertainties, the model dependence of the analysis introduces dominant\nsystematic uncertainties. We discuss several of their sources for the\n$\\pi^-\\pi^-\\pi^+$ and $K^0_S K^-$ final states and present methods to reduce\nthem. We have developed a new approach exploiting a-priori knowledge of signal\ncontinuity over adjacent final-state-mass bins to stably fit a large pool of\npartial-waves to our data, allowing a clean identification of very small\nsignals in our large data sets. For two-body final states of scalar particles,\nsuch as $K^0_S K^-$, mathematical ambiguities in the partial-wave decomposition\nlead to the same intensity distribution for different combinations of amplitude\nvalues. We will discuss these ambiguities and present solutions to resolve or\nat least reduce the number of possible solutions. Resolving these issues will\nallow for a complementary analysis of the $a_J$-like resonance sector in these\ntwo final states.",
            "author": [
                "Julien Beckers",
                "Florian Kaspar",
                "Jakob Knollm\u00fcller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00449v1",
                "http://arxiv.org/pdf/2311.00449v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00444v1",
            "title": "Form follows Function: Text-to-Text Conditional Graph Generation based\n  on Functional Requirements",
            "updated": "2023-11-01T11:12:02Z",
            "published": "2023-11-01T11:12:02Z",
            "summary": "This work focuses on the novel problem setting of generating graphs\nconditioned on a description of the graph's functional requirements in a\ndownstream task. We pose the problem as a text-to-text generation problem and\nfocus on the approach of fine-tuning a pretrained large language model (LLM) to\ngenerate graphs. We propose an inductive bias which incorporates information\nabout the structure of the graph into the LLM's generation process by\nincorporating message passing layers into an LLM's architecture. To evaluate\nour proposed method, we design a novel set of experiments using publicly\navailable and widely studied molecule and knowledge graph data sets. Results\nsuggest our proposed approach generates graphs which more closely meet the\nrequested functional requirements, outperforming baselines developed on similar\ntasks by a statistically significant margin.",
            "author": [
                "Peter A. Zachares",
                "Vahan Hovhannisyan",
                "Alan Mosca",
                "Yarin Gal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00444v1",
                "http://arxiv.org/pdf/2311.00444v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00440v2",
            "title": "Maximum $k$- vs. $\\ell$-colourings of graphs",
            "updated": "2023-11-16T04:58:02Z",
            "published": "2023-11-01T11:09:01Z",
            "summary": "We present polynomial-time SDP-based algorithms for the following problem:\nFor fixed $k \\leq \\ell$, given a real number $\\epsilon>0$ and a graph $G$ that\nadmits a $k$-colouring with a $\\rho$-fraction of the edges coloured properly,\nit returns an $\\ell$-colouring of $G$ with an $(\\alpha \\rho -\n\\epsilon)$-fraction of the edges coloured properly in polynomial time in $G$\nand $1 / \\epsilon$. Our algorithms are based on the algorithms of Frieze and\nJerrum [Algorithmica'97] and of Karger, Motwani and Sudan [JACM'98].\n  For $k = 2, \\ell = 3$, our algorithm achieves an approximation ratio $\\alpha\n= 1$, which is the best possible. When $k$ is fixed and $\\ell$ grows large, our\nalgorithm achieves an approximation ratio of $\\alpha = 1 - o(1 / \\ell)$. When\n$k, \\ell$ are both large, our algorithm achieves an approximation ratio of\n$\\alpha = 1 - 1 / \\ell + 2 \\ln \\ell / k \\ell - o(\\ln \\ell / k \\ell) - O(1 /\nk^2)$; if we fix $d = \\ell - k$ and allow $k, \\ell$ to grow large, this is\n$\\alpha = 1 - 1 / \\ell + 2 \\ln \\ell / k \\ell - o(\\ln \\ell / k \\ell)$.\n  By extending the results of Khot, Kindler, Mossel and O'Donnell [SICOMP'07]\nto the promise setting, we show that for large $k$ and $\\ell$, assuming Khot's\nUnique Games Conjecture (UGC), it is \\NP-hard to achieve an approximation ratio\n$\\alpha$ greater than $1 - 1 / \\ell + 2 \\ln \\ell / k \\ell + o(\\ln \\ell / k\n\\ell)$, provided that $\\ell$ is bounded by a function that is\n$o(\\exp(\\sqrt[3]{k}))$. For the case where $d = \\ell - k$ is fixed, this bound\nmatches the performance of our algorithm up to $o(\\ln \\ell / k \\ell)$.\nFurthermore, by extending the results of Guruswami and Sinop [ToC'13] to the\npromise setting, we prove that it is NP-hard to achieve an approximation ratio\ngreater than $1 - 1 / \\ell + 8 \\ln \\ell / k \\ell + o(\\ln \\ell / k \\ell)$,\nprovided again that $\\ell$ is bounded as before (but this time without assuming\nthe UGC).",
            "author": [
                "Tamio-Vesa Nakajima",
                "Stanislav \u017divn\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00440v2",
                "http://arxiv.org/pdf/2311.00440v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CC",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00439v1",
            "title": "Bounds on Treatment Effects under Stochastic Monotonicity Assumption in\n  Sample Selection Models",
            "updated": "2023-11-01T11:04:08Z",
            "published": "2023-11-01T11:04:08Z",
            "summary": "This paper discusses the partial identification of treatment effects in\nsample selection models when the exclusion restriction fails and the\nmonotonicity assumption in the selection effect does not hold exactly, both of\nwhich are key challenges in applying the existing methodologies. Our approach\nbuilds on Lee's (2009) procedure, who considers partial identification under\nthe monotonicity assumption, but we assume only a stochastic (and weaker)\nversion of monotonicity, which depends on a prespecified parameter $\\vartheta$\nthat represents researchers' belief in the plausibility of the monotonicity.\nUnder this assumption, we show that we can still obtain useful bounds even when\nthe monotonic behavioral model does not strictly hold. Our procedure is useful\nwhen empirical researchers anticipate that a small fraction of the population\nwill not behave monotonically in selection; it can also be an effective tool\nfor performing sensitivity analysis or examining the identification power of\nthe monotonicity assumption. Our procedure is easily extendable to other\nrelated settings; we also provide the identification result of the marginal\ntreatment effects setting as an important application. Moreover, we show that\nthe bounds can still be obtained even in the absence of the knowledge of\n$\\vartheta$ under the semiparametric models that nest the classical probit and\nlogit selection models.",
            "author": [
                "Yuta Okamoto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00439v1",
                "http://arxiv.org/pdf/2311.00439v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00437v1",
            "title": "Untangling Graphs on Surfaces",
            "updated": "2023-11-01T11:00:13Z",
            "published": "2023-11-01T11:00:13Z",
            "summary": "Consider a graph drawn on a surface (for example, the plane minus a finite\nset of obstacle points), possibly with crossings. We provide an algorithm to\ndecide whether such a drawing can be untangled, namely, if one can slide the\nvertices and edges of the graph on the surface (avoiding the obstacles) to\nremove all crossings; in other words, whether the drawing is homotopic to an\nembedding. While the problem boils down to planarity testing when the surface\nis the sphere or the disk (or equivalently the plane without any obstacle), the\nother cases have never been studied before, except when the input graph is a\ncycle, in an abundant literature in topology and more recently by Despr\\'e and\nLazarus [SoCG 2017, J. ACM 2019].\n  Our algorithm runs in O(m + poly(g+b) n log n) time, where g >= 0 and b >= 0\nare the genus and the number of boundary components of the input orientable\nsurface S, and n is the size of the input graph drawing, lying on some fixed\ngraph of size m cellularly embedded on S.\n  We use various techniques from two-dimensional computational topology and\nfrom the theory of hyperbolic surfaces. Most notably, we introduce reducing\ntriangulations, a novel discrete analog of hyperbolic surfaces in the spirit of\nsystems of quads by Lazarus and Rivaud [FOCS 2012] and Erickson and Whittlesey\n[SODA 2013], which have the additional benefit that reduced paths are unique\nand stable upon reversal; they are likely of independent interest. Tailored\ndata structures are needed to achieve certain homotopy tests efficiently on\nthese triangulations. As a key subroutine, we rely on an algorithm to test the\nweak simplicity of a graph drawn on a surface by Akitaya, Fulek, and T\\'oth\n[SODA 2018, TALG 2019].",
            "author": [
                "\u00c9ric Colin de Verdi\u00e8re",
                "Vincent Despr\u00e9",
                "Lo\u00efc Dubois"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00437v1",
                "http://arxiv.org/pdf/2311.00437v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DS",
                "05C10, 57M15, 57N05, 68Q25, 68R10, 68W05, 14E25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00430v1",
            "title": "Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo\n  Labelling",
            "updated": "2023-11-01T10:45:07Z",
            "published": "2023-11-01T10:45:07Z",
            "summary": "As the size of pre-trained speech recognition models increases, running these\nlarge models in low-latency or resource-constrained environments becomes\nchallenging. In this work, we leverage pseudo-labelling to assemble a\nlarge-scale open-source dataset which we use to distill the Whisper model into\na smaller variant, called Distil-Whisper. Using a simple word error rate (WER)\nheuristic, we select only the highest quality pseudo-labels for training. The\ndistilled model is 5.8 times faster with 51% fewer parameters, while performing\nto within 1% WER on out-of-distribution test data in a zero-shot transfer\nsetting. Distil-Whisper maintains the robustness of the Whisper model to\ndifficult acoustic conditions, while being less prone to hallucination errors\non long-form audio. Distil-Whisper is designed to be paired with Whisper for\nspeculative decoding, yielding a 2 times speed-up while mathematically ensuring\nthe same outputs as the original model. To facilitate further research in this\ndomain, we make our training code, inference code and models publicly\naccessible.",
            "author": [
                "Sanchit Gandhi",
                "Patrick von Platen",
                "Alexander M. Rush"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00430v1",
                "http://arxiv.org/pdf/2311.00430v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00428v1",
            "title": "NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust\n  Multi-Exit Neural Networks",
            "updated": "2023-11-01T10:44:05Z",
            "published": "2023-11-01T10:44:05Z",
            "summary": "While multi-exit neural networks are regarded as a promising solution for\nmaking efficient inference via early exits, combating adversarial attacks\nremains a challenging problem. In multi-exit networks, due to the high\ndependency among different submodels, an adversarial example targeting a\nspecific exit not only degrades the performance of the target exit but also\nreduces the performance of all other exits concurrently. This makes multi-exit\nnetworks highly vulnerable to simple adversarial attacks. In this paper, we\npropose NEO-KD, a knowledge-distillation-based adversarial training strategy\nthat tackles this fundamental challenge based on two key contributions. NEO-KD\nfirst resorts to neighbor knowledge distillation to guide the output of the\nadversarial examples to tend to the ensemble outputs of neighbor exits of clean\ndata. NEO-KD also employs exit-wise orthogonal knowledge distillation for\nreducing adversarial transferability across different submodels. The result is\na significantly improved robustness against adversarial attacks. Experimental\nresults on various datasets/models show that our method achieves the best\nadversarial accuracy with reduced computation budgets, compared to the\nbaselines relying on existing adversarial training or knowledge distillation\ntechniques for multi-exit networks.",
            "author": [
                "Seokil Ham",
                "Jungwuk Park",
                "Dong-Jun Han",
                "Jaekyun Moon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00428v1",
                "http://arxiv.org/pdf/2311.00428v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00423v5",
            "title": "LLMRec: Large Language Models with Graph Augmentation for Recommendation",
            "updated": "2023-11-29T05:52:56Z",
            "published": "2023-11-01T10:27:44Z",
            "summary": "The problem of data sparsity has long been a challenge in recommendation\nsystems, and previous studies have attempted to address this issue by\nincorporating side information. However, this approach often introduces side\neffects such as noise, availability issues, and low data quality, which in turn\nhinder the accurate modeling of user preferences and adversely impact\nrecommendation performance. In light of the recent advancements in large\nlanguage models (LLMs), which possess extensive knowledge bases and strong\nreasoning capabilities, we propose a novel framework called LLMRec that\nenhances recommender systems by employing three simple yet effective LLM-based\ngraph augmentation strategies. Our approach leverages the rich content\navailable within online platforms (e.g., Netflix, MovieLens) to augment the\ninteraction graph in three ways: (i) reinforcing user-item interaction egde,\n(ii) enhancing the understanding of item node attributes, and (iii) conducting\nuser node profiling, intuitively from the natural language perspective. By\nemploying these strategies, we address the challenges posed by sparse implicit\nfeedback and low-quality side information in recommenders. Besides, to ensure\nthe quality of the augmentation, we develop a denoised data robustification\nmechanism that includes techniques of noisy implicit feedback pruning and\nMAE-based feature enhancement that help refine the augmented data and improve\nits reliability. Furthermore, we provide theoretical analysis to support the\neffectiveness of LLMRec and clarify the benefits of our method in facilitating\nmodel optimization. Experimental results on benchmark datasets demonstrate the\nsuperiority of our LLM-based augmentation approach over state-of-the-art\ntechniques. To ensure reproducibility, we have made our code and augmented data\npublicly available at: https://github.com/HKUDS/LLMRec.git",
            "author": [
                "Wei Wei",
                "Xubin Ren",
                "Jiabin Tang",
                "Qinyong Wang",
                "Lixin Su",
                "Suqi Cheng",
                "Junfeng Wang",
                "Dawei Yin",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00423v5",
                "http://arxiv.org/pdf/2311.00423v5"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00422v2",
            "title": "Towards Universal Atomic Composability: A Formal Model for Multi-Rollup\n  Environments on Ethereum",
            "updated": "2023-11-04T11:34:12Z",
            "published": "2023-11-01T10:27:18Z",
            "summary": "In the rapidly evolving domain of distributed ledger technology, scalability\nand interoperability have become paramount challenges for both academic and\nindustry sectors. In this paper, we introduce a comprehensive formal model to\naddress atomic composability across multiple rollups on Ethereum. The proposed\nmodel incorporates mechanisms like buffering, dependency management,\nconcurrency control, and the groundbreaking zero-knowledge proofs. Moreover, we\nevaluate its practical repercussions, strengths, and weaknesses, ensuring\nresilience against manipulative or erroneous actions. The application of the\nproposed model to shared sequencers and other existing solutions accentuates\nits versatility and universality.",
            "author": [
                "Dipankar Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00422v2",
                "http://arxiv.org/pdf/2311.00422v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00414v1",
            "title": "Universality of the angled shear wave identity in soft viscous solids",
            "updated": "2023-11-01T10:17:52Z",
            "published": "2023-11-01T10:17:52Z",
            "summary": "Mechanical stress within biological tissue can indicate an anomaly, or can be\nvital of its function, such as stresses in arteries. Measuring these stresses\nin tissue is challenging due to the complex, and often unknown, nature of the\nmaterial properties. Recently, a method called the angled shear wave identity\nwas proposed to predict the stress by measuring the speed of two small\namplitude shear waves. The method does not require prior knowledge of the\nmaterial's constitutive law, making it ideal for complex biological tissues. We\nextend this method, and the underlying identity, to include viscous\ndissipation, which can be significant for biological tissues. To generalise the\nidentity, we consider soft viscoelastic solids described by a generalised\nNewtonian viscous stress, and account for transverse isotropy, a feature that\nis common in muscle tissue, for instance. We then derive the dispersion\nrelationship for small-amplitude shear waves superimposed on a large static\ndeformation. Similarly to the elastic case, the identity is recovered when the\nstress in the material is coaxial with the transverse anisotropy. A key result\nin this paper is that to predict the stress in a viscous materials one would\nneed to measure the wave attenuation as well as the wave speed.",
            "author": [
                "Harold Berjamin",
                "Artur L. Gower"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00414v1",
                "http://arxiv.org/pdf/2311.00414v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "74J05 (Primary) 74D10 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00405v1",
            "title": "Couples can be tractable: New algorithms and hardness results for the\n  Hospitals / Residents problem with Couples",
            "updated": "2023-11-01T09:56:59Z",
            "published": "2023-11-01T09:56:59Z",
            "summary": "In this paper we study the {\\sc Hospitals / Residents problem with Couples}\n({\\sc hrc}), where a solution is a stable matching or a report that none\nexists. We present a novel polynomial-time algorithm that can find a\nnear-feasible stable matching (adjusting the hospitals' capacities by at most\n1) in an {\\sc hrc} instance where the couples' preferences are sub-responsive\n(i.e., if one member switches to a better hospital, than the couple also\nimproves) and sub-complete (i.e., each pair of hospitals that are individually\nacceptable to both members are jointly acceptable for the couple) by reducing\nit to an instance of the {\\sc Stable Fixtures} problem. We also present a\npolynomial-time algorithm for {\\sc hrc} in a sub-responsive, sub-complete\ninstance that is a Dual Market, or where all couples are one of several\npossible types. We show that our algorithm also implies the polynomial-time\nsolvability of a stable b-matching problem, where the underlying graph is a\nmultigraph with loops.\n  We complement our algorithms with several hardness results. We show that {\\sc\nhrc} with sub-responsive and sub-complete couples is NP-hard, even with other\nstrong restrictions. We also show that {\\sc hrc} with a Dual Market is NP-hard\nunder several simultaneous restrictions. Finally, we show that the problem of\nfinding a matching with the minimum number of blocking pairs in {\\sc hrc} is\nnot approximable within $m^{1-\\varepsilon}$, for any $\\varepsilon>0$, where $m$\nis the total length of the hospitals' preference lists, unless P=NP, even if\neach couple applies to only one pair of hospitals. Our polynomial-time\nsolvability results greatly expand the class of known tractable instances of\n{\\sc hrc} and provide additional evidence as to why long-standing entry-level\nlabour markets that allow couples such as the National Resident Matching\nProgram remain successful to this day.",
            "author": [
                "Gergely Cs\u00e1ji",
                "David Manlove",
                "Iain McBride",
                "James Trimble"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00405v1",
                "http://arxiv.org/pdf/2311.00405v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.AI",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00399v1",
            "title": "Enhanced Knowledge Injection for Radiology Report Generation",
            "updated": "2023-11-01T09:50:55Z",
            "published": "2023-11-01T09:50:55Z",
            "summary": "Automatic generation of radiology reports holds crucial clinical value, as it\ncan alleviate substantial workload on radiologists and remind less experienced\nones of potential anomalies. Despite the remarkable performance of various\nimage captioning methods in the natural image field, generating accurate\nreports for medical images still faces challenges, i.e., disparities in visual\nand textual data, and lack of accurate domain knowledge. To address these\nissues, we propose an enhanced knowledge injection framework, which utilizes\ntwo branches to extract different types of knowledge. The Weighted Concept\nKnowledge (WCK) branch is responsible for introducing clinical medical concepts\nweighted by TF-IDF scores. The Multimodal Retrieval Knowledge (MRK) branch\nextracts triplets from similar reports, emphasizing crucial clinical\ninformation related to entity positions and existence. By integrating this\nfiner-grained and well-structured knowledge with the current image, we are able\nto leverage the multi-source knowledge gain to ultimately facilitate more\naccurate report generation. Extensive experiments have been conducted on two\npublic benchmarks, demonstrating that our method achieves superior performance\nover other state-of-the-art methods. Ablation studies further validate the\neffectiveness of two extracted knowledge sources.",
            "author": [
                "Qingqiu Li",
                "Jilan Xu",
                "Runtian Yuan",
                "Mohan Chen",
                "Yuejie Zhang",
                "Rui Feng",
                "Xiaobo Zhang",
                "Shang Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00399v1",
                "http://arxiv.org/pdf/2311.00399v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00394v1",
            "title": "An analysis of large speech models-based representations for speech\n  emotion recognition",
            "updated": "2023-11-01T09:40:40Z",
            "published": "2023-11-01T09:40:40Z",
            "summary": "Large speech models-derived features have recently shown increased\nperformance over signal-based features across multiple downstream tasks, even\nwhen the networks are not finetuned towards the target task. In this paper we\nshow the results of an analysis of several signal- and neural models-derived\nfeatures for speech emotion recognition. We use pretrained models and explore\ntheir inherent potential abstractions of emotions. Simple classification\nmethods are used so as to not interfere or add knowledge to the task. We show\nthat, even without finetuning, some of these large neural speech models'\nrepresentations can enclose information that enables performances close to, and\neven beyond state-of-the-art results across six standard speech emotion\nrecognition datasets.",
            "author": [
                "Adrian Bogdan St\u00e2nea",
                "Vlad Striletchi",
                "Cosmin Striletchi",
                "Adriana Stan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00394v1",
                "http://arxiv.org/pdf/2311.00394v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00393v1",
            "title": "Augmenting deep neural networks with symbolic knowledge: Towards\n  trustworthy and interpretable AI for education",
            "updated": "2023-11-01T09:38:56Z",
            "published": "2023-11-01T09:38:56Z",
            "summary": "Artificial neural networks (ANNs) have shown to be amongst the most important\nartificial intelligence (AI) techniques in educational applications, providing\nadaptive educational services. However, their educational potential is limited\nin practice due to three major challenges: i) difficulty in incorporating\nsymbolic educational knowledge (e.g., causal relationships, and practitioners'\nknowledge) in their development, ii) learning and reflecting biases, and iii)\nlack of interpretability. Given the high-risk nature of education, the\nintegration of educational knowledge into ANNs becomes crucial for developing\nAI applications that adhere to essential educational restrictions, and provide\ninterpretability over the predictions. This research argues that the\nneural-symbolic family of AI has the potential to address the named challenges.\nTo this end, it adapts a neural-symbolic AI framework and accordingly develops\nan approach called NSAI, that injects and extracts educational knowledge into\nand from deep neural networks, for modelling learners computational thinking.\nOur findings reveal that the NSAI approach has better generalizability compared\nto deep neural networks trained merely on training data, as well as training\ndata augmented by SMOTE and autoencoder methods. More importantly, unlike the\nother models, the NSAI approach prioritises robust representations that capture\ncausal relationships between input features and output labels, ensuring safety\nin learning to avoid spurious correlations and control biases in training data.\nFurthermore, the NSAI approach enables the extraction of rules from the learned\nnetwork, facilitating interpretation and reasoning about the path to\npredictions, as well as refining the initial educational knowledge. These\nfindings imply that neural-symbolic AI can overcome the limitations of ANNs in\neducation, enabling trustworthy and interpretable applications.",
            "author": [
                "Danial Hooshyar",
                "Roger Azevedo",
                "Yeongwook Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00393v1",
                "http://arxiv.org/pdf/2311.00393v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "I.2.0, I.2.1, I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00384v1",
            "title": "Impact of Investing Characteristics on Financial Performance of\n  Individual Investors: An Exploratory Study",
            "updated": "2023-11-01T09:21:04Z",
            "published": "2023-11-01T09:21:04Z",
            "summary": "This exploratory study examines which investing characteristics determine\nsuccess in an equity market. Based on data from 403 respondents, exploratory\nfactor analysis results in 13 factors: middle/long time horizon, qualitative\nanalyst, open-minded/disciplined, organized, emotional stability, na\\\"ive,\ngrowth stock, concentrated portfolio, contrarian, value stock, globalized,\nintrinsic value, and price-independent. Multiple linear regression of\nindividual investors' excess return on these factors show statistically\nsignificant relationship. These results deepen our knowledge on what sort of\ninvesting characteristics are required to survive in equity markets.",
            "author": [
                "Poompak Kusawat",
                "Nopadol Rompho"
            ],
            "link": [
                "http://dx.doi.org/10.1109/IEEM44572.2019.8978725",
                "http://arxiv.org/abs/2311.00384v1",
                "http://arxiv.org/pdf/2311.00384v1"
            ],
            "primary_category": "q-fin.GN",
            "category": [
                "q-fin.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00733v1",
            "title": "SAT Solving Using XOR-OR-AND Normal Forms",
            "updated": "2023-11-01T08:59:45Z",
            "published": "2023-11-01T08:59:45Z",
            "summary": "This paper introduces the XOR-OR-AND normal form (XNF) for logical formulas.\nIt is a generalization of the well-known Conjunctive Normal Form (CNF) where\nliterals are replaced by XORs of literals. As a first theoretic result, we show\nthat every formula is equisatisfiable to a formula in 2-XNF, i.e., a formula in\nXNF where each disjunction involves at most two XORs of literals. Subsequently,\nwe present an algorithm which converts Boolean polynomials efficiently from\ntheir Algebraic Normal Form (ANF) to formulas in 2-XNF. Experiments with the\ncipher ASCON-128 show that cryptographic problems, which by design are based\nstrongly on XOR-operations, can be represented using far fewer variables and\nclauses in 2-XNF than in CNF. In order to take advantage of this compact\nrepresentation, new SAT solvers based on input formulas in 2-XNF need to be\ndesigned. By taking inspiration from graph-based 2-CNF SAT solving, we devise a\nnew DPLL-based SAT solver for formulas in 2-XNF. Among others, we present\nadvanced pre- and in-processing techniques. Finally, we give timings for random\n2-XNF instances and instances related to key recovery attacks on round reduced\nASCON-128, where our solver outperforms state-of-the-art alternative solving\napproaches.",
            "author": [
                "Bernhard Andraschko",
                "Julian Danner",
                "Martin Kreuzer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00733v1",
                "http://arxiv.org/pdf/2311.00733v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "math.AC",
                "math.LO",
                "03B70 (Primary) 13P15, 05C90, 94A60 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00371v1",
            "title": "Learning Cooperative Trajectory Representations for Motion Forecasting",
            "updated": "2023-11-01T08:53:05Z",
            "published": "2023-11-01T08:53:05Z",
            "summary": "Motion forecasting is an essential task for autonomous driving, and the\neffective information utilization from infrastructure and other vehicles can\nenhance motion forecasting capabilities. Existing research have primarily\nfocused on leveraging single-frame cooperative information to enhance the\nlimited perception capability of the ego vehicle, while underutilizing the\nmotion and interaction information of traffic participants observed from\ncooperative devices. In this paper, we first propose the cooperative trajectory\nrepresentations learning paradigm. Specifically, we present V2X-Graph, the\nfirst interpretable and end-to-end learning framework for cooperative motion\nforecasting. V2X-Graph employs an interpretable graph to fully leverage the\ncooperative motion and interaction contexts. Experimental results on the\nvehicle-to-infrastructure (V2I) motion forecasting dataset, V2X-Seq,\ndemonstrate the effectiveness of V2X-Graph. To further evaluate on V2X\nscenario, we construct the first real-world vehicle-to-everything (V2X) motion\nforecasting dataset V2X-Traj, and the performance shows the advantage of our\nmethod. We hope both V2X-Graph and V2X-Traj can facilitate the further\ndevelopment of cooperative motion forecasting. Find project at\nhttps://github.com/AIR-THU/V2X-Graph, find data at\nhttps://github.com/AIR-THU/DAIR-V2X-Seq.",
            "author": [
                "Hongzhi Ruan",
                "Haibao Yu",
                "Wenxian Yang",
                "Siqi Fan",
                "Yingjuan Tang",
                "Zaiqing Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00371v1",
                "http://arxiv.org/pdf/2311.00371v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00369v2",
            "title": "Direct System Identification of Dynamical Networks with Partial\n  Measurements: a Maximum Likelihood Approach",
            "updated": "2023-11-03T21:55:02Z",
            "published": "2023-11-01T08:46:18Z",
            "summary": "This paper introduces a novel direct approach to system identification of\ndynamic networks with missing data based on maximum likelihood estimation.\nDynamic networks generally present a singular probability density function,\nwhich poses a challenge in the estimation of their parameters. By leveraging\nknowledge about the network's interconnections, we show that it is possible to\ntransform the problem into more tractable form by applying linear\ntransformations. This results in a nonsingular probability density function,\nenabling the application of maximum likelihood estimation techniques. Our\npreliminary numerical results suggest that when combined with global\noptimization algorithms or a suitable initialization strategy, we are able to\nobtain a good estimate the dynamic of the internal systems.",
            "author": [
                "Jo\u00e3o Victor Galv\u00e3o da Mata",
                "Anders Hansson",
                "Martin S. Andersen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00369v2",
                "http://arxiv.org/pdf/2311.00369v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00367v1",
            "title": "Prompt-based Logical Semantics Enhancement for Implicit Discourse\n  Relation Recognition",
            "updated": "2023-11-01T08:38:08Z",
            "published": "2023-11-01T08:38:08Z",
            "summary": "Implicit Discourse Relation Recognition (IDRR), which infers discourse\nrelations without the help of explicit connectives, is still a crucial and\nchallenging task for discourse parsing. Recent works tend to exploit the\nhierarchical structure information from the annotated senses, which demonstrate\nenhanced discourse relation representations can be obtained by integrating\nsense hierarchy. Nevertheless, the performance and robustness for IDRR are\nsignificantly constrained by the availability of annotated data. Fortunately,\nthere is a wealth of unannotated utterances with explicit connectives, that can\nbe utilized to acquire enriched discourse relation features. In light of such\nmotivation, we propose a Prompt-based Logical Semantics Enhancement (PLSE)\nmethod for IDRR. Essentially, our method seamlessly injects knowledge relevant\nto discourse relation into pre-trained language models through prompt-based\nconnective prediction. Furthermore, considering the prompt-based connective\nprediction exhibits local dependencies due to the deficiency of masked language\nmodel (MLM) in capturing global semantics, we design a novel self-supervised\nlearning objective based on mutual information maximization to derive enhanced\nrepresentations of logical semantics for IDRR. Experimental results on PDTB 2.0\nand CoNLL16 datasets demonstrate that our method achieves outstanding and\nconsistent performance against the current state-of-the-art models.",
            "author": [
                "Chenxu Wang",
                "Ping Jian",
                "Mu Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00367v1",
                "http://arxiv.org/pdf/2311.00367v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00365v1",
            "title": "Deriving Characteristic Mode Eigenvalue Behavior Using Subduction of\n  Group Representations",
            "updated": "2023-11-01T08:34:57Z",
            "published": "2023-11-01T08:34:57Z",
            "summary": "A method to derive features of modal eigenvalue traces from known and\nunderstood solutions is proposed. It utilizes the concept of subduction from\npoint group theory to obtain the symmetry properties of a target structure from\nthose of a structure with a higher order of symmetry. This is applied exemplary\nto the analytically known characteristic modes (CMs) of the spherical shell.\nThe eigenvalue behavior of a cube in free space and a cuboid on a perfectly\nelectrically conducting plane are continuously derived from this. In this\nprocess, formerly crossing eigenvalue traces are found to split up, forming a\nsplit trace crossing avoidance (STCA). This finding is used to explain\nindentations in eigenvalue traces observed for three-dimensional structures,\nthat are of increasing interest in recent literature. The utility of this\nknowledge is exemplified through a demonstrator antenna design. The dimensions\nof the antenna structure are chosen so the STCA is outside the target frequency\nrange, avoiding negative impacts on input matching and the frequency stability\nof the far field patterns.",
            "author": [
                "Lukas Grundmann",
                "Lukas Warkentin",
                "Dirk Manteuffel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00365v1",
                "http://arxiv.org/pdf/2311.00365v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04910v1",
            "title": "Ontology-Driven Processing of Transdisciplinary Domain Knowledge",
            "updated": "2023-11-01T07:42:34Z",
            "published": "2023-11-01T07:42:34Z",
            "summary": "The monograph discusses certain aspects of modern real-world problems facing\nhumanity, which are much more challenging than scientific ones. Modern science\nis unable to solve them in a fundamental way. Vernadsky's noosphere thesis, in\nfact, appeals to the scientific worldview that needs to be built in a way that\novercomes the interdisciplinary barriers and increases the effectiveness of\ninterdisciplinary interaction and modern science overall. We are talking about\nthe general transdisciplinary knowledge. In world practice, there is still no\nsystematic methodology and a specific form of generally accepted valid\nscientific theory that would provide transdisciplinary knowledge. Non-linear\ninterdisciplinary interaction is the standard of evolution of modern science.\nAt the same time, a new transdisciplinary theory (domain of scientific\nresearch) is being de facto created and the process is repeated many times:\nfrom an individual or group of disciplines, through interdisciplinary\ninteraction, in a direction that brings us closer to creating a holistic\ngeneral scientific worldview.",
            "author": [
                "Oleksandr Palagin",
                "Mykola Petrenko",
                "Sergii Kryvyi",
                "Mykola Boyko",
                "Kyrylo Malakhov"
            ],
            "link": [
                "http://dx.doi.org/10.31274/isudp.2023.140",
                "http://arxiv.org/abs/2311.04910v1",
                "http://arxiv.org/pdf/2311.04910v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05505v1",
            "title": "On regular 2-path Hamiltonian graphs",
            "updated": "2023-11-01T06:55:43Z",
            "published": "2023-11-01T06:55:43Z",
            "summary": "Kronk introduced the $l$-path hamiltonianicity of graphs in 1969. A graph is\n$l$-path Hamiltonian if every path of length not exceeding $l$ is contained in\na Hamiltonian cycle. We have shown that if $P=uvz$ is a 2-path of a\n2-connected, $k$-regular graph on at most $2k$ vertices and $G - V(P)$ is\nconnected, then there must exist a Hamiltonian cycle in $G$ that contains the\n2-path $P$. In this paper, we characterize a class of graphs that illustrate\nthe sharpness of the bound $2k$. Additionally, we show that by excluding the\nclass of graphs, both 2-connected, $k$-regular graphs on at most $2k + 1$\nvertices and 3-connected, $k$-regular graphs on at most $3k-6$ vertices satisfy\nthat there is a Hamiltonian cycle containing the 2-path $P$ if $G\\setminus\nV(P)$ is connected.",
            "author": [
                "Xia Li",
                "Weihua Yang",
                "Bo Zhang",
                "Shuang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05505v1",
                "http://arxiv.org/pdf/2311.05505v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01473v1",
            "title": "Adversarial Examples in the Physical World: A Survey",
            "updated": "2023-11-01T06:55:09Z",
            "published": "2023-11-01T06:55:09Z",
            "summary": "Deep neural networks (DNNs) have demonstrated high vulnerability to\nadversarial examples. Besides the attacks in the digital world, the practical\nimplications of adversarial examples in the physical world present significant\nchallenges and safety concerns. However, current research on physical\nadversarial examples (PAEs) lacks a comprehensive understanding of their unique\ncharacteristics, leading to limited significance and understanding. In this\npaper, we address this gap by thoroughly examining the characteristics of PAEs\nwithin a practical workflow encompassing training, manufacturing, and\nre-sampling processes. By analyzing the links between physical adversarial\nattacks, we identify manufacturing and re-sampling as the primary sources of\ndistinct attributes and particularities in PAEs. Leveraging this knowledge, we\ndevelop a comprehensive analysis and classification framework for PAEs based on\ntheir specific characteristics, covering over 100 studies on physical-world\nadversarial examples. Furthermore, we investigate defense strategies against\nPAEs and identify open challenges and opportunities for future research. We aim\nto provide a fresh, thorough, and systematic understanding of PAEs, thereby\npromoting the development of robust adversarial learning and its application in\nopen-world scenarios.",
            "author": [
                "Jiakai Wang",
                "Donghua Wang",
                "Jin Hu",
                "Siyang Wu",
                "Tingsong Jiang",
                "Wen Yao",
                "Aishan Liu",
                "Xianglong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01473v1",
                "http://arxiv.org/pdf/2311.01473v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00333v1",
            "title": "Caseformer: Pre-training for Legal Case Retrieval",
            "updated": "2023-11-01T06:52:41Z",
            "published": "2023-11-01T06:52:41Z",
            "summary": "Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.",
            "author": [
                "Weihang Su",
                "Qingyao Ai",
                "Yueyue Wu",
                "Yixiao Ma",
                "Haitao Li",
                "Yiqun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00333v1",
                "http://arxiv.org/pdf/2311.00333v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00327v1",
            "title": "Multi-task Representation Learning for Pure Exploration in Bilinear\n  Bandits",
            "updated": "2023-11-01T06:30:45Z",
            "published": "2023-11-01T06:30:45Z",
            "summary": "We study multi-task representation learning for the problem of pure\nexploration in bilinear bandits. In bilinear bandits, an action takes the form\nof a pair of arms from two different entity types and the reward is a bilinear\nfunction of the known feature vectors of the arms. In the \\textit{multi-task\nbilinear bandit problem}, we aim to find optimal actions for multiple tasks\nthat share a common low-dimensional linear representation. The objective is to\nleverage this characteristic to expedite the process of identifying the best\npair of arms for all tasks. We propose the algorithm GOBLIN that uses an\nexperimental design approach to optimize sample allocations for learning the\nglobal representation as well as minimize the number of samples needed to\nidentify the optimal pair of arms in individual tasks. To the best of our\nknowledge, this is the first study to give sample complexity analysis for pure\nexploration in bilinear bandits with shared representation. Our results\ndemonstrate that by learning the shared representation across tasks, we achieve\nsignificantly improved sample complexity compared to the traditional approach\nof solving tasks independently.",
            "author": [
                "Subhojyoti Mukherjee",
                "Qiaomin Xie",
                "Josiah P. Hanna",
                "Robert Nowak"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00327v1",
                "http://arxiv.org/pdf/2311.00327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00322v2",
            "title": "Robust Graph Clustering via Meta Weighting for Noisy Graphs",
            "updated": "2023-11-08T08:01:49Z",
            "published": "2023-11-01T06:12:34Z",
            "summary": "How can we find meaningful clusters in a graph robustly against noise edges?\nGraph clustering (i.e., dividing nodes into groups of similar ones) is a\nfundamental problem in graph analysis with applications in various fields.\nRecent studies have demonstrated that graph neural network (GNN) based\napproaches yield promising results for graph clustering. However, we observe\nthat their performance degenerates significantly on graphs with noise edges,\nwhich are prevalent in practice. In this work, we propose MetaGC for robust\nGNN-based graph clustering. MetaGC employs a decomposable clustering loss\nfunction, which can be rephrased as a sum of losses over node pairs. We add a\nlearnable weight to each node pair, and MetaGC adaptively adjusts the weights\nof node pairs using meta-weighting so that the weights of meaningful node pairs\nincrease and the weights of less-meaningful ones (e.g., noise edges) decrease.\nWe show empirically that MetaGC learns weights as intended and consequently\noutperforms the state-of-the-art GNN-based competitors, even when they are\nequipped with separate denoising schemes, on five real-world graphs under\nvarying levels of noise. Our code and datasets are available at\nhttps://github.com/HyeonsooJo/MetaGC.",
            "author": [
                "Hyeonsoo Jo",
                "Fanchen Bu",
                "Kijung Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00322v2",
                "http://arxiv.org/pdf/2311.00322v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00300v1",
            "title": "Entity Alignment Method of Science and Technology Patent based on Graph\n  Convolution Network and Information Fusion",
            "updated": "2023-11-01T05:04:55Z",
            "published": "2023-11-01T05:04:55Z",
            "summary": "The entity alignment of science and technology patents aims to link the\nequivalent entities in the knowledge graph of different science and technology\npatent data sources. Most entity alignment methods only use graph neural\nnetwork to obtain the embedding of graph structure or use attribute text\ndescription to obtain semantic representation, ignoring the process of\nmulti-information fusion in science and technology patents. In order to make\nuse of the graphic structure and auxiliary information such as the name,\ndescription and attribute of the patent entity, this paper proposes an entity\nalignment method based on the graph convolution network for science and\ntechnology patent information fusion. Through the graph convolution network and\nBERT model, the structure information and entity attribute information of the\nscience and technology patent knowledge graph are embedded and represented to\nachieve multi-information fusion, thus improving the performance of entity\nalignment. Experiments on three benchmark data sets show that the proposed\nmethod Hit@K The evaluation indicators are better than the existing methods.",
            "author": [
                "Runze Fang",
                "Yawen Li",
                "Yingxia Shao",
                "Zeli Guan",
                "Zhe Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00300v1",
                "http://arxiv.org/pdf/2311.00300v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00296v1",
            "title": "Semantic Representation Learning of Scientific Literature based on\n  Adaptive Feature and Graph Neural Network",
            "updated": "2023-11-01T05:00:44Z",
            "published": "2023-11-01T05:00:44Z",
            "summary": "Because most of the scientific literature data is unmarked, it makes semantic\nrepresentation learning based on unsupervised graph become crucial. At the same\ntime, in order to enrich the features of scientific literature, a learning\nmethod of semantic representation of scientific literature based on adaptive\nfeatures and graph neural network is proposed. By introducing the adaptive\nfeature method, the features of scientific literature are considered globally\nand locally. The graph attention mechanism is used to sum the features of\nscientific literature with citation relationship, and give each scientific\nliterature different feature weights, so as to better express the correlation\nbetween the features of different scientific literature. In addition, an\nunsupervised graph neural network semantic representation learning method is\nproposed. By comparing the mutual information between the positive and negative\nlocal semantic representation of scientific literature and the global graph\nsemantic representation in the potential space, the graph neural network can\ncapture the local and global information, thus improving the learning ability\nof the semantic representation of scientific literature. The experimental\nresults show that the proposed learning method of semantic representation of\nscientific literature based on adaptive feature and graph neural network is\ncompetitive on the basis of scientific literature classification, and has\nachieved good results.",
            "author": [
                "Hongrui Gao",
                "Yawen Li",
                "Meiyu Liang",
                "Zeli Guan",
                "Zhe Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00296v1",
                "http://arxiv.org/pdf/2311.00296v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00291v1",
            "title": "Graph Representation Learning for Infrared and Visible Image Fusion",
            "updated": "2023-11-01T04:46:20Z",
            "published": "2023-11-01T04:46:20Z",
            "summary": "Infrared and visible image fusion aims to extract complementary features to\nsynthesize a single fused image. Many methods employ convolutional neural\nnetworks (CNNs) to extract local features due to its translation invariance and\nlocality. However, CNNs fail to consider the image's non-local self-similarity\n(NLss), though it can expand the receptive field by pooling operations, it\nstill inevitably leads to information loss. In addition, the transformer\nstructure extracts long-range dependence by considering the correlativity among\nall image patches, leading to information redundancy of such transformer-based\nmethods. However, graph representation is more flexible than grid (CNN) or\nsequence (transformer structure) representation to address irregular objects,\nand graph can also construct the relationships among the spatially repeatable\ndetails or texture with far-space distance. Therefore, to address the above\nissues, it is significant to convert images into the graph space and thus adopt\ngraph convolutional networks (GCNs) to extract NLss. This is because the graph\ncan provide a fine structure to aggregate features and propagate information\nacross the nearest vertices without introducing redundant information.\nConcretely, we implement a cascaded NLss extraction pattern to extract NLss of\nintra- and inter-modal by exploring interactions of different image pixels in\nintra- and inter-image positional distance. We commence by preforming GCNs on\neach intra-modal to aggregate features and propagate information to extract\nindependent intra-modal NLss. Then, GCNs are performed on the concatenate\nintra-modal NLss features of infrared and visible images, which can explore the\ncross-domain NLss of inter-modal to reconstruct the fused image. Ablation\nstudies and extensive experiments illustrates the effectiveness and superiority\nof the proposed method on three datasets.",
            "author": [
                "Jing Li",
                "Lu Bai",
                "Bin Yang",
                "Chang Li",
                "Lingfei Ma",
                "Edwin R. Hancock"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00291v1",
                "http://arxiv.org/pdf/2311.00291v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00290v2",
            "title": "Inference of CO2 flow patterns -- a feasibility study",
            "updated": "2023-11-29T01:55:38Z",
            "published": "2023-11-01T04:41:25Z",
            "summary": "As the global deployment of carbon capture and sequestration (CCS) technology\nintensifies in the fight against climate change, it becomes increasingly\nimperative to establish robust monitoring and detection mechanisms for\npotential underground CO2 leakage, particularly through pre-existing or induced\nfaults in the storage reservoir's seals. While techniques such as history\nmatching and time-lapse seismic monitoring of CO2 storage have been used\nsuccessfully in tracking the evolution of CO2 plumes in the subsurface, these\nmethods lack principled approaches to characterize uncertainties related to the\nCO2 plumes' behavior. Inclusion of systematic assessment of uncertainties is\nessential for risk mitigation for the following reasons: (i) CO2 plume-induced\nchanges are small and seismic data is noisy; (ii) changes between regular and\nirregular (e.g., caused by leakage) flow patterns are small; and (iii) the\nreservoir properties that control the flow are strongly heterogeneous and\ntypically only available as distributions. To arrive at a formulation capable\nof inferring flow patterns for regular and irregular flow from well and seismic\ndata, the performance of conditional normalizing flow will be analyzed on a\nseries of carefully designed numerical experiments. While the inferences\npresented are preliminary in the context of an early CO2 leakage detection\nsystem, the results do indicate that inferences with conditional normalizing\nflows can produce high-fidelity estimates for CO2 plumes with or without\nleakage. We are also confident that the inferred uncertainty is reasonable\nbecause it correlates well with the observed errors. This uncertainty stems\nfrom noise in the seismic data and from the lack of precise knowledge of the\nreservoir's fluid flow properties.",
            "author": [
                "Abhinav Prakash Gahlot",
                "Huseyin Tuna Erdinc",
                "Rafael Orozco",
                "Ziyi Yin",
                "Felix J. Herrmann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00290v2",
                "http://arxiv.org/pdf/2311.00290v2"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI",
                "cs.LG",
                "math-ph",
                "math.MP",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00289v1",
            "title": "Precise Error Rates for Computationally Efficient Testing",
            "updated": "2023-11-01T04:41:16Z",
            "published": "2023-11-01T04:41:16Z",
            "summary": "We revisit the fundamental question of simple-versus-simple hypothesis\ntesting with an eye towards computational complexity, as the statistically\noptimal likelihood ratio test is often computationally intractable in\nhigh-dimensional settings. In the classical spiked Wigner model (with a general\ni.i.d. spike prior) we show that an existing test based on linear spectral\nstatistics achieves the best possible tradeoff curve between type I and type II\nerror rates among all computationally efficient tests, even though there are\nexponential-time tests that do better. This result is conditional on an\nappropriate complexity-theoretic conjecture, namely a natural strengthening of\nthe well-established low-degree conjecture. Our result shows that the spectrum\nis a sufficient statistic for computationally bounded tests (but not for all\ntests).\n  To our knowledge, our approach gives the first tool for reasoning about the\nprecise asymptotic testing error achievable with efficient computation. The\nmain ingredients required for our hardness result are a sharp bound on the norm\nof the low-degree likelihood ratio along with (counterintuitively) a positive\nresult on achievability of testing. This strategy appears to be new even in the\nsetting of unbounded computation, in which case it gives an alternate way to\nanalyze the fundamental statistical limits of testing.",
            "author": [
                "Ankur Moitra",
                "Alexander S. Wein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00289v1",
                "http://arxiv.org/pdf/2311.00289v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00287v1",
            "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data\n  Generation with Large Language Models",
            "updated": "2023-11-01T04:37:28Z",
            "published": "2023-11-01T04:37:28Z",
            "summary": "Clinical natural language processing requires methods that can address\ndomain-specific challenges, such as complex medical terminology and clinical\ncontexts. Recently, large language models (LLMs) have shown promise in this\ndomain. Yet, their direct deployment can lead to privacy issues and are\nconstrained by resources. To address this challenge, we delve into synthetic\nclinical text generation using LLMs for clinical NLP tasks. We propose an\ninnovative, resource-efficient approach, ClinGen, which infuses knowledge into\nthe process. Our model involves clinical knowledge extraction and\ncontext-informed LLM prompting. Both clinical topics and writing styles are\ndrawn from external domain-specific knowledge graphs and LLMs to guide data\ngeneration. Our extensive empirical study across 7 clinical NLP tasks and 16\ndatasets reveals that ClinGen consistently enhances performance across various\ntasks, effectively aligning the distribution of real datasets and significantly\nenriching the diversity of generated training instances. We will publish our\ncode and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.",
            "author": [
                "Ran Xu",
                "Hejie Cui",
                "Yue Yu",
                "Xuan Kan",
                "Wenqi Shi",
                "Yuchen Zhuang",
                "Wei Jin",
                "Joyce Ho",
                "Carl Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00287v1",
                "http://arxiv.org/pdf/2311.00287v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00285v1",
            "title": "Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space\n  Detection Approach",
            "updated": "2023-11-01T04:36:18Z",
            "published": "2023-11-01T04:36:18Z",
            "summary": "Open Set Domain Adaptation (OSDA) aims to cope with the distribution and\nlabel shifts between the source and target domains simultaneously, performing\naccurate classification for known classes while identifying unknown class\nsamples in the target domain. Most existing OSDA approaches, depending on the\nfinal image feature space of deep models, require manually-tuned thresholds,\nand may easily misclassify unknown samples as known classes. Mixture-of-Expert\n(MoE) could be a remedy. Within an MoE, different experts address different\ninput features, producing unique expert routing patterns for different classes\nin a routing feature space. As a result, unknown class samples may also display\ndifferent expert routing patterns to known classes. This paper proposes\nDual-Space Detection, which exploits the inconsistencies between the image\nfeature space and the routing feature space to detect unknown class samples\nwithout any threshold. Graph Router is further introduced to better make use of\nthe spatial information among image patches. Experiments on three different\ndatasets validated the effectiveness and superiority of our approach. The code\nwill come soon.",
            "author": [
                "Zhenbang Du",
                "Jiayu An",
                "Jiahao Hong",
                "Dongrui Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00285v1",
                "http://arxiv.org/pdf/2311.00285v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00284v1",
            "title": "Model-driven Engineering for Machine Learning Components: A Systematic\n  Literature Review",
            "updated": "2023-11-01T04:29:47Z",
            "published": "2023-11-01T04:29:47Z",
            "summary": "Context: Machine Learning (ML) has become widely adopted as a component in\nmany modern software applications. Due to the large volumes of data available,\norganizations want to increasingly leverage their data to extract meaningful\ninsights and enhance business profitability. ML components enable predictive\ncapabilities, anomaly detection, recommendation, accurate image and text\nprocessing, and informed decision-making. However, developing systems with ML\ncomponents is not trivial; it requires time, effort, knowledge, and expertise\nin ML, data processing, and software engineering. There have been several\nstudies on the use of model-driven engineering (MDE) techniques to address\nthese challenges when developing traditional software and cyber-physical\nsystems. Recently, there has been a growing interest in applying MDE for\nsystems with ML components. Objective: The goal of this study is to further\nexplore the promising intersection of MDE with ML (MDE4ML) through a systematic\nliterature review (SLR). Through this SLR, we wanted to analyze existing\nstudies, including their motivations, MDE solutions, evaluation techniques, key\nbenefits and limitations. Results: We analyzed selected studies with respect to\nseveral areas of interest and identified the following: 1) the key motivations\nbehind using MDE4ML; 2) a variety of MDE solutions applied, such as modeling\nlanguages, model transformations, tool support, targeted ML aspects,\ncontributions and more; 3) the evaluation techniques and metrics used; and 4)\nthe limitations and directions for future work. We also discuss the gaps in\nexisting literature and provide recommendations for future research.\nConclusion: This SLR highlights current trends, gaps and future research\ndirections in the field of MDE4ML, benefiting both researchers and\npractitioners",
            "author": [
                "Hira Naveed",
                "Chetan Arora",
                "Hourieh Khalajzadeh",
                "John Grundy",
                "Omar Haggag"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00284v1",
                "http://arxiv.org/pdf/2311.00284v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00280v1",
            "title": "RF-Enhanced Road Infrastructure for Intelligent Transportation",
            "updated": "2023-11-01T04:05:25Z",
            "published": "2023-11-01T04:05:25Z",
            "summary": "The EPC GEN 2 communication protocol for Ultra-high frequency Radio Frequency\nIdentification (RFID) has offered a promising avenue for advancing the\nintelligence of transportation infrastructure. With the capability of linking\nvehicles to RFID readers to crowdsource information from RFID tags on road\ninfrastructures, the RF-enhanced road infrastructure (REI) can potentially\ntransform data acquisition for urban transportation. Despite its potential, the\nbroader adoption of RFID technologies in building intelligent roads has been\nlimited by a deficiency in understanding how the GEN 2 protocol impacts system\nperformance under different transportation settings. This paper fills this\nknowledge gap by presenting the system architecture and detailing the design\nchallenges associated with REI. Comprehensive real-world experiments are\nconducted to assess REI's effectiveness across various urban contexts. The\nresults yield crucial insights into the optimal design of on-vehicle RFID\nreaders and on-road RFID tags, considering the constraints imposed by vehicle\ndynamics, road geometries, and tag placements. With the optimized designs of\nencoding schemes for reader-tag communication and on-vehicle antennas, REI is\nable to fulfill the requirements of traffic sign inventory management and\nenvironmental monitoring while falling short of catering to the demand for\nhigh-speed navigation. In particular, the Miller 2 encoding scheme strikes the\nbest balance between reading performance (e.g., throughput) and noise tolerance\nfor the multipath effect. Additionally, we show that the on-vehicle antenna\nshould be oriented to maximize the available time for reading on-road tags,\nalthough it may reduce the received power by the tags in the forward link.",
            "author": [
                "Dajiang Suo",
                "Heyi Li",
                "Rahul Bhattacharyya",
                "Zijin Wang",
                "Shengxuan Ding",
                "Ou Zheng",
                "Daniel Valderas",
                "Joan Meli\u00e0-Segu\u00ed",
                "Mohamed Abdel-Aty",
                "Sanjay E. Sarma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00280v1",
                "http://arxiv.org/pdf/2311.00280v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00279v2",
            "title": "Accelerating Maximal Clique Enumeration via Graph Reduction",
            "updated": "2023-11-02T10:19:57Z",
            "published": "2023-11-01T04:05:01Z",
            "summary": "As a fundamental task in graph data management, maximal clique enumeration\n(MCE) has attracted extensive attention from both academic and industrial\ncommunities due to its wide range of applications. However, MCE is very\nchallenging as the number of maximal cliques may grow exponentially with the\nnumber of vertices. The state-of-the-art methods adopt a recursive paradigm to\nenumerate maximal cliques exhaustively, suffering from a large amount of\nredundant computation. In this paper, we propose a novel reduction-based\nframework for MCE, namely RMCE, that aims to reduce the search space and\nminimize unnecessary computations. The proposed framework RMCE incorporates\nthree kinds of powerful reduction techniques including global reduction,\ndynamic reduction, and maximality check reduction. Global and dynamic reduction\ntechniques effectively reduce the size of the input graph and dynamically\nconstruct subgraphs during the recursive subtasks, respectively. The maximality\ncheck reduction minimizes the computation for ensuring maximality by utilizing\nneighborhood dominance between visited vertices. Extensive experiments on 18\nreal graphs demonstrate the effectiveness of our proposed method. It achieves\nremarkable speedups up to 44.7x compared to existing approaches.",
            "author": [
                "Wen Deng",
                "Weiguo Zheng",
                "Hong Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00279v2",
                "http://arxiv.org/pdf/2311.00279v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00273v1",
            "title": "SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities\n  through Fine-tuning with Multi-turn Empathy Conversations",
            "updated": "2023-11-01T03:49:52Z",
            "published": "2023-11-01T03:49:52Z",
            "summary": "Large language models (LLMs) have been widely applied in various fields due\nto their excellent capability for memorizing knowledge and chain of thought\n(CoT). When these language models are applied in the field of psychological\ncounseling, they often rush to provide universal advice. However, when users\nseek psychological support, they need to gain empathy, trust, understanding and\ncomfort, rather than just reasonable advice. To this end, we constructed a\nmulti-turn empathetic conversation dataset of more than 2 million samples, in\nwhich the input is the multi-turn conversation context, and the target is\nempathetic responses that cover expressions such as questioning, comfort,\nrecognition, listening, trust, emotional support, etc. Experiments have shown\nthat the empathy ability of LLMs can be significantly enhanced when finetuning\nby using multi-turn dialogue history and responses that are closer to the\nexpression of a psychological consultant.",
            "author": [
                "Yirong Chen",
                "Xiaofen Xing",
                "Jingkai Lin",
                "Huimin Zheng",
                "Zhenyu Wang",
                "Qi Liu",
                "Xiangmin Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00273v1",
                "http://arxiv.org/pdf/2311.00273v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00252v1",
            "title": "Active Neural Topological Mapping for Multi-Agent Exploration",
            "updated": "2023-11-01T03:06:14Z",
            "published": "2023-11-01T03:06:14Z",
            "summary": "This paper investigates the multi-agent cooperative exploration problem,\nwhich requires multiple agents to explore an unseen environment via sensory\nsignals in a limited time. A popular approach to exploration tasks is to\ncombine active mapping with planning. Metric maps capture the details of the\nspatial representation, but are with high communication traffic and may vary\nsignificantly between scenarios, resulting in inferior generalization.\nTopological maps are a promising alternative as they consist only of nodes and\nedges with abstract but essential information and are less influenced by the\nscene structures. However, most existing topology-based exploration tasks\nutilize classical methods for planning, which are time-consuming and\nsub-optimal due to their handcrafted design. Deep reinforcement learning (DRL)\nhas shown great potential for learning (near) optimal policies through fast\nend-to-end inference. In this paper, we propose Multi-Agent Neural Topological\nMapping (MANTM) to improve exploration efficiency and generalization for\nmulti-agent exploration tasks. MANTM mainly comprises a Topological Mapper and\na novel RL-based Hierarchical Topological Planner (HTP). The Topological Mapper\nemploys a visual encoder and distance-based heuristics to construct a graph\ncontaining main nodes and their corresponding ghost nodes. The HTP leverages\ngraph neural networks to capture correlations between agents and graph nodes in\na coarse-to-fine manner for effective global goal selection. Extensive\nexperiments conducted in a physically-realistic simulator, Habitat, demonstrate\nthat MANTM reduces the steps by at least 26.40% over planning-based baselines\nand by at least 7.63% over RL-based competitors in unseen scenarios.",
            "author": [
                "Xinyi Yang",
                "Yuxiang Yang",
                "Chao Yu",
                "Jiayu Chen",
                "Jingchen Yu",
                "Haibing Ren",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00252v1",
                "http://arxiv.org/pdf/2311.00252v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14678v1",
            "title": "Data-driven recommendations for enhancing real-time natural hazard\n  warnings, communication, and response",
            "updated": "2023-11-01T02:59:45Z",
            "published": "2023-11-01T02:59:45Z",
            "summary": "The effectiveness and adequacy of natural hazard warnings hinges on the\navailability of data and its transformation into actionable knowledge for the\npublic. Real-time warning communication and emergency response therefore need\nto be evaluated from a data science perspective. However, there are currently\ngaps between established data science best practices and their application in\nsupporting natural hazard warnings. This Perspective reviews existing\ndata-driven approaches that underpin real-time warning communication and\nemergency response, highlighting limitations in hazard and impact forecasts.\nFour main themes for enhancing warnings are emphasised: (i) applying\nbest-practice principles in visualising hazard forecasts, (ii) data\nopportunities for more effective impact forecasts, (iii) utilising data for\nmore localised forecasts, and (iv) improving data-driven decision-making using\nuncertainty. Motivating examples are provided from the extensive flooding\nexperienced in Australia in 2022. This Perspective shows the capacity for\nimproving the efficacy of natural hazard warnings using data science, and the\ncollaborative potential between the data science and natural hazards\ncommunities.",
            "author": [
                "Kate R. Saunders",
                "Owen Forbes",
                "Jess K. Hopf",
                "Charlotte R. Patterson",
                "Sarah A. Vollert",
                "Kaitlyn Brown",
                "Raiha Browning",
                "Miguel Canizares",
                "Richard S. Cottrell",
                "Lanxi Li",
                "Catherine J. S. Kim",
                "Tace P. Stewart",
                "Connie Susilawati",
                "Xiang Y. Zhao",
                "Kate J. Helmstedt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14678v1",
                "http://arxiv.org/pdf/2311.14678v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00226v2",
            "title": "Transformers are Efficient In-Context Estimators for Wireless\n  Communication",
            "updated": "2023-12-03T04:31:28Z",
            "published": "2023-11-01T02:16:24Z",
            "summary": "Pre-trained transformers can perform in-context learning, where they adapt to\na new task using only a small number of prompts without any explicit model\noptimization. Inspired by this attribute, we propose a novel approach, called\nin-context estimation, for the canonical communication problem of estimating\ntransmitted symbols from received symbols. A communication channel is\nessentially a noisy function that maps transmitted symbols to received symbols,\nand this function can be represented by an unknown parameter whose statistics\ndepend on an (also unknown) latent context. Conventional approaches typically\ndo not fully exploit hierarchical model with the latent context. Instead, they\noften use mismatched priors to form a linear minimum mean-squared error\nestimate of the channel parameter, which is then used to estimate successive,\nunknown transmitted symbols. We make the basic connection that transformers\nshow excellent contextual sequence completion with a few prompts, and so they\nshould be able to implicitly determine the latent context from pilot symbols to\nperform end-to-end in-context estimation of transmitted symbols. Furthermore,\nthe transformer should use information efficiently, i.e., it should utilize any\npilots received to attain the best possible symbol estimates. Through extensive\nsimulations, we show that in-context estimation not only significantly\noutperforms standard approaches, but also achieves the same performance as an\nestimator with perfect knowledge of the latent context within a few context\nexamples. Thus, we make a strong case that transformers are efficient\nin-context estimators in the communication setting.",
            "author": [
                "Vicram Rajagopalan",
                "Vishnu Teja Kunde",
                "Chandra Shekhara Kaushik Valmeekam",
                "Krishna Narayanan",
                "Srinivas Shakkottai",
                "Dileep Kalathil",
                "Jean-Francois Chamberland"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00226v2",
                "http://arxiv.org/pdf/2311.00226v2"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00219v1",
            "title": "The free boundary for a semilinear non-homogeneous Bernoulli problem",
            "updated": "2023-11-01T01:42:21Z",
            "published": "2023-11-01T01:42:21Z",
            "summary": "In the classical homogeneous one-phase Bernoulli-type problem, the free\nboundary consists of a \"regular\" part and a \"singular\" part, as Alt and\nCaffarelli have shown in their pioneer work (J. Reine Angew. Math., 325,\n105-144, 1981) that regular points are $C^{1,\\gamma}$ in two-dimensions. Later,\nWeiss (J. Geom. Anal., 9, 317-326, 1999) first realized that in higher\ndimensions a critical dimension $d^{*}$ exists so that the singularities of the\nfree boundary can only occur when $d\\geqslant d^{*}$.\n  In this paper, we consider a non-homogeneous semilinear one-phase\nBernoulli-type problem, and we show that the free boundary is a disjoint union\nof a regular and a singular set. Moreover, the regular set is locally the graph\nof a $C^{1,\\gamma}$ function for some $\\gamma\\in(0,1)$. In addition, there\nexists a critical dimension $d^{*}$ so that the singular set is empty if\n$d<d^{*}$, discrete if $d=d^{*}$ and of locally finite $\\mathcal{H}^{d-d^{*}}$\nHausdorff measure if $d>d^{*}$. As a byproduct, we relate the existence of\nviscosity solutions of a non-homogeneous problem to the Weiss-boundary adjusted\nenergy, which provides an alternative proof to existence of viscosity solutions\nfor non-homogeneous problems.",
            "author": [
                "Lili Du",
                "Chunlei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00219v1",
                "http://arxiv.org/pdf/2311.00219v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "35R35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00211v1",
            "title": "Anachronic Tertiary Studies in Software Engineering: An Exploratory\n  Quaternary Study",
            "updated": "2023-11-01T00:54:55Z",
            "published": "2023-11-01T00:54:55Z",
            "summary": "Systematic literature reviews tentativelydescribe the state of the art in a\ngiven research area. However, the continuous publication of new primary and\nsecondary studies following the release of a tertiary study can make the\ncommunication of results not integrally representative in regards to the\nadvances achieved by that time. Consequently, using such a study as a reference\nwithin specific bodies of knowledge may introduce imprecision, both in terms of\nits subareas and with respect to new methodologies, languages, and tools. Thus,\na review of tertiary studies (what could be understood as a quaternary study)\ncould contribute to show the representativeness of the reported findings in\ncomparison to the state of the art and also to compile a set of perceptions\nthat could not be previously achieved. In that direction, the main contribution\nof this paper is presenting the findings from an analysis of 34 software\nengineering tertiary studies published between 2009 and 2021. The results\nindicate that over 60% of the studies demonstrate varying degrees of\nanachronism due to the publication of primary and secondary studies following\nthe publication of the tertiary study or even due to a time elapse between its\nconduction and its publication.",
            "author": [
                "Valdemar Vicente Graciano Neto",
                "C\u00e9lia La\u00eds Rodrigues",
                "Fernando Kenji Kamei",
                "Juliano Lopes de Oliveira",
                "Eliomar Ara\u00fajo de Lima",
                "Mohamad Kassab",
                "Roberto Oliveira"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00211v1",
                "http://arxiv.org/pdf/2311.00211v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00206v1",
            "title": "ChatGPT-Powered Hierarchical Comparisons for Image Classification",
            "updated": "2023-11-01T00:26:40Z",
            "published": "2023-11-01T00:26:40Z",
            "summary": "The zero-shot open-vocabulary challenge in image classification is tackled by\npretrained vision-language models like CLIP, which benefit from incorporating\nclass-specific knowledge from large language models (LLMs) like ChatGPT.\nHowever, biases in CLIP lead to similar descriptions for distinct but related\nclasses, prompting our novel image classification framework via hierarchical\ncomparisons: using LLMs to recursively group classes into hierarchies and\nclassifying images by comparing image-text embeddings at each hierarchy level,\nresulting in an intuitive, effective, and explainable approach.",
            "author": [
                "Zhiyuan Ren",
                "Yiyang Su",
                "Xiaoming Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00206v1",
                "http://arxiv.org/pdf/2311.00206v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00204v1",
            "title": "Continuous Training and Fine-tuning for Domain-Specific Language Models\n  in Medical Question Answering",
            "updated": "2023-11-01T00:18:00Z",
            "published": "2023-11-01T00:18:00Z",
            "summary": "Large language models exhibit promising general capabilities but often lack\nspecialized knowledge for domain-specific tasks. Developing domain experts from\na base model enables a range of applications without prohibitive training\ncosts. This work demonstrates a method using continuous training and\ninstruction fine-tuning to rapidly adapt Llama 2 base models to the Chinese\nmedical domain. We first conduct continuous training on 1B tokens from Chinese\nmedical references to teach relevant vocabulary and knowledge. The models are\nthen fine-tuned on 54K examples sourced from the Chinese National Medical\nLicensing Examination. Experiments on Chinese medical data confirm the\neffectiveness of this approach, producing a model comparable to GPT-3.5-turbo\nwhile using way less computational resource. The resulting domain-specific\nmodel could be useful for various Chinese medical applications. More broadly,\nthis provides a template for domain-specific training of large language models\nin areas where pre-trained models lack the required expertise, such as law,\nscience, and engineering.",
            "author": [
                "Zhen Guo",
                "Yining Hua"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00204v1",
                "http://arxiv.org/pdf/2311.00204v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00729v2",
            "title": "ZEETAD: Adapting Pretrained Vision-Language Model for Zero-Shot\n  End-to-End Temporal Action Detection",
            "updated": "2023-11-04T23:41:21Z",
            "published": "2023-11-01T00:17:37Z",
            "summary": "Temporal action detection (TAD) involves the localization and classification\nof action instances within untrimmed videos. While standard TAD follows fully\nsupervised learning with closed-set setting on large training data, recent\nzero-shot TAD methods showcase the promising open-set setting by leveraging\nlarge-scale contrastive visual-language (ViL) pretrained models. However,\nexisting zero-shot TAD methods have limitations on how to properly construct\nthe strong relationship between two interdependent tasks of localization and\nclassification and adapt ViL model to video understanding. In this work, we\npresent ZEETAD, featuring two modules: dual-localization and zero-shot proposal\nclassification. The former is a Transformer-based module that detects action\nevents while selectively collecting crucial semantic embeddings for later\nrecognition. The latter one, CLIP-based module, generates semantic embeddings\nfrom text and frame inputs for each temporal unit. Additionally, we enhance\ndiscriminative capability on unseen classes by minimally updating the frozen\nCLIP encoder with lightweight adapters. Extensive experiments on THUMOS14 and\nActivityNet-1.3 datasets demonstrate our approach's superior performance in\nzero-shot TAD and effective knowledge transfer from ViL models to unseen action\ncategories.",
            "author": [
                "Thinh Phan",
                "Khoa Vo",
                "Duy Le",
                "Gianfranco Doretto",
                "Donald Adjeroh",
                "Ngan Le"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00729v2",
                "http://arxiv.org/pdf/2311.00729v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00201v1",
            "title": "Federated Natural Policy Gradient Methods for Multi-task Reinforcement\n  Learning",
            "updated": "2023-11-01T00:15:18Z",
            "published": "2023-11-01T00:15:18Z",
            "summary": "Federated reinforcement learning (RL) enables collaborative decision making\nof multiple distributed agents without sharing local data trajectories. In this\nwork, we consider a multi-task setting, in which each agent has its own private\nreward function corresponding to different tasks, while sharing the same\ntransition kernel of the environment. Focusing on infinite-horizon tabular\nMarkov decision processes, the goal is to learn a globally optimal policy that\nmaximizes the sum of the discounted total rewards of all the agents in a\ndecentralized manner, where each agent only communicates with its neighbors\nover some prescribed graph topology. We develop federated vanilla and\nentropy-regularized natural policy gradient (NPG) methods under softmax\nparameterization, where gradient tracking is applied to the global Q-function\nto mitigate the impact of imperfect information sharing. We establish\nnon-asymptotic global convergence guarantees under exact policy evaluation,\nwhich are nearly independent of the size of the state-action space and\nilluminate the impacts of network size and connectivity. To the best of our\nknowledge, this is the first time that global convergence is established for\nfederated multi-task RL using policy optimization. Moreover, the convergence\nbehavior of the proposed algorithms is robust against inexactness of policy\nevaluation.",
            "author": [
                "Tong Yang",
                "Shicong Cen",
                "Yuting Wei",
                "Yuxin Chen",
                "Yuejie Chi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00201v1",
                "http://arxiv.org/pdf/2311.00201v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00194v1",
            "title": "Algorithms for Chip-Firing on Weighted Graphs",
            "updated": "2023-10-31T23:55:53Z",
            "published": "2023-10-31T23:55:53Z",
            "summary": "We extend the notion of chip-firing to weighted graphs, and generalize the\nGreedy Algorithm and Dhar's Burning Algorithm to weighted graphs. For a vertex\n$q \\in V(\\Gamma)$, we give an upper bound for the number of linearly equivalent\n$q$-reduced divisors. Finally, we illustrate a method of finding all maximal\nunwinnable divisors on weighted graphs.",
            "author": [
                "Ben Doyle"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00194v1",
                "http://arxiv.org/pdf/2311.00194v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00192v1",
            "title": "Large-Scale Multi-Robot Assembly Planning for Autonomous Manufacturing",
            "updated": "2023-10-31T23:42:14Z",
            "published": "2023-10-31T23:42:14Z",
            "summary": "Mobile autonomous robots have the potential to revolutionize manufacturing\nprocesses. However, employing large robot fleets in manufacturing requires\naddressing challenges including collision-free movement in a shared workspace,\neffective multi-robot collaboration to manipulate and transport large payloads,\ncomplex task allocation due to coupled manufacturing processes, and spatial\nplanning for parallel assembly and transportation of nested subassemblies. We\npropose a full algorithmic stack for large-scale multi-robot assembly planning\nthat addresses these challenges and can synthesize construction plans for\ncomplex assemblies with thousands of parts in a matter of minutes. Our approach\ntakes in a CAD-like product specification and automatically plans a full-stack\nassembly procedure for a group of robots to manufacture the product. We propose\nan algorithmic stack that comprises: (i) an iterative radial layout\noptimization procedure to define a global staging layout for the manufacturing\nfacility, (ii) a graph-repair mixed-integer program formulation and a modified\ngreedy task allocation algorithm to optimally allocate robots and robot\nsub-teams to assembly and transport tasks, (iii) a geometric heuristic and a\nhill-climbing algorithm to plan collaborative carrying configurations of robot\nsub-teams, and (iv) a distributed control policy that enables robots to execute\nthe assembly motion plan collision-free. We also present an open-source\nmulti-robot manufacturing simulator implemented in Julia as a resource to the\nresearch community, to test our algorithms and to facilitate multi-robot\nmanufacturing research more broadly. Our empirical results demonstrate the\nscalability and effectiveness of our approach by generating plans to\nmanufacture a LEGO model of a Saturn V launch vehicle with 1845 parts, 306\nsubassemblies, and 250 robots in under three minutes on a standard laptop\ncomputer.",
            "author": [
                "Kyle Brown",
                "Dylan M. Asmar",
                "Mac Schwager",
                "Mykel J. Kochenderfer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00192v1",
                "http://arxiv.org/pdf/2311.00192v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00182v1",
            "title": "Local Max-Cut on Sparse Graphs",
            "updated": "2023-10-31T23:00:14Z",
            "published": "2023-10-31T23:00:14Z",
            "summary": "We bound the smoothed running time of the FLIP algorithm for local Max-Cut as\na function of $\\alpha$, the arboricity of the input graph. We show that, with\nhigh probability, the following holds (where $n$ is the number of nodes and\n$\\phi$ is the smoothing parameter):\n  1) When $\\alpha = O(\\sqrt{\\log n})$ FLIP terminates in $\\phi poly(n)$\niterations. Previous to our results the only graph families for which FLIP was\nknown to achieve a smoothed polynomial running time were complete graphs and\ngraphs with logarithmic maximum degree.\n  2) For arbitrary values of $\\alpha$ we get a running time of $\\phi\nn^{O(\\frac{\\alpha}{\\log n} + \\log \\alpha)}$. This improves over the best known\nrunning time for general graphs of $\\phi n^{O(\\sqrt{ \\log n })}$ for $\\alpha =\no(\\log^{1.5} n)$. Specifically, when $\\alpha = O(\\log n)$ we get a\nsignificantly faster running time of $\\phi n^{O(\\log \\log n)}$.",
            "author": [
                "Gregory Schwartzman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00182v1",
                "http://arxiv.org/pdf/2311.00182v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00173v1",
            "title": "Continuum graph dynamics via population dynamics: well-posedness,\n  duality and equilibria",
            "updated": "2023-10-31T22:28:16Z",
            "published": "2023-10-31T22:28:16Z",
            "summary": "In this paper we consider stochastic processes taking values in a set of\ncontinuum graphs we call graphemes, defined as equivalence classes of sequences\nof vertices labelled by N embedded in an uncountable Polish space (with the\ncardinality of the continuum), together with an N x N connection matrix with\nentries 0 or 1 specifying the absence or presence of edges between pairs of\nvertices. In particular, we construct a Markov process on a Polish state space\nG of graphemes suitable to describe the time-space path of countable graphs.\nThe class of dynamics we propose arises by specifying simple rules for the\nevolution of finite graphs and passing to the limit of infinite graphs. The\nevolution of graphemes is characterised by well-posed martingale problems, and\nleads to strong Markov processes with the Feller property.",
            "author": [
                "Andreas Greven",
                "Frank den Hollander",
                "Anton Klimovsky",
                "Anita Winter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00173v1",
                "http://arxiv.org/pdf/2311.00173v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "05C80, 60J68, 60J70, 92D25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00166v1",
            "title": "Flatness of anisotropic minimal graphs in $\\mathbb{R}^{n+1}$",
            "updated": "2023-10-31T21:49:50Z",
            "published": "2023-10-31T21:49:50Z",
            "summary": "We prove a Bernstein theorem for $\\Phi$-anisotropic minimal hypersurfaces in\nall dimensional Euclidean spaces that the only entire smooth solutions $u:\n\\mathbb{R}^{n}\\rightarrow \\mathbb{R}$ of $\\Phi$-anisotropic minimal\nhypersurfaces equation are linear functions provided the anisotropic area\nfunctional integrand $\\Phi$ is sufficiently $C^{3}$-close to classical area\nfunctional integrand and $|\\nabla u(x)|=o(|x|^{\\varepsilon})$ for\n$\\varepsilon\\leq \\varepsilon_{0}(n, \\Phi)$ with the constant\n$\\varepsilon_{0}(n, \\Phi)>0$.",
            "author": [
                "Wenkui Du",
                "Yang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00166v1",
                "http://arxiv.org/pdf/2311.00166v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.DG",
                "35B08, 35J93, 49Q05, 53A10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00164v1",
            "title": "Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations\n  for Accident Analysis",
            "updated": "2023-10-31T21:43:10Z",
            "published": "2023-10-31T21:43:10Z",
            "summary": "We consider the problem of traffic accident analysis on a road network based\non road network connections and traffic volume. Previous works have designed\nvarious deep-learning methods using historical records to predict traffic\naccident occurrences. However, there is a lack of consensus on how accurate\nexisting methods are, and a fundamental issue is the lack of public accident\ndatasets for comprehensive evaluations. This paper constructs a large-scale,\nunified dataset of traffic accident records from official reports of various\nstates in the US, totaling 9 million records, accompanied by road networks and\ntraffic volume reports. Using this new dataset, we evaluate existing\ndeep-learning methods for predicting the occurrence of accidents on road\nnetworks. Our main finding is that graph neural networks such as GraphSAGE can\naccurately predict the number of accidents on roads with less than 22% mean\nabsolute error (relative to the actual count) and whether an accident will\noccur or not with over 87% AUROC, averaged over states. We achieve these\nresults by using multitask learning to account for cross-state variabilities\n(e.g., availability of accident labels) and transfer learning to combine\ntraffic volume with accident prediction. Ablation studies highlight the\nimportance of road graph-structural features, amongst other features. Lastly,\nwe discuss the implications of the analysis and develop a package for easily\nusing our new dataset.",
            "author": [
                "Abhinav Nippani",
                "Dongyue Li",
                "Haotian Ju",
                "Haris N. Koutsopoulos",
                "Hongyang R. Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00164v1",
                "http://arxiv.org/pdf/2311.00164v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00149v1",
            "title": "A Knowledge Compilation Take on Binary Polynomial Optimization",
            "updated": "2023-10-31T20:47:25Z",
            "published": "2023-10-31T20:47:25Z",
            "summary": "The Binary Polynomial Optimization (BPO) problem is defined as the problem of\nmaximizing a given polynomial function over all binary points. The main\ncontribution of this paper is to draw a novel connection between BPO and the\nproblem of finding the maximal assignment for a Boolean function with weights\non variables. This connection allows us to give a strongly polynomial algorithm\nthat solves BPO with a hypergraph that is either $\\beta$-acyclic or with\nbounded incidence treewidth. This result unifies and significantly extends the\nknown tractable classes of BPO. The generality of our technique allows us to\ndeal also with extensions of BPO, where we enforce extended cardinality\nconstraints on the set of binary points, and where we seek $k$ best feasible\nsolutions. We also extend our results to the significantly more general problem\nwhere variables are replaced by literals. Preliminary computational results\nshow that the resulting algorithms can be significantly faster than current\nstate-of-the-art.",
            "author": [
                "Florent Capelli",
                "Alberto Del Pia",
                "Silvia Di Gregorio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00149v1",
                "http://arxiv.org/pdf/2311.00149v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00142v1",
            "title": "Entanglement conditions and entanglement measures",
            "updated": "2023-10-31T20:30:15Z",
            "published": "2023-10-31T20:30:15Z",
            "summary": "We examine two conditions that can be used to detect bipartite entanglement,\nand show that they can be used to provide lower bounds on the negativity of\nstates. We begin with two-qubit states, and then show how what was done there\ncan be extended to more general states. The resulting bounds are then studied\nby means of a number of examples. We also show that if one has some knowledge\nof the Schmidt vectors of a state, better bounds can be found.",
            "author": [
                "Mark Hillery",
                "Camilla Polvara",
                "Vadim Oganesyan",
                "Nada Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00142v1",
                "http://arxiv.org/pdf/2311.00142v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00140v1",
            "title": "Adaptive and non-adaptive minimax rates for weighted Laplacian-eigenmap\n  based nonparametric regression",
            "updated": "2023-10-31T20:25:36Z",
            "published": "2023-10-31T20:25:36Z",
            "summary": "We show both adaptive and non-adaptive minimax rates of convergence for a\nfamily of weighted Laplacian-Eigenmap based nonparametric regression methods,\nwhen the true regression function belongs to a Sobolev space and the sampling\ndensity is bounded from above and below. The adaptation methodology is based on\nextensions of Lepski's method and is over both the smoothness parameter\n($s\\in\\mathbb{N}_{+}$) and the norm parameter ($M>0$) determining the\nconstraints on the Sobolev space. Our results extend the non-adaptive result in\n\\cite{green2021minimax}, established for a specific normalized graph Laplacian,\nto a wide class of weighted Laplacian matrices used in practice, including the\nunnormalized Laplacian and random walk Laplacian.",
            "author": [
                "Zhaoyang Shi",
                "Krishnakumar Balasubramanian",
                "Wolfgang Polonik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00140v1",
                "http://arxiv.org/pdf/2311.00140v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00123v1",
            "title": "Q-Learning for Stochastic Control under General Information Structures\n  and Non-Markovian Environments",
            "updated": "2023-10-31T19:53:16Z",
            "published": "2023-10-31T19:53:16Z",
            "summary": "As a primary contribution, we present a convergence theorem for stochastic\niterations, and in particular, Q-learning iterates, under a general, possibly\nnon-Markovian, stochastic environment. Our conditions for convergence involve\nan ergodicity and a positivity criterion. We provide a precise characterization\non the limit of the iterates and conditions on the environment and\ninitializations for convergence. As our second contribution, we discuss the\nimplications and applications of this theorem to a variety of stochastic\ncontrol problems with non-Markovian environments involving (i) quantized\napproximations of fully observed Markov Decision Processes (MDPs) with\ncontinuous spaces (where quantization break down the Markovian structure), (ii)\nquantized approximations of belief-MDP reduced partially observable MDPS\n(POMDPs) with weak Feller continuity and a mild version of filter stability\n(which requires the knowledge of the model by the controller), (iii) finite\nwindow approximations of POMDPs under a uniform controlled filter stability\n(which does not require the knowledge of the model), and (iv) for multi-agent\nmodels where convergence of learning dynamics to a new class of equilibria,\nsubjective Q-learning equilibria, will be studied. In addition to the\nconvergence theorem, some implications of the theorem above are new to the\nliterature and others are interpreted as applications of the convergence\ntheorem. Some open problems are noted.",
            "author": [
                "Ali Devran Kara",
                "Serdar Yuksel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00123v1",
                "http://arxiv.org/pdf/2311.00123v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00115v1",
            "title": "EXTRACT: Explainable Transparent Control of Bias in Embeddings",
            "updated": "2023-10-31T19:44:32Z",
            "published": "2023-10-31T19:44:32Z",
            "summary": "Knowledge Graphs are a widely used method to represent relations between\nentities in various AI applications, and Graph Embedding has rapidly become a\nstandard technique to represent Knowledge Graphs in such a way as to facilitate\ninferences and decisions. As this representation is obtained from behavioural\ndata, and is not in a form readable by humans, there is a concern that it might\nincorporate unintended information that could lead to biases. We propose\nEXTRACT: a suite of Explainable and Transparent methods to ConTrol bias in\nknowledge graph embeddings, so as to assess and decrease the implicit presence\nof protected information. Our method uses Canonical Correlation Analysis (CCA)\nto investigate the presence, extent and origins of information leaks during\ntraining, then decomposes embeddings into a sum of their private attributes by\nsolving a linear system. Our experiments, performed on the MovieLens1M dataset,\nshow that a range of personal attributes can be inferred from a user's viewing\nbehaviour and preferences, including gender, age, and occupation. Further\nexperiments, performed on the KG20C citation dataset, show that the information\nabout the conference in which a paper was published can be inferred from the\ncitation network of that article. We propose four transparent methods to\nmaintain the capability of the embedding to make the intended predictions\nwithout retaining unwanted information. A trade-off between these two goals is\nobserved.",
            "author": [
                "Zhijin Guo",
                "Zhaozhen Xu",
                "Martha Lewis",
                "Nello Cristianini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00115v1",
                "http://arxiv.org/pdf/2311.00115v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00110v1",
            "title": "Degree sequences of triangular multigraphs",
            "updated": "2023-10-31T19:37:03Z",
            "published": "2023-10-31T19:37:03Z",
            "summary": "A simple graph is triangular if every edge is contained in a triangle. A\nsequence of integers is graphical if it is the degree sequence of a simple\ngraph. Egan and Nikolayevsky recently conjectured that every graphical sequence\nwhose terms are all at least 4 is the degree sequence of a triangular simple\ngraph, and proved this in some special cases. In this paper we state and prove\nthe analogous version of this conjecture for multigraphs.",
            "author": [
                "John Talbot",
                "Jun Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00110v1",
                "http://arxiv.org/pdf/2311.00110v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C07"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00102v1",
            "title": "On some structural properties of evolution algebras",
            "updated": "2023-10-31T19:26:12Z",
            "published": "2023-10-31T19:26:12Z",
            "summary": "We consider the intersection $\\mathfrak{M}(A)$ of all maximal ideals of an\nevolution algebra $A$ and study the structure of the quotient $A/\\M(A)$. In a\nprevious work, maximal ideals have been related to hereditary subsets of a\ngraph associated to the given algebra. We investigate the superfluous members\nboth in the family of maximal ideals and also in the set of hereditary subsets\nof the associated graphs. By using subdirect products we state a structure\ntheorem for arbitrary evolution algebras (arbitrary dimensions and ground\nfield). Specializing in the perfect finite-dimensional case, we obtain a direct\nsum decomposition instead of a subdirect product and also a uniqueness\nproperty. We also study some examples in which Grassmanians appear in a natural\nway and others that exhibit a richer structure with a nonzero semisimple part\nthat is non-associative.",
            "author": [
                "Yolanda Cabrera Casado",
                "Dolores Mart\u00edn Barquero",
                "C\u00e1ndido Mart\u00edn Gonz\u00e1lez",
                "Alicia Tocino"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00102v1",
                "http://arxiv.org/pdf/2311.00102v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "17A60, 17D92, 16T05, 05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00101v1",
            "title": "Overcoming membrane locking in quadratic NURBS-based discretizations of\n  linear Kirchhoff-Love shells: CAS elements",
            "updated": "2023-10-31T19:26:05Z",
            "published": "2023-10-31T19:26:05Z",
            "summary": "Quadratic NURBS-based discretizations of the Galerkin method suffer from\nmembrane locking when applied to Kirchhoff-Love shell formulations. Membrane\nlocking causes not only smaller displacements than expected, but also\nlarge-amplitude spurious oscillations of the membrane forces.\nContinuous-assumed-strain (CAS) elements have been recently introduced to\nremove membrane locking in quadratic NURBS-based discretizations of linear\nplane curved Kirchhoff rods (Casquero et al., CMAME, 2022). In this work, we\ngeneralize CAS elements to vanquish membrane locking in quadratic NURBS-based\ndiscretizations of linear Kirchhoff-Love shells. CAS elements bilinearly\ninterpolate the membrane strains at the four corners of each element. Thus, the\nassumed strains have C0 continuity across element boundaries. To the best of\nthe authors' knowledge, CAS elements are the first assumed-strain treatment to\neffectively overcome membrane locking in quadratic NURBS-based discretizations\nof Kirchhoff-Love shells while satisfying the following important\ncharacteristics for computational efficiency: (1) No additional degrees of\nfreedom are added, (2) No additional systems of algebraic equations need to be\nsolved, (3) No matrix multiplications or matrix inversions are needed to obtain\nthe stiffness matrix, and (4) The nonzero pattern of the stiffness matrix is\npreserved. The benchmark problems show that CAS elements, using either 2x2 or\n3x3 Gauss-Legendre quadrature points per element, are an effective locking\ntreatment since this element type results in more accurate displacements for\ncoarse meshes and excises the spurious oscillations of the membrane forces. The\nbenchmark problems also show that CAS elements outperform state-of-the-art\nelement types based on Lagrange polynomials equipped with either assumed-strain\nor reduced-integration locking treatments.",
            "author": [
                "Hugo Casquero",
                "Kyle Dakota Mathews"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.cma.2023.116523",
                "http://arxiv.org/abs/2311.00101v1",
                "http://arxiv.org/pdf/2311.00101v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00100v1",
            "title": "Smooth approximation of Lipschitz domains, weak curvatures and\n  isocapacitary estimates",
            "updated": "2023-10-31T19:24:30Z",
            "published": "2023-10-31T19:24:30Z",
            "summary": "We provide a novel approach to approximate bounded Lipschitz domains via a\nsequence of smooth, bounded domains. The flexibility of our method allows\neither inner or outer approximations of Lipschitz domains which also possess\nweakly defined curvatures, namely, domains whose boundary can be locally\ndescribed as the graph of a function belonging to the Sobolev space $W^{2,q}$\nfor some $q\\geq 1$. The sequences of approximating sets is also characterized\nby uniform isocapacitary estimates with respect to the initial domain $\\Omega$.",
            "author": [
                "Carlo Alberto Antonini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00100v1",
                "http://arxiv.org/pdf/2311.00100v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.DG",
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00083v1",
            "title": "Mapping the complete evolution of magnetic excitation in beam-plasma\n  system driven by an ultra-intense, femtosecond laser",
            "updated": "2023-10-31T18:48:35Z",
            "published": "2023-10-31T18:48:35Z",
            "summary": "Plasmas are beset with instabilities of all types, hydrodynamic,\nmagneto-hydrodynamic, and electromagnetic. These instabilities are complex,\noccur over a large range of temporal and spatial scales, are most often\nunmanageable, and have seriously challenged our efforts at applications, even\nas they have shed light on the understanding of the physics of plasmas in the\nlaboratory and astrophysical environments. A major reason for our limited\nsuccess in their containment is the lack of direct experimental information on\ntheir origins and evolution, both temporal and spatial. In plasmas produced by\nhigh-intensity, short, and ultrashort pulse lasers, our knowledge of the\ninstability stems from the (secondary) signals they generate e.g. scattering of\nelectromagnetic waves in the form of Raman or Brillouin scattering. Rarely, if\never, has a direct measurement been made of the instantaneous evolution of the\ninstabilities in plasmas. In this paper, we present direct measurements of the\nfemtosecond evolution of the electromagnetic beam-driven instability that\narises from the interaction of forward and return currents in an\nultrahigh-intensity laser-produced plasma on a solid target.",
            "author": [
                "Moniruzzaman Shaikh",
                "Amit D Lad",
                "Devshree Mandal",
                "Kamalesh Jana",
                "Deep Sarkar",
                "Amita Das",
                "G Ravindra Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00083v1",
                "http://arxiv.org/pdf/2311.00083v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00075v1",
            "title": "The maximum number of connected sets in regular graphs",
            "updated": "2023-10-31T18:40:16Z",
            "published": "2023-10-31T18:40:16Z",
            "summary": "We make some fundamental observations and conjectures on the number of\nconnected sets $N(G)$ in $d$-regular graphs $G$. We improve the best known\nlower bounds on the exponential behavior of the maximum of $N(G)$ for regular\ngraphs by considering a different construction of a family of graphs (depending\non smaller base graphs) and improve the upper bounds conditional on one of our\nconjectures. The lower bounds are estimated using combinatorial reductions and\nlinear algebra. We also determine the exact maxima of $N(G)$ for cubic and\nquartic graphs with small order.",
            "author": [
                "Stijn Cambie",
                "Jan Goedgebeur",
                "Jorik Jooken"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00075v1",
                "http://arxiv.org/pdf/2311.00075v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C07, 05C35, 05C40, 05C48, 05C50, 05C69, 05C85, 68R05, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00068v1",
            "title": "View Classification and Object Detection in Cardiac Ultrasound to\n  Localize Valves via Deep Learning",
            "updated": "2023-10-31T18:16:02Z",
            "published": "2023-10-31T18:16:02Z",
            "summary": "Echocardiography provides an important tool for clinicians to observe the\nfunction of the heart in real time, at low cost, and without harmful radiation.\nAutomated localization and classification of heart valves enables automatic\nextraction of quantities associated with heart mechanical function and related\nblood flow measurements. We propose a machine learning pipeline that uses deep\nneural networks for separate classification and localization steps. As the\nfirst step in the pipeline, we apply view classification to echocardiograms\nwith ten unique anatomic views of the heart. In the second step, we apply deep\nlearning-based object detection to both localize and identify the valves. Image\nsegmentation based object detection in echocardiography has been shown in many\nearlier studies but, to the best of our knowledge, this is the first study that\npredicts the bounding boxes around the valves along with classification from 2D\nultrasound images with the help of deep neural networks. Our object detection\nexperiments applied to the Apical views suggest that it is possible to localize\nand identify multiple valves precisely.",
            "author": [
                "Derya Gol Gungor",
                "Bimba Rao",
                "Cynthia Wolverton",
                "Ismayil Guracar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00068v1",
                "http://arxiv.org/pdf/2311.00068v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00067v1",
            "title": "Adaptive Control of Euler-Lagrange Systems under Time-varying State\n  Constraints without a Priori Bounded Uncertainty",
            "updated": "2023-10-31T18:15:55Z",
            "published": "2023-10-31T18:15:55Z",
            "summary": "In this article, a novel adaptive controller is designed for Euler-Lagrangian\nsystems under predefined time-varying state constraints. The proposed\ncontroller could achieve this objective without a priori knowledge of system\nparameters and, crucially, of state-dependent uncertainties. The closed-loop\nstability is verified using the Lyapunov method, while the overall efficacy of\nthe proposed scheme is verified using a simulated robotic arm compared to the\nstate of the art.",
            "author": [
                "Viswa Narayanan Sankaranarayanan",
                "Sumeet Gajanan Satpute",
                "Spandan Roy",
                "George Nikolakopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00067v1",
                "http://arxiv.org/pdf/2311.00067v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00055v1",
            "title": "Training-Free Generalization on Heterogeneous Tabular Data via\n  Meta-Representation",
            "updated": "2023-10-31T18:03:54Z",
            "published": "2023-10-31T18:03:54Z",
            "summary": "Tabular data is prevalent across various machine learning domains. Yet, the\ninherent heterogeneities in attribute and class spaces across different tabular\ndatasets hinder the effective sharing of knowledge, limiting a tabular model to\nbenefit from other datasets. In this paper, we propose Tabular data\nPre-Training via Meta-representation (TabPTM), which allows one tabular model\npre-training on a set of heterogeneous datasets. Then, this pre-trained model\ncan be directly applied to unseen datasets that have diverse attributes and\nclasses without additional training. Specifically, TabPTM represents an\ninstance through its distance to a fixed number of prototypes, thereby\nstandardizing heterogeneous tabular datasets. A deep neural network is then\ntrained to associate these meta-representations with dataset-specific\nclassification confidences, endowing TabPTM with the ability of training-free\ngeneralization. Experiments validate that TabPTM achieves promising performance\nin new datasets, even under few-shot scenarios.",
            "author": [
                "Han-Jia Ye",
                "Qi-Le Zhou",
                "De-Chuan Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00055v1",
                "http://arxiv.org/pdf/2311.00055v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20709v1",
            "title": "Quadratic Differentials as Stability Conditions of Graded Skew-gentle\n  Algebras",
            "updated": "2023-10-31T17:59:58Z",
            "published": "2023-10-31T17:59:58Z",
            "summary": "We prove that the principal component of the exchange graph of hearts of a\ngraded skew-gentle algebra can be identified with the corresponding exchange\ngraph of S-graphs, using the geometric models and\n$\\operatorname{Int}=\\operatorname{dim}\\operatorname{Hom}$ formula in\nQiu-Zhang-Zhou. Using the same argument in Bridgeland-Smith,\nBarbieri-M\\\"oller-Qiu-So and Christ-Haiden-Qiu, we extend this identification\nto an isomorphism between the spaces of stability conditions and of quadratic\ndifferentials.",
            "author": [
                "Suiqi Lu",
                "Yu Qiu",
                "Dongjian Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20709v1",
                "http://arxiv.org/pdf/2310.20709v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20706v1",
            "title": "DDAM-PS: Diligent Domain Adaptive Mixer for Person Search",
            "updated": "2023-10-31T17:59:14Z",
            "published": "2023-10-31T17:59:14Z",
            "summary": "Person search (PS) is a challenging computer vision problem where the\nobjective is to achieve joint optimization for pedestrian detection and\nre-identification (ReID). Although previous advancements have shown promising\nperformance in the field under fully and weakly supervised learning fashion,\nthere exists a major gap in investigating the domain adaptation ability of PS\nmodels. In this paper, we propose a diligent domain adaptive mixer (DDAM) for\nperson search (DDAP-PS) framework that aims to bridge a gap to improve\nknowledge transfer from the labeled source domain to the unlabeled target\ndomain. Specifically, we introduce a novel DDAM module that generates moderate\nmixed-domain representations by combining source and target domain\nrepresentations. The proposed DDAM module encourages domain mixing to minimize\nthe distance between the two extreme domains, thereby enhancing the ReID task.\nTo achieve this, we introduce two bridge losses and a disparity loss. The\nobjective of the two bridge losses is to guide the moderate mixed-domain\nrepresentations to maintain an appropriate distance from both the source and\ntarget domain representations. The disparity loss aims to prevent the moderate\nmixed-domain representations from being biased towards either the source or\ntarget domains, thereby avoiding overfitting. Furthermore, we address the\nconflict between the two subtasks, localization and ReID, during domain\nadaptation. To handle this cross-task conflict, we forcefully decouple the\nnorm-aware embedding, which aids in better learning of the moderate\nmixed-domain representation. We conduct experiments to validate the\neffectiveness of our proposed method. Our approach demonstrates favorable\nperformance on the challenging PRW and CUHK-SYSU datasets. Our source code is\npublicly available at \\url{https://github.com/mustansarfiaz/DDAM-PS}.",
            "author": [
                "Mohammed Khaleed Almansoori",
                "Mustansar Fiaz",
                "Hisham Cholakkal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20706v1",
                "http://arxiv.org/pdf/2310.20706v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20699v2",
            "title": "Bayesian Multistate Bennett Acceptance Ratio Methods",
            "updated": "2023-11-01T03:03:46Z",
            "published": "2023-10-31T17:57:58Z",
            "summary": "The multistate Bennett acceptance ratio (MBAR) method is a prevalent approach\nfor computing free energies of thermodynamic states. In this work, we introduce\nBayesMBAR, a Bayesian generalization of the MBAR method. By integrating\nconfigurations sampled from thermodynamic states with a prior distribution,\nBayesMBAR computes a posterior distribution of free energies. Using the\nposterior distribution, we derive free energy estimations and compute their\nassociated uncertainties. Notably, when a uniform prior distribution is used,\nBayesMBAR recovers the MBAR's result but provides more accurate uncertainty\nestimates. Additionally, when prior knowledge about free energies is available,\nBayesMBAR can incorporate this information into the estimation procedure by\nusing non-uniform prior distributions. As an example, we show that, by\nincorporating the prior knowledge about the smoothness of free energy surfaces,\nBayesMBAR provides more accurate estimates than the MBAR method. Given MBAR's\nwidespread use in free energy calculations, we anticipate BayesMBAR to be an\nessential tool in various applications of free energy calculations.",
            "author": [
                "Xinqiang Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20699v2",
                "http://arxiv.org/pdf/2310.20699v2"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG",
                "physics.comp-ph",
                "physics.data-an",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00016v3",
            "title": "Metric spaces in chess and international chess pieces graph diameters",
            "updated": "2023-12-03T15:39:33Z",
            "published": "2023-10-31T17:50:14Z",
            "summary": "This paper aims to study the graph radii and diameters induced by the\n$k$-dimensional versions of the well-known six international chess pieces on\nevery finite $\\{n \\times n \\times \\dots \\times n\\} \\subseteq \\mathbb{Z}^k$\nlattice since they originate as many interesting metric spaces for any proper\npair $(n,k)$. For this purpose, we finally discuss a mathematically consistent\ngeneralization of all the planar FIDE chess pieces to an appropriate\n$k$-dimensional environment, finding (for any $k \\in \\mathbb{Z}^+$) the exact\nvalues of the graph radii and diameters of the $k$-rook, $k$-king, $k$-bishop,\nand the corresponding values for the $3$-queen, $3$-knight, and $3$-pawn. We\nalso provide tight bounds for the graph radii and diameters of the $k$-queen,\n$k$-knight, and $k$-pawn, holding for any $k \\geq 4$.",
            "author": [
                "Marco Rip\u00e0"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00016v3",
                "http://arxiv.org/pdf/2311.00016v3"
            ],
            "primary_category": "math.HO",
            "category": [
                "math.HO",
                "05C12 (Primary) 00A08, 05C57 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20679v1",
            "title": "Latent Field Discovery In Interacting Dynamical Systems With Neural\n  Fields",
            "updated": "2023-10-31T17:45:39Z",
            "published": "2023-10-31T17:45:39Z",
            "summary": "Systems of interacting objects often evolve under the influence of field\neffects that govern their dynamics, yet previous works have abstracted away\nfrom such effects, and assume that systems evolve in a vacuum. In this work, we\nfocus on discovering these fields, and infer them from the observed dynamics\nalone, without directly observing them. We theorize the presence of latent\nforce fields, and propose neural fields to learn them. Since the observed\ndynamics constitute the net effect of local object interactions and global\nfield effects, recently popularized equivariant networks are inapplicable, as\nthey fail to capture global information. To address this, we propose to\ndisentangle local object interactions -- which are $\\mathrm{SE}(n)$ equivariant\nand depend on relative states -- from external global field effects -- which\ndepend on absolute states. We model interactions with equivariant graph\nnetworks, and combine them with neural fields in a novel graph network that\nintegrates field forces. Our experiments show that we can accurately discover\nthe underlying fields in charged particles settings, traffic scenes, and\ngravitational n-body problems, and effectively use them to learn the system and\nforecast future trajectories.",
            "author": [
                "Miltiadis Kofinas",
                "Erik J. Bekkers",
                "Naveen Shankar Nagaraja",
                "Efstratios Gavves"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20679v1",
                "http://arxiv.org/pdf/2310.20679v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20676v1",
            "title": "Kondo effect and its destruction in hetero-bilayer transition metal\n  dichalcogenides",
            "updated": "2023-10-31T17:40:08Z",
            "published": "2023-10-31T17:40:08Z",
            "summary": "Moir\\'e structures, along with line-graph-based $d$-electron systems,\nrepresent a setting to realize flat bands. One form of the associated strong\ncorrelation physics is the Kondo effect. Here, we address the Kondo-driven\nheavy fermion state and its destruction in AB-stacked hetero-bilayer transition\nmetal dichalcogenide with tunable filling factor and perpendicular displacement\nfield. In an extended range of the tunable displacement field, the relative\nfilling of the more correlated orbital is enforced to be $\\nu_d \\approx 1$ by\nthe interaction, which agrees with the experimental observation. We also argue\nthat the qualitative behavior of the crossover associated with the Kondo\npicture in an extended correlation regime provides the understanding of the\nenergy scales that have been observed in this system. Our results set the stage\nto address the amplified quantum fluctuations that the Kondo effect may produce\nin these structures and new regimes that the systems open up for\nKondo-destruction quantum criticality.",
            "author": [
                "Fang Xie",
                "Lei Chen",
                "Qimiao Si"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20676v1",
                "http://arxiv.org/pdf/2310.20676v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20672v1",
            "title": "Evaluating the reconstruction of individual haloes in constrained\n  cosmological simulations",
            "updated": "2023-10-31T17:35:29Z",
            "published": "2023-10-31T17:35:29Z",
            "summary": "Constrained cosmological simulations play an important role in modelling the\nlocal Universe, enabling investigation of the dark matter content of local\nstructures and their formation histories. We introduce a method for determining\nthe extent to which individual haloes are reliably reconstructed between\nconstrained simulations, and apply it to the Constrained Simulations in BORG\n(CSiBORG) suite of $101$ high-resolution realisations across the posterior\nprobability distribution of initial conditions from the Bayesian Origin\nReconstruction from Galaxies (BORG) algorithm. The method is based on the\noverlap of the initial Lagrangian patch of a halo in one simulation with those\nin another, and therefore measures the degree to which the haloes' particles\nare initially coincident. By this metric we find consistent reconstructions of\n$M\\gtrsim10^{14}~M_\\odot / h$ haloes across the CSiBORG simulations, indicating\nthat the constraints from the BORG algorithm are sufficient to pin down the\nmasses, positions and peculiar velocities of clusters to high precision. The\neffect of the constraints tapers off towards lower mass however, and the halo\nspins and concentrations are largely unconstrained at all masses. We document\nthe advantages of evaluating halo consistency in the initial conditions,\ndescribe how the method may be used to quantify our knowledge of the halo field\ngiven galaxy survey data analysed through the lens of probabilistic inference\nmachines such as BORG, and describe applications to matched but unconstrained\nsimulations.",
            "author": [
                "Richard Stiskalek",
                "Harry Desmond",
                "Julien Devriendt",
                "Adrianne Slyz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20672v1",
                "http://arxiv.org/pdf/2310.20672v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20660v1",
            "title": "Pseudo-K\u00e4hler and hypersymplectic structures on semidirect products",
            "updated": "2023-10-31T17:27:12Z",
            "published": "2023-10-31T17:27:12Z",
            "summary": "We study left-invariant pseudo-K\\\"ahler and hypersymplectic structures on\nsemidirect products $G\\rtimes H$; we work at the level of the Lie algebra\n$\\mathfrak{g}\\rtimes\\mathfrak{h}$. In particular we consider the structures\ninduced on $\\mathfrak{g}\\rtimes\\mathfrak{h}$ by existing pseudo-K\\\"ahler\nstructures on $\\mathfrak{g}$ and $\\mathfrak{h}$; we classify all semidirect\nproducts of this type with $\\mathfrak{g}$ of dimension $4$ and\n$\\mathfrak{h}=\\mathbb{R}^2$. In the hypersymplectic setting, we consider a more\ngeneral construction on semidirect products. We construct new $2$-step\nnilpotent hypersymplectic Lie algebras; to our knowledge, these are the first\nsuch examples whose underlying complex structure is not abelian",
            "author": [
                "Diego Conti",
                "Alejandro Gil-Garc\u00eda"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20660v1",
                "http://arxiv.org/pdf/2310.20660v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "53C26, 53C50, 22E25, 53C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20639v1",
            "title": "The two-variable hypergraph Tutte polynomial via embedding activities",
            "updated": "2023-10-31T17:07:41Z",
            "published": "2023-10-31T17:07:41Z",
            "summary": "We prove that the two-variable Tutte polynomial of hypergraphs can be defined\nvia embedding activities. We also prove that embedding activities of\nhypergraphs yield a Crapo-style decomposition of $\\mathbb{Z}^E$, thus\ngeneralizing Bernardi's results from graphs to hypergraphs.",
            "author": [
                "Lilla T\u00f3thm\u00e9r\u00e9sz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20639v1",
                "http://arxiv.org/pdf/2310.20639v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C31, 05C65, 52B40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20632v1",
            "title": "Constrained Planarity in Practice -- Engineering the Synchronized\n  Planarity Algorithm",
            "updated": "2023-10-31T17:01:32Z",
            "published": "2023-10-31T17:01:32Z",
            "summary": "In the constrained planarity setting, we ask whether a graph admits a planar\ndrawing that additionally satisfies a given set of constraints. These\nconstraints are often derived from very natural problems; prominent examples\nare Level Planarity, where vertices have to lie on given horizontal lines\nindicating a hierarchy, and Clustered Planarity, where we additionally draw the\nboundaries of clusters which recursively group the vertices in a crossing-free\nmanner. Despite receiving significant amount of attention and substantial\ntheoretical progress on these problems, only very few of the found solutions\nhave been put into practice and evaluated experimentally.\n  In this paper, we describe our implementation of the recent quadratic-time\nalgorithm by Bl\\\"asius et al. [TALG Vol 19, No 4] for solving the problem\nSynchronized Planarity, which can be seen as a common generalization of several\nconstrained planarity problems, including the aforementioned ones. Our\nexperimental evaluation on an existing benchmark set shows that even our\nbaseline implementation outperforms all competitors by at least an order of\nmagnitude. We systematically investigate the degrees of freedom in the\nimplementation of the Synchronized Planarity algorithm for larger instances and\npropose several modifications that further improve the performance. Altogether,\nthis allows us to solve instances with up to 100 vertices in milliseconds and\ninstances with up to 100 000 vertices within a few minutes.",
            "author": [
                "Simon D. Fink",
                "Ignaz Rutter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20632v1",
                "http://arxiv.org/pdf/2310.20632v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20623v1",
            "title": "Fully dynamic approximation schemes on planar and apex-minor-free graphs",
            "updated": "2023-10-31T16:54:54Z",
            "published": "2023-10-31T16:54:54Z",
            "summary": "The classic technique of Baker [J. ACM '94] is the most fundamental approach\nfor designing approximation schemes on planar, or more generally\ntopologically-constrained graphs, and it has been applied in a myriad of\ndifferent variants and settings throughout the last 30 years. In this work we\npropose a dynamic variant of Baker's technique, where instead of finding an\napproximate solution in a given static graph, the task is to design a data\nstructure for maintaining an approximate solution in a fully dynamic graph,\nthat is, a graph that is changing over time by edge deletions and edge\ninsertions. Specifically, we address the two most basic problems -- Maximum\nWeight Independent Set and Minimum Weight Dominating Set -- and we prove the\nfollowing: for a fully dynamic $n$-vertex planar graph $G$, one can:\n  * maintain a $(1-\\varepsilon)$-approximation of the maximum weight of an\nindependent set in $G$ with amortized update time $f(\\varepsilon)\\cdot\nn^{o(1)}$; and,\n  * under the additional assumption that the maximum degree of the graph is\nbounded at all times by a constant, also maintain a\n$(1+\\varepsilon)$-approximation of the minimum weight of a dominating set in\n$G$ with amortized update time $f(\\varepsilon)\\cdot n^{o(1)}$.\n  In both cases, $f(\\varepsilon)$ is doubly-exponential in\n$\\mathrm{poly}(1/\\varepsilon)$ and the data structure can be initialized in\ntime $f(\\varepsilon)\\cdot n^{1+o(1)}$. All our results in fact hold in the\nlarger generality of any graph class that excludes a fixed apex-graph as a\nminor.",
            "author": [
                "Tuukka Korhonen",
                "Wojciech Nadara",
                "Micha\u0142 Pilipczuk",
                "Marek Soko\u0142owski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20623v1",
                "http://arxiv.org/pdf/2310.20623v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20609v1",
            "title": "Graph Matching via convex relaxation to the simplex",
            "updated": "2023-10-31T16:44:26Z",
            "published": "2023-10-31T16:44:26Z",
            "summary": "This paper addresses the Graph Matching problem, which consists of finding\nthe best possible alignment between two input graphs, and has many applications\nin computer vision, network deanonymization and protein alignment. A common\napproach to tackle this problem is through convex relaxations of the NP-hard\n\\emph{Quadratic Assignment Problem} (QAP).\n  Here, we introduce a new convex relaxation onto the unit simplex and develop\nan efficient mirror descent scheme with closed-form iterations for solving this\nproblem. Under the correlated Gaussian Wigner model, we show that the simplex\nrelaxation admits a unique solution with high probability. In the noiseless\ncase, this is shown to imply exact recovery of the ground truth permutation.\nAdditionally, we establish a novel sufficiency condition for the input matrix\nin standard greedy rounding methods, which is less restrictive than the\ncommonly used `diagonal dominance' condition. We use this condition to show\nexact one-step recovery of the ground truth (holding almost surely) via the\nmirror descent scheme, in the noiseless setting. We also use this condition to\nobtain significantly improved conditions for the GRAMPA algorithm [Fan et al.\n2019] in the noiseless setting.",
            "author": [
                "Ernesto Araya Valdivia",
                "Hemant Tyagi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20609v1",
                "http://arxiv.org/pdf/2310.20609v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20599v1",
            "title": "Brain-like Flexible Visual Inference by Harnessing Feedback-Feedforward\n  Alignment",
            "updated": "2023-10-31T16:35:27Z",
            "published": "2023-10-31T16:35:27Z",
            "summary": "In natural vision, feedback connections support versatile visual inference\ncapabilities such as making sense of the occluded or noisy bottom-up sensory\ninformation or mediating pure top-down processes such as imagination. However,\nthe mechanisms by which the feedback pathway learns to give rise to these\ncapabilities flexibly are not clear. We propose that top-down effects emerge\nthrough alignment between feedforward and feedback pathways, each optimizing\nits own objectives. To achieve this co-optimization, we introduce\nFeedback-Feedforward Alignment (FFA), a learning algorithm that leverages\nfeedback and feedforward pathways as mutual credit assignment computational\ngraphs, enabling alignment. In our study, we demonstrate the effectiveness of\nFFA in co-optimizing classification and reconstruction tasks on widely used\nMNIST and CIFAR10 datasets. Notably, the alignment mechanism in FFA endows\nfeedback connections with emergent visual inference functions, including\ndenoising, resolving occlusions, hallucination, and imagination. Moreover, FFA\noffers bio-plausibility compared to traditional backpropagation (BP) methods in\nimplementation. By repurposing the computational graph of credit assignment\ninto a goal-driven feedback pathway, FFA alleviates weight transport problems\nencountered in BP, enhancing the bio-plausibility of the learning algorithm.\nOur study presents FFA as a promising proof-of-concept for the mechanisms\nunderlying how feedback connections in the visual cortex support flexible\nvisual functions. This work also contributes to the broader field of visual\ninference underlying perceptual phenomena and has implications for developing\nmore biologically inspired learning algorithms.",
            "author": [
                "Tahereh Toosi",
                "Elias B. Issa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20599v1",
                "http://arxiv.org/pdf/2310.20599v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.CV",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20588v1",
            "title": "Zero-Shot Medical Information Retrieval via Knowledge Graph Embedding",
            "updated": "2023-10-31T16:26:33Z",
            "published": "2023-10-31T16:26:33Z",
            "summary": "In the era of the Internet of Things (IoT), the retrieval of relevant medical\ninformation has become essential for efficient clinical decision-making. This\npaper introduces MedFusionRank, a novel approach to zero-shot medical\ninformation retrieval (MIR) that combines the strengths of pre-trained language\nmodels and statistical methods while addressing their limitations. The proposed\napproach leverages a pre-trained BERT-style model to extract compact yet\ninformative keywords. These keywords are then enriched with domain knowledge by\nlinking them to conceptual entities within a medical knowledge graph.\nExperimental evaluations on medical datasets demonstrate MedFusion Rank's\nsuperior performance over existing methods, with promising results with a\nvariety of evaluation metrics. MedFusionRank demonstrates efficacy in\nretrieving relevant information, even from short or single-term queries.",
            "author": [
                "Yuqi Wang",
                "Zeqiang Wang",
                "Wei Wang",
                "Qi Chen",
                "Kaizhu Huang",
                "Anh Nguyen",
                "Suparna De"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20588v1",
                "http://arxiv.org/pdf/2310.20588v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20587v4",
            "title": "Unleashing the Power of Pre-trained Language Models for Offline\n  Reinforcement Learning",
            "updated": "2023-11-27T07:38:06Z",
            "published": "2023-10-31T16:24:17Z",
            "summary": "Offline reinforcement learning (RL) aims to find a near-optimal policy using\npre-collected datasets. In real-world scenarios, data collection could be\ncostly and risky; therefore, offline RL becomes particularly challenging when\nthe in-domain data is limited. Given recent advances in Large Language Models\n(LLMs) and their few-shot learning prowess, this paper introduces\n$\\textbf{La}$nguage Models for $\\textbf{Mo}$tion Control ($\\textbf{LaMo}$), a\ngeneral framework based on Decision Transformers to effectively use pre-trained\nLanguage Models (LMs) for offline RL. Our framework highlights four crucial\ncomponents: (1) Initializing Decision Transformers with sequentially\npre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to\nfull-weight fine-tuning, to combine the pre-trained knowledge from LMs and\nin-domain knowledge effectively, (3) using the non-linear MLP transformation\ninstead of linear projections, to generate embeddings, and (4) integrating an\nauxiliary language prediction loss during fine-tuning to stabilize the LMs and\nretain their original abilities on languages. Empirical results indicate\n$\\textbf{LaMo}$ achieves state-of-the-art performance in sparse-reward tasks\nand closes the gap between value-based offline RL methods and decision\ntransformers in dense-reward tasks. In particular, our method demonstrates\nsuperior performance in scenarios with limited data samples.",
            "author": [
                "Ruizhe Shi",
                "Yuyao Liu",
                "Yanjie Ze",
                "Simon S. Du",
                "Huazhe Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20587v4",
                "http://arxiv.org/pdf/2310.20587v4"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20581v1",
            "title": "Stochastic Gradient Descent for Gaussian Processes Done Right",
            "updated": "2023-10-31T16:15:13Z",
            "published": "2023-10-31T16:15:13Z",
            "summary": "We study the optimisation problem associated with Gaussian process regression\nusing squared loss. The most common approach to this problem is to apply an\nexact solver, such as conjugate gradient descent, either directly, or to a\nreduced-order version of the problem. Recently, driven by successes in deep\nlearning, stochastic gradient descent has gained traction as an alternative. In\nthis paper, we show that when done right$\\unicode{x2014}$by which we mean using\nspecific insights from the optimisation and kernel\ncommunities$\\unicode{x2014}$this approach is highly effective. We thus\nintroduce a particular stochastic dual gradient descent algorithm, that may be\nimplemented with a few lines of code using any deep learning framework. We\nexplain our design decisions by illustrating their advantage against\nalternatives with ablation studies and show that the new method is highly\ncompetitive. Our evaluations on standard regression benchmarks and a Bayesian\noptimisation task set our approach apart from preconditioned conjugate\ngradients, variational Gaussian process approximations, and a previous version\nof stochastic gradient descent for Gaussian processes. On a molecular binding\naffinity prediction task, our method places Gaussian process regression on par\nin terms of performance with state-of-the-art graph neural networks.",
            "author": [
                "Jihao Andreas Lin",
                "Shreyas Padhy",
                "Javier Antor\u00e1n",
                "Austin Tripp",
                "Alexander Terenin",
                "Csaba Szepesv\u00e1ri",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                "David Janz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20581v1",
                "http://arxiv.org/pdf/2310.20581v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20552v1",
            "title": "Privacy-preserving design of graph neural networks with applications to\n  vertical federated learning",
            "updated": "2023-10-31T15:34:59Z",
            "published": "2023-10-31T15:34:59Z",
            "summary": "The paradigm of vertical federated learning (VFL), where institutions\ncollaboratively train machine learning models via combining each other's local\nfeature or label information, has achieved great success in applications to\nfinancial risk management (FRM). The surging developments of graph\nrepresentation learning (GRL) have opened up new opportunities for FRM\napplications under FL via efficiently utilizing the graph-structured data\ngenerated from underlying transaction networks. Meanwhile, transaction\ninformation is often considered highly sensitive. To prevent data leakage\nduring training, it is critical to develop FL protocols with formal privacy\nguarantees. In this paper, we present an end-to-end GRL framework in the VFL\nsetting called VESPER, which is built upon a general privatization scheme\ntermed perturbed message passing (PMP) that allows the privatization of many\npopular graph neural architectures.Based on PMP, we discuss the strengths and\nweaknesses of specific design choices of concrete graph neural architectures\nand provide solutions and improvements for both dense and sparse graphs.\nExtensive empirical evaluations over both public datasets and an industry\ndataset demonstrate that VESPER is capable of training high-performance GNN\nmodels over both sparse and dense graphs under reasonable privacy budgets.",
            "author": [
                "Ruofan Wu",
                "Mingyang Zhang",
                "Lingjuan Lyu",
                "Xiaolong Xu",
                "Xiuquan Hao",
                "Xinyi Fu",
                "Tengfei Liu",
                "Tianyi Zhang",
                "Weiqiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20552v1",
                "http://arxiv.org/pdf/2310.20552v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20550v2",
            "title": "CapsFusion: Rethinking Image-Text Data at Scale",
            "updated": "2023-11-02T11:25:20Z",
            "published": "2023-10-31T15:31:39Z",
            "summary": "Large multimodal models demonstrate remarkable generalist ability to perform\ndiverse multimodal tasks in a zero-shot manner. Large-scale web-based\nimage-text pairs contribute fundamentally to this success, but suffer from\nexcessive noise. Recent studies use alternative captions synthesized by\ncaptioning models and have achieved notable benchmark performance. However, our\nexperiments reveal significant Scalability Deficiency and World Knowledge Loss\nissues in models trained with synthetic captions, which have been largely\nobscured by their initial benchmark success. Upon closer examination, we\nidentify the root cause as the overly-simplified language structure and lack of\nknowledge details in existing synthetic captions. To provide higher-quality and\nmore scalable multimodal pretraining data, we propose CapsFusion, an advanced\nframework that leverages large language models to consolidate and refine\ninformation from both web-based image-text pairs and synthetic captions.\nExtensive experiments show that CapsFusion captions exhibit remarkable\nall-round superiority over existing captions in terms of model performance\n(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample\nefficiency (requiring 11-16 times less computation than baselines), world\nknowledge depth, and scalability. These effectiveness, efficiency and\nscalability advantages position CapsFusion as a promising candidate for future\nscaling of LMM training.",
            "author": [
                "Qiying Yu",
                "Quan Sun",
                "Xiaosong Zhang",
                "Yufeng Cui",
                "Fan Zhang",
                "Yue Cao",
                "Xinlong Wang",
                "Jingjing Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20550v2",
                "http://arxiv.org/pdf/2310.20550v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20548v1",
            "title": "Ordering dynamics of nonlinear voter models",
            "updated": "2023-10-31T15:28:10Z",
            "published": "2023-10-31T15:28:10Z",
            "summary": "We study the ordering dynamics of nonlinear voter models with multiple\nstates, also providing a discussion of the two-state model. The rate with which\nan individual adopts an opinion scales as the $q$-th power of the number of the\nindividual's neighbours in that state. For $q>1$ the dynamics favor the opinion\nheld by the most agents. The ordering to consensus is driven by deterministic\ndrift, and noise only plays a minor role. For $q<1$ the dynamics favors\nminority opinions, and for multistate models the ordering proceeds through a\nnoise-driven succession of metastable states. Unlike linear multi-state\nsystems, the nonlinear model cannot be reduced to an effective two-state model.\nWe find that the average density of active interfaces in the model with\nmultiple opinion states does not show a single exponential decay in time for\n$q<1$, again at variance with the linear model. This highlights the special\ncharacter of the conventional (linear) voter model, in which deterministic\ndrift is absent. As part of our analysis, we develop a pair approximation for\nthe multi-state model on graphs, valid for any positive real value of $q$,\nimproving on previous approximations for nonlinear two-state voter models.",
            "author": [
                "Luc\u00eda Soledad Ramirez",
                "Federico Vazquez",
                "Maxi San Miguel",
                "Tobias Galla"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20548v1",
                "http://arxiv.org/pdf/2310.20548v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20544v1",
            "title": "Information-theoretic causality and applications to turbulence: energy\n  cascade and inner/outer layer interactions",
            "updated": "2023-10-31T15:26:18Z",
            "published": "2023-10-31T15:26:18Z",
            "summary": "We introduce an information-theoretic method for quantifying causality in\nchaotic systems. The approach, referred to as IT-causality, quantifies\ncausality by measuring the information gained about future events conditioned\non the knowledge of past events. The causal interactions are classified into\nredundant, unique, and synergistic contributions depending on their nature. The\nformulation is non-intrusive, invariance under invertible transformations of\nthe variables, and provides the missing causality due to unobserved variables.\nThe method only requires pairs of past-future events of the quantities of\ninterest, making it convenient for both computational simulations and\nexperimental investigations. IT-causality is validated in four scenarios\nrepresenting basic causal interactions among variables: mediator, confounder,\nredundant collider, and synergistic collider. The approach is leveraged to\naddress two questions relevant to turbulence research: i) the scale locality of\nthe energy cascade in isotropic turbulence, and ii) the interactions between\ninner and outer layer flow motions in wall-bounded turbulence. In the former\ncase, we demonstrate that causality in the energy cascade flows sequentially\nfrom larger to smaller scales without requiring intermediate scales.\nConversely, the flow of information from small to large scales is shown to be\nredundant. In the second problem, we observe a unidirectional causality flow,\nwith causality predominantly originating from the outer layer and propagating\ntowards the inner layer, but not vice versa. The decomposition of IT-causality\ninto intensities also reveals that the causality is primarily associated with\nhigh-velocity streaks.",
            "author": [
                "Adri\u00e1n Lozano-Dur\u00e1n",
                "Gonzalo Arranz",
                "Yuenong Ling"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20544v1",
                "http://arxiv.org/pdf/2310.20544v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cs.IT",
                "math.IT",
                "nlin.CD",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20537v1",
            "title": "Directed Cyclic Graph for Causal Discovery from Multivariate Functional\n  Data",
            "updated": "2023-10-31T15:19:24Z",
            "published": "2023-10-31T15:19:24Z",
            "summary": "Discovering causal relationship using multivariate functional data has\nreceived a significant amount of attention very recently. In this article, we\nintroduce a functional linear structural equation model for causal structure\nlearning when the underlying graph involving the multivariate functions may\nhave cycles. To enhance interpretability, our model involves a low-dimensional\ncausal embedded space such that all the relevant causal information in the\nmultivariate functional data is preserved in this lower-dimensional subspace.\nWe prove that the proposed model is causally identifiable under standard\nassumptions that are often made in the causal discovery literature. To carry\nout inference of our model, we develop a fully Bayesian framework with suitable\nprior specifications and uncertainty quantification through posterior\nsummaries. We illustrate the superior performance of our method over existing\nmethods in terms of causal graph estimation through extensive simulation\nstudies. We also demonstrate the proposed method using a brain EEG dataset.",
            "author": [
                "Saptarshi Roy",
                "Raymond K. W. Wong",
                "Yang Ni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20537v1",
                "http://arxiv.org/pdf/2310.20537v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20522v2",
            "title": "Tight bounds on adjacency labels for monotone graph classes",
            "updated": "2023-11-16T13:09:51Z",
            "published": "2023-10-31T15:00:42Z",
            "summary": "A class of graphs admits an adjacency labeling scheme of size $b(n)$, if the\nvertices in each of its $n$-vertex graphs can be assigned binary strings\n(called labels) of length $b(n)$ so that the adjacency of two vertices can be\ndetermined solely from their labels. We give tight bounds on the size of\nadjacency labels for every family of monotone (i.e., subgraph-closed) classes\nwith a well-behaved growth function between $2^{O(n \\log n)}$ and\n$2^{O(n^{2-\\delta})}$ for any $\\delta > 0$. Specifically, we show that for any\nfunction $f: \\mathbb N \\to \\mathbb R$ satisfying $\\log n \\leqslant f(n)\n\\leqslant n^{1-\\delta}$ for any fixed $\\delta > 0$, and some sub-multiplicative\ncondition, there are monotone graph classes with growth $2^{O(nf(n))}$ that do\nnot admit adjacency labels of size at most $f(n) \\log n$. On the other hand,\nany such class does admit adjacency labels of size $O(f(n)\\log n)$.\nSurprisingly this tight bound is a $\\Theta(\\log n)$ factor away from the\ninformation-theoretic bound of $O(f(n))$. The special case when $f = \\log$\nimplies that the recently-refuted Implicit Graph Conjecture [Hatami and Hatami,\nFOCS 2022] also fails within monotone classes.",
            "author": [
                "\u00c9douard Bonnet",
                "Julien Duron",
                "John Sylvester",
                "Viktor Zamaraev",
                "Maksim Zhukovskii"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20522v2",
                "http://arxiv.org/pdf/2310.20522v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "cs.DS",
                "68R01, 68R05, 05C80",
                "G.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20519v1",
            "title": "Enhancing Graph Neural Networks with Quantum Computed Encodings",
            "updated": "2023-10-31T14:56:52Z",
            "published": "2023-10-31T14:56:52Z",
            "summary": "Transformers are increasingly employed for graph data, demonstrating\ncompetitive performance in diverse tasks. To incorporate graph information into\nthese models, it is essential to enhance node and edge features with positional\nencodings. In this work, we propose novel families of positional encodings\ntailored for graph transformers. These encodings leverage the long-range\ncorrelations inherent in quantum systems, which arise from mapping the topology\nof a graph onto interactions between qubits in a quantum computer. Our\ninspiration stems from the recent advancements in quantum processing units,\nwhich offer computational capabilities beyond the reach of classical hardware.\nWe prove that some of these quantum features are theoretically more expressive\nfor certain graphs than the commonly used relative random walk probabilities.\nEmpirically, we show that the performance of state-of-the-art models can be\nimproved on standard benchmarks and large-scale datasets by computing tractable\nversions of quantum features. Our findings highlight the potential of\nleveraging quantum computing capabilities to potentially enhance the\nperformance of transformers in handling graph data.",
            "author": [
                "Slimane Thabet",
                "Romain Fouilland",
                "Mehdi Djellabi",
                "Igor Sokolov",
                "Sachin Kasture",
                "Louis-Paul Henry",
                "Lo\u00efc Henriet"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20519v1",
                "http://arxiv.org/pdf/2310.20519v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20508v1",
            "title": "Parametric Fairness with Statistical Guarantees",
            "updated": "2023-10-31T14:52:39Z",
            "published": "2023-10-31T14:52:39Z",
            "summary": "Algorithmic fairness has gained prominence due to societal and regulatory\nconcerns about biases in Machine Learning models. Common group fairness metrics\nlike Equalized Odds for classification or Demographic Parity for both\nclassification and regression are widely used and a host of computationally\nadvantageous post-processing methods have been developed around them. However,\nthese metrics often limit users from incorporating domain knowledge. Despite\nmeeting traditional fairness criteria, they can obscure issues related to\nintersectional fairness and even replicate unwanted intra-group biases in the\nresulting fair solution. To avoid this narrow perspective, we extend the\nconcept of Demographic Parity to incorporate distributional properties in the\npredictions, allowing expert knowledge to be used in the fair solution. We\nillustrate the use of this new metric through a practical example of wages, and\ndevelop a parametric method that efficiently addresses practical challenges\nlike limited training data and constraints on total spending, offering a robust\nsolution for real-life applications.",
            "author": [
                "Fran\u00e7ois HU",
                "Philipp Ratz",
                "Arthur Charpentier"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20508v1",
                "http://arxiv.org/pdf/2310.20508v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20500v1",
            "title": "Small doubling implies small tripling at large scales",
            "updated": "2023-10-31T14:41:21Z",
            "published": "2023-10-31T14:41:21Z",
            "summary": "We show that if $K\\ge1$ is a parameter and $S$ is a finite symmetric subset\nof a group containing the identity such $|S^{2n}|\\le K|S^n|$ for some integer\n$n\\ge 2K^2$, then $|S^{3n}|\\le\\exp(\\exp(O(K^2)))|S^n|$. Such a result was\npreviously known only under the stronger assumption that $|S^{2n+1}|\\le\nK|S^n|$. We prove similar results for locally compact groups and\nvertex-transitive graphs. We indicate some results in the structure theory of\nvertex-transitive graphs of polynomial growth whose hypotheses can be weakened\nas a result.",
            "author": [
                "Romain Tessera",
                "Matthew Tointon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20500v1",
                "http://arxiv.org/pdf/2310.20500v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20494v1",
            "title": "A Transformer-Based Model With Self-Distillation for Multimodal Emotion\n  Recognition in Conversations",
            "updated": "2023-10-31T14:33:30Z",
            "published": "2023-10-31T14:33:30Z",
            "summary": "Emotion recognition in conversations (ERC), the task of recognizing the\nemotion of each utterance in a conversation, is crucial for building empathetic\nmachines. Existing studies focus mainly on capturing context- and\nspeaker-sensitive dependencies on the textual modality but ignore the\nsignificance of multimodal information. Different from emotion recognition in\ntextual conversations, capturing intra- and inter-modal interactions between\nutterances, learning weights between different modalities, and enhancing modal\nrepresentations play important roles in multimodal ERC. In this paper, we\npropose a transformer-based model with self-distillation (SDT) for the task.\nThe transformer-based model captures intra- and inter-modal interactions by\nutilizing intra- and inter-modal transformers, and learns weights between\nmodalities dynamically by designing a hierarchical gated fusion strategy.\nFurthermore, to learn more expressive modal representations, we treat soft\nlabels of the proposed model as extra training supervision. Specifically, we\nintroduce self-distillation to transfer knowledge of hard and soft labels from\nthe proposed model to each modality. Experiments on IEMOCAP and MELD datasets\ndemonstrate that SDT outperforms previous state-of-the-art baselines.",
            "author": [
                "Hui Ma",
                "Jian Wang",
                "Hongfei Lin",
                "Bo Zhang",
                "Yijia Zhang",
                "Bo Xu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMM.2023.3271019",
                "http://arxiv.org/abs/2310.20494v1",
                "http://arxiv.org/pdf/2310.20494v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20491v1",
            "title": "Collaborative Decision-Making Using Spatiotemporal Graphs in Connected\n  Autonomy",
            "updated": "2023-10-31T14:30:39Z",
            "published": "2023-10-31T14:30:39Z",
            "summary": "Collaborative decision-making is an essential capability for multi-robot\nsystems, such as connected vehicles, to collaboratively control autonomous\nvehicles in accident-prone scenarios. Under limited communication bandwidth,\ncapturing comprehensive situational awareness by integrating connected agents'\nobservation is very challenging. In this paper, we propose a novel\ncollaborative decision-making method that efficiently and effectively\nintegrates collaborators' representations to control the ego vehicle in\naccident-prone scenarios. Our approach formulates collaborative decision-making\nas a classification problem. We first represent sequences of raw observations\nas spatiotemporal graphs, which significantly reduce the package size to share\namong connected vehicles. Then we design a novel spatiotemporal graph neural\nnetwork based on heterogeneous graph learning, which analyzes spatial and\ntemporal connections of objects in a unified way for collaborative\ndecision-making. We evaluate our approach using a high-fidelity simulator that\nconsiders realistic traffic, communication bandwidth, and vehicle sensing among\nconnected autonomous vehicles. The experimental results show that our\nrepresentation achieves over 100x reduction in the shared data size that meets\nthe requirements of communication bandwidth for connected autonomous driving.\nIn addition, our approach achieves over 30% improvements in driving safety.",
            "author": [
                "Peng Gao",
                "Yu Shen",
                "Ming C. Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20491v1",
                "http://arxiv.org/pdf/2310.20491v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20486v1",
            "title": "Optimal Binary Differential Privacy via Graphs",
            "updated": "2023-10-31T14:27:42Z",
            "published": "2023-10-31T14:27:42Z",
            "summary": "We present the notion of \\emph{reasonable utility} for binary mechanisms,\nwhich applies to all utility functions in the literature. This notion induces a\npartial ordering on the performance of all binary differentially private (DP)\nmechanisms. DP mechanisms that are maximal elements of this ordering are\noptimal DP mechanisms for every reasonable utility. By looking at differential\nprivacy as a randomized graph coloring, we characterize these optimal DP in\nterms of their behavior on a certain subset of the boundary datasets we call a\nboundary hitting set. In the process of establishing our results, we also\nintroduce a useful notion that generalizes DP conditions for binary-valued\nqueries, which we coin as suitable pairs. Suitable pairs abstract away the\nalgebraic roles of $\\varepsilon,\\delta$ in the DP framework, making the\nderivations and understanding of our proofs simpler. Additionally, the notion\nof a suitable pair can potentially capture privacy conditions in frameworks\nother than DP and may be of independent interest.",
            "author": [
                "Sahel Torkamani",
                "Javad B. Ebrahimi",
                "Parastoo Sadeghi",
                "Rafael G. L. D'Oliveira",
                "Muriel M\u00e9dard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20486v1",
                "http://arxiv.org/pdf/2310.20486v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20475v1",
            "title": "Linked Papers With Code: The Latest in Machine Learning as an RDF\n  Knowledge Graph",
            "updated": "2023-10-31T14:09:15Z",
            "published": "2023-10-31T14:09:15Z",
            "summary": "In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge\ngraph that provides comprehensive, current information about almost 400,000\nmachine learning publications. This includes the tasks addressed, the datasets\nutilized, the methods implemented, and the evaluations conducted, along with\ntheir results. Compared to its non-RDF-based counterpart Papers With Code, LPWC\nnot only translates the latest advancements in machine learning into RDF\nformat, but also enables novel ways for scientific impact quantification and\nscholarly key content recommendation. LPWC is openly accessible at\nhttps://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a\nknowledge graph in the Linked Open Data cloud, we offer LPWC in multiple\nformats, from RDF dump files to a SPARQL endpoint for direct web queries, as\nwell as a data source with resolvable URIs and links to the data sources\nSemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph\nembeddings, enabling LPWC to be readily applied in machine learning\napplications.",
            "author": [
                "Michael F\u00e4rber",
                "David Lamprecht"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20475v1",
                "http://arxiv.org/pdf/2310.20475v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20473v1",
            "title": "Improved Roundtrip Spanners, Emulators, and Directed Girth Approximation",
            "updated": "2023-10-31T14:07:36Z",
            "published": "2023-10-31T14:07:36Z",
            "summary": "Roundtrip spanners are the analog of spanners in directed graphs, where the\nroundtrip metric is used as a notion of distance. Recent works have shown\nexistential results of roundtrip spanners nearly matching the undirected case,\nbut the time complexity for constructing roundtrip spanners is still widely\nopen.\n  This paper focuses on developing fast algorithms for roundtrip spanners and\nrelated problems. For any $n$-vertex directed graph $G$ with $m$ edges (with\nnon-negative edge weights), our results are as follows:\n  - 3-roundtrip spanner faster than APSP: We give an\n$\\tilde{O}(m\\sqrt{n})$-time algorithm that constructs a roundtrip spanner of\nstretch $3$ and optimal size $O(n^{3/2})$. Previous constructions of roundtrip\nspanners of the same size either required $\\Omega(nm)$ time [Roditty, Thorup,\nZwick SODA'02; Cen, Duan, Gu ICALP'20], or had worse stretch $4$ [Chechik and\nLifshitz SODA'21].\n  - Optimal roundtrip emulator in dense graphs: For integer $k\\ge 3$, we give\nan $O(kn^2\\log n)$-time algorithm that constructs a roundtrip \\emph{emulator}\nof stretch $(2k-1)$ and size $O(kn^{1+1/k})$, which is optimal for constant $k$\nunder Erd\\H{o}s' girth conjecture. Previous work of [Thorup and Zwick STOC'01]\nimplied a roundtrip emulator of the same size and stretch, but it required\n$\\Omega(nm)$ construction time. Our improved running time is near-optimal for\ndense graphs.\n  - Faster girth approximation in sparse graphs: We give an\n$\\tilde{O}(mn^{1/3})$-time algorithm that $4$-approximates the girth of a\ndirected graph. This can be compared with the previous $2$-approximation\nalgorithm in $\\tilde{O}(n^2, m\\sqrt{n})$ time by [Chechik and Lifshitz\nSODA'21]. In sparse graphs, our algorithm achieves better running time at the\ncost of a larger approximation ratio.",
            "author": [
                "Alina Harbuzova",
                "Ce Jin",
                "Virginia Vassilevska Williams",
                "Zixuan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20473v1",
                "http://arxiv.org/pdf/2310.20473v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20469v1",
            "title": "Amoeba: Circumventing ML-supported Network Censorship via Adversarial\n  Reinforcement Learning",
            "updated": "2023-10-31T14:01:24Z",
            "published": "2023-10-31T14:01:24Z",
            "summary": "Embedding covert streams into a cover channel is a common approach to\ncircumventing Internet censorship, due to censors' inability to examine\nencrypted information in otherwise permitted protocols (Skype, HTTPS, etc.).\nHowever, recent advances in machine learning (ML) enable detecting a range of\nanti-censorship systems by learning distinct statistical patterns hidden in\ntraffic flows. Therefore, designing obfuscation solutions able to generate\ntraffic that is statistically similar to innocuous network activity, in order\nto deceive ML-based classifiers at line speed, is difficult.\n  In this paper, we formulate a practical adversarial attack strategy against\nflow classifiers as a method for circumventing censorship. Specifically, we\ncast the problem of finding adversarial flows that will be misclassified as a\nsequence generation task, which we solve with Amoeba, a novel reinforcement\nlearning algorithm that we design. Amoeba works by interacting with censoring\nclassifiers without any knowledge of their model structure, but by crafting\npackets and observing the classifiers' decisions, in order to guide the\nsequence generation process. Our experiments using data collected from two\npopular anti-censorship systems demonstrate that Amoeba can effectively shape\nadversarial flows that have on average 94% attack success rate against a range\nof ML algorithms. In addition, we show that these adversarial flows are robust\nin different network environments and possess transferability across various ML\nmodels, meaning that once trained against one, our agent can subvert other\ncensoring classifiers without retraining.",
            "author": [
                "Haoyu Liu",
                "Alec F. Diallo",
                "Paul Patras"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3629131",
                "http://arxiv.org/abs/2310.20469v1",
                "http://arxiv.org/pdf/2310.20469v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20466v1",
            "title": "Quantum DNA Sequencing using Gaussian Amplitude Amplification",
            "updated": "2023-10-31T13:58:03Z",
            "published": "2023-10-31T13:58:03Z",
            "summary": "In this study, we explore how quantum pathfinding algorithm called Gaussian\nAmplitude Amplification (GAA) can be used to solve the DNA sequencing problem.\nTo do this, sequencing by hybridization was assumed wherein short fragments of\nthe nucleic acids called oligonucleotides of length l were gathered and were\nthen assembled. The process of reassembling the sequence was then abstracted\ninto a graph problem of finding the Hamiltonian path with the least cost. The\nconstructed directed graph was then converted into sequential bipartite graphs\nin order to use GAA. The results of our simulation revealed that for the case\nwhere l = 2 and spectrum size of |S| = 4, the probability of finding the\noptimal solution (with the least cost) is approximately 70.92% - a significant\nimprovement compared to 4.17% when the path is chosen randomly. While this\nstudy only focused on the ideal scenario where there are no errors in the\nspectrum, the outcomes presented here demonstrate the plausibility of using GAA\nas a genome sequencing method.",
            "author": [
                "Richard Marin",
                "Carlos Baldo III"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20466v1",
                "http://arxiv.org/pdf/2310.20466v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20462v1",
            "title": "Anti-van der Waerden Numbers of Graph Products with Trees",
            "updated": "2023-10-31T13:54:38Z",
            "published": "2023-10-31T13:54:38Z",
            "summary": "Given a graph $G$, an exact $r$-coloring of $G$ is a surjective function\n$c:V(G) \\to [1,\\dots,r]$. An arithmetic progression in $G$ of length $j$ with\ncommon difference $d$ is a set of vertices $\\{v_1,\\dots, v_j\\}$ such that\n$dist(v_i,v_{i+1}) = d$ for $1\\le i < j$. An arithmetic progression is rainbow\nif all of the vertices are colored distinctly. The fewest number of colors that\nguarantees a rainbow arithmetic progression of length three is called the\nanti-van der Waerden number of $G$ and is denoted $aw(G,3)$. It is known that\n$3 \\le aw(G\\square H,3) \\le 4$. Here we determine exact values $aw(T\\square\nT',3)$ for some trees $T$ and $T'$, determine $aw(G\\square T,3)$ for some trees\n$T$, and determine $aw(G\\square H,3)$ for some graphs $G$ and $H$.",
            "author": [
                "Zhanar Berikkyzy",
                "Joe Miller",
                "Elizabeth Sprangel",
                "Shanise Walker",
                "Nathan Warnberg"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20462v1",
                "http://arxiv.org/pdf/2310.20462v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35, 05C15, 05C12",
                "G.2.2; G.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20461v1",
            "title": "Ramsey numbers of bounded degree trees versus general graphs",
            "updated": "2023-10-31T13:54:30Z",
            "published": "2023-10-31T13:54:30Z",
            "summary": "For every $k\\ge 2$ and $\\Delta$, we prove that there exists a constant\n$C_{\\Delta,k}$ such that the following holds. For every graph $H$ with\n$\\chi(H)=k$ and every tree with at least $C_{\\Delta,k}|H|$ vertices and maximum\ndegree at most $\\Delta$, the Ramsey number $R(T,H)$ is\n$(k-1)(|T|-1)+\\sigma(H)$, where $\\sigma(H)$ is the size of a smallest colour\nclass across all proper $k$-colourings of $H$. This is tight up to the value of\n$C_{\\Delta,k}$, and confirms a conjecture of Balla, Pokrovskiy, and Sudakov.",
            "author": [
                "Richard Montgomery",
                "Mat\u00edas Pavez-Sign\u00e9",
                "Jun Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20461v1",
                "http://arxiv.org/pdf/2310.20461v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20446v1",
            "title": "LAVSS: Location-Guided Audio-Visual Spatial Audio Separation",
            "updated": "2023-10-31T13:30:24Z",
            "published": "2023-10-31T13:30:24Z",
            "summary": "Existing machine learning research has achieved promising results in monaural\naudio-visual separation (MAVS). However, most MAVS methods purely consider what\nthe sound source is, not where it is located. This can be a problem in VR/AR\nscenarios, where listeners need to be able to distinguish between similar audio\nsources located in different directions. To address this limitation, we have\ngeneralized MAVS to spatial audio separation and proposed LAVSS: a\nlocation-guided audio-visual spatial audio separator. LAVSS is inspired by the\ncorrelation between spatial audio and visual location. We introduce the phase\ndifference carried by binaural audio as spatial cues, and we utilize positional\nrepresentations of sounding objects as additional modality guidance. We also\nleverage multi-level cross-modal attention to perform visual-positional\ncollaboration with audio features. In addition, we adopt a pre-trained monaural\nseparator to transfer knowledge from rich mono sounds to boost spatial audio\nseparation. This exploits the correlation between monaural and binaural\nchannels. Experiments on the FAIR-Play dataset demonstrate the superiority of\nthe proposed LAVSS over existing benchmarks of audio-visual separation. Our\nproject page: https://yyx666660.github.io/LAVSS/.",
            "author": [
                "Yuxin Ye",
                "Wenming Yang",
                "Yapeng Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20446v1",
                "http://arxiv.org/pdf/2310.20446v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CV",
                "cs.MM",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20443v1",
            "title": "Ontologies for Models and Algorithms in Applied Mathematics and Related\n  Disciplines",
            "updated": "2023-10-31T13:24:28Z",
            "published": "2023-10-31T13:24:28Z",
            "summary": "In applied mathematics and related disciplines, the\nmodeling-simulation-optimization workflow is a prominent scheme, with\nmathematical models and numerical algorithms playing a crucial role. For these\ntypes of mathematical research data, the Mathematical Research Data Initiative\nhas developed, merged and implemented ontologies and knowledge graphs. This\ncontributes to making mathematical research data FAIR by introducing semantic\ntechnology and documenting the mathematical foundations accordingly. Using the\nconcrete example of microfracture analysis of porous media, it is shown how the\nknowledge of the underlying mathematical model and the corresponding numerical\nalgorithms for its solution can be represented by the ontologies.",
            "author": [
                "Bj\u00f6rn Schembera",
                "Frank W\u00fcbbeling",
                "Hendrik Kleikamp",
                "Christine Biedinger",
                "Jochen Fiedler",
                "Marco Reidelbach",
                "Aurela Shehu",
                "Burkhard Schmidt",
                "Thomas Koprucki",
                "Dorothea Iglezakis",
                "Dominik G\u00f6ddeke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20443v1",
                "http://arxiv.org/pdf/2310.20443v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "cs.DL",
                "cs.IR",
                "H.3; H.4; I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20440v1",
            "title": "The SourceData-NLP dataset: integrating curation into scientific\n  publishing for training large language models",
            "updated": "2023-10-31T13:22:38Z",
            "published": "2023-10-31T13:22:38Z",
            "summary": "Introduction: The scientific publishing landscape is expanding rapidly,\ncreating challenges for researchers to stay up-to-date with the evolution of\nthe literature. Natural Language Processing (NLP) has emerged as a potent\napproach to automating knowledge extraction from this vast amount of\npublications and preprints. Tasks such as Named-Entity Recognition (NER) and\nNamed-Entity Linking (NEL), in conjunction with context-dependent semantic\ninterpretation, offer promising and complementary approaches to extracting\nstructured information and revealing key concepts.\n  Results: We present the SourceData-NLP dataset produced through the routine\ncuration of papers during the publication process. A unique feature of this\ndataset is its emphasis on the annotation of bioentities in figure legends. We\nannotate eight classes of biomedical entities (small molecules, gene products,\nsubcellular components, cell lines, cell types, tissues, organisms, and\ndiseases), their role in the experimental design, and the nature of the\nexperimental method as an additional class. SourceData-NLP contains more than\n620,000 annotated biomedical entities, curated from 18,689 figures in 3,223\npapers in molecular and cell biology. We illustrate the dataset's usefulness by\nassessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned\non the SourceData-NLP dataset for NER. We also introduce a novel\ncontext-dependent semantic task that infers whether an entity is the target of\na controlled intervention or the object of measurement.\n  Conclusions: SourceData-NLP's scale highlights the value of integrating\ncuration into publishing. Models trained with SourceData-NLP will furthermore\nenable the development of tools able to extract causal hypotheses from the\nliterature and assemble them into knowledge graphs.",
            "author": [
                "Jorge Abreu-Vicente",
                "Hannah Sonntag",
                "Thomas Eidens",
                "Thomas Lemberger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20440v1",
                "http://arxiv.org/pdf/2310.20440v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20433v1",
            "title": "Simple and tight complexity lower bounds for solving Rabin games",
            "updated": "2023-10-31T13:11:04Z",
            "published": "2023-10-31T13:11:04Z",
            "summary": "We give a simple proof that assuming the Exponential Time Hypothesis (ETH),\ndetermining the winner of a Rabin game cannot be done in time $2^{o(k \\log k)}\n\\cdot n^{O(1)}$, where $k$ is the number of pairs of vertex subsets involved in\nthe winning condition and $n$ is the vertex count of the game graph. While this\nresult follows from the lower bounds provided by Calude et al [SIAM J. Comp.\n2022], our reduction is simpler and arguably provides more insight into the\ncomplexity of the problem. In fact, the analogous lower bounds discussed by\nCalude et al, for solving Muller games and multidimensional parity games,\nfollow as simple corollaries of our approach. Our reduction also highlights the\nusefulness of a certain pivot problem -- Permutation SAT -- which may be of\nindependent interest.",
            "author": [
                "Antonio Casares",
                "Marcin Pilipczuk",
                "Micha\u0142 Pilipczuk",
                "U\u00e9verton S. Souza",
                "K. S. Thejaswini"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20433v1",
                "http://arxiv.org/pdf/2310.20433v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20423v1",
            "title": "Limits of chordal graphs with bounded tree-width",
            "updated": "2023-10-31T12:48:39Z",
            "published": "2023-10-31T12:48:39Z",
            "summary": "We study random $k$-connected chordal graphs with bounded tree-width. Our\nmain results are scaling limits and quenched local limits.",
            "author": [
                "Jordi Castellv\u00ed",
                "Benedikt Stufler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20423v1",
                "http://arxiv.org/pdf/2310.20423v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO",
                "60C05, 05C80"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20419v1",
            "title": "Relative NN-Descent: A Fast Index Construction for Graph-Based\n  Approximate Nearest Neighbor Search",
            "updated": "2023-10-31T12:46:18Z",
            "published": "2023-10-31T12:46:18Z",
            "summary": "Approximate Nearest Neighbor Search (ANNS) is the task of finding the\ndatabase vector that is closest to a given query vector. Graph-based ANNS is\nthe family of methods with the best balance of accuracy and speed for\nmillion-scale datasets. However, graph-based methods have the disadvantage of\nlong index construction time. Recently, many researchers have improved the\ntradeoff between accuracy and speed during a search. However, there is little\nresearch on accelerating index construction. We propose a fast graph\nconstruction algorithm, Relative NN-Descent (RNN-Descent). RNN-Descent combines\nNN-Descent, an algorithm for constructing approximate K-nearest neighbor graphs\n(K-NN graphs), and RNG Strategy, an algorithm for selecting edges effective for\nsearch. This algorithm allows the direct construction of graph-based indexes\nwithout ANNS. Experimental results demonstrated that the proposed method had\nthe fastest index construction speed, while its search performance is\ncomparable to existing state-of-the-art methods such as NSG. For example, in\nexperiments on the GIST1M dataset, the construction of the proposed method is\n2x faster than NSG. Additionally, it was even faster than the construction\nspeed of NN-Descent.",
            "author": [
                "Naoki Ono",
                "Yusuke Matsui"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20419v1",
                "http://arxiv.org/pdf/2310.20419v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20418v1",
            "title": "Multipartite entanglement sudden death and birth in randomized\n  hypergraph states",
            "updated": "2023-10-31T12:45:26Z",
            "published": "2023-10-31T12:45:26Z",
            "summary": "We introduce and analyze the entanglement properties of randomized hypergraph\nstates, as an extended notion of the randomization procedure in the quantum\nlogic gates for the usual graph states, recently proposed in the literature.\nThe probabilities of applying imperfect generalized controlled-$Z$ gates\nsimulate the noisy operations over the qubits. We obtain entanglement measures\nas negativity, concurrence, and genuine multiparticle negativity, and show that\nentanglement exhibits a non-monotonic behavior in terms of the randomness\nparameters, which is a consequence of the non-uniformity of the associated\nhypergraphs, reinforcing the claim that the entanglement of randomized graph\nstates is monotonic since they are related to $2$-uniform hypergraphs.\nMoreover, we observed the phenomena of entanglement sudden death and\nentanglement sudden birth in RH states. This work comes to unveil a connection\nbetween the non-uniformity of hypergraphs and loss of entanglement.",
            "author": [
                "Vinicius Salem",
                "Alison A. Silva",
                "Fabiano M. Andrade"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20418v1",
                "http://arxiv.org/pdf/2310.20418v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20398v1",
            "title": "A hybrid approach for solving the gravitational N-body problem with\n  Artificial Neural Networks",
            "updated": "2023-10-31T12:20:20Z",
            "published": "2023-10-31T12:20:20Z",
            "summary": "Simulating the evolution of the gravitational N-body problem becomes\nextremely computationally expensive as N increases since the problem complexity\nscales quadratically with the number of bodies. We study the use of Artificial\nNeural Networks (ANNs) to replace expensive parts of the integration of\nplanetary systems. Neural networks that include physical knowledge have grown\nin popularity in the last few years, although few attempts have been made to\nuse them to speed up the simulation of the motion of celestial bodies. We study\nthe advantages and limitations of using Hamiltonian Neural Networks to replace\ncomputationally expensive parts of the numerical simulation. We compare the\nresults of the numerical integration of a planetary system with asteroids with\nthose obtained by a Hamiltonian Neural Network and a conventional Deep Neural\nNetwork, with special attention to understanding the challenges of this\nproblem. Due to the non-linear nature of the gravitational equations of motion,\nerrors in the integration propagate. To increase the robustness of a method\nthat uses neural networks, we propose a hybrid integrator that evaluates the\nprediction of the network and replaces it with the numerical solution if\nconsidered inaccurate. Hamiltonian Neural Networks can make predictions that\nresemble the behavior of symplectic integrators but are challenging to train\nand in our case fail when the inputs differ ~7 orders of magnitude. In\ncontrast, Deep Neural Networks are easy to train but fail to conserve energy,\nleading to fast divergence from the reference solution. The hybrid integrator\ndesigned to include the neural networks increases the reliability of the method\nand prevents large energy errors without increasing the computing cost\nsignificantly. For this problem, the use of neural networks results in faster\nsimulations when the number of asteroids is >70.",
            "author": [
                "Veronica Saz Ulibarrena",
                "Philipp Horn",
                "Simon Portegies Zwart",
                "Elena Sellentin",
                "Barry Koren",
                "Maxwell X. Cai"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jcp.2023.112596",
                "http://arxiv.org/abs/2310.20398v1",
                "http://arxiv.org/pdf/2310.20398v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20391v1",
            "title": "Serverless Scheduling Policies based on Cost Analysis",
            "updated": "2023-10-31T12:09:40Z",
            "published": "2023-10-31T12:09:40Z",
            "summary": "Current proprietary and open-source serverless platforms follow opinionated,\nhardcoded scheduling policies to deploy the functions to be executed over the\navailable workers. Such policies may decrease the performance and the security\nof the application due to locality issues (e.g., functions executed by workers\nfar from the databases to be accessed). These limitations are partially\novercome by the adoption of APP, a new platform-agnostic declarative language\nthat allows serverless platforms to support multiple scheduling logics.\nDefining the \"right\" scheduling policy in APP is far from being a trivial task\nsince it often requires rounds of refinement involving knowledge of the\nunderlying infrastructure, guesswork, and empirical testing. In this paper, we\nstart investigating how information derived from static analysis could be\nincorporated into APP scheduling function policies to help users select the\nbest-performing workers at function allocation. We substantiate our proposal by\npresenting a pipeline able to extract cost equations from functions' code,\nsynthesising cost expressions through the usage of off-the-shelf solvers, and\nextending APP allocation policies to consider this information.",
            "author": [
                "Giuseppe De Palma",
                "Saverio Giallorenzo",
                "Cosimo Laneve",
                "Jacopo Mauro",
                "Matteo Trentin",
                "Gianluigi Zavattaro"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.392.3",
                "http://arxiv.org/abs/2310.20391v1",
                "http://arxiv.org/pdf/2310.20391v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20385v1",
            "title": "Free energy and metastable states in the square-lattice J1-J2 Ising\n  model",
            "updated": "2023-10-31T11:49:13Z",
            "published": "2023-10-31T11:49:13Z",
            "summary": "We calculate the (restricted) free energy as a function of polarization for\nthe square-lattice J1-J2 Ising model using the Random local field approximation\n(RLFA) and Monte Carlo (MC) simulations. Here we consider mainly coupling\nconstants in the range 0 < J2 < 1/2 at J1 = - 1, for which the ground state is\nferromagnetic (or N{\\'e}el antiferromagnetic when J1 = 1). Within RLFA, a\nmetastable state with zero polarization is present in the ordered phase, which\nwas recently discussed by V.A. Abalmasov and B.E. Vugmeister, Phys. Rev. E 107,\n034124 (2023). In addition, the free energy calculated within RLFA indicates a\ngeometric slab-droplet phase transition at low temperature, which cannot be\ndetected in the mean field approximation. In turn, exact calculations of the\nfree energy for the sample size L = 6 and MC simulations for L = 10 reveal\nmetastable states with a wide range of polarization values in the ordered\nphase, the origin of which we discuss. The calculations also reveal additional\nslab-droplet transitions (at J2 > 0.25). These findings enrich our knowledge of\nthe J1-J2 Ising model and the RLFA as a useful theoretical tool to study phase\ntransitions in spin systems.",
            "author": [
                "V. A. Abalmasov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20385v1",
                "http://arxiv.org/pdf/2310.20385v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.stat-mech",
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20381v3",
            "title": "A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical\n  Imaging",
            "updated": "2023-11-06T10:57:24Z",
            "published": "2023-10-31T11:39:09Z",
            "summary": "This paper presents a comprehensive evaluation of GPT-4V's capabilities\nacross diverse medical imaging tasks, including Radiology Report Generation,\nMedical Visual Question Answering (VQA), and Visual Grounding. While prior\nefforts have explored GPT-4V's performance in medical image analysis, to the\nbest of our knowledge, our study represents the first quantitative evaluation\non publicly available benchmarks. Our findings highlight GPT-4V's potential in\ngenerating descriptive reports for chest X-ray images, particularly when guided\nby well-structured prompts. Meanwhile, its performance on the MIMIC-CXR dataset\nbenchmark reveals areas for improvement in certain evaluation metrics, such as\nCIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in\ndistinguishing between question types but falls short of the VQA-RAD benchmark\nin terms of accuracy. Furthermore, our analysis finds the limitations of\nconventional evaluation metrics like the BLEU scores, advocating for the\ndevelopment of more semantically robust assessment methods. In the field of\nVisual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding\nboxes, but its precision is lacking, especially in identifying specific medical\norgans and signs. Our evaluation underscores the significant potential of\nGPT-4V in the medical imaging domain, while also emphasizing the need for\ntargeted refinements to fully unlock its capabilities.",
            "author": [
                "Yingshu Li",
                "Yunyi Liu",
                "Zhanyu Wang",
                "Xinyu Liang",
                "Lingqiao Liu",
                "Lei Wang",
                "Leyang Cui",
                "Zhaopeng Tu",
                "Longyue Wang",
                "Luping Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20381v3",
                "http://arxiv.org/pdf/2310.20381v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20366v1",
            "title": "Distil the informative essence of loop detector data set: Is\n  network-level traffic forecasting hungry for more data?",
            "updated": "2023-10-31T11:23:10Z",
            "published": "2023-10-31T11:23:10Z",
            "summary": "Network-level traffic condition forecasting has been intensively studied for\ndecades. Although prediction accuracy has been continuously improved with\nemerging deep learning models and ever-expanding traffic data, traffic\nforecasting still faces many challenges in practice. These challenges include\nthe robustness of data-driven models, the inherent unpredictability of traffic\ndynamics, and whether further improvement of traffic forecasting requires more\nsensor data. In this paper, we focus on this latter question and particularly\non data from loop detectors. To answer this, we propose an uncertainty-aware\ntraffic forecasting framework to explore how many samples of loop data are\ntruly effective for training forecasting models. Firstly, the model design\ncombines traffic flow theory with graph neural networks, ensuring the\nrobustness of prediction and uncertainty quantification. Secondly, evidential\nlearning is employed to quantify different sources of uncertainty in a single\npass. The estimated uncertainty is used to \"distil\" the essence of the dataset\nthat sufficiently covers the information content. Results from a case study of\na highway network around Amsterdam show that, from 2018 to 2021, more than 80\\%\nof the data during daytime can be removed. The remaining 20\\% samples have\nequal prediction power for training models. This result suggests that indeed\nlarge traffic datasets can be subdivided into significantly smaller but equally\ninformative datasets. From these findings, we conclude that the proposed\nmethodology proves valuable in evaluating large traffic datasets' true\ninformation content. Further extensions, such as extracting smaller, spatially\nnon-redundant datasets, are possible with this method.",
            "author": [
                "Guopeng Li",
                "Victor L. Knoop",
                "J. W. C.",
                "van Lint"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20366v1",
                "http://arxiv.org/pdf/2310.20366v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20357v2",
            "title": "Enhancing the Spatial Awareness Capability of Multi-Modal Large Language\n  Model",
            "updated": "2023-11-01T02:13:59Z",
            "published": "2023-10-31T10:57:35Z",
            "summary": "The Multi-Modal Large Language Model (MLLM) refers to an extension of the\nLarge Language Model (LLM) equipped with the capability to receive and infer\nmulti-modal data. Spatial awareness stands as one of the crucial abilities of\nMLLM, encompassing diverse skills related to understanding spatial\nrelationships among objects and between objects and the scene area. Industries\nsuch as autonomous driving, smart healthcare, robotics, virtual, and augmented\nreality heavily demand MLLM's spatial awareness capabilities. However, there\nexists a noticeable gap between the current spatial awareness capabilities of\nMLLM and the requirements set by human needs. To address this issue, this paper\nproposes using more precise spatial position information between objects to\nguide MLLM in providing more accurate responses to user-related inquiries.\nSpecifically, for a particular multi-modal task, we utilize algorithms for\nacquiring geometric spatial information and scene graphs to obtain relevant\ngeometric spatial information and scene details of objects involved in the\nquery. Subsequently, based on this information, we direct MLLM to address\nspatial awareness-related queries posed by the user. Extensive experiments were\nconducted in benchmarks such as MME, MM-Vet, and other multi-modal large\nlanguage models. The experimental results thoroughly confirm the efficacy of\nthe proposed method in enhancing the spatial awareness tasks and associated\ntasks of MLLM.",
            "author": [
                "Yongqiang Zhao",
                "Zhenyu Li",
                "Zhi Jin",
                "Feng Zhang",
                "Haiyan Zhao",
                "Chengfeng Dou",
                "Zhengwei Tao",
                "Xinhai Xu",
                "Donghong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20357v2",
                "http://arxiv.org/pdf/2310.20357v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20354v1",
            "title": "Statistical Complexity of Heterogeneous Geometric Networks",
            "updated": "2023-10-31T10:51:10Z",
            "published": "2023-10-31T10:51:10Z",
            "summary": "Heterogeneity and geometry are key explanatory components underlying the\nstructure of real-world networks. The relationship between these components and\nthe statistical complexity of networks is not well understood. We introduce a\nparsimonious normalised measure of statistical complexity for networks --\nnormalised hierarchical complexity. The measure is trivially 0 in regular\ngraphs and we prove that this measure tends to 0 in Erd\\\"os-R\\'enyi random\ngraphs in the thermodynamic limit. We go on to demonstrate that greater\ncomplexity arises from the combination of hierarchical and geometric components\nto the network structure than either on their own. Further, the levels of\ncomplexity achieved are similar to those found in many real-world networks. We\nalso find that real world networks establish connections in a way which\nincreases hierarchical complexity and which our null models and a range of\nattachment mechanisms fail to explain. This underlines the non-trivial nature\nof statistical complexity in real-world networks and provides foundations for\nthe comparative analysis of network complexity within and across disciplines.",
            "author": [
                "Keith Malcolm Smith",
                "Jason P. Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20354v1",
                "http://arxiv.org/pdf/2310.20354v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20350v1",
            "title": "Combining Shape Completion and Grasp Prediction for Fast and Versatile\n  Grasping with a Multi-Fingered Hand",
            "updated": "2023-10-31T10:46:19Z",
            "published": "2023-10-31T10:46:19Z",
            "summary": "Grasping objects with limited or no prior knowledge about them is a highly\nrelevant skill in assistive robotics. Still, in this general setting, it has\nremained an open problem, especially when it comes to only partial\nobservability and versatile grasping with multi-fingered hands. We present a\nnovel, fast, and high fidelity deep learning pipeline consisting of a shape\ncompletion module that is based on a single depth image, and followed by a\ngrasp predictor that is based on the predicted object shape. The shape\ncompletion network is based on VQDIF and predicts spatial occupancy values at\narbitrary query points. As grasp predictor, we use our two-stage architecture\nthat first generates hand poses using an autoregressive model and then\nregresses finger joint configurations per pose. Critical factors turn out to be\nsufficient data realism and augmentation, as well as special attention to\ndifficult cases during training. Experiments on a physical robot platform\ndemonstrate successful grasping of a wide range of household objects based on a\ndepth image from a single viewpoint. The whole pipeline is fast, taking only\nabout 1 s for completing the object's shape (0.7 s) and generating 1000 grasps\n(0.3 s).",
            "author": [
                "Matthias Humt",
                "Dominik Winkelbauer",
                "Ulrich Hillenbrand",
                "Berthold B\u00e4uml"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20350v1",
                "http://arxiv.org/pdf/2310.20350v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20349v1",
            "title": "A Low-cost Strategic Monitoring Approach for Scalable and Interpretable\n  Error Detection in Deep Neural Networks",
            "updated": "2023-10-31T10:45:55Z",
            "published": "2023-10-31T10:45:55Z",
            "summary": "We present a highly compact run-time monitoring approach for deep computer\nvision networks that extracts selected knowledge from only a few (down to\nmerely two) hidden layers, yet can efficiently detect silent data corruption\noriginating from both hardware memory and input faults. Building on the insight\nthat critical faults typically manifest as peak or bulk shifts in the\nactivation distribution of the affected network layers, we use strategically\nplaced quantile markers to make accurate estimates about the anomaly of the\ncurrent inference as a whole. Importantly, the detector component itself is\nkept algorithmically transparent to render the categorization of regular and\nabnormal behavior interpretable to a human. Our technique achieves up to ~96%\nprecision and ~98% recall of detection. Compared to state-of-the-art anomaly\ndetection techniques, this approach requires minimal compute overhead (as\nlittle as 0.3% with respect to non-supervised inference time) and contributes\nto the explainability of the model.",
            "author": [
                "Florian Geissler",
                "Syed Qutub",
                "Michael Paulitsch",
                "Karthik Pattabiraman"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-40923-3_7",
                "http://arxiv.org/abs/2310.20349v1",
                "http://arxiv.org/pdf/2310.20349v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20340v1",
            "title": "Near-Optimal Coverage Path Planning with Turn Costs",
            "updated": "2023-10-31T10:24:45Z",
            "published": "2023-10-31T10:24:45Z",
            "summary": "Coverage path planning is a fundamental challenge in robotics, with diverse\napplications in aerial surveillance, manufacturing, cleaning, inspection,\nagriculture, and more. The main objective is to devise a trajectory for an\nagent that efficiently covers a given area, while minimizing time or energy\nconsumption. Existing practical approaches often lack a solid theoretical\nfoundation, relying on purely heuristic methods, or overly abstracting the\nproblem to a simple Traveling Salesman Problem in Grid Graphs. Moreover, the\nconsidered cost functions only rarely consider turn cost, prize-collecting\nvariants for uneven cover demand, or arbitrary geometric regions.\n  In this paper, we describe an array of systematic methods for handling\narbitrary meshes derived from intricate, polygonal environments. This\nadaptation paves the way to compute efficient coverage paths with a robust\ntheoretical foundation for real-world robotic applications. Through\ncomprehensive evaluations, we demonstrate that the algorithm also exhibits low\noptimality gaps, while efficiently handling complex environments. Furthermore,\nwe showcase its versatility in handling partial coverage and accommodating\nheterogeneous passage costs, offering the flexibility to trade off coverage\nquality and time efficiency.",
            "author": [
                "Dominik Michael Krupke"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20340v1",
                "http://arxiv.org/pdf/2310.20340v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20339v1",
            "title": "ExoRecovery: Push Recovery with a Lower-Limb Exoskeleton based on\n  Stepping Strategy",
            "updated": "2023-10-31T10:24:37Z",
            "published": "2023-10-31T10:24:37Z",
            "summary": "Balance loss is a significant challenge in lower-limb exoskeleton\napplications, as it can lead to potential falls, thereby impacting user safety\nand confidence. We introduce a control framework for omnidirectional recovery\nstep planning by online optimization of step duration and position in response\nto external forces. We map the step duration and position to a human-like foot\ntrajectory, which is then translated into joint trajectories using inverse\nkinematics. These trajectories are executed via an impedance controller,\npromoting cooperation between the exoskeleton and the user.\n  Moreover, our framework is based on the concept of the divergent component of\nmotion, also known as the Extrapolated Center of Mass, which has been\nestablished as a consistent dynamic for describing human movement. This\nreal-time online optimization framework enhances the adaptability of\nexoskeleton users under unforeseen forces thereby improving the overall user\nstability and safety. To validate the effectiveness of our approach,\nsimulations, and experiments were conducted. Our push recovery experiments\nemploying the exoskeleton in zero-torque mode (without assistance) exhibit an\nalignment with the exoskeleton's recovery assistance mode, that shows the\nconsistency of the control framework with human intention. To the best of our\nknowledge, this is the first cooperative push recovery framework for the\nlower-limb human exoskeleton that relies on the simultaneous adaptation of\nintra-stride parameters in both frontal and sagittal directions. The proposed\ncontrol scheme has been validated with human subject experiments.",
            "author": [
                "Zeynep \u00d6zge Orhan",
                "Milad Shafiee",
                "Vincent Juillard",
                "Joel Coelho Oliveira",
                "Auke Ijspeert",
                "Mohamed Bouri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20339v1",
                "http://arxiv.org/pdf/2310.20339v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20335v1",
            "title": "Uplifting edges in higher order networks: spectral centralities for\n  non-uniform hypergraphs",
            "updated": "2023-10-31T10:21:58Z",
            "published": "2023-10-31T10:21:58Z",
            "summary": "Spectral analysis of networks states that many structural properties of\ngraphs, such as centrality of their nodes, are given in terms of their\nadjacency matrices. The natural extension of such spectral analysis to higher\norder networks is strongly limited by the fact that a given hypergraph could\nhave several different adjacency hypermatrices, hence the results obtained so\nfar are mainly restricted to the class of uniform hypergraphs, which leaves\nmany real systems unattended. A new method for analysing non-linear\neigenvector-like centrality measures of non-uniform hypergraphs is presented in\nthis paper that could be useful for studying properties of\n$\\mathcal{H}$-eigenvectors and $\\mathcal{Z}$-eigenvectors in the non-uniform\ncase. In order to do so, a new operation - the $\\textit{uplift}$ - is\nintroduced, incorporating auxiliary nodes in the hypergraph to allow for a\nuniform-like analysis. We later argue why this is a mathematically sound\noperation, and we furthermore use it to classify a whole family of hypergraphs\nwith unique Perron-like $\\mathcal{Z}$-eigenvectors. We supplement the\ntheoretical analysis with several examples and numerical simulations on\nsynthetic and real datasets.",
            "author": [
                "Gonzalo Contreras-Aso",
                "Cristian P\u00e9rez-Corral",
                "Miguel Romance"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20335v1",
                "http://arxiv.org/pdf/2310.20335v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP",
                "math-ph",
                "math.MP",
                "physics.comp-ph",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20333v1",
            "title": "Semidefinite network games: multiplayer minimax and semidefinite\n  complementarity problems",
            "updated": "2023-10-31T10:20:19Z",
            "published": "2023-10-31T10:20:19Z",
            "summary": "Network games are an important class of games that model agent interactions\nin networked systems, where players are situated at the nodes of a graph and\ntheir payoffs depend on the actions taken by their neighbors. We extend the\nclassical framework by considering a game model where the strategies are\npositive semidefinite matrices having trace one. These (continuous) games can\nserve as a simple model of quantum strategic interactions. We focus on the\nzero-sum case, where the sum of all players' payoffs is equal to zero. We\nestablish that in this class of games, Nash equilibria can be characterized as\nthe projection of a spectrahedron, that is, the feasible region of a\nsemidefinite program. Furthermore, we demonstrate that determining whether a\ngame is a semidefinite network game is equivalent to deciding if the value of a\nsemidefinite program is zero. Beyond the zero-sum case, we characterize Nash\nequilibria as the solutions of a semidefinite linear complementarity problem.",
            "author": [
                "Constantin Ickstadt",
                "Thorsten Theobald",
                "Elias Tsigaridas",
                "Antonios Varvitsiotis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20333v1",
                "http://arxiv.org/pdf/2310.20333v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.GT",
                "52A20, 68Q25, 90C22, 91A43, 91A81"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20325v1",
            "title": "A polynomial-time $\\text{OPT}^\u03b5$-approximation algorithm for\n  maximum independent set of connected subgraphs in a planar graph",
            "updated": "2023-10-31T10:02:36Z",
            "published": "2023-10-31T10:02:36Z",
            "summary": "In the Maximum Independent Set of Objects problem, we are given an $n$-vertex\nplanar graph $G$ and a family $\\mathcal{D}$ of $N$ objects, where each object\nis a connected subgraph of $G$. The task is to find a subfamily $\\mathcal{F}\n\\subseteq \\mathcal{D}$ of maximum cardinality that consists of pairwise\ndisjoint objects. This problem is $\\mathsf{NP}$-hard and is equivalent to the\nproblem of finding the maximum number of pairwise disjoint polygons in a given\nfamily of polygons in the plane.\n  As shown by Adamaszek et al. (J. ACM '19), the problem admits a\n\\emph{quasi-polynomial time approximation scheme} (QPTAS): a\n$(1-\\varepsilon)$-approximation algorithm whose running time is bounded by\n$2^{\\mathrm{poly}(\\log(N),1/\\epsilon)} \\cdot n^{\\mathcal{O}(1)}$. Nevertheless,\nto the best of our knowledge, in the polynomial-time regime only the trivial\n$\\mathcal{O}(N)$-approximation is known for the problem in full generality. In\nthe restricted setting where the objects are pseudolines in the plane, Fox and\nPach (SODA '11) gave an $N^{\\varepsilon}$-approximation algorithm with running\ntime $N^{2^{\\tilde{\\mathcal{O}}(1/\\varepsilon)}}$, for any $\\varepsilon>0$.\n  In this work, we present an $\\text{OPT}^{\\varepsilon}$-approximation\nalgorithm for the problem that runs in time\n$N^{\\tilde{\\mathcal{O}}(1/\\varepsilon^2)} n^{\\mathcal{O}(1)}$, for any\n$\\varepsilon>0$, thus improving upon the result of Fox and Pach both in terms\nof generality and in terms of the running time. Our approach combines the\nmethodology of Voronoi separators, introduced by Marx and Pilipczuk (TALG '22),\nwith a new analysis of the approximation factor.",
            "author": [
                "Jana Cslovjecsek",
                "Micha\u0142 Pilipczuk",
                "Karol W\u0119grzycki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20325v1",
                "http://arxiv.org/pdf/2310.20325v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20310v1",
            "title": "Energy Conserving Higher Order Mixed Finite Element Discretizations of\n  Maxwell's Equations",
            "updated": "2023-10-31T09:36:24Z",
            "published": "2023-10-31T09:36:24Z",
            "summary": "We study a system of Maxwell's equations that describes the time evolution of\nelectromagnetic fields with an additional electric scalar variable to make the\nsystem amenable to a mixed finite element spatial discretization. We\ndemonstrate stability and energy conservation for the variational formulation\nof this Maxwell's system. We then discuss two implicit, energy conserving\nschemes for its temporal discretization: the classical Crank-Nicholson scheme\nand an implicit leapfrog scheme. We next show discrete stability and discrete\nenergy conservation for the semi-discretization using these two time\nintegration methods. We complete our discussion by showing that the error for\nthe full discretization of the Maxwell's system with each of the two implicit\ntime discretization schemes and with spatial discretization through a\nconforming sequence of de Rham finite element spaces converges quadratically in\nthe step size of the time discretization and as an appropriate polynomial power\nof the mesh parameter in accordance with the choice of approximating polynomial\nspaces. Our results for the Crank-Nicholson method are generally well known but\nhave not been demonstrated for this Maxwell's system. Our implicit leapfrog\nscheme is a new method to the best of our knowledge and we provide a complete\nerror analysis for it. Finally, we show computational results to validate our\ntheoretical claims using linear and quadratic Whitney forms for the finite\nelement discretization for some model problems in two and three spatial\ndimensions.",
            "author": [
                "Archana Arya",
                "Kaushik Kalyanaraman"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20310v1",
                "http://arxiv.org/pdf/2310.20310v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "5Q61, 65M06, 65M12, 65M15, 65M22, 65M60, 65Z05, 78M10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00725v2",
            "title": "Light-yield response of liquid scintillators using 2--6 MeV tagged\n  neutrons",
            "updated": "2023-11-24T08:27:01Z",
            "published": "2023-10-31T09:23:00Z",
            "summary": "Knowledge of the neutron light-yield response is crucial to the understanding\nof scintillator-based neutron detectors. In this work, neutrons from 2--6 MeV\nhave been used to study the scintillation light-yield response of the liquid\nscintillators NE 213A, EJ 305, EJ 331 and EJ 321P using event-by-event waveform\ndigitization. Energy calibration was performed using a GEANT model to locate\nthe edge positions of the Compton distributions produced by gamma-ray sources.\nThe simulated light yield for neutrons from a PuBe source was compared to\nmeasured recoil proton distributions, where neutron energy was selected by\ntime-of-flight. This resulted in an energy-dependent Birks parametrization to\ncharacterize the non-linear response to the lower energy neutrons. The NE 213A\nand EJ 305 results agree very well with existing data and are reproduced nicely\nby the simulation. New results for EJ 331 and EJ 321P, where the simulation\nalso reproduces the data well, are presented.",
            "author": [
                "N. Mauritzson",
                "K. G. Fissum",
                "J. R. M. Annand",
                "H. Perrey",
                "R. Al Jebali",
                "A. Backis",
                "R. Hall-Wilton",
                "K. Kanaki",
                "V. Maulerova-Subert",
                "F. Messi",
                "R. J. W. Frost",
                "E. Rofors",
                "J. Scherzinger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00725v2",
                "http://arxiv.org/pdf/2311.00725v2"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20291v1",
            "title": "Translation algorithms for graph covers",
            "updated": "2023-10-31T09:03:31Z",
            "published": "2023-10-31T09:03:31Z",
            "summary": "Graph covers are a way to describe continuous maps (and homeomorphisms) of\nthe Cantor set, more generally than e.g.\\ Bratteli-Vershik systems.\n  Every continuous map on a zero-dimensional compact set can be expressed by a\ngraph cover (e.g.\\ non-minimality or aperiodicty are no restrictions).\n  We give a survey on the construction, properties and some special cases of\ngraph covers.",
            "author": [
                "Jan Boro\u0144ski",
                "Henk Bruin",
                "Przemys\u0142aw Kucharski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20291v1",
                "http://arxiv.org/pdf/2310.20291v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "37B05, 57H20, 37B10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20274v1",
            "title": "Extracting Entities of Interest from Comparative Product Reviews",
            "updated": "2023-10-31T08:43:11Z",
            "published": "2023-10-31T08:43:11Z",
            "summary": "This paper presents a deep learning based approach to extract product\ncomparison information out of user reviews on various e-commerce websites. Any\ncomparative product review has three major entities of information: the names\nof the products being compared, the user opinion (predicate) and the feature or\naspect under comparison. All these informing entities are dependent on each\nother and bound by the rules of the language, in the review. We observe that\ntheir inter-dependencies can be captured well using LSTMs. We evaluate our\nsystem on existing manually labeled datasets and observe out-performance over\nthe existing Semantic Role Labeling (SRL) framework popular for this task.",
            "author": [
                "Jatin Arora",
                "Sumit Agrawal",
                "Pawan Goyal",
                "Sayan Pathak"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3132847.3133141",
                "http://arxiv.org/abs/2310.20274v1",
                "http://arxiv.org/pdf/2310.20274v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL",
                "cs.LG",
                "I.2.7; H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20268v1",
            "title": "Constructing Sample-to-Class Graph for Few-Shot Class-Incremental\n  Learning",
            "updated": "2023-10-31T08:38:14Z",
            "published": "2023-10-31T08:38:14Z",
            "summary": "Few-shot class-incremental learning (FSCIL) aims to build machine learning\nmodel that can continually learn new concepts from a few data samples, without\nforgetting knowledge of old classes.\n  The challenges of FSCIL lies in the limited data of new classes, which not\nonly lead to significant overfitting issues but also exacerbates the notorious\ncatastrophic forgetting problems. As proved in early studies, building sample\nrelationships is beneficial for learning from few-shot samples. In this paper,\nwe promote the idea to the incremental scenario, and propose a Sample-to-Class\n(S2C) graph learning method for FSCIL.\n  Specifically, we propose a Sample-level Graph Network (SGN) that focuses on\nanalyzing sample relationships within a single session. This network helps\naggregate similar samples, ultimately leading to the extraction of more refined\nclass-level features.\n  Then, we present a Class-level Graph Network (CGN) that establishes\nconnections across class-level features of both new and old classes. This\nnetwork plays a crucial role in linking the knowledge between different\nsessions and helps improve overall learning in the FSCIL scenario. Moreover, we\ndesign a multi-stage strategy for training S2C model, which mitigates the\ntraining challenges posed by limited data in the incremental process.\n  The multi-stage training strategy is designed to build S2C graph from base to\nfew-shot stages, and improve the capacity via an extra pseudo-incremental\nstage. Experiments on three popular benchmark datasets show that our method\nclearly outperforms the baselines and sets new state-of-the-art results in\nFSCIL.",
            "author": [
                "Fuyuan Hu",
                "Jian Zhang",
                "Fan Lyu",
                "Linyan Li",
                "Fenglei Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20268v1",
                "http://arxiv.org/pdf/2310.20268v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20260v1",
            "title": "Learning to Play Chess from Textbooks (LEAP): a Corpus for Evaluating\n  Chess Moves based on Sentiment Analysis",
            "updated": "2023-10-31T08:26:02Z",
            "published": "2023-10-31T08:26:02Z",
            "summary": "Learning chess strategies has been investigated widely, with most studies\nfocussing on learning from previous games using search algorithms. Chess\ntextbooks encapsulate grandmaster knowledge, explain playing strategies and\nrequire a smaller search space compared to traditional chess agents. This paper\nexamines chess textbooks as a new knowledge source for enabling machines to\nlearn how to play chess -- a resource that has not been explored previously. We\ndeveloped the LEAP corpus, a first and new heterogeneous dataset with\nstructured (chess move notations and board states) and unstructured data\n(textual descriptions) collected from a chess textbook containing 1164\nsentences discussing strategic moves from 91 games. We firstly labelled the\nsentences based on their relevance, i.e., whether they are discussing a move.\nEach relevant sentence was then labelled according to its sentiment towards the\ndescribed move. We performed empirical experiments that assess the performance\nof various transformer-based baseline models for sentiment analysis. Our\nresults demonstrate the feasibility of employing transformer-based sentiment\nanalysis models for evaluating chess moves, with the best performing model\nobtaining a weighted micro F_1 score of 68%. Finally, we synthesised the LEAP\ncorpus to create a larger dataset, which can be used as a solution to the\nlimited textual resource in the chess domain.",
            "author": [
                "Haifa Alrdahi",
                "Riza Batista-Navarro"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20260v1",
                "http://arxiv.org/pdf/2310.20260v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20259v1",
            "title": "Persistence of sub-chain groups",
            "updated": "2023-10-31T08:25:27Z",
            "published": "2023-10-31T08:25:27Z",
            "summary": "In this work, we present a generalization of extended persistent homology to\nfiltrations of graded sub-groups by defining relative homology in this setting.\nOur work provides a more comprehensive and flexible approach to get an\nalgebraic invariant overcoming the limitations of the standard approach. The\nmain contribution of our work is the development of a stability theorem for\nextended persistence modules using an extension of the definition of\ninterleaving and the rectangle measure. This stability theorem is a crucial\nproperty for the application of mathematical tools in data analysis. We apply\nthe stability theorem to extended persistence modules obtained from extended\npath homology of directed graphs and extended homology of hypergraphs, which\nare two important examples in topological data analysis.",
            "author": [
                "Fang Sun",
                "Shengwen Xie",
                "Xuezhi Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20259v1",
                "http://arxiv.org/pdf/2310.20259v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20253v1",
            "title": "Cut elimination for Zermelo set theory",
            "updated": "2023-10-31T08:14:24Z",
            "published": "2023-10-31T08:14:24Z",
            "summary": "We show how to express intuitionistic Zermelo set theory in deduction modulo\n(i.e. by replacing its axioms by rewrite rules) in such a way that the\ncorresponding notion of proof enjoys the normalization property. To do so, we\nfirst rephrase set theory as a theory of pointed graphs (following a paradigm\ndue to P. Aczel) by interpreting set-theoretic equality as bisimilarity, and\nshow that in this setting, Zermelo's axioms can be decomposed into\ngraph-theoretic primitives that can be turned into rewrite rules. We then show\nthat the theory we obtain in deduction modulo is a conservative extension of (a\nminor extension of) Zermelo set theory. Finally, we prove the normalization of\nthe intuitionistic fragment of the theory.",
            "author": [
                "Gilles Dowek",
                "Alexandre Miquel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20253v1",
                "http://arxiv.org/pdf/2310.20253v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20250v1",
            "title": "Diversified Node Sampling based Hierarchical Transformer Pooling for\n  Graph Representation Learning",
            "updated": "2023-10-31T08:13:21Z",
            "published": "2023-10-31T08:13:21Z",
            "summary": "Graph pooling methods have been widely used on downsampling graphs, achieving\nimpressive results on multiple graph-level tasks like graph classification and\ngraph generation. An important line called node dropping pooling aims at\nexploiting learnable scoring functions to drop nodes with comparatively lower\nsignificance scores. However, existing node dropping methods suffer from two\nlimitations: (1) for each pooled node, these models struggle to capture\nlong-range dependencies since they mainly take GNNs as the backbones; (2)\npooling only the highest-scoring nodes tends to preserve similar nodes, thus\ndiscarding the affluent information of low-scoring nodes. To address these\nissues, we propose a Graph Transformer Pooling method termed GTPool, which\nintroduces Transformer to node dropping pooling to efficiently capture\nlong-range pairwise interactions and meanwhile sample nodes diversely.\nSpecifically, we design a scoring module based on the self-attention mechanism\nthat takes both global context and local context into consideration, measuring\nthe importance of nodes more comprehensively. GTPool further utilizes a\ndiversified sampling method named Roulette Wheel Sampling (RWS) that is able to\nflexibly preserve nodes across different scoring intervals instead of only\nhigher scoring nodes. In this way, GTPool could effectively obtain long-range\ninformation and select more representative nodes. Extensive experiments on 11\nbenchmark datasets demonstrate the superiority of GTPool over existing popular\ngraph pooling methods.",
            "author": [
                "Gaichao Li",
                "Jinsong Chen",
                "John E. Hopcroft",
                "Kun He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20250v1",
                "http://arxiv.org/pdf/2310.20250v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20245v1",
            "title": "Finding a Maximum Restricted $t$-Matching via Boolean Edge-CSP",
            "updated": "2023-10-31T08:05:34Z",
            "published": "2023-10-31T08:05:34Z",
            "summary": "The problem of finding a maximum $2$-matching without short cycles has\nreceived significant attention due to its relevance to the Hamilton cycle\nproblem. This problem is generalized to finding a maximum $t$-matching which\nexcludes specified complete $t$-partite subgraphs, where $t$ is a fixed\npositive integer. The polynomial solvability of this generalized problem\nremains an open question. In this paper, we present polynomial-time algorithms\nfor the following two cases of this problem: in the first case the forbidden\ncomplete $t$-partite subgraphs are edge-disjoint; and in the second case the\nmaximum degree of the input graph is at most $2t-1$. Our result for the first\ncase extends the previous work of Nam (1994) showing the polynomial solvability\nof the problem of finding a maximum $2$-matching without cycles of length four,\nwhere the cycles of length four are vertex-disjoint. The second result expands\nupon the works of B\\'{e}rczi and V\\'{e}gh (2010) and Kobayashi and Yin (2012),\nwhich focused on graphs with maximum degree at most $t+1$. Our algorithms are\nobtained from exploiting the discrete structure of restricted $t$-matchings and\nemploying an algorithm for the Boolean edge-CSP.",
            "author": [
                "Yuni Iwamasa",
                "Yusuke Kobayashi",
                "Kenjiro Takazawa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20245v1",
                "http://arxiv.org/pdf/2310.20245v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08415v1",
            "title": "Scanning phase imaging without accurate positioning system",
            "updated": "2023-10-31T07:58:10Z",
            "published": "2023-10-31T07:58:10Z",
            "summary": "Ptychography, a high-resolution phase imaging technique using precise\nin-plane translation information, has been widely applied in modern synchrotron\nradiation sources across the globe. A key requirement for successful\nptychographic reconstruction is the precise knowledge of the scanning\npositions, which are typically obtained by a physical interferometric\npositioning system. Whereas high-throughput positioning poses a challenge in\nengineering, especially in nano or even smaller scale. In this work, we propose\na novel scanning imaging framework that does not require any prior position\ninformation from the positioning system. Specifically, our scheme utilizes the\nwavefront modulation mechanism to reconstruct the object functions at each scan\nposition and the shared illumination function, simultaneously. The scanning\ntrajectory information is extracted by our subpixel image registration\nalgorithm from the overlap region of reconstructed object functions. Then, a\ncompleted object function can be obtained by assembling each part of the\nreconstructed sample functions. High-quality imaging of biological sample and\nposition recovery with sub-pixel accuracy are demonstrated in proof-of-concept\nexperiment. Based on current results, we find it may have great potential\napplications in high-resolution and high throughput phase imaging.",
            "author": [
                "Tao Liu",
                "Bingyang Wang",
                "JiangTao Zhao",
                "Fu rong Chen",
                "Fucai Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08415v1",
                "http://arxiv.org/pdf/2311.08415v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20237v1",
            "title": "The Toll Walk Transit Function of a Graph: Axiomatic Characterizations\n  and First-Order Non-definability",
            "updated": "2023-10-31T07:42:12Z",
            "published": "2023-10-31T07:42:12Z",
            "summary": "A walk $W=w_1w_2\\dots w_k$, $k\\geq 2$, is called a toll walk if $w_1\\neq w_k$\nand $w_2$ and $w_{k-1}$ are the only neighbors of $w_1$ and $w_k$,\nrespectively, on $W$ in a graph $G$. A toll walk interval $T(u,v)$, $u,v\\in\nV(G)$, contains all the vertices that belong to a toll walk between $u$ and\n$v$. The toll walk intervals yield a toll walk transit function $T:V(G)\\times\nV(G)\\rightarrow 2^{V(G)}$. We represent several axioms that characterize the\ntoll walk transit function among chordal graphs, trees, asteroidal triple-free\ngraphs, Ptolemaic graphs, and distance hereditary graphs. We also show that the\ntoll walk transit function can not be described in the language of first-order\nlogic for an arbitrary graph.",
            "author": [
                "Manoj Changat",
                "Jeny Jacob",
                "Lekshmi Kamal K. Sheela",
                "Iztok Peterin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20237v1",
                "http://arxiv.org/pdf/2310.20237v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C38"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20235v1",
            "title": "Powers of generalized binomial edge ideals of path graphs",
            "updated": "2023-10-31T07:36:31Z",
            "published": "2023-10-31T07:36:31Z",
            "summary": "In this article, we study the powers of the generalized binomial edge ideal\n$\\mathcal{J}_{K_m,P_n}$ of a path graph $P_n$. We explicitly compute their\nregularities and determine the limit of their depths. We also show that these\nordinary powers coincide with their symbolic powers. Additionally, we study the\nRees algebra and the special fiber ring of $\\mathcal{J}_{K_m,P_n}$ via Sagbi\nbasis theory. In particular, we obtain exact formulas for the regularity of\nthese blowup algebras.",
            "author": [
                "Yi-Huang Shen",
                "Guangjun Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20235v1",
                "http://arxiv.org/pdf/2310.20235v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "Primary 13C15, 13P10, Secondary 05E40, 13F20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20230v1",
            "title": "On the eigenvalues and Seidel eigenvalues of chain graphs",
            "updated": "2023-10-31T07:16:53Z",
            "published": "2023-10-31T07:16:53Z",
            "summary": "In this paper we consider the eigenvalues and the Seidel eigenvalues of a\nchain graph. An$\\dbar$eli\\'{c}, da Fonseca, Simi\\'{c}, and Du\n\\cite{andelic2020tridiagonal} conjectured that there do not exist\nnon-isomorphic cospectral chain graphs with respect to the adjacency spectrum.\nHere we disprove this conjecture. Furthermore, by considering the relation\nbetween the Seidel matrix and the adjacency matrix of a graph, we solve two\nproblems on the number of distinct Seidel eigenvalues of a chain graph, which\nwas posed by Mandal, Mehatari, and Das \\cite{mandal2022spectrum}.",
            "author": [
                "Zhuang Xiong",
                "Yaoping Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20230v1",
                "http://arxiv.org/pdf/2310.20230v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20224v1",
            "title": "Choose A Table: Tensor Dirichlet Process Multinomial Mixture Model with\n  Graphs for Passenger Trajectory Clustering",
            "updated": "2023-10-31T06:53:04Z",
            "published": "2023-10-31T06:53:04Z",
            "summary": "Passenger clustering based on trajectory records is essential for\ntransportation operators. However, existing methods cannot easily cluster the\npassengers due to the hierarchical structure of the passenger trip information,\nincluding multiple trips within each passenger and multi-dimensional\ninformation about each trip. Furthermore, existing approaches rely on an\naccurate specification of the clustering number to start. Finally, existing\nmethods do not consider spatial semantic graphs such as geographical proximity\nand functional similarity between the locations. In this paper, we propose a\nnovel tensor Dirichlet Process Multinomial Mixture model with graphs, which can\npreserve the hierarchical structure of the multi-dimensional trip information\nand cluster them in a unified one-step manner with the ability to determine the\nnumber of clusters automatically. The spatial graphs are utilized in community\ndetection to link the semantic neighbors. We further propose a tensor version\nof Collapsed Gibbs Sampling method with a minimum cluster size requirement. A\ncase study based on Hong Kong metro passenger data is conducted to demonstrate\nthe automatic process of cluster amount evolution and better cluster quality\nmeasured by within-cluster compactness and cross-cluster separateness. The code\nis available at https://github.com/bonaldli/TensorDPMM-G.",
            "author": [
                "Ziyue Li",
                "Hao Yan",
                "Chen Zhang",
                "Lijun Sun",
                "Wolfgang Ketter",
                "Fugee Tsung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20224v1",
                "http://arxiv.org/pdf/2310.20224v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.AI",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20223v1",
            "title": "STDA-Meta: A Meta-Learning Framework for Few-Shot Traffic Prediction",
            "updated": "2023-10-31T06:52:56Z",
            "published": "2023-10-31T06:52:56Z",
            "summary": "As the development of cities, traffic congestion becomes an increasingly\npressing issue, and traffic prediction is a classic method to relieve that\nissue. Traffic prediction is one specific application of spatio-temporal\nprediction learning, like taxi scheduling, weather prediction, and ship\ntrajectory prediction. Against these problems, classical spatio-temporal\nprediction learning methods including deep learning, require large amounts of\ntraining data. In reality, some newly developed cities with insufficient\nsensors would not hold that assumption, and the data scarcity makes predictive\nperformance worse. In such situation, the learning method on insufficient data\nis known as few-shot learning (FSL), and the FSL of traffic prediction remains\nchallenges. On the one hand, graph structures' irregularity and dynamic nature\nof graphs cannot hold the performance of spatio-temporal learning method. On\nthe other hand, conventional domain adaptation methods cannot work well on\ninsufficient training data, when transferring knowledge from different domains\nto the intended target domain.To address these challenges, we propose a novel\nspatio-temporal domain adaptation (STDA) method that learns transferable\nspatio-temporal meta-knowledge from data-sufficient cities in an adversarial\nmanner. This learned meta-knowledge can improve the prediction performance of\ndata-scarce cities. Specifically, we train the STDA model using a\nModel-Agnostic Meta-Learning (MAML) based episode learning process, which is a\nmodel-agnostic meta-learning framework that enables the model to solve new\nlearning tasks using only a small number of training samples. We conduct\nnumerous experiments on four traffic prediction datasets, and our results show\nthat the prediction performance of our model has improved by 7\\% compared to\nbaseline models on the two metrics of MAE and RMSE.",
            "author": [
                "Maoxiang Sun",
                "Weilong Ding",
                "Tianpu Zhang",
                "Zijian Liu",
                "Mengda Xing"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20223v1",
                "http://arxiv.org/pdf/2310.20223v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20207v1",
            "title": "Supersymmetric QCD on the Lattice: Fine-Tuning of the Yukawa Couplings",
            "updated": "2023-10-31T06:08:03Z",
            "published": "2023-10-31T06:08:03Z",
            "summary": "We determine the fine-tuning of the Yukawa couplings of supersymmetric QCD,\ndiscretized on a lattice. We use perturbation theory at one-loop level. The\nModified Minimal Subtraction scheme ($\\overline{{\\rm MS}}$) is employed; by its\ndefinition, this scheme requires perturbative calculations, in the continuum\nand/or on the lattice. On the lattice, we utilize the Wilson formulation for\ngluon, quark and gluino fields; for squark fields we use na\\\"ive\ndiscretization. The sheer difficulties of this study lie in the fact that\ndifferent components of squark fields mix among themselves at the quantum level\nand the action's symmetries, such as parity and charge conjugation, allow an\nadditional Yukawa coupling. Consequently, for an appropriate fine-tuning of the\nYukawa terms, these mixings must be taken into account in the renormalization\nconditions. All Green's functions and renormalization factors are analytic\nexpressions depending on the number of colors, $N_c$, the number of flavors,\n$N_f$, and the gauge parameter, $\\alpha$, which are left unspecified. Knowledge\nof these renormalization factors is necessary in order to relate numerical\nresults, coming from nonperturbative studies, to the renormalized, ``physical\"\nGreen's functions of the theory.",
            "author": [
                "Marios Costa",
                "Herodotos Herodotou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20207v1",
                "http://arxiv.org/pdf/2310.20207v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20191v1",
            "title": "Subspace Correction for Constraints",
            "updated": "2023-10-31T05:23:50Z",
            "published": "2023-10-31T05:23:50Z",
            "summary": "We demonstrate that it is possible to construct operators that stabilize the\nconstraint-satisfying subspaces of computational problems in their Ising\nrepresentations. We provide an explicit recipe to construct unitaries and\nassociated measurements for some such constraints. The stabilizer measurements\nallow the detection of constraint violations, and provide a route to recovery\nback into the constrained subspace. We call this technique ``subspace\ncorrection\". As an example, we explicitly investigate the stabilizers using the\nsimplest local constraint subspace: Independent Set. We find an algorithm that\nis guaranteed to produce a perfect uniform or weighted distribution over all\nconstraint-satisfying states when paired with a stopping condition: a quantum\nanalogue of partial rejection sampling. The stopping condition can be modified\nfor sub-graph approximations. We show that it can prepare exact Gibbs\ndistributions on $d-$regular graphs below a critical hardness $\\lambda_d^*$ in\nsub-linear time. Finally, we look at a potential use of subspace correction for\nfault-tolerant depth-reduction. In particular we investigate how the technique\ndetects and recovers errors induced by Trotterization in preparing maximum\nindependent set using an adiabatic state preparation algorithm.",
            "author": [
                "Kelly Ann Pawlak",
                "Jeffrey M. Epstein",
                "Daniel Crow",
                "Srilekha Gandhari",
                "Ming Li",
                "Thomas C. Bohdanowicz",
                "Jonathan King"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20191v1",
                "http://arxiv.org/pdf/2310.20191v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20184v2",
            "title": "Graph Theoretic Approach Identifies Critical Thresholds at which Galaxy\n  Filamentary Structures Form",
            "updated": "2023-11-20T11:08:16Z",
            "published": "2023-10-31T05:09:18Z",
            "summary": "We describe a graph theoretical technique for assessing intrinsic patterns in\npoint data sets. A unique construction, the minimal spanning tree, can be\nassociated with any point data set given all the inter-point separations. This\nconstruction enables the skeletal pattern of galaxy clustering to be singled\nout in quantitative fashion and differs from other statistics applied to these\ndata sets. We describe and apply this technique to two- and three-dimensional\ndistributions of galaxies and also to comparable random samples and numerical\nsimulations. The observed CfA and Zwicky data exhibit characteristic\ndistributions of edge-lengths in their minimal spanning trees which are\ndistinct from those found in random samples. These statistics are also\nre-evaluated after normalizing to account for the level of clustering in the\nsamples.",
            "author": [
                "Sophia-Gisela Strey",
                "Alexander Castronovo",
                "Kailash Elumalai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20184v2",
                "http://arxiv.org/pdf/2310.20184v2"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20174v2",
            "title": "GraphTransformers for Geospatial Forecasting of Hurricane Trajectories",
            "updated": "2023-11-26T17:53:23Z",
            "published": "2023-10-31T04:53:10Z",
            "summary": "In this paper we introduce a novel framework for trajectory prediction of\ngeospatial sequences using GraphTransformers. When viewed across several\nsequences, we observed that a graph structure automatically emerges between\ndifferent geospatial points that is often not taken into account for such\nsequence modeling tasks. We show that by leveraging this graph structure\nexplicitly, geospatial trajectory prediction can be significantly improved. Our\nGraphTransformer approach improves upon state-of-the-art Transformer based\nbaseline significantly on HURDAT, a dataset where we are interested in\npredicting the trajectory of a hurricane on a 6 hourly basis.",
            "author": [
                "Pallavi Banerjee",
                "Satyaki Chakraborty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20174v2",
                "http://arxiv.org/pdf/2310.20174v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20170v1",
            "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain\n  Question Answering over Knowledge Base and Text",
            "updated": "2023-10-31T04:37:57Z",
            "published": "2023-10-31T04:37:57Z",
            "summary": "Large Language Models (LLMs) have exhibited impressive generation\ncapabilities, but they suffer from hallucinations when solely relying on their\ninternal knowledge, especially when answering questions that require less\ncommonly known information. Retrieval-augmented LLMs have emerged as a\npotential solution to ground LLMs in external knowledge. Nonetheless, recent\napproaches have primarily emphasized retrieval from unstructured text corpora,\nowing to its seamless integration into prompts. When using structured data such\nas knowledge graphs, most methods simplify it into natural text, neglecting the\nunderlying structures. Moreover, a significant gap in the current landscape is\nthe absence of a realistic benchmark for evaluating the effectiveness of\ngrounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and\ntext). To fill this gap, we have curated a comprehensive dataset that poses two\nunique challenges: (1) Two-hop multi-source questions that require retrieving\ninformation from both open-domain structured and unstructured knowledge\nsources; retrieving information from structured knowledge sources is a critical\ncomponent in correctly answering the questions. (2) The generation of symbolic\nqueries (e.g., SPARQL for Wikidata) is a key requirement, which adds another\nlayer of challenge. Our dataset is created using a combination of automatic\ngeneration through predefined reasoning chains and human annotation. We also\nintroduce a novel approach that leverages multiple retrieval tools, including\ntext passage retrieval and symbolic language-assisted retrieval. Our model\noutperforms previous approaches by a significant margin, demonstrating its\neffectiveness in addressing the above-mentioned reasoning challenges.",
            "author": [
                "Wenting Zhao",
                "Ye Liu",
                "Tong Niu",
                "Yao Wan",
                "Philip S. Yu",
                "Shafiq Joty",
                "Yingbo Zhou",
                "Semih Yavuz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20170v1",
                "http://arxiv.org/pdf/2310.20170v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20163v1",
            "title": "A Perturbative Solution to the Linear Influence/Network Autocorrelation\n  Model Under Network Dynamics",
            "updated": "2023-10-31T04:18:19Z",
            "published": "2023-10-31T04:18:19Z",
            "summary": "Known by many names and arising in many settings, the forced linear diffusion\nmodel is central to the modeling of power and influence within social networks\n(while also serving as the mechanistic justification for the widely used\nspatial/network autocorrelation models). The standard equilibrium solution to\nthe diffusion model depends on strict timescale separation between network\ndynamics and attribute dynamics, such that the diffusion network can be\nconsidered fixed with respect to the diffusion process. Here, we consider a\nrelaxation of this assumption, in which the network changes only slowly\nrelative to the diffusion dynamics. In this case, we show that one can obtain a\nperturbative solution to the diffusion model, which depends on knowledge of\npast states in only a minimal way.",
            "author": [
                "Carter T. Butts"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20163v1",
                "http://arxiv.org/pdf/2310.20163v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20159v1",
            "title": "Language Guided Visual Question Answering: Elevate Your Multimodal\n  Language Model Using Knowledge-Enriched Prompts",
            "updated": "2023-10-31T03:54:11Z",
            "published": "2023-10-31T03:54:11Z",
            "summary": "Visual question answering (VQA) is the task of answering questions about an\nimage. The task assumes an understanding of both the image and the question to\nprovide a natural language answer. VQA has gained popularity in recent years\ndue to its potential applications in a wide range of fields, including\nrobotics, education, and healthcare. In this paper, we focus on\nknowledge-augmented VQA, where answering the question requires commonsense\nknowledge, world knowledge, and reasoning about ideas and concepts not present\nin the image. We propose a multimodal framework that uses language guidance\n(LG) in the form of rationales, image captions, scene graphs, etc to answer\nquestions more accurately. We benchmark our method on the multi-choice\nquestion-answering task of the A-OKVQA, Science-QA, VSR, and IconQA datasets\nusing CLIP and BLIP models. We show that the use of language guidance is a\nsimple but powerful and effective strategy for visual question answering. Our\nlanguage guidance improves the performance of CLIP by 7.6% and BLIP-2 by 4.8%\nin the challenging A-OKVQA dataset. We also observe consistent improvement in\nperformance on the Science-QA, VSR, and IconQA datasets when using the proposed\nlanguage guidances. The implementation of LG-VQA is publicly available at\nhttps:// github.com/declare-lab/LG-VQA.",
            "author": [
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Roy Ka-Wei Lee",
                "Rada Mihalcea",
                "Soujanya Poria"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20159v1",
                "http://arxiv.org/pdf/2310.20159v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20154v2",
            "title": "A Quantum Optimization Method for Geometric Constrained Image\n  Segmentation",
            "updated": "2023-11-06T21:16:40Z",
            "published": "2023-10-31T03:41:21Z",
            "summary": "Quantum image processing is a growing field attracting attention from both\nthe quantum computing and image processing communities. We propose a novel\nmethod in combining a graph-theoretic approach for optimal surface segmentation\nand hybrid quantum-classical optimization of the problem-directed graph. The\nsurface segmentation is modeled classically as a graph partitioning problem in\nwhich a smoothness constraint is imposed to control surface variation for\nrealistic segmentation. Specifically, segmentation refers to a source set\nidentified by a minimum s-t cut that divides graph nodes into the source (s)\nand sink (t) sets. The resulting surface consists of graph nodes located on the\nboundary between the source and the sink. Characteristics of the\nproblem-specific graph, including its directed edges, connectivity, and edge\ncapacities, are embedded in a quadratic objective function whose minimum value\ncorresponds to the ground state energy of an equivalent Ising Hamiltonian. This\nwork explores the use of quantum processors in image segmentation problems,\nwhich has important applications in medical image analysis. Here, we present a\ntheoretical basis for the quantum implementation of LOGISMOS and the results of\na simulation study on simple images. Quantum Approximate Optimization Algorithm\n(QAOA) approach was utilized to conduct two simulation studies whose objective\nwas to determine the ground state energies and identify bitstring solutions\nthat encode the optimal segmentation of objective functions. The objective\nfunction encodes tasks associated with surface segmentation in 2-D and 3-D\nimages while incorporating a smoothness constraint. In this work, we\ndemonstrate that the proposed approach can solve the geometric-constrained\nsurface segmentation problem optimally with the capability of locating multiple\nminimum points corresponding to the globally minimal solution.",
            "author": [
                "Nam H. Le",
                "Milan Sonka",
                "Fatima Toor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20154v2",
                "http://arxiv.org/pdf/2310.20154v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20153v1",
            "title": "Interactive Multi-fidelity Learning for Cost-effective Adaptation of\n  Language Model with Sparse Human Supervision",
            "updated": "2023-10-31T03:39:23Z",
            "published": "2023-10-31T03:39:23Z",
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nvarious tasks. However, their suitability for domain-specific tasks, is limited\ndue to their immense scale at deployment, susceptibility to misinformation, and\nmore importantly, high data annotation costs. We propose a novel Interactive\nMulti-Fidelity Learning (IMFL) framework for the cost-effective development of\nsmall domain-specific LMs under limited annotation budgets. Our approach\nformulates the domain-specific fine-tuning process as a multi-fidelity learning\nproblem, focusing on identifying the optimal acquisition strategy that balances\nbetween low-fidelity automatic LLM annotations and high-fidelity human\nannotations to maximize model performance. We further propose an\nexploration-exploitation query strategy that enhances annotation diversity and\ninformativeness, incorporating two innovative designs: 1) prompt retrieval that\nselects in-context examples from human-annotated samples to improve LLM\nannotation, and 2) variable batch size that controls the order for choosing\neach fidelity to facilitate knowledge distillation, ultimately enhancing\nannotation quality. Extensive experiments on financial and medical tasks\ndemonstrate that IMFL achieves superior performance compared with single\nfidelity annotations. Given a limited budget of human annotation, IMFL\nsignificantly outperforms the human annotation baselines in all four tasks and\nachieves very close performance as human annotations on two of the tasks. These\npromising results suggest that the high human annotation costs in\ndomain-specific tasks can be significantly reduced by employing IMFL, which\nutilizes fewer human annotations, supplemented with cheaper and faster LLM\n(e.g., GPT-3.5) annotations to achieve comparable performance.",
            "author": [
                "Jiaxin Zhang",
                "Zhuohang Li",
                "Kamalika Das",
                "Sricharan Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20153v1",
                "http://arxiv.org/pdf/2310.20153v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20138v2",
            "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language\n  Models",
            "updated": "2023-12-05T16:14:24Z",
            "published": "2023-10-31T03:09:36Z",
            "summary": "Large language models pretrained on a huge amount of data capture rich\nknowledge and information in the training data. The ability of data\nmemorization and regurgitation in pretrained language models, revealed in\nprevious studies, brings the risk of data leakage. In order to effectively\nreduce these risks, we propose a framework DEPN to Detect and Edit Privacy\nNeurons in pretrained language models, partially inspired by knowledge neurons\nand model editing. In DEPN, we introduce a novel method, termed as privacy\nneuron detector, to locate neurons associated with private information, and\nthen edit these detected privacy neurons by setting their activations to zero.\nFurthermore, we propose a privacy neuron aggregator dememorize private\ninformation in a batch processing manner. Experimental results show that our\nmethod can significantly and efficiently reduce the exposure of private data\nleakage without deteriorating the performance of the model. Additionally, we\nempirically demonstrate the relationship between model memorization and privacy\nneurons, from multiple perspectives, including model size, training time,\nprompts, privacy neuron distribution, illustrating the robustness of our\napproach.",
            "author": [
                "Xinwei Wu",
                "Junzhuo Li",
                "Minghui Xu",
                "Weilong Dong",
                "Shuangzhi Wu",
                "Chao Bian",
                "Deyi Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20138v2",
                "http://arxiv.org/pdf/2310.20138v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20132v1",
            "title": "Linear codes with few weights from non-weakly regular plateaued\n  functions",
            "updated": "2023-10-31T02:52:27Z",
            "published": "2023-10-31T02:52:27Z",
            "summary": "Linear codes with few weights have significant applications in secret sharing\nschemes, authentication codes, association schemes, and strongly regular\ngraphs. There are a number of methods to construct linear codes, one of which\nis based on functions. Furthermore, two generic constructions of linear codes\nfrom functions called the first and the second generic constructions, have\naroused the research interest of scholars. Recently, in \\cite{Nian}, Li and\nMesnager proposed two open problems: Based on the first and the second generic\nconstructions, respectively, construct linear codes from non-weakly regular\nplateaued functions and determine their weight distributions.\n  Motivated by these two open problems, in this paper, firstly, based on the\nfirst generic construction, we construct some three-weight and at most\nfive-weight linear codes from non-weakly regular plateaued functions and\ndetermine the weight distributions of the constructed codes. Next, based on the\nsecond generic construction, we construct some three-weight and at most\nfive-weight linear codes from non-weakly regular plateaued functions belonging\nto $\\mathcal{NWRF}$ (defined in this paper) and determine the weight\ndistributions of the constructed codes. We also give the punctured codes of\nthese codes obtained based on the second generic construction and determine\ntheir weight distributions. Meanwhile, we obtain some optimal and almost\noptimal linear codes. Besides, by the Ashikhmin-Barg condition, we have that\nthe constructed codes are minimal for almost all cases and obtain some secret\nsharing schemes with nice access structures based on their dual codes.",
            "author": [
                "Yadi Wei",
                "Jiaxin Wang",
                "Fang-Wei Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20132v1",
                "http://arxiv.org/pdf/2310.20132v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20121v1",
            "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula",
            "updated": "2023-10-31T01:44:33Z",
            "published": "2023-10-31T01:44:33Z",
            "summary": "We employ a characterization of linguistic complexity from psycholinguistic\nand language acquisition research to develop data-driven curricula to\nunderstand the underlying linguistic knowledge that models learn to address NLP\ntasks. The novelty of our approach is in the development of linguistic\ncurricula derived from data, existing knowledge about linguistic complexity,\nand model behavior during training. By analyzing several benchmark NLP\ndatasets, our curriculum learning approaches identify sets of linguistic\nmetrics (indices) that inform the challenges and reasoning required to address\neach task. Our work will inform future research in all NLP areas, allowing\nlinguistic complexity to be considered early in the research and development\nprocess. In addition, our work prompts an examination of gold standards and\nfair evaluation in NLP.",
            "author": [
                "Mohamed Elgaar",
                "Hadi Amiri"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20121v1",
                "http://arxiv.org/pdf/2310.20121v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20106v1",
            "title": "Progress and outlook on advanced fly scans based on Mamba",
            "updated": "2023-10-31T00:57:58Z",
            "published": "2023-10-31T00:57:58Z",
            "summary": "Development related to PandABox-based fly scans is an important part of the\nactive work on Mamba, the software framework for beamline experiments at the\nHigh Energy Photon Source (HEPS); presented in this paper is the progress of\nour development, and some outlook for advanced fly scans based on knowledge\nlearned during the process. By treating fly scans as a collaboration between a\nfew loosely coupled subsystems - motors / mechanics, detectors / data\nprocessing, sequencer devices like PandABox - systematic analyses of issues in\nfly scans are conducted. Interesting products of these analyses include a\ngeneral-purpose software-based fly-scan mechanism, a general way to design\nundulator-monochromator fly scans, a sketch of how to practically implement\nonline tuning of fly-scan behaviours based on processing of the data acquired,\nand many more. Based on the results above, an architectural discussion on\n>=10kHz fly scans is given.",
            "author": [
                "Peng-Cheng Li",
                "Cheng-Long Zhang",
                "Zong-Yang Yue",
                "Xiao-Bao Deng",
                "Chun Li",
                "Ai-Yu Zhou",
                "Gang Li",
                "Yu Liu",
                "Yi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20106v1",
                "http://arxiv.org/pdf/2310.20106v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20097v1",
            "title": "A note on the indivisibility of the Henson graphs",
            "updated": "2023-10-31T00:21:25Z",
            "published": "2023-10-31T00:21:25Z",
            "summary": "We show that in contrast to the Rado graph, the Henson graphs are not\ncomputably indivisible.",
            "author": [
                "Kenneth Gill"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20097v1",
                "http://arxiv.org/pdf/2310.20097v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.00007v2",
            "title": "Mathematics and the formal turn",
            "updated": "2023-11-04T14:44:47Z",
            "published": "2023-10-31T00:05:12Z",
            "summary": "Since the early twentieth century, it has been understood that mathematical\ndefinitions and proofs can be represented in formal systems systems with\nprecise grammars and rules of use. Building on such foundations, computational\nproof assistants now make it possible to encode mathematical knowledge in\ndigital form. This article enumerates some of the ways that these and related\ntechnologies can help us do mathematics.",
            "author": [
                "Jeremy Avigad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.00007v2",
                "http://arxiv.org/pdf/2311.00007v2"
            ],
            "primary_category": "math.HO",
            "category": [
                "math.HO",
                "math.LO",
                "03B35, 68V20 (Primary) 68Q60, 68V15, 68V25, 68V35, 68T01, 97U50\n  (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20083v1",
            "title": "Facial asymmetry: A Computer Vision based behaviometric index for\n  assessment during a face-to-face interview",
            "updated": "2023-10-30T23:42:08Z",
            "published": "2023-10-30T23:42:08Z",
            "summary": "Choosing the right person for the right job makes the personnel interview\nprocess a cognitively demanding task. Psychometric tests, followed by an\ninterview, have often been used to aid the process although such mechanisms\nhave their limitations. While psychometric tests suffer from faking or social\ndesirability of responses, the interview process depends on the way the\nresponses are analyzed by the interviewers. We propose the use of behaviometry\nas an assistive tool to facilitate an objective assessment of the interviewee\nwithout increasing the cognitive load of the interviewer. Behaviometry is a\nrelatively little explored field of study in the selection process, that\nutilizes inimitable behavioral characteristics like facial expressions,\nvocalization patterns, pupillary reactions, proximal behavior, body language,\netc. The method analyzes thin slices of behavior and provides unbiased\ninformation about the interviewee. The current study proposes the methodology\nbehind this tool to capture facial expressions, in terms of facial asymmetry\nand micro-expressions. Hemi-facial composites using a structural similarity\nindex was used to develop a progressive time graph of facial asymmetry, as a\ntest case. A frame-by-frame analysis was performed on three YouTube video\nsamples, where Structural similarity index (SSID) scores of 75% and more showed\nbehavioral congruence. The research utilizes open-source computer vision\nalgorithms and libraries (python-opencv and dlib) to formulate the procedure\nfor analysis of the facial asymmetry.",
            "author": [
                "Shuvam Keshari",
                "Tanusree Dutta",
                "Raju Mullick",
                "Ashish Rathor",
                "Priyadarshi Patnaik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20083v1",
                "http://arxiv.org/pdf/2310.20083v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20082v1",
            "title": "Efficient Subgraph GNNs by Learning Effective Selection Policies",
            "updated": "2023-10-30T23:41:05Z",
            "published": "2023-10-30T23:41:05Z",
            "summary": "Subgraph GNNs are provably expressive neural architectures that learn graph\nrepresentations from sets of subgraphs. Unfortunately, their applicability is\nhampered by the computational complexity associated with performing message\npassing on many subgraphs. In this paper, we consider the problem of learning\nto select a small subset of the large set of possible subgraphs in a\ndata-driven fashion. We first motivate the problem by proving that there are\nfamilies of WL-indistinguishable graphs for which there exist efficient\nsubgraph selection policies: small subsets of subgraphs that can already\nidentify all the graphs within the family. We then propose a new approach,\ncalled Policy-Learn, that learns how to select subgraphs in an iterative\nmanner. We prove that, unlike popular random policies and prior work addressing\nthe same problem, our architecture is able to learn the efficient policies\nmentioned above. Our experimental results demonstrate that Policy-Learn\noutperforms existing baselines across a wide range of datasets.",
            "author": [
                "Beatrice Bevilacqua",
                "Moshe Eliasof",
                "Eli Meirom",
                "Bruno Ribeiro",
                "Haggai Maron"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20082v1",
                "http://arxiv.org/pdf/2310.20082v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20078v1",
            "title": "TorchProbe: Fuzzing Dynamic Deep Learning Compilers",
            "updated": "2023-10-30T23:20:47Z",
            "published": "2023-10-30T23:20:47Z",
            "summary": "Static and dynamic computational graphs represent two distinct approaches to\nconstructing deep learning frameworks. The former prioritizes compiler-based\noptimizations, while the latter focuses on programmability and\nuser-friendliness. The recent release of PyTorch 2.0, which supports compiling\narbitrary deep learning programs in Python, signifies a new direction in the\nevolution of deep learning infrastructure to incorporate compiler techniques in\na more dynamic manner and support more dynamic language features like dynamic\ncontrol flows and closures. Given PyTorch's seamless integration with Python,\nits compiler aims to support arbitrary deep learning code written in Python.\nHowever, the inherent dynamism of Python poses challenges to the completeness\nand robustness of the compiler. While recent research has introduced fuzzing to\ntest deep learning compilers, there is still a lack of comprehensive analysis\non how to test dynamic features. To address this issue, we propose several code\ntransformations to generate test cases involving dynamic features. These\ntransformations preserve the program's semantics, ensuring that any discrepancy\nbetween the transformed and original programs indicates the presence of a bug.\nThrough our approach, we have successfully identified twenty previously unknown\nbugs in the PyTorch compiler and its underlying tensor compiler Triton.",
            "author": [
                "Qidong Su",
                "Chuqin Geng",
                "Gennady Pekhimenko",
                "Xujie Si"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20078v1",
                "http://arxiv.org/pdf/2310.20078v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20075v1",
            "title": "Meek Separators and Their Applications in Targeted Causal Discovery",
            "updated": "2023-10-30T23:15:27Z",
            "published": "2023-10-30T23:15:27Z",
            "summary": "Learning causal structures from interventional data is a fundamental problem\nwith broad applications across various fields. While many previous works have\nfocused on recovering the entire causal graph, in practice, there are scenarios\nwhere learning only part of the causal graph suffices. This is called\n$targeted$ causal discovery. In our work, we focus on two such well-motivated\nproblems: subset search and causal matching. We aim to minimize the number of\ninterventions in both cases.\n  Towards this, we introduce the $Meek~separator$, which is a subset of\nvertices that, when intervened, decomposes the remaining unoriented edges into\nsmaller connected components. We then present an efficient algorithm to find\nMeek separators that are of small sizes. Such a procedure is helpful in\ndesigning various divide-and-conquer-based approaches. In particular, we\npropose two randomized algorithms that achieve logarithmic approximation for\nsubset search and causal matching, respectively. Our results provide the first\nknown average-case provable guarantees for both problems. We believe that this\nopens up possibilities to design near-optimal methods for many other targeted\ncausal structure learning problems arising from various applications.",
            "author": [
                "Kirankumar Shiragur",
                "Jiaqi Zhang",
                "Caroline Uhler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20075v1",
                "http://arxiv.org/pdf/2310.20075v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DM",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20067v1",
            "title": "Vignat: Vulnerability identification by learning code semantics via\n  graph attention networks",
            "updated": "2023-10-30T22:31:38Z",
            "published": "2023-10-30T22:31:38Z",
            "summary": "Vulnerability identification is crucial to protect software systems from\nattacks for cyber-security. However, huge projects have more than millions of\nlines of code, and the complex dependencies make it hard to carry out\ntraditional static and dynamic methods. Furthermore, the semantic structure of\nvarious types of vulnerabilities differs greatly and may occur simultaneously,\nmaking general rule-based methods difficult to extend. In this paper, we\npropose \\textit{Vignat}, a novel attention-based framework for identifying\nvulnerabilities by learning graph-level semantic representations of code. We\nrepresent codes with code property graphs (CPGs) in fine grain and use graph\nattention networks (GATs) for vulnerability detection. The results show that\nVignat is able to achieve $57.38\\%$ accuracy on reliable datasets derived from\npopular C libraries. Furthermore, the interpretability of our GATs provides\nvaluable insights into vulnerability patterns.",
            "author": [
                "Shuo Liu",
                "Gail Kaiser"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20067v1",
                "http://arxiv.org/pdf/2310.20067v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20056v1",
            "title": "On the data-driven description of lattice materials mechanics",
            "updated": "2023-10-30T22:21:08Z",
            "published": "2023-10-30T22:21:08Z",
            "summary": "In the emerging field of mechanical metamaterials, using periodic lattice\nstructures as a primary ingredient is relatively frequent. However, the choice\nof aperiodic lattices in these structures presents unique advantages regarding\nfailure, e.g., buckling or fracture, because avoiding repeated patterns\nprevents global failures, with local failures occurring in turn that can\nbeneficially delay structural collapse. Therefore, it is expedient to develop\nmodels for computing efficiently the effective mechanical properties in\nlattices from different general features while addressing the challenge of\npresenting topologies (or graphs) of different sizes. In this paper, we develop\na deep learning model to predict energetically-equivalent mechanical properties\nof linear elastic lattices effectively. Considering the lattice as a graph and\ndefining material and geometrical features on such, we show that Graph Neural\nNetworks provide more accurate predictions than a dense, fully connected\nstrategy, thanks to the geometrically induced bias through graph\nrepresentation, closer to the underlying equilibrium laws from mechanics solved\nin the direct problem. Leveraging the efficient forward-evaluation of a vast\nnumber of lattices using this surrogate enables the inverse problem, i.e., to\nobtain a structure having prescribed specific behavior, which is ultimately\nsuitable for multiscale structural optimization problems.",
            "author": [
                "Ismael Ben-Yelun",
                "Luis Irastorza-Valera",
                "Luis Saucedo-Mora",
                "Francisco Javier Mont\u00e1ns",
                "Francisco Chinesta"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20056v1",
                "http://arxiv.org/pdf/2310.20056v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20052v1",
            "title": "Look At Me, No Replay! SurpriseNet: Anomaly Detection Inspired Class\n  Incremental Learning",
            "updated": "2023-10-30T22:16:26Z",
            "published": "2023-10-30T22:16:26Z",
            "summary": "Continual learning aims to create artificial neural networks capable of\naccumulating knowledge and skills through incremental training on a sequence of\ntasks. The main challenge of continual learning is catastrophic interference,\nwherein new knowledge overrides or interferes with past knowledge, leading to\nforgetting. An associated issue is the problem of learning \"cross-task\nknowledge,\" where models fail to acquire and retain knowledge that helps\ndifferentiate classes across task boundaries. A common solution to both\nproblems is \"replay,\" where a limited buffer of past instances is utilized to\nlearn cross-task knowledge and mitigate catastrophic interference. However, a\nnotable drawback of these methods is their tendency to overfit the limited\nreplay buffer. In contrast, our proposed solution, SurpriseNet, addresses\ncatastrophic interference by employing a parameter isolation method and\nlearning cross-task knowledge using an auto-encoder inspired by anomaly\ndetection. SurpriseNet is applicable to both structured and unstructured data,\nas it does not rely on image-specific inductive biases. We have conducted\nempirical experiments demonstrating the strengths of SurpriseNet on various\ntraditional vision continual-learning benchmarks, as well as on structured data\ndatasets. Source code made available at https://doi.org/10.5281/zenodo.8247906\nand https://github.com/tachyonicClock/SurpriseNet-CIKM-23",
            "author": [
                "Anton Lee",
                "Yaqian Zhang",
                "Heitor Murilo Gomes",
                "Albert Bifet",
                "Bernhard Pfahringer"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3583780.3615236",
                "http://arxiv.org/abs/2310.20052v1",
                "http://arxiv.org/pdf/2310.20052v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20049v3",
            "title": "SURF: A Generalization Benchmark for GNNs Predicting Fluid Dynamics",
            "updated": "2023-11-20T15:16:59Z",
            "published": "2023-10-30T22:12:35Z",
            "summary": "Simulating fluid dynamics is crucial for the design and development process,\nranging from simple valves to complex turbomachinery. Accurately solving the\nunderlying physical equations is computationally expensive. Therefore,\nlearning-based solvers that model interactions on meshes have gained interest\ndue to their promising speed-ups. However, it is unknown to what extent these\nmodels truly understand the underlying physical principles and can generalize\nrather than interpolate. Generalization is a key requirement for a\ngeneral-purpose fluid simulator, which should adapt to different topologies,\nresolutions, or thermodynamic ranges. We propose SURF, a benchmark designed to\ntest the $\\textit{generalization}$ of learned graph-based fluid simulators.\nSURF comprises individual datasets and provides specific performance and\ngeneralization metrics for evaluating and comparing different models. We\nempirically demonstrate the applicability of SURF by thoroughly investigating\nthe two state-of-the-art graph-based models, yielding new insights into their\ngeneralization.",
            "author": [
                "Stefan K\u00fcnzli",
                "Florian Gr\u00f6tschla",
                "Jo\u00ebl Mathys",
                "Roger Wattenhofer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20049v3",
                "http://arxiv.org/pdf/2310.20049v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20047v2",
            "title": "Baire Measurable Matchings in Non-Amenable Graphs",
            "updated": "2023-11-12T04:46:56Z",
            "published": "2023-10-30T22:08:35Z",
            "summary": "We prove that every Schreier graph of a free Borel action of a finitely\ngenerated non-amenable group has a Baire measurable perfect matching. This\nresult was previously only known in the bipartite setting. We also prove that\nevery Borel non-amenable bounded degree graph with only even degrees has a\nBaire measurable balanced orientation.",
            "author": [
                "Alexander Kastner",
                "Clark Lyons"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20047v2",
                "http://arxiv.org/pdf/2310.20047v2"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20034v1",
            "title": "GG-LLM: Geometrically Grounding Large Language Models for Zero-shot\n  Human Activity Forecasting in Human-Aware Task Planning",
            "updated": "2023-10-30T21:36:20Z",
            "published": "2023-10-30T21:36:20Z",
            "summary": "A robot in a human-centric environment needs to account for the human's\nintent and future motion in its task and motion planning to ensure safe and\neffective operation. This requires symbolic reasoning about probable future\nactions and the ability to tie these actions to specific locations in the\nphysical environment. While one can train behavioral models capable of\npredicting human motion from past activities, this approach requires large\namounts of data to achieve acceptable long-horizon predictions. More\nimportantly, the resulting models are constrained to specific data formats and\nmodalities. Moreover, connecting predictions from such models to the\nenvironment at hand to ensure the applicability of these predictions is an\nunsolved problem. We present a system that utilizes a Large Language Model\n(LLM) to infer a human's next actions from a range of modalities without\nfine-tuning. A novel aspect of our system that is critical to robotics\napplications is that it links the predicted actions to specific locations in a\nsemantic map of the environment. Our method leverages the fact that LLMs,\ntrained on a vast corpus of text describing typical human behaviors, encode\nsubstantial world knowledge, including probable sequences of human actions and\nactivities. We demonstrate how these localized activity predictions can be\nincorporated in a human-aware task planner for an assistive robot to reduce the\noccurrences of undesirable human-robot interactions by 29.2% on average.",
            "author": [
                "Moritz A. Graule",
                "Volkan Isler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20034v1",
                "http://arxiv.org/pdf/2310.20034v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20033v2",
            "title": "Synthetic Imitation Edit Feedback for Factual Alignment in Clinical\n  Summarization",
            "updated": "2023-11-03T13:49:16Z",
            "published": "2023-10-30T21:33:22Z",
            "summary": "Large Language Models (LLMs) like the GPT and LLaMA families have\ndemonstrated exceptional capabilities in capturing and condensing critical\ncontextual information and achieving state-of-the-art performance in the\nsummarization task. However, community concerns about these models'\nhallucination issues continue to rise. LLMs sometimes generate factually\nhallucinated summaries, which can be extremely harmful in the clinical domain\nNLP tasks (e.g., clinical note summarization), where factually incorrect\nstatements can lead to critically erroneous diagnoses. Fine-tuning LLMs using\nhuman feedback has shown the promise of aligning LLMs to be factually\nconsistent during generation, but such training procedure requires high-quality\nhuman-annotated data, which can be extremely expensive to get in the clinical\ndomain. In this work, we propose a new pipeline using ChatGPT instead of human\nexperts to generate high-quality feedback data for improving factual\nconsistency in the clinical note summarization task. We focus specifically on\nedit feedback because recent work discusses the shortcomings of human alignment\nvia preference feedback in complex situations (such as clinical NLP tasks that\nrequire extensive expert knowledge), as well as some advantages of collecting\nedit feedback from domain experts. In addition, although GPT has reached the\nexpert level in many clinical NLP tasks (e.g., USMLE QA), there is not much\nprevious work discussing whether GPT can generate expert-level edit feedback\nfor LMs in the clinical note summarization task. We hope to fill this gap.\nFinally, our evaluations demonstrate the potential use of GPT edits in human\nalignment, especially from a factuality perspective.",
            "author": [
                "Prakamya Mishra",
                "Zonghai Yao",
                "Shuwei Chen",
                "Beining Wang",
                "Rohan Mittal",
                "Hong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20033v2",
                "http://arxiv.org/pdf/2310.20033v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20026v1",
            "title": "Hybrid propagation physics for the design and modeling of astronomical\n  observatories: a coronagraphic example",
            "updated": "2023-10-30T21:22:15Z",
            "published": "2023-10-30T21:22:15Z",
            "summary": "For diffraction-limited optical systems an accurate physical optics model is\nnecessary to properly evaluate instrument performance. Astronomical\nobservatories outfitted with coronagraphs for direct exoplanet imaging require\nphysical optics models to simulate the effects of misalignment and diffraction.\nAccurate knowledge of the observatory's PSF is integral for the design of\nhigh-contrast imaging instruments and simulation of astrophysical observations.\nThe state of the art is to model the misalignment, ray aberration, and\ndiffraction across multiple software packages, which complicates the design\nprocess. Gaussian Beamlet Decomposition (GBD) is a ray-based method of\ndiffraction calculation that has been widely implemented in commercial optical\ndesign software. By performing the coherent calculation with data from the ray\nmodel of the observatory, the ray aberration errors can be fed directly into\nthe physical optics model of the coronagraph, enabling a more integrated model\nof the observatory. We develop a formal algorithm for the transfer-matrix\nmethod of GBD, and evaluate it against analytical results and a traditional\nphysical optics model to assess the suitability of GBD for high-contrast\nimaging simulations. Our GBD simulations of the observatory PSF, when compared\nto the analytical Airy function, have a sum-normalized RMS difference of\n~10^-6. These fields are then propagated through a Fraunhofer model of a\nexoplanet imaging coronagraph where the mean residual numerical contrast is\n4x10^-11, with a maximum near the inner working angle at 5x10^-9. These results\nshow considerable promise for the future development of GBD as a viable\npropagation technique in high-contrast imaging. We developed this algorithm in\nan open-source software package and outlined a path for its continued\ndevelopment to increase the fidelity and flexibility of diffraction simulations\nusing GBD.",
            "author": [
                "Jaren N. Ashcraft",
                "Ewan S. Douglas",
                "Daewook Kim",
                "A. J. E. Riggs"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20026v1",
                "http://arxiv.org/pdf/2310.20026v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.20012v1",
            "title": "Multiscale Feature Attribution for Outliers",
            "updated": "2023-10-30T20:58:28Z",
            "published": "2023-10-30T20:58:28Z",
            "summary": "Machine learning techniques can automatically identify outliers in massive\ndatasets, much faster and more reproducible than human inspection ever could.\nBut finding such outliers immediately leads to the question: which features\nrender this input anomalous? We propose a new feature attribution method,\nInverse Multiscale Occlusion, that is specifically designed for outliers, for\nwhich we have little knowledge of the type of features we want to identify and\nexpect that the model performance is questionable because anomalous test data\nlikely exceed the limits of the training data. We demonstrate our method on\noutliers detected in galaxy spectra from the Dark Energy Survey Instrument and\nfind its results to be much more interpretable than alternative attribution\napproaches.",
            "author": [
                "Jeff Shen",
                "Peter Melchior"
            ],
            "link": [
                "http://arxiv.org/abs/2310.20012v1",
                "http://arxiv.org/pdf/2310.20012v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "astro-ph.IM",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19998v1",
            "title": "Generative retrieval-augmented ontologic graph and multi-agent\n  strategies for interpretive large language model-based materials design",
            "updated": "2023-10-30T20:31:50Z",
            "published": "2023-10-30T20:31:50Z",
            "summary": "Transformer neural networks show promising capabilities, in particular for\nuses in materials analysis, design and manufacturing, including their capacity\nto work effectively with both human language, symbols, code, and numerical\ndata. Here we explore the use of large language models (LLMs) as a tool that\ncan support engineering analysis of materials, applied to retrieving key\ninformation about subject areas, developing research hypotheses, discovery of\nmechanistic relationships across disparate areas of knowledge, and writing and\nexecuting simulation codes for active knowledge generation based on physical\nground truths. When used as sets of AI agents with specific features,\ncapabilities, and instructions, LLMs can provide powerful problem solution\nstrategies for applications in analysis and design problems. Our experiments\nfocus on using a fine-tuned model, MechGPT, developed based on training data in\nthe mechanics of materials domain. We first affirm how finetuning endows LLMs\nwith reasonable understanding of domain knowledge. However, when queried\noutside the context of learned matter, LLMs can have difficulty to recall\ncorrect information. We show how this can be addressed using\nretrieval-augmented Ontological Knowledge Graph strategies that discern how the\nmodel understands what concepts are important and how they are related.\nIllustrated for a use case of relating distinct areas of knowledge - here,\nmusic and proteins - such strategies can also provide an interpretable graph\nstructure with rich information at the node, edge and subgraph level. We\ndiscuss nonlinear sampling strategies and agent-based modeling applied to\ncomplex question answering, code generation and execution in the context of\nautomated force field development from actively learned Density Functional\nTheory (DFT) modeling, and data analysis.",
            "author": [
                "Markus J. Buehler"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19998v1",
                "http://arxiv.org/pdf/2310.19998v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cond-mat.dis-nn",
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19997v1",
            "title": "State-Dependent Dynamic Tube MPC: A Novel Tube MPC Method with a Fuzzy\n  Model of Disturbances",
            "updated": "2023-10-30T20:30:12Z",
            "published": "2023-10-30T20:30:12Z",
            "summary": "Most real-world systems are affected by external disturbances, which may be\nimpossible or costly to measure. For instance, when autonomous robots move in\ndusty environments, the perception of their sensors is disturbed. Moreover,\nuneven terrains can cause ground robots to deviate from their planned\ntrajectories. Thus, learning the external disturbances and incorporating this\nknowledge into the future predictions in decision-making can significantly\ncontribute to improved performance. Our core idea is to learn the external\ndisturbances that vary with the states of the system, and to incorporate this\nknowledge into a novel formulation for robust tube model predictive control\n(TMPC). Robust TMPC provides robustness to bounded disturbances considering the\nknown (fixed) upper bound of the disturbances, but it does not consider the\ndynamics of the disturbances. This can lead to highly conservative solutions.\nWe propose a new dynamic version of robust TMPC (with proven robust stability),\ncalled state-dependent dynamic TMPC (SDD-TMPC), which incorporates the dynamics\nof the disturbances into the decision-making of TMPC. In order to learn the\ndynamics of the disturbances as a function of the system states, a fuzzy model\nis proposed. We compare the performance of SDD-TMPC, MPC, and TMPC via\nsimulations, in designed search-and-rescue scenarios. The results show that,\nwhile remaining robust to bounded external disturbances, SDD-TMPC generates\nless conservative solutions and remains feasible in more cases, compared to\nTMPC.",
            "author": [
                "Filip Surma",
                "Anahita Jamshidnejad"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19997v1",
                "http://arxiv.org/pdf/2310.19997v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "93B45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19996v1",
            "title": "Adaptive Anchor Label Propagation for Transductive Few-Shot Learning",
            "updated": "2023-10-30T20:29:31Z",
            "published": "2023-10-30T20:29:31Z",
            "summary": "Few-shot learning addresses the issue of classifying images using limited\nlabeled data. Exploiting unlabeled data through the use of transductive\ninference methods such as label propagation has been shown to improve the\nperformance of few-shot learning significantly. Label propagation infers\npseudo-labels for unlabeled data by utilizing a constructed graph that exploits\nthe underlying manifold structure of the data. However, a limitation of the\nexisting label propagation approaches is that the positions of all data points\nare fixed and might be sub-optimal so that the algorithm is not as effective as\npossible. In this work, we propose a novel algorithm that adapts the feature\nembeddings of the labeled data by minimizing a differentiable loss function\noptimizing their positions in the manifold in the process. Our novel algorithm,\nAdaptive Anchor Label Propagation}, outperforms the standard label propagation\nalgorithm by as much as 7% and 2% in the 1-shot and 5-shot settings\nrespectively. We provide experimental results highlighting the merits of our\nalgorithm on four widely used few-shot benchmark datasets, namely miniImageNet,\ntieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and\nWideResNet-28-10. The source code can be found at\nhttps://github.com/MichalisLazarou/A2LP.",
            "author": [
                "Michalis Lazarou",
                "Yannis Avrithis",
                "Guangyu Ren",
                "Tania Stathaki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19996v1",
                "http://arxiv.org/pdf/2310.19996v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19995v1",
            "title": "Emotional Theory of Mind: Bridging Fast Visual Processing with Slow\n  Linguistic Reasoning",
            "updated": "2023-10-30T20:26:12Z",
            "published": "2023-10-30T20:26:12Z",
            "summary": "The emotional theory of mind problem in images is an emotion recognition\ntask, specifically asking \"How does the person in the bounding box feel?\"\nFacial expressions, body pose, contextual information and implicit commonsense\nknowledge all contribute to the difficulty of the task, making this task\ncurrently one of the hardest problems in affective computing. The goal of this\nwork is to evaluate the emotional commonsense knowledge embedded in recent\nlarge vision language models (CLIP, LLaVA) and large language models (GPT-3.5)\non the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely\ntext-based language model on images, we construct \"narrative captions\" relevant\nto emotion perception, using a set of 872 physical social signal descriptions\nrelated to 26 emotional categories, along with 224 labels for emotionally\nsalient environmental contexts, sourced from writer's guides for character\nexpressions and settings. We evaluate the use of the resulting captions in an\nimage-to-language-to-emotion task. Experiments using zero-shot vision-language\nmodels on EMOTIC show that combining \"fast\" and \"slow\" reasoning is a promising\nway forward to improve emotion recognition systems. Nevertheless, a gap remains\nin the zero-shot emotional theory of mind task compared to prior work trained\non the EMOTIC dataset.",
            "author": [
                "Yasaman Etesam",
                "Ozge Nilay Yalcin",
                "Chuxuan Zhang",
                "Angelica Lim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19995v1",
                "http://arxiv.org/pdf/2310.19995v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19979v1",
            "title": "Using Fibonacci Numbers and Chebyshev Polynomials to Express Fox\n  Coloring Groups and Alexander-Burau-Fox Modules of Diagrams of Wheel Graphs",
            "updated": "2023-10-30T19:54:07Z",
            "published": "2023-10-30T19:54:07Z",
            "summary": "In this paper we compute the Reduced Fox Coloring Group of the diagrams of\nWheel Graphs which can also be represented at the closure of the braids\n$(\\sigma_1 \\sigma_2^{-1})^n$. In doing so, we utilize Fibonacci numbers and\ntheir properties.\n  Following this, we generalize our result to compute the Alexander-Burau-Fox\nModule over the ring $\\mathbb{Z}[t^{\\pm 1}]$ for the same class of links. In\nour computation, Chebyshev polynomials function as a generalization of\nFibonacci Numbers.",
            "author": [
                "Anthony Christiana",
                "Huizheng Guo",
                "Jozef H. Przytycki"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19979v1",
                "http://arxiv.org/pdf/2310.19979v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.NT",
                "Primary: 57K10 Secondary: 57M12, 11B39"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19978v1",
            "title": "Scaling Up Differentially Private LASSO Regularized Logistic Regression\n  via Faster Frank-Wolfe Iterations",
            "updated": "2023-10-30T19:52:43Z",
            "published": "2023-10-30T19:52:43Z",
            "summary": "To the best of our knowledge, there are no methods today for training\ndifferentially private regression models on sparse input data. To remedy this,\nwe adapt the Frank-Wolfe algorithm for $L_1$ penalized linear regression to be\naware of sparse inputs and to use them effectively. In doing so, we reduce the\ntraining time of the algorithm from $\\mathcal{O}( T D S + T N S)$ to\n$\\mathcal{O}(N S + T \\sqrt{D} \\log{D} + T S^2)$, where $T$ is the number of\niterations and a sparsity rate $S$ of a dataset with $N$ rows and $D$ features.\nOur results demonstrate that this procedure can reduce runtime by a factor of\nup to $2,200\\times$, depending on the value of the privacy parameter $\\epsilon$\nand the sparsity of the dataset.",
            "author": [
                "Edward Raff",
                "Amol Khanna",
                "Fred Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19978v1",
                "http://arxiv.org/pdf/2310.19978v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.CO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19967v2",
            "title": "Early detection of inflammatory arthritis to improve referrals using\n  multimodal machine learning from blood testing, semi-structured and\n  unstructured patient records",
            "updated": "2023-11-03T19:32:02Z",
            "published": "2023-10-30T19:30:00Z",
            "summary": "Early detection of inflammatory arthritis (IA) is critical to efficient and\naccurate hospital referral triage for timely treatment and preventing the\ndeterioration of the IA disease course, especially under limited healthcare\nresources. The manual assessment process is the most common approach in\npractice for the early detection of IA, but it is extremely labor-intensive and\ninefficient. A large amount of clinical information needs to be assessed for\nevery referral from General Practice (GP) to the hospitals. Machine learning\nshows great potential in automating repetitive assessment tasks and providing\ndecision support for the early detection of IA. However, most machine\nlearning-based methods for IA detection rely on blood testing results. But in\npractice, blood testing data is not always available at the point of referrals,\nso we need methods to leverage multimodal data such as semi-structured and\nunstructured data for early detection of IA. In this research, we present\nfusion and ensemble learning-based methods using multimodal data to assist\ndecision-making in the early detection of IA, and a conformal prediction-based\nmethod to quantify the uncertainty of the prediction and detect any unreliable\npredictions. To the best of our knowledge, our study is the first attempt to\nutilize multimodal data to support the early detection of IA from GP referrals.",
            "author": [
                "Bing Wang",
                "Weizi Li",
                "Anthony Bradlow",
                "Antoni T. Y. Chan",
                "Eghosa Bazuaye"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19967v2",
                "http://arxiv.org/pdf/2310.19967v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19961v1",
            "title": "ExPT: Synthetic Pretraining for Few-Shot Experimental Design",
            "updated": "2023-10-30T19:25:43Z",
            "published": "2023-10-30T19:25:43Z",
            "summary": "Experimental design is a fundamental problem in many science and engineering\nfields. In this problem, sample efficiency is crucial due to the time, money,\nand safety costs of real-world design evaluations. Existing approaches either\nrely on active data collection or access to large, labeled datasets of past\nexperiments, making them impractical in many real-world scenarios. In this\nwork, we address the more challenging yet realistic setting of few-shot\nexperimental design, where only a few labeled data points of input designs and\ntheir corresponding values are available. We approach this problem as a\nconditional generation task, where a model conditions on a few labeled examples\nand the desired output to generate an optimal input design. To this end, we\nintroduce Experiment Pretrained Transformers (ExPT), a foundation model for\nfew-shot experimental design that employs a novel combination of synthetic\npretraining with in-context learning. In ExPT, we only assume knowledge of a\nfinite collection of unlabelled data points from the input domain and pretrain\na transformer neural network to optimize diverse synthetic functions defined\nover this domain. Unsupervised pretraining allows ExPT to adapt to any design\ntask at test time in an in-context fashion by conditioning on a few labeled\ndata points from the target task and generating the candidate optima. We\nevaluate ExPT on few-shot experimental design in challenging domains and\ndemonstrate its superior generality and performance compared to existing\nmethods. The source code is available at https://github.com/tung-nd/ExPT.git.",
            "author": [
                "Tung Nguyen",
                "Sudhanshu Agrawal",
                "Aditya Grover"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19961v1",
                "http://arxiv.org/pdf/2310.19961v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19949v1",
            "title": "Builder-Blocker General Position Games",
            "updated": "2023-10-30T19:00:29Z",
            "published": "2023-10-30T19:00:29Z",
            "summary": "This paper considers a game version of the general position problem in which\na general position set is built through adversarial play. Two players in a\ngraph, Builder and Blocker, take it in turns to add a vertex to a set, such\nthat the vertices of this set are always in general position. The goal of\nBuilder is to create a large general position set, whilst the aim of Blocker is\nto frustrate Builder's plans by making the set as small as possible. The game\nfinishes when no further vertices can be added without creating three-in-a-line\nand the number of vertices in this set is the game general position number. We\ndetermine this number for some common graph classes and provide sharp bounds,\nin particular for the case of trees. We also discuss the effect of changing the\norder of the players.",
            "author": [
                "Sandi Klav\u017ear",
                "Jing Tian",
                "James Tuite"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19949v1",
                "http://arxiv.org/pdf/2310.19949v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C12, 05C57, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19943v1",
            "title": "The Acquisition of Physical Knowledge in Generative Neural Networks",
            "updated": "2023-10-30T18:58:03Z",
            "published": "2023-10-30T18:58:03Z",
            "summary": "As children grow older, they develop an intuitive understanding of the\nphysical processes around them. Their physical understanding develops in\nstages, moving along developmental trajectories which have been mapped out\nextensively in previous empirical research. Here, we investigate how the\nlearning trajectories of deep generative neural networks compare to children's\ndevelopmental trajectories using physical understanding as a testbed. We\noutline an approach that allows us to examine two distinct hypotheses of human\ndevelopment - stochastic optimization and complexity increase. We find that\nwhile our models are able to accurately predict a number of physical processes,\ntheir learning trajectories under both hypotheses do not follow the\ndevelopmental trajectories of children.",
            "author": [
                "Luca M. Schulze Buschoff",
                "Eric Schulz",
                "Marcel Binz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19943v1",
                "http://arxiv.org/pdf/2310.19943v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19920v1",
            "title": "Solving a Class of Cut-Generating Linear Programs via Machine Learning",
            "updated": "2023-10-30T18:31:52Z",
            "published": "2023-10-30T18:31:52Z",
            "summary": "Cut-generating linear programs (CGLPs) play a key role as a separation oracle\nto produce valid inequalities for the feasible region of mixed-integer\nprograms. When incorporated inside branch-and-bound, the cutting planes\nobtained from CGLPs help to tighten relaxations and improve dual bounds.\nHowever, running the CGLPs at the nodes of the branch-and-bound tree is\ncomputationally cumbersome due to the large number of node candidates and the\nlack of a priori knowledge on which nodes admit useful cutting planes. As a\nresult, CGLPs are often avoided at default settings of branch-and-cut\nalgorithms despite their potential impact on improving dual bounds. In this\npaper, we propose a novel framework based on machine learning to approximate\nthe optimal value of a CGLP class that determines whether a cutting plane can\nbe generated at a node of the branch-and-bound tree. Translating the CGLP as an\nindicator function of the objective function vector, we show that it can be\napproximated through conventional data classification techniques. We provide a\nsystematic procedure to efficiently generate training data sets for the\ncorresponding classification problem based on the CGLP structure. We conduct\ncomputational experiments on benchmark instances using classification methods\nsuch as logistic regression. These results suggest that the approximate CGLP\nobtained from classification can improve the solution time compared to that of\nconventional cutting plane methods. Our proposed framework can be efficiently\napplied to a large number of nodes in the branch-and-bound tree to identify the\nbest candidates for adding a cut.",
            "author": [
                "Atefeh Rajabalizadeh",
                "Danial Davarnia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19920v1",
                "http://arxiv.org/pdf/2310.19920v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19916v1",
            "title": "Spectator-model studies for spin-dependent gluon TMD PDFs at the LHC and\n  EIC",
            "updated": "2023-10-30T18:29:10Z",
            "published": "2023-10-30T18:29:10Z",
            "summary": "We present novel analyses on accessing the 3D gluon content of the proton via\nspin-dependent TMD gluon densities, calculated through the spectator-model\napproach. Our formalism embodies a fit-based spectator-mass modulation\nfunction, suited to catch longitudinal-momentum effects in a wide kinematic\nrange. Particular attention is paid to the time-reversal even Boer--Mulders and\nthe time-reversal odd Sivers functions, whose accurate knowledge, needed to\nperform precise 3D analyses of nucleons, motivates synergies between LHC and\nEIC Communities.",
            "author": [
                "Alessandro Bacchetta",
                "Francesco Giovanni Celiberto",
                "Marco Radici"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19916v1",
                "http://arxiv.org/pdf/2310.19916v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19906v1",
            "title": "Interpretable Prototype-based Graph Information Bottleneck",
            "updated": "2023-10-30T18:16:19Z",
            "published": "2023-10-30T18:16:19Z",
            "summary": "The success of Graph Neural Networks (GNNs) has led to a need for\nunderstanding their decision-making process and providing explanations for\ntheir predictions, which has given rise to explainable AI (XAI) that offers\ntransparent explanations for black-box models. Recently, the use of prototypes\nhas successfully improved the explainability of models by learning prototypes\nto imply training graphs that affect the prediction. However, these approaches\ntend to provide prototypes with excessive information from the entire graph,\nleading to the exclusion of key substructures or the inclusion of irrelevant\nsubstructures, which can limit both the interpretability and the performance of\nthe model in downstream tasks. In this work, we propose a novel framework of\nexplainable GNNs, called interpretable Prototype-based Graph Information\nBottleneck (PGIB) that incorporates prototype learning within the information\nbottleneck framework to provide prototypes with the key subgraph from the input\ngraph that is important for the model prediction. This is the first work that\nincorporates prototype learning into the process of identifying the key\nsubgraphs that have a critical impact on the prediction performance. Extensive\nexperiments, including qualitative analysis, demonstrate that PGIB outperforms\nstate-of-the-art methods in terms of both prediction performance and\nexplainability.",
            "author": [
                "Sangwoo Seo",
                "Sungwon Kim",
                "Chanyoung Park"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19906v1",
                "http://arxiv.org/pdf/2310.19906v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19897v2",
            "title": "Free fermions with no Jordan-Wigner transformation",
            "updated": "2023-12-07T17:31:02Z",
            "published": "2023-10-30T18:07:36Z",
            "summary": "The Jordan-Wigner transformation is frequently utilised to rewrite quantum\nspin chains in terms of fermionic operators. When the resulting Hamiltonian is\nbilinear in these fermions, i.e. the fermions are free, the exact spectrum\nfollows from the eigenvalues of a matrix whose size grows only linearly with\nthe volume of the system. However, several Hamiltonians that do not admit a\nJordan-Wigner transformation to fermion bilinears still have the same type of\nfree-fermion spectra. The spectra of such ``free fermions in disguise\" models\ncan be found exactly by an intricate but explicit construction of the raising\nand lowering operators. We generalise the methods further to find a family of\nsuch spin chains. We compute the exact spectrum, and generalise an elegant\ngraph-theory construction. We also explain how this family admits an N=2\nlattice supersymmetry.",
            "author": [
                "Paul Fendley",
                "Balazs Pozsgay"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19897v2",
                "http://arxiv.org/pdf/2310.19897v2"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "math-ph",
                "math.MP",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19891v1",
            "title": "Upper bounds for linear graph codes",
            "updated": "2023-10-30T18:01:48Z",
            "published": "2023-10-30T18:01:48Z",
            "summary": "A linear graph code is a family $\\mathcal{C}$ of graphs on $n$ vertices with\nthe property that the symmetric difference of the edge sets of any two graphs\nin $\\mathcal{C}$ is also the edge set of a graph in $\\mathcal{C}$. In this\narticle, we investigate the maximal size of a linear graph code that does not\ncontain a copy of a fixed graph $H$. In particular, we show that if $H$ has an\neven number of edges, the size of the code is $O(2^{\\binom{n}{2}}/\\log n)$,\nmaking progress on a question of Alon. Furthermore, we show that for almost all\ngraphs $H$ with an even number of edges, there exists $\\varepsilon_H>0$ such\nthat the size of a linear graph code without a copy of $H$ is at most\n$2^{\\binom{n}{2}}/n^{\\varepsilon_H}$.",
            "author": [
                "Leo Versteegen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19891v1",
                "http://arxiv.org/pdf/2310.19891v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19883v2",
            "title": "Two Loop Renormalization of Scalar Theories using a Geometric Approach",
            "updated": "2023-11-09T19:25:02Z",
            "published": "2023-10-30T18:00:04Z",
            "summary": "We derive a general formula for two-loop counterterms in Effective Field\nTheories (EFTs) using a geometric approach. This formula allows the two-loop\nresults of our previous paper to be applied to a wide range of theories. The\ntwo-loop results hold for loop graphs in EFTs where the interaction vertices\ncontain operators of arbitrarily high dimension, but at most two derivatives.\nWe also extend our previous one-loop result to include operators with an\narbitrary number of derivatives, as long as there is at most one derivative\nacting on each field. The final result for the two-loop counterterms is written\nin terms of geometric quantities such as the Riemann curvature tensor of the\nscalar manifold and its covariant derivatives. As applications of our results,\nwe give the two-loop counterterms and renormalization group equations for the\nO(n) EFT to dimension six, the scalar sector of the Standard Model Effective\nField Theory (SMEFT) to dimension six, and chiral perturbation theory to order\n$p^6$.",
            "author": [
                "Elizabeth E. Jenkins",
                "Aneesh V. Manohar",
                "Luca Naterop",
                "Julie Pag\u00e8s"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19883v2",
                "http://arxiv.org/pdf/2310.19883v2"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19874v1",
            "title": "Bounding Entanglement Entropy with Contracted Graphs",
            "updated": "2023-10-30T18:00:01Z",
            "published": "2023-10-30T18:00:01Z",
            "summary": "Following on our previous work arXiv:2204.07593 and arXiv:2306.01043 studying\nthe orbits of quantum states under Clifford circuits via `reachability graphs',\nwe introduce `contracted graphs' whose vertices represent classes of quantum\nstates with the same entropy vector. These contracted graphs represent the\ndouble cosets of the Clifford group, where the left cosets are built from the\nstabilizer subgroup of the starting state and the right cosets are built from\nthe entropy-preserving operators. We study contracted graphs for stabilizer\nstates, as well as W states and Dicke states, discussing how the diameter of a\nstate's contracted graph constrains the `entropic diversity' of its $2$-qubit\nClifford orbit. We derive an upper bound on the number of entropy vectors that\ncan be generated using any $n$-qubit Clifford circuit, for any quantum state.\nWe speculate on the holographic implications for the relative proximity of\ngravitational duals of states within the same Clifford orbit. Although we\nconcentrate on how entropy evolves under the Clifford group, our double-coset\nformalism, and thus the contracted graph picture, is extendable to generic gate\nsets and generic state properties.",
            "author": [
                "Cynthia Keeler",
                "William Munizzi",
                "Jason Pollack"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19874v1",
                "http://arxiv.org/pdf/2310.19874v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-th",
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19861v1",
            "title": "Posterior Sampling for Competitive RL: Function Approximation and\n  Partial Observation",
            "updated": "2023-10-30T17:59:26Z",
            "published": "2023-10-30T17:59:26Z",
            "summary": "This paper investigates posterior sampling algorithms for competitive\nreinforcement learning (RL) in the context of general function approximations.\nFocusing on zero-sum Markov games (MGs) under two critical settings, namely\nself-play and adversarial learning, we first propose the self-play and\nadversarial generalized eluder coefficient (GEC) as complexity measures for\nfunction approximation, capturing the exploration-exploitation trade-off in\nMGs. Based on self-play GEC, we propose a model-based self-play posterior\nsampling method to control both players to learn Nash equilibrium, which can\nsuccessfully handle the partial observability of states. Furthermore, we\nidentify a set of partially observable MG models fitting MG learning with the\nadversarial policies of the opponent. Incorporating the adversarial GEC, we\npropose a model-based posterior sampling method for learning adversarial MG\nwith potential partial observability. We further provide low regret bounds for\nproposed algorithms that can scale sublinearly with the proposed GEC and the\nnumber of episodes $T$. To the best of our knowledge, we for the first time\ndevelop generic model-based posterior sampling algorithms for competitive RL\nthat can be applied to a majority of tractable zero-sum MG classes in both\nfully observable and partially observable MGs with self-play and adversarial\nlearning.",
            "author": [
                "Shuang Qiu",
                "Ziyu Dai",
                "Han Zhong",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Tong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19861v1",
                "http://arxiv.org/pdf/2310.19861v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.GT",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19794v1",
            "title": "Robust Causal Bandits for Linear Models",
            "updated": "2023-10-30T17:58:01Z",
            "published": "2023-10-30T17:58:01Z",
            "summary": "Sequential design of experiments for optimizing a reward function in causal\nsystems can be effectively modeled by the sequential design of interventions in\ncausal bandits (CBs). In the existing literature on CBs, a critical assumption\nis that the causal models remain constant over time. However, this assumption\ndoes not necessarily hold in complex systems, which constantly undergo temporal\nmodel fluctuations. This paper addresses the robustness of CBs to such model\nfluctuations. The focus is on causal systems with linear structural equation\nmodels (SEMs). The SEMs and the time-varying pre- and post-interventional\nstatistical models are all unknown. Cumulative regret is adopted as the design\ncriteria, based on which the objective is to design a sequence of interventions\nthat incur the smallest cumulative regret with respect to an oracle aware of\nthe entire causal model and its fluctuations. First, it is established that the\nexisting approaches fail to maintain regret sub-linearity with even a few\ninstances of model deviation. Specifically, when the number of instances with\nmodel deviation is as few as $T^\\frac{1}{2L}$, where $T$ is the time horizon\nand $L$ is the longest causal path in the graph, the existing algorithms will\nhave linear regret in $T$. Next, a robust CB algorithm is designed, and its\nregret is analyzed, where upper and information-theoretic lower bounds on the\nregret are established. Specifically, in a graph with $N$ nodes and maximum\ndegree $d$, under a general measure of model deviation $C$, the cumulative\nregret is upper bounded by $\\tilde{\\mathcal{O}}(d^{L-\\frac{1}{2}}(\\sqrt{NT} +\nNC))$ and lower bounded by $\\Omega(d^{\\frac{L}{2}-2}\\max\\{\\sqrt{T},d^2C\\})$.\nComparing these bounds establishes that the proposed algorithm achieves nearly\noptimal $\\tilde{\\mathcal{O}}(\\sqrt{T})$ regret when $C$ is $o(\\sqrt{T})$ and\nmaintains sub-linear regret for a broader range of $C$.",
            "author": [
                "Zirui Yan",
                "Arpan Mukherjee",
                "Burak Var\u0131c\u0131",
                "Ali Tajer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19794v1",
                "http://arxiv.org/pdf/2310.19794v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19791v1",
            "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting\n  Code",
            "updated": "2023-10-30T17:55:02Z",
            "published": "2023-10-30T17:55:02Z",
            "summary": "While large language models (LLMs) now excel at code generation, a key aspect\nof software development is the art of refactoring: consolidating code into\nlibraries of reusable and readable programs. In this paper, we introduce LILO,\na neurosymbolic framework that iteratively synthesizes, compresses, and\ndocuments code to build libraries tailored to particular problem domains. LILO\ncombines LLM-guided program synthesis with recent algorithmic advances in\nautomated refactoring from Stitch: a symbolic compression system that\nefficiently identifies optimal lambda abstractions across large code corpora.\nTo make these abstractions interpretable, we introduce an auto-documentation\n(AutoDoc) procedure that infers natural language names and docstrings based on\ncontextual examples of usage. In addition to improving human readability, we\nfind that AutoDoc boosts performance by helping LILO's synthesizer to interpret\nand deploy learned abstractions. We evaluate LILO on three inductive program\nsynthesis benchmarks for string editing, scene reasoning, and graphics\ncomposition. Compared to existing neural and symbolic methods - including the\nstate-of-the-art library learning algorithm DreamCoder - LILO solves more\ncomplex tasks and learns richer libraries that are grounded in linguistic\nknowledge.",
            "author": [
                "Gabriel Grand",
                "Lionel Wong",
                "Matthew Bowers",
                "Theo X. Olausson",
                "Muxin Liu",
                "Joshua B. Tenenbaum",
                "Jacob Andreas"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19791v1",
                "http://arxiv.org/pdf/2310.19791v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19783v1",
            "title": "Approximate t-designs in generic circuit architectures",
            "updated": "2023-10-30T17:49:44Z",
            "published": "2023-10-30T17:49:44Z",
            "summary": "Unitary t-designs are distributions on the unitary group whose first t\nmoments appear maximally random. Previous work has established several upper\nbounds on the depths at which certain specific random quantum circuit ensembles\napproximate t-designs. Here we show that these bounds can be extended to any\nfixed architecture of Haar-random two-site gates. This is accomplished by\nrelating the spectral gaps of such architectures to those of 1D brickwork\narchitectures. Our bound depends on the details of the architecture only via\nthe typical number of layers needed for a block of the circuit to form a\nconnected graph over the sites. When this quantity is independent of width, the\ncircuit forms an approximate t-design in linear depth. We also give an implicit\nbound for nondeterministic architectures in terms of properties of the\ncorresponding distribution over fixed architectures.",
            "author": [
                "Daniel Belkin",
                "James Allen",
                "Soumik Ghosh",
                "Christopher Kang",
                "Sophia Lin",
                "James Sud",
                "Fred Chong",
                "Bill Fefferman",
                "Bryan K. Clark"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19783v1",
                "http://arxiv.org/pdf/2310.19783v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19777v1",
            "title": "Conditional gradients for total variation regularization with PDE\n  constraints: a graph cuts approach",
            "updated": "2023-10-30T17:45:45Z",
            "published": "2023-10-30T17:45:45Z",
            "summary": "Total variation regularization has proven to be a valuable tool in the\ncontext of optimal control of differential equations. This is particularly\nattributed to the observation that TV-penalties often favor piecewise constant\nminimizers with well-behaved jumpsets. On the downside, their intricate\nproperties significantly complicate every aspect of their analysis, from the\nderivation of first-order optimality conditions to their discrete approximation\nand the choice of a suitable solution algorithm. In this paper, we investigate\na general class of minimization problems with TV-regularization, comprising\nboth continuous and discretized control spaces, from a convex geometry\nperspective. This leads to a variety of novel theoretical insights on\nminimization problems with total variation regularization as well as tools for\ntheir practical realization. First, by studying the extremal points of the\nrespective total variation unit balls, we enable their efficient solution by\ngeometry exploiting algorithms, e.g. fully-corrective generalized conditional\ngradient methods. We give a detailed account on the practical realization of\nsuch a method for piecewise constant finite element approximations of the\ncontrol on triangulations of the spatial domain. Second, in the same setting\nand for suitable sequences of uniformly refined meshes, it is shown that\nminimizers to discretized PDE-constrained optimal control problems approximate\nsolutions to a continuous limit problem involving an anisotropic total\nvariation reflecting the fine-scale geometry of the mesh.",
            "author": [
                "Giacomo Cristinelli",
                "Jos\u00e9 A. Iglesias",
                "Daniel Walter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19777v1",
                "http://arxiv.org/pdf/2310.19777v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.NA",
                "math.AP",
                "math.NA",
                "49M41, 65J20, 52A40, 49J45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19763v1",
            "title": "Autoregressive Renaissance in Neural PDE Solvers",
            "updated": "2023-10-30T17:35:26Z",
            "published": "2023-10-30T17:35:26Z",
            "summary": "Recent developments in the field of neural partial differential equation\n(PDE) solvers have placed a strong emphasis on neural operators. However, the\npaper \"Message Passing Neural PDE Solver\" by Brandstetter et al. published in\nICLR 2022 revisits autoregressive models and designs a message passing graph\nneural network that is comparable with or outperforms both the state-of-the-art\nFourier Neural Operator and traditional classical PDE solvers in its\ngeneralization capabilities and performance. This blog post delves into the key\ncontributions of this work, exploring the strategies used to address the common\nproblem of instability in autoregressive models and the design choices of the\nmessage passing graph neural network architecture.",
            "author": [
                "Yolanne Yi Ran Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19763v1",
                "http://arxiv.org/pdf/2310.19763v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19750v1",
            "title": "Chain-of-Thought Embeddings for Stance Detection on Social Media",
            "updated": "2023-10-30T17:18:10Z",
            "published": "2023-10-30T17:18:10Z",
            "summary": "Stance detection on social media is challenging for Large Language Models\n(LLMs), as emerging slang and colloquial language in online conversations often\ncontain deeply implicit stance labels. Chain-of-Thought (COT) prompting has\nrecently been shown to improve performance on stance detection tasks --\nalleviating some of these issues. However, COT prompting still struggles with\nimplicit stance identification. This challenge arises because many samples are\ninitially challenging to comprehend before a model becomes familiar with the\nslang and evolving knowledge related to different topics, all of which need to\nbe acquired through the training data. In this study, we address this problem\nby introducing COT Embeddings which improve COT performance on stance detection\ntasks by embedding COT reasonings and integrating them into a traditional\nRoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text\nencoders can leverage COT reasonings with minor errors or hallucinations that\nwould otherwise distort the COT output label. 2) Text encoders can overlook\nmisleading COT reasoning when a sample's prediction heavily depends on\ndomain-specific patterns. Our model achieves SOTA performance on multiple\nstance detection datasets collected from social media.",
            "author": [
                "Joseph Gatto",
                "Omar Sharif",
                "Sarah Masud Preum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19750v1",
                "http://arxiv.org/pdf/2310.19750v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19736v3",
            "title": "Evaluating Large Language Models: A Comprehensive Survey",
            "updated": "2023-11-25T17:35:12Z",
            "published": "2023-10-30T17:00:52Z",
            "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na broad spectrum of tasks. They have attracted significant attention and been\ndeployed in numerous downstream applications. Nevertheless, akin to a\ndouble-edged sword, LLMs also present potential risks. They could suffer from\nprivate data leaks or yield inappropriate, harmful, or misleading content.\nAdditionally, the rapid progress of LLMs raises concerns about the potential\nemergence of superintelligent systems without adequate safeguards. To\neffectively capitalize on LLM capacities as well as ensure their safe and\nbeneficial development, it is critical to conduct a rigorous and comprehensive\nevaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of\nLLMs. We categorize the evaluation of LLMs into three major groups: knowledge\nand capability evaluation, alignment evaluation and safety evaluation. In\naddition to the comprehensive review on the evaluation methodologies and\nbenchmarks on these three aspects, we collate a compendium of evaluations\npertaining to LLMs' performance in specialized domains, and discuss the\nconstruction of comprehensive evaluation platforms that cover LLM evaluations\non capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research\ninterests in the evaluation of LLMs, with the ultimate goal of making\nevaluation serve as a cornerstone in guiding the responsible development of\nLLMs. We envision that this will channel their evolution into a direction that\nmaximizes societal benefit while minimizing potential risks. A curated list of\nrelated papers has been publicly available at\nhttps://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.",
            "author": [
                "Zishan Guo",
                "Renren Jin",
                "Chuang Liu",
                "Yufei Huang",
                "Dan Shi",
                "Supryadi",
                "Linhao Yu",
                "Yan Liu",
                "Jiaxuan Li",
                "Bojian Xiong",
                "Deyi Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19736v3",
                "http://arxiv.org/pdf/2310.19736v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19732v2",
            "title": "Combinatorics of Permutreehedra and Geometry of $s$-Permutahedra",
            "updated": "2023-11-07T15:05:00Z",
            "published": "2023-10-30T16:57:54Z",
            "summary": "This thesis finds its place in the interplay between algebraic and geometric\ncombinatorics. We focus on studying two different families of lattices in\nrelation to the weak order: the permutree lattices and the $s$-weak order. The\nfirst part involves the permutree quotients of the weak order. We define\ninversion and cubic vectors on permutrees which respectively give a\nconstructive meet operation between permutrees and a cubical realization of\npermutreehedra. We characterize minimal elements of permutree congruence\nclasses using automata that capture ${ijk}/{kij}$-pattern avoidances and\ngeneralize stack sorting and Coxeter sorting. The second part centers on flow\npolytopes. More specifically, we give a positive answer to a conjecture of\nCeballos and Pons on the $s$-permutahedron when $s$ is a composition. We define\nthe $s$-oruga graph whose flow polytope recovers the $s$-weak order with\nexplicit coordinates. Finally, we introduce the bicho graphs whose flow\npolytopes describe permutree lattices.",
            "author": [
                "Daniel Tamayo Jim\u00e9nez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19732v2",
                "http://arxiv.org/pdf/2310.19732v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19721v3",
            "title": "Promise:Prompt-driven 3D Medical Image Segmentation Using Pretrained\n  Image Foundation Models",
            "updated": "2023-11-13T21:28:24Z",
            "published": "2023-10-30T16:49:03Z",
            "summary": "To address prevalent issues in medical imaging, such as data acquisition\nchallenges and label availability, transfer learning from natural to medical\nimage domains serves as a viable strategy to produce reliable segmentation\nresults. However, several existing barriers between domains need to be broken\ndown, including addressing contrast discrepancies, managing anatomical\nvariability, and adapting 2D pretrained models for 3D segmentation tasks. In\nthis paper, we propose ProMISe,a prompt-driven 3D medical image segmentation\nmodel using only a single point prompt to leverage knowledge from a pretrained\n2D image foundation model. In particular, we use the pretrained vision\ntransformer from the Segment Anything Model (SAM) and integrate lightweight\nadapters to extract depth-related (3D) spatial context without updating the\npretrained weights. For robust results, a hybrid network with complementary\nencoders is designed, and a boundary-aware loss is proposed to achieve precise\nboundaries. We evaluate our model on two public datasets for colon and pancreas\ntumor segmentations, respectively. Compared to the state-of-the-art\nsegmentation methods with and without prompt engineering, our proposed method\nachieves superior performance. The code is publicly available at\nhttps://github.com/MedICL-VU/ProMISe.",
            "author": [
                "Hao Li",
                "Han Liu",
                "Dewei Hu",
                "Jiacheng Wang",
                "Ipek Oguz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19721v3",
                "http://arxiv.org/pdf/2310.19721v3"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19711v1",
            "title": "Flip Graph Connectivity for Arrangements of Pseudolines and\n  Pseudocircles",
            "updated": "2023-10-30T16:37:59Z",
            "published": "2023-10-30T16:37:59Z",
            "summary": "Flip graphs of combinatorial and geometric objects are at the heart of many\ndeep structural insights and connections between different branches of discrete\nmathematics and computer science. They also provide a natural framework for the\nstudy of reconfiguration problems. We study flip graphs of arrangements of\npseudolines and of arrangements of pseudocircles, which are combinatorial\ngeneralizations of lines and circles, respectively. In both cases we consider\ntriangle flips as local transformation and prove conjectures regarding their\nconnectivity.\n  In the case of $n$ pseudolines we show that the connectivity of the flip\ngraph equals its minimum degree, which is exactly $n-2$. For the proof we\nintroduce the class of shellable line arrangements, which serve as reference\nobjects for the construction of disjoint paths. In fact, shellable arrangements\nare elements of a flip graph of line arrangements which are vertices of a\npolytope (Felsner and Ziegler; DM 241 (2001), 301--312). This polytope forms a\ncluster of good connectivity in the flip graph of pseudolines. In the case of\npseudocircles we show that triangle flips induce a connected flip graph on\n\\emph{intersecting} arrangements and also on cylindrical intersecting\narrangements. The result for cylindrical arrangements is used in the proof for\nintersecting arrangements. We also show that in both settings the diameter of\nthe flip graph is in $\\Theta(n^3)$. Our constructions make essential use of\nvariants of the sweeping lemma for pseudocircle arrangements (Snoeyink and\nHershberger; Proc.\\ SoCG 1989: 354--363). We finally study cylindrical\narrangements in their own right and provide new combinatorial characterizations\nof this class.",
            "author": [
                "Yan Alves Radtke",
                "Stefan Felsner",
                "Johannes Obenaus",
                "Sandro Roch",
                "Manfred Scheucher",
                "Birgit Vogtenhuber"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19711v1",
                "http://arxiv.org/pdf/2310.19711v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.CG",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19704v1",
            "title": "A Survey on Knowledge Editing of Neural Networks",
            "updated": "2023-10-30T16:29:47Z",
            "published": "2023-10-30T16:29:47Z",
            "summary": "Deep neural networks are becoming increasingly pervasive in academia and\nindustry, matching and surpassing human performance on a wide variety of fields\nand related tasks. However, just as humans, even the largest artificial neural\nnetworks make mistakes, and once-correct predictions can become invalid as the\nworld progresses in time. Augmenting datasets with samples that account for\nmistakes or up-to-date information has become a common workaround in practical\napplications. However, the well-known phenomenon of catastrophic forgetting\nposes a challenge in achieving precise changes in the implicitly memorized\nknowledge of neural network parameters, often requiring a full model\nre-training to achieve desired behaviors. That is expensive, unreliable, and\nincompatible with the current trend of large self-supervised pre-training,\nmaking it necessary to find more efficient and effective methods for adapting\nneural network models to changing data. To address this need, knowledge editing\nis emerging as a novel area of research that aims to enable reliable,\ndata-efficient, and fast changes to a pre-trained target model, without\naffecting model behaviors on previously learned tasks. In this survey, we\nprovide a brief review of this recent artificial intelligence field of\nresearch. We first introduce the problem of editing neural networks, formalize\nit in a common framework and differentiate it from more notorious branches of\nresearch such as continuous learning. Next, we provide a review of the most\nrelevant knowledge editing approaches and datasets proposed so far, grouping\nworks under four different families: regularization techniques, meta-learning,\ndirect model editing, and architectural strategies. Finally, we outline some\nintersections with other fields of research and potential directions for future\nworks.",
            "author": [
                "Vittorio Mazzia",
                "Alessandro Pedrani",
                "Andrea Caciolai",
                "Kay Rottmann",
                "Davide Bernardi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19704v1",
                "http://arxiv.org/pdf/2310.19704v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19702v3",
            "title": "Rank and Select on Degenerate Strings",
            "updated": "2023-12-04T12:27:31Z",
            "published": "2023-10-30T16:23:55Z",
            "summary": "A 'degenerate string' is a sequence of subsets of some alphabet; it\nrepresents any string obtainable by selecting one character from each set from\nleft to right. Recently, Alanko et al. generalized the rank-select problem to\ndegenerate strings, where given a character $c$ and position $i$ the goal is to\nfind either the $i$th set containing $c$ or the number of occurrences of $c$ in\nthe first $i$ sets [SEA 2023]. The problem has applications to pangenomics; in\nanother work by Alanko et al. they use it as the basis for a compact\nrepresentation of 'de Bruijn Graphs' that supports fast membership queries.\n  In this paper we revisit the rank-select problem on degenerate strings,\nintroducing a new, natural parameter and reanalyzing existing reductions to\nrank-select on regular strings. Plugging in standard data structures, the time\nbounds for queries are improved exponentially while essentially matching, or\nimproving, the space bounds. Furthermore, we provide a lower bound on space\nthat shows that the reductions lead to succinct data structures in a wide range\nof cases. Finally, we provide implementations; our most compact structure\nmatches the space of the most compact structure of Alanko et al. while\nanswering queries twice as fast. We also provide an implementation using modern\nvector processing features; it uses less than one percent more space than the\nmost compact structure of Alanko et al. while supporting queries four to seven\ntimes faster, and has competitive query time with all the remaining structures.",
            "author": [
                "Philip Bille",
                "Inge Li G\u00f8rtz",
                "Tord Stordalen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19702v3",
                "http://arxiv.org/pdf/2310.19702v3"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19691v1",
            "title": "Causal Context Connects Counterfactual Fairness to Robust Prediction and\n  Group Fairness",
            "updated": "2023-10-30T16:07:57Z",
            "published": "2023-10-30T16:07:57Z",
            "summary": "Counterfactual fairness requires that a person would have been classified in\nthe same way by an AI or other algorithmic system if they had a different\nprotected class, such as a different race or gender. This is an intuitive\nstandard, as reflected in the U.S. legal system, but its use is limited because\ncounterfactuals cannot be directly observed in real-world data. On the other\nhand, group fairness metrics (e.g., demographic parity or equalized odds) are\nless intuitive but more readily observed. In this paper, we use $\\textit{causal\ncontext}$ to bridge the gaps between counterfactual fairness, robust\nprediction, and group fairness. First, we motivate counterfactual fairness by\nshowing that there is not necessarily a fundamental trade-off between fairness\nand accuracy because, under plausible conditions, the counterfactually fair\npredictor is in fact accuracy-optimal in an unbiased target distribution.\nSecond, we develop a correspondence between the causal graph of the\ndata-generating process and which, if any, group fairness metrics are\nequivalent to counterfactual fairness. Third, we show that in three common\nfairness contexts$\\unicode{x2013}$measurement error, selection on label, and\nselection on predictors$\\unicode{x2013}$counterfactual fairness is equivalent\nto demographic parity, equalized odds, and calibration, respectively.\nCounterfactual fairness can sometimes be tested by measuring relatively simple\ngroup fairness metrics.",
            "author": [
                "Jacy Reese Anthis",
                "Victor Veitch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19691v1",
                "http://arxiv.org/pdf/2310.19691v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19681v1",
            "title": "Distributed multi-UAV shield formation based on virtual surface\n  constraints",
            "updated": "2023-10-30T16:01:08Z",
            "published": "2023-10-30T16:01:08Z",
            "summary": "This paper proposes a method for the deployment of a multi-agent system of\nunmanned aerial vehicles (UAVs) as a shield with potential applications in the\nprotection of infrastructures. For this purpose, a distributed control law\nbased on the gradient of a potential function is proposed to acquire the\ndesired shield shape, which is modeled as a quadric surface in the 3D space.\nThe graph of the formation is a Delaunay triangulation, which guarantees the\nformation to be rigid. An algorithm is proposed to design the formation (target\ndistances between agents and interconnections) to distribute the agents over\nthe virtual surface, where the input parameters are just the parametrization of\nthe quadric and the number of agents of the system. Proofs of system stability\nwith the proposed control law are provided, as well as a new method to\nguarantee that the resulting triangulation over the surface is Delaunay, which\ncan be executed locally. Simulation and experimental results illustrate the\neffectiveness of the proposed approach.",
            "author": [
                "Mar\u00eda Guinaldo",
                "Jos\u00e9 S\u00e1nchez-Moreno",
                "Salvador Zaragoza",
                "Francisco Jos\u00e9 Ma\u00f1as-\u00c1lvarez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19681v1",
                "http://arxiv.org/pdf/2310.19681v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16127v1",
            "title": "SeamlessNeRF: Stitching Part NeRFs with Gradient Propagation",
            "updated": "2023-10-30T15:52:35Z",
            "published": "2023-10-30T15:52:35Z",
            "summary": "Neural Radiance Fields (NeRFs) have emerged as promising digital mediums of\n3D objects and scenes, sparking a surge in research to extend the editing\ncapabilities in this domain. The task of seamless editing and merging of\nmultiple NeRFs, resembling the ``Poisson blending'' in 2D image editing,\nremains a critical operation that is under-explored by existing work. To fill\nthis gap, we propose SeamlessNeRF, a novel approach for seamless appearance\nblending of multiple NeRFs. In specific, we aim to optimize the appearance of a\ntarget radiance field in order to harmonize its merge with a source field. We\npropose a well-tailored optimization procedure for blending, which is\nconstrained by 1) pinning the radiance color in the intersecting boundary area\nbetween the source and target fields and 2) maintaining the original gradient\nof the target. Extensive experiments validate that our approach can effectively\npropagate the source appearance from the boundary area to the entire target\nfield through the gradients. To the best of our knowledge, SeamlessNeRF is the\nfirst work that introduces gradient-guided appearance editing to radiance\nfields, offering solutions for seamless stitching of 3D objects represented in\nNeRFs.",
            "author": [
                "Bingchen Gong",
                "Yuehao Wang",
                "Xiaoguang Han",
                "Qi Dou"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3610548.3618238",
                "http://arxiv.org/abs/2311.16127v1",
                "http://arxiv.org/pdf/2311.16127v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19666v1",
            "title": "Dynamic Tensor Decomposition via Neural Diffusion-Reaction Processes",
            "updated": "2023-10-30T15:49:45Z",
            "published": "2023-10-30T15:49:45Z",
            "summary": "Tensor decomposition is an important tool for multiway data analysis. In\npractice, the data is often sparse yet associated with rich temporal\ninformation. Existing methods, however, often under-use the time information\nand ignore the structural knowledge within the sparsely observed tensor\nentries. To overcome these limitations and to better capture the underlying\ntemporal structure, we propose Dynamic EMbedIngs fOr dynamic Tensor\ndEcomposition (DEMOTE). We develop a neural diffusion-reaction process to\nestimate dynamic embeddings for the entities in each tensor mode. Specifically,\nbased on the observed tensor entries, we build a multi-partite graph to encode\nthe correlation between the entities. We construct a graph diffusion process to\nco-evolve the embedding trajectories of the correlated entities and use a\nneural network to construct a reaction process for each individual entity. In\nthis way, our model can capture both the commonalities and personalities during\nthe evolution of the embeddings for different entities. We then use a neural\nnetwork to model the entry value as a nonlinear function of the embedding\ntrajectories. For model estimation, we combine ODE solvers to develop a\nstochastic mini-batch learning algorithm. We propose a stratified sampling\nmethod to balance the cost of processing each mini-batch so as to improve the\noverall efficiency. We show the advantage of our approach in both simulation\nstudy and real-world applications. The code is available at\nhttps://github.com/wzhut/Dynamic-Tensor-Decomposition-via-Neural-Diffusion-Reaction-Processes.",
            "author": [
                "Zheng Wang",
                "Shikai Fang",
                "Shibo Li",
                "Shandian Zhe"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19666v1",
                "http://arxiv.org/pdf/2310.19666v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19662v1",
            "title": "Generating synthetic power grids using exponential random graphs models",
            "updated": "2023-10-30T15:43:07Z",
            "published": "2023-10-30T15:43:07Z",
            "summary": "Synthetic power grids enable secure, real-world energy system simulations and\nare crucial for algorithm testing, resilience assessment, and policy\nformulation. We propose a novel method for the generation of synthetic\ntransmission power grids using Exponential Random Graph (ERG) models. Our two\nmain contributions are: (1) the formulation of an ERG model tailored\nspecifically for capturing the topological nuances of power grids, and (2) a\ngeneral procedure for estimating the parameters of such a model conditioned on\nworking with connected graphs. From a modeling perspective, we identify the\nedge counts per bus type and $k$-triangles as crucial topological\ncharacteristics for synthetic power grid generation. From a technical\nperspective, we develop a rigorous methodology to estimate the parameters of an\nERG constrained to the space of connected graphs. The proposed model is\nflexible, easy to implement, and successfully captures the desired topological\nproperties of power grids.",
            "author": [
                "Francesco Giacomarra",
                "Gianmarco Bet",
                "Alessandro Zocca"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19662v1",
                "http://arxiv.org/pdf/2310.19662v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.PR",
                "math.ST",
                "physics.data-an",
                "stat.AP",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19658v1",
            "title": "Explaining Tree Model Decisions in Natural Language for Network\n  Intrusion Detection",
            "updated": "2023-10-30T15:40:34Z",
            "published": "2023-10-30T15:40:34Z",
            "summary": "Network intrusion detection (NID) systems which leverage machine learning\nhave been shown to have strong performance in practice when used to detect\nmalicious network traffic. Decision trees in particular offer a strong balance\nbetween performance and simplicity, but require users of NID systems to have\nbackground knowledge in machine learning to interpret. In addition, they are\nunable to provide additional outside information as to why certain features may\nbe important for classification.\n  In this work, we explore the use of large language models (LLMs) to provide\nexplanations and additional background knowledge for decision tree NID systems.\nFurther, we introduce a new human evaluation framework for decision tree\nexplanations, which leverages automatically generated quiz questions that\nmeasure human evaluators' understanding of decision tree inference. Finally, we\nshow LLM generated decision tree explanations correlate highly with human\nratings of readability, quality, and use of background knowledge while\nsimultaneously providing better understanding of decision boundaries.",
            "author": [
                "Noah Ziems",
                "Gang Liu",
                "John Flanagan",
                "Meng Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19658v1",
                "http://arxiv.org/pdf/2310.19658v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19636v1",
            "title": "Leave No Stone Unturned: Mine Extra Knowledge for Imbalanced Facial\n  Expression Recognition",
            "updated": "2023-10-30T15:26:26Z",
            "published": "2023-10-30T15:26:26Z",
            "summary": "Facial expression data is characterized by a significant imbalance, with most\ncollected data showing happy or neutral expressions and fewer instances of fear\nor disgust. This imbalance poses challenges to facial expression recognition\n(FER) models, hindering their ability to fully understand various human\nemotional states. Existing FER methods typically report overall accuracy on\nhighly imbalanced test sets but exhibit low performance in terms of the mean\naccuracy across all expression classes. In this paper, our aim is to address\nthe imbalanced FER problem. Existing methods primarily focus on learning\nknowledge of minor classes solely from minor-class samples. However, we propose\na novel approach to extract extra knowledge related to the minor classes from\nboth major and minor class samples. Our motivation stems from the belief that\nFER resembles a distribution learning task, wherein a sample may contain\ninformation about multiple classes. For instance, a sample from the major class\nsurprise might also contain useful features of the minor class fear. Inspired\nby that, we propose a novel method that leverages re-balanced attention maps to\nregularize the model, enabling it to extract transformation invariant\ninformation about the minor classes from all training samples. Additionally, we\nintroduce re-balanced smooth labels to regulate the cross-entropy loss, guiding\nthe model to pay more attention to the minor classes by utilizing the extra\ninformation regarding the label distribution of the imbalanced training data.\nExtensive experiments on different datasets and backbones show that the two\nproposed modules work together to regularize the model and achieve\nstate-of-the-art performance under the imbalanced FER task. Code is available\nat https://github.com/zyh-uaiaaaa.",
            "author": [
                "Yuhang Zhang",
                "Yaqi Li",
                "Lixiong Qin",
                "Xuannan Liu",
                "Weihong Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19636v1",
                "http://arxiv.org/pdf/2310.19636v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19634v1",
            "title": "Iris: Dynamic Privacy Preserving Search in Structured Peer-to-Peer\n  Networks",
            "updated": "2023-10-30T15:24:47Z",
            "published": "2023-10-30T15:24:47Z",
            "summary": "In structured peer-to-peer networks like Chord, the users manage to retrieve\nthe information they seek by asking other nodes from the network for the\ninformation they search. Revealing to other nodes the search target makes\nstructured peer-to-peer networks unsuitable for applications that demand query\nprivacy, i.e., hiding the query's target from the intermediate nodes that take\npart in the routing. This paper studies the query privacy of structured P2P\nnetworks, particularly the Chord protocol.\n  We initially observe that already proposed privacy notions, such as\n$k$-anonymity, do not allow us to reason about the privacy guarantees of a\nquery in Chord in the presence of a strong adversary. Thus, we introduce a new\nprivacy notion that we call $(\\alpha,\\delta)$-privacy that allows us to\nevaluate the privacy guarantees even when considering the worst-case scenario\nregarding an attacker's background knowledge.\n  We then design Iris, an algorithm that allows a requester to conceal the\ntarget of a query in Chord from the intermediate nodes that take part in the\nrouting. Iris achieves that by having the requester query for other than the\ntarget addresses so as reaching each one of them allows the requester to get\ncloser to the target address.\n  We perform a security analysis of the proposed algorithm, based on the\nprivacy notion we introduce. We also develop a prototype of the algorithm in\nMatlab and evaluate its performance. Our analysis proves Iris to be\n$(\\alpha,\\delta)$-private while introducing a modest performance overhead.",
            "author": [
                "Angeliki Aktypi",
                "Kasper Rasmussen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19634v1",
                "http://arxiv.org/pdf/2310.19634v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19626v1",
            "title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for\n  Arts and Humanities",
            "updated": "2023-10-30T15:19:15Z",
            "published": "2023-10-30T15:19:15Z",
            "summary": "Recent advances in artificial general intelligence (AGI), particularly large\nlanguage models and creative image generation systems have demonstrated\nimpressive capabilities on diverse tasks spanning the arts and humanities.\nHowever, the swift evolution of AGI has also raised critical questions about\nits responsible deployment in these culturally significant domains\ntraditionally seen as profoundly human. This paper provides a comprehensive\nanalysis of the applications and implications of AGI for text, graphics, audio,\nand video pertaining to arts and the humanities. We survey cutting-edge systems\nand their usage in areas ranging from poetry to history, marketing to film, and\ncommunication to classical art. We outline substantial concerns pertaining to\nfactuality, toxicity, biases, and public safety in AGI systems, and propose\nmitigation strategies. The paper argues for multi-stakeholder collaboration to\nensure AGI promotes creativity, knowledge, and cultural values without\nundermining truth or human dignity. Our timely contribution summarizes a\nrapidly developing field, highlighting promising directions while advocating\nfor responsible progress centering on human flourishing. The analysis lays the\ngroundwork for further research on aligning AGI's technological capacities with\nenduring social goods.",
            "author": [
                "Zhengliang Liu",
                "Yiwei Li",
                "Qian Cao",
                "Junwen Chen",
                "Tianze Yang",
                "Zihao Wu",
                "John Hale",
                "John Gibbs",
                "Khaled Rasheed",
                "Ninghao Liu",
                "Gengchen Mai",
                "Tianming Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19626v1",
                "http://arxiv.org/pdf/2310.19626v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "J.5; I.2.7; I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19598v1",
            "title": "The Last Iterate Convergence of Stochastic Gradient Descent with\n  Momentum: From a Continuous-Time Perspective",
            "updated": "2023-10-30T14:54:24Z",
            "published": "2023-10-30T14:54:24Z",
            "summary": "In this paper, we study the stochastic optimization problem from a\ncontinuous-time perspective. We propose a stochastic first-order algorithm,\ncalled Stochastic Gradient Descent with Momentum (SGDM), and show that the\nsequence $\\{x_k\\}$ generated by SGDM, despite its stochastic nature, converges\nto a deterministic second-order Ordinary Differential Equation (ODE) in\n$L_2$-norm, as the stepsize goes to zero. The connection between the ODE and\nthe algorithm results in delightful patterns in the discrete-time convergence\nanalysis. More specifically, we develop convergence results for the ODE through\na Lyapunov function, and translate the whole argument to the discrete-time\ncase. This approach yields an $\\widetilde{\\mathcal{O}}\\left(\\frac{\\log\n\\frac{1}{\\beta}}{\\sqrt{k}}\\right)$ anytime last-iterate convergence guarantee\nfor SGDM, where $k$ is the number of iterates and $1-\\beta$ is the desired\nsuccess probability. Notably, the Lyapunov argument helps us to remove both the\nprojection step and the bounded gradient assumption, and our algorithm does not\nrely on quantities that cannot be known in general. Additionally, we prove that\na subsequence of $\\{x_k\\}$ generated by SGDM converges in expectation at rate\n$o\\left(\\frac{1}{\\sqrt{k}\\log\\log k}\\right)$. To the best of our knowledge,\nboth of these results, enabled by the continuous-time perspective, improve\nexisting works in the field.",
            "author": [
                "Yasong Feng",
                "Tianyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19598v1",
                "http://arxiv.org/pdf/2310.19598v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19594v1",
            "title": "Superpolynomial smoothed complexity of 3-FLIP in Local Max-Cut",
            "updated": "2023-10-30T14:51:40Z",
            "published": "2023-10-30T14:51:40Z",
            "summary": "We construct a graph with $n$ vertices where the smoothed runtime of the\n3-FLIP algorithm for the 3-Opt Local Max-Cut problem can be as large as\n$2^{\\Omega(\\sqrt{n})}$. This provides the first example where a local search\nalgorithm for the Max-Cut problem can fail to be efficient in the framework of\nsmoothed analysis. We also give a new construction of graphs where the runtime\nof the FLIP algorithm for the Local Max-Cut problem is $2^{\\Omega(n)}$ for any\npivot rule. This graph is much smaller and has a simpler structure than\nprevious constructions.",
            "author": [
                "Lukas Michel",
                "Alex Scott"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19594v1",
                "http://arxiv.org/pdf/2310.19594v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19589v2",
            "title": "Modeling Dynamics over Meshes with Gauge Equivariant Nonlinear Message\n  Passing",
            "updated": "2023-11-03T02:20:30Z",
            "published": "2023-10-30T14:45:59Z",
            "summary": "Data over non-Euclidean manifolds, often discretized as surface meshes,\nnaturally arise in computer graphics and biological and physical systems. In\nparticular, solutions to partial differential equations (PDEs) over manifolds\ndepend critically on the underlying geometry. While graph neural networks have\nbeen successfully applied to PDEs, they do not incorporate surface geometry and\ndo not consider local gauge symmetries of the manifold. Alternatively, recent\nworks on gauge equivariant convolutional and attentional architectures on\nmeshes leverage the underlying geometry but underperform in modeling surface\nPDEs with complex nonlinear dynamics. To address these issues, we introduce a\nnew gauge equivariant architecture using nonlinear message passing. Our novel\narchitecture achieves higher performance than either convolutional or\nattentional networks on domains with highly complex and nonlinear dynamics.\nHowever, similar to the non-mesh case, design trade-offs favor convolutional,\nattentional, or message passing networks for different tasks; we investigate in\nwhich circumstances our message passing method provides the most benefit.",
            "author": [
                "Jung Yeon Park",
                "Lawson L. S. Wong",
                "Robin Walters"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19589v2",
                "http://arxiv.org/pdf/2310.19589v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19583v2",
            "title": "GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View\n  Stereo",
            "updated": "2023-11-29T14:33:09Z",
            "published": "2023-10-30T14:41:53Z",
            "summary": "Traditional multi-view stereo (MVS) methods rely heavily on photometric and\ngeometric consistency constraints, but newer machine learning-based MVS methods\ncheck geometric consistency across multiple source views only as a\npost-processing step. In this paper, we present a novel approach that\nexplicitly encourages geometric consistency of reference view depth maps across\nmultiple source views at different scales during learning (see Fig. 1). We find\nthat adding this geometric consistency loss significantly accelerates learning\nby explicitly penalizing geometrically inconsistent pixels, reducing the\ntraining iteration requirements to nearly half that of other MVS methods. Our\nextensive experiments show that our approach achieves a new state-of-the-art on\nthe DTU and BlendedMVS datasets, and competitive results on the Tanks and\nTemples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt\nto enforce multi-view, multi-scale geometric consistency during learning.",
            "author": [
                "Vibhas K. Vats",
                "Sripad Joshi",
                "David J. Crandall",
                "Md. Alimoor Reza",
                "Soon-heung Jung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19583v2",
                "http://arxiv.org/pdf/2310.19583v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19569v1",
            "title": "Stratified Ehrhart ring theory on periodic graphs",
            "updated": "2023-10-30T14:24:36Z",
            "published": "2023-10-30T14:24:36Z",
            "summary": "We investigate the \"stratified Ehrhart ring theory\" for periodic graphs,\nwhich gives an algorithm for determining the growth sequences of periodic\ngraphs. The growth sequence $(s_{\\Gamma, x_0, i})_{i \\ge 0}$ is defined for a\ngraph $\\Gamma$ and its fixed vertex $x_0$, where $s_{\\Gamma, x_0, i}$ is\ndefined as the number of vertices of $\\Gamma$ at distance $i$ from $x_0$.\nAlthough the sequences $(s_{\\Gamma, x_0, i})_{i \\ge 0}$ for periodic graphs are\nknown to be of quasi-polynomial type, their determination had not been\nestablished, even in dimension two. Our algorithm and the proofs are based on\nalgebraic combinatorics, analogous to the Ehrhart theory. As an application of\nthe algorithm, we determine the growth sequences in several new examples.",
            "author": [
                "Takuya Inoue",
                "Yusuke Nakamura"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19569v1",
                "http://arxiv.org/pdf/2310.19569v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AC",
                "Primary 05A15, Secondary 52B20, 05C30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19559v2",
            "title": "Disentangled Counterfactual Learning for Physical Audiovisual\n  Commonsense Reasoning",
            "updated": "2023-11-02T02:36:12Z",
            "published": "2023-10-30T14:16:34Z",
            "summary": "In this paper, we propose a Disentangled Counterfactual Learning~(DCL)\napproach for physical audiovisual commonsense reasoning. The task aims to infer\nobjects' physics commonsense based on both video and audio input, with the main\nchallenge is how to imitate the reasoning ability of humans. Most of the\ncurrent methods fail to take full advantage of different characteristics in\nmulti-modal data, and lacking causal reasoning ability in models impedes the\nprogress of implicit physical knowledge inferring. To address these issues, our\nproposed DCL method decouples videos into static (time-invariant) and dynamic\n(time-varying) factors in the latent space by the disentangled sequential\nencoder, which adopts a variational autoencoder (VAE) to maximize the mutual\ninformation with a contrastive loss function. Furthermore, we introduce a\ncounterfactual learning module to augment the model's reasoning ability by\nmodeling physical knowledge relationships among different objects under\ncounterfactual intervention. Our proposed method is a plug-and-play module that\ncan be incorporated into any baseline. In experiments, we show that our\nproposed method improves baseline methods and achieves state-of-the-art\nperformance. Our source code is available at https://github.com/Andy20178/DCL.",
            "author": [
                "Changsheng Lv",
                "Shuai Zhang",
                "Yapeng Tian",
                "Mengshi Qi",
                "Huadong Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19559v2",
                "http://arxiv.org/pdf/2310.19559v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19537v2",
            "title": "On consequences of finetuning on data with highly discriminative\n  features",
            "updated": "2023-11-15T22:09:08Z",
            "published": "2023-10-30T13:43:50Z",
            "summary": "In the era of transfer learning, training neural networks from scratch is\nbecoming obsolete. Transfer learning leverages prior knowledge for new tasks,\nconserving computational resources. While its advantages are well-documented,\nwe uncover a notable drawback: networks tend to prioritize basic data patterns,\nforsaking valuable pre-learned features. We term this behavior \"feature\nerosion\" and analyze its impact on network performance and internal\nrepresentations.",
            "author": [
                "Wojciech Masarczyk",
                "Tomasz Trzci\u0144ski",
                "Mateusz Ostaszewski"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19537v2",
                "http://arxiv.org/pdf/2310.19537v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19521v1",
            "title": "Maximum principle preserving time implicit DGSEM for linear scalar\n  hyperbolic conservation laws",
            "updated": "2023-10-30T13:21:46Z",
            "published": "2023-10-30T13:21:46Z",
            "summary": "We investigate the properties of the high-order discontinuous Galerkin\nspectral element method (DGSEM) with implicit backward-Euler time stepping for\nthe approximation of hyperbolic linear scalar conservation equation in multiple\nspace dimensions. We first prove that the DGSEM scheme in one space dimension\npreserves a maximum principle for the cell-averaged solution when the time step\nis large enough. This property however no longer holds in multiple space\ndimensions and we propose to use the flux-corrected transport limiting [Boris\nand Book, J. Comput. Phys., 11 (1973)] based on a low-order approximation using\ngraph viscosity to impose a maximum principle on the cell-averaged solution.\nThese results allow to use a linear scaling limiter [Zhang and Shu, J. Comput.\nPhys., 229 (2010)] in order to impose a maximum principle at nodal values\nwithin elements. Then, we investigate the inversion of the linear systems\nresulting from the time implicit discretization at each time step. We prove\nthat the diagonal blocks are invertible and provide efficient algorithms for\ntheir inversion. Numerical experiments in one and two space dimensions are\npresented to illustrate the conclusions of the present analyses.",
            "author": [
                "Riccardo Milani",
                "Florent Renac",
                "Jean Ruel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19521v1",
                "http://arxiv.org/pdf/2310.19521v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65M12, 65M70, 76T10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19514v3",
            "title": "Approximate Earth Mover's Distance in Truly-Subquadratic Time",
            "updated": "2023-11-10T22:53:39Z",
            "published": "2023-10-30T13:13:03Z",
            "summary": "We design an additive approximation scheme for estimating the cost of the\nmin-weight bipartite matching problem: given a bipartite graph with\nnon-negative edge costs and $\\varepsilon > 0$, our algorithm estimates the cost\nof matching all but $O(\\varepsilon)$-fraction of the vertices in truly\nsubquadratic time $O(n^{2-\\delta(\\varepsilon)})$.\n  Our algorithm has a natural interpretation for computing the Earth Mover's\nDistance (EMD), up to a $\\varepsilon$-additive approximation. Notably, we make\nno assumptions about the underlying metric (more generally, the costs do not\nhave to satisfy triangle inequality). Note that compared to the size of the\ninstance (an arbitrary $n \\times n$ cost matrix), our algorithm runs in {\\em\nsublinear} time.\n  Our algorithm can approximate a slightly more general problem:\nmax-cardinality bipartite matching with a knapsack constraint, where the goal\nis to maximize the number of vertices that can be matched up to a total cost\n$B$.",
            "author": [
                "Lorenzo Beretta",
                "Aviad Rubinstein"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19514v3",
                "http://arxiv.org/pdf/2310.19514v3"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19503v2",
            "title": "Trust, Accountability, and Autonomy in Knowledge Graph-based AI for\n  Self-determination",
            "updated": "2023-10-31T09:16:13Z",
            "published": "2023-10-30T12:51:52Z",
            "summary": "Knowledge Graphs (KGs) have emerged as fundamental platforms for powering\nintelligent decision-making and a wide range of Artificial Intelligence (AI)\nservices across major corporations such as Google, Walmart, and AirBnb. KGs\ncomplement Machine Learning (ML) algorithms by providing data context and\nsemantics, thereby enabling further inference and question-answering\ncapabilities. The integration of KGs with neuronal learning (e.g., Large\nLanguage Models (LLMs)) is currently a topic of active research, commonly named\nneuro-symbolic AI. Despite the numerous benefits that can be accomplished with\nKG-based AI, its growing ubiquity within online services may result in the loss\nof self-determination for citizens as a fundamental societal issue. The more we\nrely on these technologies, which are often centralised, the less citizens will\nbe able to determine their own destinies. To counter this threat, AI\nregulation, such as the European Union (EU) AI Act, is being proposed in\ncertain regions. The regulation sets what technologists need to do, leading to\nquestions concerning: How can the output of AI systems be trusted? What is\nneeded to ensure that the data fuelling and the inner workings of these\nartefacts are transparent? How can AI be made accountable for its\ndecision-making? This paper conceptualises the foundational topics and research\npillars to support KG-based AI for self-determination. Drawing upon this\nconceptual framework, challenges and opportunities for citizen\nself-determination are illustrated and analysed in a real-world scenario. As a\nresult, we propose a research agenda aimed at accomplishing the recommended\nobjectives.",
            "author": [
                "Luis-Daniel Ib\u00e1\u00f1ez",
                "John Domingue",
                "Sabrina Kirrane",
                "Oshani Seneviratne",
                "Aisling Third",
                "Maria-Esther Vidal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19503v2",
                "http://arxiv.org/pdf/2310.19503v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CY",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19500v1",
            "title": "Coarse-grained crystal graph neural networks for reticular materials\n  design",
            "updated": "2023-10-30T12:48:45Z",
            "published": "2023-10-30T12:48:45Z",
            "summary": "Reticular materials, including metal-organic frameworks and covalent organic\nframeworks, combine relative ease of synthesis and impressive range of\napplications in various fields, from gas storage to biomedicine. Diverse\nproperties arise from the variation of building units, metal centers and\norganic linkers, in an almost infinite chemical space. Such a variability\nsubstantially complicates experimental design and promotes the use of\ncomputational methods. In particular, the most successful artificial\nintelligence algorithms for predicting properties of reticular materials are\natomic-level graph neural networks with optional domain knowledge. Nonetheless,\nthe data-driven inverse design utilizing such models suffers from incorporating\nirrelevant and redundant features such as full atomistic graph and network\ntopology. In this study, we propose a new way of representing materials, aiming\nto overcome the limitations of existing methods; the message passing is\nperformed on the coarse-grained crystal graph that comprises molecular building\nunits. We assess the predictive performance and energy efficiency of neural\nnetworks built on different materials representations, including\ncomposition-based and crystal-structure-aware models, to highlight the merits\nof our approach. Coarse-grained crystal graph neural networks show decent\naccuracy at low computational costs, making them a valuable alternative to\nomnipresent atomic-level algorithms. Moreover, the presented models can be\nsuccessfully integrated into the inverse materials design pipeline as\nestimators of the objective function. Overall, the coarse-grained crystal graph\nframework aims to challenge the prevailing atomic-centric perspective on\nreticular materials design.",
            "author": [
                "Vadim Korolev",
                "Artem Mitrofanov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19500v1",
                "http://arxiv.org/pdf/2310.19500v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19482v1",
            "title": "The dimension of the region of feasible tournament profiles",
            "updated": "2023-10-30T12:11:02Z",
            "published": "2023-10-30T12:11:02Z",
            "summary": "Erd\\H os, Lov\\'asz and Spencer showed in the late 1970s that the dimension of\nthe region of $k$-vertex graph profiles, i.e., the region of feasible densities\nof $k$-vertex graphs in large graphs, is equal to the number of non-trivial\nconnected graphs with at most $k$ vertices. We determine the dimension of the\nregion of $k$-vertex tournament profiles. Our result, which explores an\ninteresting connection to Lyndon words, yields that the dimension is much\nlarger than just the number of strongly connected tournaments, which would be\nthe answer expected as the analogy to the setting of graphs.",
            "author": [
                "Daniel Kral",
                "Ander Lamaison",
                "Magdalena Prorok",
                "Xichao Shu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19482v1",
                "http://arxiv.org/pdf/2310.19482v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19474v1",
            "title": "Structure-Informed Neural Networks for Boundary Observation Problems",
            "updated": "2023-10-30T12:01:41Z",
            "published": "2023-10-30T12:01:41Z",
            "summary": "We introduce Structure Informed Neural Networks (SINNs), a novel method for\nsolving boundary observation problems involving PDEs. The SINN methodology is a\ndata-driven framework for creating approximate solutions to internal variables\non the interior of a domain, given only boundary data. The key idea is to use\nneural networks to identify a co-ordinate transformation to a latent space,\nupon which a well-posed elliptic system of partial differential equations is\nconstructed. The use of elliptic systems enables the low-cost transfer of\ninformation from the domain's boundary to its interior. This enables\napproximate solutions to PDE boundary observation problems to be constructed\nfor generic, and even ill-posed, problems. A further advantage of the proposed\nmethod is its ability to be trained on experimental or numerical data without\nany knowledge of the underlying PDE. We demonstrate the ability of SINNs to\naccurately solve boundary observation problems by considering two challenging\nexamples of a non-linear heat equation and boundary observation for the\nNavier-Stokes equations.",
            "author": [
                "Jakub Horsky",
                "Andrew Wynn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19474v1",
                "http://arxiv.org/pdf/2310.19474v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19472v1",
            "title": "Arc connectivity and submodular flows in digraphs",
            "updated": "2023-10-30T11:59:18Z",
            "published": "2023-10-30T11:59:18Z",
            "summary": "Let $D=(V,A)$ be a digraph. For an integer $k\\geq 1$, a $k$-arc-connected\nflip is an arc subset of $D$ such that after reversing the arcs in it the\ndigraph becomes (strongly) $k$-arc-connected.\n  The first main result of this paper introduces a sufficient condition for the\nexistence of a $k$-arc-connected flip that is also a submodular flow for a\ncrossing submodular function. More specifically, given some integer $\\tau\\geq\n1$, suppose $d_A^+(U)+(\\frac{\\tau}{k}-1)d_A^-(U)\\geq \\tau$ for all $U\\subsetneq\nV, U\\neq \\emptyset$, where $d_A^+(U)$ and $d_A^-(U)$ denote the number of arcs\nin $A$ leaving and entering $U$, respectively. Let $\\mathcal{C}$ be a crossing\nfamily over ground set $V$, and let $f:\\mathcal{C}\\to \\mathbb{Z}$ be a crossing\nsubmodular function such that $f(U)\\geq \\frac{k}{\\tau}(d_A^+(U)-d_A^-(U))$ for\nall $U\\in \\mathcal{C}$. Then $D$ has a $k$-arc-connected flip $J$ such that\n$f(U)\\geq d_J^+(U)-d_J^-(U)$ for all $U\\in \\mathcal{C}$. The result has several\napplications to Graph Orientations and Combinatorial Optimization. In\nparticular, it strengthens Nash-Williams' so-called weak orientation theorem,\nand proves a weaker variant of Woodall's conjecture on digraphs whose\nunderlying undirected graph is $\\tau$-edge-connected.\n  The second main result of this paper is even more general. It introduces a\nsufficient condition for the existence of capacitated integral solutions to the\nintersection of two submodular flow systems. This sufficient condition implies\nthe classic result of Edmonds and Giles on the box-total dual integrality of a\nsubmodular flow system. It also has the consequence that in a weakly connected\ndigraph, the intersection of two submodular flow systems is totally dual\nintegral.",
            "author": [
                "Ahmad Abdi",
                "G\u00e9rard Cornu\u00e9jols",
                "Giacomo Zambelli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19472v1",
                "http://arxiv.org/pdf/2310.19472v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.OC",
                "90-XX, 05-XX"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19455v1",
            "title": "Arborescences, Colorful Forests, and Popularity",
            "updated": "2023-10-30T11:29:02Z",
            "published": "2023-10-30T11:29:02Z",
            "summary": "Our input is a directed, rooted graph $G = (V \\cup \\{r\\},E)$ where each\nvertex in $V$ has a partial order preference over its incoming edges. The\npreferences of a vertex extend naturally to preferences over arborescences\nrooted at $r$. We seek a popular arborescence in $G$, i.e., one for which there\nis no \"more popular\" arborescence. Popular arborescences have applications in\nliquid democracy or collective decision making; however, they need not exist in\nevery input instance. The popular arborescence problem is to decide if a given\ninput instance admits a popular arborescence or not. We show a polynomial-time\nalgorithm for this problem, whose computational complexity was not known\npreviously.\n  Our algorithm is combinatorial, and can be regarded as a primal-dual\nalgorithm. It searches for an arborescence along with its dual certificate, a\nchain of subsets of $E$, witnessing its popularity. In fact, our algorithm\nsolves the more general popular common base problem in the intersection of two\nmatroids, where one matroid is the partition matroid defined by any partition\n$E = \\bigcup_{v\\in V} \\delta(v)$ and the other is an arbitrary matroid on $E$\nof rank $|V|$, with each $v \\in V$ having a partial order over elements in\n$\\delta(v)$. We extend our algorithm to the case with forced or forbidden\nedges.\n  We also study the related popular colorful forest (or more generally, the\npopular common independent set) problem where edges are partitioned into color\nclasses, and the task is to find a colorful forest that is popular within the\nset of all colorful forests. For the case with weak rankings, we formulate the\npopular colorful forest polytope, and thus show that a minimum-cost popular\ncolorful forest can be computed efficiently. By contrast, we prove that it is\nNP-hard to compute a minimum-cost popular arborescence, even when rankings are\nstrict.",
            "author": [
                "Telikepalli Kavitha",
                "Kazuhisa Makino",
                "Ildik\u00f3 Schlotter",
                "Yu Yokoi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19455v1",
                "http://arxiv.org/pdf/2310.19455v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19453v2",
            "title": "FLIP: Towards Fine-grained Alignment between ID-based Models and\n  Pretrained Language Models for CTR Prediction",
            "updated": "2023-11-26T12:40:27Z",
            "published": "2023-10-30T11:25:03Z",
            "summary": "Click-through rate (CTR) prediction plays as a core function module in\nvarious personalized online services. The traditional ID-based models for CTR\nprediction take as inputs the one-hot encoded ID features of tabular modality,\nwhich capture the collaborative signals via feature interaction modeling. But\nthe one-hot encoding discards the semantic information conceived in the\noriginal feature texts. Recently, the emergence of Pretrained Language Models\n(PLMs) has given rise to another paradigm, which takes as inputs the sentences\nof textual modality obtained by hard prompt templates and adopts PLMs to\nextract the semantic knowledge. However, PLMs generally tokenize the input text\ndata into subword tokens and ignore field-wise collaborative signals.\nTherefore, these two lines of research focus on different characteristics of\nthe same input data (i.e., textual and tabular modalities), forming a distinct\ncomplementary relationship with each other. In this paper, we propose to\nconduct Fine-grained feature-level ALignment between ID-based Models and\nPretrained Language Models (FLIP) for CTR prediction. We design a novel joint\nreconstruction pretraining task for both masked language and tabular modeling.\nSpecifically, the masked data of one modality (i.e., tokens or features) has to\nbe recovered with the help of the other modality, which establishes the\nfeature-level interaction and alignment via sufficient mutual information\nextraction between dual modalities. Moreover, we propose to jointly finetune\nthe ID-based model and PLM for downstream CTR prediction tasks, thus achieving\nsuperior performance by combining the advantages of both models. Extensive\nexperiments on three real-world datasets demonstrate that FLIP outperforms SOTA\nbaselines, and is highly compatible for various ID-based models and PLMs.",
            "author": [
                "Hangyu Wang",
                "Jianghao Lin",
                "Xiangyang Li",
                "Bo Chen",
                "Chenxu Zhu",
                "Ruiming Tang",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19453v2",
                "http://arxiv.org/pdf/2310.19453v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19452v1",
            "title": "Incorporating Zero-Knowledge Succinct Non-interactive Argument of\n  Knowledge for Blockchain-based Identity Management with off-chain\n  computations",
            "updated": "2023-10-30T11:24:05Z",
            "published": "2023-10-30T11:24:05Z",
            "summary": "In today's world, secure and efficient biometric authentication is of keen\nimportance. Traditional authentication methods are no longer considered\nreliable due to their susceptibility to cyber-attacks. Biometric\nauthentication, particularly fingerprint authentication, has emerged as a\npromising alternative, but it raises concerns about the storage and use of\nbiometric data, as well as centralized storage, which could make it vulnerable\nto cyber-attacks. In this paper, a novel blockchain-based fingerprint\nauthentication system is proposed that integrates zk-SNARKs, which are\nzero-knowledge proofs that enable secure and efficient authentication without\nrevealing sensitive biometric information. A KNN-based approach on the FVC2002,\nFVC2004 and FVC2006 datasets is used to generate a cancelable template for\nsecure, faster, and robust biometric registration and authentication which is\nstored using the Interplanetary File System. The proposed approach provides an\naverage accuracy of 99.01%, 98.97% and 98.52% over the FVC2002, FVC2004 and\nFVC2006 datasets respectively for fingerprint authentication. Incorporation of\nzk-SNARK facilitates smaller proof size. Overall, the proposed method has the\npotential to provide a secure and efficient solution for blockchain-based\nidentity management.",
            "author": [
                "Pranay Kothari",
                "Deepak Chopra",
                "Manjot Singh",
                "Shivam Bhardwaj",
                "Rudresh Dwivedi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19452v1",
                "http://arxiv.org/pdf/2310.19452v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19450v2",
            "title": "Hodge-Compositional Edge Gaussian Processes",
            "updated": "2023-10-31T11:57:06Z",
            "published": "2023-10-30T11:22:25Z",
            "summary": "We propose principled Gaussian processes (GPs) for modeling functions defined\nover the edge set of a simplicial 2-complex, a structure similar to a graph in\nwhich edges may form triangular faces. This approach is intended for learning\nflow-type data on networks where edge flows can be characterized by the\ndiscrete divergence and curl. Drawing upon the Hodge decomposition, we first\ndevelop classes of divergence-free and curl-free edge GPs, suitable for various\napplications. We then combine them to create \\emph{Hodge-compositional edge\nGPs} that are expressive enough to represent any edge function. These GPs\nfacilitate direct and independent learning for the different Hodge components\nof edge functions, enabling us to capture their relevance during hyperparameter\noptimization. To highlight their practical potential, we apply them for flow\ndata inference in currency exchange, ocean flows and water supply networks,\ncomparing them to alternative models.",
            "author": [
                "Maosheng Yang",
                "Viacheslav Borovitskiy",
                "Elvin Isufi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19450v2",
                "http://arxiv.org/pdf/2310.19450v2"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19444v1",
            "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in\n  Knowledge Distillation",
            "updated": "2023-10-30T11:13:02Z",
            "published": "2023-10-30T11:13:02Z",
            "summary": "Knowledge distillation~(KD) has proven to be a highly effective approach for\nenhancing model performance through a teacher-student training scheme. However,\nmost existing distillation methods are designed under the assumption that the\nteacher and student models belong to the same model family, particularly the\nhint-based approaches. By using centered kernel alignment (CKA) to compare the\nlearned features between heterogeneous teacher and student models, we observe\nsignificant feature divergence. This divergence illustrates the ineffectiveness\nof previous hint-based methods in cross-architecture distillation. To tackle\nthe challenge in distilling heterogeneous models, we propose a simple yet\neffective one-for-all KD framework called OFA-KD, which significantly improves\nthe distillation performance between heterogeneous architectures. Specifically,\nwe project intermediate features into an aligned latent space such as the\nlogits space, where architecture-specific information is discarded.\nAdditionally, we introduce an adaptive target enhancement scheme to prevent the\nstudent from being disturbed by irrelevant information. Extensive experiments\nwith various architectures, including CNN, Transformer, and MLP, demonstrate\nthe superiority of our OFA-KD framework in enabling distillation between\nheterogeneous architectures. Specifically, when equipped with our OFA-KD, the\nstudent models achieve notable performance improvements, with a maximum gain of\n8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code\nand checkpoints can be found at https://github.com/Hao840/OFAKD.",
            "author": [
                "Zhiwei Hao",
                "Jianyuan Guo",
                "Kai Han",
                "Yehui Tang",
                "Han Hu",
                "Yunhe Wang",
                "Chang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19444v1",
                "http://arxiv.org/pdf/2310.19444v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19437v1",
            "title": "Swap-Robust and Almost Supermagic Complete Graphs for Dynamical\n  Distributed Storage",
            "updated": "2023-10-30T10:54:52Z",
            "published": "2023-10-30T10:54:52Z",
            "summary": "To prevent service time bottlenecks in distributed storage systems, the\naccess balancing problem has been studied by designing almost supermagic edge\nlabelings of certain graphs to balance the access requests to different\nservers. In this paper, we introduce the concept of robustness of edge\nlabelings under limited-magnitude swaps, which is important for studying the\ndynamical access balancing problem with respect to changes in data popularity.\nWe provide upper and lower bounds on the robustness ratio for complete graphs\nwith $n$ vertices, and construct $O(n)$-almost supermagic labelings that are\nasymptotically optimal in terms of the robustness ratio.",
            "author": [
                "Xin Wei",
                "Xiande Zhang",
                "Gennian Ge"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19437v1",
                "http://arxiv.org/pdf/2310.19437v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.01606v1",
            "title": "KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic\n  Relations",
            "updated": "2023-10-30T10:53:02Z",
            "published": "2023-10-30T10:53:02Z",
            "summary": "In the current paper, we present the KG-FRUS dataset, comprised of more than\n300,000 US government diplomatic documents encoded in a Knowledge Graph (KG).\nWe leverage the data of the Foreign Relations of the United States (FRUS)\n(available as XML files) to extract information about the documents and the\nindividuals and countries mentioned within them. We use the extracted entities,\nand associated metadata, to create a graph-based dataset. Further, we\nsupplement the created KG with additional entities and relations from Wikidata.\nThe relations in the KG capture the synergies and dynamics required to study\nand understand the complex fields of diplomacy, foreign relations, and\npolitics. This goes well beyond a simple collection of documents which neglects\nthe relations between entities in the text. We showcase a range of\npossibilities of the current dataset by illustrating different approaches to\nprobe the KG. In the paper, we exemplify how to use a query language to answer\nsimple research questions and how to use graph algorithms such as Node2Vec and\nPageRank, that benefit from the complete graph structure. More importantly, the\nchosen structure provides total flexibility for continuously expanding and\nenriching the graph. Our solution is general, so the proposed pipeline for\nbuilding the KG can encode other original corpora of time-dependent and complex\nphenomena. Overall, we present a mechanism to create KG databases providing a\nmore versatile representation of time-dependent related text data and a\nparticular application to the all-important FRUS database.",
            "author": [
                "G\u00f6kberk \u00d6zsoy",
                "Luis Salamanca",
                "Matthew Connelly",
                "Raymond Hicks",
                "Fernando P\u00e9rez-Cruz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.01606v1",
                "http://arxiv.org/pdf/2311.01606v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19436v1",
            "title": "Resummation of local and non-local scalar self energies via the\n  Schwinger-Dyson equation in de Sitter spacetime",
            "updated": "2023-10-30T10:49:34Z",
            "published": "2023-10-30T10:49:34Z",
            "summary": "We consider a massless and minimally coupled self interacting quantum scalar\nfield in the inflationary de Sitter spacetime. The scalar potential is taken to\nbe a hybrid of cubic and quartic self interactions, $V(\\phi)= \\lambda\n\\phi^4/4!+\\beta \\phi^3/3!$ ($\\lambda >0$). Compared to the earlier well studied\n$\\beta=0$ case, the present potential has a rolling down effect due to the\n$\\phi^3$ term, along with the usual bounding effect due to the $\\phi^4$ term.\n$V(\\phi)$ has shapewise qualitative similarity with the standard slow roll\nsingle field inflationary potentials. We begin by constructing the\nSchwinger-Dyson equation for the scalar Feynman propagator up to two loop, at\n${\\cal O}(\\lambda)$, ${\\cal O}(\\beta^2)$, ${\\cal O}(\\lambda^2)$ and ${\\cal\nO}(\\lambda \\beta^2)$. Using this equation, we consider first the local part of\nthe scalar self energy and compute the mass of the scalar field, dynamically\ngenerated via the late time non-perturbative secular logarithms, by resumming\nthe daisy-like graphs. We also argue that unlike the quartic case, considering\nmerely the one loop results for the purpose of resummation does not give us any\nsensible result here. We next construct the non-perturbative two particle\nirreducible effective action up to three loop and derive from it the\nSchwinger-Dyson equation once again. This equation is satisfied by the\nnon-perturbative Feynman propagator. By series expanding this propagator, the\nresummed local part of the self energy is shown to yield the same dynamical\nmass as that of the above. We next use this equation to resum the effect of the\nnon-local part of the scalar self energy in the Feynman propagator, and show\nthat even though the perturbatively corrected propagator shows secular growth\nat late times, there exists a resummed solution which is vanishing for large\nspacelike separations.",
            "author": [
                "Sourav Bhattacharya",
                "Nitin Joshi",
                "Kinsuk Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19436v1",
                "http://arxiv.org/pdf/2310.19436v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19432v1",
            "title": "Explaining the Decisions of Deep Policy Networks for Robotic\n  Manipulations",
            "updated": "2023-10-30T10:44:12Z",
            "published": "2023-10-30T10:44:12Z",
            "summary": "Deep policy networks enable robots to learn behaviors to solve various\nreal-world complex tasks in an end-to-end fashion. However, they lack\ntransparency to provide the reasons of actions. Thus, such a black-box model\noften results in low reliability and disruptive actions during the deployment\nof the robot in practice. To enhance its transparency, it is important to\nexplain robot behaviors by considering the extent to which each input feature\ncontributes to determining a given action. In this paper, we present an\nexplicit analysis of deep policy models through input attribution methods to\nexplain how and to what extent each input feature affects the decisions of the\nrobot policy models. To this end, we present two methods for applying input\nattribution methods to robot policy networks: (1) we measure the importance\nfactor of each joint torque to reflect the influence of the motor torque on the\nend-effector movement, and (2) we modify a relevance propagation method to\nhandle negative inputs and outputs in deep policy networks properly. To the\nbest of our knowledge, this is the first report to identify the dynamic changes\nof input attributions of multi-modal sensor inputs in deep policy networks\nonline for robotic manipulation.",
            "author": [
                "Seongun Kim",
                "Jaesik Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19432v1",
                "http://arxiv.org/pdf/2310.19432v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19425v1",
            "title": "Artificial intelligence and the limits of the humanities",
            "updated": "2023-10-30T10:35:23Z",
            "published": "2023-10-30T10:35:23Z",
            "summary": "The complexity of cultures in the modern world is now beyond human\ncomprehension. Cognitive sciences cast doubts on the traditional explanations\nbased on mental models. The core subjects in humanities may lose their\nimportance. Humanities have to adapt to the digital age. New, interdisciplinary\nbranches of humanities emerge. Instant access to information will be replaced\nby instant access to knowledge. Understanding the cognitive limitations of\nhumans and the opportunities opened by the development of artificial\nintelligence and interdisciplinary research necessary to address global\nchallenges is the key to the revitalization of humanities. Artificial\nintelligence will radically change humanities, from art to political sciences\nand philosophy, making these disciplines attractive to students and enabling\nthem to go beyond current limitations.",
            "author": [
                "W\u0142odzis\u0142aw Duch"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19425v1",
                "http://arxiv.org/pdf/2310.19425v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "68T01",
                "J.5"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19420v1",
            "title": "Mean BERTs make erratic language teachers: the effectiveness of latent\n  bootstrapping in low-resource settings",
            "updated": "2023-10-30T10:31:32Z",
            "published": "2023-10-30T10:31:32Z",
            "summary": "This paper explores the use of latent bootstrapping, an alternative\nself-supervision technique, for pretraining language models. Unlike the typical\npractice of using self-supervision on discrete subwords, latent bootstrapping\nleverages contextualized embeddings for a richer supervision signal. We conduct\nexperiments to assess how effective this approach is for acquiring linguistic\nknowledge from limited resources. Specifically, our experiments are based on\nthe BabyLM shared task, which includes pretraining on two small curated corpora\nand an evaluation on four linguistic benchmarks.",
            "author": [
                "David Samuel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19420v1",
                "http://arxiv.org/pdf/2310.19420v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19406v2",
            "title": "Discovering Black Hole Mass Scaling Relations with Symbolic Regression",
            "updated": "2023-11-20T10:33:36Z",
            "published": "2023-10-30T10:19:38Z",
            "summary": "Our knowledge of supermassive black holes (SMBHs) and their relation to their\nhost galaxies is still limited, and there are only around 150 SMBHs that have\ntheir masses directly measured and confirmed. Better black hole mass scaling\nrelations will help us reveal the physics of black holes, as well as predict\nblack hole masses that are not yet measured. Here, we apply symbolic\nregression, combined with random forest to those directly-measured black hole\nmasses and host galaxy properties, and find a collection of higher-dimensional\n(N-D) black hole mass scaling relations. These N-D black hole mass scaling\nrelations have scatter smaller than any of the existing black hole mass scaling\nrelations. One of the best among them involves the parameters of central\nstellar velocity dispersion, bulge-to-total ratio, and density at the black\nhole's sphere-of-influence with an intrinsic scatter of $\\epsilon=0.083\\,\\\n\\text{dex}$, significantly lower than $\\epsilon \\sim 0.3\\,\\ \\text{dex}$ for the\nM-$\\sigma$ relation. These relations will inspire black hole physics, test\nblack hole models implemented in simulations, and estimate unknown black hole\nmasses on an unprecedented precision.",
            "author": [
                "Zehao Jin",
                "Benjamin L. Davis"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19406v2",
                "http://arxiv.org/pdf/2310.19406v2"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19394v1",
            "title": "LightSAGE: Graph Neural Networks for Large Scale Item Retrieval in\n  Shopee's Advertisement Recommendation",
            "updated": "2023-10-30T09:57:06Z",
            "published": "2023-10-30T09:57:06Z",
            "summary": "Graph Neural Network (GNN) is the trending solution for item retrieval in\nrecommendation problems. Most recent reports, however, focus heavily on new\nmodel architectures. This may bring some gaps when applying GNN in the\nindustrial setup, where, besides the model, constructing the graph and handling\ndata sparsity also play critical roles in the overall success of the project.\nIn this work, we report how GNN is applied for large-scale e-commerce item\nretrieval at Shopee. We introduce our simple yet novel and impactful techniques\nin graph construction, modeling, and handling data skewness. Specifically, we\nconstruct high-quality item graphs by combining strong-signal user behaviors\nwith high-precision collaborative filtering (CF) algorithm. We then develop a\nnew GNN architecture named LightSAGE to produce high-quality items' embeddings\nfor vector search. Finally, we design multiple strategies to handle cold-start\nand long-tail items, which are critical in an advertisement (ads) system. Our\nmodels bring improvement in offline evaluations, online A/B tests, and are\ndeployed to the main traffic of Shopee's Recommendation Advertisement system.",
            "author": [
                "Dang Minh Nguyen",
                "Chenfei Wang",
                "Yan Shen",
                "Yifan Zeng"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3604915.3608863",
                "http://arxiv.org/abs/2310.19394v1",
                "http://arxiv.org/pdf/2310.19394v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG",
                "H.3.3; I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19392v1",
            "title": "A Clinical Guideline Driven Automated Linear Feature Extraction for\n  Vestibular Schwannoma",
            "updated": "2023-10-30T09:54:24Z",
            "published": "2023-10-30T09:54:24Z",
            "summary": "Vestibular Schwannoma is a benign brain tumour that grows from one of the\nbalance nerves. Patients may be treated by surgery, radiosurgery or with a\nconservative \"wait-and-scan\" strategy. Clinicians typically use manually\nextracted linear measurements to aid clinical decision making. This work aims\nto automate and improve this process by using deep learning based segmentation\nto extract relevant clinical features through computational algorithms. To the\nbest of our knowledge, our study is the first to propose an automated approach\nto replicate local clinical guidelines. Our deep learning based segmentation\nprovided Dice-scores of 0.8124 +- 0.2343 and 0.8969 +- 0.0521 for extrameatal\nand whole tumour regions respectively for T2 weighted MRI, whereas 0.8222 +-\n0.2108 and 0.9049 +- 0.0646 were obtained for T1 weighted MRI. We propose a\nnovel algorithm to choose and extract the most appropriate maximum linear\nmeasurement from the segmented regions based on the size of the extrameatal\nportion of the tumour. Using this tool, clinicians will be provided with a\nvisual guide and related metrics relating to tumour progression that will\nfunction as a clinical decision aid. In this study, we utilize 187 scans\nobtained from 50 patients referred to a tertiary specialist neurosurgical\nservice in the United Kingdom. The measurements extracted manually by an expert\nneuroradiologist indicated a significant correlation with the automated\nmeasurements (p < 0.0001).",
            "author": [
                "Navodini Wijethilake",
                "Steve Connor",
                "Anna Oviedova",
                "Rebecca Burger",
                "Tom Vercauteren",
                "Jonathan Shapey"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19392v1",
                "http://arxiv.org/pdf/2310.19392v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19370v1",
            "title": "The connected generalized Cayley graphs",
            "updated": "2023-10-30T09:22:17Z",
            "published": "2023-10-30T09:22:17Z",
            "summary": "In this paper, we provide the sufficient and necessary conditions for\ngeneralized Cayley graphs to be connected and bipartite, respectively. As a\nconsequence, we determine the groups whose all generalized Cayley cubic graphs\nare connected and integral.",
            "author": [
                "Liao Qianfen",
                "Liu Weijun"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19370v1",
                "http://arxiv.org/pdf/2310.19370v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19365v1",
            "title": "Observation of a low-lying metastable electronic state in highly charged\n  lead by Penning-trap mass spectrometry",
            "updated": "2023-10-30T09:08:26Z",
            "published": "2023-10-30T09:08:26Z",
            "summary": "Highly charged ions (HCIs) offer many opportunities for next-generation clock\nresearch due to the vast landscape of available electronic transitions in\ndifferent charge states. The development of XUV frequency combs has enabled the\nsearch for clock transitions based on shorter wavelengths in HCIs. However,\nwithout initial knowledge of the energy of the clock states, these narrow\ntransitions are difficult to be probed by lasers. In this Letter, we provide\nexperimental observation and theoretical calculation of a long-lived electronic\nstate in Nb-like Pb$^{41+}$ which could be used as a clock state. With the mass\nspectrometer Pentatrap, the excitation energy of this metastable state is\ndirectly determined as a mass difference at an energy of 31.2(8) eV,\ncorresponding to one of the most precise relative mass determinations to date\nwith a fractional uncertainty of $4\\times10^{-12}$. This experimental result\nagrees within 1 $\\sigma$ with two partially different \\textit{ab initio}\nmulti-configuration Dirac-Hartree-Fock calculations of 31.68(13) eV and\n31.76(35) eV, respectively. With a calculated lifetime of 26.5(5.3) days, the\ntransition from this metastable state to the ground state bears a quality\nfactor of $1.1\\times10^{23}$ and allows for the construction of a HCI clock\nwith a fractional frequency instability of $<10^{-19}/\\sqrt{\\tau}$.",
            "author": [
                "Kathrin Kromer",
                "Chunhai Lyu",
                "Menno Door",
                "Pavel Filianin",
                "Zolt\u00e1n Harman",
                "Jost Herkenhoff",
                "Paul Indelicato",
                "Christoph H. Keitel",
                "Daniel Lange",
                "Yuri N. Novikov",
                "Christoph Schweiger",
                "Sergey Eliseev",
                "Klaus Blaum"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19365v1",
                "http://arxiv.org/pdf/2310.19365v1"
            ],
            "primary_category": "physics.atom-ph",
            "category": [
                "physics.atom-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19356v1",
            "title": "Nonlinear interaction of head$-$on solitary waves in integrable and\n  nonintegrable systems",
            "updated": "2023-10-30T08:49:56Z",
            "published": "2023-10-30T08:49:56Z",
            "summary": "This study numerically investigates the nonlinear interaction of head-on\nsolitary waves in a granular chain (a nonintegrable system) and compares the\nsimulation results with the theoretical results in fluid (an integrable\nsystem). Three stages (i.e., pre-in-phase traveling stage, central-collision\nstage, and post-in-phase traveling stage) are identified to describe the\nnonlinear interaction processes in the granular chain. The nonlinear scattering\neffect occurs in the central-collision stage, which decreases the amplitude of\nincident solitary waves. Compared with the leading-time phase in the incident\nand separation collision processes, the lagging-time phase in the separation\ncollision process is smaller. This asymmetrical nonlinear collision results in\nan occurrence of leading phase shifts of time and space in the post-in-phase\ntraveling stage. We next find that solitary wave amplitude does not influence\nthe immediate space-phase shift in the granular chain. The space$-$phase shift\nof the post-in-phase traveling stage is only determined by measurement position\nrather than wave amplitude. The results are reversed in the fluid. An increase\nin solitary wave amplitude leads to decreased attachment, detachment and\nresidence times for granular chain and fluid. For the immediate time-phase\nshift, leading and lagging phenomena appear in the granular chain and the\nfluid, respectively. These results offer new knowledge for designing mechanical\nmetamaterials and energy-mitigating systems.",
            "author": [
                "Shutian Zhang",
                "Shikun Liu",
                "Tengfei Jiao",
                "Min Sun",
                "Decai Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19356v1",
                "http://arxiv.org/pdf/2310.19356v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19355v1",
            "title": "Local random quantum circuits form approximate designs on arbitrary\n  architectures",
            "updated": "2023-10-30T08:48:14Z",
            "published": "2023-10-30T08:48:14Z",
            "summary": "We consider random quantum circuits (RQC) on arbitrary connected graphs whose\nedges determine the allowed $2$-qudit interactions. Prior work has established\nthat such $n$-qudit circuits with local dimension $q$ on 1D, complete, and\n$D$-dimensional graphs form approximate unitary designs, that is, they generate\nunitaries from distributions close to the Haar measure on the unitary group\n$U(q^n)$ after polynomially many gates. Here, we extend those results by\nproving that RQCs comprised of $O(\\mathrm{poly}(n,k))$ gates on a wide class of\ngraphs form approximate unitary $k$-designs. We prove that RQCs on graphs with\nspanning trees of bounded degree and height form $k$-designs after\n$O(|E|n\\,\\mathrm{poly}(k))$ gates, where $|E|$ is the number of edges in the\ngraph. Furthermore, we identify larger classes of graphs for which RQCs\ngenerate approximate designs in polynomial circuit size. For $k \\leq 4$, we\nshow that RQCs on graphs of certain maximum degrees form designs after\n$O(|E|n)$ gates, providing explicit constants. We determine our circuit size\nbounds from the spectral gaps of local Hamiltonians. To that end, we extend the\nfinite-size (or Knabe) method for bounding gaps of frustration-free\nHamiltonians on regular graphs to arbitrary connected graphs. We further\nintroduce a new method based on the Detectability Lemma for determining the\nspectral gaps of Hamiltonians on arbitrary graphs. Our methods have wider\napplicability as the first method provides a succinct alternative proof of\n[Commun. Math. Phys. 291, 257 (2009)] and the second method proves that RQCs on\nany connected architecture form approximate designs in quasi-polynomial circuit\nsize.",
            "author": [
                "Shivan Mittal",
                "Nicholas Hunter-Jones"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19355v1",
                "http://arxiv.org/pdf/2310.19355v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19342v1",
            "title": "Label-Only Model Inversion Attacks via Knowledge Transfer",
            "updated": "2023-10-30T08:32:12Z",
            "published": "2023-10-30T08:32:12Z",
            "summary": "In a model inversion (MI) attack, an adversary abuses access to a machine\nlearning (ML) model to infer and reconstruct private training data. Remarkable\nprogress has been made in the white-box and black-box setups, where the\nadversary has access to the complete model or the model's soft output\nrespectively. However, there is very limited study in the most challenging but\npractically important setup: Label-only MI attacks, where the adversary only\nhas access to the model's predicted label (hard label) without confidence\nscores nor any other model information.\n  In this work, we propose LOKT, a novel approach for label-only MI attacks.\nOur idea is based on transfer of knowledge from the opaque target model to\nsurrogate models. Subsequently, using these surrogate models, our approach can\nharness advanced white-box attacks. We propose knowledge transfer based on\ngenerative modelling, and introduce a new model, Target model-assisted ACGAN\n(T-ACGAN), for effective knowledge transfer. Our method casts the challenging\nlabel-only MI into the more tractable white-box setup. We provide analysis to\nsupport that surrogate models based on our approach serve as effective proxies\nfor the target model for MI. Our experiments show that our method significantly\noutperforms existing SOTA Label-only MI attack by more than 15% across all MI\nbenchmarks. Furthermore, our method compares favorably in terms of query\nbudget. Our study highlights rising privacy threats for ML models even when\nminimal information (i.e., hard labels) is exposed. Our study highlights rising\nprivacy threats for ML models even when minimal information (i.e., hard labels)\nis exposed. Our code, demo, models and reconstructed data are available at our\nproject page: https://ngoc-nguyen-0.github.io/lokt/",
            "author": [
                "Ngoc-Bao Nguyen",
                "Keshigeyan Chandrasegaran",
                "Milad Abdollahzadeh",
                "Ngai-Man Cheung"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19342v1",
                "http://arxiv.org/pdf/2310.19342v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19339v1",
            "title": "Linear Realisability and Cobordisms",
            "updated": "2023-10-30T08:26:50Z",
            "published": "2023-10-30T08:26:50Z",
            "summary": "Cobordism categories are known to be compact closed. They can therefore be\nused to define non-degenerate models of multiplicative linear logic by\ncombining the Int construction with double glueing. In this work we detail such\nconstruction in the case of low-dimensional cobordisms, and exhibit a connexion\nbetween those models and the model of Interaction graphs introduced by Seiller.\nIn particular, we exhibit how the so-called trefoil property is a consequence\nof the associativity of composition of higher structures, providing a first\nstep toward establishing models as obtained from a double glueing construction.\nWe discuss possible extensions to higher-dimensional cobordisms categories",
            "author": [
                "Valentin Maestracci",
                "Thomas Seiller"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19339v1",
                "http://arxiv.org/pdf/2310.19339v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19324v1",
            "title": "TempME: Towards the Explainability of Temporal Graph Neural Networks via\n  Motif Discovery",
            "updated": "2023-10-30T07:51:41Z",
            "published": "2023-10-30T07:51:41Z",
            "summary": "Temporal graphs are widely used to model dynamic systems with time-varying\ninteractions. In real-world scenarios, the underlying mechanisms of generating\nfuture interactions in dynamic systems are typically governed by a set of\nrecurring substructures within the graph, known as temporal motifs. Despite the\nsuccess and prevalence of current temporal graph neural networks (TGNN), it\nremains uncertain which temporal motifs are recognized as the significant\nindications that trigger a certain prediction from the model, which is a\ncritical challenge for advancing the explainability and trustworthiness of\ncurrent TGNNs. To address this challenge, we propose a novel approach, called\nTemporal Motifs Explainer (TempME), which uncovers the most pivotal temporal\nmotifs guiding the prediction of TGNNs. Derived from the information bottleneck\nprinciple, TempME extracts the most interaction-related motifs while minimizing\nthe amount of contained information to preserve the sparsity and succinctness\nof the explanation. Events in the explanations generated by TempME are verified\nto be more spatiotemporally correlated than those of existing approaches,\nproviding more understandable insights. Extensive experiments validate the\nsuperiority of TempME, with up to 8.21% increase in terms of explanation\naccuracy across six real-world datasets and up to 22.96% increase in boosting\nthe prediction Average Precision of current TGNNs.",
            "author": [
                "Jialin Chen",
                "Rex Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19324v1",
                "http://arxiv.org/pdf/2310.19324v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19321v1",
            "title": "D4Explainer: In-Distribution GNN Explanations via Discrete Denoising\n  Diffusion",
            "updated": "2023-10-30T07:41:42Z",
            "published": "2023-10-30T07:41:42Z",
            "summary": "The widespread deployment of Graph Neural Networks (GNNs) sparks significant\ninterest in their explainability, which plays a vital role in model auditing\nand ensuring trustworthy graph learning. The objective of GNN explainability is\nto discern the underlying graph structures that have the most significant\nimpact on model predictions. Ensuring that explanations generated are reliable\nnecessitates consideration of the in-distribution property, particularly due to\nthe vulnerability of GNNs to out-of-distribution data. Unfortunately,\nprevailing explainability methods tend to constrain the generated explanations\nto the structure of the original graph, thereby downplaying the significance of\nthe in-distribution property and resulting in explanations that lack\nreliability. To address these challenges, we propose D4Explainer, a novel\napproach that provides in-distribution GNN explanations for both counterfactual\nand model-level explanation scenarios. The proposed D4Explainer incorporates\ngenerative graph distribution learning into the optimization objective, which\naccomplishes two goals: 1) generate a collection of diverse counterfactual\ngraphs that conform to the in-distribution property for a given instance, and\n2) identify the most discriminative graph patterns that contribute to a\nspecific class prediction, thus serving as model-level explanations. It is\nworth mentioning that D4Explainer is the first unified framework that combines\nboth counterfactual and model-level explanations. Empirical evaluations\nconducted on synthetic and real-world datasets provide compelling evidence of\nthe state-of-the-art performance achieved by D4Explainer in terms of\nexplanation accuracy, faithfulness, diversity, and robustness.",
            "author": [
                "Jialin Chen",
                "Shirley Wu",
                "Abhijit Gupta",
                "Rex Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19321v1",
                "http://arxiv.org/pdf/2310.19321v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19306v1",
            "title": "A Planning-and-Exploring Approach to Extreme-Mechanics Force Fields",
            "updated": "2023-10-30T06:59:01Z",
            "published": "2023-10-30T06:59:01Z",
            "summary": "Extreme mechanical processes such as strong lattice distortion and bond\nbreakage during fracture are ubiquitous in nature and engineering, which often\nlead to catastrophic failure of structures. However, understanding the\nnucleation and growth of cracks is challenged by their multiscale\ncharacteristics spanning from atomic-level structures at the crack tip to the\nstructural features where the load is applied. Molecular simulations offer an\nimportant tool to resolve the progressive microstructural changes at crack\nfronts and are widely used to explore processes therein, such as mechanical\nenergy dissipation, crack path selection, and dynamic instabilities (e.g.,\nkinking, branching). Empirical force fields developed based on local\ndescriptors based on atomic positions and the bond orders do not yield\nsatisfying predictions of fracture, even for the nonlinear, anisotropic\nstress-strain relations and the energy densities of edges. High-fidelity force\nfields thus should include the tensorial nature of strain and the energetics of\nrare events during fracture, which, unfortunately, have not been taken into\naccount in both the state-of-the-art empirical and machine-learning force\nfields. Based on data generated by first-principles calculations, we develop a\nneural network-based force field for fracture, NN-F$^3$, by combining\npre-sampling of the space of strain states and active-learning techniques to\nexplore the transition states at critical bonding distances. The capability of\nNN-F$^3$ is demonstrated by studying the rupture of h-BN and twisted bilayer\ngraphene as model problems. The simulation results confirm recent experimental\nfindings and highlight the necessity to include the knowledge of electronic\nstructures from first-principles calculations in predicting extreme mechanical\nprocesses.",
            "author": [
                "Pengjie Shi",
                "Zhiping Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19306v1",
                "http://arxiv.org/pdf/2310.19306v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cond-mat.stat-mech",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19301v1",
            "title": "ROME: Evaluating Pre-trained Vision-Language Models on Reasoning beyond\n  Visual Common Sense",
            "updated": "2023-10-30T06:35:37Z",
            "published": "2023-10-30T06:35:37Z",
            "summary": "Humans possess a strong capability for reasoning beyond common sense. For\nexample, given an unconventional image of a goldfish laying on the table next\nto an empty fishbowl, a human would effortlessly determine that the fish is not\ninside the fishbowl. The case, however, may be different for a vision-language\nmodel, whose reasoning could gravitate towards the common scenario that the\nfish is inside the bowl, despite the visual input. In this paper, we introduce\na novel probing dataset named ROME (reasoning beyond commonsense knowledge) to\nevaluate whether the state-of-the-art pre-trained vision-language models have\nthe reasoning capability to correctly interpret counter-intuitive content. ROME\ncontains images that defy commonsense knowledge with regards to color, shape,\nmaterial, size and positional relation. Experiments on the state-of-the-art\npre-trained vision-language models reveal that most of these models are still\nlargely incapable of interpreting counter-intuitive scenarios. We hope that\nROME will spur further investigations on reasoning beyond commonsense knowledge\nin vision-language research.",
            "author": [
                "Kankan Zhou",
                "Eason Lai",
                "Wei Bin Au Yeong",
                "Kyriakos Mouratidis",
                "Jing Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19301v1",
                "http://arxiv.org/pdf/2310.19301v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19296v1",
            "title": "Complex-valued Wigner entropy of a quantum state",
            "updated": "2023-10-30T06:30:03Z",
            "published": "2023-10-30T06:30:03Z",
            "summary": "It is common knowledge that the Wigner function of a quantum state may admit\nnegative values, so that it cannot be viewed as a genuine probability density.\nHere, we examine the difficulty in finding an entropy-like functional in phase\nspace that extends to negative Wigner functions and then advocate the merits of\ndefining a complex-valued entropy associated with any Wigner function. This\nquantity, which we call the complex Wigner entropy, is defined via the analytic\ncontinuation of Shannon's differential entropy of the Wigner function in the\ncomplex plane. We show that the complex Wigner entropy enjoys interesting\nproperties, especially its real and imaginary parts are both invariant under\nGaussian unitaries (displacements, rotations, and squeezing in phase space).\nIts real part is physically relevant when considering the evolution of the\nWigner function under a Gaussian convolution, while its imaginary part is\nsimply proportional to the negative volume of the Wigner function. Finally, we\ndefine the complex-valued Fisher information of any Wigner function, which is\nlinked (via an extended de Bruijn's identity) to the time derivative of the\ncomplex Wigner entropy when the state undergoes Gaussian additive noise.\nOverall, it is anticipated that the complex plane yields a proper framework for\nanalyzing the entropic properties of quasiprobability distributions in phase\nspace.",
            "author": [
                "Nicolas J. Cerf",
                "Anaelle Hertz",
                "Zacharie Van Herstraeten"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19296v1",
                "http://arxiv.org/pdf/2310.19296v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19295v1",
            "title": "ROAM: memory-efficient large DNN training via optimized operator\n  ordering and memory layout",
            "updated": "2023-10-30T06:29:21Z",
            "published": "2023-10-30T06:29:21Z",
            "summary": "As deep learning models continue to increase in size, the memory requirements\nfor training have surged. While high-level techniques like offloading,\nrecomputation, and compression can alleviate memory pressure, they also\nintroduce overheads. However, a memory-efficient execution plan that includes a\nreasonable operator execution order and tensor memory layout can significantly\nincrease the models' memory efficiency and reduce overheads from high-level\ntechniques. In this paper, we propose ROAM which operates on computation graph\nlevel to derive memory-efficient execution plan with optimized operator order\nand tensor memory layout for models. We first propose sophisticated theories\nthat carefully consider model structure and training memory load to support\noptimization for large complex graphs that have not been well supported in the\npast. An efficient tree-based algorithm is further proposed to search task\ndivisions automatically, along with delivering high performance and\neffectiveness to solve the problem. Experiments show that ROAM achieves a\nsubstantial memory reduction of 35.7%, 13.3%, and 27.2% compared to Pytorch and\ntwo state-of-the-art methods and offers a remarkable 53.7x speedup. The\nevaluation conducted on the expansive GPT2-XL further validates ROAM's\nscalability.",
            "author": [
                "Huiyao Shu",
                "Ang Wang",
                "Ziji Shi",
                "Hanyu Zhao",
                "Yong Li",
                "Lu Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19295v1",
                "http://arxiv.org/pdf/2310.19295v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19292v1",
            "title": "Fusing Temporal Graphs into Transformers for Time-Sensitive Question\n  Answering",
            "updated": "2023-10-30T06:12:50Z",
            "published": "2023-10-30T06:12:50Z",
            "summary": "Answering time-sensitive questions from long documents requires temporal\nreasoning over the times in questions and documents. An important open question\nis whether large language models can perform such reasoning solely using a\nprovided text document, or whether they can benefit from additional temporal\ninformation extracted using other systems. We address this research question by\napplying existing temporal information extraction systems to construct temporal\ngraphs of events, times, and temporal relations in questions and documents. We\nthen investigate different approaches for fusing these graphs into Transformer\nmodels. Experimental results show that our proposed approach for fusing\ntemporal graphs into input text substantially enhances the temporal reasoning\ncapabilities of Transformer models with or without fine-tuning. Additionally,\nour proposed method outperforms various graph convolution-based approaches and\nestablishes a new state-of-the-art performance on SituatedQA and three splits\nof TimeQA.",
            "author": [
                "Xin Su",
                "Phillip Howard",
                "Nagib Hakim",
                "Steven Bethard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19292v1",
                "http://arxiv.org/pdf/2310.19292v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19289v1",
            "title": "AMLNet: Adversarial Mutual Learning Neural Network for\n  Non-AutoRegressive Multi-Horizon Time Series Forecasting",
            "updated": "2023-10-30T06:10:00Z",
            "published": "2023-10-30T06:10:00Z",
            "summary": "Multi-horizon time series forecasting, crucial across diverse domains,\ndemands high accuracy and speed. While AutoRegressive (AR) models excel in\nshort-term predictions, they suffer speed and error issues as the horizon\nextends. Non-AutoRegressive (NAR) models suit long-term predictions but\nstruggle with interdependence, yielding unrealistic results. We introduce\nAMLNet, an innovative NAR model that achieves realistic forecasts through an\nonline Knowledge Distillation (KD) approach. AMLNet harnesses the strengths of\nboth AR and NAR models by training a deep AR decoder and a deep NAR decoder in\na collaborative manner, serving as ensemble teachers that impart knowledge to a\nshallower NAR decoder. This knowledge transfer is facilitated through two key\nmechanisms: 1) outcome-driven KD, which dynamically weights the contribution of\nKD losses from the teacher models, enabling the shallow NAR decoder to\nincorporate the ensemble's diversity; and 2) hint-driven KD, which employs\nadversarial training to extract valuable insights from the model's hidden\nstates for distillation. Extensive experimentation showcases AMLNet's\nsuperiority over conventional AR and NAR models, thereby presenting a promising\navenue for multi-horizon time series forecasting that enhances accuracy and\nexpedites computation.",
            "author": [
                "Yang Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19289v1",
                "http://arxiv.org/pdf/2310.19289v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19288v1",
            "title": "EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing\n  Image Super-Resolution",
            "updated": "2023-10-30T06:09:33Z",
            "published": "2023-10-30T06:09:33Z",
            "summary": "Recently, convolutional networks have achieved remarkable development in\nremote sensing image Super-Resoltuion (SR) by minimizing the regression\nobjectives, e.g., MSE loss. However, despite achieving impressive performance,\nthese methods often suffer from poor visual quality with over-smooth issues.\nGenerative adversarial networks have the potential to infer intricate details,\nbut they are easy to collapse, resulting in undesirable artifacts. To mitigate\nthese issues, in this paper, we first introduce Diffusion Probabilistic Model\n(DPM) for efficient remote sensing image SR, dubbed EDiffSR. EDiffSR is easy to\ntrain and maintains the merits of DPM in generating perceptual-pleasant images.\nSpecifically, different from previous works using heavy UNet for noise\nprediction, we develop an Efficient Activation Network (EANet) to achieve\nfavorable noise prediction performance by simplified channel attention and\nsimple gate operation, which dramatically reduces the computational budget.\nMoreover, to introduce more valuable prior knowledge into the proposed EDiffSR,\na practical Conditional Prior Enhancement Module (CPEM) is developed to help\nextract an enriched condition. Unlike most DPM-based SR models that directly\ngenerate conditions by amplifying LR images, the proposed CPEM helps to retain\nmore informative cues for accurate SR. Extensive experiments on four remote\nsensing datasets demonstrate that EDiffSR can restore visual-pleasant images on\nsimulated and real-world remote sensing images, both quantitatively and\nqualitatively. The code of EDiffSR will be available at\nhttps://github.com/XY-boy/EDiffSR",
            "author": [
                "Yi Xiao",
                "Qiangqiang Yuan",
                "Kui Jiang",
                "Jiang He",
                "Xianyu Jin",
                "Liangpei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19288v1",
                "http://arxiv.org/pdf/2310.19288v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19285v1",
            "title": "Facilitating Graph Neural Networks with Random Walk on Simplicial\n  Complexes",
            "updated": "2023-10-30T06:03:34Z",
            "published": "2023-10-30T06:03:34Z",
            "summary": "Node-level random walk has been widely used to improve Graph Neural Networks.\nHowever, there is limited attention to random walk on edge and, more generally,\non $k$-simplices. This paper systematically analyzes how random walk on\ndifferent orders of simplicial complexes (SC) facilitates GNNs in their\ntheoretical expressivity. First, on $0$-simplices or node level, we establish a\nconnection between existing positional encoding (PE) and structure encoding\n(SE) methods through the bridge of random walk. Second, on $1$-simplices or\nedge level, we bridge edge-level random walk and Hodge $1$-Laplacians and\ndesign corresponding edge PE respectively. In the spatial domain, we directly\nmake use of edge level random walk to construct EdgeRWSE. Based on the spectral\nanalysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation\nequivariant and expressive edge-level positional encoding. Third, we generalize\nour theory to random walk on higher-order simplices and propose the general\nprinciple to design PE on simplices based on random walk and Hodge Laplacians.\nInter-level random walk is also introduced to unify a wide range of simplicial\nnetworks. Extensive experiments verify the effectiveness of our random\nwalk-based methods.",
            "author": [
                "Cai Zhou",
                "Xiyuan Wang",
                "Muhan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19285v1",
                "http://arxiv.org/pdf/2310.19285v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19275v1",
            "title": "Eliciting Topic Hierarchies from Large Language Models",
            "updated": "2023-10-30T05:14:43Z",
            "published": "2023-10-30T05:14:43Z",
            "summary": "Finding topics to write about can be a mentally demanding process. However,\ntopic hierarchies can help writers explore topics of varying levels of\nspecificity. In this paper, we use large language models (LLMs) to help\nconstruct topic hierarchies. Although LLMs have access to such knowledge, it\ncan be difficult to elicit due to issues of specificity, scope, and repetition.\nWe designed and tested three different prompting techniques to find one that\nmaximized accuracy. We found that prepending the general topic area to a prompt\nyielded the most accurate results with 85% accuracy. We discuss applications of\nthis research including STEM writing, education, and content creation.",
            "author": [
                "Grace Li",
                "Tao Long",
                "Lydia B. Chilton"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19275v1",
                "http://arxiv.org/pdf/2310.19275v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19274v2",
            "title": "Prediction of Effective Elastic Moduli of Rocks using Graph Neural\n  Networks",
            "updated": "2023-11-22T18:27:15Z",
            "published": "2023-10-30T05:13:58Z",
            "summary": "This study presents a Graph Neural Networks (GNNs)-based approach for\npredicting the effective elastic moduli of rocks from their digital CT-scan\nimages. We use the Mapper algorithm to transform 3D digital rock images into\ngraph datasets, encapsulating essential geometrical information. These graphs,\nafter training, prove effective in predicting elastic moduli. Our GNN model\nshows robust predictive capabilities across various graph sizes derived from\nvarious subcube dimensions. Not only does it perform well on the test dataset,\nbut it also maintains high prediction accuracy for unseen rocks and unexplored\nsubcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)\nreveals the superior performance of GNNs in predicting unseen rock properties.\nMoreover, the graph representation of microstructures significantly reduces GPU\nmemory requirements (compared to the grid representation for CNNs), enabling\ngreater flexibility in the batch size selection. This work demonstrates the\npotential of GNN models in enhancing the prediction accuracy of rock properties\nand boosting the efficiency of digital rock analysis.",
            "author": [
                "Jaehong Chung",
                "Rasool Ahmad",
                "WaiChing Sun",
                "Wei Cai",
                "Tapan Mukerji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19274v2",
                "http://arxiv.org/pdf/2310.19274v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.comp-ph",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19272v1",
            "title": "NPCL: Neural Processes for Uncertainty-Aware Continual Learning",
            "updated": "2023-10-30T05:10:00Z",
            "published": "2023-10-30T05:10:00Z",
            "summary": "Continual learning (CL) aims to train deep neural networks efficiently on\nstreaming data while limiting the forgetting caused by new tasks. However,\nlearning transferable knowledge with less interference between tasks is\ndifficult, and real-world deployment of CL models is limited by their inability\nto measure predictive uncertainties. To address these issues, we propose\nhandling CL tasks with neural processes (NPs), a class of meta-learners that\nencode different tasks into probabilistic distributions over functions all\nwhile providing reliable uncertainty estimates. Specifically, we propose an\nNP-based CL approach (NPCL) with task-specific modules arranged in a\nhierarchical latent variable model. We tailor regularizers on the learned\nlatent distributions to alleviate forgetting. The uncertainty estimation\ncapabilities of the NPCL can also be used to handle the task head/module\ninference challenge in CL. Our experiments show that the NPCL outperforms\nprevious CL approaches. We validate the effectiveness of uncertainty estimation\nin the NPCL for identifying novel data and evaluating instance-level model\nconfidence. Code is available at \\url{https://github.com/srvCodes/NPCL}.",
            "author": [
                "Saurav Jha",
                "Dong Gong",
                "He Zhao",
                "Lina Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19272v1",
                "http://arxiv.org/pdf/2310.19272v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19264v1",
            "title": "Sound of Story: Multi-modal Storytelling with Audio",
            "updated": "2023-10-30T04:26:24Z",
            "published": "2023-10-30T04:26:24Z",
            "summary": "Storytelling is multi-modal in the real world. When one tells a story, one\nmay use all of the visualizations and sounds along with the story itself.\nHowever, prior studies on storytelling datasets and tasks have paid little\nattention to sound even though sound also conveys meaningful semantics of the\nstory. Therefore, we propose to extend story understanding and telling areas by\nestablishing a new component called \"background sound\" which is story\ncontext-based audio without any linguistic information. For this purpose, we\nintroduce a new dataset, called \"Sound of Story (SoS)\", which has paired image\nand text sequences with corresponding sound or background music for a story. To\nthe best of our knowledge, this is the largest well-curated dataset for\nstorytelling with sound. Our SoS dataset consists of 27,354 stories with 19.6\nimages per story and 984 hours of speech-decoupled audio such as background\nmusic and other sounds. As benchmark tasks for storytelling with sound and the\ndataset, we propose retrieval tasks between modalities, and audio generation\ntasks from image-text sequences, introducing strong baselines for them. We\nbelieve the proposed dataset and tasks may shed light on the multi-modal\nunderstanding of storytelling in terms of sound. Downloading the dataset and\nbaseline codes for each task will be released in the link:\nhttps://github.com/Sosdatasets/SoS_Dataset.",
            "author": [
                "Jaeyeon Bae",
                "Seokhoon Jeong",
                "Seokun Kang",
                "Namgi Han",
                "Jae-Yon Lee",
                "Hyounghun Kim",
                "Taehwan Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19264v1",
                "http://arxiv.org/pdf/2310.19264v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19263v1",
            "title": "A Metadata-Driven Approach to Understand Graph Neural Networks",
            "updated": "2023-10-30T04:25:02Z",
            "published": "2023-10-30T04:25:02Z",
            "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\napplications, but their performance can be sensitive to specific data\nproperties of the graph datasets they operate on. Current literature on\nunderstanding the limitations of GNNs has primarily employed a\n$\\textit{model-driven}$ approach that leverage heuristics and domain knowledge\nfrom network science or graph theory to model the GNN behaviors, which is\ntime-consuming and highly subjective. In this work, we propose a\n$\\textit{metadata-driven}$ approach to analyze the sensitivity of GNNs to graph\ndata properties, motivated by the increasing availability of graph learning\nbenchmarks. We perform a multivariate sparse regression analysis on the\nmetadata derived from benchmarking GNN performance across diverse datasets,\nyielding a set of salient data properties. To validate the effectiveness of our\ndata-driven approach, we focus on one identified data property, the degree\ndistribution, and investigate how this property influences GNN performance\nthrough theoretical analysis and controlled experiments. Our theoretical\nfindings reveal that datasets with more balanced degree distribution exhibit\nbetter linear separability of node representations, thus leading to better GNN\nperformance. We also conduct controlled experiments using synthetic datasets\nwith varying degree distributions, and the results align well with our\ntheoretical findings. Collectively, both the theoretical analysis and\ncontrolled experiments verify that the proposed metadata-driven approach is\neffective in identifying critical data properties for GNNs.",
            "author": [
                "Ting Wei Li",
                "Qiaozhu Mei",
                "Jiaqi Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19263v1",
                "http://arxiv.org/pdf/2310.19263v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19261v1",
            "title": "Diversify & Conquer: Outcome-directed Curriculum RL via\n  Out-of-Distribution Disagreement",
            "updated": "2023-10-30T04:12:19Z",
            "published": "2023-10-30T04:12:19Z",
            "summary": "Reinforcement learning (RL) often faces the challenges of uninformed search\nproblems where the agent should explore without access to the domain knowledge\nsuch as characteristics of the environment or external rewards. To tackle these\nchallenges, this work proposes a new approach for curriculum RL called\nDiversify for Disagreement & Conquer (D2C). Unlike previous curriculum learning\nmethods, D2C requires only a few examples of desired outcomes and works in any\nenvironment, regardless of its geometry or the distribution of the desired\noutcome examples. The proposed method performs diversification of the\ngoal-conditional classifiers to identify similarities between visited and\ndesired outcome states and ensures that the classifiers disagree on states from\nout-of-distribution, which enables quantifying the unexplored region and\ndesigning an arbitrary goal-conditioned intrinsic reward signal in a simple and\nintuitive way. The proposed method then employs bipartite matching to define a\ncurriculum learning objective that produces a sequence of well-adjusted\nintermediate goals, which enable the agent to automatically explore and conquer\nthe unexplored region. We present experimental results demonstrating that D2C\noutperforms prior curriculum RL methods in both quantitative and qualitative\naspects, even with the arbitrarily distributed desired outcome examples.",
            "author": [
                "Daesol Cho",
                "Seungjae Lee",
                "H. Jin Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19261v1",
                "http://arxiv.org/pdf/2310.19261v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19259v1",
            "title": "Distance spectral conditions for $ID$-factor-critical and fractional\n  $[a, b]$-factor of graphs",
            "updated": "2023-10-30T04:06:37Z",
            "published": "2023-10-30T04:06:37Z",
            "summary": "Let $G=(V(G), E(G))$ be a graph with vertex set $V(G)$ and edge set $E(G)$. A\ngraph is $ID$-factor-critical if for every independent set $I$ of $G$ whose\nsize has the same parity as $|V(G)|$, $G-I$ has a perfect matching. For two\npositive integers $a$ and $b$ with $a\\leq b$, let $h$: $E(G)\\rightarrow [0, 1]$\nbe a function on $E(G)$ satisfying $a\\leq\\sum _{e\\in E_{G}(v_{i})}h(e)\\leq b$\nfor any vertex $v_{i}\\in V(G)$. Then the spanning subgraph with edge set\n$E_{h}$, denoted by $G[E_{h}]$, is called a fractional $[a, b]$-factor of $G$\nwith indicator function $h$, where $E_{h}=\\{e\\in E(G)\\mid h(e)>0\\}$ and\n$E_{G}(v_{i})=\\{e\\in E(G)\\mid e$ is incident with $v_{i}$ in $G$\\}. A graph is\ndefined as a fractional $[a, b]$-deleted graph if for any $e\\in E(G)$, $G-e$\ncontains a fractional $[a, b]$-factor. For any integer $k\\geq 1$, a graph has a\n$k$-factor if it contains a $k$-regular spanning subgraph. In this paper, we\nfirstly give a distance spectral radius condition of $G$ to guarantee that $G$\nis $ID$-factor-critical. Furthermore, we provide sufficient conditions in terms\nof distance spectral radius and distance signless Laplacian spectral radius for\na graph to contain a fractional $[a, b]$-factor, fractional $[a,\nb]$-deleted-factor and $k$-factor.",
            "author": [
                "Tingyan Ma",
                "Ligong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19259v1",
                "http://arxiv.org/pdf/2310.19259v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19256v1",
            "title": "Online Data-Driven Safety Certification for Systems Subject to Unknown\n  Disturbances",
            "updated": "2023-10-30T03:57:34Z",
            "published": "2023-10-30T03:57:34Z",
            "summary": "Deploying autonomous systems in safety critical settings necessitates methods\nto verify their safety properties. This is challenging because real-world\nsystems may be subject to disturbances that affect their performance, but are\nunknown a priori. This work develops a safety-verification strategy wherein\ndata is collected online and incorporated into a reachability analysis approach\nto check in real-time that the system avoids dangerous regions of the state\nspace. Specifically, we employ an optimization-based moving horizon estimator\n(MHE) to characterize the disturbance affecting the system, which is\nincorporated into an online reachability calculation. Reachable sets are\ncalculated using a computational graph analysis tool to predict the possible\nfuture states of the system and verify that they satisfy safety constraints. We\ninclude theoretical arguments proving our approach generates reachable sets\nthat bound the future states of the system, as well as numerical results\ndemonstrating how it can be used for safety verification. Finally, we present\nresults from hardware experiments demonstrating our approach's ability to\nperform online reachability calculations for an unmanned surface vehicle\nsubject to currents and actuator failures.",
            "author": [
                "Nicholas Rober",
                "Karan Mahesh",
                "Tyler M. Paine",
                "Max L. Greene",
                "Steven Lee",
                "Sildomar T. Monteiro",
                "Michael R. Benjamin",
                "Jonathan P. How"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19256v1",
                "http://arxiv.org/pdf/2310.19256v1"
            ],
            "primary_category": "cs.SY",
            "category": [
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19250v1",
            "title": "Assessment of Differentially Private Synthetic Data for Utility and\n  Fairness in End-to-End Machine Learning Pipelines for Tabular Data",
            "updated": "2023-10-30T03:37:16Z",
            "published": "2023-10-30T03:37:16Z",
            "summary": "Differentially private (DP) synthetic data sets are a solution for sharing\ndata while preserving the privacy of individual data providers. Understanding\nthe effects of utilizing DP synthetic data in end-to-end machine learning\npipelines impacts areas such as health care and humanitarian action, where data\nis scarce and regulated by restrictive privacy laws. In this work, we\ninvestigate the extent to which synthetic data can replace real, tabular data\nin machine learning pipelines and identify the most effective synthetic data\ngeneration techniques for training and evaluating machine learning models. We\ninvestigate the impacts of differentially private synthetic data on downstream\nclassification tasks from the point of view of utility as well as fairness. Our\nanalysis is comprehensive and includes representatives of the two main types of\nsynthetic data generation algorithms: marginal-based and GAN-based. To the best\nof our knowledge, our work is the first that: (i) proposes a training and\nevaluation framework that does not assume that real data is available for\ntesting the utility and fairness of machine learning models trained on\nsynthetic data; (ii) presents the most extensive analysis of synthetic data set\ngeneration algorithms in terms of utility and fairness when used for training\nmachine learning models; and (iii) encompasses several different definitions of\nfairness. Our findings demonstrate that marginal-based synthetic data\ngenerators surpass GAN-based ones regarding model training utility for tabular\ndata. Indeed, we show that models trained using data generated by\nmarginal-based algorithms can exhibit similar utility to models trained using\nreal data. Our analysis also reveals that the marginal-based synthetic data\ngenerator MWEM PGM can train models that simultaneously achieve utility and\nfairness characteristics close to those obtained by models trained with real\ndata.",
            "author": [
                "Mayana Pereira",
                "Meghana Kshirsagar",
                "Sumit Mukherjee",
                "Rahul Dodhia",
                "Juan Lavista Ferres",
                "Rafael de Sousa"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19250v1",
                "http://arxiv.org/pdf/2310.19250v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19242v2",
            "title": "Rainbow Stars and Rota's Basis Conjecture for Graphic Matroids",
            "updated": "2023-11-01T04:06:08Z",
            "published": "2023-10-30T03:18:13Z",
            "summary": "Let $G$ be a connected multigraph with $n$ vertices, and suppose $G$ has been\nedge-colored with $n-1$ colors so that each color class induces a spanning\ntree. Rota's Basis Conjecture for graphic matroids posits that one can find\n$n-1$ mutually edge-disjoint rainbow spanning trees. In a recent paper, Maezawa\nand Yazawa have shown that the conjecture holds if one assumes that the color\nclasses induce spanning stars. We delve further into the star case to explore\nsome extreme subcases including: all stars with different centers, the same\ncenter, or one of two centers. In addition, we identify the cases in which a\ngraph composed of monochromatic stars can be decomposed into rainbow stars. We\nalso show that the statement is false if one replaces `stars' with `paths'.",
            "author": [
                "Anant Asthana",
                "Shreev Goyal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19242v2",
                "http://arxiv.org/pdf/2310.19242v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16126v1",
            "title": "A Hierarchical Training Paradigm for Antibody Structure-sequence\n  Co-design",
            "updated": "2023-10-30T02:39:15Z",
            "published": "2023-10-30T02:39:15Z",
            "summary": "Therapeutic antibodies are an essential and rapidly expanding drug modality.\nThe binding specificity between antibodies and antigens is decided by\ncomplementarity-determining regions (CDRs) at the tips of these Y-shaped\nproteins. In this paper, we propose a hierarchical training paradigm (HTP) for\nthe antibody sequence-structure co-design. HTP consists of four levels of\ntraining stages, each corresponding to a specific protein modality within a\nparticular protein domain. Through carefully crafted tasks in different stages,\nHTP seamlessly and effectively integrates geometric graph neural networks\n(GNNs) with large-scale protein language models to excavate evolutionary\ninformation from not only geometric structures but also vast antibody and\nnon-antibody sequence databases, which determines ligand binding pose and\nstrength. Empirical experiments show that HTP sets the new state-of-the-art\nperformance in the co-design problem as well as the fix-backbone design. Our\nresearch offers a hopeful path to unleash the potential of deep generative\narchitectures and seeks to illuminate the way forward for the antibody sequence\nand structure co-design challenge.",
            "author": [
                "Fang Wu",
                "Stan Z. Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16126v1",
                "http://arxiv.org/pdf/2311.16126v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19222v1",
            "title": "Maximum Knowledge Orthogonality Reconstruction with Gradients in\n  Federated Learning",
            "updated": "2023-10-30T02:01:48Z",
            "published": "2023-10-30T02:01:48Z",
            "summary": "Federated learning (FL) aims at keeping client data local to preserve\nprivacy. Instead of gathering the data itself, the server only collects\naggregated gradient updates from clients. Following the popularity of FL, there\nhas been considerable amount of work, revealing the vulnerability of FL\napproaches by reconstructing the input data from gradient updates. Yet, most\nexisting works assume an FL setting with unrealistically small batch size, and\nhave poor image quality when the batch size is large. Other works modify the\nneural network architectures or parameters to the point of being suspicious,\nand thus, can be detected by clients. Moreover, most of them can only\nreconstruct one sample input from a large batch. To address these limitations,\nwe propose a novel and completely analytical approach, referred to as the\nmaximum knowledge orthogonality reconstruction (MKOR), to reconstruct clients'\ninput data. Our proposed method reconstructs a mathematically proven high\nquality image from large batches. MKOR only requires the server to send\nsecretly modified parameters to clients and can efficiently and inconspicuously\nreconstruct the input images from clients' gradient updates. We evaluate MKOR's\nperformance on the MNIST, CIFAR-100, and ImageNet dataset and compare it with\nthe state-of-the-art works. The results show that MKOR outperforms the existing\napproaches, and draws attention to a pressing need for further research on the\nprivacy protection of FL so that comprehensive defense approaches can be\ndeveloped.",
            "author": [
                "Feng Wang",
                "Senem Velipasalar",
                "M. Cenk Gursoy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19222v1",
                "http://arxiv.org/pdf/2310.19222v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19211v1",
            "title": "Investigative Pattern Detection Framework for Counterterrorism",
            "updated": "2023-10-30T00:45:05Z",
            "published": "2023-10-30T00:45:05Z",
            "summary": "Law-enforcement investigations aimed at preventing attacks by violent\nextremists have become increasingly important for public safety. The problem is\nexacerbated by the massive data volumes that need to be scanned to identify\ncomplex behaviors of extremists and groups. Automated tools are required to\nextract information to respond queries from analysts, continually scan new\ninformation, integrate them with past events, and then alert about emerging\nthreats. We address challenges in investigative pattern detection and develop\nan Investigative Pattern Detection Framework for Counterterrorism (INSPECT).\nThe framework integrates numerous computing tools that include machine learning\ntechniques to identify behavioral indicators and graph pattern matching\ntechniques to detect risk profiles/groups. INSPECT also automates multiple\ntasks for large-scale mining of detailed forensic biographies, forming\nknowledge networks, and querying for behavioral indicators and radicalization\ntrajectories. INSPECT targets human-in-the-loop mode of investigative search\nand has been validated and evaluated using an evolving dataset on domestic\njihadism.",
            "author": [
                "Shashika R. Muramudalige",
                "Benjamin W. K. Hung",
                "Rosanne Libretti",
                "Jytte Klausen",
                "Anura P. Jayasumana"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19211v1",
                "http://arxiv.org/pdf/2310.19211v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19206v1",
            "title": "Leveraging generative artificial intelligence to simulate student\n  learning behavior",
            "updated": "2023-10-30T00:09:59Z",
            "published": "2023-10-30T00:09:59Z",
            "summary": "Student simulation presents a transformative approach to enhance learning\noutcomes, advance educational research, and ultimately shape the future of\neffective pedagogy. We explore the feasibility of using large language models\n(LLMs), a remarkable achievement in AI, to simulate student learning behaviors.\nUnlike conventional machine learning based prediction, we leverage LLMs to\ninstantiate virtual students with specific demographics and uncover intricate\ncorrelations among learning experiences, course materials, understanding\nlevels, and engagement. Our objective is not merely to predict learning\noutcomes but to replicate learning behaviors and patterns of real students. We\nvalidate this hypothesis through three experiments. The first experiment, based\non a dataset of N = 145, simulates student learning outcomes from demographic\ndata, revealing parallels with actual students concerning various demographic\nfactors. The second experiment (N = 4524) results in increasingly realistic\nsimulated behaviors with more assessment history for virtual students\nmodelling. The third experiment (N = 27), incorporating prior knowledge and\ncourse interactions, indicates a strong link between virtual students' learning\nbehaviors and fine-grained mappings from test questions, course materials,\nengagement and understanding levels. Collectively, these findings deepen our\nunderstanding of LLMs and demonstrate its viability for student simulation,\nempowering more adaptable curricula design to enhance inclusivity and\neducational effectiveness.",
            "author": [
                "Songlin Xu",
                "Xinyu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19206v1",
                "http://arxiv.org/pdf/2310.19206v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19169v4",
            "title": "Observations on Graph Invariants with the Lov\u00e1sz $\u03b8$-Function",
            "updated": "2023-12-05T12:54:20Z",
            "published": "2023-10-29T22:11:08Z",
            "summary": "This paper explores three research directions that rely on the properties of\nthe Lov\\'{a}sz $\\theta$-function of a graph. The first part provides some new\nresults on the Shannon capacity of graphs, which include its determination for\ntwo infinite subclasses of strongly regular graphs, and extensions of some\nearlier results. The second part derives new results on cospectral and\nnon-isomorphic graphs, and related properties of types of joins of graphs; it\nrelies in part on a recent work by Berman and Hamud (2023). The last part\nderives bounds on graph invariants, and their tightness is compared to known\nbounds with a focus on perfect graphs, triangle-free graphs, and strongly\nregular graphs. Spectral upper and lower bounds on the vector and strict vector\nchromatic numbers of regular graphs are derived, followed by sufficient\nconditions for their attainability. Closed-form expressions for the vector and\nstrict vector chromatic numbers are consequently derived for all strongly\nregular graphs and for all graphs that are both vertex- and edge-transitive,\nand these two types of chromatic numbers are proved to coincide for all such\ngraphs. It is also shown by a counter-example that, unlike the Lov\\'{a}sz\n$\\theta$-function that forms an upper bound on the Shannon capacity of a graph,\nthe variant of the $\\theta$-function by Schrijver does not share that property;\nit settles a query since the introduction of the latter $\\theta$-function by\nMcEliece et al. and by Schrijver (1978). The core of this paper is the\nLov\\'{a}sz $\\theta$-function of graphs and spectral graph theory, and it is\naimed to have tutorial value as well.",
            "author": [
                "Igal Sason"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19169v4",
                "http://arxiv.org/pdf/2310.19169v4"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19163v1",
            "title": "RAIFLE: Reconstruction Attacks on Interaction-based Federated Learning\n  with Active Data Manipulation",
            "updated": "2023-10-29T21:47:24Z",
            "published": "2023-10-29T21:47:24Z",
            "summary": "Federated learning (FL) has recently emerged as a privacy-preserving approach\nfor machine learning in domains that rely on user interactions, particularly\nrecommender systems (RS) and online learning to rank (OLTR). While there has\nbeen substantial research on the privacy of traditional FL, little attention\nhas been paid to studying the privacy properties of these interaction-based FL\n(IFL) systems. In this work, we show that IFL can introduce unique challenges\nconcerning user privacy, particularly when the central server has knowledge and\ncontrol over the items that users interact with. Specifically, we demonstrate\nthe threat of reconstructing user interactions by presenting RAIFLE, a general\noptimization-based reconstruction attack framework customized for IFL. RAIFLE\nemploys Active Data Manipulation (ADM), a novel attack technique unique to IFL,\nwhere the server actively manipulates the training features of the items to\ninduce adversarial behaviors in the local FL updates. We show that RAIFLE is\nmore impactful than existing FL privacy attacks in the IFL context, and\ndescribe how it can undermine privacy defenses like secure aggregation and\nprivate information retrieval. Based on our findings, we propose and discuss\ncountermeasure guidelines to mitigate our attack in the context of federated\nRS/OLTR specifically and IFL more broadly.",
            "author": [
                "Dzung Pham",
                "Shreyas Kulkarni",
                "Amir Houmansadr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19163v1",
                "http://arxiv.org/pdf/2310.19163v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19149v1",
            "title": "Simple Constructions of Unique Neighbor Expanders from Error-correcting\n  Codes",
            "updated": "2023-10-29T20:53:45Z",
            "published": "2023-10-29T20:53:45Z",
            "summary": "In this note, we give very simple constructions of unique neighbor expander\ngraphs starting from spectral or combinatorial expander graphs of mild\nexpansion. These constructions and their analysis are simple variants of the\nconstructions of LDPC error-correcting codes from expanders, given by\nSipser-Spielman\\cite{SS96} (and Tanner\\cite{Tanner81}), and their analysis. We\nalso show how to obtain expanders with many unique neighbors using similar\nideas.\n  There were many exciting results on this topic recently, starting with\nAsherov-Dinur\\cite{AD23} and Hsieh-McKenzie-Mohanty-Paredes\\cite{HMMP23}, who\ngave a similar construction of unique neighbor expander graphs, but using more\nsophisticated ingredients (such as almost-Ramanujan graphs) and a more involved\nanalysis. Subsequent beautiful works of Cohen-Roth-TaShma\\cite{CRT23} and\nGolowich\\cite{Golowich23} gave even stronger objects (lossless expanders), but\nalso using sophisticated ingredients.\n  The main contribution of this work is that we get much more elementary\nconstructions of unique neighbor expanders and with a simpler analysis.",
            "author": [
                "Swastik Kopparty",
                "Noga Ron-Zewi",
                "Shubhangi Saraf"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19149v1",
                "http://arxiv.org/pdf/2310.19149v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19142v1",
            "title": "MAG-GNN: Reinforcement Learning Boosted Graph Neural Network",
            "updated": "2023-10-29T20:32:21Z",
            "published": "2023-10-29T20:32:21Z",
            "summary": "While Graph Neural Networks (GNNs) recently became powerful tools in graph\nlearning tasks, considerable efforts have been spent on improving GNNs'\nstructural encoding ability. A particular line of work proposed subgraph GNNs\nthat use subgraph information to improve GNNs' expressivity and achieved great\nsuccess. However, such effectivity sacrifices the efficiency of GNNs by\nenumerating all possible subgraphs. In this paper, we analyze the necessity of\ncomplete subgraph enumeration and show that a model can achieve a comparable\nlevel of expressivity by considering a small subset of the subgraphs. We then\nformulate the identification of the optimal subset as a combinatorial\noptimization problem and propose Magnetic Graph Neural Network (MAG-GNN), a\nreinforcement learning (RL) boosted GNN, to solve the problem. Starting with a\ncandidate subgraph set, MAG-GNN employs an RL agent to iteratively update the\nsubgraphs to locate the most expressive set for prediction. This reduces the\nexponential complexity of subgraph enumeration to the constant complexity of a\nsubgraph search algorithm while keeping good expressivity. We conduct extensive\nexperiments on many datasets, showing that MAG-GNN achieves competitive\nperformance to state-of-the-art methods and even outperforms many subgraph\nGNNs. We also demonstrate that MAG-GNN effectively reduces the running time of\nsubgraph GNNs.",
            "author": [
                "Lecheng Kong",
                "Jiarui Feng",
                "Hao Liu",
                "Dacheng Tao",
                "Yixin Chen",
                "Muhan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19142v1",
                "http://arxiv.org/pdf/2310.19142v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19139v2",
            "title": "Back to the Future! Studying Data Cleanness in Defects4J and its Impact\n  on Fault Localization",
            "updated": "2023-11-09T22:46:14Z",
            "published": "2023-10-29T20:19:06Z",
            "summary": "For software testing research, Defects4J stands out as the primary benchmark\ndataset, offering a controlled environment to study real bugs from prominent\nopen-source systems. However, prior research indicates that Defects4J might\ninclude tests added post-bug report, embedding developer knowledge and\naffecting fault localization efficacy. In this paper, we examine Defects4J's\nfault-triggering tests, emphasizing the implications of developer knowledge of\nSBFL techniques. We study the timelines of changes made to these tests\nconcerning bug report creation. Then, we study the effectiveness of SBFL\ntechniques without developer knowledge in the tests. We found that 1) 55% of\nthe fault-triggering tests were newly added to replicate the bug or to test for\nregression; 2) 22% of the fault-triggering tests were modified after the bug\nreports were created, containing developer knowledge of the bug; 3) developers\noften modify the tests to include new assertions or change the test code to\nreflect the changes in the source code; and 4) the performance of SBFL\ntechniques degrades significantly (up to --415% for Mean First Rank) when\nevaluated on the bugs without developer knowledge. We provide a dataset of bugs\nwithout developer insights, aiding future SBFL evaluations in Defects4J and\ninforming considerations for future bug benchmarks.",
            "author": [
                "An Ran Chen",
                "Md Nakhla Rafi",
                "Tse-Hsun Chen",
                "Shaohua Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19139v2",
                "http://arxiv.org/pdf/2310.19139v2"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19138v1",
            "title": "Backward and Forward Inference in Interacting Independent-Cascade\n  Processes: A Scalable and Convergent Message-Passing Approach",
            "updated": "2023-10-29T20:03:38Z",
            "published": "2023-10-29T20:03:38Z",
            "summary": "We study the problems of estimating the past and future evolutions of two\ndiffusion processes that spread concurrently on a network. Specifically, given\na known network $G=(V, \\overrightarrow{E})$ and a (possibly noisy) snapshot\n$\\mathcal{O}_n$ of its state taken at (a possibly unknown) time $W$, we wish to\ndetermine the posterior distributions of the initial state of the network and\nthe infection times of its nodes. These distributions are useful in finding\nsource nodes of epidemics and rumors -- $\\textit{backward inference}$ -- , and\nestimating the spread of a fixed set of source nodes -- $\\textit{forward\ninference}$.\n  To model the interaction between the two processes, we study an extension of\nthe independent-cascade (IC) model where, when a node gets infected with either\nprocess, its susceptibility to the other one changes. First, we derive the\nexact joint probability of the initial state of the network and the\nobservation-snapshot $\\mathcal{O}_n$. Then, using the machinery of\nfactor-graphs, factor-graph transformations, and the generalized\ndistributive-law, we derive a Belief-Propagation (BP) based algorithm that is\nscalable to large networks and can converge on graphs of arbitrary topology (at\na likely expense in approximation accuracy).",
            "author": [
                "Nouman Khan",
                "Kangle Mu",
                "Mehrdad Moharrami",
                "Vijay Subramanian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19138v1",
                "http://arxiv.org/pdf/2310.19138v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19137v1",
            "title": "Automaton Distillation: Neuro-Symbolic Transfer Learning for Deep\n  Reinforcement Learning",
            "updated": "2023-10-29T19:59:55Z",
            "published": "2023-10-29T19:59:55Z",
            "summary": "Reinforcement learning (RL) is a powerful tool for finding optimal policies\nin sequential decision processes. However, deep RL methods suffer from two\nweaknesses: collecting the amount of agent experience required for practical RL\nproblems is prohibitively expensive, and the learned policies exhibit poor\ngeneralization on tasks outside of the training distribution. To mitigate these\nissues, we introduce automaton distillation, a form of neuro-symbolic transfer\nlearning in which Q-value estimates from a teacher are distilled into a\nlow-dimensional representation in the form of an automaton. We then propose two\nmethods for generating Q-value estimates: static transfer, which reasons over\nan abstract Markov Decision Process constructed based on prior knowledge, and\ndynamic transfer, where symbolic information is extracted from a teacher Deep\nQ-Network (DQN). The resulting Q-value estimates from either method are used to\nbootstrap learning in the target environment via a modified DQN loss function.\nWe list several failure modes of existing automaton-based transfer methods and\ndemonstrate that both static and dynamic automaton distillation decrease the\ntime required to find optimal policies for various decision tasks.",
            "author": [
                "Suraj Singireddy",
                "Andre Beckus",
                "George Atia",
                "Sumit Jha",
                "Alvaro Velasquez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19137v1",
                "http://arxiv.org/pdf/2310.19137v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19126v1",
            "title": "Worst-case Performance of Popular Approximate Nearest Neighbor Search\n  Implementations: Guarantees and Limitations",
            "updated": "2023-10-29T19:25:48Z",
            "published": "2023-10-29T19:25:48Z",
            "summary": "Graph-based approaches to nearest neighbor search are popular and powerful\ntools for handling large datasets in practice, but they have limited\ntheoretical guarantees. We study the worst-case performance of recent\ngraph-based approximate nearest neighbor search algorithms, such as HNSW, NSG\nand DiskANN. For DiskANN, we show that its \"slow preprocessing\" version\nprovably supports approximate nearest neighbor search query with constant\napproximation ratio and poly-logarithmic query time, on data sets with bounded\n\"intrinsic\" dimension. For the other data structure variants studied, including\nDiskANN with \"fast preprocessing\", HNSW and NSG, we present a family of\ninstances on which the empirical query time required to achieve a \"reasonable\"\naccuracy is linear in instance size. For example, for DiskANN, we show that the\nquery procedure can take at least $0.1 n$ steps on instances of size $n$ before\nit encounters any of the $5$ nearest neighbors of the query.",
            "author": [
                "Piotr Indyk",
                "Haike Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19126v1",
                "http://arxiv.org/pdf/2310.19126v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.CG",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19105v1",
            "title": "Updated Standard for Secure Satellite Communications: Analysis of\n  Satellites, Attack Vectors, Existing Standards, and Enterprise and Security\n  Architectures",
            "updated": "2023-10-29T18:39:23Z",
            "published": "2023-10-29T18:39:23Z",
            "summary": "Satellites play a vital role in remote communication where traditional\ncommunication mediums struggle to provide benefits over associated costs and\nefficiency. In recent years, satellite communication has achieved utter\ninterest in the industry due to the achievement of high data rates through the\nmassive deployment of LEO satellites. Because of the complex diversity in types\nof satellites, communication methodologies, technological obstacles,\nenvironmental limitations, elements in the entire ecosystem, massive financial\nimpact, geopolitical conflict and domination, easier access to satellite\ncommunications, and various other reasons, the threat vectors are rising in the\nthreat landscape. To achieve resilience against those, only technological\nsolutions are not enough. An effective approach will be through security\nstandards. However, there is a considerable gap in the industry regarding a\ngeneric security standard framework for satellite communication and space data\nsystems. A few countries and space agencies have their own standard framework\nand private policies. However, many of those are either private, serve the\nspecific requirements of specific missions, or have not been updated for a long\ntime.\n  This project report will focus on identifying, categorizing, comparing, and\nassessing elements, threat landscape, enterprise security architectures, and\navailable public standards of satellite communication and space data systems.\nAfter that, it will utilize the knowledge to propose an updated standard\nframework for secure satellite communications and space data systems.",
            "author": [
                "Rupok Chowdhury Protik"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19105v1",
                "http://arxiv.org/pdf/2310.19105v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19077v1",
            "title": "Near-Optimal Packet Scheduling in Multihop Networks with End-to-End\n  Deadline Constraints",
            "updated": "2023-10-29T17:00:19Z",
            "published": "2023-10-29T17:00:19Z",
            "summary": "Scheduling packets with end-to-end deadline constraints in multihop networks\nis an important problem that has been notoriously difficult to tackle.\nRecently, there has been progress on this problem in the worst-case traffic\nsetting, with the objective of maximizing the number of packets delivered\nwithin their deadlines. Specifically, the proposed algorithms were shown to\nachieve $\\Omega(1/\\log(L))$ fraction of the optimal objective value if the\nminimum link capacity in the network is $C_{\\min}=\\Omega(\\log (L))$, where $L$\nis the maximum length of a packet's route in the network (which is bounded by\nthe packet's maximum deadline). However, such guarantees can be quite\npessimistic due to the strict worst-case traffic assumption and may not\naccurately reflect real-world settings. In this work, we aim to address this\nlimitation by exploring whether it is possible to design algorithms that\nachieve a constant fraction of the optimal value while relaxing the worst-case\ntraffic assumption.\n  We provide a positive answer by demonstrating that in stochastic traffic\nsettings, such as i.i.d. packet arrivals, near-optimal,\n$(1-\\epsilon)$-approximation algorithms can be designed if $C_{\\min} =\n\\Omega\\big(\\frac{\\log (L/\\epsilon) } {\\epsilon^2}\\big)$. To the best of our\nknowledge, this is the first result that shows this problem can be solved\nnear-optimally under nontrivial assumptions on traffic and link capacity. We\nfurther present extended simulations using real network traces with\nnon-stationary traffic, which demonstrate that our algorithms outperform\nworst-case-based algorithms in practical settings.",
            "author": [
                "Christos Tsanikidis",
                "Javad Ghaderi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19077v1",
                "http://arxiv.org/pdf/2310.19077v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19073v1",
            "title": "Deffuant opinion dynamics with attraction and repulsion",
            "updated": "2023-10-29T16:55:10Z",
            "published": "2023-10-29T16:55:10Z",
            "summary": "In the Deffuant model, individuals are located on the vertices of a graph,\nand are characterized by their opinion, a number in $[-1, 1]$. The dynamics\ndepends on two parameters: a confidence threshold $\\theta < 2$ and a convergent\nparameter $\\mu_- \\leq 1/2$. Neighbors on the graph interact at rate one, which\nresults in no changes if the neighbors disagree by more than $\\theta$, and a\ncompromise with the opinions moving toward each other by a factor $\\mu_-$ if\nthey disagree by less than $\\theta$ (attraction). The main conjecture about the\nDeffuant model, which was proved for the process on the integers, states that,\nfor all $\\mu_- > 0$ and starting from the product measure in which the opinions\nare uniformly distributed in the interval $[-1, 1]$, there is a phase\ntransition from discordance to consensus at the confidence threshold one. In\nthis paper, we study a natural variant of the model in which neighbors who\ndisagree by more than $\\theta$ feel more strongly about their own opinion,\nwhich is modeled by assuming that the opinions move away from each other by a\ndivergent parameter $\\mu_+$ (repulsion). We prove, for the process on the\nintegers, the absence of a phase transition even for arbitrarily small $\\mu_+ >\n0$, in the sense that, for every nontrivial choice of $\\theta$, there is always\ndiscordance.",
            "author": [
                "Nicolas Lanchier",
                "Max Mercer"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19073v1",
                "http://arxiv.org/pdf/2310.19073v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60K35, 91D25, 91D30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19070v2",
            "title": "Myriad: Large Multimodal Model by Applying Vision Experts for Industrial\n  Anomaly Detection",
            "updated": "2023-11-01T03:50:52Z",
            "published": "2023-10-29T16:49:45Z",
            "summary": "Existing industrial anomaly detection (IAD) methods predict anomaly scores\nfor both anomaly detection and localization. However, they struggle to perform\na multi-turn dialog and detailed descriptions for anomaly regions, e.g., color,\nshape, and categories of industrial anomalies. Recently, large multimodal\n(i.e., vision and language) models (LMMs) have shown eminent perception\nabilities on multiple vision tasks such as image captioning, visual\nunderstanding, visual reasoning, etc., making it a competitive potential choice\nfor more comprehensible anomaly detection. However, the knowledge about anomaly\ndetection is absent in existing general LMMs, while training a specific LMM for\nanomaly detection requires a tremendous amount of annotated data and massive\ncomputation resources. In this paper, we propose a novel large multi-modal\nmodel by applying vision experts for industrial anomaly detection (dubbed\nMyriad), which leads to definite anomaly detection and high-quality anomaly\ndescription. Specifically, we adopt MiniGPT-4 as the base LMM and design an\nExpert Perception module to embed the prior knowledge from vision experts as\ntokens which are intelligible to Large Language Models (LLMs). To compensate\nfor the errors and confusions of vision experts, we introduce a domain adapter\nto bridge the visual representation gaps between generic and industrial images.\nFurthermore, we propose a Vision Expert Instructor, which enables the Q-Former\nto generate IAD domain vision-language tokens according to vision expert prior.\nExtensive experiments on MVTec-AD and VisA benchmarks demonstrate that our\nproposed method not only performs favorably against state-of-the-art methods\nunder the 1-class and few-shot settings, but also provide definite anomaly\nprediction along with detailed descriptions in IAD domain.",
            "author": [
                "Yuanze Li",
                "Haolin Wang",
                "Shihao Yuan",
                "Ming Liu",
                "Debin Zhao",
                "Yiwen Guo",
                "Chen Xu",
                "Guangming Shi",
                "Wangmeng Zuo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19070v2",
                "http://arxiv.org/pdf/2310.19070v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19059v1",
            "title": "Escaping Saddle Points in Heterogeneous Federated Learning via\n  Distributed SGD with Communication Compression",
            "updated": "2023-10-29T16:24:53Z",
            "published": "2023-10-29T16:24:53Z",
            "summary": "We consider the problem of finding second-order stationary points of\nheterogeneous federated learning (FL). Previous works in FL mostly focus on\nfirst-order convergence guarantees, which do not rule out the scenario of\nunstable saddle points. Meanwhile, it is a key bottleneck of FL to achieve\ncommunication efficiency without compensating the learning accuracy, especially\nwhen local data are highly heterogeneous across different clients. Given this,\nwe propose a novel algorithm Power-EF that only communicates compressed\ninformation via a novel error-feedback scheme. To our knowledge, Power-EF is\nthe first distributed and compressed SGD algorithm that provably escapes saddle\npoints in heterogeneous FL without any data homogeneity assumptions. In\nparticular, Power-EF improves to second-order stationary points after visiting\nfirst-order (possibly saddle) points, using additional gradient queries and\ncommunication rounds only of almost the same order required by first-order\nconvergence, and the convergence rate exhibits a linear speedup in terms of the\nnumber of workers. Our theory improves/recovers previous results, while\nextending to much more tolerant settings on the local data. Numerical\nexperiments are provided to complement the theory.",
            "author": [
                "Sijin Chen",
                "Zhize Li",
                "Yuejie Chi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19059v1",
                "http://arxiv.org/pdf/2310.19059v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19056v2",
            "title": "MILL: Mutual Verification with Large Language Models for Zero-Shot Query\n  Expansion",
            "updated": "2023-11-13T15:09:01Z",
            "published": "2023-10-29T16:04:10Z",
            "summary": "Query expansion is a commonly-used technique in many search systems to better\nrepresent users' information needs with additional query terms. Existing\nstudies for this task usually propose to expand a query with retrieved or\ngenerated contextual documents. However, both types of methods have clear\nlimitations. For retrieval-based methods, the documents retrieved with the\noriginal query might not be accurate enough to reveal the search intent,\nespecially when the query is brief or ambiguous. For generation-based methods,\nexisting models can hardly be trained or aligned on a particular corpus, due to\nthe lack of corpus-specific labeled data. In this paper, we propose a novel\nLarge Language Model (LLM) based mutual verification framework for query\nexpansion, which alleviates the aforementioned limitations. Specifically, we\nfirst design a query-query-document generation pipeline, which can effectively\nleverage the contextual knowledge encoded in LLMs to generate sub-queries and\ncorresponding documents from multiple perspectives. Next, we employ a mutual\nverification method for both generated and retrieved contextual documents,\nwhere 1) retrieved documents are filtered with the external contextual\nknowledge in generated documents, and 2) generated documents are filtered with\nthe corpus-specific knowledge in retrieved documents. Overall, the proposed\nmethod allows retrieved and generated documents to complement each other to\nfinalize a better query expansion. We conduct extensive experiments on three\ninformation retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO.\nThe results demonstrate that our method outperforms other baselines\nsignificantly.",
            "author": [
                "Pengyue Jia",
                "Yiding Liu",
                "Xiangyu Zhao",
                "Xiaopeng Li",
                "Changying Hao",
                "Shuaiqiang Wang",
                "Dawei Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19056v2",
                "http://arxiv.org/pdf/2310.19056v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19055v1",
            "title": "A Survey on Recent Named Entity Recognition and Relation Classification\n  Methods with Focus on Few-Shot Learning Approaches",
            "updated": "2023-10-29T16:02:46Z",
            "published": "2023-10-29T16:02:46Z",
            "summary": "Named entity recognition and relation classification are key stages for\nextracting information from unstructured text. Several natural language\nprocessing applications utilize the two tasks, such as information retrieval,\nknowledge graph construction and completion, question answering and other\ndomain-specific applications, such as biomedical data mining. We present a\nsurvey of recent approaches in the two tasks with focus on few-shot learning\napproaches. Our work compares the main approaches followed in the two\nparadigms. Additionally, we report the latest metric scores in the two tasks\nwith a structured analysis that considers the results in the few-shot learning\nscope.",
            "author": [
                "Sakher Alqaaidi",
                "Elika Bozorgi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19055v1",
                "http://arxiv.org/pdf/2310.19055v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19049v2",
            "title": "Estimation of Semiconductor Power Losses Through Automatic Thermal\n  Modeling",
            "updated": "2023-11-03T11:37:23Z",
            "published": "2023-10-29T15:50:47Z",
            "summary": "The optimal design of power converters requires accurate knowledge of the\ndissipation elements of its system to achieve the desired performance and\nsecurity requirements. Calorimetric methods have surpassed classical electrical\nmethods for the estimation of semiconductor power losses but have mechanical\nlimitations and resort to analytical electrothermal equivalent circuits for\nthis task. These electrothermal models are highly dependent on the topology and\ntechnology used on the power converter leading to either simplifications that\nunderestimate the thermal effects or intractable sets of differential\nequations. To overcome these issues, we propose a novel data-driven\nidentification method to characterize the thermal dynamics of power converters\nallowing the designer to obtain semiconductor total power losses only by means\nof temperature measurements without the need of a calorimeter. Given a set of\npower vs.temperature profiles, our solution identifies the linear model that\nbest fits the data. The solution is based on an optimization problem that\nallows not only accurate identification but also coding of the desired modeling\nrequirements, such as dynamics' invertibility to allow the estimation of power\nlosses from the temperature profiles. The proposed methodology can be applied\nto any power converter topology. Furthermore, by obtaining a linear model,\nstandard control theory techniques can be exploited to analyze and control the\nthermal dynamics. Real experiments validate the generality and accuracy of the\nproposal.",
            "author": [
                "Jose Miguel Sanz-Alcaine",
                "Eduardo Sebastian",
                "Francisco Jose Perez-Cebolla",
                "Asier Arruti",
                "Carlos Bernal-Ruiz",
                "Iosu Aizpuru"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19049v2",
                "http://arxiv.org/pdf/2310.19049v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19048v1",
            "title": "Counting triangles in graphs without vertex disjoint odd cycles",
            "updated": "2023-10-29T15:50:28Z",
            "published": "2023-10-29T15:50:28Z",
            "summary": "Given two graphs $H$ and $F$, the maximum possible number of copies of $H$ in\nan $F$-free graph on $n$ vertices is denoted by $\\mathrm{ex}(n, H, F)$. Let\n$(\\ell+1) \\cdot F$ denote $\\ell+1$ vertex disjoint copies of $F$. In this\npaper, we determine the exact value of $\\mathrm{ex}(n, C_3, (\\ell+1)\\cdot\nC_{2k+1})$ and its extremal graph, which generalizes some known results.",
            "author": [
                "Jianfeng Hou",
                "Caihong Yang",
                "Qinghou Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19048v1",
                "http://arxiv.org/pdf/2310.19048v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19046v2",
            "title": "Large Language Models as Evolutionary Optimizers",
            "updated": "2023-11-01T05:21:47Z",
            "published": "2023-10-29T15:44:52Z",
            "summary": "Evolutionary algorithms (EAs) have achieved remarkable success in tackling\ncomplex combinatorial optimization problems. However, EAs often demand\ncarefully-designed operators with the aid of domain expertise to achieve\nsatisfactory performance. In this work, we present the first study on large\nlanguage models (LLMs) as evolutionary combinatorial optimizers. The main\nadvantage is that it requires minimal domain knowledge and human efforts, as\nwell as no additional training of the model. This approach is referred to as\nLLM-driven EA (LMEA). Specifically, in each generation of the evolutionary\nsearch, LMEA instructs the LLM to select parent solutions from current\npopulation, and perform crossover and mutation to generate offspring solutions.\nThen, LMEA evaluates these new solutions and include them into the population\nfor the next generation. LMEA is equipped with a self-adaptation mechanism that\ncontrols the temperature of the LLM. This enables it to balance between\nexploration and exploitation and prevents the search from getting stuck in\nlocal optima. We investigate the power of LMEA on the classical traveling\nsalesman problems (TSPs) widely used in combinatorial optimization research.\nNotably, the results show that LMEA performs competitively to traditional\nheuristics in finding high-quality solutions on TSP instances with up to 20\nnodes. Additionally, we also study the effectiveness of LLM-driven\ncrossover/mutation and the self-adaptation mechanism in evolutionary search. In\nsummary, our results reveal the great potentials of LLMs as evolutionary\noptimizers for solving combinatorial problems. We hope our research shall\ninspire future explorations on LLM-driven EAs for complex optimization\nchallenges.",
            "author": [
                "Shengcai Liu",
                "Caishun Chen",
                "Xinghua Qu",
                "Ke Tang",
                "Yew-Soon Ong"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19046v2",
                "http://arxiv.org/pdf/2310.19046v2"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19035v1",
            "title": "Does Invariant Graph Learning via Environment Augmentation Learn\n  Invariance?",
            "updated": "2023-10-29T14:57:37Z",
            "published": "2023-10-29T14:57:37Z",
            "summary": "Invariant graph representation learning aims to learn the invariance among\ndata from different environments for out-of-distribution generalization on\ngraphs. As the graph environment partitions are usually expensive to obtain,\naugmenting the environment information has become the de facto approach.\nHowever, the usefulness of the augmented environment information has never been\nverified. In this work, we find that it is fundamentally impossible to learn\ninvariant graph representations via environment augmentation without additional\nassumptions. Therefore, we develop a set of minimal assumptions, including\nvariation sufficiency and variation consistency, for feasible invariant graph\nlearning. We then propose a new framework Graph invAriant Learning Assistant\n(GALA). GALA incorporates an assistant model that needs to be sensitive to\ngraph environment changes or distribution shifts. The correctness of the proxy\npredictions by the assistant model hence can differentiate the variations in\nspurious subgraphs. We show that extracting the maximally invariant subgraph to\nthe proxy predictions provably identifies the underlying invariant subgraph for\nsuccessful OOD generalization under the established minimal assumptions.\nExtensive experiments on datasets including DrugOOD with various graph\ndistribution shifts confirm the effectiveness of GALA.",
            "author": [
                "Yongqiang Chen",
                "Yatao Bian",
                "Kaiwen Zhou",
                "Binghui Xie",
                "Bo Han",
                "James Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19035v1",
                "http://arxiv.org/pdf/2310.19035v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19027v1",
            "title": "Electrical-Field Modulation of the Charge-Density-Wave Quantum\n  Condensate in h-BN/NbS$_3$ Heterostructure Devices",
            "updated": "2023-10-29T14:35:51Z",
            "published": "2023-10-29T14:35:51Z",
            "summary": "We report on the field-effect modulation of the charge-density-wave quantum\ncondensate in the top-gated heterostructure devices implemented with\nquasi-one-dimensional NbS$_3$ nanowire channels and quasi-two-dimensional h-BN\ngate dielectric layers. The charge-density-wave phases and collective current\nin quasi-1D NbS$_3$ nanowires were verified via temperature dependence of the\nresistivity, non-linear current-voltage characteristics, and Shapiro steps that\nappeared in the device response under radio frequency excitation mixed with the\nDC bias. It was demonstrated that the electric field of the applied gate bias\ncan reversibly modulate the collective current of the sliding\ncharge-density-wave condensate. The collective current reduces with more\npositive bias suggesting a surface effect on the condensate mobility. The\nsingle particle current, at small source-drain biases, shows small amplitude\nfluctuation behavior, attributed to the variations in the background potential\ndue to the pinned or creeping charge-density-wave condensate. The knowledge of\nthe electric-field effect on the charge density waves in quasi-1D NbS$_3$\nnanowires is useful for potential electronic applications of such quantum\nmaterials.",
            "author": [
                "Maedeh Taheri",
                "Nicholas Sesing",
                "Tina",
                "T. Salguero",
                "Alexander A. Balandin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19027v1",
                "http://arxiv.org/pdf/2310.19027v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19024v1",
            "title": "FPGAN-Control: A Controllable Fingerprint Generator for Training with\n  Synthetic Data",
            "updated": "2023-10-29T14:30:01Z",
            "published": "2023-10-29T14:30:01Z",
            "summary": "Training fingerprint recognition models using synthetic data has recently\ngained increased attention in the biometric community as it alleviates the\ndependency on sensitive personal data. Existing approaches for fingerprint\ngeneration are limited in their ability to generate diverse impressions of the\nsame finger, a key property for providing effective data for training\nrecognition models. To address this gap, we present FPGAN-Control, an identity\npreserving image generation framework which enables control over the\nfingerprint's image appearance (e.g., fingerprint type, acquisition device,\npressure level) of generated fingerprints. We introduce a novel appearance loss\nthat encourages disentanglement between the fingerprint's identity and\nappearance properties. In our experiments, we used the publicly available NIST\nSD302 (N2N) dataset for training the FPGAN-Control model. We demonstrate the\nmerits of FPGAN-Control, both quantitatively and qualitatively, in terms of\nidentity preservation level, degree of appearance control, and low\nsynthetic-to-real domain gap. Finally, training recognition models using only\nsynthetic datasets generated by FPGAN-Control lead to recognition accuracies\nthat are on par or even surpass models trained using real data. To the best of\nour knowledge, this is the first work to demonstrate this.",
            "author": [
                "Alon Shoshan",
                "Nadav Bhonker",
                "Emanuel Ben Baruch",
                "Ori Nizan",
                "Igor Kviatkovsky",
                "Joshua Engelsma",
                "Manoj Aggarwal",
                "Gerard Medioni"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19024v1",
                "http://arxiv.org/pdf/2310.19024v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19018v1",
            "title": "Securing Refugee Identity: A Literature Review on Blockchain-based Smart\n  Contract",
            "updated": "2023-10-29T14:16:26Z",
            "published": "2023-10-29T14:16:26Z",
            "summary": "Identity documentation for refugees is a complex process and crucial for host\nnations. A secured identity management system ensures both security and the\nefficient provision of services for the host nation and the donor\norganizations. Realizing the benefits, a handful of studies enriched the\nblockchain-based security identification for refugees. The research studies\npresented the introductory, conceptual, and practical solution related to the\nblockchain-based smart contract. There is a common agreement in the studies\nthat blockchain-based smart contract not only streamlines refugee identity\nverification but also safeguards against unauthorized entries. Since it is a\ntechnology as well, it has been essential to know the present status of the\ntechnology in the social context. In such a situation it becomes essential to\nreview the existing research studies to provide insight for future studies. In\nthis study, we reviewed current studies using a thematic approach. Our findings\nsuggest researchers are more inclined to provide conceptual models as the\nmodels are important in advancing technology; however, the models need to be\nimplemented for practical advances. However, the main contribution of this\nstudy is that this study gathers current efforts in smart contract-based\nrefugee identity management. This study is important for the refugee host\nnations as well as for stakeholders. Knowledge gained from the study is\nexpected to provide insight into how the technology can be developed using\nexisting theory and implementation frameworks.",
            "author": [
                "Md Taimur Ahad",
                "Yousuf Rayhan Emon"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19018v1",
                "http://arxiv.org/pdf/2310.19018v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19013v1",
            "title": "Optimizing simultaneous autoscaling for serverless cloud computing",
            "updated": "2023-10-29T14:00:41Z",
            "published": "2023-10-29T14:00:41Z",
            "summary": "This paper explores resource allocation in serverless cloud computing\nplatforms and proposes an optimization approach for autoscaling systems.\nServerless computing relieves users from resource management tasks, enabling\nfocus on application functions. However, dynamic resource allocation and\nfunction replication based on changing loads remain crucial. Typically,\nautoscalers in these platforms utilize threshold-based mechanisms to adjust\nfunction replicas independently. We model applications as interconnected graphs\nof functions, where requests probabilistically traverse the graph, triggering\nassociated function execution. Our objective is to develop a control policy\nthat optimally allocates resources on servers, minimizing failed requests and\nresponse time in reaction to load changes. Using a fluid approximation model\nand Separated Continuous Linear Programming (SCLP), we derive an optimal\ncontrol policy that determines the number of resources per replica and the\nrequired number of replicas over time. We evaluate our approach using a\nsimulation framework built with Python and simpy. Comparing against\nthreshold-based autoscaling, our approach demonstrates significant improvements\nin average response times and failed requests, ranging from 15% to over 300% in\nmost cases. We also explore the impact of system and workload parameters on\nperformance, providing insights into the behavior of our optimization approach\nunder different conditions. Overall, our study contributes to advancing\nresource allocation strategies, enhancing efficiency and reliability in\nserverless cloud computing platforms.",
            "author": [
                "Harold Ship",
                "Evgeny Shindin",
                "Chen Wang",
                "Diana Arroyo",
                "Asser Tantawi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19013v1",
                "http://arxiv.org/pdf/2310.19013v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19007v2",
            "title": "Behavior Alignment via Reward Function Optimization",
            "updated": "2023-10-31T04:58:20Z",
            "published": "2023-10-29T13:45:07Z",
            "summary": "Designing reward functions for efficiently guiding reinforcement learning\n(RL) agents toward specific behaviors is a complex task. This is challenging\nsince it requires the identification of reward structures that are not sparse\nand that avoid inadvertently inducing undesirable behaviors. Naively modifying\nthe reward structure to offer denser and more frequent feedback can lead to\nunintended outcomes and promote behaviors that are not aligned with the\ndesigner's intended goal. Although potential-based reward shaping is often\nsuggested as a remedy, we systematically investigate settings where deploying\nit often significantly impairs performance. To address these issues, we\nintroduce a new framework that uses a bi-level objective to learn\n\\emph{behavior alignment reward functions}. These functions integrate auxiliary\nrewards reflecting a designer's heuristics and domain knowledge with the\nenvironment's primary rewards. Our approach automatically determines the most\neffective way to blend these types of feedback, thereby enhancing robustness\nagainst heuristic reward misspecification. Remarkably, it can also adapt an\nagent's policy optimization process to mitigate suboptimalities resulting from\nlimitations and biases inherent in the underlying RL algorithms. We evaluate\nour method's efficacy on a diverse set of tasks, from small-scale experiments\nto high-dimensional control challenges. We investigate heuristic auxiliary\nrewards of varying quality -- some of which are beneficial and others\ndetrimental to the learning process. Our results show that our framework offers\na robust and principled way to integrate designer-specified heuristics. It not\nonly addresses key shortcomings of existing approaches but also consistently\nleads to high-performing solutions, even when given misaligned or\npoorly-specified auxiliary reward functions.",
            "author": [
                "Dhawal Gupta",
                "Yash Chandak",
                "Scott M. Jordan",
                "Philip S. Thomas",
                "Bruno Castro da Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19007v2",
                "http://arxiv.org/pdf/2310.19007v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19006v1",
            "title": "The Weisfeiler-Leman Dimension of Existential Conjunctive Queries",
            "updated": "2023-10-29T13:41:41Z",
            "published": "2023-10-29T13:41:41Z",
            "summary": "The Weisfeiler-Leman (WL) dimension of a graph parameter $f$ is the minimum\n$k$ such that, if $G_1$ and $G_2$ are indistinguishable by the $k$-dimensional\nWL-algorithm then $f(G_1)=f(G_2)$. The WL-dimension of $f$ is $\\infty$ if no\nsuch $k$ exists. We study the WL-dimension of graph parameters characterised by\nthe number of answers from a fixed conjunctive query to the graph. Given a\nconjunctive query $\\varphi$, we quantify the WL-dimension of the function that\nmaps every graph $G$ to the number of answers of $\\varphi$ in $G$.\n  The works of Dvor\\'ak (J. Graph Theory 2010), Dell, Grohe, and Rattan (ICALP\n2018), and Neuen (ArXiv 2023) have answered this question for full conjunctive\nqueries, which are conjunctive queries without existentially quantified\nvariables. For such queries $\\varphi$, the WL-dimension is equal to the\ntreewidth of the Gaifman graph of $\\varphi$.\n  In this work, we give a characterisation that applies to all conjunctive\nqureies. Given any conjunctive query $\\varphi$, we prove that its WL-dimension\nis equal to the semantic extension width $\\mathsf{sew}(\\varphi)$, a novel width\nmeasure that can be thought of as a combination of the treewidth of $\\varphi$\nand its quantified star size, an invariant introduced by Durand and Mengel\n(ICDT 2013) describing how the existentially quantified variables of $\\varphi$\nare connected with the free variables. Using the recently established\nequivalence between the WL-algorithm and higher-order Graph Neural Networks\n(GNNs) due to Morris et al. (AAAI 2019), we obtain as a consequence that the\nfunction counting answers to a conjunctive query $\\varphi$ cannot be computed\nby GNNs of order smaller than $\\mathsf{sew}(\\varphi)$.",
            "author": [
                "Andreas G\u00f6bel",
                "Leslie Ann Goldberg",
                "Marc Roth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19006v1",
                "http://arxiv.org/pdf/2310.19006v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.DB",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19005v2",
            "title": "Kernel-based Joint Multiple Graph Learning and Clustering of Graph\n  Signals",
            "updated": "2023-11-07T11:12:31Z",
            "published": "2023-10-29T13:41:12Z",
            "summary": "Within the context of Graph Signal Processing (GSP), Graph Learning (GL) is\nconcerned with the inference of the graph's underlying structure from nodal\nobservations. However, real-world data often contains diverse information,\nnecessitating the simultaneous clustering and learning of multiple graphs. In\npractical applications, valuable node-specific covariates, represented as\nkernels, have been underutilized by existing graph signal clustering methods.\nIn this letter, we propose a new framework, named Kernel-based joint Multiple\nGL and clustering of graph signals (KMGL), that leverages a multi-convex\noptimization approach. This allows us to integrate node-side information,\nconstruct low-pass filters, and efficiently solve the optimization problem. The\nexperiments demonstrate that KMGL significantly enhances the robustness of GL\nand clustering, particularly in scenarios with high noise levels and a\nsubstantial number of clusters. These findings underscore the potential of KMGL\nfor improving the performance of GSP methods in diverse, real-world\napplications.",
            "author": [
                "Mohamad H. Alizade",
                "Aref Einizade",
                "Jhony H. Giraldo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19005v2",
                "http://arxiv.org/pdf/2310.19005v2"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19001v1",
            "title": "Uncovering Prototypical Knowledge for Weakly Open-Vocabulary Semantic\n  Segmentation",
            "updated": "2023-10-29T13:18:00Z",
            "published": "2023-10-29T13:18:00Z",
            "summary": "This paper studies the problem of weakly open-vocabulary semantic\nsegmentation (WOVSS), which learns to segment objects of arbitrary classes\nusing mere image-text pairs. Existing works turn to enhance the vanilla vision\ntransformer by introducing explicit grouping recognition, i.e., employing\nseveral group tokens/centroids to cluster the image tokens and perform the\ngroup-text alignment. Nevertheless, these methods suffer from a granularity\ninconsistency regarding the usage of group tokens, which are aligned in the\nall-to-one v.s. one-to-one manners during the training and inference phases,\nrespectively. We argue that this discrepancy arises from the lack of elaborate\nsupervision for each group token. To bridge this granularity gap, this paper\nexplores explicit supervision for the group tokens from the prototypical\nknowledge. To this end, this paper proposes the non-learnable prototypical\nregularization (NPR) where non-learnable prototypes are estimated from source\nfeatures to serve as supervision and enable contrastive matching of the group\ntokens. This regularization encourages the group tokens to segment objects with\nless redundancy and capture more comprehensive semantic regions, leading to\nincreased compactness and richness. Based on NPR, we propose the prototypical\nguidance segmentation network (PGSeg) that incorporates multi-modal\nregularization by leveraging prototypical sources from both images and texts at\ndifferent levels, progressively enhancing the segmentation capability with\ndiverse prototypical patterns. Experimental results show that our proposed\nmethod achieves state-of-the-art performance on several benchmark datasets. The\nsource code is available at https://github.com/Ferenas/PGSeg.",
            "author": [
                "Fei Zhang",
                "Tianfei Zhou",
                "Boyang Li",
                "Hao He",
                "Chaofan Ma",
                "Tianjiao Zhang",
                "Jiangchao Yao",
                "Ya Zhang",
                "Yanfeng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19001v1",
                "http://arxiv.org/pdf/2310.19001v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18992v1",
            "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization\n  with Graph Convolutional Auto-Encoders",
            "updated": "2023-10-29T12:27:18Z",
            "published": "2023-10-29T12:27:18Z",
            "summary": "Pre-trained sentence representations are crucial for identifying significant\nsentences in unsupervised document extractive summarization. However, the\ntraditional two-step paradigm of pre-training and sentence-ranking, creates a\ngap due to differing optimization objectives. To address this issue, we argue\nthat utilizing pre-trained embeddings derived from a process specifically\ndesigned to optimize cohensive and distinctive sentence representations helps\nrank significant sentences. To do so, we propose a novel graph pre-training\nauto-encoder to obtain sentence embeddings by explicitly modelling\nintra-sentential distinctive features and inter-sentential cohesive features\nthrough sentence-word bipartite graphs. These pre-trained sentence\nrepresentations are then utilized in a graph-based ranking algorithm for\nunsupervised summarization. Our method produces predominant performance for\nunsupervised summarization frameworks by providing summary-worthy sentence\nrepresentations. It surpasses heavy BERT- or RoBERTa-based sentence\nrepresentations in downstream tasks.",
            "author": [
                "Qianren Mao",
                "Shaobo Zhao",
                "Jiarui Li",
                "Xiaolei Gu",
                "Shizhu He",
                "Bo Li",
                "Jianxin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18992v1",
                "http://arxiv.org/pdf/2310.18992v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18979v1",
            "title": "Methodology of Algorithm Engineering",
            "updated": "2023-10-29T11:14:02Z",
            "published": "2023-10-29T11:14:02Z",
            "summary": "Research on algorithms has drastically increased in recent years. Various\nsub-disciplines of computer science investigate algorithms according to\ndifferent objectives and standards. This plurality of the field has led to\nvarious methodological advances that have not yet been transferred to\nneighboring sub-disciplines. The central roadblock for a better knowledge\nexchange is the lack of a common methodological framework integrating the\nperspectives of these sub-disciplines. It is the objective of this paper to\ndevelop a research framework for algorithm engineering. Our framework builds on\nthree areas discussed in the philosophy of science: ontology, epistemology and\nmethodology. In essence, ontology describes algorithm engineering as being\nconcerned with algorithmic problems, algorithmic tasks, algorithm designs and\nalgorithm implementations. Epistemology describes the body of knowledge of\nalgorithm engineering as a collection of prescriptive and descriptive\nknowledge, residing in World 3 of Popper's Three Worlds model. Methodology\nrefers to the steps how we can systematically enhance our knowledge of specific\nalgorithms. The framework helps us to identify and discuss various validity\nconcerns relevant to any algorithm engineering contribution. In this way, our\nframework has important implications for researching algorithms in various\nareas of computer science.",
            "author": [
                "Jan Mendling",
                "Henrik Leopold",
                "Henning Meyerhenke",
                "Beno\u00eet Depaire"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18979v1",
                "http://arxiv.org/pdf/2310.18979v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18974v1",
            "title": "EtiCor: Corpus for Analyzing LLMs for Etiquettes",
            "updated": "2023-10-29T10:47:23Z",
            "published": "2023-10-29T10:47:23Z",
            "summary": "Etiquettes are an essential ingredient of day-to-day interactions among\npeople. Moreover, etiquettes are region-specific, and etiquettes in one region\nmight contradict those in other regions. In this paper, we propose EtiCor, an\nEtiquettes Corpus, having texts about social norms from five different regions\nacross the globe. The corpus provides a test bed for evaluating LLMs for\nknowledge and understanding of region-specific etiquettes. Additionally, we\npropose the task of Etiquette Sensitivity. We experiment with state-of-the-art\nLLMs (Delphi, Falcon40B, and GPT-3.5). Initial results indicate that LLMs,\nmostly fail to understand etiquettes from regions from non-Western world.",
            "author": [
                "Ashutosh Dwivedi",
                "Pradhyumna Lavania",
                "Ashutosh Modi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18974v1",
                "http://arxiv.org/pdf/2310.18974v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18971v1",
            "title": "Structures of $R(f)-\\overline{P(f)}$ for graph maps $f$",
            "updated": "2023-10-29T10:34:20Z",
            "published": "2023-10-29T10:34:20Z",
            "summary": "Let $G$ be a graph and $f: G\\rightarrow G$ be a continuous map. We establish\na structure theorem which describes the structures of the set\n$R(f)-\\overline{P(f)}$, where $R(f)$ and $P(f)$ are the recurrent point set and\nthe periodic point set of $f$ respectively. Roughly speaking, the set\n$R(f)-\\overline{P(f)}$ is covered by finitely many pairwise disjoint\n$f$-invariant open sets $U_{1\\,},\\,\\cdots,\\,U_{n\\,}$; each $U_i$ contains a\nunique minimal set $W_i$ which absorbs each point of $U_i$; each $W_i$ lies in\nfinitely many pairwise disjoint circles each of which is contained in a\nconnected closed set; all of these connected closed sets are contained in $U_i$\nand permutated cyclically by $f$. As applications of the structure theorem,\nseveral known results are improved or reproved.",
            "author": [
                "Jiehua Mai",
                "Enhui Shi",
                "Kesong Yan",
                "Fanping Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18971v1",
                "http://arxiv.org/pdf/2310.18971v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "37E25 (Primary), 37B20, 37C25, 54H20 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18951v1",
            "title": "A Multimodal Ecological Civilization Pattern Recommendation Method Based\n  on Large Language Models and Knowledge Graph",
            "updated": "2023-10-29T09:41:21Z",
            "published": "2023-10-29T09:41:21Z",
            "summary": "The Ecological Civilization Pattern Recommendation System (ECPRS) aims to\nrecommend suitable ecological civilization patterns for target regions,\npromoting sustainable development and reducing regional disparities. However,\nthe current representative recommendation methods are not suitable for\nrecommending ecological civilization patterns in a geographical context. There\nare two reasons for this. Firstly, regions have spatial heterogeneity, and the\n(ECPRS)needs to consider factors like climate, topography, vegetation, etc., to\nrecommend civilization patterns adapted to specific ecological environments,\nensuring the feasibility and practicality of the recommendations. Secondly, the\nabstract features of the ecological civilization patterns in the real world\nhave not been fully utilized., resulting in poor richness in their embedding\nrepresentations and consequently, lower performance of the recommendation\nsystem. Considering these limitations, we propose the ECPR-MML method.\nInitially, based on the novel method UGPIG, we construct a knowledge graph to\nextract regional representations incorporating spatial heterogeneity features.\nFollowing that, inspired by the significant progress made by Large Language\nModels (LLMs) in the field of Natural Language Processing (NLP), we employ\nLarge LLMs to generate multimodal features for ecological civilization patterns\nin the form of text and images. We extract and integrate these multimodal\nfeatures to obtain semantically rich representations of ecological\ncivilization. Through extensive experiments, we validate the performance of our\nECPR-MML model. Our results show that F1@5 is 2.11% higher compared to\nstate-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG.\nFurthermore, multimodal data can indeed enhance recommendation performance.\nHowever, the data generated by LLM is not as effective as real data to a\ncertain extent.",
            "author": [
                "Zhihang Yu",
                "Shu Wang",
                "Yunqiang Zhu",
                "Zhiqiang Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18951v1",
                "http://arxiv.org/pdf/2310.18951v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18933v1",
            "title": "Label Poisoning is All You Need",
            "updated": "2023-10-29T08:03:45Z",
            "published": "2023-10-29T08:03:45Z",
            "summary": "In a backdoor attack, an adversary injects corrupted data into a model's\ntraining dataset in order to gain control over its predictions on images with a\nspecific attacker-defined trigger. A typical corrupted training example\nrequires altering both the image, by applying the trigger, and the label.\nModels trained on clean images, therefore, were considered safe from backdoor\nattacks. However, in some common machine learning scenarios, the training\nlabels are provided by potentially malicious third-parties. This includes\ncrowd-sourced annotation and knowledge distillation. We, hence, investigate a\nfundamental question: can we launch a successful backdoor attack by only\ncorrupting labels? We introduce a novel approach to design label-only backdoor\nattacks, which we call FLIP, and demonstrate its strengths on three datasets\n(CIFAR-10, CIFAR-100, and Tiny-ImageNet) and four architectures (ResNet-32,\nResNet-18, VGG-19, and Vision Transformer). With only 2% of CIFAR-10 labels\ncorrupted, FLIP achieves a near-perfect attack success rate of 99.4% while\nsuffering only a 1.8% drop in the clean test accuracy. Our approach builds upon\nthe recent advances in trajectory matching, originally introduced for dataset\ndistillation.",
            "author": [
                "Rishi D. Jha",
                "Jonathan Hayase",
                "Sewoong Oh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18933v1",
                "http://arxiv.org/pdf/2310.18933v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18930v1",
            "title": "Retrofitting Light-weight Language Models for Emotions using Supervised\n  Contrastive Learning",
            "updated": "2023-10-29T07:43:34Z",
            "published": "2023-10-29T07:43:34Z",
            "summary": "We present a novel retrofitting method to induce emotion aspects into\npre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates\npre-trained network weights using contrastive learning so that the text\nfragments exhibiting similar emotions are encoded nearby in the representation\nspace, and the fragments with different emotion content are pushed apart. While\ndoing so, it also ensures that the linguistic knowledge already present in PLMs\nis not inadvertently perturbed. The language models retrofitted by our method,\ni.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as\nevaluated through different clustering and retrieval metrics. For the\ndownstream tasks on sentiment analysis and sarcasm detection, they perform\nbetter than their pre-trained counterparts (about 1% improvement in F1-score)\nand other existing approaches. Additionally, a more significant boost in\nperformance is observed for the retrofitted models over pre-trained ones in\nfew-shot learning setting.",
            "author": [
                "Sapan Shah",
                "Sreedhar Reddy",
                "Pushpak Bhattacharyya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18930v1",
                "http://arxiv.org/pdf/2310.18930v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18927v2",
            "title": "The physical basis of teleological evolutions",
            "updated": "2023-11-22T16:05:19Z",
            "published": "2023-10-29T07:38:24Z",
            "summary": "Optimal quantum algorithms have been shown to be quantum superpositions of\ncausal loops in each of which, logically, it is as if the problem solver knew\nin advance one of the possible halves of the information about the solution she\nwill produce and measure in the future and used this knowledge to produce the\nsolution with a correspondingly reduced number of computation steps. This also\nanswers the pre-scientific notion of teleological evolution, that is, an\nevolution toward a goal with an attractor in the goal it will produce in the\nfuture. This notion, formerly applied to the evolutions of the living, was\ndismissed with the advent of modern science for the alleged absence of a\nphysical basis. In this work, we show that the quantum causal loops at the\nphysical basis of the teleological character of quantum algorithms could also\nbe at the physical basis of that of the evolutions of life. A quantum\ncosmological interpretation of the Anthropic Principle yields a formal identity\nbetween a random search for life by a quantum evolution of the universe under\nthe fundamental physical laws and constants and a random search for the\nsolution by quantum search algorithms. According to it, the evolution of the\nuniverse toward life should be endowed with quantum speedup. We show that the\nevolution of a living system, in principle obtainable from that of the universe\nby tracing out all the rest, even if decohered, should have the same quantum\nspeedup of its image in the universe's evolution. It would therefore be\nteleological in character.",
            "author": [
                "Giuseppe Castagnoli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18927v2",
                "http://arxiv.org/pdf/2310.18927v2"
            ],
            "primary_category": "physics.gen-ph",
            "category": [
                "physics.gen-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18923v2",
            "title": "Random generation of subgroups of the modular group with a fixed\n  isomorphism type",
            "updated": "2023-11-14T09:31:05Z",
            "published": "2023-10-29T07:04:23Z",
            "summary": "We show how to efficiently count and generate uniformly at random finitely\ngenerated subgroups of the modular group $\\textsf{PSL}(2,\\mathbb{Z})$ of a\ngiven isomorphism type. The method to achieve these results relies on a natural\nmap of independent interest, which associates with any finitely generated\nsubgroup of $\\textsf{PSL}(2,\\mathbb{Z})$ a graph which we call its silhouette,\nand which can be interpreted as a conjugacy class of free finite index\nsubgroups of $\\textsf{PSL}(2,\\mathbb{Z})$.",
            "author": [
                "Fr\u00e9d\u00e9rique Bassino",
                "Cyril Nicaud",
                "Pascal Weil"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18923v2",
                "http://arxiv.org/pdf/2310.18923v2"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20E07, 20F69, 05A15, 05E16, 05C30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18918v1",
            "title": "Hyperbolic Graph Neural Networks at Scale: A Meta Learning Approach",
            "updated": "2023-10-29T06:11:49Z",
            "published": "2023-10-29T06:11:49Z",
            "summary": "The progress in hyperbolic neural networks (HNNs) research is hindered by\ntheir absence of inductive bias mechanisms, which are essential for\ngeneralizing to new tasks and facilitating scalable learning over large\ndatasets. In this paper, we aim to alleviate these issues by learning\ngeneralizable inductive biases from the nodes' local subgraph and transfer them\nfor faster learning over new subgraphs with a disjoint set of nodes, edges, and\nlabels in a few-shot setting. We introduce a novel method, Hyperbolic GRAph\nMeta Learner (H-GRAM), that, for the tasks of node classification and link\nprediction, learns transferable information from a set of support local\nsubgraphs in the form of hyperbolic meta gradients and label hyperbolic\nprotonets to enable faster learning over a query set of new tasks dealing with\ndisjoint subgraphs. Furthermore, we show that an extension of our meta-learning\nframework also mitigates the scalability challenges seen in HNNs faced by\nexisting approaches. Our comparative analysis shows that H-GRAM effectively\nlearns and transfers information in multiple challenging few-shot settings\ncompared to other state-of-the-art baselines. Additionally, we demonstrate\nthat, unlike standard HNNs, our approach is able to scale over large graph\ndatasets and improve performance over its Euclidean counterparts.",
            "author": [
                "Nurendra Choudhary",
                "Nikhil Rao",
                "Chandan K. Reddy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18918v1",
                "http://arxiv.org/pdf/2310.18918v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18912v1",
            "title": "Sentence Bag Graph Formulation for Biomedical Distant Supervision\n  Relation Extraction",
            "updated": "2023-10-29T05:48:04Z",
            "published": "2023-10-29T05:48:04Z",
            "summary": "We introduce a novel graph-based framework for alleviating key challenges in\ndistantly-supervised relation extraction and demonstrate its effectiveness in\nthe challenging and important domain of biomedical data. Specifically, we\npropose a graph view of sentence bags referring to an entity pair, which\nenables message-passing based aggregation of information related to the entity\npair over the sentence bag. The proposed framework alleviates the common\nproblem of noisy labeling in distantly supervised relation extraction and also\neffectively incorporates inter-dependencies between sentences within a bag.\nExtensive experiments on two large-scale biomedical relation datasets and the\nwidely utilized NYT dataset demonstrate that our proposed framework\nsignificantly outperforms the state-of-the-art methods for biomedical distant\nsupervision relation extraction while also providing excellent performance for\nrelation extraction in the general text mining domain.",
            "author": [
                "Hao Zhang",
                "Yang Liu",
                "Xiaoyan Liu",
                "Tianming Liang",
                "Gaurav Sharma",
                "Liang Xue",
                "Maozu Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18912v1",
                "http://arxiv.org/pdf/2310.18912v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18907v1",
            "title": "Topological, or Non-topological? A Deep Learning Based Prediction",
            "updated": "2023-10-29T05:29:49Z",
            "published": "2023-10-29T05:29:49Z",
            "summary": "Prediction and discovery of new materials with desired properties are at the\nforefront of quantum science and technology research. A major bottleneck in\nthis field is the computational resources and time complexity related to\nfinding new materials from ab initio calculations. In this work, an effective\nand robust deep learning-based model is proposed by incorporating persistent\nhomology and graph neural network which offers an accuracy of 91.4% and an F1\nscore of 88.5% in classifying topological vs. non-topological materials,\noutperforming the other state-of-the-art classifier models. The incorporation\nof the graph neural network encodes the underlying relation between the atoms\ninto the model based on their own crystalline structures and thus proved to be\nan effective method to represent and process non-euclidean data like molecules\nwith a relatively shallow network. The persistent homology pipeline in the\nsuggested neural network is capable of integrating the atom-specific\ntopological information into the deep learning model, increasing robustness,\nand gain in performance. It is believed that the presented work will be an\nefficacious tool for predicting the topological class and therefore enable the\nhigh-throughput search for novel materials in this field.",
            "author": [
                "Ashiqur Rasul",
                "Md Shafayat Hossain",
                "Ankan Ghosh Dastider",
                "Himaddri Roy",
                "M. Zahid Hasan",
                "Quazi D. M. Khosru"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18907v1",
                "http://arxiv.org/pdf/2310.18907v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18903v1",
            "title": "Visibility graph analysis of crude oil futures markets: Insights from\n  the COVID-19 pandemic and Russia-Ukraine conflict",
            "updated": "2023-10-29T05:20:15Z",
            "published": "2023-10-29T05:20:15Z",
            "summary": "Drawing inspiration from the significant impact of the ongoing Russia-Ukraine\nconflict and the recent COVID-19 pandemic on global financial markets, this\nstudy conducts a thorough analysis of three key crude oil futures markets: WTI,\nBrent, and Shanghai (SC). Employing the visibility graph (VG) methodology, we\nexamine both static and dynamic characteristics using daily and high-frequency\ndata. We identified a clear power-law decay in most VG degree distributions and\nhighlighted the pronounced clustering tendencies within crude oil futures VGs.\nOur results also confirm an inverse correlation between clustering coefficient\nand node degree and further reveal that all VGs not only adhere to the\nsmall-world property but also exhibit intricate assortative mixing. Through the\ntime-varying characteristics of VGs, we found that WTI and Brent demonstrate\naligned behavior, while the SC market, with its unique trading mechanics,\ndeviates. The 5-minute VGs' assortativity coefficient provides a deeper\nunderstanding of these markets' reactions to the pandemic and geopolitical\nevents. Furthermore, the differential responses during the COVID-19 and\nRussia-Ukraine conflict underline the unique sensitivities of each market to\nglobal disruptions. Overall, this research offers profound insights into the\nstructure, dynamics, and adaptability of these essential commodities markets in\nthe face of worldwide challenges.",
            "author": [
                "Ying-Hui Shao",
                "Yan-Hong Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18903v1",
                "http://arxiv.org/pdf/2310.18903v1"
            ],
            "primary_category": "q-fin.ST",
            "category": [
                "q-fin.ST",
                "q-fin.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18893v1",
            "title": "Ever Evolving Evaluator (EV3): Towards Flexible and Reliable\n  Meta-Optimization for Knowledge Distillation",
            "updated": "2023-10-29T04:00:33Z",
            "published": "2023-10-29T04:00:33Z",
            "summary": "We introduce EV3, a novel meta-optimization framework designed to efficiently\ntrain scalable machine learning models through an intuitive\nexplore-assess-adapt protocol. In each iteration of EV3, we explore various\nmodel parameter updates, assess them using pertinent evaluation methods, and\nadapt the model based on the optimal updates and previous progress history. EV3\noffers substantial flexibility without imposing stringent constraints like\ndifferentiability on the key objectives relevant to the tasks of interest.\nMoreover, this protocol welcomes updates with biased gradients and allows for\nthe use of a diversity of losses and optimizers. Additionally, in scenarios\nwith multiple objectives, it can be used to dynamically prioritize tasks. With\ninspiration drawn from evolutionary algorithms, meta-learning, and neural\narchitecture search, we investigate an application of EV3 to knowledge\ndistillation. Our experimental results illustrate EV3's capability to safely\nexplore model spaces, while hinting at its potential applicability across\nnumerous domains due to its inherent flexibility and adaptability.",
            "author": [
                "Li Ding",
                "Masrour Zoghi",
                "Guy Tennenholtz",
                "Maryam Karimzadehgan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18893v1",
                "http://arxiv.org/pdf/2310.18893v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18890v1",
            "title": "Towards Generalized Multi-stage Clustering: Multi-view Self-distillation",
            "updated": "2023-10-29T03:35:34Z",
            "published": "2023-10-29T03:35:34Z",
            "summary": "Existing multi-stage clustering methods independently learn the salient\nfeatures from multiple views and then perform the clustering task.\nParticularly, multi-view clustering (MVC) has attracted a lot of attention in\nmulti-view or multi-modal scenarios. MVC aims at exploring common semantics and\npseudo-labels from multiple views and clustering in a self-supervised manner.\nHowever, limited by noisy data and inadequate feature learning, such a\nclustering paradigm generates overconfident pseudo-labels that mis-guide the\nmodel to produce inaccurate predictions. Therefore, it is desirable to have a\nmethod that can correct this pseudo-label mistraction in multi-stage clustering\nto avoid the bias accumulation. To alleviate the effect of overconfident\npseudo-labels and improve the generalization ability of the model, this paper\nproposes a novel multi-stage deep MVC framework where multi-view\nself-distillation (DistilMVC) is introduced to distill dark knowledge of label\ndistribution. Specifically, in the feature subspace at different hierarchies,\nwe explore the common semantics of multiple views through contrastive learning\nand obtain pseudo-labels by maximizing the mutual information between views.\nAdditionally, a teacher network is responsible for distilling pseudo-labels\ninto dark knowledge, supervising the student network and improving its\npredictive capabilities to enhance the robustness. Extensive experiments on\nreal-world multi-view datasets show that our method has better clustering\nperformance than state-of-the-art methods.",
            "author": [
                "Jiatai Wang",
                "Zhiwei Xu",
                "Xin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18890v1",
                "http://arxiv.org/pdf/2310.18890v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18885v1",
            "title": "A foundational neural operator that continuously learns without\n  forgetting",
            "updated": "2023-10-29T03:20:10Z",
            "published": "2023-10-29T03:20:10Z",
            "summary": "Machine learning has witnessed substantial growth, leading to the development\nof advanced artificial intelligence models crafted to address a wide range of\nreal-world challenges spanning various domains, such as computer vision,\nnatural language processing, and scientific computing. Nevertheless, the\ncreation of custom models for each new task remains a resource-intensive\nundertaking, demanding considerable computational time and memory resources. In\nthis study, we introduce the concept of the Neural Combinatorial Wavelet Neural\nOperator (NCWNO) as a foundational model for scientific computing. This model\nis specifically designed to excel in learning from a diverse spectrum of\nphysics and continuously adapt to the solution operators associated with\nparametric partial differential equations (PDEs). The NCWNO leverages a gated\nstructure that employs local wavelet experts to acquire shared features across\nmultiple physical systems, complemented by a memory-based ensembling approach\namong these local wavelet experts. This combination enables rapid adaptation to\nnew challenges. The proposed foundational model offers two key advantages: (i)\nit can simultaneously learn solution operators for multiple parametric PDEs,\nand (ii) it can swiftly generalize to new parametric PDEs with minimal\nfine-tuning. The proposed NCWNO is the first foundational operator learning\nalgorithm distinguished by its (i) robustness against catastrophic forgetting,\n(ii) the maintenance of positive transfer for new parametric PDEs, and (iii)\nthe facilitation of knowledge transfer across dissimilar tasks. Through an\nextensive set of benchmark examples, we demonstrate that the NCWNO can\noutperform task-specific baseline operator learning frameworks with minimal\nhyperparameter tuning at the prediction stage. We also show that with minimal\nfine-tuning, the NCWNO performs accurate combinatorial learning of new\nparametric PDEs.",
            "author": [
                "Tapas Tripura",
                "Souvik Chakraborty"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18885v1",
                "http://arxiv.org/pdf/2310.18885v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18884v1",
            "title": "Simple and Asymmetric Graph Contrastive Learning without Augmentations",
            "updated": "2023-10-29T03:14:20Z",
            "published": "2023-10-29T03:14:20Z",
            "summary": "Graph Contrastive Learning (GCL) has shown superior performance in\nrepresentation learning in graph-structured data. Despite their success, most\nexisting GCL methods rely on prefabricated graph augmentation and homophily\nassumptions. Thus, they fail to generalize well to heterophilic graphs where\nconnected nodes may have different class labels and dissimilar features. In\nthis paper, we study the problem of conducting contrastive learning on\nhomophilic and heterophilic graphs. We find that we can achieve promising\nperformance simply by considering an asymmetric view of the neighboring nodes.\nThe resulting simple algorithm, Asymmetric Contrastive Learning for Graphs\n(GraphACL), is easy to implement and does not rely on graph augmentations and\nhomophily assumptions. We provide theoretical and empirical evidence that\nGraphACL can capture one-hop local neighborhood information and two-hop\nmonophily similarity, which are both important for modeling heterophilic\ngraphs. Experimental results show that the simple GraphACL significantly\noutperforms state-of-the-art graph contrastive learning and self-supervised\nlearning methods on homophilic and heterophilic graphs. The code of GraphACL is\navailable at https://github.com/tengxiao1/GraphACL.",
            "author": [
                "Teng Xiao",
                "Huaisheng Zhu",
                "Zhengyu Chen",
                "Suhang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18884v1",
                "http://arxiv.org/pdf/2310.18884v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18883v1",
            "title": "A Zeroth-Order Variance-Reduced Method for Decentralized Stochastic\n  Non-convex Optimization",
            "updated": "2023-10-29T03:13:45Z",
            "published": "2023-10-29T03:13:45Z",
            "summary": "In this paper, we consider a distributed stochastic non-convex optimization\nproblem, which is about minimizing a sum of $n$ local cost functions over a\nnetwork with only zeroth-order information. A novel single-loop Decentralized\nZeroth-Order Variance Reduction algorithm, called DZOVR, is proposed, which\ncombines two-point gradient estimation, momentum-based variance reduction\ntechnique, and gradient tracking. Under mild assumptions, we show that the\nalgorithm is able to achieve $\\mathcal{O}(dn^{-1}\\epsilon^{-3})$ sampling\ncomplexity at each node to reach an $\\epsilon$-accurate stationary point and\nalso exhibits network-independent and linear speedup properties. To the best of\nour knowledge, this is the first stochastic decentralized zeroth-order\nalgorithm that achieves this sampling complexity. Numerical experiments\ndemonstrate that DZOVR outperforms the other state-of-the-art algorithms and\nhas network-independent and linear speedup properties.",
            "author": [
                "Hongxu Chen",
                "Jinchi Chen",
                "Ke Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18883v1",
                "http://arxiv.org/pdf/2310.18883v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18871v1",
            "title": "Compressed Gradient Tracking Algorithms for Distributed Nonconvex\n  Optimization",
            "updated": "2023-10-29T02:03:21Z",
            "published": "2023-10-29T02:03:21Z",
            "summary": "In this paper, we study the distributed nonconvex optimization problem, which\naims to minimize the average value of the local nonconvex cost functions using\nlocal information exchange. To reduce the communication overhead, we introduce\nthree general classes of compressors, i.e., compressors with bounded relative\ncompression error, compressors with globally bounded absolute compression\nerror, and compressors with locally bounded absolute compression error. By\nintegrating them with distributed gradient tracking algorithm, we then propose\nthree compressed distributed nonconvex optimization algorithms. For each\nalgorithm, we design a novel Lyapunov function to demonstrate its sublinear\nconvergence to a stationary point if the local cost functions are smooth.\nFurthermore, when the global cost function satisfies the Polyak--{\\L}ojasiewicz\n(P--{\\L}) condition, we show that our proposed algorithms linearly converge to\na global optimal point. It is worth noting that, for compressors with bounded\nrelative compression error and globally bounded absolute compression error, our\nproposed algorithms' parameters do not require prior knowledge of the P--{\\L}\nconstant. The theoretical results are illustrated by numerical examples, which\ndemonstrate the effectiveness of the proposed algorithms in significantly\nreducing the communication burden while maintaining the convergence\nperformance. Moreover, simulation results show that the proposed algorithms\noutperform state-of-the-art compressed distributed nonconvex optimization\nalgorithms.",
            "author": [
                "Lei Xu",
                "Xinlei Yi",
                "Guanghui Wen",
                "Yang Shi",
                "Karl H. Johansson",
                "Tao Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18871v1",
                "http://arxiv.org/pdf/2310.18871v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18865v1",
            "title": "MUST: A Multilingual Student-Teacher Learning approach for low-resource\n  speech recognition",
            "updated": "2023-10-29T01:38:36Z",
            "published": "2023-10-29T01:38:36Z",
            "summary": "Student-teacher learning or knowledge distillation (KD) has been previously\nused to address data scarcity issue for training of speech recognition (ASR)\nsystems. However, a limitation of KD training is that the student model classes\nmust be a proper or improper subset of the teacher model classes. It prevents\ndistillation from even acoustically similar languages if the character sets are\nnot same. In this work, the aforementioned limitation is addressed by proposing\na MUltilingual Student-Teacher (MUST) learning which exploits a posteriors\nmapping approach. A pre-trained mapping model is used to map posteriors from a\nteacher language to the student language ASR. These mapped posteriors are used\nas soft labels for KD learning. Various teacher ensemble schemes are\nexperimented to train an ASR model for low-resource languages. A model trained\nwith MUST learning reduces relative character error rate (CER) up to 9.5% in\ncomparison with a baseline monolingual ASR.",
            "author": [
                "Muhammad Umar Farooq",
                "Rehan Ahmad",
                "Thomas Hain"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18865v1",
                "http://arxiv.org/pdf/2310.18865v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18852v2",
            "title": "AI for Open Science: A Multi-Agent Perspective for Ethically Translating\n  Data to Knowledge",
            "updated": "2023-10-31T17:54:20Z",
            "published": "2023-10-28T23:57:15Z",
            "summary": "AI for Science (AI4Science), particularly in the form of self-driving labs,\nhas the potential to sideline human involvement and hinder scientific discovery\nwithin the broader community. While prior research has focused on ensuring the\nresponsible deployment of AI applications, enhancing security, and ensuring\ninterpretability, we also propose that promoting openness in AI4Science\ndiscoveries should be carefully considered. In this paper, we introduce the\nconcept of AI for Open Science (AI4OS) as a multi-agent extension of AI4Science\nwith the core principle of maximizing open knowledge translation throughout the\nscientific enterprise rather than a single organizational unit. We use the\nestablished principles of Knowledge Discovery and Data Mining (KDD) to\nformalize a language around AI4OS. We then discuss three principle stages of\nknowledge translation embedded in AI4Science systems and detail specific points\nwhere openness can be applied to yield an AI4OS alternative. Lastly, we\nformulate a theoretical metric to assess AI4OS with a supporting ethical\nargument highlighting its importance. Our goal is that by drawing attention to\nAI4OS we can ensure the natural consequence of AI4Science (e.g., self-driving\nlabs) is a benefit not only for its developers but for society as a whole.",
            "author": [
                "Chase Yakaboski",
                "Gregory Hyde",
                "Clement Nyanhongo",
                "Eugene Santos Jr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18852v2",
                "http://arxiv.org/pdf/2310.18852v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18846v1",
            "title": "INCODE: Implicit Neural Conditioning with Prior Knowledge Embeddings",
            "updated": "2023-10-28T23:16:49Z",
            "published": "2023-10-28T23:16:49Z",
            "summary": "Implicit Neural Representations (INRs) have revolutionized signal\nrepresentation by leveraging neural networks to provide continuous and smooth\nrepresentations of complex data. However, existing INRs face limitations in\ncapturing fine-grained details, handling noise, and adapting to diverse signal\ntypes. To address these challenges, we introduce INCODE, a novel approach that\nenhances the control of the sinusoidal-based activation function in INRs using\ndeep prior knowledge. INCODE comprises a harmonizer network and a composer\nnetwork, where the harmonizer network dynamically adjusts key parameters of the\nactivation function. Through a task-specific pre-trained model, INCODE adapts\nthe task-specific parameters to optimize the representation process. Our\napproach not only excels in representation, but also extends its prowess to\ntackle complex tasks such as audio, image, and 3D shape reconstructions, as\nwell as intricate challenges such as neural radiance fields (NeRFs), and\ninverse problems, including denoising, super-resolution, inpainting, and CT\nreconstruction. Through comprehensive experiments, INCODE demonstrates its\nsuperiority in terms of robustness, accuracy, quality, and convergence rate,\nbroadening the scope of signal representation. Please visit the project's\nwebsite for details on the proposed method and access to the code.",
            "author": [
                "Amirhossein Kazerouni",
                "Reza Azad",
                "Alireza Hosseini",
                "Dorit Merhof",
                "Ulas Bagci"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18846v1",
                "http://arxiv.org/pdf/2310.18846v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18831v1",
            "title": "Paired 2-disjoint path covers of burnt pancake graphs with faulty\n  elements",
            "updated": "2023-10-28T22:11:30Z",
            "published": "2023-10-28T22:11:30Z",
            "summary": "The burnt pancake graph $BP_n$ is the Cayley graph of the hyperoctahedral\ngroup using prefix reversals as generators. Let $\\{u,v\\}$ and $\\{x,y\\}$ be any\ntwo pairs of distinct vertices of $BP_n$ for $n\\geq 4$. We show that there are\n$u-v$ and $x-y$ paths whose vertices partition the vertex set of $BP_n$ even if\n$BP_n$ has up to $n-4$ faulty elements. On the other hand, for every $n\\ge3$\nthere is a set of $n-2$ faulty edges or faulty vertices for which such a\nfault-free disjoint path cover does not exist.",
            "author": [
                "Tom\u00e1\u0161 Dvo\u0159\u00e1k",
                "Mei-Mei Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18831v1",
                "http://arxiv.org/pdf/2310.18831v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "math.CO",
                "05C70, 05C45, 68M10, 68R10",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18828v1",
            "title": "Kerr-Enhanced Optical Spring",
            "updated": "2023-10-28T21:57:04Z",
            "published": "2023-10-28T21:57:04Z",
            "summary": "We propose and experimentally demonstrate the generation of enhanced optical\nsprings using the optical Kerr effect. A nonlinear optical crystal is inserted\ninto a Fabry-Perot cavity with a movable mirror, and a chain of second-order\nnonlinear optical effects in the phase-mismatched condition induces the Kerr\neffect. The optical spring constant is enhanced by a factor of $1.6\\pm0.1$ over\nlinear theory. To our knowledge, this is the first realization of\noptomechanical coupling enhancement using a nonlinear optical effect, which has\nbeen theoretically investigated to overcome the performance limitations of\nlinear optomechanical systems. The tunable nonlinearity of demonstrated system\nhas a wide range of potential applications, from observing gravitational waves\nemitted by binary neutron star post-merger remnants to cooling macroscopic\noscillators to their quantum ground state.",
            "author": [
                "Sotatsu Otabe",
                "Wataru Usukura",
                "Kaido Suzuki",
                "Kentaro Komori",
                "Yuta Michimura",
                "Ken-ichi Harada",
                "Kentaro Somiya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18828v1",
                "http://arxiv.org/pdf/2310.18828v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "gr-qc",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18811v1",
            "title": "Hierarchical Framework for Interpretable and Probabilistic Model-Based\n  Safe Reinforcement Learning",
            "updated": "2023-10-28T20:30:57Z",
            "published": "2023-10-28T20:30:57Z",
            "summary": "The difficulty of identifying the physical model of complex systems has led\nto exploring methods that do not rely on such complex modeling of the systems.\nDeep reinforcement learning has been the pioneer for solving this problem\nwithout the need for relying on the physical model of complex systems by just\ninteracting with it. However, it uses a black-box learning approach that makes\nit difficult to be applied within real-world and safety-critical systems\nwithout providing explanations of the actions derived by the model.\nFurthermore, an open research question in deep reinforcement learning is how to\nfocus the policy learning of critical decisions within a sparse domain. This\npaper proposes a novel approach for the use of deep reinforcement learning in\nsafety-critical systems. It combines the advantages of probabilistic modeling\nand reinforcement learning with the added benefits of interpretability and\nworks in collaboration and synchronization with conventional decision-making\nstrategies. The BC-SRLA is activated in specific situations which are\nidentified autonomously through the fused information of probabilistic model\nand reinforcement learning, such as abnormal conditions or when the system is\nnear-to-failure. Further, it is initialized with a baseline policy using policy\ncloning to allow minimum interactions with the environment to address the\nchallenges associated with using RL in safety-critical industries. The\neffectiveness of the BC-SRLA is demonstrated through a case study in\nmaintenance applied to turbofan engines, where it shows superior performance to\nthe prior art and other baselines.",
            "author": [
                "Ammar N. Abbas",
                "Georgios C. Chasparis",
                "John D. Kelleher"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18811v1",
                "http://arxiv.org/pdf/2310.18811v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18804v1",
            "title": "Open Visual Knowledge Extraction via Relation-Oriented Multimodality\n  Model Prompting",
            "updated": "2023-10-28T20:09:29Z",
            "published": "2023-10-28T20:09:29Z",
            "summary": "Images contain rich relational knowledge that can help machines understand\nthe world. Existing methods on visual knowledge extraction often rely on the\npre-defined format (e.g., sub-verb-obj tuples) or vocabulary (e.g., relation\ntypes), restricting the expressiveness of the extracted knowledge. In this\nwork, we take a first exploration to a new paradigm of open visual knowledge\nextraction. To achieve this, we present OpenVik which consists of an open\nrelational region detector to detect regions potentially containing relational\nknowledge and a visual knowledge generator that generates format-free knowledge\nby prompting the large multimodality model with the detected region of\ninterest. We also explore two data enhancement techniques for diversifying the\ngenerated format-free visual knowledge. Extensive knowledge quality evaluations\nhighlight the correctness and uniqueness of the extracted open visual knowledge\nby OpenVik. Moreover, integrating our extracted knowledge across various visual\nreasoning applications shows consistent improvements, indicating the real-world\napplicability of OpenVik.",
            "author": [
                "Hejie Cui",
                "Xinyu Fang",
                "Zihan Zhang",
                "Ran Xu",
                "Xuan Kan",
                "Xin Liu",
                "Yue Yu",
                "Manling Li",
                "Yangqiu Song",
                "Carl Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18804v1",
                "http://arxiv.org/pdf/2310.18804v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18801v2",
            "title": "Integrated Relative-Measurement-Based Network Localization and Formation\n  Maneuver Control (Extended Version)",
            "updated": "2023-11-14T01:54:17Z",
            "published": "2023-10-28T20:06:43Z",
            "summary": "This paper studies the problem of integrated distributed network localization\nand formation maneuver control. We develop an integrated\nrelative-measurement-based scheme, which only uses relative positions,\ndistances, bearings, angles, ratio-of-distances, or their combination to\nachieve distributed network localization and formation maneuver control in\n$\\mathbb{R}^d (d \\ge 2)$. By exploring the localizability and invariance of the\ntarget formation, the scale, rotation, and translation of the formation can be\ncontrolled simultaneously by only tuning the leaders' positions, i.e., the\nfollowers do not need to know parameters of the scale, rotation, and\ntranslation of the target formation. The proposed method can globally drive the\nformation errors to zero in finite time over multi-layer $d\\!+\\!1$-rooted\ngraphs. A simulation example is given to illustrate the theoretical results.",
            "author": [
                "Xu Fang",
                "Lihua Xie",
                "Xiaolei Li"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TAC.2023.3330801",
                "http://arxiv.org/abs/2310.18801v2",
                "http://arxiv.org/pdf/2310.18801v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18794v1",
            "title": "Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded\n  Dialogue Generation",
            "updated": "2023-10-28T19:42:28Z",
            "published": "2023-10-28T19:42:28Z",
            "summary": "Model hallucination has been a crucial interest of research in Natural\nLanguage Generation (NLG). In this work, we propose sequence-level certainty as\na common theme over hallucination in NLG, and explore the correlation between\nsequence-level certainty and the level of hallucination in model responses. We\ncategorize sequence-level certainty into two aspects: probabilistic certainty\nand semantic certainty, and reveal through experiments on Knowledge-Grounded\nDialogue Generation (KGDG) task that both a higher level of probabilistic\ncertainty and a higher level of semantic certainty in model responses are\nsignificantly correlated with a lower level of hallucination. What's more, we\nprovide theoretical proof and analysis to show that semantic certainty is a\ngood estimator of probabilistic certainty, and therefore has the potential as\nan alternative to probability-based certainty estimation in black-box\nscenarios. Based on the observation on the relationship between certainty and\nhallucination, we further propose Certainty-based Response Ranking (CRR), a\ndecoding-time method for mitigating hallucination in NLG. Based on our\ncategorization of sequence-level certainty, we propose 2 types of CRR approach:\nProbabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually\nsampled model responses using their arithmetic mean log-probability of the\nentire sequence. S-CRR approaches certainty estimation from meaning-space, and\nranks a number of model response candidates based on their semantic certainty\nlevel, which is estimated by the entailment-based Agreement Score (AS). Through\nextensive experiments across 3 KGDG datasets, 3 decoding methods, and on 4\ndifferent models, we validate the effectiveness of our 2 proposed CRR methods\nto reduce model hallucination.",
            "author": [
                "Yixin Wan",
                "Fanyou Wu",
                "Weijie Xu",
                "Srinivasan H. Sengamedu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18794v1",
                "http://arxiv.org/pdf/2310.18794v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18783v1",
            "title": "Are NLP Models Good at Tracing Thoughts: An Overview of Narrative\n  Understanding",
            "updated": "2023-10-28T18:47:57Z",
            "published": "2023-10-28T18:47:57Z",
            "summary": "Narrative understanding involves capturing the author's cognitive processes,\nproviding insights into their knowledge, intentions, beliefs, and desires.\nAlthough large language models (LLMs) excel in generating grammatically\ncoherent text, their ability to comprehend the author's thoughts remains\nuncertain. This limitation hinders the practical applications of narrative\nunderstanding. In this paper, we conduct a comprehensive survey of narrative\nunderstanding tasks, thoroughly examining their key features, definitions,\ntaxonomy, associated datasets, training objectives, evaluation metrics, and\nlimitations. Furthermore, we explore the potential of expanding the\ncapabilities of modularized LLMs to address novel narrative understanding\ntasks. By framing narrative understanding as the retrieval of the author's\nimaginative cues that outline the narrative structure, our study introduces a\nfresh perspective on enhancing narrative comprehension.",
            "author": [
                "Lixing Zhu",
                "Runcong Zhao",
                "Lin Gui",
                "Yulan He"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18783v1",
                "http://arxiv.org/pdf/2310.18783v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18777v1",
            "title": "Improving Compositional Generalization Using Iterated Learning and\n  Simplicial Embeddings",
            "updated": "2023-10-28T18:30:30Z",
            "published": "2023-10-28T18:30:30Z",
            "summary": "Compositional generalization, the ability of an agent to generalize to unseen\ncombinations of latent factors, is easy for humans but hard for deep neural\nnetworks. A line of research in cognitive science has hypothesized a process,\n``iterated learning,'' to help explain how human language developed this\nability; the theory rests on simultaneous pressures towards compressibility\n(when an ignorant agent learns from an informed one) and expressivity (when it\nuses the representation for downstream tasks). Inspired by this process, we\npropose to improve the compositional generalization of deep networks by using\niterated learning on models with simplicial embeddings, which can approximately\ndiscretize representations. This approach is further motivated by an analysis\nof compositionality based on Kolmogorov complexity. We show that this\ncombination of changes improves compositional generalization over other\napproaches, demonstrating these improvements both on vision tasks with\nwell-understood latent factors and on real molecular graph prediction tasks\nwhere the latent structure is unknown.",
            "author": [
                "Yi Ren",
                "Samuel Lavoie",
                "Mikhail Galkin",
                "Danica J. Sutherland",
                "Aaron Courville"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18777v1",
                "http://arxiv.org/pdf/2310.18777v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18773v1",
            "title": "CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale\n  Point Cloud Data",
            "updated": "2023-10-28T18:05:32Z",
            "published": "2023-10-28T18:05:32Z",
            "summary": "City-scale 3D point cloud is a promising way to express detailed and\ncomplicated outdoor structures. It encompasses both the appearance and geometry\nfeatures of segmented city components, including cars, streets, and buildings,\nthat can be utilized for attractive applications such as user-interactive\nnavigation of autonomous vehicles and drones. However, compared to the\nextensive text annotations available for images and indoor scenes, the scarcity\nof text annotations for outdoor scenes poses a significant challenge for\nachieving these applications. To tackle this problem, we introduce the\nCityRefer dataset for city-level visual grounding. The dataset consists of 35k\nnatural language descriptions of 3D objects appearing in SensatUrban city\nscenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the\nquality and accuracy of the dataset, all descriptions and labels in the\nCityRefer dataset are manually verified. We also have developed a baseline\nsystem that can learn encoded language descriptions, 3D object instances, and\ngeographical information about the city's landmarks to perform visual grounding\non the CityRefer dataset. To the best of our knowledge, the CityRefer dataset\nis the largest city-level visual grounding dataset for localizing specific 3D\nobjects.",
            "author": [
                "Taiki Miyanishi",
                "Fumiya Kitamori",
                "Shuhei Kurita",
                "Jungdae Lee",
                "Motoaki Kawanabe",
                "Nakamasa Inoue"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18773v1",
                "http://arxiv.org/pdf/2310.18773v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18765v2",
            "title": "Rethinking Semi-Supervised Imbalanced Node Classification from\n  Bias-Variance Decomposition",
            "updated": "2023-11-10T13:23:07Z",
            "published": "2023-10-28T17:28:07Z",
            "summary": "This paper introduces a new approach to address the issue of class imbalance\nin graph neural networks (GNNs) for learning on graph-structured data. Our\napproach integrates imbalanced node classification and Bias-Variance\nDecomposition, establishing a theoretical framework that closely relates data\nimbalance to model variance. We also leverage graph augmentation technique to\nestimate the variance, and design a regularization term to alleviate the impact\nof imbalance. Exhaustive tests are conducted on multiple benchmarks, including\nnaturally imbalanced datasets and public-split class-imbalanced datasets,\ndemonstrating that our approach outperforms state-of-the-art methods in various\nimbalanced scenarios. This work provides a novel theoretical perspective for\naddressing the problem of imbalanced node classification in GNNs.",
            "author": [
                "Divin Yan",
                "Gengchen Wei",
                "Chen Yang",
                "Shengzhong Zhang",
                "Zengfeng Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18765v2",
                "http://arxiv.org/pdf/2310.18765v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18742v1",
            "title": "Data Ambiguity Strikes Back: How Documentation Improves GPT's\n  Text-to-SQL",
            "updated": "2023-10-28T15:55:53Z",
            "published": "2023-10-28T15:55:53Z",
            "summary": "Text-to-SQL allows experts to use databases without in-depth knowledge of\nthem. However, real-world tasks have both query and data ambiguities. Most\nworks on Text-to-SQL focused on query ambiguities and designed chat interfaces\nfor experts to provide clarifications. In contrast, the data management\ncommunity has long studied data ambiguities, but mainly addresses error\ndetection and correction, rather than documenting them for disambiguation in\ndata tasks. This work delves into these data ambiguities in real-world\ndatasets. We have identified prevalent data ambiguities of value consistency,\ndata coverage, and data granularity that affect tasks. We examine how\ndocumentation, originally made to help humans to disambiguate data, can help\nGPT-4 with Text-to-SQL tasks. By offering documentation on these, we found\nGPT-4's performance improved by 28.9%.",
            "author": [
                "Zezhou Huang",
                "Pavan Kalyan Damalapati",
                "Eugene Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18742v1",
                "http://arxiv.org/pdf/2310.18742v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18740v1",
            "title": "TraceDiag: Adaptive, Interpretable, and Efficient Root Cause Analysis on\n  Large-Scale Microservice Systems",
            "updated": "2023-10-28T15:49:00Z",
            "published": "2023-10-28T15:49:00Z",
            "summary": "Root Cause Analysis (RCA) is becoming increasingly crucial for ensuring the\nreliability of microservice systems. However, performing RCA on modern\nmicroservice systems can be challenging due to their large scale, as they\nusually comprise hundreds of components, leading significant human effort. This\npaper proposes TraceDiag, an end-to-end RCA framework that addresses the\nchallenges for large-scale microservice systems. It leverages reinforcement\nlearning to learn a pruning policy for the service dependency graph to\nautomatically eliminates redundant components, thereby significantly improving\nthe RCA efficiency. The learned pruning policy is interpretable and fully\nadaptive to new RCA instances. With the pruned graph, a causal-based method can\nbe executed with high accuracy and efficiency. The proposed TraceDiag framework\nis evaluated on real data traces collected from the Microsoft Exchange system,\nand demonstrates superior performance compared to state-of-the-art RCA\napproaches. Notably, TraceDiag has been integrated as a critical component in\nthe Microsoft M365 Exchange, resulting in a significant improvement in the\nsystem's reliability and a considerable reduction in the human effort required\nfor RCA.",
            "author": [
                "Ruomeng Ding",
                "Chaoyun Zhang",
                "Lu Wang",
                "Yong Xu",
                "Minghua Ma",
                "Xiaomin Wu",
                "Meng Zhang",
                "Qingjun Chen",
                "Xin Gao",
                "Xuedong Gao",
                "Hao Fan",
                "Saravan Rajmohan",
                "Qingwei Lin",
                "Dongmei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18740v1",
                "http://arxiv.org/pdf/2310.18740v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18735v1",
            "title": "Curriculum Learning for Graph Neural Networks: Which Edges Should We\n  Learn First",
            "updated": "2023-10-28T15:35:34Z",
            "published": "2023-10-28T15:35:34Z",
            "summary": "Graph Neural Networks (GNNs) have achieved great success in representing data\nwith dependencies by recursively propagating and aggregating messages along the\nedges. However, edges in real-world graphs often have varying degrees of\ndifficulty, and some edges may even be noisy to the downstream tasks.\nTherefore, existing GNNs may lead to suboptimal learned representations because\nthey usually treat every edge in the graph equally. On the other hand,\nCurriculum Learning (CL), which mimics the human learning principle of learning\ndata samples in a meaningful order, has been shown to be effective in improving\nthe generalization ability and robustness of representation learners by\ngradually proceeding from easy to more difficult samples during training.\nUnfortunately, existing CL strategies are designed for independent data samples\nand cannot trivially generalize to handle data dependencies. To address these\nissues, we propose a novel CL strategy to gradually incorporate more edges into\ntraining according to their difficulty from easy to hard, where the degree of\ndifficulty is measured by how well the edges are expected given the model\ntraining status. We demonstrate the strength of our proposed method in\nimproving the generalization ability and robustness of learned representations\nthrough extensive experiments on nine synthetic datasets and nine real-world\ndatasets. The code for our proposed method is available at\nhttps://github.com/rollingstonezz/Curriculum_learning_for_GNNs.",
            "author": [
                "Zheng Zhang",
                "Junxiang Wang",
                "Liang Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18735v1",
                "http://arxiv.org/pdf/2310.18735v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18729v1",
            "title": "Using Large Language Models to Support Thematic Analysis in Empirical\n  Legal Studies",
            "updated": "2023-10-28T15:20:44Z",
            "published": "2023-10-28T15:20:44Z",
            "summary": "Thematic analysis and other variants of inductive coding are widely used\nqualitative analytic methods within empirical legal studies (ELS). We propose a\nnovel framework facilitating effective collaboration of a legal expert with a\nlarge language model (LLM) for generating initial codes (phase 2 of thematic\nanalysis), searching for themes (phase 3), and classifying the data in terms of\nthe themes (to kick-start phase 4). We employed the framework for an analysis\nof a dataset (n=785) of facts descriptions from criminal court opinions\nregarding thefts. The goal of the analysis was to discover classes of typical\nthefts. Our results show that the LLM, namely OpenAI's GPT-4, generated\nreasonable initial codes, and it was capable of improving the quality of the\ncodes based on expert feedback. They also suggest that the model performed well\nin zero-shot classification of facts descriptions in terms of the themes.\nFinally, the themes autonomously discovered by the LLM appear to map fairly\nwell to the themes arrived at by legal experts. These findings can be leveraged\nby legal researchers to guide their decisions in integrating LLMs into their\nthematic analyses, as well as other inductive coding projects.",
            "author": [
                "Jakub Dr\u00e1pal",
                "Hannes Westermann",
                "Jaromir Savelka"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18729v1",
                "http://arxiv.org/pdf/2310.18729v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18719v1",
            "title": "Oriented trees and paths in digraphs",
            "updated": "2023-10-28T14:50:52Z",
            "published": "2023-10-28T14:50:52Z",
            "summary": "Which conditions ensure that a digraph contains all oriented paths of some\ngiven length, or even a all oriented trees of some given size, as a subgraph?\nOne possible condition could be that the host digraph is a tournament of a\ncertain order. In arbitrary digraphs and oriented graphs, conditions on the\nchromatic number, on the edge density, on the minimum outdegree and on the\nminimum semidegree have been proposed. In this survey, we review the known\nresults, and highlight some open questions in the area.",
            "author": [
                "Maya Stein"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18719v1",
                "http://arxiv.org/pdf/2310.18719v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18716v1",
            "title": "Laplacian Canonization: A Minimalist Approach to Sign and Basis\n  Invariant Spectral Embedding",
            "updated": "2023-10-28T14:35:10Z",
            "published": "2023-10-28T14:35:10Z",
            "summary": "Spectral embedding is a powerful graph embedding technique that has received\na lot of attention recently due to its effectiveness on Graph Transformers.\nHowever, from a theoretical perspective, the universal expressive power of\nspectral embedding comes at the price of losing two important invariance\nproperties of graphs, sign and basis invariance, which also limits its\neffectiveness on graph data. To remedy this issue, many previous methods\ndeveloped costly approaches to learn new invariants and suffer from high\ncomputation complexity. In this work, we explore a minimal approach that\nresolves the ambiguity issues by directly finding canonical directions for the\neigenvectors, named Laplacian Canonization (LC). As a pure pre-processing\nmethod, LC is light-weighted and can be applied to any existing GNNs. We\nprovide a thorough investigation, from theory to algorithm, on this approach,\nand discover an efficient algorithm named Maximal Axis Projection (MAP) that\nworks for both sign and basis invariance and successfully canonizes more than\n90% of all eigenvectors. Experiments on real-world benchmark datasets like\nZINC, MOLTOX21, and MOLPCBA show that MAP consistently outperforms existing\nmethods while bringing minimal computation overhead. Code is available at\nhttps://github.com/PKU-ML/LaplacianCanonization.",
            "author": [
                "Jiangyan Ma",
                "Yifei Wang",
                "Yisen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18716v1",
                "http://arxiv.org/pdf/2310.18716v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18713v1",
            "title": "Episodic Multi-Task Learning with Heterogeneous Neural Processes",
            "updated": "2023-10-28T14:12:40Z",
            "published": "2023-10-28T14:12:40Z",
            "summary": "This paper focuses on the data-insufficiency problem in multi-task learning\nwithin an episodic training setup. Specifically, we explore the potential of\nheterogeneous information across tasks and meta-knowledge among episodes to\neffectively tackle each task with limited data. Existing meta-learning methods\noften fail to take advantage of crucial heterogeneous information in a single\nepisode, while multi-task learning models neglect reusing experience from\nearlier episodes. To address the problem of insufficient data, we develop\nHeterogeneous Neural Processes (HNPs) for the episodic multi-task setup. Within\nthe framework of hierarchical Bayes, HNPs effectively capitalize on prior\nexperiences as meta-knowledge and capture task-relatedness among heterogeneous\ntasks, mitigating data-insufficiency. Meanwhile, transformer-structured\ninference modules are designed to enable efficient inferences toward\nmeta-knowledge and task-relatedness. In this way, HNPs can learn more powerful\nfunctional priors for adapting to novel heterogeneous tasks in each meta-test\nepisode. Experimental results show the superior performance of the proposed\nHNPs over typical baselines, and ablation studies verify the effectiveness of\nthe designed inference modules.",
            "author": [
                "Jiayi Shen",
                "Xiantong Zhen",
                "Qi",
                "Wang",
                "Marcel Worring"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18713v1",
                "http://arxiv.org/pdf/2310.18713v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18709v1",
            "title": "Audio-Visual Instance Segmentation",
            "updated": "2023-10-28T13:37:52Z",
            "published": "2023-10-28T13:37:52Z",
            "summary": "In this paper, we propose a new multi-modal task, namely audio-visual\ninstance segmentation (AVIS), in which the goal is to identify, segment, and\ntrack individual sounding object instances in audible videos, simultaneously.\nTo our knowledge, it is the first time that instance segmentation has been\nextended into the audio-visual domain. To better facilitate this research, we\nconstruct the first audio-visual instance segmentation benchmark (AVISeg).\nSpecifically, AVISeg consists of 1,258 videos with an average duration of 62.6\nseconds from YouTube and public audio-visual datasets, where 117 videos have\nbeen annotated by using an interactive semi-automatic labeling tool based on\nthe Segment Anything Model (SAM). In addition, we present a simple baseline\nmodel for the AVIS task. Our new model introduces an audio branch and a\ncross-modal fusion module to Mask2Former to locate all sounding objects.\nFinally, we evaluate the proposed method using two backbones on AVISeg. We\nbelieve that AVIS will inspire the community towards a more comprehensive\nmulti-modal understanding.",
            "author": [
                "Ruohao Guo",
                "Yaru Chen",
                "Yanyu Qi",
                "Wenzhen Yue",
                "Dantong Niu",
                "Xianghua Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18709v1",
                "http://arxiv.org/pdf/2310.18709v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.MM",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.19831v1",
            "title": "Explaining by Imitating: Understanding Decisions by Interpretable Policy\n  Learning",
            "updated": "2023-10-28T13:06:14Z",
            "published": "2023-10-28T13:06:14Z",
            "summary": "Understanding human behavior from observed data is critical for transparency\nand accountability in decision-making. Consider real-world settings such as\nhealthcare, in which modeling a decision-maker's policy is challenging -- with\nno access to underlying states, no knowledge of environment dynamics, and no\nallowance for live experimentation. We desire learning a data-driven\nrepresentation of decision-making behavior that (1) inheres transparency by\ndesign, (2) accommodates partial observability, and (3) operates completely\noffline. To satisfy these key criteria, we propose a novel model-based Bayesian\nmethod for interpretable policy learning (\"Interpole\") that jointly estimates\nan agent's (possibly biased) belief-update process together with their\n(possibly suboptimal) belief-action mapping. Through experiments on both\nsimulated and real-world data for the problem of Alzheimer's disease diagnosis,\nwe illustrate the potential of our approach as an investigative device for\nauditing, quantifying, and understanding human decision-making behavior.",
            "author": [
                "Alihan H\u00fcy\u00fck",
                "Daniel Jarrett",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.19831v1",
                "http://arxiv.org/pdf/2310.19831v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18696v1",
            "title": "Probing LLMs for Joint Encoding of Linguistic Categories",
            "updated": "2023-10-28T12:46:40Z",
            "published": "2023-10-28T12:46:40Z",
            "summary": "Large Language Models (LLMs) exhibit impressive performance on a range of NLP\ntasks, due to the general-purpose linguistic knowledge acquired during\npretraining. Existing model interpretability research (Tenney et al., 2019)\nsuggests that a linguistic hierarchy emerges in the LLM layers, with lower\nlayers better suited to solving syntactic tasks and higher layers employed for\nsemantic processing. Yet, little is known about how encodings of different\nlinguistic phenomena interact within the models and to what extent processing\nof linguistically-related categories relies on the same, shared model\nrepresentations. In this paper, we propose a framework for testing the joint\nencoding of linguistic categories in LLMs. Focusing on syntax, we find evidence\nof joint encoding both at the same (related part-of-speech (POS) classes) and\ndifferent (POS classes and related syntactic dependency relations) levels of\nlinguistic hierarchy. Our cross-lingual experiments show that the same patterns\nhold across languages in multilingual LLMs.",
            "author": [
                "Giulio Starace",
                "Konstantinos Papakostas",
                "Rochelle Choenni",
                "Apostolos Panagiotopoulos",
                "Matteo Rosati",
                "Alina Leidinger",
                "Ekaterina Shutova"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18696v1",
                "http://arxiv.org/pdf/2310.18696v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18688v1",
            "title": "Clairvoyance: A Pipeline Toolkit for Medical Time Series",
            "updated": "2023-10-28T12:08:03Z",
            "published": "2023-10-28T12:08:03Z",
            "summary": "Time-series learning is the bread and butter of data-driven *clinical\ndecision support*, and the recent explosion in ML research has demonstrated\ngreat potential in various healthcare settings. At the same time, medical\ntime-series problems in the wild are challenging due to their highly\n*composite* nature: They entail design choices and interactions among\ncomponents that preprocess data, impute missing values, select features, issue\npredictions, estimate uncertainty, and interpret models. Despite exponential\ngrowth in electronic patient data, there is a remarkable gap between the\npotential and realized utilization of ML for clinical research and decision\nsupport. In particular, orchestrating a real-world project lifecycle poses\nchallenges in engineering (i.e. hard to build), evaluation (i.e. hard to\nassess), and efficiency (i.e. hard to optimize). Designed to address these\nissues simultaneously, Clairvoyance proposes a unified, end-to-end,\nautoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical\nstandard, and (iii) interface for optimization. Our ultimate goal lies in\nfacilitating transparent and reproducible experimentation with complex\ninference workflows, providing integrated pathways for (1) personalized\nprediction, (2) treatment-effect estimation, and (3) information acquisition.\nThrough illustrative examples on real-world data in outpatient, general wards,\nand intensive-care settings, we illustrate the applicability of the pipeline\nparadigm on core tasks in the healthcare journey. To the best of our knowledge,\nClairvoyance is the first to demonstrate viability of a comprehensive and\nautomatable pipeline for clinical time-series ML.",
            "author": [
                "Daniel Jarrett",
                "Jinsung Yoon",
                "Ioana Bica",
                "Zhaozhi Qian",
                "Ari Ercole",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18688v1",
                "http://arxiv.org/pdf/2310.18688v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18687v1",
            "title": "Unsupervised Behavior Extraction via Random Intent Priors",
            "updated": "2023-10-28T12:03:34Z",
            "published": "2023-10-28T12:03:34Z",
            "summary": "Reward-free data is abundant and contains rich prior knowledge of human\nbehaviors, but it is not well exploited by offline reinforcement learning (RL)\nalgorithms. In this paper, we propose UBER, an unsupervised approach to extract\nuseful behaviors from offline reward-free datasets via diversified rewards.\nUBER assigns different pseudo-rewards sampled from a given prior distribution\nto different agents to extract a diverse set of behaviors, and reuse them as\ncandidate policies to facilitate the learning of new tasks. Perhaps\nsurprisingly, we show that rewards generated from random neural networks are\nsufficient to extract diverse and useful behaviors, some even close to expert\nones. We provide both empirical and theoretical evidence to justify the use of\nrandom priors for the reward function. Experiments on multiple benchmarks\nshowcase UBER's ability to learn effective and diverse behavior sets that\nenhance sample efficiency for online RL, outperforming existing baselines. By\nreducing reliance on human supervision, UBER broadens the applicability of RL\nto real-world scenarios with abundant reward-free data.",
            "author": [
                "Hao Hu",
                "Yiqin Yang",
                "Jianing Ye",
                "Ziqing Mai",
                "Chongjie Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18687v1",
                "http://arxiv.org/pdf/2310.18687v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18685v1",
            "title": "When Reviewers Lock Horn: Finding Disagreement in Scientific Peer\n  Reviews",
            "updated": "2023-10-28T11:57:51Z",
            "published": "2023-10-28T11:57:51Z",
            "summary": "To this date, the efficacy of the scientific publishing enterprise\nfundamentally rests on the strength of the peer review process. The journal\neditor or the conference chair primarily relies on the expert reviewers'\nassessment, identify points of agreement and disagreement and try to reach a\nconsensus to make a fair and informed decision on whether to accept or reject a\npaper. However, with the escalating number of submissions requiring review,\nespecially in top-tier Artificial Intelligence (AI) conferences, the\neditor/chair, among many other works, invests a significant, sometimes\nstressful effort to mitigate reviewer disagreements. Here in this work, we\nintroduce a novel task of automatically identifying contradictions among\nreviewers on a given article. To this end, we introduce ContraSciView, a\ncomprehensive review-pair contradiction dataset on around 8.5k papers (with\naround 28k review pairs containing nearly 50k review pair comments) from the\nopen review-based ICLR and NeurIPS conferences. We further propose a baseline\nmodel that detects contradictory statements from the review pairs. To the best\nof our knowledge, we make the first attempt to identify disagreements among\npeer reviewers automatically. We make our dataset and code public for further\ninvestigations.",
            "author": [
                "Sandeep Kumar",
                "Tirthankar Ghosal",
                "Asif Ekbal"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18685v1",
                "http://arxiv.org/pdf/2310.18685v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18676v1",
            "title": "Efficient Object Detection in Optical Remote Sensing Imagery via\n  Attention-based Feature Distillation",
            "updated": "2023-10-28T11:15:37Z",
            "published": "2023-10-28T11:15:37Z",
            "summary": "Efficient object detection methods have recently received great attention in\nremote sensing. Although deep convolutional networks often have excellent\ndetection accuracy, their deployment on resource-limited edge devices is\ndifficult. Knowledge distillation (KD) is a strategy for addressing this issue\nsince it makes models lightweight while maintaining accuracy. However, existing\nKD methods for object detection have encountered two constraints. First, they\ndiscard potentially important background information and only distill nearby\nforeground regions. Second, they only rely on the global context, which limits\nthe student detector's ability to acquire local information from the teacher\ndetector. To address the aforementioned challenges, we propose Attention-based\nFeature Distillation (AFD), a new KD approach that distills both local and\nglobal information from the teacher detector. To enhance local distillation, we\nintroduce a multi-instance attention mechanism that effectively distinguishes\nbetween background and foreground elements. This approach prompts the student\ndetector to focus on the pertinent channels and pixels, as identified by the\nteacher detector. Local distillation lacks global information, thus attention\nglobal distillation is proposed to reconstruct the relationship between various\npixels and pass it from teacher to student detector. The performance of AFD is\nevaluated on two public aerial image benchmarks, and the evaluation results\ndemonstrate that AFD in object detection can attain the performance of other\nstate-of-the-art models while being efficient.",
            "author": [
                "Pourya Shamsolmoali",
                "Jocelyn Chanussot",
                "Huiyu Zhou",
                "Yue Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18676v1",
                "http://arxiv.org/pdf/2310.18676v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18672v1",
            "title": "Maximum Independent Set: Self-Training through Dynamic Programming",
            "updated": "2023-10-28T10:58:25Z",
            "published": "2023-10-28T10:58:25Z",
            "summary": "This work presents a graph neural network (GNN) framework for solving the\nmaximum independent set (MIS) problem, inspired by dynamic programming (DP).\nSpecifically, given a graph, we propose a DP-like recursive algorithm based on\nGNNs that firstly constructs two smaller sub-graphs, predicts the one with the\nlarger MIS, and then uses it in the next recursive call. To train our\nalgorithm, we require annotated comparisons of different graphs concerning\ntheir MIS size. Annotating the comparisons with the output of our algorithm\nleads to a self-training process that results in more accurate self-annotation\nof the comparisons and vice versa. We provide numerical evidence showing the\nsuperiority of our method vs prior methods in multiple synthetic and real-world\ndatasets.",
            "author": [
                "Lorenzo Brusca",
                "Lars C. P. M. Quaedvlieg",
                "Stratis Skoulakis",
                "Grigorios G Chrysos",
                "Volkan Cevher"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18672v1",
                "http://arxiv.org/pdf/2310.18672v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18662v1",
            "title": "ASTormer: An AST Structure-aware Transformer Decoder for Text-to-SQL",
            "updated": "2023-10-28T10:21:40Z",
            "published": "2023-10-28T10:21:40Z",
            "summary": "Text-to-SQL aims to generate an executable SQL program given the user\nutterance and the corresponding database schema. To ensure the well-formedness\nof output SQLs, one prominent approach adopts a grammar-based recurrent decoder\nto produce the equivalent SQL abstract syntax tree (AST). However, previous\nmethods mainly utilize an RNN-series decoder, which 1) is time-consuming and\ninefficient and 2) introduces very few structure priors. In this work, we\npropose an AST structure-aware Transformer decoder (ASTormer) to replace\ntraditional RNN cells. The structural knowledge, such as node types and\npositions in the tree, is seamlessly incorporated into the decoder via both\nabsolute and relative position embeddings. Besides, the proposed framework is\ncompatible with different traversing orders even considering adaptive node\nselection. Extensive experiments on five text-to-SQL benchmarks demonstrate the\neffectiveness and efficiency of our structured decoder compared to competitive\nbaselines.",
            "author": [
                "Ruisheng Cao",
                "Hanchong Zhang",
                "Hongshen Xu",
                "Jieyu Li",
                "Da Ma",
                "Lu Chen",
                "Kai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18662v1",
                "http://arxiv.org/pdf/2310.18662v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18659v1",
            "title": "From Indeterminacy to Determinacy: Augmenting Logical Reasoning\n  Capabilities with Large Language Models",
            "updated": "2023-10-28T10:05:51Z",
            "published": "2023-10-28T10:05:51Z",
            "summary": "Recent advances in LLMs have revolutionized the landscape of reasoning tasks.\nTo enhance the capabilities of LLMs to emulate human reasoning, prior works\nfocus on modeling reasoning steps using specific thought structures like\nchains, trees, or graphs. However, LLM-based reasoning continues to encounter\nthree challenges: 1) Selecting appropriate reasoning structures for various\ntasks; 2) Exploiting known conditions sufficiently and efficiently to deduce\nnew insights; 3) Considering the impact of historical reasoning experience. To\naddress these challenges, we propose DetermLR, a novel reasoning framework that\nformulates the reasoning process as a transformational journey from\nindeterminate premises to determinate ones. This process is marked by the\nincremental accumulation of determinate premises, making the conclusion\nprogressively closer to clarity. DetermLR includes three essential components:\n1) Premise identification: We categorize premises into two distinct types:\ndeterminate and indeterminate. This empowers LLMs to customize reasoning\nstructures to match the specific task complexities. 2) Premise prioritization\nand exploration: We leverage quantitative measurements to assess the relevance\nof each premise to the target, prioritizing more relevant premises for\nexploring new insights. 3) Iterative process with reasoning memory: We\nintroduce a reasoning memory module to automate storage and extraction of\navailable premises and reasoning paths, preserving historical reasoning details\nfor more accurate premise prioritization. Comprehensive experimental results\nshow that DetermLR outperforms all baselines on four challenging logical\nreasoning tasks: LogiQA, ProofWriter, FOLIO, and LogicalDeduction. DetermLR can\nachieve better reasoning performance while requiring fewer visited states,\nhighlighting its superior efficiency and effectiveness in tackling logical\nreasoning tasks.",
            "author": [
                "Hongda Sun",
                "Weikai Xu",
                "Wei Liu",
                "Jian Luan",
                "Bin Wang",
                "Shuo Shang",
                "Ji-Rong Wen",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18659v1",
                "http://arxiv.org/pdf/2310.18659v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18654v1",
            "title": "Causal discovery in a complex industrial system: A time series benchmark",
            "updated": "2023-10-28T09:47:02Z",
            "published": "2023-10-28T09:47:02Z",
            "summary": "Causal discovery outputs a causal structure, represented by a graph, from\nobserved data. For time series data, there is a variety of methods, however, it\nis difficult to evaluate these on real data as realistic use cases very rarely\ncome with a known causal graph to which output can be compared. In this paper,\nwe present a dataset from an industrial subsystem at the European Spallation\nSource along with its causal graph which has been constructed from expert\nknowledge. This provides a testbed for causal discovery from time series\nobservations of complex systems, and we believe this can help inform the\ndevelopment of causal discovery methodology.",
            "author": [
                "S\u00f8ren Wengel Mogensen",
                "Karin Rathsman",
                "Per Nilsson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18654v1",
                "http://arxiv.org/pdf/2310.18654v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18639v2",
            "title": "Towards Plastic and Stable Exemplar-Free Incremental Learning: A\n  Dual-Learner Framework with Cumulative Parameter Averaging",
            "updated": "2023-11-21T03:23:39Z",
            "published": "2023-10-28T08:48:44Z",
            "summary": "The dilemma between plasticity and stability presents a significant challenge\nin Incremental Learning (IL), especially in the exemplar-free scenario where\naccessing old-task samples is strictly prohibited during the learning of a new\ntask. A straightforward solution to this issue is learning and storing an\nindependent model for each task, known as Single Task Learning (STL). Despite\nthe linear growth in model storage with the number of tasks in STL, we\nempirically discover that averaging these model parameters can potentially\npreserve knowledge across all tasks. Inspired by this observation, we propose a\nDual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA\nemploys a dual-learner design: a plastic learner focused on acquiring new-task\nknowledge and a stable learner responsible for accumulating all learned\nknowledge. The knowledge from the plastic learner is transferred to the stable\nlearner via cumulative parameter averaging. Additionally, several task-specific\nclassifiers work in cooperation with the stable learner to yield the final\nprediction. Specifically, when learning a new task, these modules are updated\nin a cyclic manner: i) the plastic learner is initially optimized using a\nself-supervised loss besides the supervised loss to enhance the feature\nextraction robustness; ii) the stable learner is then updated with respect to\nthe plastic learner in a cumulative parameter averaging manner to maintain its\ntask-wise generalization; iii) the task-specific classifier is accordingly\noptimized to align with the stable learner. Experimental results on CIFAR-100\nand Tiny-ImageNet show that DLCPA outperforms several state-of-the-art\nexemplar-free baselines in both Task-IL and Class-IL settings.",
            "author": [
                "Wenju Sun",
                "Qingyong Li",
                "Wen Wang",
                "Yangli-ao Geng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18639v2",
                "http://arxiv.org/pdf/2310.18639v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18620v2",
            "title": "ODM3D: Alleviating Foreground Sparsity for Semi-Supervised Monocular 3D\n  Object Detection",
            "updated": "2023-11-07T02:55:02Z",
            "published": "2023-10-28T07:12:09Z",
            "summary": "Monocular 3D object detection (M3OD) is a significant yet inherently\nchallenging task in autonomous driving due to absence of explicit depth cues in\na single RGB image. In this paper, we strive to boost currently underperforming\nmonocular 3D object detectors by leveraging an abundance of unlabelled data via\nsemi-supervised learning. Our proposed ODM3D framework entails cross-modal\nknowledge distillation at various levels to inject LiDAR-domain knowledge into\na monocular detector during training. By identifying foreground sparsity as the\nmain culprit behind existing methods' suboptimal training, we exploit the\nprecise localisation information embedded in LiDAR points to enable more\nforeground-attentive and efficient distillation via the proposed BEV occupancy\nguidance mask, leading to notably improved knowledge transfer and M3OD\nperformance. Besides, motivated by insights into why existing cross-modal\nGT-sampling techniques fail on our task at hand, we further design a novel\ncross-modal object-wise data augmentation strategy for effective RGB-LiDAR\njoint learning. Our method ranks 1st in both KITTI validation and test\nbenchmarks, significantly surpassing all existing monocular methods, supervised\nor semi-supervised, on both BEV and 3D detection metrics.",
            "author": [
                "Weijia Zhang",
                "Dongnan Liu",
                "Chao Ma",
                "Weidong Cai"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18620v2",
                "http://arxiv.org/pdf/2310.18620v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18614v1",
            "title": "Hierarchical Mutual Information Analysis: Towards Multi-view Clustering\n  in The Wild",
            "updated": "2023-10-28T06:43:57Z",
            "published": "2023-10-28T06:43:57Z",
            "summary": "Multi-view clustering (MVC) can explore common semantics from unsupervised\nviews generated by different sources, and thus has been extensively used in\napplications of practical computer vision. Due to the spatio-temporal\nasynchronism, multi-view data often suffer from view missing and are unaligned\nin real-world applications, which makes it difficult to learn consistent\nrepresentations. To address the above issues, this work proposes a deep MVC\nframework where data recovery and alignment are fused in a hierarchically\nconsistent way to maximize the mutual information among different views and\nensure the consistency of their latent spaces. More specifically, we first\nleverage dual prediction to fill in missing views while achieving the\ninstance-level alignment, and then take the contrastive reconstruction to\nachieve the class-level alignment. To the best of our knowledge, this could be\nthe first successful attempt to handle the missing and unaligned data problem\nseparately with different learning paradigms. Extensive experiments on public\ndatasets demonstrate that our method significantly outperforms state-of-the-art\nmethods on multi-view clustering even in the cases of view missing and\nunalignment.",
            "author": [
                "Jiatai Wang",
                "Zhiwei Xu",
                "Xuewen Yang",
                "Xin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18614v1",
                "http://arxiv.org/pdf/2310.18614v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18610v1",
            "title": "Optical ranging with quantum advantage",
            "updated": "2023-10-28T06:37:39Z",
            "published": "2023-10-28T06:37:39Z",
            "summary": "The quantum illumination technique requires joint measurement between the\nidler and the probe reflected from the low-reflective target present in a noisy\nenvironment. The joint measurement is only possible with prior knowledge about\nthe target's location. The technique in this article overcomes this limitation\nby using entanglement and a cross-correlated homodyne measurement. This\ntechnique does not require quantum storage of the idler and prior knowledge\nabout the target's distance. The cross-correlation measurement makes this\ntechnique completely immune to environmental noise, as the correlation between\nthe idler and the environment is zero. The low reflectivity of the target is\nnegated by increasing the intensity of the reference fields (non-entangled) in\nthe homodyne. Based on heuristic arguments, a lower bound of the target's\nreflectivity for optimum application of this technique is described.",
            "author": [
                "Sankar Davuluri",
                "Greeshma Gopinath",
                "Matt J. Woolley"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18610v1",
                "http://arxiv.org/pdf/2310.18610v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18608v1",
            "title": "Embedding in Recommender Systems: A Survey",
            "updated": "2023-10-28T06:31:06Z",
            "published": "2023-10-28T06:31:06Z",
            "summary": "Recommender systems have become an essential component of many online\nplatforms, providing personalized recommendations to users. A crucial aspect is\nembedding techniques that coverts the high-dimensional discrete features, such\nas user and item IDs, into low-dimensional continuous vectors and can enhance\nthe recommendation performance. Applying embedding techniques captures complex\nentity relationships and has spurred substantial research. In this survey, we\nprovide an overview of the recent literature on embedding techniques in\nrecommender systems. This survey covers embedding methods like collaborative\nfiltering, self-supervised learning, and graph-based techniques. Collaborative\nfiltering generates embeddings capturing user-item preferences, excelling in\nsparse data. Self-supervised methods leverage contrastive or generative\nlearning for various tasks. Graph-based techniques like node2vec exploit\ncomplex relationships in network-rich environments. Addressing the scalability\nchallenges inherent to embedding methods, our survey delves into innovative\ndirections within the field of recommendation systems. These directions aim to\nenhance performance and reduce computational complexity, paving the way for\nimproved recommender systems. Among these innovative approaches, we will\nintroduce Auto Machine Learning (AutoML), hash techniques, and quantization\ntechniques in this survey. We discuss various architectures and techniques and\nhighlight the challenges and future directions in these aspects. This survey\naims to provide a comprehensive overview of the state-of-the-art in this\nrapidly evolving field and serve as a useful resource for researchers and\npractitioners working in the area of recommender systems.",
            "author": [
                "Xiangyu Zhao",
                "Maolin Wang",
                "Xinjian Zhao",
                "Jiansheng Li",
                "Shucheng Zhou",
                "Dawei Yin",
                "Qing Li",
                "Jiliang Tang",
                "Ruocheng Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18608v1",
                "http://arxiv.org/pdf/2310.18608v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18606v1",
            "title": "Where have you been? A Study of Privacy Risk for Point-of-Interest\n  Recommendation",
            "updated": "2023-10-28T06:17:52Z",
            "published": "2023-10-28T06:17:52Z",
            "summary": "As location-based services (LBS) have grown in popularity, the collection of\nhuman mobility data has become increasingly extensive to build machine learning\n(ML) models offering enhanced convenience to LBS users. However, the\nconvenience comes with the risk of privacy leakage since this type of data\nmight contain sensitive information related to user identities, such as\nhome/work locations. Prior work focuses on protecting mobility data privacy\nduring transmission or prior to release, lacking the privacy risk evaluation of\nmobility data-based ML models. To better understand and quantify the privacy\nleakage in mobility data-based ML models, we design a privacy attack suite\ncontaining data extraction and membership inference attacks tailored for\npoint-of-interest (POI) recommendation models, one of the most widely used\nmobility data-based ML models. These attacks in our attack suite assume\ndifferent adversary knowledge and aim to extract different types of sensitive\ninformation from mobility data, providing a holistic privacy risk assessment\nfor POI recommendation models. Our experimental evaluation using two real-world\nmobility datasets demonstrates that current POI recommendation models are\nvulnerable to our attacks. We also present unique findings to understand what\ntypes of mobility data are more susceptible to privacy attacks. Finally, we\nevaluate defenses against these attacks and highlight future directions and\nchallenges.",
            "author": [
                "Kunlin Cai",
                "Jinghuai Zhang",
                "Will Shand",
                "Zhiqing Hong",
                "Guang Wang",
                "Desheng Zhang",
                "Jianfeng Chi",
                "Yuan Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18606v1",
                "http://arxiv.org/pdf/2310.18606v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18604v1",
            "title": "Anaphor Assisted Document-Level Relation Extraction",
            "updated": "2023-10-28T06:11:18Z",
            "published": "2023-10-28T06:11:18Z",
            "summary": "Document-level relation extraction (DocRE) involves identifying relations\nbetween entities distributed in multiple sentences within a document. Existing\nmethods focus on building a heterogeneous document graph to model the internal\nstructure of an entity and the external interaction between entities. However,\nthere are two drawbacks in existing methods. On one hand, anaphor plays an\nimportant role in reasoning to identify relations between entities but is\nignored by these methods. On the other hand, these methods achieve\ncross-sentence entity interactions implicitly by utilizing a document or\nsentences as intermediate nodes. Such an approach has difficulties in learning\nfine-grained interactions between entities across different sentences,\nresulting in sub-optimal performance. To address these issues, we propose an\nAnaphor-Assisted (AA) framework for DocRE tasks. Experimental results on the\nwidely-used datasets demonstrate that our model achieves a new state-of-the-art\nperformance.",
            "author": [
                "Chonggang Lu",
                "Richong Zhang",
                "Kai Sun",
                "Jaein Kim",
                "Cunwang Zhang",
                "Yongyi Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18604v1",
                "http://arxiv.org/pdf/2310.18604v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18591v1",
            "title": "Inverse Decision Modeling: Learning Interpretable Representations of\n  Behavior",
            "updated": "2023-10-28T05:05:01Z",
            "published": "2023-10-28T05:05:01Z",
            "summary": "Decision analysis deals with modeling and enhancing decision processes. A\nprincipal challenge in improving behavior is in obtaining a transparent\ndescription of existing behavior in the first place. In this paper, we develop\nan expressive, unifying perspective on inverse decision modeling: a framework\nfor learning parameterized representations of sequential decision behavior.\nFirst, we formalize the forward problem (as a normative standard), subsuming\ncommon classes of control behavior. Second, we use this to formalize the\ninverse problem (as a descriptive model), generalizing existing work on\nimitation/reward learning -- while opening up a much broader class of research\nproblems in behavior representation. Finally, we instantiate this approach with\nan example (inverse bounded rational control), illustrating how this structure\nenables learning (interpretable) representations of (bounded) rationality --\nwhile naturally capturing intuitive notions of suboptimal actions, biased\nbeliefs, and imperfect knowledge of environments.",
            "author": [
                "Daniel Jarrett",
                "Alihan H\u00fcy\u00fck",
                "Mihaela van der Schaar"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18591v1",
                "http://arxiv.org/pdf/2310.18591v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18590v2",
            "title": "Using Early Readouts to Mediate Featural Bias in Distillation",
            "updated": "2023-11-08T13:13:13Z",
            "published": "2023-10-28T04:58:15Z",
            "summary": "Deep networks tend to learn spurious feature-label correlations in real-world\nsupervised learning tasks. This vulnerability is aggravated in distillation,\nwhere a student model may have lesser representational capacity than the\ncorresponding teacher model. Often, knowledge of specific spurious correlations\nis used to reweight instances & rebalance the learning process. We propose a\nnovel early readout mechanism whereby we attempt to predict the label using\nrepresentations from earlier network layers. We show that these early readouts\nautomatically identify problem instances or groups in the form of confident,\nincorrect predictions. Leveraging these signals to modulate the distillation\nloss on an instance level allows us to substantially improve not only group\nfairness measures across benchmark datasets, but also overall accuracy of the\nstudent model. We also provide secondary analyses that bring insight into the\nrole of feature learning in supervision and distillation.",
            "author": [
                "Rishabh Tiwari",
                "Durga Sivasubramanian",
                "Anmol Mekala",
                "Ganesh Ramakrishnan",
                "Pradeep Shenoy"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18590v2",
                "http://arxiv.org/pdf/2310.18590v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18548v1",
            "title": "MEDAVET: Traffic Vehicle Anomaly Detection Mechanism based on spatial\n  and temporal structures in vehicle traffic",
            "updated": "2023-10-28T00:36:50Z",
            "published": "2023-10-28T00:36:50Z",
            "summary": "Currently, there are computer vision systems that help us with tasks that\nwould be dull for humans, such as surveillance and vehicle tracking. An\nimportant part of this analysis is to identify traffic anomalies. An anomaly\ntells us that something unusual has happened, in this case on the highway. This\npaper aims to model vehicle tracking using computer vision to detect traffic\nanomalies on a highway. We develop the steps of detection, tracking, and\nanalysis of traffic: the detection of vehicles from video of urban traffic, the\ntracking of vehicles using a bipartite graph and the Convex Hull algorithm to\ndelimit moving areas. Finally for anomaly detection we use two data structures\nto detect the beginning and end of the anomaly. The first is the QuadTree that\ngroups vehicles that are stopped for a long time on the road and the second\nthat approaches vehicles that are occluded. Experimental results show that our\nmethod is acceptable on the Track4 test set, with an F1 score of 85.7% and a\nmean squared error of 25.432.",
            "author": [
                "Ana Rosal\u00eda Huam\u00e1n Reyna",
                "Alex Josu\u00e9 Fl\u00f3rez Farf\u00e1n",
                "Geraldo Pereira Rocha Filho",
                "Sandra Sampaio",
                "Robson de Grande",
                "Luis Hideo",
                "Vasconcelos Nakamura",
                "Rodolfo Ipolito Meneguette"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18548v1",
                "http://arxiv.org/pdf/2310.18548v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CY",
                "I.2.10; I.4.9"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18545v1",
            "title": "Identifying Conspiracy Theories News based on Event Relation Graph",
            "updated": "2023-10-28T00:27:21Z",
            "published": "2023-10-28T00:27:21Z",
            "summary": "Conspiracy theories, as a type of misinformation, are narratives that\nexplains an event or situation in an irrational or malicious manner. While most\nprevious work examined conspiracy theory in social media short texts, limited\nattention was put on such misinformation in long news documents. In this paper,\nwe aim to identify whether a news article contains conspiracy theories. We\nobserve that a conspiracy story can be made up by mixing uncorrelated events\ntogether, or by presenting an unusual distribution of relations between events.\nAchieving a contextualized understanding of events in a story is essential for\ndetecting conspiracy theories. Thus, we propose to incorporate an event\nrelation graph for each article, in which events are nodes, and four common\ntypes of event relations, coreference, temporal, causal, and subevent\nrelations, are considered as edges. Then, we integrate the event relation graph\ninto conspiracy theory identification in two ways: an event-aware language\nmodel is developed to augment the basic language model with the knowledge of\nevents and event relations via soft labels; further, a heterogeneous graph\nattention network is designed to derive a graph embedding based on hard labels.\nExperiments on a large benchmark dataset show that our approach based on event\nrelation graph improves both precision and recall of conspiracy theory\nidentification, and generalizes well for new unseen media sources.",
            "author": [
                "Yuanyuan Lei",
                "Ruihong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18545v1",
                "http://arxiv.org/pdf/2310.18545v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18544v1",
            "title": "Discourse Structures Guided Fine-grained Propaganda Identification",
            "updated": "2023-10-28T00:18:19Z",
            "published": "2023-10-28T00:18:19Z",
            "summary": "Propaganda is a form of deceptive narratives that instigate or mislead the\npublic, usually with a political purpose. In this paper, we aim to identify\npropaganda in political news at two fine-grained levels: sentence-level and\ntoken-level. We observe that propaganda content is more likely to be embedded\nin sentences that attribute causality or assert contrast to nearby sentences,\nas well as seen in opinionated evaluation, speculation and discussions of\nfuture expectation. Hence, we propose to incorporate both local and global\ndiscourse structures for propaganda discovery and construct two teacher models\nfor identifying PDTB-style discourse relations between nearby sentences and\ncommon discourse roles of sentences in a news article respectively. We further\ndevise two methods to incorporate the two types of discourse structures for\npropaganda identification by either using teacher predicted probabilities as\nadditional features or soliciting guidance in a knowledge distillation\nframework. Experiments on the benchmark dataset demonstrate that leveraging\nguidance from discourse structures can significantly improve both precision and\nrecall of propaganda content identification.",
            "author": [
                "Yuanyuan Lei",
                "Ruihong Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18544v1",
                "http://arxiv.org/pdf/2310.18544v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18543v2",
            "title": "Robust Graph Matching when Nodes are Corrupt",
            "updated": "2023-11-01T00:09:47Z",
            "published": "2023-10-28T00:16:04Z",
            "summary": "Two models are introduced to investigate graph matching in the presence of\ncorrupt nodes. The weak model, inspired by biological networks, allows one or\nboth networks to have a positive fraction of molecular entities interact\nrandomly with their network. For this model, it is shown that no estimator can\ncorrectly recover a positive fraction of the corrupt nodes. Necessary\nconditions for any estimator to correctly identify and match all the uncorrupt\nnodes are derived, and it is shown that these conditions are also sufficient\nfor the k-core estimator.\n  The strong model, inspired by social networks, permits one or both networks\nto have a positive fraction of users connect arbitrarily. For this model,\ndetection of corrupt nodes is impossible. Even so, we show that if only one of\nthe networks is compromised, then under appropriate conditions, the maximum\noverlap estimator can correctly match a positive fraction of nodes albeit\nwithout explicitly identifying them.",
            "author": [
                "Taha Ameen",
                "Bruce Hajek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18543v2",
                "http://arxiv.org/pdf/2310.18543v2"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.AP",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18537v1",
            "title": "Heuristics for Inequality minimization in PageRank values",
            "updated": "2023-10-27T23:36:12Z",
            "published": "2023-10-27T23:36:12Z",
            "summary": "This research study investigates the minimization of inequality in the ranks\nof vertices obtained using the PageRank algorithm. PageRank is a widely used\nalgorithm for ranking webpages and plays a significant role in determining web\ntraffic. This study employs the Gini coefficient, a measure of income/wealth\ninequality, to assess the inequality in PageRank distributions on various types\nof graphs. The investigation involves two experiments: one that modifies\nstrategies for handling dead-end nodes and another that explores six\ndeterministic methods for reducing inequality. Our findings indicate that a\ncombination of two distinct heuristics may present an effective strategy for\nminimizing inequality.",
            "author": [
                "Subhajit Sahu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18537v1",
                "http://arxiv.org/pdf/2310.18537v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.SI",
                "K.4.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18518v2",
            "title": "Reconfiguration of plane trees in convex geometric graphs",
            "updated": "2023-10-31T08:22:05Z",
            "published": "2023-10-27T22:30:44Z",
            "summary": "A non-crossing spanning tree of a set of points in the plane is a spanning\ntree whose edges pairwise do not cross. Avis and Fukuda in 1996 proved that\nthere always exists a flip sequence of length at most $2n-4$ between any pair\nof non-crossing spanning trees (where $n$ denotes the number of points).\nHernando et al. proved that the length of a minimal flip sequence can be of\nlength at least $\\frac 32 n$. Two recent results of Aichholzer et al. and\nBousquet et al. improved the Avis and Fukuda upper bound by proving that there\nalways exists a flip sequence of length respectively at most $2n - \\log n$ and\n$2n - \\sqrt{n}$. We improve the upper bound by a linear factor for the first\ntime in 25 years by proving that there always exists a flip sequence between\nany pair of non-crossing spanning trees $T_1,T_2$ of length at most $c n$ where\n$c \\approx 1.95$. Our result is actually stronger since we prove that, for any\ntwo trees $T_1,T_2$, there exists a flip sequence from $T_1$ to $T_2$ of length\nat most $c |T_1 \\setminus T_2|$. We also improve the best lower bound in terms\nof the symmetric difference by proving that there exists a pair of trees\n$T_1,T_2$ such that a minimal flip sequence has length $\\frac 53 |T_1 \\setminus\nT_2|$, improving the lower bound of Hernando et al. by considering the\nsymmetric difference instead of the number of vertices. We generalize this\nlower bound construction to non-crossing flips (where we close the gap between\nupper and lower bounds) and rotations.",
            "author": [
                "Nicolas Bousquet",
                "Lucas De Meyer",
                "Th\u00e9o Pierron",
                "Alexandra Wesolek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18518v2",
                "http://arxiv.org/pdf/2310.18518v2"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DM",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18517v1",
            "title": "Learning to recognize occluded and small objects with partial inputs",
            "updated": "2023-10-27T22:29:27Z",
            "published": "2023-10-27T22:29:27Z",
            "summary": "Recognizing multiple objects in an image is challenging due to occlusions,\nand becomes even more so when the objects are small. While promising, existing\nmulti-label image recognition models do not explicitly learn context-based\nrepresentations, and hence struggle to correctly recognize small and occluded\nobjects. Intuitively, recognizing occluded objects requires knowledge of\npartial input, and hence context. Motivated by this intuition, we propose\nMasked Supervised Learning (MSL), a single-stage, model-agnostic learning\nparadigm for multi-label image recognition. The key idea is to learn\ncontext-based representations using a masked branch and to model label\nco-occurrence using label consistency. Experimental results demonstrate the\nsimplicity, applicability and more importantly the competitive performance of\nMSL against previous state-of-the-art methods on standard multi-label image\nrecognition benchmarks. In addition, we show that MSL is robust to random\nmasking and demonstrate its effectiveness in recognizing non-masked objects.\nCode and pretrained models are available on GitHub.",
            "author": [
                "Hasib Zunair",
                "A. Ben Hamza"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18517v1",
                "http://arxiv.org/pdf/2310.18517v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18513v1",
            "title": "The zero forcing numbers and propagation times of gear graphs and helm\n  graphs",
            "updated": "2023-10-27T22:10:06Z",
            "published": "2023-10-27T22:10:06Z",
            "summary": "Zero forcing is a dynamic coloring process on graphs. Initially, each vertex\nof a graph is assigned a color of either blue or white, and then a process\nbegins by which blue vertices force white vertices to become blue. The zero\nforcing number is the cardinality of the smallest set of initially blue\nvertices which can force the entire graph to become blue, and the propagation\ntime is the minimum number of steps in such a zero forcing process. In this\npaper we will determine the zero forcing numbers and propagation times of two\ninfinite classes of graphs called gear graphs and helm graphs.",
            "author": [
                "Sara Anderton",
                "Rilee Burden",
                "McKenzie Fontenot",
                "Noah Fredrickson",
                "Alexandria Kwon",
                "Sydney Le",
                "Kanno Mizozoe",
                "Erin Raign",
                "August Sangalli",
                "Houston Schuerger",
                "Andrew Schwartz"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18513v1",
                "http://arxiv.org/pdf/2310.18513v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18494v1",
            "title": "Knowledge-based in silico models and dataset for the comparative\n  evaluation of mammography AI for a range of breast characteristics, lesion\n  conspicuities and doses",
            "updated": "2023-10-27T21:14:30Z",
            "published": "2023-10-27T21:14:30Z",
            "summary": "To generate evidence regarding the safety and efficacy of artificial\nintelligence (AI) enabled medical devices, AI models need to be evaluated on a\ndiverse population of patient cases, some of which may not be readily\navailable. We propose an evaluation approach for testing medical imaging AI\nmodels that relies on in silico imaging pipelines in which stochastic digital\nmodels of human anatomy (in object space) with and without pathology are imaged\nusing a digital replica imaging acquisition system to generate realistic\nsynthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with\nfour breast fibroglandular density distributions imaged at different exposure\nlevels using Monte Carlo x-ray simulations with the publicly available Virtual\nImaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize\nthe synthetic dataset to analyze AI model performance and find that model\nperformance decreases with increasing breast density and increases with higher\nmass density, as expected. As exposure levels decrease, AI model performance\ndrops with the highest performance achieved at exposure levels lower than the\nnominal recommended dose for the breast type.",
            "author": [
                "Elena Sizikova",
                "Niloufar Saharkhiz",
                "Diksha Sharma",
                "Miguel Lago",
                "Berkman Sahiner",
                "Jana G. Delfino",
                "Aldo Badano"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18494v1",
                "http://arxiv.org/pdf/2310.18494v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18474v1",
            "title": "Robust Bayesian Graphical Regression Models for Assessing Tumor\n  Heterogeneity in Proteomic Networks",
            "updated": "2023-10-27T20:41:32Z",
            "published": "2023-10-27T20:41:32Z",
            "summary": "Graphical models are powerful tools to investigate complex dependency\nstructures in high-throughput datasets. However, most existing graphical models\nmake one of the two canonical assumptions: (i) a homogeneous graph with a\ncommon network for all subjects; or (ii) an assumption of normality especially\nin the context of Gaussian graphical models. Both assumptions are restrictive\nand can fail to hold in certain applications such as proteomic networks in\ncancer. To this end, we propose an approach termed robust Bayesian graphical\nregression (rBGR) to estimate heterogeneous graphs for non-normally distributed\ndata. rBGR is a flexible framework that accommodates non-normality through\nrandom marginal transformations and constructs covariate-dependent graphs to\naccommodate heterogeneity through graphical regression techniques. We formulate\na new characterization of edge dependencies in such models called conditional\nsign independence with covariates along with an efficient posterior sampling\nalgorithm. In simulation studies, we demonstrate that rBGR outperforms existing\ngraphical regression models for data generated under various levels of\nnon-normality in both edge and covariate selection. We use rBGR to assess\nproteomic networks across two cancers: lung and ovarian, to systematically\ninvestigate the effects of immunogenic heterogeneity within tumors. Our\nanalyses reveal several important protein-protein interactions that are\ndifferentially impacted by the immune cell abundance; some corroborate existing\nbiological knowledge whereas others are novel findings.",
            "author": [
                "Tsung-Hung Yao",
                "Yang Ni",
                "Anindya Bhadra",
                "Jian Kang",
                "Veerabhadran Baladandayuthapani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18474v1",
                "http://arxiv.org/pdf/2310.18474v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18472v1",
            "title": "Parameter-Efficient Methods for Metastases Detection from Clinical Notes",
            "updated": "2023-10-27T20:30:59Z",
            "published": "2023-10-27T20:30:59Z",
            "summary": "Understanding the progression of cancer is crucial for defining treatments\nfor patients. The objective of this study is to automate the detection of\nmetastatic liver disease from free-style computed tomography (CT) radiology\nreports. Our research demonstrates that transferring knowledge using three\napproaches can improve model performance. First, we utilize generic language\nmodels (LMs), pretrained in a self-supervised manner. Second, we use a\nsemi-supervised approach to train our model by automatically annotating a large\nunlabeled dataset; this approach substantially enhances the model's\nperformance. Finally, we transfer knowledge from related tasks by designing a\nmulti-task transfer learning methodology. We leverage the recent advancement of\nparameter-efficient LM adaptation strategies to improve performance and\ntraining efficiency. Our dataset consists of CT reports collected at Memorial\nSloan Kettering Cancer Center (MSKCC) over the course of 12 years. 2,641\nreports were manually annotated by domain experts; among them, 841 reports have\nbeen annotated for the presence of liver metastases. Our best model achieved an\nF1-score of 73.8%, a precision of 84%, and a recall of 65.8%.",
            "author": [
                "Maede Ashofteh Barabadi",
                "Xiaodan Zhu",
                "Wai Yip Chan",
                "Amber L. Simpson",
                "Richard K. G. Do"
            ],
            "link": [
                "http://dx.doi.org/10.21428/594757db.8bee12fd",
                "http://arxiv.org/abs/2310.18472v1",
                "http://arxiv.org/pdf/2310.18472v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18471v2",
            "title": "Causal disentanglement of multimodal data",
            "updated": "2023-11-08T18:54:52Z",
            "published": "2023-10-27T20:30:11Z",
            "summary": "Causal representation learning algorithms discover lower-dimensional\nrepresentations of data that admit a decipherable interpretation of cause and\neffect; as achieving such interpretable representations is challenging, many\ncausal learning algorithms utilize elements indicating prior information, such\nas (linear) structural causal models, interventional data, or weak supervision.\nUnfortunately, in exploratory causal representation learning, such elements and\nprior information may not be available or warranted. Alternatively, scientific\ndatasets often have multiple modalities or physics-based constraints, and the\nuse of such scientific, multimodal data has been shown to improve\ndisentanglement in fully unsupervised settings. Consequently, we introduce a\ncausal representation learning algorithm (causalPIMA) that can use multimodal\ndata and known physics to discover important features with causal\nrelationships. Our innovative algorithm utilizes a new differentiable\nparametrization to learn a directed acyclic graph (DAG) together with a latent\nspace of a variational autoencoder in an end-to-end differentiable framework\nvia a single, tractable evidence lower bound loss function. We place a Gaussian\nmixture prior on the latent space and identify each of the mixtures with an\noutcome of the DAG nodes; this novel identification enables feature discovery\nwith causal relationships. Tested against a synthetic and a scientific dataset,\nour results demonstrate the capability of learning an interpretable causal\nstructure while simultaneously discovering key features in a fully unsupervised\nsetting.",
            "author": [
                "Elise Walker",
                "Jonas A. Actor",
                "Carianne Martinez",
                "Nathaniel Trask"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18471v2",
                "http://arxiv.org/pdf/2310.18471v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML",
                "68T07"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18455v1",
            "title": "Approximate Heavy Tails in Offline (Multi-Pass) Stochastic Gradient\n  Descent",
            "updated": "2023-10-27T20:06:03Z",
            "published": "2023-10-27T20:06:03Z",
            "summary": "A recent line of empirical studies has demonstrated that SGD might exhibit a\nheavy-tailed behavior in practical settings, and the heaviness of the tails\nmight correlate with the overall performance. In this paper, we investigate the\nemergence of such heavy tails. Previous works on this problem only considered,\nup to our knowledge, online (also called single-pass) SGD, in which the\nemergence of heavy tails in theoretical findings is contingent upon access to\nan infinite amount of data. Hence, the underlying mechanism generating the\nreported heavy-tailed behavior in practical settings, where the amount of\ntraining data is finite, is still not well-understood. Our contribution aims to\nfill this gap. In particular, we show that the stationary distribution of\noffline (also called multi-pass) SGD exhibits 'approximate' power-law tails and\nthe approximation error is controlled by how fast the empirical distribution of\nthe training data converges to the true underlying data distribution in the\nWasserstein metric. Our main takeaway is that, as the number of data points\nincreases, offline SGD will behave increasingly 'power-law-like'. To achieve\nthis result, we first prove nonasymptotic Wasserstein convergence bounds for\noffline SGD to online SGD as the number of data points increases, which can be\ninteresting on their own. Finally, we illustrate our theory on various\nexperiments conducted on synthetic data and neural networks.",
            "author": [
                "Krunoslav Lehman Pavasovic",
                "Alain Durmus",
                "Umut Simsekli"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18455v1",
                "http://arxiv.org/pdf/2310.18455v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18452v2",
            "title": "Induced subdivisions in $K_{s,s}$-free graphs with polynomial average\n  degree",
            "updated": "2023-11-12T18:01:15Z",
            "published": "2023-10-27T19:52:55Z",
            "summary": "In this paper we prove that for every $s\\geq 2$ and every graph $H$ the\nfollowing holds. Let $G$ be a graph with average degree $\\Omega_H(s^{C|H|^2})$,\nfor some absolute constant $C>0$, then $G$ either contains a $K_{s,s}$ or an\ninduced subdivision of $H$. This is essentially tight and confirms a conjecture\nof Bonamy, Bousquet, Pilipczuk, Rz\\k{a}\\.zewski, Thomass\\'e, and Walczak. A\nslightly weaker form of this has been independently proved by Bourneuf,\nBuci\\'c, Cook, and Davies.\n  We actually prove a much more general result which implies the above (with\nworse dependence on $|H|$). We show that for every $ k\\geq 2$ there is $C_k>0$\nsuch that any graph $G$ with average degree $s^{C_k}$ either contains a\n$K_{s,s}$ or an induced subgraph $G'\\subseteq G$ without $C_4$'s and with\naverage degree at least $k$.\n  Finally, using similar methods we can prove the following. For every $k,t\\geq\n2$ every graph $G$ with average degree at least $C_tk^{\\Omega(t)}$ must contain\neither a $K_k$, an induced $K_{t,t}$ or an induced subdivision of $K_k$. This\nis again essentially tight up to the implied constants and answers in a strong\nform a question of Davies.",
            "author": [
                "Ant\u00f3nio Gir\u00e3o",
                "Zach Hunter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18452v2",
                "http://arxiv.org/pdf/2310.18452v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18451v1",
            "title": "Fusion of the Power from Citations: Enhance your Influence by\n  Integrating Information from References",
            "updated": "2023-10-27T19:51:44Z",
            "published": "2023-10-27T19:51:44Z",
            "summary": "Influence prediction plays a crucial role in the academic community. The\namount of scholars' influence determines whether their work will be accepted by\nothers. Most existing research focuses on predicting one paper's citation count\nafter a period or identifying the most influential papers among the massive\ncandidates, without concentrating on an individual paper's negative or positive\nimpact on its authors. Thus, this study aims to formulate the prediction\nproblem to identify whether one paper can increase scholars' influence or not,\nwhich can provide feedback to the authors before they publish their papers.\nFirst, we presented the self-adapted ACC (Average Annual Citation Counts)\nmetric to measure authors' impact yearly based on their annual published\npapers, paper citation counts, and contributions in each paper. Then, we\nproposed the RD-GAT (Reference-Depth Graph Attention Network) model to\nintegrate heterogeneous graph information from different depth of references by\nassigning attention coefficients on them. Experiments on AMiner dataset\ndemonstrated that the proposed ACC metrics could represent the authors\ninfluence effectively, and the RD-GAT model is more efficiently on the academic\ncitation network, and have stronger robustness against the overfitting problem\ncompared with the baseline models. By applying the framework in this work,\nscholars can identify whether their papers can improve their influence in the\nfuture.",
            "author": [
                "Cong Qi",
                "Qin Liu",
                "Kan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18451v1",
                "http://arxiv.org/pdf/2310.18451v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18446v2",
            "title": "A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem",
            "updated": "2023-11-25T19:05:40Z",
            "published": "2023-10-27T19:42:23Z",
            "summary": "Optimal transportation is a fundamental topic that has attracted a great\namount of attention from machine learning community in the past decades. In\nthis paper, we consider an interesting discrete dynamic optimal transport\nproblem: can we efficiently update the optimal transport plan when the weights\nor the locations of the data points change? This problem is naturally motivated\nby several applications in machine learning. For example, we often need to\ncompute the optimal transportation cost between two different data sets; if\nsome change happens to a few data points, should we re-compute the high\ncomplexity cost function or update the cost by some efficient dynamic data\nstructure? We are aware that several dynamic maximum flow algorithms have been\nproposed before, however, the research on dynamic minimum cost flow problem is\nstill quite limited, to the best of our knowledge. We propose a novel 2D Skip\nOrthogonal List together with some dynamic tree techniques. Although our\nalgorithm is based on the conventional simplex method, it can efficiently\ncomplete each pivoting operation within $O(|V|)$ time with high probability\nwhere $V$ is the set of all supply and demand nodes. Since dynamic\nmodifications typically do not introduce significant changes, our algorithm\nrequires only a few simplex iterations in practice. So our algorithm is more\nefficient than re-computing the optimal transportation cost that needs at least\none traversal over all the $O(|E|) = O(|V|^2)$ variables in general cases. Our\nexperiments demonstrate that our algorithm significantly outperforms existing\nalgorithms in the dynamic scenarios.",
            "author": [
                "Xiaoyang Xu",
                "Hu Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18446v2",
                "http://arxiv.org/pdf/2310.18446v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.AI",
                "cs.CG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18444v1",
            "title": "M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning\n  of Mixture Graph Matching and Clustering",
            "updated": "2023-10-27T19:40:34Z",
            "published": "2023-10-27T19:40:34Z",
            "summary": "Existing graph matching methods typically assume that there are similar\nstructures between graphs and they are matchable. However, these assumptions do\nnot align with real-world applications. This work addresses a more realistic\nscenario where graphs exhibit diverse modes, requiring graph grouping before or\nalong with matching, a task termed mixture graph matching and clustering. We\nintroduce Minorize-Maximization Matching and Clustering (M3C), a learning-free\nalgorithm that guarantees theoretical convergence through the\nMinorize-Maximization framework and offers enhanced flexibility via relaxed\nclustering. Building on M3C, we develop UM3C, an unsupervised model that\nincorporates novel edge-wise affinity learning and pseudo label selection.\nExtensive experimental results on public benchmarks demonstrate that our method\noutperforms state-of-the-art graph matching and mixture graph matching and\nclustering approaches in both accuracy and efficiency. Source code will be made\npublicly available.",
            "author": [
                "Jiaxin Lu",
                "Zetian Jiang",
                "Tianzhe Wang",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18444v1",
                "http://arxiv.org/pdf/2310.18444v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18285v2",
            "title": "Unlocking the Potential of Prompt-Tuning in Bridging Generalized and\n  Personalized Federated Learning",
            "updated": "2023-11-24T06:49:25Z",
            "published": "2023-10-27T17:22:09Z",
            "summary": "Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve\nstate-of-the-art performance with improved efficiency in various computer\nvision tasks. This suggests a promising paradigm shift of adapting pre-trained\nViT models to Federated Learning (FL) settings. However, the challenge of data\nheterogeneity among FL clients presents a significant hurdle in effectively\ndeploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL)\nmethods have limitations in balancing performance across both global and local\ndata distributions. In this paper, we present a novel algorithm, SGPT, that\nintegrates GFL and PFL approaches by employing a unique combination of both\nshared and group-specific prompts. This design enables SGPT to capture both\ncommon and group-specific features. A key feature of SGPT is its prompt\nselection module, which facilitates the training of a single global model\ncapable of automatically adapting to diverse local client data distributions\nwithout the need for local fine-tuning. To effectively train the prompts, we\nutilize block coordinate descent (BCD), learning from common feature\ninformation (shared prompts), and then more specialized knowledge (group\nprompts) iteratively. Theoretically, we justify that learning the proposed\nprompts can reduce the gap between global and local performance. Empirically,\nwe conduct experiments on both label and feature heterogeneity settings in\ncomparison with state-of-the-art baselines, along with extensive ablation\nstudies, to substantiate the superior performance of SGPT.",
            "author": [
                "Wenlong Deng",
                "Christos Thrampoulidis",
                "Xiaoxiao Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18285v2",
                "http://arxiv.org/pdf/2310.18285v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18284v1",
            "title": "Rainbow subgraphs of uniformly coloured randomly perturbed graphs",
            "updated": "2023-10-27T17:21:52Z",
            "published": "2023-10-27T17:21:52Z",
            "summary": "For a given $\\delta \\in (0,1)$, the randomly perturbed graph model is defined\nas the union of any $n$-vertex graph $G_0$ with minimum degree $\\delta n$ and\nthe binomial random graph $\\mathbf{G}(n,p)$ on the same vertex set. Moreover,\nwe say that a graph is uniformly coloured with colours in $\\mathcal{C}$ if each\nedge is coloured independently and uniformly at random with a colour from\n$\\mathcal{C}$.\n  Based on a coupling idea of McDiarmird, we provide a general tool to tackle\nproblems concerning finding a rainbow copy of a graph $H=H(n)$ in a uniformly\ncoloured perturbed $n$-vertex graph with colours in $[(1+o(1))e(H)]$. For\nexample, our machinery easily allows to recover a result of Aigner-Horev and\nHefetz concerning rainbow Hamilton cycles, and to improve a result of\nAigner-Horev, Hefetz and Lahiri concerning rainbow bounded-degree spanning\ntrees.\n  Furthermore, using different methods, we prove that for any $\\delta \\in\n(0,1)$ and integer $d \\ge 2$, there exists $C=C(\\delta,d)>0$ such that the\nfollowing holds. Let $T$ be a tree on $n$ vertices with maximum degree at most\n$d$ and $G_0$ be an $n$-vertex graph with $\\delta(G_0)\\ge \\delta n$. Then a\nuniformly coloured $G_0 \\cup \\mathbf{G}(n,C/n)$ with colours in $[n-1]$\ncontains a rainbow copy of $T$ with high probability. This is optimal both in\nterms of colours and edge probability (up to a constant factor).",
            "author": [
                "Kyriakos Katsamaktsis",
                "Shoham Letzter",
                "Amedeo Sgueglia"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18284v1",
                "http://arxiv.org/pdf/2310.18284v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18265v1",
            "title": "Structured Semidefinite Programming for Recovering Structured\n  Preconditioners",
            "updated": "2023-10-27T16:54:29Z",
            "published": "2023-10-27T16:54:29Z",
            "summary": "We develop a general framework for finding approximately-optimal\npreconditioners for solving linear systems. Leveraging this framework we obtain\nimproved runtimes for fundamental preconditioning and linear system solving\nproblems including the following. We give an algorithm which, given positive\ndefinite $\\mathbf{K} \\in \\mathbb{R}^{d \\times d}$ with\n$\\mathrm{nnz}(\\mathbf{K})$ nonzero entries, computes an $\\epsilon$-optimal\ndiagonal preconditioner in time $\\widetilde{O}(\\mathrm{nnz}(\\mathbf{K}) \\cdot\n\\mathrm{poly}(\\kappa^\\star,\\epsilon^{-1}))$, where $\\kappa^\\star$ is the\noptimal condition number of the rescaled matrix. We give an algorithm which,\ngiven $\\mathbf{M} \\in \\mathbb{R}^{d \\times d}$ that is either the pseudoinverse\nof a graph Laplacian matrix or a constant spectral approximation of one, solves\nlinear systems in $\\mathbf{M}$ in $\\widetilde{O}(d^2)$ time. Our diagonal\npreconditioning results improve state-of-the-art runtimes of $\\Omega(d^{3.5})$\nattained by general-purpose semidefinite programming, and our solvers improve\nstate-of-the-art runtimes of $\\Omega(d^{\\omega})$ where $\\omega > 2.3$ is the\ncurrent matrix multiplication constant. We attain our results via new\nalgorithms for a class of semidefinite programs (SDPs) we call\nmatrix-dictionary approximation SDPs, which we leverage to solve an associated\nproblem we call matrix-dictionary recovery.",
            "author": [
                "Arun Jambulapati",
                "Jerry Li",
                "Christopher Musco",
                "Kirankumar Shiragur",
                "Aaron Sidford",
                "Kevin Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18265v1",
                "http://arxiv.org/pdf/2310.18265v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18239v1",
            "title": "Fine-Tuning Language Models Using Formal Methods Feedback",
            "updated": "2023-10-27T16:24:24Z",
            "published": "2023-10-27T16:24:24Z",
            "summary": "Although pre-trained language models encode generic knowledge beneficial for\nplanning and control, they may fail to generate appropriate control policies\nfor domain-specific tasks. Existing fine-tuning methods use human feedback to\naddress this limitation, however, sourcing human feedback is labor intensive\nand costly. We present a fully automated approach to fine-tune pre-trained\nlanguage models for applications in autonomous systems, bridging the gap\nbetween generic knowledge and domain-specific requirements while reducing cost.\nThe method synthesizes automaton-based controllers from pre-trained models\nguided by natural language task descriptions. These controllers are verifiable\nagainst independently provided specifications within a world model, which can\nbe abstract or obtained from a high-fidelity simulator. Controllers with high\ncompliance with the desired specifications receive higher ranks, guiding the\niterative fine-tuning process. We provide quantitative evidences, primarily in\nautonomous driving, to demonstrate the method's effectiveness across multiple\ntasks. The results indicate an improvement in percentage of specifications\nsatisfied by the controller from 60% to 90%.",
            "author": [
                "Yunhao Yang",
                "Neel P. Bhatt",
                "Tyler Ingebrand",
                "William Ward",
                "Steven Carr",
                "Zhangyang Wang",
                "Ufuk Topcu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18239v1",
                "http://arxiv.org/pdf/2310.18239v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18235v2",
            "title": "Davidsonian Scene Graph: Improving Reliability in Fine-grained\n  Evaluation for Text-to-Image Generation",
            "updated": "2023-10-30T16:00:49Z",
            "published": "2023-10-27T16:20:10Z",
            "summary": "Evaluating text-to-image models is notoriously difficult. A strong recent\napproach for assessing text-image faithfulness is based on QG/A (question\ngeneration and answering), which uses pre-trained foundational models to\nautomatically generate a set of questions and answers from the prompt, and\noutput images are scored based on whether these answers extracted with a visual\nquestion answering model are consistent with the prompt-based answers. This\nkind of evaluation is naturally dependent on the quality of the underlying QG\nand QA models. We identify and address several reliability challenges in\nexisting QG/A work: (a) QG questions should respect the prompt (avoiding\nhallucinations, duplications, and omissions) and (b) VQA answers should be\nconsistent (not asserting that there is no motorcycle in an image while also\nclaiming the motorcycle is blue). We address these issues with Davidsonian\nScene Graph (DSG), an empirically grounded evaluation framework inspired by\nformal semantics. DSG is an automatic, graph-based QG/A that is modularly\nimplemented to be adaptable to any QG/A module. DSG produces atomic and unique\nquestions organized in dependency graphs, which (i) ensure appropriate semantic\ncoverage and (ii) sidestep inconsistent answers. With extensive experimentation\nand human evaluation on a range of model configurations (LLM, VQA, and T2I), we\nempirically demonstrate that DSG addresses the challenges noted above. Finally,\nwe present DSG-1k, an open-sourced evaluation benchmark that includes 1,060\nprompts, covering a wide range of fine-grained semantic categories with a\nbalanced distribution. We release the DSG-1k prompts and the corresponding DSG\nquestions.",
            "author": [
                "Jaemin Cho",
                "Yushi Hu",
                "Roopal Garg",
                "Peter Anderson",
                "Ranjay Krishna",
                "Jason Baldridge",
                "Mohit Bansal",
                "Jordi Pont-Tuset",
                "Su Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18235v2",
                "http://arxiv.org/pdf/2310.18235v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18225v1",
            "title": "Distributed Delay-Tolerant Strategies for Equality-Constraint\n  Sum-Preserving Resource Allocation",
            "updated": "2023-10-27T15:52:02Z",
            "published": "2023-10-27T15:52:02Z",
            "summary": "This paper proposes two nonlinear dynamics to solve constrained distributed\noptimization problem for resource allocation over a multi-agent network. In\nthis setup, coupling constraint refers to resource-demand balance which is\npreserved at all-times. The proposed solutions can address various model\nnonlinearities, for example, due to quantization and/or saturation. Further, it\nallows to reach faster convergence or to robustify the solution against\nimpulsive noise or uncertainties. We prove convergence over weakly connected\nnetworks using convex analysis and Lyapunov theory. Our findings show that\nconvergence can be reached for general sign-preserving odd nonlinearity. We\nfurther propose delay-tolerant mechanisms to handle general bounded\nheterogeneous time-varying delays over the communication network of agents\nwhile preserving all-time feasibility. This work finds application in CPU\nscheduling and coverage control among others. This paper advances the\nstate-of-the-art by addressing (i) possible nonlinearity on the agents/links,\nmeanwhile handling (ii) resource-demand feasibility at all times, (iii)\nuniform-connectivity instead of all-time connectivity, and (iv) possible\nheterogeneous and time-varying delays. To our best knowledge, no existing work\naddresses contributions (i)-(iv) altogether. Simulations and comparative\nanalysis are provided to corroborate our contributions.",
            "author": [
                "Mohammadreza Doostmohammadian",
                "Alireza Aghasi",
                "Maria Vrakopoulou",
                "Hamid R. Rabiee",
                "Usman A. Khan",
                "Themistoklis Charalambou"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18225v1",
                "http://arxiv.org/pdf/2310.18225v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.MA",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18215v1",
            "title": "One Model Fits All: Cross-Region Taxi-Demand Forecasting",
            "updated": "2023-10-27T15:42:04Z",
            "published": "2023-10-27T15:42:04Z",
            "summary": "The growing demand for ride-hailing services has led to an increasing need\nfor accurate taxi demand prediction. Existing systems are limited to specific\nregions, lacking generalizability to unseen areas. This paper presents a novel\ntaxi demand forecasting system that leverages a graph neural network to capture\nspatial dependencies and patterns in urban environments. Additionally, the\nproposed system employs a region-neutral approach, enabling it to train a model\nthat can be applied to any region, including unseen regions. To achieve this,\nthe framework incorporates the power of Variational Autoencoder to disentangle\nthe input features into region-specific and region-neutral components. The\nregion-neutral features facilitate cross-region taxi demand predictions,\nallowing the model to generalize well across different urban areas.\nExperimental results demonstrate the effectiveness of the proposed system in\naccurately forecasting taxi demand, even in previously unobserved regions, thus\nshowcasing its potential for optimizing taxi services and improving\ntransportation efficiency on a broader scale.",
            "author": [
                "Ren Ozeki",
                "Haruki Yonekura",
                "Aidana Baimbetova",
                "Hamada Rizk",
                "Hirozumi Yamaguchi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18215v1",
                "http://arxiv.org/pdf/2310.18215v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18209v1",
            "title": "Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive\n  Learning",
            "updated": "2023-10-27T15:31:42Z",
            "published": "2023-10-27T15:31:42Z",
            "summary": "Learning good self-supervised graph representations that are beneficial to\ndownstream tasks is challenging. Among a variety of methods, contrastive\nlearning enjoys competitive performance. The embeddings of contrastive learning\nare arranged on a hypersphere that enables the Cosine distance measurement in\nthe Euclidean space. However, the underlying structure of many domains such as\ngraphs exhibits highly non-Euclidean latent geometry. To this end, we propose a\nnovel contrastive learning framework to learn high-quality graph embedding.\nSpecifically, we design the alignment metric that effectively captures the\nhierarchical data-invariant information, as well as we propose a substitute of\nuniformity metric to prevent the so-called dimensional collapse. We show that\nin the hyperbolic space one has to address the leaf- and height-level\nuniformity which are related to properties of trees, whereas in the ambient\nspace of the hyperbolic manifold, these notions translate into imposing an\nisotropic ring density towards boundaries of Poincar\\'e ball. This ring density\ncan be easily imposed by promoting the isotropic feature distribution on the\ntangent space of manifold. In the experiments, we demonstrate the efficacy of\nour proposed method across different hyperbolic graph embedding techniques in\nboth supervised and self-supervised learning settings.",
            "author": [
                "Yifei Zhang",
                "Hao Zhu",
                "Jiahong Liu",
                "Piotr Koniusz",
                "Irwin King"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18209v1",
                "http://arxiv.org/pdf/2310.18209v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18202v1",
            "title": "Abundance: Asymmetric Graph Removal Lemmas and Integer Solutions to\n  Linear Equations",
            "updated": "2023-10-27T15:20:48Z",
            "published": "2023-10-27T15:20:48Z",
            "summary": "We prove that a large family of pairs of graphs satisfy a polynomial\ndependence in asymmetric graph removal lemmas. In particular, we give an\nunexpected answer to a question of Gishboliner, Shapira, and Wigderson by\nshowing that for every $t \\geqslant 4$, there are $K_t$-abundant graphs of\nchromatic number $t$. Using similar methods, we also extend work of Ruzsa by\nproving that a set $\\mathcal{A} \\subset \\{1,\\dots,N\\}$ which avoids solutions\nwith distinct integers to an equation of genus at least two has size\n$\\mathcal{O}(\\sqrt{N})$. The best previous bound was $N^{1 - o(1)}$ and the\nexponent of $1/2$ is best possible in such a result. Finally, we investigate\nthe relationship between polynomial dependencies in asymmetric removal lemmas\nand the problem of avoiding integer solutions to equations. The results suggest\na potentially deep correspondence. Many open questions remain.",
            "author": [
                "Ant\u00f3nio Gir\u00e3o",
                "Eoin Hurley",
                "Freddie Illingworth",
                "Lukas Michel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18202v1",
                "http://arxiv.org/pdf/2310.18202v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16124v1",
            "title": "DiffAttack: Evasion Attacks Against Diffusion-Based Adversarial\n  Purification",
            "updated": "2023-10-27T15:17:50Z",
            "published": "2023-10-27T15:17:50Z",
            "summary": "Diffusion-based purification defenses leverage diffusion models to remove\ncrafted perturbations of adversarial examples and achieve state-of-the-art\nrobustness. Recent studies show that even advanced attacks cannot break such\ndefenses effectively, since the purification process induces an extremely deep\ncomputational graph which poses the potential problem of gradient obfuscation,\nhigh memory cost, and unbounded randomness. In this paper, we propose a unified\nframework DiffAttack to perform effective and efficient attacks against\ndiffusion-based purification defenses, including both DDPM and score-based\napproaches. In particular, we propose a deviated-reconstruction loss at\nintermediate diffusion steps to induce inaccurate density gradient estimation\nto tackle the problem of vanishing/exploding gradients. We also provide a\nsegment-wise forwarding-backwarding algorithm, which leads to memory-efficient\ngradient backpropagation. We validate the attack effectiveness of DiffAttack\ncompared with existing adaptive attacks on CIFAR-10 and ImageNet. We show that\nDiffAttack decreases the robust accuracy of models compared with SOTA attacks\nby over 20% on CIFAR-10 under $\\ell_\\infty$ attack $(\\epsilon=8/255)$, and over\n10% on ImageNet under $\\ell_\\infty$ attack $(\\epsilon=4/255)$. We conduct a\nseries of ablations studies, and we find 1) DiffAttack with the\ndeviated-reconstruction loss added over uniformly sampled time steps is more\neffective than that added over only initial/final steps, and 2) diffusion-based\npurification with a moderate diffusion length is more robust under DiffAttack.",
            "author": [
                "Mintong Kang",
                "Dawn Song",
                "Bo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16124v1",
                "http://arxiv.org/pdf/2311.16124v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18193v1",
            "title": "Morphology of Vaccine RD&D translation",
            "updated": "2023-10-27T15:06:05Z",
            "published": "2023-10-27T15:06:05Z",
            "summary": "Translation as a concept coordinates participation in innovation but remains\na qualitative construct. We provide multivariate accounting of linkages between\nmarket entries of vaccines, clinical trials, patents, publications, funders,\nand grants to quantify biomedical translation. We found that the most prevalent\ntypes of biomedical translation are those between basic and applied research\n(52 percent) followed by those between research and product development (36\npercent). Although many biomedical stakeholders assume knowledge flows one way\nfrom upstream research to downstream application, knowledge feedbacks that\nmediate translation are prevalent. We also cluster biomedical funders based on\nthe types of translations they fund. Large-scale funding agencies such as NIH\nare similarly involved in early-stage translation, whereas pharmaceuticals and\nmission-oriented agencies such as DARPA involve diverse translation types, and\neach leaves different translation footprints.",
            "author": [
                "Martin Ho",
                "Henry CW Price",
                "Tim S Evans",
                "Eoin O'Sullivan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18193v1",
                "http://arxiv.org/pdf/2310.18193v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18192v1",
            "title": "Artifact-Robust Graph-Based Learning in Digital Pathology",
            "updated": "2023-10-27T15:06:01Z",
            "published": "2023-10-27T15:06:01Z",
            "summary": "Whole slide images~(WSIs) are digitized images of tissues placed in glass\nslides using advanced scanners. The digital processing of WSIs is challenging\nas they are gigapixel images and stored in multi-resolution format. A common\nchallenge with WSIs is that perturbations/artifacts are inevitable during\nstoring the glass slides and digitizing them. These perturbations include\nmotion, which often arises from slide movement during placement, and changes in\nhue and brightness due to variations in staining chemicals and the quality of\ndigitizing scanners. In this work, a novel robust learning approach to account\nfor these artifacts is presented. Due to the size and resolution of WSIs and to\naccount for neighborhood information, graph-based methods are called for. We\nuse graph convolutional network~(GCN) to extract features from the graph\nrepresenting WSI. Through a denoiser {and pooling layer}, the effects of\nperturbations in WSIs are controlled and the output is followed by a\ntransformer for the classification of different grades of prostate cancer. To\ncompare the efficacy of the proposed approach, the model without denoiser is\ntrained and tested with WSIs without any perturbation and then different\nperturbations are introduced in WSIs and passed through the network with the\ndenoiser. The accuracy and kappa scores of the proposed model with prostate\ncancer dataset compared with non-robust algorithms show significant improvement\nin cancer diagnosis.",
            "author": [
                "Saba Heidari Gheshlaghi",
                "Milan Aryal",
                "Nasim Yahyasoltani",
                "Masoud Ganji"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18192v1",
                "http://arxiv.org/pdf/2310.18192v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18186v1",
            "title": "Model-free Posterior Sampling via Learning Rate Randomization",
            "updated": "2023-10-27T14:59:44Z",
            "published": "2023-10-27T14:59:44Z",
            "summary": "In this paper, we introduce Randomized Q-learning (RandQL), a novel\nrandomized model-free algorithm for regret minimization in episodic Markov\nDecision Processes (MDPs). To the best of our knowledge, RandQL is the first\ntractable model-free posterior sampling-based algorithm. We analyze the\nperformance of RandQL in both tabular and non-tabular metric space settings. In\ntabular MDPs, RandQL achieves a regret bound of order\n$\\widetilde{\\mathcal{O}}(\\sqrt{H^{5}SAT})$, where $H$ is the planning horizon,\n$S$ is the number of states, $A$ is the number of actions, and $T$ is the\nnumber of episodes. For a metric state-action space, RandQL enjoys a regret\nbound of order $\\widetilde{\\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where\n$d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic\nexploration without using bonuses, relying instead on a novel idea of learning\nrate randomization. Our empirical study shows that RandQL outperforms existing\napproaches on baseline exploration environments.",
            "author": [
                "Daniil Tiapkin",
                "Denis Belomestny",
                "Daniele Calandriello",
                "Eric Moulines",
                "Remi Munos",
                "Alexey Naumov",
                "Pierre Perrault",
                "Michal Valko",
                "Pierre Menard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18186v1",
                "http://arxiv.org/pdf/2310.18186v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.04224v1",
            "title": "MELEP: A Novel Predictive Measure of Transferability in Multi-Label ECG\n  Analysis",
            "updated": "2023-10-27T14:57:10Z",
            "published": "2023-10-27T14:57:10Z",
            "summary": "We introduce MELEP, which stands for Muti-label Expected Log of Empirical\nPredictions, a novel measure to estimate how effective it is to transfer\nknowledge from a pre-trained model to a downstream task in a multi-label\nsettings. The measure is generic to work with new target data having a\ndifferent label set from source data. It is also computationally efficient,\nonly requires forward passing the downstream dataset through the pre-trained\nmodel once. To the best of our knowledge, we are the first to develop such a\ntransferability metric for multi-label ECG classification problems. Our\nexperiments show that MELEP can predict the performance of pre-trained\nconvolutional and recurrent deep neural networks, on small and imbalanced ECG\ndata. Specifically, strong correlation coefficients, with absolute values\nexceeding 0.6 in most cases, were observed between MELEP and the actual average\nF1 scores of the fine-tuned models.",
            "author": [
                "Cuong V. Nguyen",
                "Hieu Minh Duong",
                "Cuong D. Do"
            ],
            "link": [
                "http://arxiv.org/abs/2311.04224v1",
                "http://arxiv.org/pdf/2311.04224v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18167v1",
            "title": "MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading\n  Comprehension",
            "updated": "2023-10-27T14:24:06Z",
            "published": "2023-10-27T14:24:06Z",
            "summary": "The large language models have achieved superior performance on various\nnatural language tasks. One major drawback of such approaches is they are\nresource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a\nresource-efficient solution to fine-tune the pre-trained language models (PLMs)\nwhile keeping their weight frozen. Existing soft prompt methods mainly focus on\ndesigning the input-independent prompts that steer the model to fit the domain\nof the new dataset. Those methods often ignore the fine-grained information\nabout the task and context of the text. In this paper, we propose a multi-level\nprompt tuning (MPrompt) method for machine reading comprehension. It utilizes\nprompts at task-specific, domain-specific, and context-specific levels to\nenhance the comprehension of input semantics at different granularities. We\nalso propose an independence constraint to steer each domain-specific prompt to\nfocus on information within its domain to avoid redundancy. Moreover, we\npresent a prompt generator that incorporates context-related knowledge in the\nprompt generation to enhance contextual relevancy. We conducted extensive\nexperiments on 12 benchmarks of various QA formats and achieved an average\nimprovement of 1.94\\% over the state-of-the-art methods.",
            "author": [
                "Guoxin Chen",
                "Yiming Qian",
                "Bowen Wang",
                "Liangzhi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18167v1",
                "http://arxiv.org/pdf/2310.18167v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18163v1",
            "title": "A collection of open problems in celebration of Imre Leader's 60th\n  birthday",
            "updated": "2023-10-27T14:15:44Z",
            "published": "2023-10-27T14:15:44Z",
            "summary": "One of the great pleasures of working with Imre Leader is to experience his\ninfectious delight on encountering a compelling combinatorial problem. This\ncollection of open problems in combinatorics has been put together by a subset\nof his former PhD students and students-of-students for the occasion of his\n60th birthday. All of the contributors have been influenced (directly or\nindirectly) by Imre: his personality, enthusiasm and his approach to\nmathematics. The problems included cover many of the areas of combinatorial\nmathematics that Imre is most associated with: including extremal problems on\ngraphs, set systems and permutations, and Ramsey theory. This is a personal\nselection of problems which we find intriguing and deserving of being better\nknown. It is not intended to be systematic, or to consist of the most\nsignificant or difficult questions in any area. Rather, our main aim is to\ncelebrate Imre and his mathematics and to hope that these problems will make\nhim smile. We also hope this collection will be a useful resource for\nresearchers in combinatorics and will stimulate some enjoyable collaborations\nand beautiful mathematics.",
            "author": [
                "Rahil Baber",
                "Natalie Behague",
                "Asier Calbet",
                "David Ellis",
                "Joshua Erde",
                "Ron Gray",
                "Maria-Romina Ivan",
                "Barnab\u00e1s Janzer",
                "Robert Johnson",
                "Luka Mili\u0107evi\u0107",
                "John Talbot",
                "Ta Sheng Tan",
                "Belinda Wickes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18163v1",
                "http://arxiv.org/pdf/2310.18163v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05D05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18152v2",
            "title": "Disentangled Representation Learning with Large Language Models for\n  Text-Attributed Graphs",
            "updated": "2023-11-06T12:54:14Z",
            "published": "2023-10-27T14:00:04Z",
            "summary": "Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs\nsuch as citation networks, e-commerce networks and social networks has\nattracted considerable attention in the web community. Recently, large language\nmodels (LLMs) have demonstrated exceptional capabilities across a wide range of\ntasks. However, the existing works focus on harnessing the potential of LLMs\nsolely relying on prompts to convey graph structure information to LLMs, thus\nsuffering from insufficient understanding of the complex structural\nrelationships within TAGs. To address this problem, in this paper we present\nthe Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the\nreasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model\nincorporates graph structure information through tailored disentangled graph\nneural network (GNN) layers, enabling LLMs to capture the intricate\nrelationships hidden in text-attributed graphs from multiple structural\nfactors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing\ncomputational costs and allowing much more flexibility in combining with\ndifferent LLM models. Experimental evaluations demonstrate the effectiveness of\nthe proposed DGTL model on achieving superior or comparable performance over\nstate-of-the-art baselines. Additionally, we also demonstrate that our DGTL\nmodel can offer natural language explanations for predictions, thereby\nsignificantly enhancing model interpretability.",
            "author": [
                "Yijian Qin",
                "Xin Wang",
                "Ziwei Zhang",
                "Wenwu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18152v2",
                "http://arxiv.org/pdf/2310.18152v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18146v3",
            "title": "Adaptive Out-Orientations with Applications",
            "updated": "2023-11-04T10:45:16Z",
            "published": "2023-10-27T13:51:53Z",
            "summary": "We give improved algorithms for maintaining edge-orientations of a\nfully-dynamic graph, such that the out-degree of each vertex is bounded. On one\nhand, we show how to orient the edges such that the out-degree of each vertex\nis proportional to the arboricity $\\alpha$ of the graph, in, either, an\namortised update time of $O(\\log^2 n \\log \\alpha)$, or a worst-case update time\nof $O(\\log^3 n \\log \\alpha)$. On the other hand, motivated by applications\nincluding dynamic maximal matching, we obtain a different trade-off, namely\neither $O(\\log n \\log \\alpha)$, amortised, or $O(\\log ^2 n \\log \\alpha)$,\nworst-case time, for the problem of maintaining an edge-orientation with at\nmost $O(\\alpha + \\log n)$ out-edges per vertex. Since our algorithms have\nupdate times with worst-case guarantees, the number of changes to the solution\n(i.e. the recourse) is naturally limited. Our algorithms adapt to the current\narboricity of the graph, and yield improvements over previous work: Firstly, we\nobtain an $O(\\varepsilon^{-6}\\log^3 n \\log \\rho)$ worst-case update time\nalgorithm for maintaining a $(1+\\varepsilon)$ approximation of the maximum\nsubgraph density, $\\rho$.\n  Secondly, we obtain an $O(\\varepsilon^{-6}\\log^3 n \\log \\alpha)$ worst-case\nupdate time algorithm for maintaining a $(1 + \\varepsilon) \\cdot OPT + 2$\napproximation of the optimal out-orientation of a graph with adaptive\narboricity $\\alpha$. This yields the first worst-case polylogarithmic dynamic\nalgorithm for decomposing into $O(\\alpha)$ forests.Thirdly, we obtain\narboricity-adaptive fully-dynamic deterministic algorithms for a variety, of\nproblems including maximal matching, $\\Delta+1$ coloring, and matrix vector\nmultiplication. All update times are worst-case $O(\\alpha+\\log^2n \\log\n\\alpha)$, where $\\alpha$ is the current arboricity of the graph.",
            "author": [
                "Chandra Chekuri",
                "Aleksander Bj\u00f8rn Christiansen",
                "Jacob Holm",
                "Ivor van der Hoog",
                "Kent Quanrud",
                "Eva Rotenberg",
                "Chris Schwiegelshohn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18146v3",
                "http://arxiv.org/pdf/2310.18146v3"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18130v2",
            "title": "DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial\n  Issues",
            "updated": "2023-11-07T20:29:53Z",
            "published": "2023-10-27T13:23:02Z",
            "summary": "Controversy is a reflection of our zeitgeist, and an important aspect to any\ndiscourse. The rise of large language models (LLMs) as conversational systems\nhas increased public reliance on these systems for answers to their various\nquestions. Consequently, it is crucial to systematically examine how these\nmodels respond to questions that pertaining to ongoing debates. However, few\nsuch datasets exist in providing human-annotated labels reflecting the\ncontemporary discussions. To foster research in this area, we propose a novel\nconstruction of a controversial questions dataset, expanding upon the publicly\nreleased Quora Question Pairs Dataset. This dataset presents challenges\nconcerning knowledge recency, safety, fairness, and bias. We evaluate different\nLLMs using a subset of this dataset, illuminating how they handle controversial\nissues and the stances they adopt. This research ultimately contributes to our\nunderstanding of LLMs' interaction with controversial issues, paving the way\nfor improvements in their comprehension and handling of complex societal\ndebates.",
            "author": [
                "David Q. Sun",
                "Artem Abzaliev",
                "Hadas Kotek",
                "Zidi Xiu",
                "Christopher Klein",
                "Jason D. Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18130v2",
                "http://arxiv.org/pdf/2310.18130v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18119v1",
            "title": "Towards a Unified Conversational Recommendation System: Multi-task\n  Learning via Contextualized Knowledge Distillation",
            "updated": "2023-10-27T13:06:24Z",
            "published": "2023-10-27T13:06:24Z",
            "summary": "In Conversational Recommendation System (CRS), an agent is asked to recommend\na set of items to users within natural language conversations. To address the\nneed for both conversational capability and personalized recommendations, prior\nworks have utilized separate recommendation and dialogue modules. However, such\napproach inevitably results in a discrepancy between recommendation results and\ngenerated responses. To bridge the gap, we propose a multi-task learning for a\nunified CRS, where a single model jointly learns both tasks via Contextualized\nKnowledge Distillation (ConKD). We introduce two versions of ConKD: hard gate\nand soft gate. The former selectively gates between two task-specific teachers,\nwhile the latter integrates knowledge from both teachers. Our gates are\ncomputed on-the-fly in a context-specific manner, facilitating flexible\nintegration of relevant knowledge. Extensive experiments demonstrate that our\nsingle model significantly improves recommendation performance while enhancing\nfluency, and achieves comparable results in terms of diversity.",
            "author": [
                "Yeongseo Jung",
                "Eunseo Jung",
                "Lei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18119v1",
                "http://arxiv.org/pdf/2310.18119v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18107v1",
            "title": "Improved covering results for conjugacy classes of symmetric groups via\n  hypercontractivity",
            "updated": "2023-10-27T12:47:46Z",
            "published": "2023-10-27T12:47:46Z",
            "summary": "We study covering numbers of subsets of the symmetric group $S_n$ that\nexhibit closure under conjugation, known as \\emph{normal} sets. We show that\nfor any $\\epsilon>0$, there exists $n_0$ such that if $n>n_0$ and $A$ is a\nnormal subset of the symmetric group $S_n$ of density $\\ge e^{-n^{2/5 -\n\\epsilon}}$, then $A^2 \\supseteq A_n$. This improves upon a seminal result of\nLarsen and Shalev (Inventiones Math., 2008), with our $2/5$ in the double\nexponent replacing their $1/4$.\n  Our proof strategy combines two types of techniques. The first is\n`traditional' techniques rooted in character bounds and asymptotics for the\nWitten zeta function, drawing from the foundational works of Liebeck--Shalev,\nLarsen--Shalev, and more recently, Larsen--Tiep. The second is a sharp\nhypercontractivity theorem in the symmetric group, which was recently obtained\nby Keevash and Lifshitz. This synthesis of algebraic and analytic methodologies\nnot only allows us to attain our improved bounds but also provides new insights\ninto the behavior of general independent sets in normal Cayley graphs over\nsymmetric groups.",
            "author": [
                "Nathan Keller",
                "Noam Lifshitz",
                "Ohad Sheinfeld"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18107v1",
                "http://arxiv.org/pdf/2310.18107v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18097v1",
            "title": "A graph database for feature characterization of dislocation networks",
            "updated": "2023-10-27T12:33:13Z",
            "published": "2023-10-27T12:33:13Z",
            "summary": "Three-dimensional dislocation networks control the mechanical properties such\nas strain hardening of crystals. Due to the complexity of dislocation networks\nand their temporal evolution, analysis tools are needed that fully resolve the\ndynamic processes of the intrinsic dislocation graph structure. We propose the\nuse of a graph database for the analysis of three-dimensional dislocation\nnetworks obtained from discrete dislocation dynamics simulations. This makes it\npossible to extract (sub-)graphs and their features with relative ease. That\nallows for a more holistic view of the evolution of dislocation networks and\nfor the extraction of homogenized graph features to be incorporated into\ncontinuum formulation. As an illustration, we describe the static and dynamic\nanalysis of spatio-temporal dislocation graphs as well as graph feature\nanalysis.",
            "author": [
                "Balduin Katzer",
                "Daniel Betsche",
                "Klemens B\u00f6hm",
                "Daniel Weygand",
                "Katrin Schulz"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.scriptamat.2023.115841",
                "http://arxiv.org/abs/2310.18097v1",
                "http://arxiv.org/pdf/2310.18097v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18077v1",
            "title": "Detrimental Contexts in Open-Domain Question Answering",
            "updated": "2023-10-27T11:45:16Z",
            "published": "2023-10-27T11:45:16Z",
            "summary": "For knowledge intensive NLP tasks, it has been widely accepted that accessing\nmore information is a contributing factor to improvements in the model's\nend-to-end performance. However, counter-intuitively, too much context can have\na negative impact on the model when evaluated on common question answering (QA)\ndatasets. In this paper, we analyze how passages can have a detrimental effect\non retrieve-then-read architectures used in question answering. Our empirical\nevidence indicates that the current read architecture does not fully leverage\nthe retrieved passages and significantly degrades its performance when using\nthe whole passages compared to utilizing subsets of them. Our findings\ndemonstrate that model accuracy can be improved by 10% on two popular QA\ndatasets by filtering out detrimental passages. Additionally, these outcomes\nare attained by utilizing existing retrieval methods without further training\nor data. We further highlight the challenges associated with identifying the\ndetrimental passages. First, even with the correct context, the model can make\nan incorrect prediction, posing a challenge in determining which passages are\nmost influential. Second, evaluation typically considers lexical matching,\nwhich is not robust to variations of correct answers. Despite these\nlimitations, our experimental results underscore the pivotal role of\nidentifying and removing these detrimental passages for the context-efficient\nretrieve-then-read pipeline. Code and data are available at\nhttps://github.com/xfactlab/emnlp2023-damaging-retrieval",
            "author": [
                "Philhoon Oh",
                "James Thorne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18077v1",
                "http://arxiv.org/pdf/2310.18077v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18076v1",
            "title": "Knowledge Corpus Error in Question Answering",
            "updated": "2023-10-27T11:44:06Z",
            "published": "2023-10-27T11:44:06Z",
            "summary": "Recent works in open-domain question answering (QA) have explored generating\ncontext passages from large language models (LLMs), replacing the traditional\nretrieval step in the QA pipeline. However, it is not well understood why\ngenerated passages can be more effective than retrieved ones. This study\nrevisits the conventional formulation of QA and introduces the concept of\nknowledge corpus error. This error arises when the knowledge corpus used for\nretrieval is only a subset of the entire string space, potentially excluding\nmore helpful passages that exist outside the corpus. LLMs may mitigate this\nshortcoming by generating passages in a larger space. We come up with an\nexperiment of paraphrasing human-annotated gold context using LLMs to observe\nknowledge corpus error empirically. Our results across three QA benchmarks\nreveal an increased performance (10% - 13%) when using paraphrased passage,\nindicating a signal for the existence of knowledge corpus error. Our code is\navailable at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error",
            "author": [
                "Yejoon Lee",
                "Philhoon Oh",
                "James Thorne"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18076v1",
                "http://arxiv.org/pdf/2310.18076v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18053v2",
            "title": "Phylogenetic invariants: straightforward from the general Markov to\n  equivariant models",
            "updated": "2023-11-09T06:48:48Z",
            "published": "2023-10-27T11:03:39Z",
            "summary": "In the last decade, some algebraic tools have been successfully applied to\nphylogenetic reconstruction. These tools are mainly based on the knowledge of\nequations describing algebraic varieties associated to phylogenetic trees\nevolving under Markov processes of molecular substitution, the so called\nphylogenetic invariants. Although the theory involved allows to explicitly\nobtain these equations for all equivariant models (which include some of the\nmost popular nucleotide substitution models), practical uses of these algebraic\ntools have been restricted to the case of the general Markov model. Arguably,\none of the reasons for this restriction is that knowledge of linear\nrepresentation theory is required before making these equations explicit.\n  With the aim of enlarging the practical uses of algebraic phylogenetics, in\nthis paper we prove that phylogenetic invariants for trees evolving under\nequivariant models can be derived from phylogenetic invariants for the general\nMarkov model, without the need of representation theory. Our main result states\nthat the algebraic variety corresponding to a phylogenetic tree evolving under\nan equivariant model is an irreducible component of the variety corresponding\nto the same tree under the general Markov model cut with the linear space\ndefined by the model. We also prove that, for any equivariant model, those\nphylogenetic invariants that are relevant for practical uses (e.g. tree\nreconstruction) can be simply deduced from a single rank constraint on the\nmatrices obtained by flattening the joint distribution at the leaves of the\ntree. This condition can be easily tested from singular values of the matrices\nand extends our results from trees to phylogenetic networks.",
            "author": [
                "Marta Casanellas",
                "Jes\u00fas Fern\u00e1ndez-S\u00e1nchez"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18053v2",
                "http://arxiv.org/pdf/2310.18053v2"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "math.AG",
                "14J99, 92D15, 05C85, 62R01"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18051v1",
            "title": "On stability of weighted spanning tree degree enumerators",
            "updated": "2023-10-27T10:57:28Z",
            "published": "2023-10-27T10:57:28Z",
            "summary": "Our previous paper shows that the (vertex) spanning tree degree enumerator\npolynomial of a connected graph $G$ is a real stable polynomial (id est is\nnon-zero if all variables have positive imaginary parts) if and only if $G$ is\ndistance-hereditary. In this note we generalize the result on weighted graphs.\nThis generalization allows us to define the class of weighted\ndistance-hereditary graphs.",
            "author": [
                "Danila Cherkashin",
                "Pavel Prozorov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18051v1",
                "http://arxiv.org/pdf/2310.18051v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18036v1",
            "title": "Fast and simple unrooted dynamic forests",
            "updated": "2023-10-27T10:28:24Z",
            "published": "2023-10-27T10:28:24Z",
            "summary": "A dynamic forest data structure maintains a forest (and associated data like\nedge weights) under edge insertions and deletions. Dynamic forests are widely\nused to solve online and offline graph problems. Well-known examples of dynamic\nforest data structures are link-cut trees [Sleator and Tarjan '83] and top\ntrees [Alstrup, Holm, de Lichtenberg, and Thorup '05], both of which need O(log\nn) time per operation. While top trees are more flexible and arguably easier to\nuse, link-cut trees are faster in practice [Tarjan and Werneck '10].\n  In this paper, we propose an alternative to link-cut trees. Our data\nstructure is based on search trees on trees (STTs, also known as elimination\ntrees) and an STT algorithm [Berendsohn and Kozma '22] based on the classical\nSplay trees [Sleator and Tarjan '85]. While link-cut trees maintain a hierarchy\nof binary search trees, we maintain a single STT. Most of the complexity of our\ndata structure lies in the implementation of the STT rotation primitive, which\ncan easily be reused, simplifying the development of new STT-based approaches.\n  We implement several variants of our data structure in the Rust programming\nlanguage, along with an implementation of link-cut trees for comparison.\nExperimental evaluation suggests that our algorithms are faster when the\ndynamic forest is unrooted, while link-cut trees are faster for rooted dynamic\nforests.",
            "author": [
                "Benjamin Aram Berendsohn"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18036v1",
                "http://arxiv.org/pdf/2310.18036v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18024v1",
            "title": "Encounter-based approach to target search problems: a review",
            "updated": "2023-10-27T10:02:21Z",
            "published": "2023-10-27T10:02:21Z",
            "summary": "In this review, we present the encounter-based approach to target search\nproblems, in which the diffusive dynamics is described by the joint probability\nof the position of the particle and the number of its encounters with a given\ntarget set. The knowledge of the statistics of encounters allows one to\nimplement various mechanisms of reactions on the target set, beyond\nconventional reaction schemes. We formulate this approach for three relevant\nsettings: discrete random walks, Brownian motion with bulk reactions, and\nreflected Brownian motion with surface reactions. In all cases, we discuss the\nadvantages of this approach, its recent applications and possible extensions.",
            "author": [
                "Denis S. Grebenkov"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18024v1",
                "http://arxiv.org/pdf/2310.18024v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "math.PR",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17998v1",
            "title": "Closing the Gap Between the Upper Bound and the Lower Bound of Adam's\n  Iteration Complexity",
            "updated": "2023-10-27T09:16:58Z",
            "published": "2023-10-27T09:16:58Z",
            "summary": "Recently, Arjevani et al. [1] established a lower bound of iteration\ncomplexity for the first-order optimization under an $L$-smooth condition and a\nbounded noise variance assumption. However, a thorough review of existing\nliterature on Adam's convergence reveals a noticeable gap: none of them meet\nthe above lower bound. In this paper, we close the gap by deriving a new\nconvergence guarantee of Adam, with only an $L$-smooth condition and a bounded\nnoise variance assumption. Our results remain valid across a broad spectrum of\nhyperparameters. Especially with properly chosen hyperparameters, we derive an\nupper bound of the iteration complexity of Adam and show that it meets the\nlower bound for first-order optimizers. To the best of our knowledge, this is\nthe first to establish such a tight upper bound for Adam's convergence. Our\nproof utilizes novel techniques to handle the entanglement between momentum and\nadaptive learning rate and to convert the first-order term in the Descent Lemma\nto the gradient norm, which may be of independent interest.",
            "author": [
                "Bohan Wang",
                "Jingwen Fu",
                "Huishuai Zhang",
                "Nanning Zheng",
                "Wei Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17998v1",
                "http://arxiv.org/pdf/2310.17998v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17989v1",
            "title": "Uncovering a Paleotsunami Triggered by Mass-Movement in an Alpine Lake",
            "updated": "2023-10-27T09:01:31Z",
            "published": "2023-10-27T09:01:31Z",
            "summary": "Mass movements and delta collapses are significant sources of tsunamis in\nlacustrine environments, impacting human societies enormously. Palaeotsunamis\nplay an essential role in understanding historical events and their\nconsequences along with their return periods. Here, we focus on a palaeo event\nthat occurred during the Younger Dryas to Early Holocene climatic transition,\nca., 12,000 years ago in the Lake Aiguebelette (NW Alps, France). Based on\nhighresolution seismic and bathymetric surveys and sedimentological,\ngeochemical, and magnetic analyses, a seismically induced large mass transport\ndeposit with an initial volume of 767172 m3 was identified, dated and mapped.\nTo investigate whether this underwater mass transport produced a palaeotsunami\nin the Lake Aiguebelette, this research combines sedimentary records and\nnumerical models. Numerical simulations of tsunamis are performed using a\nviscoplastic landslide model for tsunami source generation and two-dimensional\ndepth-averaged nonlinear shallow water equations for tsunami wave propagation\nand inundation modelling. Our simulations conclude that this sublacustrine\nlandslide produced a tsunami wave with a maximum amplitude of approximately 2 m\nand run-up heights of up to 3.6 m. The modelled sediment thickness resulting\nfrom this mass transport corroborates well with the event deposits mapped in\nthe lake. Based on our results, we suggest that this sublacustrine mass\ntransport generated a significant tsunami wave that has not been reported\npreviously to the best of our knowledge.",
            "author": [
                "Muhammad Naveed Zafar",
                "Denys Dutykh",
                "Pierre Sabatier",
                "Mathilde Banjan",
                "Jihwan Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17989v1",
                "http://arxiv.org/pdf/2310.17989v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "math.AP",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17986v1",
            "title": "Fuzzy Multi-Agent Simulation of COVID-19 Pandemic Spreading",
            "updated": "2023-10-27T08:59:41Z",
            "published": "2023-10-27T08:59:41Z",
            "summary": "In this paper, we present a new approach for Covid-19 Pandemic spreading\nsimulation based on fuzzy multi agents. The agent parameters consider\ndistribution of the population according to age, and the index of\nsocio-economic fragility. Medical knowledge affirms that the COVID-19 main risk\nfactors are age and obesity. The worst medical situation is caused by the\ncombination of these two risk factors which in almost99% of cases finish in\nICU. The appearance of virus variants is another aspect parameter by our\nsimulation through a simplified modeling of the contagiousness. Using real data\nfrom people from West Indies (Guadeloupe, F.W.I.), we modeled the infection\nrate of the risk population, if neither vaccination nor barrier gestures are\nrespected. The results show that hospital capacities are exceeded, and the\nnumber of deaths exceeds 2% of the infected population, which is close to the\nreality.",
            "author": [
                "Didier El Baz",
                "Andrei Doncescu"
            ],
            "link": [
                "http://dx.doi.org/10.26717/BJSTR.2021.39.006331",
                "http://arxiv.org/abs/2310.17986v1",
                "http://arxiv.org/pdf/2310.17986v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17951v2",
            "title": "Understanding Parameter Saliency via Extreme Value Theory",
            "updated": "2023-12-05T05:00:20Z",
            "published": "2023-10-27T07:48:36Z",
            "summary": "Deep neural networks are being increasingly implemented throughout society in\nrecent years. It is useful to identify which parameters trigger\nmisclassification in diagnosing undesirable model behaviors. The concept of\nparameter saliency is proposed and used to diagnose convolutional neural\nnetworks (CNNs) by ranking convolution filters that may have caused\nmisclassification on the basis of parameter saliency. It is also shown that\nfine-tuning the top ranking salient filters efficiently corrects\nmisidentification on ImageNet. However, there is still a knowledge gap in terms\nof understanding why parameter saliency ranking can find the filters inducing\nmisidentification. In this work, we attempt to bridge the gap by analyzing\nparameter saliency ranking from a statistical viewpoint, namely, extreme value\ntheory. We first show that the existing work implicitly assumes that the\ngradient norm computed for each filter follows a normal distribution. Then, we\nclarify the relationship between parameter saliency and the score based on the\npeaks-over-threshold (POT) method, which is often used to model extreme values.\nFinally, we reformulate parameter saliency in terms of the POT method, where\nthis reformulation is regarded as statistical anomaly detection and does not\nrequire the implicit assumptions of the existing parameter-saliency\nformulation. Our experimental results demonstrate that our reformulation can\ndetect malicious filters as well. Furthermore, we show that the existing\nparameter saliency method exhibits a bias against the depth of layers in deep\nneural networks. In particular, this bias has the potential to inhibit the\ndiscovery of filters that cause misidentification in situations where domain\nshift occurs. In contrast, parameter saliency based on POT shows less of this\nbias.",
            "author": [
                "Shuo Wang",
                "Issei Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17951v2",
                "http://arxiv.org/pdf/2310.17951v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17936v1",
            "title": "Transformers as Graph-to-Graph Models",
            "updated": "2023-10-27T07:21:37Z",
            "published": "2023-10-27T07:21:37Z",
            "summary": "We argue that Transformers are essentially graph-to-graph models, with\nsequences just being a special case. Attention weights are functionally\nequivalent to graph edges. Our Graph-to-Graph Transformer architecture makes\nthis ability explicit, by inputting graph edges into the attention weight\ncomputations and predicting graph edges with attention-like functions, thereby\nintegrating explicit graphs into the latent graphs learned by pretrained\nTransformers. Adding iterative graph refinement provides a joint embedding of\ninput, output, and latent graphs, allowing non-autoregressive graph prediction\nto optimise the complete graph without any bespoke pipeline or decoding\nstrategy. Empirical results show that this architecture achieves\nstate-of-the-art accuracies for modelling a variety of linguistic structures,\nintegrating very effectively with the latent linguistic representations learned\nby pretraining.",
            "author": [
                "James Henderson",
                "Alireza Mohammadshahi",
                "Andrei C. Coman",
                "Lesly Miculicich"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17936v1",
                "http://arxiv.org/pdf/2310.17936v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17927v4",
            "title": "A Quantum Approximate Optimization Algorithm Based on CNR Operation",
            "updated": "2023-12-07T11:22:48Z",
            "published": "2023-10-27T06:54:39Z",
            "summary": "This paper introduces the ``comparison and replacement\" (CNR) operation and\npropose a general-purposed pure quantum approximate algorithm for combinatorial\noptimization problems. The CNR operation can increase the probability of\nobtaining a string with high object function value. And our algorithm is\nconstructed to a $p$-level divide-and-conquer structure with CNR operations to\nimprove the quality of optimization. For a fixed size problem, the algorithm\nperformance is improved with the increase of $p$ directly. And the algorithm\nperformance in application converges to the theoretical case as the number of\nancillary qubits $t$ increases. Furthermore, we demonstrate in theory and\napplication that for sufficiently general combinatorial optimization problems,\nthe algorithm can work and output a state with considerable approximation\nratio. Moreover, the string with higher object function can be measured in the\nfinal state with higher probability. To put it further, we prove that the\nstrings with the top $\\frac{1}{2^p}$ object function value occupy the dominant\nprobability(with lower bound around $1-\\frac{1}{\\mathrm{e}}\\approx0.6321$) in\nfinal measurement after $p$-level algorithm. As an illustration, we show the\nresults of our algorithm when applied to MAX-2-XOR instances and Gaussian\nweighted 2-edge graphs.",
            "author": [
                "Da You Lv",
                "An Min Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17927v4",
                "http://arxiv.org/pdf/2310.17927v4"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17923v1",
            "title": "Dynamic Grasping of Unknown Objects with a Multi-Fingered Hand",
            "updated": "2023-10-27T06:37:33Z",
            "published": "2023-10-27T06:37:33Z",
            "summary": "An important prerequisite for autonomous robots is their ability to reliably\ngrasp a wide variety of objects. Most state-of-the-art systems employ\nspecialized or simple end-effectors, such as two-jaw grippers, which severely\nlimit the range of objects to manipulate. Additionally, they conventionally\nrequire a structured and fully predictable environment while the vast majority\nof our world is complex, unstructured, and dynamic. This paper presents an\nimplementation to overcome both issues. Firstly, the integration of a\nfive-finger hand enhances the variety of possible grasps and manipulable\nobjects. This kinematically complex end-effector is controlled by a deep\nlearning based generative grasping network. The required virtual model of the\nunknown target object is iteratively completed by processing visual sensor\ndata. Secondly, this visual feedback is employed to realize closed-loop servo\ncontrol which compensates for external disturbances. Our experiments on real\nhardware confirm the system's capability to reliably grasp unknown dynamic\ntarget objects without a priori knowledge of their trajectories. To the best of\nour knowledge, this is the first method to achieve dynamic multi-fingered\ngrasping for unknown objects. A video of the experiments is available at\nhttps://youtu.be/Ut28yM1gnvI.",
            "author": [
                "Yannick Burkhardt",
                "Qian Feng",
                "Karan Sharma",
                "Zhaopeng Chen",
                "Alois Knoll"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17923v1",
                "http://arxiv.org/pdf/2310.17923v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17912v1",
            "title": "Restoring the Broken Covenant Between Compilers and Deep Learning\n  Accelerators",
            "updated": "2023-10-27T06:14:45Z",
            "published": "2023-10-27T06:14:45Z",
            "summary": "Deep learning accelerators address the computational demands of Deep Neural\nNetworks (DNNs), departing from the traditional Von Neumann execution model.\nThey leverage specialized hardware to align with the application domain's\nstructure. Compilers for these accelerators face distinct challenges compared\nto those for general-purpose processors. These challenges include exposing and\nmanaging more micro-architectural features, handling software-managed scratch\npads for on-chip storage, explicitly managing data movement, and matching DNN\nlayers with varying hardware capabilities. These complexities necessitate a new\napproach to compiler design, as traditional compilers mainly focused on\ngenerating fine-grained instruction sequences while abstracting\nmicro-architecture details. This paper introduces the Architecture Covenant\nGraph (ACG), an abstract representation of an architectural structure's\ncomponents and their programmable capabilities. By enabling the compiler to\nwork with the ACG, it allows for adaptable compilation workflows when making\nchanges to accelerator design, reducing the need for a complete compiler\nredevelopment. Codelets, which express DNN operation functionality and evolve\ninto execution mappings on the ACG, are key to this process. The Covenant\ncompiler efficiently targets diverse deep learning accelerators, achieving\n93.8% performance compared to state-of-the-art, hand-tuned DNN layer\nimplementations when compiling 14 DNN layers from various models on two\ndifferent architectures.",
            "author": [
                "Sean Kinzer",
                "Soroush Ghodrati",
                "Rohan Mahapatra",
                "Byung Hoon Ahn",
                "Edwin Mascarenhas",
                "Xiaolong Li",
                "Janarbek Matai",
                "Liang Zhang",
                "Hadi Esmaeilzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17912v1",
                "http://arxiv.org/pdf/2310.17912v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17909v1",
            "title": "The Innovation-to-Occupations Ontology: Linking Business Transformation\n  Initiatives to Occupations and Skills",
            "updated": "2023-10-27T05:57:41Z",
            "published": "2023-10-27T05:57:41Z",
            "summary": "The fast adoption of new technologies forces companies to continuously adapt\ntheir operations making it harder to predict workforce requirements. Several\nrecent studies have attempted to predict the emergence of new roles and skills\nin the labour market from online job ads. This paper aims to present a novel\nontology linking business transformation initiatives to occupations and an\napproach to automatically populating it by leveraging embeddings extracted from\njob ads and Wikipedia pages on business transformation and emerging\ntechnologies topics. To our knowledge, no previous research explicitly links\nbusiness transformation initiatives, like the adoption of new technologies or\nthe entry into new markets, to the roles needed. Our approach successfully\nmatches occupations to transformation initiatives under ten different\nscenarios, five linked to technology adoption and five related to business.\nThis framework presents an innovative approach to guide enterprises and\neducational institutions on the workforce requirements for specific business\ntransformation initiatives.",
            "author": [
                "Daniela Elia",
                "Fang Chen",
                "Didar Zowghi",
                "Marian-Andrei Rizoiu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17909v1",
                "http://arxiv.org/pdf/2310.17909v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17905v1",
            "title": "Small antimicrobial resistance proteins (SARPs): Small proteins\n  conferring antimicrobial resistance",
            "updated": "2023-10-27T05:37:56Z",
            "published": "2023-10-27T05:37:56Z",
            "summary": "Small open reading frames are understudied as they have been historically\nexcluded from genome annotations. However, evidence for the functional\nsignificance of small proteins in various cellular processes accumulates.\nProteins with less than 70 residues can also confer resistance to antimicrobial\ncompounds, including intracellularly-acting protein toxins, membrane-acting\nantimicrobial peptides and various small-molecule antibiotics. Such herein\ncoined Small Antimicrobial Resistance Proteins (SARPs) have emerged on\nevolutionary timescales or can be enriched from protein libraries using\nlaboratory evolution. Our review consolidates existing knowledge on SARPs and\nhighlights recent advancements in proteomics and genomics that reveal pervasive\ntranslation of unannotated genetic regions into small proteins that show\nfeatures of known SARPs. The potential contribution of small proteins to\nantimicrobial resistance is awaiting exploration.",
            "author": [
                "Rianne C. Prins",
                "Sonja Billerbeck"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17905v1",
                "http://arxiv.org/pdf/2310.17905v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17904v1",
            "title": "Forts, (fractional) zero forcing, and Cartesian products of graphs",
            "updated": "2023-10-27T05:33:49Z",
            "published": "2023-10-27T05:33:49Z",
            "summary": "The (disjoint) fort number and fractional zero forcing number are introduced\nand related to existing parameters including the (standard) zero forcing\nnumber. The fort hypergraph is introduced and hypergraph results on\ntransversals and matchings are applied to the zero forcing number and fort\nnumber. These results are used to establish a Vizing-like lower bound for the\nzero forcing number of a Cartesian product of graphs for certain families of\ngraphs, and a family of graphs achieving this lower bound is exhibited.",
            "author": [
                "Thomas R. Cameron",
                "Leslie Hogben",
                "Franklin H. J. Kenter",
                "Seyed Ahmad Mojallal",
                "Houston Schuerger"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17904v1",
                "http://arxiv.org/pdf/2310.17904v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17901v1",
            "title": "Improving the Knowledge Gradient Algorithm",
            "updated": "2023-10-27T05:25:02Z",
            "published": "2023-10-27T05:25:02Z",
            "summary": "The knowledge gradient (KG) algorithm is a popular policy for the best arm\nidentification (BAI) problem. It is built on the simple idea of always choosing\nthe measurement that yields the greatest expected one-step improvement in the\nestimate of the best mean of the arms. In this research, we show that this\npolicy has limitations, causing the algorithm not asymptotically optimal. We\nnext provide a remedy for it, by following the manner of one-step look ahead of\nKG, but instead choosing the measurement that yields the greatest one-step\nimprovement in the probability of selecting the best arm. The new policy is\ncalled improved knowledge gradient (iKG). iKG can be shown to be asymptotically\noptimal. In addition, we show that compared to KG, it is easier to extend iKG\nto variant problems of BAI, with the $\\epsilon$-good arm identification and\nfeasible arm identification as two examples. The superior performances of iKG\non these problems are further demonstrated using numerical examples.",
            "author": [
                "Yang Le",
                "Gao Siyang",
                "Ho Chin Pang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17901v1",
                "http://arxiv.org/pdf/2310.17901v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17892v1",
            "title": "Prompt-NER: Zero-shot Named Entity Recognition in Astronomy Literature\n  via Large Language Models",
            "updated": "2023-10-27T04:50:16Z",
            "published": "2023-10-27T04:50:16Z",
            "summary": "This study delves into the application of Large Language Models (LLMs) for\nNamed Entity Recognition (NER) tasks in the field of astronomy literature. To\nenhance the zero-shot recognition capabilities of LLMs for astronomical named\nentities, we propose a strategy called Prompt-NER. Prompt-NER includes five\nprompt elements: Task Descriptions, Entity Definitions, Task Emphasis, Task\nExamples, and Second Conversation. To assess the effectiveness of the\nPrompt-NER strategy, we utilize three representative LLMs (Claude-2, GPT-3.5,\nand LLaMA-2-70b) to identify telescope and celestial object named entities in\nastronomical literature. Our experiments are conducted based on two distinct\ndatasets. The first dataset comprises 30 original PDF documents, which we split\ninto paragraphs in sequential order, resulting in a second dataset consisting\nof 30 paragraph collections. Additionally, we incorporate 30 astronomical\ntelegrams to diversify our experiments and assess the performance of LLMs based\non Prompt-NER on concise, complete texts. Our experimental results indicate\nthat the Prompt-NER strategy enables LLMs to effectively accomplish NER tasks\nin the field of astronomy, even without prior astronomical knowledge during\ntraining. We carefully analyze the experimental results, including the\nmechanism of different prompt elements and the influence of different features\nof long and short texts on their respective experimental results. This research\nprovides experience for zero-shot NER tasks in astronomical literature and\nsuggests future work in this area.",
            "author": [
                "Wujun Shao",
                "Yaohua Hu",
                "Pengli Ji",
                "Xiaoran Yan",
                "Dongwei Fan",
                "Rui Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17892v1",
                "http://arxiv.org/pdf/2310.17892v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17886v2",
            "title": "Simple Linear-Size Additive Emulators",
            "updated": "2023-10-31T23:07:58Z",
            "published": "2023-10-27T04:21:34Z",
            "summary": "Given an input graph $G = (V, E)$, an additive emulator $H = (V, E', w)$ is a\nsparse weighted graph that preserves all distances in $G$ with small additive\nerror. A recent line of inquiry has sought to determine the best additive error\nachievable in the sparsest setting, when $H$ has a linear number of edges. In\nparticular, the work of [Kogan and Parter, ICALP 2023], following [Pettie,\nICALP 2007], constructed linear size emulators with $+O(n^{0.222})$ additive\nerror. It is known that the worst-case additive error must be at least\n$+\\Omega(n^{2/29})$ due to [Lu, Vassilevska Williams, Wein, and Xu, SODA 2022].\n  We present a simple linear-size emulator construction that achieves additive\nerror $+O(n^{0.191})$. Our approach extends the path-buying framework developed\nby [Baswana, Kavitha, Mehlhorn, and Pettie, SODA 2005] and [Vassilevska\nWilliams and Bodwin, SODA 2016] to the setting of sparse additive emulators.",
            "author": [
                "Gary Hoppenworth"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17886v2",
                "http://arxiv.org/pdf/2310.17886v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17878v1",
            "title": "A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing\n  Time",
            "updated": "2023-10-27T03:40:37Z",
            "published": "2023-10-27T03:40:37Z",
            "summary": "We address the problem of designing a sublinear-time spectral clustering\noracle for graphs that exhibit strong clusterability. Such graphs contain $k$\nlatent clusters, each characterized by a large inner conductance (at least\n$\\varphi$) and a small outer conductance (at most $\\varepsilon$). Our aim is to\npreprocess the graph to enable clustering membership queries, with the key\nrequirement that both preprocessing and query answering should be performed in\nsublinear time, and the resulting partition should be consistent with a\n$k$-partition that is close to the ground-truth clustering. Previous oracles\nhave relied on either a $\\textrm{poly}(k)\\log n$ gap between inner and outer\nconductances or exponential (in $k/\\varepsilon$) preprocessing time. Our\nalgorithm relaxes these assumptions, albeit at the cost of a slightly higher\nmisclassification ratio. We also show that our clustering oracle is robust\nagainst a few random edge deletions. To validate our theoretical bounds, we\nconducted experiments on synthetic networks.",
            "author": [
                "Ranran Shen",
                "Pan Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17878v1",
                "http://arxiv.org/pdf/2310.17878v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17874v1",
            "title": "SmooSeg: Smoothness Prior for Unsupervised Semantic Segmentation",
            "updated": "2023-10-27T03:29:25Z",
            "published": "2023-10-27T03:29:25Z",
            "summary": "Unsupervised semantic segmentation is a challenging task that segments images\ninto semantic groups without manual annotation. Prior works have primarily\nfocused on leveraging prior knowledge of semantic consistency or priori\nconcepts from self-supervised learning methods, which often overlook the\ncoherence property of image segments. In this paper, we demonstrate that the\nsmoothness prior, asserting that close features in a metric space share the\nsame semantics, can significantly simplify segmentation by casting unsupervised\nsemantic segmentation as an energy minimization problem. Under this paradigm,\nwe propose a novel approach called SmooSeg that harnesses self-supervised\nlearning methods to model the closeness relationships among observations as\nsmoothness signals. To effectively discover coherent semantic segments, we\nintroduce a novel smoothness loss that promotes piecewise smoothness within\nsegments while preserving discontinuities across different segments.\nAdditionally, to further enhance segmentation quality, we design an asymmetric\nteacher-student style predictor that generates smoothly updated pseudo labels,\nfacilitating an optimal fit between observations and labeling outputs. Thanks\nto the rich supervision cues of the smoothness prior, our SmooSeg significantly\noutperforms STEGO in terms of pixel accuracy on three datasets: COCOStuff\n(+14.9%), Cityscapes (+13.0%), and Potsdam-3 (+5.7%).",
            "author": [
                "Mengcheng Lan",
                "Xinjiang Wang",
                "Yiping Ke",
                "Jiaxing Xu",
                "Litong Feng",
                "Wayne Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17874v1",
                "http://arxiv.org/pdf/2310.17874v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17836v1",
            "title": "Positional Encoding-based Resident Identification in Multi-resident\n  Smart Homes",
            "updated": "2023-10-27T01:29:41Z",
            "published": "2023-10-27T01:29:41Z",
            "summary": "We propose a novel resident identification framework to identify residents in\na multi-occupant smart environment. The proposed framework employs a feature\nextraction model based on the concepts of positional encoding. The feature\nextraction model considers the locations of homes as a graph. We design a novel\nalgorithm to build such graphs from layout maps of smart environments. The\nNode2Vec algorithm is used to transform the graph into high-dimensional node\nembeddings. A Long Short-Term Memory (LSTM) model is introduced to predict the\nidentities of residents using temporal sequences of sensor events with the node\nembeddings. Extensive experiments show that our proposed scheme effectively\nidentifies residents in a multi-occupant environment. Evaluation results on two\nreal-world datasets demonstrate that our proposed approach achieves 94.5% and\n87.9% accuracy, respectively.",
            "author": [
                "Zhiyi Song",
                "Dipankar Chaki",
                "Abdallah Lakhdari",
                "Athman Bouguettaya"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17836v1",
                "http://arxiv.org/pdf/2310.17836v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18376v1",
            "title": "SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL\n  Translation",
            "updated": "2023-10-27T00:13:59Z",
            "published": "2023-10-27T00:13:59Z",
            "summary": "In recent years, there has been growing interest in text-to-SQL translation,\nwhich is the task of converting natural language questions into executable SQL\nqueries. This technology is important for its potential to democratize data\nextraction from databases. However, some of its key hurdles include domain\ngeneralisation, which is the ability to adapt to previously unseen databases,\nand alignment of natural language questions with the corresponding SQL queries.\nTo overcome these challenges, we introduce SQLformer, a novel Transformer\narchitecture specifically crafted to perform text-to-SQL translation tasks. Our\nmodel predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive\nway, incorporating structural inductive bias in the encoder and decoder layers.\nThis bias, guided by database table and column selection, aids the decoder in\ngenerating SQL query ASTs represented as graphs in a Breadth-First Search\ncanonical order. Comprehensive experiments illustrate the state-of-the-art\nperformance of SQLformer in the challenging text-to-SQL Spider benchmark. Our\nimplementation is available at https://github.com/AdrianBZG/SQLformer",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18376v1",
                "http://arxiv.org/pdf/2310.18376v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17811v2",
            "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot\n  Prompting",
            "updated": "2023-10-31T17:07:17Z",
            "published": "2023-10-26T23:06:38Z",
            "summary": "Automatically generated reports from medical images promise to improve the\nworkflow of radiologists. Existing methods consider an image-to-report modeling\ntask by directly generating a fully-fledged report from an image. However, this\nconflates the content of the report (e.g., findings and their attributes) with\nits style (e.g., format and choice of words), which can lead to clinically\ninaccurate reports. To address this, we propose a two-step approach for\nradiology report generation. First, we extract the content from an image; then,\nwe verbalize the extracted content into a report that matches the style of a\nspecific radiologist. For this, we leverage RadGraph -- a graph representation\nof reports -- together with large language models (LLMs). In our quantitative\nevaluations, we find that our approach leads to beneficial performance. Our\nhuman evaluation with clinical raters highlights that the AI-generated reports\nare indistinguishably tailored to the style of individual radiologist despite\nleveraging only a few examples as context.",
            "author": [
                "Benjamin Yan",
                "Ruochen Liu",
                "David E. Kuo",
                "Subathra Adithan",
                "Eduardo Pontes Reis",
                "Stephen Kwak",
                "Vasantha Kumar Venugopal",
                "Chloe P. O'Connell",
                "Agustina Saenz",
                "Pranav Rajpurkar",
                "Michael Moor"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17811v2",
                "http://arxiv.org/pdf/2310.17811v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17798v1",
            "title": "Maximum entropy-based modeling of community-level hazard responses for\n  civil infrastructures",
            "updated": "2023-10-26T22:02:06Z",
            "published": "2023-10-26T22:02:06Z",
            "summary": "Perturbed by natural hazards, community-level infrastructure networks operate\nlike many-body systems, with behaviors emerging from coupling individual\ncomponent dynamics with group correlations and interactions. It follows that we\ncan borrow methods from statistical physics to study the response of\ninfrastructure systems to natural disasters. This study aims to construct a\njoint probability distribution model to describe the post-hazard state of\ninfrastructure networks and propose an efficient surrogate model of the joint\ndistribution for large-scale systems. Specifically, we present maximum entropy\nmodeling of the regional impact of natural hazards on civil infrastructures.\nProvided with the current state of knowledge, the principle of maximum entropy\nyields the ``most unbiased`` joint distribution model for the performances of\ninfrastructures. In the general form, the model can handle multivariate\nperformance states and higher-order correlations. In a particular yet typical\nscenario of binary performance state variables with knowledge of their mean and\npairwise correlation, the joint distribution reduces to the Ising model in\nstatistical physics. In this context, we propose using a dichotomized Gaussian\nmodel as an efficient surrogate for the maximum entropy model, facilitating the\napplication to large systems. Using the proposed method, we investigate the\nseismic collective behavior of a large-scale road network (with 8,694 nodes and\n26,964 links) in San Francisco, showcasing the non-trivial collective behaviors\nof infrastructure systems.",
            "author": [
                "Xiaolei Chu",
                "Ziqi Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17798v1",
                "http://arxiv.org/pdf/2310.17798v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17796v2",
            "title": "ControlLLM: Augment Language Models with Tools by Searching on Graphs",
            "updated": "2023-10-30T17:30:47Z",
            "published": "2023-10-26T21:57:21Z",
            "summary": "We present ControlLLM, a novel framework that enables large language models\n(LLMs) to utilize multi-modal tools for solving complex real-world tasks.\nDespite the remarkable performance of LLMs, they still struggle with tool\ninvocation due to ambiguous user prompts, inaccurate tool selection and\nparameterization, and inefficient tool scheduling. To overcome these\nchallenges, our framework comprises three key components: (1) a \\textit{task\ndecomposer} that breaks down a complex task into clear subtasks with\nwell-defined inputs and outputs; (2) a \\textit{Thoughts-on-Graph (ToG)\nparadigm} that searches the optimal solution path on a pre-built tool graph,\nwhich specifies the parameter and dependency relations among different tools;\nand (3) an \\textit{execution engine with a rich toolbox} that interprets the\nsolution path and runs the tools efficiently on different computational\ndevices. We evaluate our framework on diverse tasks involving image, audio, and\nvideo processing, demonstrating its superior accuracy, efficiency, and\nversatility compared to existing methods. The code is at\nhttps://github.com/OpenGVLab/ControlLLM .",
            "author": [
                "Zhaoyang Liu",
                "Zeqiang Lai",
                "Zhangwei Gao",
                "Erfei Cui",
                "Zhiheng Li",
                "Xizhou Zhu",
                "Lewei Lu",
                "Qifeng Chen",
                "Yu Qiao",
                "Jifeng Dai",
                "Wenhai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17796v2",
                "http://arxiv.org/pdf/2310.17796v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17795v1",
            "title": "Weak diameter choosability of graphs with an excluded minor",
            "updated": "2023-10-26T21:56:19Z",
            "published": "2023-10-26T21:56:19Z",
            "summary": "Weak diameter coloring of graphs recently attracted attention partially due\nto its connection to asymptotic dimension of metric spaces. We consider weak\ndiameter list-coloring of graphs in this paper. Dvo\\v{r}\\'{a}k and Norin proved\nthat graphs with bounded Euler genus are 3-choosable with bounded weak\ndiameter. In this paper, we extend their result by showing that for every graph\n$H$, $H$-minor free graphs are 3-choosable with bounded weak diameter. The\nupper bound 3 is optimal and it strengthens an earlier result for\nnon-list-coloring $H$-minor free graphs with bounded weak diameter. As a\ncorollary, $H$-minor free graphs with bounded maximum degree are 3-choosable\nwith bounded clustering, strengthening an earlier result for non-list-coloring.\n  When $H$ is planar, we prove a much stronger result: for every\n2-list-assignment $L$ of an $H$-minor free graph, every precoloring with\nbounded weak diameter can be extended to an $L$-coloring with bounded weak\ndiameter. It is a common generalization of earlier results for\nnon-list-coloring with bounded weak diameter and for list-coloring with bounded\nclustering without allowing precolorings. As a corollary, for any planar graph\n$H$ and $H$-minor free graph $G$, there are exponentially many list-colorings\nof $G$ with bounded weak diameter (and with bounded clustering if $G$ also has\nbounded maximum degree).\n  We also show that the aforementioned results for list-coloring cannot be\nextended to odd minor free graphs by showing that some bipartite graphs with\nmaximum degree $\\Delta$ are $k$-choosable with bounded weak diameter only when\n$k=\\Omega(\\log\\Delta/\\log\\log\\Delta)$. On the other hand, we show that odd\n$H$-minor graphs are 3-colorable with bounded weak diameter, implying an\nearlier result about clustered coloring of odd $H$-minor free graphs with\nbounded maximum degree.",
            "author": [
                "Joshua Crouch",
                "Chun-Hung Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17795v1",
                "http://arxiv.org/pdf/2310.17795v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17793v1",
            "title": "\"You Are An Expert Linguistic Annotator\": Limits of LLMs as Analyzers of\n  Abstract Meaning Representation",
            "updated": "2023-10-26T21:47:59Z",
            "published": "2023-10-26T21:47:59Z",
            "summary": "Large language models (LLMs) show amazing proficiency and fluency in the use\nof language. Does this mean that they have also acquired insightful linguistic\nknowledge about the language, to an extent that they can serve as an \"expert\nlinguistic annotator\"? In this paper, we examine the successes and limitations\nof the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning\nstructure, focusing on the Abstract Meaning Representation (AMR; Banarescu et\nal. 2013) parsing formalism, which provides rich graphical representations of\nsentence meaning structure while abstracting away from surface forms. We\ncompare models' analysis of this semantic structure across two settings: 1)\ndirect production of AMR parses based on zero- and few-shot prompts, and 2)\nindirect partial reconstruction of AMR via metalinguistic natural language\nqueries (e.g., \"Identify the primary event of this sentence, and the predicate\ncorresponding to that event.\"). Across these settings, we find that models can\nreliably reproduce the basic format of AMR, and can often capture core event,\nargument, and modifier structure -- however, model outputs are prone to\nfrequent and major errors, and holistic analysis of parse acceptability shows\nthat even with few-shot demonstrations, models have virtually 0% success in\nproducing fully accurate parses. Eliciting natural language responses produces\nsimilar patterns of errors. Overall, our findings indicate that these models\nout-of-the-box can capture aspects of semantic structure, but there remain key\nlimitations in their ability to support fully accurate semantic analyses or\nparses.",
            "author": [
                "Allyson Ettinger",
                "Jena D. Hwang",
                "Valentina Pyatkin",
                "Chandra Bhagavatula",
                "Yejin Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17793v1",
                "http://arxiv.org/pdf/2310.17793v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17786v1",
            "title": "Understanding when Dynamics-Invariant Data Augmentations Benefit\n  Model-Free Reinforcement Learning Updates",
            "updated": "2023-10-26T21:28:50Z",
            "published": "2023-10-26T21:28:50Z",
            "summary": "Recently, data augmentation (DA) has emerged as a method for leveraging\ndomain knowledge to inexpensively generate additional data in reinforcement\nlearning (RL) tasks, often yielding substantial improvements in data\nefficiency. While prior work has demonstrated the utility of incorporating\naugmented data directly into model-free RL updates, it is not well-understood\nwhen a particular DA strategy will improve data efficiency. In this paper, we\nseek to identify general aspects of DA responsible for observed learning\nimprovements. Our study focuses on sparse-reward tasks with dynamics-invariant\ndata augmentation functions, serving as an initial step towards a more general\nunderstanding of DA and its integration into RL training. Experimentally, we\nisolate three relevant aspects of DA: state-action coverage, reward density,\nand the number of augmented transitions generated per update (the augmented\nreplay ratio). From our experiments, we draw two conclusions: (1) increasing\nstate-action coverage often has a much greater impact on data efficiency than\nincreasing reward density, and (2) decreasing the augmented replay ratio\nsubstantially improves data efficiency. In fact, certain tasks in our empirical\nstudy are solvable only when the replay ratio is sufficiently low.",
            "author": [
                "Nicholas E. Corrado",
                "Josiah P. Hanna"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17786v1",
                "http://arxiv.org/pdf/2310.17786v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17773v1",
            "title": "Graph Convolutional Networks for Complex Traffic Scenario Classification",
            "updated": "2023-10-26T20:51:24Z",
            "published": "2023-10-26T20:51:24Z",
            "summary": "A scenario-based testing approach can reduce the time required to obtain\nstatistically significant evidence of the safety of Automated Driving Systems\n(ADS). Identifying these scenarios in an automated manner is a challenging\ntask. Most methods on scenario classification do not work for complex scenarios\nwith diverse environments (highways, urban) and interaction with other traffic\nagents. This is mirrored in their approaches which model an individual vehicle\nin relation to its environment, but neglect the interaction between multiple\nvehicles (e.g. cut-ins, stationary lead vehicle). Furthermore, existing\ndatasets lack diversity and do not have per-frame annotations to accurately\nlearn the start and end time of a scenario. We propose a method for complex\ntraffic scenario classification that is able to model the interaction of a\nvehicle with the environment, as well as other agents. We use Graph\nConvolutional Networks to model spatial and temporal aspects of these\nscenarios. Expanding the nuScenes and Argoverse 2 driving datasets, we\nintroduce a scenario-labeled dataset, which covers different driving\nenvironments and is annotated per frame. Training our method on this dataset,\nwe present a promising baseline for future research on per-frame complex\nscenario classification.",
            "author": [
                "Tobias Hoek",
                "Holger Caesar",
                "Andreas Falkov\u00e9n",
                "Tommy Johansson"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17773v1",
                "http://arxiv.org/pdf/2310.17773v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "cs.MA",
                "I.2; I.4; I.5"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17762v1",
            "title": "Symmetric Exponential Time Requires Near-Maximum Circuit Size:\n  Simplified, Truly Uniform",
            "updated": "2023-10-26T20:12:25Z",
            "published": "2023-10-26T20:12:25Z",
            "summary": "In a recent breakthrough, Chen, Hirahara and Ren prove that $\\mathsf{S_2E}/_1\n\\not\\subset \\mathsf{SIZE}[2^n/n]$ by giving a single-valued $\\mathsf{FS_2P}$\nalgorithm for the Range Avoidance Problem ($\\mathsf{Avoid}$) that works for\ninfinitely many input size $n$.\n  Building on their work, we present a simple single-valued $\\mathsf{FS_2P}$\nalgorithm for $\\mathsf{Avoid}$ that works for all input size $n$. As a result,\nwe obtain the circuit lower bound $\\mathsf{S_2E} \\not\\subset\n\\mathsf{SIZE}[2^n/n]$ and many other corollaries:\n  1. Near-maximum circuit lower bound for $\\mathsf{\\Sigma_2E} \\cap\n\\mathsf{\\Pi_2E}$ and $\\mathsf{ZPE}^{\\mathsf{NP}}$.\n  2. Pseudodeterministic $\\mathsf{FZPP}^{\\mathsf{NP}}$ constructions for:\nRamsey graphs, rigid matrices, pseudorandom generators, two-source extractors,\nlinear codes, hard truth tables, and $K^{poly}$-random strings.",
            "author": [
                "Zeyong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17762v1",
                "http://arxiv.org/pdf/2310.17762v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17758v2",
            "title": "Graph Neural Networks for Enhanced Decoding of Quantum LDPC Codes",
            "updated": "2023-11-06T20:44:45Z",
            "published": "2023-10-26T19:56:25Z",
            "summary": "In this work, we propose a fully differentiable iterative decoder for quantum\nlow-density parity-check (LDPC) codes. The proposed algorithm is composed of\nclassical belief propagation (BP) decoding stages and intermediate graph neural\nnetwork (GNN) layers. Both component decoders are defined over the same sparse\ndecoding graph enabling a seamless integration and scalability to large codes.\nThe core idea is to use the GNN component between consecutive BP runs, so that\nthe knowledge from the previous BP run, if stuck in a local minima caused by\ntrapping sets or short cycles in the decoding graph, can be leveraged to better\ninitialize the next BP run. By doing so, the proposed decoder can learn to\ncompensate for sub-optimal BP decoding graphs that result from the design\nconstraints of quantum LDPC codes. Since the entire decoder remains\ndifferentiable, gradient descent-based training is possible. We compare the\nerror rate performance of the proposed decoder against various post-processing\nmethods such as random perturbation, enhanced feedback, augmentation, and\nordered-statistics decoding (OSD) and show that a carefully designed training\nprocess lowers the error-floor significantly. As a result, our proposed decoder\noutperforms the former three methods using significantly fewer post-processing\nattempts. The source code of our experiments is available online.",
            "author": [
                "Anqi Gong",
                "Sebastian Cammerer",
                "Joseph M. Renes"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17758v2",
                "http://arxiv.org/pdf/2310.17758v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17752v1",
            "title": "PockEngine: Sparse and Efficient Fine-tuning in a Pocket",
            "updated": "2023-10-26T19:46:11Z",
            "published": "2023-10-26T19:46:11Z",
            "summary": "On-device learning and efficient fine-tuning enable continuous and\nprivacy-preserving customization (e.g., locally fine-tuning large language\nmodels on personalized data). However, existing training frameworks are\ndesigned for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and\nlack the optimizations for learning on the edge, which faces challenges of\nresource limitations and edge hardware diversity. We introduce PockEngine: a\ntiny, sparse and efficient engine to enable fine-tuning on various edge\ndevices. PockEngine supports sparse backpropagation: it prunes the backward\ngraph and sparsely updates the model with measured memory saving and latency\nreduction while maintaining the model quality. Secondly, PockEngine is\ncompilation first: the entire training graph (including forward, backward and\noptimization steps) is derived at compile-time, which reduces the runtime\noverhead and brings opportunities for graph transformations. PockEngine also\nintegrates a rich set of training graph optimizations, thus can further\naccelerate the training cost, including operator reordering and backend\nswitching. PockEngine supports diverse applications, frontends and hardware\nbackends: it flexibly compiles and tunes models defined in\nPyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We\nevaluated PockEngine on both vision models and large language models.\nPockEngine achieves up to 15 $\\times$ speedup over off-the-shelf TensorFlow\n(Raspberry Pi), 5.6 $\\times$ memory saving back-propagation (Jetson AGX Orin).\nRemarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin\nat 550 tokens/s, 7.9$\\times$ faster than the PyTorch.",
            "author": [
                "Ligeng Zhu",
                "Lanxiang Hu",
                "Ji Lin",
                "Wei-Chen Wang",
                "Wei-Ming Chen",
                "Chuang Gan",
                "Song Han"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3613424.3614307",
                "http://arxiv.org/abs/2310.17752v1",
                "http://arxiv.org/pdf/2310.17752v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17749v1",
            "title": "Salespeople vs SalesBot: Exploring the Role of Educational Value in\n  Conversational Recommender Systems",
            "updated": "2023-10-26T19:44:06Z",
            "published": "2023-10-26T19:44:06Z",
            "summary": "Making big purchases requires consumers to research or consult a salesperson\nto gain domain expertise. However, existing conversational recommender systems\n(CRS) often overlook users' lack of background knowledge, focusing solely on\ngathering preferences. In this work, we define a new problem space for\nconversational agents that aim to provide both product recommendations and\neducational value through mixed-type mixed-initiative dialog. We introduce\nSalesOps, a framework that facilitates the simulation and evaluation of such\nsystems by leveraging recent advancements in large language models (LLMs). We\nbuild SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate\neither side of the framework. A comprehensive human study compares SalesBot\nagainst professional salespeople, revealing that although SalesBot approaches\nprofessional performance in terms of fluency and informativeness, it lags\nbehind in recommendation quality. We emphasize the distinct limitations both\nface in providing truthful information, highlighting the challenges of ensuring\nfaithfulness in the CRS context. We release our code and make all data\navailable.",
            "author": [
                "Lidiya Murakhovs'ka",
                "Philippe Laban",
                "Tian Xie",
                "Caiming Xiong",
                "Chien-Sheng Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17749v1",
                "http://arxiv.org/pdf/2310.17749v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17732v1",
            "title": "GNN-GMVO: Graph Neural Networks for Optimizing Gross Merchandise Value\n  in Similar Item Recommendation",
            "updated": "2023-10-26T18:43:16Z",
            "published": "2023-10-26T18:43:16Z",
            "summary": "Similar item recommendation is a critical task in the e-Commerce industry,\nwhich helps customers explore similar and relevant alternatives based on their\ninterested products. Despite the traditional machine learning models, Graph\nNeural Networks (GNNs), by design, can understand complex relations like\nsimilarity between products. However, in contrast to their wide usage in\nretrieval tasks and their focus on optimizing the relevance, the current GNN\narchitectures are not tailored toward maximizing revenue-related objectives\nsuch as Gross Merchandise Value (GMV), which is one of the major business\nmetrics for e-Commerce companies. In addition, defining accurate edge relations\nin GNNs is non-trivial in large-scale e-Commerce systems, due to the\nheterogeneity nature of the item-item relationships. This work aims to address\nthese issues by designing a new GNN architecture called GNN-GMVO (Graph Neural\nNetwork - Gross Merchandise Value Optimizer). This model directly optimizes GMV\nwhile considering the complex relations between items. In addition, we propose\na customized edge construction method to tailor the model toward similar item\nrecommendation task and alleviate the noisy and complex item-item relations. In\nour comprehensive experiments on three real-world datasets, we show higher\nprediction performance and expected GMV for top ranked items recommended by our\nmodel when compared with selected state-of-the-art benchmark models.",
            "author": [
                "Ramin Giahi",
                "Reza Yousefi Maragheh",
                "Nima Farrokhsiar",
                "Jianpeng Xu",
                "Jason Cho",
                "Evren Korpeoglu",
                "Sushant Kumar",
                "Kannan Achan"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17732v1",
                "http://arxiv.org/pdf/2310.17732v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17730v2",
            "title": "Towards Erd\u0151s-Hajnal property for dp-minimal graphs",
            "updated": "2023-10-31T19:57:24Z",
            "published": "2023-10-26T18:41:25Z",
            "summary": "We introduce the notion of strongly $\\binom{k}{2}$-free graphs, which contain\ndp-minimal graphs. We show that under some sparsity assumption, given a rainbow\n$\\binom{k}{2}$-free blockade we can find a rainbow $\\binom{k-1}{2}$-free\nblockade. This might serve as an intermediate step towards Erd\\H os-Hajnal\nproperty for dp-minimal graphs.",
            "author": [
                "Yayi Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17730v2",
                "http://arxiv.org/pdf/2310.17730v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17729v1",
            "title": "Improving Traffic Density Forecasting in Intelligent Transportation\n  Systems Using Gated Graph Neural Networks",
            "updated": "2023-10-26T18:40:28Z",
            "published": "2023-10-26T18:40:28Z",
            "summary": "This study delves into the application of graph neural networks in the realm\nof traffic forecasting, a crucial facet of intelligent transportation systems.\nAccurate traffic predictions are vital for functions like trip planning,\ntraffic control, and vehicle routing in such systems. Three prominent GNN\narchitectures Graph Convolutional Networks (Graph Sample and Aggregation) and\nGated Graph Neural Networks are explored within the context of traffic\nprediction. Each architecture's methodology is thoroughly examined, including\nlayer configurations, activation functions,and hyperparameters. The primary\ngoal is to minimize prediction errors, with GGNNs emerging as the most\neffective choice among the three models. The research outlines outcomes for\neach architecture, elucidating their predictive performance through root mean\nsquared error and mean absolute error (MAE). Hypothetical results reveal\nintriguing insights: GCNs display an RMSE of 9.10 and an MAE of 8.00, while\nGraphSAGE shows improvement with an RMSE of 8.3 and an MAE of 7.5. Gated Graph\nNeural Networks (GGNNs) exhibit the lowest RMSE at 9.15 and an impressive MAE\nof 7.1, positioning them as the frontrunner.",
            "author": [
                "Razib Hayat Khan",
                "Jonayet Miah",
                "S M Yasir Arafat",
                "M M Mahbubul Syeed",
                "Duc M Ca"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17729v1",
                "http://arxiv.org/pdf/2310.17729v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17721v1",
            "title": "From Transcripts to Insights: Uncovering Corporate Risks Using\n  Generative AI",
            "updated": "2023-10-26T18:30:37Z",
            "published": "2023-10-26T18:30:37Z",
            "summary": "We explore the value of generative AI tools, such as ChatGPT, in helping\ninvestors uncover dimensions of corporate risk. We develop and validate\nfirm-level measures of risk exposure to political, climate, and AI-related\nrisks. Using the GPT 3.5 model to generate risk summaries and assessments from\nthe context provided by earnings call transcripts, we show that GPT-based\nmeasures possess significant information content and outperform the existing\nrisk measures in predicting (abnormal) firm-level volatility and firms' choices\nsuch as investment and innovation. Importantly, information in risk assessments\ndominates that in risk summaries, establishing the value of general AI\nknowledge. We also find that generative AI is effective at detecting emerging\nrisks, such as AI risk, which has soared in recent quarters. Our measures\nperform well both within and outside the GPT's training window and are priced\nin equity markets. Taken together, an AI-based approach to risk measurement\nprovides useful insights to users of corporate disclosures at a low cost.",
            "author": [
                "Alex Kim",
                "Maximilian Muhn",
                "Valeri Nikolaev"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17721v1",
                "http://arxiv.org/pdf/2310.17721v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "cs.AI",
                "cs.CL",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17715v1",
            "title": "Outlier Dimensions Encode Task-Specific Knowledge",
            "updated": "2023-10-26T18:22:13Z",
            "published": "2023-10-26T18:22:13Z",
            "summary": "Representations from large language models (LLMs) are known to be dominated\nby a small subset of dimensions with exceedingly high variance. Previous works\nhave argued that although ablating these outlier dimensions in LLM\nrepresentations hurts downstream performance, outlier dimensions are\ndetrimental to the representational quality of embeddings. In this study, we\ninvestigate how fine-tuning impacts outlier dimensions and show that 1) outlier\ndimensions that occur in pre-training persist in fine-tuned models and 2) a\nsingle outlier dimension can complete downstream tasks with a minimal error\nrate. Our results suggest that outlier dimensions can encode crucial\ntask-specific knowledge and that the value of a representation in a single\noutlier dimension drives downstream model decisions.",
            "author": [
                "William Rudman",
                "Catherine Chen",
                "Carsten Eickhoff"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17715v1",
                "http://arxiv.org/pdf/2310.17715v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17709v1",
            "title": "Singularity formation along the line bundle mean curvature flow",
            "updated": "2023-10-26T18:08:48Z",
            "published": "2023-10-26T18:08:48Z",
            "summary": "The line bundle mean curvature flow is a complex analogue of the mean\ncurvature flow for Lagrangian graphs, with fixed points solving the deformed\nHermitian-Yang-Mills equation. In this paper we construct two distinct examples\nof singularities along the flow. First, we find a finite time singularity,\nruling out long time existence of the flow in general. Next we show long time\nexistence of the flow with a Calabi symmetry assumption on the blowup of\n$\\mathbb P^n$, $n\\geq 3$, if one assumes supercritical phase. Using this, we\nfind an example where a singularity occurs at infinite time along the\ndestabilizing subvariety in the semi-stable case.",
            "author": [
                "Yu Hin Chan",
                "Adam Jacob"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17709v1",
                "http://arxiv.org/pdf/2310.17709v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17653v1",
            "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of\n  General Knowledge Transfer between Any Pretrained Model",
            "updated": "2023-10-26T17:59:46Z",
            "published": "2023-10-26T17:59:46Z",
            "summary": "Training deep networks requires various design decisions regarding for\ninstance their architecture, data augmentation, or optimization. In this work,\nwe find these training variations to result in networks learning unique feature\nsets from the data. Using public model libraries comprising thousands of models\ntrained on canonical datasets like ImageNet, we observe that for arbitrary\npairings of pretrained models, one model extracts significant data context\nunavailable in the other -- independent of overall performance. Given any\narbitrary pairing of pretrained models and no external rankings (such as\nseparate test sets, e.g. due to data privacy), we investigate if it is possible\nto transfer such \"complementary\" knowledge from one model to another without\nperformance degradation -- a task made particularly difficult as additional\nknowledge can be contained in stronger, equiperformant or weaker models. Yet\nfacilitating robust transfer in scenarios agnostic to pretrained model pairings\nwould unlock auxiliary gains and knowledge fusion from any model repository\nwithout restrictions on model and problem specifics - including from weaker,\nlower-performance models. This work therefore provides an initial, in-depth\nexploration on the viability of such general-purpose knowledge transfer. Across\nlarge-scale experiments, we first reveal the shortcomings of standard knowledge\ndistillation techniques, and then propose a much more general extension through\ndata partitioning for successful transfer between nearly all pretrained models,\nwhich we show can also be done unsupervised. Finally, we assess both the\nscalability and impact of fundamental model properties on successful\nmodel-agnostic knowledge transfer.",
            "author": [
                "Karsten Roth",
                "Lukas Thede",
                "Almut Sophia Koepke",
                "Oriol Vinyals",
                "Olivier H\u00e9naff",
                "Zeynep Akata"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17653v1",
                "http://arxiv.org/pdf/2310.17653v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17646v1",
            "title": "Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic\n  Simulations of a Plasma Sheet Model",
            "updated": "2023-10-26T17:58:12Z",
            "published": "2023-10-26T17:58:12Z",
            "summary": "We explore the possibility of fully replacing a plasma physics kinetic\nsimulator with a graph neural network-based simulator. We focus on this class\nof surrogate models given the similarity between their message-passing update\nmechanism and the traditional physics solver update, and the possibility of\nenforcing known physical priors into the graph construction and update. We show\nthat our model learns the kinetic plasma dynamics of the one-dimensional plasma\nmodel, a predecessor of contemporary kinetic plasma simulation codes, and\nrecovers a wide range of well-known kinetic plasma processes, including plasma\nthermalization, electrostatic fluctuations about thermal equilibrium, and the\ndrag on a fast sheet and Landau damping. We compare the performance against the\noriginal plasma model in terms of run-time, conservation laws, and temporal\nevolution of key physical quantities. The limitations of the model are\npresented and possible directions for higher-dimensional surrogate models for\nkinetic plasmas are discussed.",
            "author": [
                "Diogo D Carvalho",
                "Diogo R Ferreira",
                "Luis O Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17646v1",
                "http://arxiv.org/pdf/2310.17646v1"
            ],
            "primary_category": "physics.plasm-ph",
            "category": [
                "physics.plasm-ph",
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17644v1",
            "title": "torchdistill Meets Hugging Face Libraries for Reproducible, Coding-Free\n  Deep Learning Studies: A Case Study on NLP",
            "updated": "2023-10-26T17:57:15Z",
            "published": "2023-10-26T17:57:15Z",
            "summary": "Reproducibility in scientific work has been becoming increasingly important\nin research communities such as machine learning, natural language processing,\nand computer vision communities due to the rapid development of the research\ndomains supported by recent advances in deep learning. In this work, we present\na significantly upgraded version of torchdistill, a modular-driven coding-free\ndeep learning framework significantly upgraded from the initial release, which\nsupports only image classification and object detection tasks for reproducible\nknowledge distillation experiments. To demonstrate that the upgraded framework\ncan support more tasks with third-party libraries, we reproduce the GLUE\nbenchmark results of BERT models using a script based on the upgraded\ntorchdistill, harmonizing with various Hugging Face libraries. All the 27\nfine-tuned BERT models and configurations to reproduce the results are\npublished at Hugging Face, and the model weights have already been widely used\nin research communities. We also reimplement popular small-sized models and new\nknowledge distillation methods and perform additional experiments for computer\nvision tasks.",
            "author": [
                "Yoshitomo Matsubara"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17644v1",
                "http://arxiv.org/pdf/2310.17644v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17638v1",
            "title": "Generative Fractional Diffusion Models",
            "updated": "2023-10-26T17:53:24Z",
            "published": "2023-10-26T17:53:24Z",
            "summary": "We generalize the continuous time framework for score-based generative models\nfrom an underlying Brownian motion (BM) to an approximation of fractional\nBrownian motion (FBM). We derive a continuous reparameterization trick and the\nreverse time model by representing FBM as a stochastic integral over a family\nof Ornstein-Uhlenbeck processes to define generative fractional diffusion\nmodels (GFDM) with driving noise converging to a non-Markovian process of\ninfinite quadratic variation. The Hurst index $H\\in(0,1)$ of FBM enables\ncontrol of the roughness of the distribution transforming path. To the best of\nour knowledge, this is the first attempt to build a generative model upon a\nstochastic process with infinite quadratic variation.",
            "author": [
                "Gabriel Nobis",
                "Marco Aversa",
                "Maximilian Springenberg",
                "Michael Detzel",
                "Stefano Ermon",
                "Shinichi Nakajima",
                "Roderick Murray-Smith",
                "Sebastian Lapuschkin",
                "Christoph Knochenhauer",
                "Luis Oala",
                "Wojciech Samek"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17638v1",
                "http://arxiv.org/pdf/2310.17638v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML",
                "I.2.4; F.4.1; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17631v1",
            "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
            "updated": "2023-10-26T17:48:58Z",
            "published": "2023-10-26T17:48:58Z",
            "summary": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.",
            "author": [
                "Lianghui Zhu",
                "Xinggang Wang",
                "Xinlong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17631v1",
                "http://arxiv.org/pdf/2310.17631v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17626v1",
            "title": "A Survey on Transferability of Adversarial Examples across Deep Neural\n  Networks",
            "updated": "2023-10-26T17:45:26Z",
            "published": "2023-10-26T17:45:26Z",
            "summary": "The emergence of Deep Neural Networks (DNNs) has revolutionized various\ndomains, enabling the resolution of complex tasks spanning image recognition,\nnatural language processing, and scientific problem-solving. However, this\nprogress has also exposed a concerning vulnerability: adversarial examples.\nThese crafted inputs, imperceptible to humans, can manipulate machine learning\nmodels into making erroneous predictions, raising concerns for safety-critical\napplications. An intriguing property of this phenomenon is the transferability\nof adversarial examples, where perturbations crafted for one model can deceive\nanother, often with a different architecture. This intriguing property enables\n\"black-box\" attacks, circumventing the need for detailed knowledge of the\ntarget model. This survey explores the landscape of the adversarial\ntransferability of adversarial examples. We categorize existing methodologies\nto enhance adversarial transferability and discuss the fundamental principles\nguiding each approach. While the predominant body of research primarily\nconcentrates on image classification, we also extend our discussion to\nencompass other vision tasks and beyond. Challenges and future prospects are\ndiscussed, highlighting the importance of fortifying DNNs against adversarial\nvulnerabilities in an evolving landscape.",
            "author": [
                "Jindong Gu",
                "Xiaojun Jia",
                "Pau de Jorge",
                "Wenqain Yu",
                "Xinwei Liu",
                "Avery Ma",
                "Yuan Xun",
                "Anjun Hu",
                "Ashkan Khakzar",
                "Zhijiang Li",
                "Xiaochun Cao",
                "Philip Torr"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17626v1",
                "http://arxiv.org/pdf/2310.17626v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17613v1",
            "title": "On The Toric Ideals of the Coloured Graphs of Reduced Words",
            "updated": "2023-10-26T17:35:20Z",
            "published": "2023-10-26T17:35:20Z",
            "summary": "We study a family $\\mathcal{B}$ of pseudo-multipartite graphs indexed by\nstaircase partitions. They are realised from the reduced words of certain class\nof permutations. We investigate the vertex proper colouring of these graphs and\ngive the general chromatic polynomial. For each member $B_{\\lambda}$, we\nconstruct an affine toric ideal $\\mathcal{I}_{B_{\\lambda}}$ associated to\nvertex proper colouring using partition identity. It turns out that the\nprojective version $\\mathcal{V}(\\mathcal{I}_{B_{\\lambda}})$ is realised from\nthe cartoon diagram associated with the vertex proper colouring.",
            "author": [
                "Praise Adeyemo"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17613v1",
                "http://arxiv.org/pdf/2310.17613v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17609v1",
            "title": "LeCaRDv2: A Large-Scale Chinese Legal Case Retrieval Dataset",
            "updated": "2023-10-26T17:32:55Z",
            "published": "2023-10-26T17:32:55Z",
            "summary": "As an important component of intelligent legal systems, legal case retrieval\nplays a critical role in ensuring judicial justice and fairness. However, the\ndevelopment of legal case retrieval technologies in the Chinese legal system is\nrestricted by three problems in existing datasets: limited data size, narrow\ndefinitions of legal relevance, and naive candidate pooling strategies used in\ndata sampling. To alleviate these issues, we introduce LeCaRDv2, a large-scale\nLegal Case Retrieval Dataset (version 2). It consists of 800 queries and 55,192\ncandidates extracted from 4.3 million criminal case documents. To the best of\nour knowledge, LeCaRDv2 is one of the largest Chinese legal case retrieval\ndatasets, providing extensive coverage of criminal charges. Additionally, we\nenrich the existing relevance criteria by considering three key aspects:\ncharacterization, penalty, procedure. This comprehensive criteria enriches the\ndataset and may provides a more holistic perspective. Furthermore, we propose a\ntwo-level candidate set pooling strategy that effectively identify potential\ncandidates for each query case. It's important to note that all cases in the\ndataset have been annotated by multiple legal experts specializing in criminal\nlaw. Their expertise ensures the accuracy and reliability of the annotations.\nWe evaluate several state-of-the-art retrieval models at LeCaRDv2,\ndemonstrating that there is still significant room for improvement in legal\ncase retrieval. The details of LeCaRDv2 can be found at the anonymous website\nhttps://github.com/anonymous1113243/LeCaRDv2.",
            "author": [
                "Haitao Li",
                "Yunqiu Shao",
                "Yueyue Wu",
                "Qingyao Ai",
                "Yixiao Ma",
                "Yiqun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17609v1",
                "http://arxiv.org/pdf/2310.17609v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17606v1",
            "title": "Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in\n  Ghana",
            "updated": "2023-10-26T17:30:13Z",
            "published": "2023-10-26T17:30:13Z",
            "summary": "This paper reports on a set of three recent experiments utilizing large-scale\nspeech models to evaluate the oral reading fluency (ORF) of students in Ghana.\nWhile ORF is a well-established measure of foundational literacy, assessing it\ntypically requires one-on-one sessions between a student and a trained\nevaluator, a process that is time-consuming and costly. Automating the\nevaluation of ORF could support better literacy instruction, particularly in\neducation contexts where formative assessment is uncommon due to large class\nsizes and limited resources. To our knowledge, this research is among the first\nto examine the use of the most recent versions of large-scale speech models\n(Whisper V2 wav2vec2.0) for ORF assessment in the Global South.\n  We find that Whisper V2 produces transcriptions of Ghanaian students reading\naloud with a Word Error Rate of 13.5. This is close to the model's average WER\non adult speech (12.8) and would have been considered state-of-the-art for\nchildren's speech transcription only a few years ago. We also find that when\nthese transcriptions are used to produce fully automated ORF scores, they\nclosely align with scores generated by expert human graders, with a correlation\ncoefficient of 0.96. Importantly, these results were achieved on a\nrepresentative dataset (i.e., students with regional accents, recordings taken\nin actual classrooms), using a free and publicly available speech model out of\nthe box (i.e., no fine-tuning). This suggests that using large-scale speech\nmodels to assess ORF may be feasible to implement and scale in lower-resource,\nlinguistically diverse educational contexts.",
            "author": [
                "Owen Henkel",
                "Hannah Horne-Robinson",
                "Libby Hills",
                "Bill Roberts",
                "Joshua McGrane"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17606v1",
                "http://arxiv.org/pdf/2310.17606v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17594v2",
            "title": "SPA: A Graph Spectral Alignment Perspective for Domain Adaptation",
            "updated": "2023-10-27T08:40:15Z",
            "published": "2023-10-26T17:13:48Z",
            "summary": "Unsupervised domain adaptation (UDA) is a pivotal form in machine learning to\nextend the in-domain model to the distinctive target domains where the data\ndistributions differ. Most prior works focus on capturing the inter-domain\ntransferability but largely overlook rich intra-domain structures, which\nempirically results in even worse discriminability. In this work, we introduce\na novel graph SPectral Alignment (SPA) framework to tackle the tradeoff. The\ncore of our method is briefly condensed as follows: (i)-by casting the DA\nproblem to graph primitives, SPA composes a coarse graph alignment mechanism\nwith a novel spectral regularizer towards aligning the domain graphs in\neigenspaces; (ii)-we further develop a fine-grained message propagation module\n-- upon a novel neighbor-aware self-training mechanism -- in order for enhanced\ndiscriminability in the target domain. On standardized benchmarks, the\nextensive experiments of SPA demonstrate that its performance has surpassed the\nexisting cutting-edge DA methods. Coupled with dense model analysis, we\nconclude that our approach indeed possesses superior efficacy, robustness,\ndiscriminability, and transferability. Code and data are available at:\nhttps://github.com/CrownX/SPA.",
            "author": [
                "Zhiqing Xiao",
                "Haobo Wang",
                "Ying Jin",
                "Lei Feng",
                "Gang Chen",
                "Fei Huang",
                "Junbo Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17594v2",
                "http://arxiv.org/pdf/2310.17594v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.18373v1",
            "title": "Can LLMs Grade Short-answer Reading Comprehension Questions :\n  Foundational Literacy Assessment in LMICs",
            "updated": "2023-10-26T17:05:40Z",
            "published": "2023-10-26T17:05:40Z",
            "summary": "This paper presents emerging evidence of using generative large language\nmodels (i.e., GPT-4) to reliably evaluate short-answer reading comprehension\nquestions. Specifically, we explore how various configurations of generative\n(LLMs) are able to evaluate student responses from a new dataset, drawn from a\nbattery of reading assessments conducted with over 150 students in Ghana. As\nthis dataset is novel and hence not used in training runs of GPT, it offers an\nopportunity to test for domain shift and evaluate the generalizability of\ngenerative LLMs, which are predominantly designed and trained on data from\nhigh-income North American countries. We found that GPT-4, with minimal prompt\nengineering performed extremely well on evaluating the novel dataset (Quadratic\nWeighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning\nbased approaches, and even exceeding expert human raters (Quadratic Weighted\nKappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to\nempirically evaluate the performance of generative LLMs on short-answer reading\ncomprehension questions, using real student data, and suggests that generative\nLLMs have the potential to reliably evaluate foundational literacy. Currently\nthe assessment of formative literacy and numeracy is infrequent in many low and\nmiddle-income countries (LMICs) due to the cost and operational complexities of\nconducting them at scale. Automating the grading process for reading assessment\ncould enable wider usage, and in turn improve decision-making regarding\ncurricula, school management, and teaching practice at the classroom level.\nImportantly, in contrast transfer learning based approaches, generative LLMs\ngeneralize well and the technical barriers to their use are low, making them\nmore feasible to implement and scale in lower resource educational contexts.",
            "author": [
                "Owen Henkel",
                "Libby Hills",
                "Bill Roberts",
                "Joshua McGrane"
            ],
            "link": [
                "http://arxiv.org/abs/2310.18373v1",
                "http://arxiv.org/pdf/2310.18373v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17579v1",
            "title": "BLIS-Net: Classifying and Analyzing Signals on Graphs",
            "updated": "2023-10-26T17:03:14Z",
            "published": "2023-10-26T17:03:14Z",
            "summary": "Graph neural networks (GNNs) have emerged as a powerful tool for tasks such\nas node classification and graph classification. However, much less work has\nbeen done on signal classification, where the data consists of many functions\n(referred to as signals) defined on the vertices of a single graph. These tasks\nrequire networks designed differently from those designed for traditional GNN\ntasks. Indeed, traditional GNNs rely on localized low-pass filters, and signals\nof interest may have intricate multi-frequency behavior and exhibit long range\ninteractions. This motivates us to introduce the BLIS-Net (Bi-Lipschitz\nScattering Net), a novel GNN that builds on the previously introduced geometric\nscattering transform. Our network is able to capture both local and global\nsignal structure and is able to capture both low-frequency and high-frequency\ninformation. We make several crucial changes to the original geometric\nscattering architecture which we prove increase the ability of our network to\ncapture information about the input signal and show that BLIS-Net achieves\nsuperior performance on both synthetic and real-world data sets based on\ntraffic flow and fMRI data.",
            "author": [
                "Charles Xu",
                "Laney Goldman",
                "Valentina Guo",
                "Benjamin Hollander-Bodie",
                "Maedee Trank-Greene",
                "Ian Adelstein",
                "Edward De Brouwer",
                "Rex Ying",
                "Smita Krishnaswamy",
                "Michael Perlmutter"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17579v1",
                "http://arxiv.org/pdf/2310.17579v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17574v1",
            "title": "Effective Prime Factorization via Quantum Annealing by Modular\n  Locally-structured Embedding",
            "updated": "2023-10-26T17:00:22Z",
            "published": "2023-10-26T17:00:22Z",
            "summary": "This paper investigates novel techniques to solve prime factorization by\nquantum annealing (QA). Our contribution is twofold. First, we present a novel\nand very compact modular encoding of a binary multiplier circuit into the\nPegasus architecture of current D-Wave QA devices. The key contribution is a\ncompact encoding of a controlled full-adder into an 8-qubit module in the\nPegasus topology, which we synthesized offline by means of Optimization Modulo\nTheories. This allows us to encode up to a 21*12-bit multiplier (and a 22*8-bit\none) into the Pegasus 5760-qubit topology of current annealers. To the best of\nour knowledge, these are the largest factorization problems ever encoded into a\nquantum annealer. Second, we have investigated the problem of actually solving\nencoded PF problems by running an extensive experimental evaluation on a D-Wave\nAdvantage 4.1 quantum annealer. In order to help the annealer in reaching the\nglobal minimum, in the experiments we introduced different approaches to\ninitialize the multiplier qubits and adopted several performance enhancement\ntechniques. Overall, exploiting all the encoding and solving techniques\ndescribed in this paper, 8, 219, 999 = 32, 749 * 251 was the highest prime\nproduct we were able to factorize within the limits of our QPU resources. To\nthe best of our knowledge, this is the largest number which was ever factorized\nby means of a quantum annealer, and, more generally, by a quantum device.",
            "author": [
                "Jingwen Ding",
                "Giuseppe Spallitta",
                "Roberto Sebastiani"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17574v1",
                "http://arxiv.org/pdf/2310.17574v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17545v1",
            "title": "Using Buckingham's $\u03c0$ Theorem for Multi-System Learning Transfer: a\n  Case-study with 3 Vehicles Sharing a Database",
            "updated": "2023-10-26T16:42:13Z",
            "published": "2023-10-26T16:42:13Z",
            "summary": "Learning schemes for planning and control are limited by the difficulty of\ncollecting large amounts of experimental data or having to rely on\nhigh-fidelity simulations. This paper explores the potential of a proposed\nlearning scheme that leverages dimensionless numbers based on Buckingham's\n$\\pi$ theorem to improve data efficiency and facilitate knowledge sharing\nbetween similar systems. A case study using car-like robots compares\ntraditional and dimensionless learning models on simulated and experimental\ndata to validate the benefits of the new dimensionless learning approach.\nPreliminary results show that this new dimensionless approach could accelerate\nthe learning rate and improve the accuracy of the model and should be\ninvestigated further.",
            "author": [
                "William Therrien",
                "Olivier Lecompte",
                "Alexandre Girard"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17545v1",
                "http://arxiv.org/pdf/2310.17545v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17534v1",
            "title": "SoK: Pitfalls in Evaluating Black-Box Attacks",
            "updated": "2023-10-26T16:23:40Z",
            "published": "2023-10-26T16:23:40Z",
            "summary": "Numerous works study black-box attacks on image classifiers. However, these\nworks make different assumptions on the adversary's knowledge and current\nliterature lacks a cohesive organization centered around the threat model. To\nsystematize knowledge in this area, we propose a taxonomy over the threat space\nspanning the axes of feedback granularity, the access of interactive queries,\nand the quality and quantity of the auxiliary data available to the attacker.\nOur new taxonomy provides three key insights. 1) Despite extensive literature,\nnumerous under-explored threat spaces exist, which cannot be trivially solved\nby adapting techniques from well-explored settings. We demonstrate this by\nestablishing a new state-of-the-art in the less-studied setting of access to\ntop-k confidence scores by adapting techniques from well-explored settings of\naccessing the complete confidence vector, but show how it still falls short of\nthe more restrictive setting that only obtains the prediction label,\nhighlighting the need for more research. 2) Identification the threat model of\ndifferent attacks uncovers stronger baselines that challenge prior\nstate-of-the-art claims. We demonstrate this by enhancing an initially weaker\nbaseline (under interactive query access) via surrogate models, effectively\noverturning claims in the respective paper. 3) Our taxonomy reveals\ninteractions between attacker knowledge that connect well to related areas,\nsuch as model inversion and extraction attacks. We discuss how advances in\nother areas can enable potentially stronger black-box attacks. Finally, we\nemphasize the need for a more realistic assessment of attack success by\nfactoring in local attack runtime. This approach reveals the potential for\ncertain attacks to achieve notably higher success rates and the need to\nevaluate attacks in diverse and harder settings, highlighting the need for\nbetter selection criteria.",
            "author": [
                "Fnu Suya",
                "Anshuman Suri",
                "Tingwei Zhang",
                "Jingtao Hong",
                "Yuan Tian",
                "David Evans"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17534v1",
                "http://arxiv.org/pdf/2310.17534v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17520v2",
            "title": "On the nontrivial extremal eigenvalues of graphs",
            "updated": "2023-10-30T06:54:31Z",
            "published": "2023-10-26T16:13:08Z",
            "summary": "We present a finer quantitative version of an observation due to Breuillard,\nGreen, Guralnick and Tao which tells that for finite non-bipartite Cayley\ngraphs, once the nontrivial eigenvalues of their normalized adjacency matrices\nare uniformly bounded away from $1$, then they are also uniformly bounded away\nfrom $-1$. Unlike previous works which depend heavily on combinatorial\narguments, we rely more on analysis of eigenfunctions. We establish a new\nexplicit lower bound for the gap between $-1$ and the smallest normalized\nadjacency eigenvalue, which improves previous lower bounds in terms of\nedge-expansion, and is comparable to the best known lower bound in terms of\nvertex-expansion.",
            "author": [
                "Wenbo Li",
                "Shiping Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17520v2",
                "http://arxiv.org/pdf/2310.17520v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.SP",
                "05C50, 05C48, 20F65"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17506v1",
            "title": "Predicting Patient No-Shows in Community Health Clinics: A Case Study in\n  Designing a Data Analytic Product",
            "updated": "2023-10-26T15:56:43Z",
            "published": "2023-10-26T15:56:43Z",
            "summary": "The data science revolution has highlighted the varying roles that data\nanalytic products can play in a different industries and applications. There\nhas been particular interest in using analytic products coupled with\nalgorithmic prediction models to aid in human decision-making. However,\ndetailed descriptions of the decision-making process that leads to the design\nand development of analytic products are lacking in the statistical literature,\nmaking it difficult to accumulate a body of knowledge where students interested\nin the field of data science may look to learn about this process. In this\npaper, we present a case study describing the development of an analytic\nproduct for predicting whether patients will show up for scheduled appointments\nat a community health clinic. We consider the stakeholders involved and their\ninterests, along with the real-world analytical and technical trade-offs\ninvolved in developing and deploying the product. Our goal here is to highlight\nthe decisions made and evaluate them in the context of possible alternatives.\nWe find that although this case study has some unique characteristics, there\nare lessons to be learned that could translate to other settings and\napplications.",
            "author": [
                "Roger D. Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17506v1",
                "http://arxiv.org/pdf/2310.17506v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17493v2",
            "title": "A Hybrid Graph Network for Complex Activity Detection in Video",
            "updated": "2023-10-30T15:47:44Z",
            "published": "2023-10-26T15:49:35Z",
            "summary": "Interpretation and understanding of video presents a challenging computer\nvision task in numerous fields - e.g. autonomous driving and sports analytics.\nExisting approaches to interpreting the actions taking place within a video\nclip are based upon Temporal Action Localisation (TAL), which typically\nidentifies short-term actions. The emerging field of Complex Activity Detection\n(CompAD) extends this analysis to long-term activities, with a deeper\nunderstanding obtained by modelling the internal structure of a complex\nactivity taking place within the video. We address the CompAD problem using a\nhybrid graph neural network which combines attention applied to a graph\nencoding the local (short-term) dynamic scene with a temporal graph modelling\nthe overall long-duration activity. Our approach is as follows: i) Firstly, we\npropose a novel feature extraction technique which, for each video snippet,\ngenerates spatiotemporal `tubes' for the active elements (`agents') in the\n(local) scene by detecting individual objects, tracking them and then\nextracting 3D features from all the agent tubes as well as the overall scene.\nii) Next, we construct a local scene graph where each node (representing either\nan agent tube or the scene) is connected to all other nodes. Attention is then\napplied to this graph to obtain an overall representation of the local dynamic\nscene. iii) Finally, all local scene graph representations are interconnected\nvia a temporal graph, to estimate the complex activity class together with its\nstart and end time. The proposed framework outperforms all previous\nstate-of-the-art methods on all three datasets including ActivityNet-1.3,\nThumos-14, and ROAD.",
            "author": [
                "Salman Khan",
                "Izzeddin Teeti",
                "Andrew Bradley",
                "Mohamed Elhoseiny",
                "Fabio Cuzzolin"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17493v2",
                "http://arxiv.org/pdf/2310.17493v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17488v2",
            "title": "LightLM: A Lightweight Deep and Narrow Language Model for Generative\n  Recommendation",
            "updated": "2023-10-30T02:50:17Z",
            "published": "2023-10-26T15:44:57Z",
            "summary": "This paper presents LightLM, a lightweight Transformer-based language model\nfor generative recommendation. While Transformer-based generative modeling has\ngained importance in various AI sub-fields such as NLP and vision, generative\nrecommendation is still in its infancy due to its unique demand on personalized\ngenerative modeling. Existing works on generative recommendation often use\nNLP-oriented Transformer architectures such as T5, GPT, LLaMA and M6, which are\nheavy-weight and are not specifically designed for recommendation tasks.\nLightLM tackles the issue by introducing a light-weight deep and narrow\nTransformer architecture, which is specifically tailored for direct generation\nof recommendation items. This structure is especially apt for straightforward\ngenerative recommendation and stems from the observation that language model\ndoes not have to be too wide for this task, as the input predominantly consists\nof short tokens that are well-suited for the model's capacity. We also show\nthat our devised user and item ID indexing methods, i.e., Spectral\nCollaborative Indexing (SCI) and Graph Collaborative Indexing (GCI), enables\nthe deep and narrow Transformer architecture to outperform large-scale language\nmodels for recommendation. Besides, to address the hallucination problem of\ngenerating items as output, we propose the constrained generation process for\ngenerative recommenders. Experiments on real-world datasets show that LightLM\noutperforms various competitive baselines in terms of both recommendation\naccuracy and efficiency. The code can be found at\nhttps://github.com/dongyuanjushi/LightLM.",
            "author": [
                "Kai Mei",
                "Yongfeng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17488v2",
                "http://arxiv.org/pdf/2310.17488v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17469v1",
            "title": "Improved asymptotic upper bounds for the minimum number of pairwise\n  distinct longest cycles in regular graphs",
            "updated": "2023-10-26T15:15:13Z",
            "published": "2023-10-26T15:15:13Z",
            "summary": "We study how few pairwise distinct longest cycles a regular graph can have\nunder additional constraints. For each integer $r \\geq 5$, we give exponential\nimprovements for the best asymptotic upper bounds for this invariant under the\nadditional constraint that the graphs are $r$-regular hamiltonian graphs.\nEarlier work showed that a conjecture by Haythorpe on a lower bound for this\ninvariant is false because of an incorrect constant factor, whereas our results\nimply that the conjecture is even asymptotically incorrect. Motivated by a\nquestion of Zamfirescu and work of Chia and Thomassen, we also study this\ninvariant for non-hamiltonian 2-connected $r$-regular graphs and show that in\nthis case the invariant can be bounded from above by a constant for all large\nenough graphs, even for graphs with arbitrarily large girth.",
            "author": [
                "Jorik Jooken"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17469v1",
                "http://arxiv.org/pdf/2310.17469v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17463v1",
            "title": "Bayesian Neural Controlled Differential Equations for Treatment Effect\n  Estimation",
            "updated": "2023-10-26T15:10:35Z",
            "published": "2023-10-26T15:10:35Z",
            "summary": "Treatment effect estimation in continuous time is crucial for personalized\nmedicine. However, existing methods for this task are limited to point\nestimates of the potential outcomes, whereas uncertainty estimates have been\nignored. Needless to say, uncertainty quantification is crucial for reliable\ndecision-making in medical applications. To fill this gap, we propose a novel\nBayesian neural controlled differential equation (BNCDE) for treatment effect\nestimation in continuous time. In our BNCDE, the time dimension is modeled\nthrough a coupled system of neural controlled differential equations and neural\nstochastic differential equations, where the neural stochastic differential\nequations allow for tractable variational Bayesian inference. Thereby, for an\nassigned sequence of treatments, our BNCDE provides meaningful posterior\npredictive distributions of the potential outcomes. To the best of our\nknowledge, ours is the first tailored neural method to provide uncertainty\nestimates of treatment effects in continuous time. As such, our method is of\ndirect practical value for promoting reliable decision-making in medicine.",
            "author": [
                "Konstantin Hess",
                "Valentyn Melnychuk",
                "Dennis Frauen",
                "Stefan Feuerriegel"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17463v1",
                "http://arxiv.org/pdf/2310.17463v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2310.17462v2",
            "title": "Towards Learning Monocular 3D Object Localization From 2D Labels using\n  the Physical Laws of Motion",
            "updated": "2023-11-29T14:33:28Z",
            "published": "2023-10-26T15:10:10Z",
            "summary": "We present a novel method for precise 3D object localization in single images\nfrom a single calibrated camera using only 2D labels. No expensive 3D labels\nare needed. Thus, instead of using 3D labels, our model is trained with\neasy-to-annotate 2D labels along with the physical knowledge of the object's\nmotion. Given this information, the model can infer the latent third dimension,\neven though it has never seen this information during training. Our method is\nevaluated on both synthetic and real-world datasets, and we are able to achieve\na mean distance error of just 6 cm in our experiments on real data. The results\nindicate the method's potential as a step towards learning 3D object location\nestimation, where collecting 3D data for training is not feasible.",
            "author": [
                "Daniel Kienzle",
                "Julian Lorenz",
                "Katja Ludwig",
                "Rainer Lienhart"
            ],
            "link": [
                "http://arxiv.org/abs/2310.17462v2",
                "http://arxiv.org/pdf/2310.17462v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    }
]