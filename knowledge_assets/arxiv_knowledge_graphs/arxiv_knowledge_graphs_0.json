[
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04532v1",
            "title": "Tournaments on signed graphs",
            "updated": "2023-12-07T18:50:50Z",
            "published": "2023-12-07T18:50:50Z",
            "summary": "We classify the set of tournament score sequences on signed graphs,\ngeneralizing a classical result of Landau. We also study the combinatorics of\nsuch tournaments with given score sequence. Specifically, we show that the\nCoxeter analogues of the interchange graphs introduced by Brualdi and Li remain\ndegree regular. This degree can be interpreted geometrically, in terms of\ndistances in the Coxeter permutahedra studied recently by Ardila, Castillo, Eur\nand Postnikov.",
            "author": [
                "Brett Kolesnik",
                "Rivka Mitchell",
                "Tomasz Przyby\u0142owski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04532v1",
                "http://arxiv.org/pdf/2312.04532v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C20, 11P21, 17B22, 20F55, 51F15, 51M20, 52B05, 62J15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04530v1",
            "title": "Camera Height Doesn't Change: Unsupervised Monocular Scale-Aware\n  Road-Scene Depth Estimation",
            "updated": "2023-12-07T18:50:01Z",
            "published": "2023-12-07T18:50:01Z",
            "summary": "Monocular depth estimators either require explicit scale supervision through\nauxiliary sensors or suffer from scale ambiguity, which renders them difficult\nto deploy in downstream applications. A possible source of scale is the sizes\nof objects found in the scene, but inaccurate localization makes them difficult\nto exploit. In this paper, we introduce a novel scale-aware monocular depth\nestimation method called StableCamH that does not require any auxiliary sensor\nor supervision. The key idea is to exploit prior knowledge of object heights in\nthe scene but aggregate the height cues into a single invariant measure common\nto all frames in a road video sequence, namely the camera height. By\nformulating monocular depth estimation as camera height optimization, we\nachieve robust and accurate unsupervised end-to-end training. To realize\nStableCamH, we devise a novel learning-based size prior that can directly\nconvert car appearance into its dimensions. Extensive experiments on KITTI and\nCityscapes show the effectiveness of StableCamH, its state-of-the-art accuracy\ncompared with related methods, and its generalizability. The training framework\nof StableCamH can be used for any monocular depth estimation method and will\nhopefully become a fundamental building block for further work.",
            "author": [
                "Genki Kinoshita",
                "Ko Nishino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04530v1",
                "http://arxiv.org/pdf/2312.04530v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04526v1",
            "title": "Algorithms for the Global Domination Problem",
            "updated": "2023-12-07T18:45:16Z",
            "published": "2023-12-07T18:45:16Z",
            "summary": "A dominating set D in a graph G is a subset of its vertices such that every\nvertex of the graph which does not belong to set D is adjacent to at least one\nvertex from set D. A set of vertices of graph G is a global dominating set if\nit is a dominating set for both, graph G and its complement. The objective is\nto find a global dominating set with the minimum cardinality. The problem is\nknown to be NP-hard. Neither exact nor approximation algorithm existed . We\npropose two exact solution methods, one of them being based on an integer\nlinear program (ILP) formulation, three heuristic algorithms and a special\npurification procedure that further reduces the size of a global dominated set\ndelivered by any of our heuristic algorithms. We show that the problem remains\nNP-hard for restricted types of graphs and specify some families of graphs for\nwhich the heuristics guarantee the optimality. The second exact algorithm\nturned out to be about twice faster than ILP for graphs with more than 230\nvertices and up to 1080 vertices, which were the largest benchmark instances\nthat were solved optimally. The heuristics were tested for the existing 2284\nbenchmark problem instances with up to 14000 vertices and delivered solutions\nfor the largest instances in less than one minute. Remarkably, for about 52% of\nthe 1000 instances with the obtained optimal solutions, at least one of the\nheuristics generated an optimal solution, where the average approximation error\nfor the remaining instances was 1.07%.",
            "author": [
                "Ernesto Parra Inza",
                "Nodari Vakhania",
                "Jose M. Sigarreta Almira",
                "Frank A. Hern\u00e1ndez Mira"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04526v1",
                "http://arxiv.org/pdf/2312.04526v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04511v1",
            "title": "An LLM Compiler for Parallel Function Calling",
            "updated": "2023-12-07T18:32:04Z",
            "published": "2023-12-07T18:32:04Z",
            "summary": "Large Language Models (LLMs) have shown remarkable results on various complex\nreasoning benchmarks. The reasoning capabilities of LLMs enable them to execute\nfunction calls, using user-provided functions to overcome their inherent\nlimitations, such as knowledge cutoffs, poor arithmetic skills, or lack of\naccess to private data. This development has expanded LLMs' scope to include\nmulti-function calling, where LLMs are equipped with a variety of functions and\nselect the proper functions based on the context. Multi-function calling\nabilities of LLMs have catalyzed LLM-based software development, allowing them\nto tackle more complex problems. However, current methods for multi-function\ncalling often require sequential reasoning and acting for each function which\ncan result in high latency, cost, and sometimes inaccurate behavior. To address\nthis, we introduce LLMCompiler, which executes functions in parallel to\nefficiently orchestrate multi-function calling. Drawing from the principles of\nclassical compilers, LLMCompiler streamlines parallel function calling with\nthree components: (i) an LLM Planner, formulating execution strategies and\ndependencies; (ii) a Task Fetching Unit, dispatching function calling tasks;\nand (iii) an Executor, executing these tasks in parallel. LLMCompiler\nautomatically computes an optimized orchestration for the function calls and\ncan be used with open-source models such as LLaMA-2. We have benchmarked\nLLMCompiler on a range of tasks including cases with non-trivial\ninter-dependency between function calls, as well as cases that require dynamic\nreplanning based on intermediate results. We observe consistent latency speedup\nof up to 3.7x, cost savings of up to 6.7x, and accuracy improvement of up to\n~9% as compared to ReAct. Additionally, LLMCompiler achieves up to 1.35x\nlatency gain over OpenAI's recent parallel function calling, while achieving\nsimilar accuracy.",
            "author": [
                "Sehoon Kim",
                "Suhong Moon",
                "Ryan Tabrizi",
                "Nicholas Lee",
                "Michael W. Mahoney",
                "Kurt Keutzer",
                "Amir Gholami"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04511v1",
                "http://arxiv.org/pdf/2312.04511v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04504v1",
            "title": "Coordination-free Decentralised Federated Learning on Complex Networks:\n  Overcoming Heterogeneity",
            "updated": "2023-12-07T18:24:19Z",
            "published": "2023-12-07T18:24:19Z",
            "summary": "Federated Learning (FL) is a well-known framework for successfully performing\na learning task in an edge computing scenario where the devices involved have\nlimited resources and incomplete data representation. The basic assumption of\nFL is that the devices communicate directly or indirectly with a parameter\nserver that centrally coordinates the whole process, overcoming several\nchallenges associated with it. However, in highly pervasive edge scenarios, the\npresence of a central controller that oversees the process cannot always be\nguaranteed, and the interactions (i.e., the connectivity graph) between devices\nmight not be predetermined, resulting in a complex network structure. Moreover,\nthe heterogeneity of data and devices further complicates the learning process.\nThis poses new challenges from a learning standpoint that we address by\nproposing a communication-efficient Decentralised Federated Learning (DFL)\nalgorithm able to cope with them. Our solution allows devices communicating\nonly with their direct neighbours to train an accurate model, overcoming the\nheterogeneity induced by data and different training histories. Our results\nshow that the resulting local models generalise better than those trained with\ncompeting approaches, and do so in a more communication-efficient way.",
            "author": [
                "Lorenzo Valerio",
                "Chiara Boldrini",
                "Andrea Passarella",
                "J\u00e1nos Kert\u00e9sz",
                "M\u00e1rton Karsai",
                "Gerardo I\u00f1iguez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04504v1",
                "http://arxiv.org/pdf/2312.04504v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC",
                "cs.MA",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04501v1",
            "title": "Graph Metanetworks for Processing Diverse Neural Architectures",
            "updated": "2023-12-07T18:21:52Z",
            "published": "2023-12-07T18:21:52Z",
            "summary": "Neural networks efficiently encode learned information within their\nparameters. Consequently, many tasks can be unified by treating neural networks\nthemselves as input data. When doing so, recent studies demonstrated the\nimportance of accounting for the symmetries and geometry of parameter spaces.\nHowever, those works developed architectures tailored to specific networks such\nas MLPs and CNNs without normalization layers, and generalizing such\narchitectures to other types of networks can be challenging. In this work, we\novercome these challenges by building new metanetworks - neural networks that\ntake weights from other neural networks as input. Put simply, we carefully\nbuild graphs representing the input neural networks and process the graphs\nusing graph neural networks. Our approach, Graph Metanetworks (GMNs),\ngeneralizes to neural architectures where competing methods struggle, such as\nmulti-head attention layers, normalization layers, convolutional layers, ResNet\nblocks, and group-equivariant linear layers. We prove that GMNs are expressive\nand equivariant to parameter permutation symmetries that leave the input neural\nnetwork functions unchanged. We validate the effectiveness of our method on\nseveral metanetwork tasks over diverse neural network architectures.",
            "author": [
                "Derek Lim",
                "Haggai Maron",
                "Marc T. Law",
                "Jonathan Lorraine",
                "James Lucas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04501v1",
                "http://arxiv.org/pdf/2312.04501v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04494v1",
            "title": "AVA: Towards Autonomous Visualization Agents through Visual\n  Perception-Driven Decision-Making",
            "updated": "2023-12-07T18:13:42Z",
            "published": "2023-12-07T18:13:42Z",
            "summary": "With recent advances in multi-modal foundation models, the previously\ntext-only large language models (LLM) have evolved to incorporate visual input,\nopening up unprecedented opportunities for various applications in\nvisualization. Our work explores the utilization of the visual perception\nability of multi-modal LLMs to develop Autonomous Visualization Agents (AVAs)\nthat can interpret and accomplish user-defined visualization objectives through\nnatural language. We propose the first framework for the design of AVAs and\npresent several usage scenarios intended to demonstrate the general\napplicability of the proposed paradigm. The addition of visual perception\nallows AVAs to act as the virtual visualization assistant for domain experts\nwho may lack the knowledge or expertise in fine-tuning visualization outputs.\nOur preliminary exploration and proof-of-concept agents suggest that this\napproach can be widely applicable whenever the choices of appropriate\nvisualization parameters require the interpretation of previous visual output.\nFeedback from unstructured interviews with experts in AI research, medical\nvisualization, and radiology has been incorporated, highlighting the\npracticality and potential of AVAs. Our study indicates that AVAs represent a\ngeneral paradigm for designing intelligent visualization systems that can\nachieve high-level visualization goals, which pave the way for developing\nexpert-level visualization agents in the future.",
            "author": [
                "Shusen Liu",
                "Haichao Miao",
                "Zhimin Li",
                "Matthew Olson",
                "Valerio Pascucci",
                "Peer-Timo Bremer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04494v1",
                "http://arxiv.org/pdf/2312.04494v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04492v1",
            "title": "Ergodic theorems for continuous-time quantum walks on crystal lattices\n  and the torus",
            "updated": "2023-12-07T18:08:57Z",
            "published": "2023-12-07T18:08:57Z",
            "summary": "We give several quantum dynamical analogs of the classical Kronecker-Weyl\ntheorem, which says that the trajectory of free motion on the torus along\nalmost every direction tends to equidistribute. As a quantum analog, we study\nthe quantum walk $\\exp(-i t \\Delta) \\psi$ starting from a localized initial\nstate $\\psi$. Then the flow will be ergodic if this evolved state becomes\nequidistributed as time goes on. We prove that this is indeed the case for\nevolutions on the flat torus, provided we start from a point mass, and we prove\ndiscrete analogs of this result for crystal lattices. On some periodic graphs,\nthe mass spreads out non-uniformly, on others it stays localized. Finally, we\ngive examples of quantum evolutions on the sphere which do not equidistribute.",
            "author": [
                "Anne Boutet de Monvel",
                "Mostafa Sabri"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04492v1",
                "http://arxiv.org/pdf/2312.04492v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP",
                "math.SP",
                "47A35, 58J51"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04487v1",
            "title": "On The Maximum Linear Arrangement Problem for Trees",
            "updated": "2023-12-07T18:02:48Z",
            "published": "2023-12-07T18:02:48Z",
            "summary": "Linear arrangements of graphs are a well-known type of graph labeling and are\nfound at the heart of many important computational problems, such as the\nMinimum Linear Arrangement Problem (minLA). A linear arrangement is usually\ndefined as a permutation of the $n$ vertices of a graph. An intuitive geometric\nsetting is that of vertices lying on consecutive integer positions in the real\nline, starting at 1; edges are typically drawn as semicircles above the real\nline. In this paper we study the Maximum Linear Arrangement problem (MaxLA),\nthe maximization variant of minLA and a less studied problem than minLA. We a\ndevise new characterization of maximum arrangements of general graphs, and\nprove that MaxLA can be solved for cycle graphs in constant time, and for\n$k$-linear trees ($k\\le2$) in time $O(n)$. We present a simple algorithm that\nsolves a constrained variant of MaxLA, which we call bipartite MaxLA, in time\n$O(n)$. This algorithm has two promising characteristics. First, it solves\nMaxLA for most trees consisting of a few tenths of nodes. Second, it produces a\nhigh quality approximation to MaxLA for trees where the algorithm fails to\nsolve MaxLA. Furthermore, we conjecture this algorithm solves MaxLA for at\nleast $50\\%$ of all free trees.",
            "author": [
                "Llu\u00eds Alemany-Puig",
                "Juan Luis Esteban",
                "Ramon Ferrer-i-Cancho"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04487v1",
                "http://arxiv.org/pdf/2312.04487v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04479v1",
            "title": "GSGFormer: Generative Social Graph Transformer for Multimodal Pedestrian\n  Trajectory Prediction",
            "updated": "2023-12-07T17:53:02Z",
            "published": "2023-12-07T17:53:02Z",
            "summary": "Pedestrian trajectory prediction, vital for selfdriving cars and\nsocially-aware robots, is complicated due to intricate interactions between\npedestrians, their environment, and other Vulnerable Road Users. This paper\npresents GSGFormer, an innovative generative model adept at predicting\npedestrian trajectories by considering these complex interactions and offering\na plethora of potential modal behaviors. We incorporate a heterogeneous graph\nneural network to capture interactions between pedestrians, semantic maps, and\npotential destinations. The Transformer module extracts temporal features,\nwhile our novel CVAE-Residual-GMM module promotes diverse behavioral modality\ngeneration. Through evaluations on multiple public datasets, GSGFormer not only\noutperforms leading methods with ample data but also remains competitive when\ndata is limited.",
            "author": [
                "Zhongchang Luo",
                "Marion Robin",
                "Pavan Vasishta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04479v1",
                "http://arxiv.org/pdf/2312.04479v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04423v1",
            "title": "Scalable Knowledge Graph Construction and Inference on Human Genome\n  Variants",
            "updated": "2023-12-07T16:48:32Z",
            "published": "2023-12-07T16:48:32Z",
            "summary": "Real-world knowledge can be represented as a graph consisting of entities and\nrelationships between the entities. The need for efficient and scalable\nsolutions arises when dealing with vast genomic data, like RNA-sequencing.\nKnowledge graphs offer a powerful approach for various tasks in such\nlarge-scale genomic data, such as analysis and inference. In this work,\nvariant-level information extracted from the RNA-sequences of vaccine-na\\\"ive\nCOVID-19 patients have been represented as a unified, large knowledge graph.\nVariant call format (VCF) files containing the variant-level information were\nannotated to include further information for each variant. The data records in\nthe annotated files were then converted to Resource Description Framework (RDF)\ntriples. Each VCF file obtained had an associated CADD scores file that\ncontained the raw and Phred-scaled scores for each variant. An ontology was\ndefined for the VCF and CADD scores files. Using this ontology and the\nextracted information, a large, scalable knowledge graph was created. Available\ngraph storage was then leveraged to query and create datasets for further\ndownstream tasks. We also present a case study using the knowledge graph and\nperform a classification task using graph machine learning. We also draw\ncomparisons between different Graph Neural Networks (GNNs) for the case study.",
            "author": [
                "Shivika Prasanna",
                "Deepthi Rao",
                "Eduardo Simoes",
                "Praveen Rao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04423v1",
                "http://arxiv.org/pdf/2312.04423v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04393v1",
            "title": "PhysHOI: Physics-Based Imitation of Dynamic Human-Object Interaction",
            "updated": "2023-12-07T16:06:31Z",
            "published": "2023-12-07T16:06:31Z",
            "summary": "Humans interact with objects all the time. Enabling a humanoid to learn\nhuman-object interaction (HOI) is a key step for future smart animation and\nintelligent robotics systems. However, recent progress in physics-based HOI\nrequires carefully designed task-specific rewards, making the system unscalable\nand labor-intensive. This work focuses on dynamic HOI imitation: teaching\nhumanoid dynamic interaction skills through imitating kinematic HOI\ndemonstrations. It is quite challenging because of the complexity of the\ninteraction between body parts and objects and the lack of dynamic HOI data. To\nhandle the above issues, we present PhysHOI, the first physics-based whole-body\nHOI imitation approach without task-specific reward designs. Except for the\nkinematic HOI representations of humans and objects, we introduce the contact\ngraph to model the contact relations between body parts and objects explicitly.\nA contact graph reward is also designed, which proved to be critical for\nprecise HOI imitation. Based on the key designs, PhysHOI can imitate diverse\nHOI tasks simply yet effectively without prior knowledge. To make up for the\nlack of dynamic HOI scenarios in this area, we introduce the BallPlay dataset\nthat contains eight whole-body basketball skills. We validate PhysHOI on\ndiverse HOI tasks, including whole-body grasping and basketball skills.",
            "author": [
                "Yinhuai Wang",
                "Jing Lin",
                "Ailing Zeng",
                "Zhengyi Luo",
                "Jian Zhang",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04393v1",
                "http://arxiv.org/pdf/2312.04393v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04387v1",
            "title": "Observations of high definition symmetric quasi-periodic oscillations in\n  the mid-latitude ionosphere with LOFAR",
            "updated": "2023-12-07T15:58:04Z",
            "published": "2023-12-07T15:58:04Z",
            "summary": "We present broadband ionospheric scintillation observations of highly defined\nsymmetric quasi-periodic oscillations (QPO: Maruyama 1991) caused by plasma\nstructures in the midlatitude ionosphere using the LOw Frequency ARray (LOFAR:\nvan Haarlem et al., 2013). Two case studies are shown, one from 15th December\n2016, and one from 30th January 2018, in which well-defined main signal fades\nand secondary diffraction fringing are observed. In particular, the broadband\nobserving capabilities of LOFAR permit us to see considerable frequency\ndependent behaviour in the QPOs which, to our knowledge, is a new result. We\nextract some of the clearest examples of scintillation arcs reported in an\nionospheric context, from delay-Doppler spectral analysis of these two events.\nThese arcs permit the extraction of propagation velocities for the plasma\nstructures causing the QPOs ranging from 50 - 200 ms$^{-1}$, depending on the\nassumed altitude. The spacing between the individual plasma structures ranges\nbetween 5 - 20 km. The periodicities of the main signal fades in each event\nand, in the case of the 2018 data, co-temporal ionosonde data, suggest the\npropagation of the plasma structures causing the QPOs is in the E-region. Each\nof the two events is accurately reproduced using a Gaussian perturbation phase\nscreen model. Individual signal fades and enhancements were modelled using\nsmall variations in total electron content (TEC) amplitudes of order 1 mTECu,\ndemonstrating the sensitivity of LOFAR to very small fluctuations in\nionospheric plasma density. To our knowledge these results are among the most\ndetailed observations and modelling of QPOs in the literature.",
            "author": [
                "Hannah Trigg",
                "Gareth Dorrian",
                "Ben Boyde",
                "Alan Wood",
                "Richard Fallows",
                "Maaijke Mevius"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04387v1",
                "http://arxiv.org/pdf/2312.04387v1"
            ],
            "primary_category": "physics.space-ph",
            "category": [
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04368v1",
            "title": "Deployment of Reference Nodes to Guarantee a LoS Condition for Accurate\n  Indoor Positioning",
            "updated": "2023-12-07T15:38:41Z",
            "published": "2023-12-07T15:38:41Z",
            "summary": "Accurate and precise positioning is required to guarantee the massive\nadoption of a wide range of 5G indoor applications, such as logistics and smart\nmanufacturing. Native support for New Radio (NR) positioning services was\nincluded in 3GPP Rel-16, where angles-of-arrival/departure and time-\n(difference-)of-arrival measurements were specified in uplink and downlink.\nHowever, all these positioning techniques assume Lineof-Sight (LoS)\npropagation, suffering from systematic bias errors when such a condition cannot\nbe guaranteed. To improve the accuracy and precision of indoor positioning\nsystems that rely on proximity, triangulation, or trilateration principles,\nthis paper considers the deployment of reference nodes to ensure LoS to one,\ntwo, or three nodes, respectively. For this purpose, the indoor service area is\nmodeled with a graph whose nodes represent the polygons that partition the\nfloor plan. Then, the graph is partitioned into the minimum number of cliques,\nwhich specify the minimum number of reference nodes and their placement to\nguarantee a LoS condition regardless the user terminal position. The desired\naccuracy for positioning is guaranteed by setting a minimum distance and a\nminimum angle between the reference nodes as two configuration parameters of\nthe derived algorithms.",
            "author": [
                "Mohsen Abedi",
                "Alexis A. Dowhuszko",
                "Risto Wichman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04368v1",
                "http://arxiv.org/pdf/2312.04368v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04367v1",
            "title": "Paraconsistent Existential Graphs Gamma Peirce System",
            "updated": "2023-12-07T15:36:34Z",
            "published": "2023-12-07T15:36:34Z",
            "summary": "In this paper, the paraconsistent propositional logic LG is presented, along\nwith its semantic characterization. It is shown that LG's set of theorems\ncorresponds to the set of valid existential graphs, GET, which turns out to be\nan extension of Peirce's Gamma system, without becoming Zeman's gamma-4 system.\nAll evidence is presented in a complete, rigorous, and detailed manner. This\nresult is generalized by constructing the paraconsistent system of existential\ngraphs GET4, and its semantic-deductive characterization. Finally, Zeman's\nGamma-4, Gamma-4.2, and Gamma-5 existential graph systems are proven to be\nparaconsistent.",
            "author": [
                "Manuel Sierra-Aristizabal"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04367v1",
                "http://arxiv.org/pdf/2312.04367v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04358v1",
            "title": "Balance Correlations, Agentic Zeros, and Networks: The Structure of 192\n  Years of War and Peace",
            "updated": "2023-12-07T15:25:44Z",
            "published": "2023-12-07T15:25:44Z",
            "summary": "Original balance theory (Heider 1944) predicts human relations based on\nperceptions and attitudes between a pair of individuals (P - O) towards an\ninanimate object X. Social network extensions of his theory have replaced this\nX with a third individual. This has led to a plethora of adaptations that have\noften been inconsistent with Heider and with each other. We present a general\nmodel and formal notation for these social network extensions that permit\nsocial scientists to be more explicit about their balance theoretic statements.\nSpecifically, we formulate statements as a comparison of two conditional\nprobabilities of a tie, where the conditionals are defined by the 2-path\nrelation Ego - X - Alter. Given the importance Heider assigns to the role of\nnegative associations, we further identify negative ties as separate from\nnon-ties (neutral or zero-valued ties) and consider a signed graph to be a\nrestricted multigraph composed of three mutually exclusive and exhaustive\nrelations: positive ties, negative ties, and zero-ties. We stress that\nneutrality is the result of a triadic process. Combining these two features\ninto our theoretical frame results in 27 identifiable configurations. Drawing\non the work on Transitivity Correlation models, we propose a set of simple\ndescriptive statistics to measure the extent to which evidence for any\nstipulated balance configuration is present in a network. Finally, we\ndemonstrate how to apply this approach to assess network-level balance in a\nlarge data set consisting of friendly vs hostile relations between countries\nfrom 1816 to 2007. We find strong evidence particularly for one of the four\nclassic Heiderian balance theory predictions, and virtually no evidence in\nsupport of the imbalance predictions. However, we do find stable and surprising\nevidence that `neutral' ties are important in balancing the relations among\nnations.",
            "author": [
                "David Dekker",
                "David Krackhardt",
                "Patrick Doreian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04358v1",
                "http://arxiv.org/pdf/2312.04358v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04357v1",
            "title": "Sliced Max-Flow on Circular Mapper Graphs",
            "updated": "2023-12-07T15:23:38Z",
            "published": "2023-12-07T15:23:38Z",
            "summary": "In this work we develop a novel global invariant of circle-parametrized\nmapper graphs in order to analyse periodic sets, which often arise in materials\nscience applications. This invariant describes a flow in a graph, slicing it\nwith the fibers of the associated map onto the circle.",
            "author": [
                "Matteo Pegoraro",
                "Lisbeth Fajstrup"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04357v1",
                "http://arxiv.org/pdf/2312.04357v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04350v1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language\n  Models",
            "updated": "2023-12-07T15:12:12Z",
            "published": "2023-12-07T15:12:12Z",
            "summary": "The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsight into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.",
            "author": [
                "Zhijing Jin",
                "Yuen Chen",
                "Felix Leeb",
                "Luigi Gresele",
                "Ojasv Kamal",
                "Zhiheng Lyu",
                "Kevin Blin",
                "Fernando Gonzalez Adauto",
                "Max Kleiman-Weiner",
                "Mrinmaya Sachan",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04350v1",
                "http://arxiv.org/pdf/2312.04350v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04343v1",
            "title": "Causality and Explainability for Trustworthy Integrated Pest Management",
            "updated": "2023-12-07T15:05:26Z",
            "published": "2023-12-07T15:05:26Z",
            "summary": "Pesticides serve as a common tool in agricultural pest control but\nsignificantly contribute to the climate crisis. To combat this, Integrated Pest\nManagement (IPM) stands as a climate-smart alternative. Despite its potential,\nIPM faces low adoption rates due to farmers' skepticism about its\neffectiveness. To address this challenge, we introduce an advanced data\nanalysis framework tailored to enhance IPM adoption. Our framework provides i)\nrobust pest population predictions across diverse environments with invariant\nand causal learning, ii) interpretable pest presence predictions using\ntransparent models, iii) actionable advice through counterfactual explanations\nfor in-season IPM interventions, iv) field-specific treatment effect\nestimations, and v) assessments of the effectiveness of our advice using causal\ninference. By incorporating these features, our framework aims to alleviate\nskepticism and encourage wider adoption of IPM practices among farmers.",
            "author": [
                "Ilias Tsoumas",
                "Vasileios Sitokonstantinou",
                "Georgios Giannarakis",
                "Evagelia Lampiri",
                "Christos Athanassiou",
                "Gustau Camps-Valls",
                "Charalampos Kontoes",
                "Ioannis Athanasiadis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04343v1",
                "http://arxiv.org/pdf/2312.04343v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04333v1",
            "title": "Beyond Surface: Probing LLaMA Across Scales and Layers",
            "updated": "2023-12-07T14:50:41Z",
            "published": "2023-12-07T14:50:41Z",
            "summary": "This paper presents an in-depth analysis of Large Language Models (LLMs),\nfocusing on LLaMA, a prominent open-source foundational model in natural\nlanguage processing. Instead of assessing LLaMA through its generative output,\nwe design multiple-choice tasks to probe its intrinsic understanding in\nhigh-order tasks such as reasoning and computation. We examine the model\nhorizontally, comparing different sizes, and vertically, assessing different\nlayers. We unveil several key and uncommon findings based on the designed\nprobing tasks: (1) Horizontally, enlarging model sizes almost could not\nautomatically impart additional knowledge or computational prowess. Instead, it\ncan enhance reasoning abilities, especially in math problem solving, and helps\nreduce hallucinations, but only beyond certain size thresholds; (2) In vertical\nanalysis, the lower layers of LLaMA lack substantial arithmetic and factual\nknowledge, showcasing logical thinking, multilingual and recognitive abilities,\nwith top layers housing most computational power and real-world knowledge.",
            "author": [
                "Nuo Chen",
                "Ning Wu",
                "Shining Liang",
                "Ming Gong",
                "Linjun Shou",
                "Dongmei Zhang",
                "Jia Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04333v1",
                "http://arxiv.org/pdf/2312.04333v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04323v1",
            "title": "Equivariant Scalar Fields for Molecular Docking with Fast Fourier\n  Transforms",
            "updated": "2023-12-07T14:32:32Z",
            "published": "2023-12-07T14:32:32Z",
            "summary": "Molecular docking is critical to structure-based virtual screening, yet the\nthroughput of such workflows is limited by the expensive optimization of\nscoring functions involved in most docking algorithms. We explore how machine\nlearning can accelerate this process by learning a scoring function with a\nfunctional form that allows for more rapid optimization. Specifically, we\ndefine the scoring function to be the cross-correlation of multi-channel ligand\nand protein scalar fields parameterized by equivariant graph neural networks,\nenabling rapid optimization over rigid-body degrees of freedom with fast\nFourier transforms. The runtime of our approach can be amortized at several\nlevels of abstraction, and is particularly favorable for virtual screening\nsettings with a common binding pocket. We benchmark our scoring functions on\ntwo simplified docking-related tasks: decoy pose scoring and rigid conformer\ndocking. Our method attains similar but faster performance on crystal\nstructures compared to the widely-used Vina and Gnina scoring functions, and is\nmore robust on computationally predicted structures. Code is available at\nhttps://github.com/bjing2016/scalar-fields.",
            "author": [
                "Bowen Jing",
                "Tommi Jaakkola",
                "Bonnie Berger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04323v1",
                "http://arxiv.org/pdf/2312.04323v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04316v1",
            "title": "Towards Knowledge-driven Autonomous Driving",
            "updated": "2023-12-07T14:17:17Z",
            "published": "2023-12-07T14:17:17Z",
            "summary": "This paper explores the emerging knowledge-driven autonomous driving\ntechnologies. Our investigation highlights the limitations of current\nautonomous driving systems, in particular their sensitivity to data bias,\ndifficulty in handling long-tail scenarios, and lack of interpretability.\nConversely, knowledge-driven methods with the abilities of cognition,\ngeneralization and life-long learning emerge as a promising way to overcome\nthese challenges. This paper delves into the essence of knowledge-driven\nautonomous driving and examines its core components: dataset \\& benchmark,\nenvironment, and driver agent. By leveraging large language models, world\nmodels, neural rendering, and other advanced artificial intelligence\ntechniques, these components collectively contribute to a more holistic,\nadaptive, and intelligent autonomous driving system. The paper systematically\norganizes and reviews previous research efforts in this area, and provides\ninsights and guidance for future research and practical applications of\nautonomous driving. We will continually share the latest updates on\ncutting-edge developments in knowledge-driven autonomous driving along with the\nrelevant valuable open-source resources at:\n\\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.",
            "author": [
                "Xin Li",
                "Yeqi Bai",
                "Pinlong Cai",
                "Licheng Wen",
                "Daocheng Fu",
                "Bo Zhang",
                "Xuemeng Yang",
                "Xinyu Cai",
                "Tao Ma",
                "Jianfei Guo",
                "Xing Gao",
                "Min Dou",
                "Botian Shi",
                "Yong Liu",
                "Liang He",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04316v1",
                "http://arxiv.org/pdf/2312.04316v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04314v1",
            "title": "GPT4SGG: Synthesizing Scene Graphs from Holistic and Region-specific\n  Narratives",
            "updated": "2023-12-07T14:11:00Z",
            "published": "2023-12-07T14:11:00Z",
            "summary": "Learning scene graphs from natural language descriptions has proven to be a\ncheap and promising scheme for Scene Graph Generation (SGG). However, such\nunstructured caption data and its processing are troubling the learning an\nacurrate and complete scene graph. This dilema can be summarized as three\npoints. First, traditional language parsers often fail to extract meaningful\nrelationship triplets from caption data. Second, grounding unlocalized objects\nin parsed triplets will meet ambiguity in visual-language alignment. Last,\ncaption data typically are sparse and exhibit bias to partial observations of\nimage content. These three issues make it hard for the model to generate\ncomprehensive and accurate scene graphs. To fill this gap, we propose a simple\nyet effective framework, GPT4SGG, to synthesize scene graphs from holistic and\nregion-specific narratives. The framework discards traditional language parser,\nand localize objects before obtaining relationship triplets. To obtain\nrelationship triplets, holistic and dense region-specific narratives are\ngenerated from the image. With such textual representation of image data and a\ntask-specific prompt, an LLM, particularly GPT-4, directly synthesizes a scene\ngraph as \"pseudo labels\". Experimental results showcase GPT4SGG significantly\nimproves the performance of SGG models trained on image-caption data. We\nbelieve this pioneering work can motivate further research into mining the\nvisual reasoning capabilities of LLMs.",
            "author": [
                "Zuyao Chen",
                "Jinlin Wu",
                "Zhen Lei",
                "Zhaoxiang Zhang",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04314v1",
                "http://arxiv.org/pdf/2312.04314v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04312v1",
            "title": "Stochastic-Constrained Stochastic Optimization with Markovian Data",
            "updated": "2023-12-07T14:09:27Z",
            "published": "2023-12-07T14:09:27Z",
            "summary": "This paper considers stochastic-constrained stochastic optimization where the\nstochastic constraint is to satisfy that the expectation of a random function\nis below a certain threshold. In particular, we study the setting where data\nsamples are drawn from a Markov chain and thus are not independent and\nidentically distributed. We generalize the drift-plus-penalty framework, a\nprimal-dual stochastic gradient method developed for the i.i.d. case, to the\nMarkov chain sampling setting. We propose two variants of drift-plus-penalty;\none is for the case when the mixing time of the underlying Markov chain is\nknown while the other is for the case of unknown mixing time. In fact, our\nalgorithms apply to a more general setting of constrained online convex\noptimization where the sequence of constraint functions follows a Markov chain.\nBoth algorithms are adaptive in that the first works without knowledge of the\ntime horizon while the second uses AdaGrad-style algorithm parameters, which is\nof independent interest. We demonstrate the effectiveness of our proposed\nmethods through numerical experiments on classification with fairness\nconstraints.",
            "author": [
                "Yeongjong Kim",
                "Dabeen Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04312v1",
                "http://arxiv.org/pdf/2312.04312v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04307v1",
            "title": "A Structural-Clustering Based Active Learning for Graph Neural Networks",
            "updated": "2023-12-07T14:04:38Z",
            "published": "2023-12-07T14:04:38Z",
            "summary": "In active learning for graph-structured data, Graph Neural Networks (GNNs)\nhave shown effectiveness. However, a common challenge in these applications is\nthe underutilization of crucial structural information. To address this\nproblem, we propose the Structural-Clustering PageRank method for improved\nActive learning (SPA) specifically designed for graph-structured data. SPA\nintegrates community detection using the SCAN algorithm with the PageRank\nscoring method for efficient and informative sample selection. SPA prioritizes\nnodes that are not only informative but also central in structure. Through\nextensive experiments, SPA demonstrates higher accuracy and macro-F1 score over\nexisting methods across different annotation budgets and achieves significant\nreductions in query time. In addition, the proposed method only adds two\nhyperparameters, $\\epsilon$ and $\\mu$ in the algorithm to finely tune the\nbalance between structural learning and node selection. This simplicity is a\nkey advantage in active learning scenarios, where extensive hyperparameter\ntuning is often impractical.",
            "author": [
                "Ricky Maulana Fajri",
                "Yulong Pei",
                "Lu Yin",
                "Mykola Pechenizkiy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04307v1",
                "http://arxiv.org/pdf/2312.04307v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04291v1",
            "title": "Simulating the Air Quality Impact of Prescribed Fires Using a Graph\n  Neural Network-Based PM$_{2.5}$ Emissions Forecasting System",
            "updated": "2023-12-07T13:18:36Z",
            "published": "2023-12-07T13:18:36Z",
            "summary": "The increasing size and severity of wildfires across western North America\nhave generated dangerous levels of PM$_{2.5}$ pollution in recent years. In a\nwarming climate, expanding the use of prescribed fires is widely considered to\nbe the most robust fire mitigation strategy. However, reliably forecasting the\npotential air quality impact from these prescribed fires, a critical ingredient\nin determining the fires' location and time, at hourly to daily time scales\nremains a challenging problem. This paper proposes a novel integration of\nprescribed fire simulation with a spatio-temporal graph neural network-based\nPM$_{2.5}$ forecasting model. The experiments in this work focus on determining\nthe optimal time for implementing prescribed fires in California as well as\nquantifying the potential air quality trade-offs involved in conducting more\nprescribed fires outside the fire season.",
            "author": [
                "Kyleen Liao",
                "Jatan Buch",
                "Kara Lamb",
                "Pierre Gentine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04291v1",
                "http://arxiv.org/pdf/2312.04291v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04281v1",
            "title": "Factor-Assisted Federated Learning for Personalized Optimization with\n  Heterogeneous Data",
            "updated": "2023-12-07T13:05:47Z",
            "published": "2023-12-07T13:05:47Z",
            "summary": "Federated learning is an emerging distributed machine learning framework\naiming at protecting data privacy. Data heterogeneity is one of the core\nchallenges in federated learning, which could severely degrade the convergence\nrate and prediction performance of deep neural networks. To address this issue,\nwe develop a novel personalized federated learning framework for heterogeneous\ndata, which we refer to as FedSplit. This modeling framework is motivated by\nthe finding that, data in different clients contain both common knowledge and\npersonalized knowledge. Then the hidden elements in each neural layer can be\nsplit into the shared and personalized groups. With this decomposition, a novel\nobjective function is established and optimized. We demonstrate FedSplit\nenjoyers a faster convergence speed than the standard federated learning method\nboth theoretically and empirically. The generalization bound of the FedSplit\nmethod is also studied. To practically implement the proposed method on real\ndatasets, factor analysis is introduced to facilitate the decoupling of hidden\nelements. This leads to a practically implemented model for FedSplit and we\nfurther refer to as FedFac. We demonstrated by simulation studies that, using\nfactor analysis can well recover the underlying shared/personalized\ndecomposition. The superior prediction performance of FedFac is further\nverified empirically by comparison with various state-of-the-art federated\nlearning methods on several real datasets.",
            "author": [
                "Feifei Wang",
                "Huiyun Tang",
                "Yang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04281v1",
                "http://arxiv.org/pdf/2312.04281v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04257v1",
            "title": "Proxima: Near-storage Acceleration for Graph-based Approximate Nearest\n  Neighbor Search in 3D NAND",
            "updated": "2023-12-07T12:32:18Z",
            "published": "2023-12-07T12:32:18Z",
            "summary": "Approximate nearest neighbor search (ANNS) plays an indispensable role in a\nwide variety of applications, including recommendation systems, information\nretrieval, and semantic search. Among the cutting-edge ANNS algorithms,\ngraph-based approaches provide superior accuracy and scalability on massive\ndatasets. However, the best-performing graph-based ANN search solutions incur\ntens of hundreds of memory footprints as well as costly distance computation,\nthus hindering their efficient deployment at scale. The 3D NAND flash is\nemerging as a promising device for data-intensive applications due to its high\ndensity and nonvolatility. In this work, we present the near-storage processing\n(NSP)-based ANNS solution Proxima, to accelerate graph-based ANNS with\nalgorithm-hardware co-design in 3D NAND flash. Proxima significantly reduces\nthe complexity of graph search by leveraging the distance approximation and\nearly termination. On top of the algorithmic enhancement, we implement Proxima\nsearch algorithm in 3D NAND flash using the heterogeneous integration\ntechnique. To maximize 3D NAND's bandwidth utilization, we present customized\ndataflow and optimized data allocation scheme. Our evaluation results show\nthat: compared to graph ANNS on CPU and GPU, Proxima achieves a magnitude\nimprovement in throughput or energy efficiency. Proxima yields 7x to 13x\nspeedup over existing ASIC designs. Furthermore, Proxima achieves a good\nbalance between accuracy, efficiency and storage density compared to previous\nNSP-based accelerators.",
            "author": [
                "Weihong Xu",
                "Junwei Chen",
                "Po-Kai Hsu",
                "Jaeyoung Kang",
                "Minxuan Zhou",
                "Sumukh Pinge",
                "Shimeng Yu",
                "Tajana Rosing"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04257v1",
                "http://arxiv.org/pdf/2312.04257v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04248v1",
            "title": "TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes",
            "updated": "2023-12-07T12:10:05Z",
            "published": "2023-12-07T12:10:05Z",
            "summary": "Recent progress in the text-driven 3D stylization of a single object has been\nconsiderably promoted by CLIP-based methods. However, the stylization of\nmulti-object 3D scenes is still impeded in that the image-text pairs used for\npre-training CLIP mostly consist of an object. Meanwhile, the local details of\nmultiple objects may be susceptible to omission due to the existing supervision\nmanner primarily relying on coarse-grained contrast of image-text pairs. To\novercome these challenges, we present a novel framework, dubbed TeMO, to parse\nmulti-object 3D scenes and edit their styles under the contrast supervision at\nmultiple levels. We first propose a Decoupled Graph Attention (DGA) module to\ndistinguishably reinforce the features of 3D surface points. Particularly, a\ncross-modal graph is constructed to align the object points accurately and noun\nphrases decoupled from the 3D mesh and textual description. Then, we develop a\nCross-Grained Contrast (CGC) supervision system, where a fine-grained loss\nbetween the words in the textual description and the randomly rendered images\nare constructed to complement the coarse-grained loss. Extensive experiments\nshow that our method can synthesize high-quality stylized content and\noutperform the existing methods over a wide range of multi-object 3D meshes.\nOur code and results will be made publicly available",
            "author": [
                "Xuying Zhang",
                "Bo-Wen Yin",
                "Yuming Chen",
                "Zheng Lin",
                "Yunheng Li",
                "Qibin Hou",
                "Ming-Ming Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04248v1",
                "http://arxiv.org/pdf/2312.04248v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04245v1",
            "title": "Mastering Complex Coordination through Attention-based Dynamic Graph",
            "updated": "2023-12-07T12:02:14Z",
            "published": "2023-12-07T12:02:14Z",
            "summary": "The coordination between agents in multi-agent systems has become a popular\ntopic in many fields. To catch the inner relationship between agents, the graph\nstructure is combined with existing methods and improves the results. But in\nlarge-scale tasks with numerous agents, an overly complex graph would lead to a\nboost in computational cost and a decline in performance. Here we present\nDAGMIX, a novel graph-based value factorization method. Instead of a complete\ngraph, DAGMIX generates a dynamic graph at each time step during training, on\nwhich it realizes a more interpretable and effective combining process through\nthe attention mechanism. Experiments show that DAGMIX significantly outperforms\nprevious SOTA methods in large-scale scenarios, as well as achieving promising\nresults on other tasks.",
            "author": [
                "Guangchong Zhou",
                "Zhiwei Xu",
                "Zeren Zhang",
                "Guoliang Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04245v1",
                "http://arxiv.org/pdf/2312.04245v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04235v1",
            "title": "Distances and shortest paths on graphs of bounded highway dimension:\n  simple, fast, dynamic",
            "updated": "2023-12-07T11:41:07Z",
            "published": "2023-12-07T11:41:07Z",
            "summary": "Dijkstra's algorithm is the standard method for computing shortest paths on\narbitrary graphs. However, it is slow for large graphs, taking at least linear\ntime. It has been long known that for real world road networks, creating a\nhierarchy of well-chosen shortcuts allows fast distance and path computation,\nwith exact distance queries seemingly being answered in logarithmic time.\nHowever, these methods were but heuristics until the work of Abraham et\nal.~[JACM 2016], where they defined a graph parameter called highway dimension\nwhich is constant for real-world road networks, and showed that in graphs of\nconstant highway dimension, a shortcut hierarchy exists that guarantees\nshortest distance computation takes $O(\\log (U+V))$ time and $O(V \\log (U+V))$\nspace, where $U$ is the ratio of the smallest to largest edge, and $V$ is the\nnumber of vertices. The problem is that they were unable to efficiently compute\nthe hierarchy of shortcuts. Here we present a simple and efficient algorithm to\ncompute the needed hierarchy of shortcuts in time and space $O(V \\log (U+V))$,\nas well as supporting updates in time $O( \\log (U+V))$.",
            "author": [
                "S\u00e9bastien Collette",
                "John Iacono"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04235v1",
                "http://arxiv.org/pdf/2312.04235v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04234v1",
            "title": "Graph Convolutions Enrich the Self-Attention in Transformers!",
            "updated": "2023-12-07T11:40:32Z",
            "published": "2023-12-07T11:40:32Z",
            "summary": "Transformers, renowned for their self-attention mechanism, have achieved\nstate-of-the-art performance across various tasks in natural language\nprocessing, computer vision, time-series modeling, etc. However, one of the\nchallenges with deep Transformer models is the oversmoothing problem, where\nrepresentations across layers converge to indistinguishable values, leading to\nsignificant performance degradation. We interpret the original self-attention\nas a simple graph filter and redesign it from a graph signal processing (GSP)\nperspective. We propose graph-filter-based self-attention (GFSA) to learn a\ngeneral yet effective one, whose complexity, however, is slightly larger than\nthat of the original self-attention mechanism. We demonstrate that GFSA\nimproves the performance of Transformers in various fields, including computer\nvision, natural language processing, graph pattern classification, speech\nrecognition, and code classification.",
            "author": [
                "Jeongwhan Choi",
                "Hyowon Wi",
                "Jayoung Kim",
                "Yehjin Shin",
                "Kookjin Lee",
                "Nathaniel Trask",
                "Noseong Park"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04234v1",
                "http://arxiv.org/pdf/2312.04234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04209v1",
            "title": "Constrained Hierarchical Clustering via Graph Coarsening and Optimal\n  Cuts",
            "updated": "2023-12-07T10:52:06Z",
            "published": "2023-12-07T10:52:06Z",
            "summary": "Motivated by extracting and summarizing relevant information in short\nsentence settings, such as satisfaction questionnaires, hotel reviews, and\nX/Twitter, we study the problem of clustering words in a hierarchical fashion.\nIn particular, we focus on the problem of clustering with horizontal and\nvertical structural constraints. Horizontal constraints are typically\ncannot-link and must-link among words, while vertical constraints are\nprecedence constraints among cluster levels. We overcome state-of-the-art\nbottlenecks by formulating the problem in two steps: first, as a\nsoft-constrained regularized least-squares which guides the result of a\nsequential graph coarsening algorithm towards the horizontal feasible set.\nThen, flat clusters are extracted from the resulting hierarchical tree by\ncomputing optimal cut heights based on the available constraints. We show that\nthe resulting approach compares very well with respect to existing algorithms\nand is computationally light.",
            "author": [
                "Eliabelle Mauduit",
                "Andrea Simonetto"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04209v1",
                "http://arxiv.org/pdf/2312.04209v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04193v1",
            "title": "Language Model Knowledge Distillation for Efficient Question Answering\n  in Spanish",
            "updated": "2023-12-07T10:21:22Z",
            "published": "2023-12-07T10:21:22Z",
            "summary": "Recent advances in the development of pre-trained Spanish language models has\nled to significant progress in many Natural Language Processing (NLP) tasks,\nsuch as question answering. However, the lack of efficient models imposes a\nbarrier for the adoption of such models in resource-constrained environments.\nTherefore, smaller distilled models for the Spanish language could be proven to\nbe highly scalable and facilitate their further adoption on a variety of tasks\nand scenarios. In this work, we take one step in this direction by developing\nSpanishTinyRoBERTa, a compressed language model based on RoBERTa for efficient\nquestion answering in Spanish. To achieve this, we employ knowledge\ndistillation from a large model onto a lighter model that allows for a wider\nimplementation, even in areas with limited computational resources, whilst\nattaining negligible performance sacrifice. Our experiments show that the dense\ndistilled model can still preserve the performance of its larger counterpart,\nwhile significantly increasing inference speedup. This work serves as a\nstarting point for further research and investigation of model compression\nefforts for Spanish language models across various NLP tasks.",
            "author": [
                "Adri\u00e1n Bazaga",
                "Pietro Li\u00f2",
                "Gos Micklem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04193v1",
                "http://arxiv.org/pdf/2312.04193v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04191v1",
            "title": "Subsets of groups with context-free preimages",
            "updated": "2023-12-07T10:18:46Z",
            "published": "2023-12-07T10:18:46Z",
            "summary": "We study subsets $E$ of finitely generated groups where the set of all words\nover a given finite generating set that lie in $E$ forms a context-free\nlanguage. We call these sets recognisably context-free. They are invariant of\nthe choice of generating set and a theorem of Muller and Schupp fully\nclassifies when the set $\\{1\\}$ can be recognisably context-free. We extend\nMuller and Schupp's result to show that a group $G$ admits a finite\nrecognisably context-free subset if and only if $G$ is virtually free. We show\nthat every conjugacy class of a group $G$ is recognisably context-free if and\nonly if $G$ is virtually free. We conclude by showing that a coset is\nrecognisably context-free if and only if the Schreier coset graph of the\ncorresponding subgroup is quasi-isometric to a tree.",
            "author": [
                "Alex Levine"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04191v1",
                "http://arxiv.org/pdf/2312.04191v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "cs.FL",
                "03D05, 20F10, 20F65, 68Q45"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04190v1",
            "title": "Receding Horizon Re-ordering of Multi-Agent Execution Schedules",
            "updated": "2023-12-07T10:17:05Z",
            "published": "2023-12-07T10:17:05Z",
            "summary": "The trajectory planning for a fleet of Automated Guided Vehicles (AGVs) on a\nroadmap is commonly referred to as the Multi-Agent Path Finding (MAPF) problem,\nthe solution to which dictates each AGV's spatial and temporal location until\nit reaches it's goal without collision. When executing MAPF plans in dynamic\nworkspaces, AGVs can be frequently delayed, e.g., due to encounters with humans\nor third-party vehicles. If the remainder of the AGVs keeps following their\nindividual plans, synchrony of the fleet is lost and some AGVs may pass through\nroadmap intersections in a different order than originally planned. Although\nthis could reduce the cumulative route completion time of the AGVs, generally,\na change in the original ordering can cause conflicts such as deadlocks. In\npractice, synchrony is therefore often enforced by using a MAPF execution\npolicy employing, e.g., an Action Dependency Graph (ADG) to maintain ordering.\nTo safely re-order without introducing deadlocks, we present the concept of the\nSwitchable Action Dependency Graph (SADG). Using the SADG, we formulate a\ncomparatively low-dimensional Mixed-Integer Linear Program (MILP) that\nrepeatedly re-orders AGVs in a recursively feasible manner, thus maintaining\ndeadlock-free guarantees, while dynamically minimizing the cumulative route\ncompletion time of all AGVs. Various simulations validate the efficiency of our\napproach when compared to the original ADG method as well as robust MAPF\nsolution approaches.",
            "author": [
                "Alexander Berndt",
                "Niels van Duijkeren",
                "Luigi Palmieri",
                "Alexander Kleiner",
                "Tam\u00e1s Keviczky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04190v1",
                "http://arxiv.org/pdf/2312.04190v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04181v1",
            "title": "Cell segmentation of in situ transcriptomics data using signed graph\n  partitioning",
            "updated": "2023-12-07T10:08:07Z",
            "published": "2023-12-07T10:08:07Z",
            "summary": "The locations of different mRNA molecules can be revealed by multiplexed in\nsitu RNA detection. By assigning detected mRNA molecules to individual cells,\nit is possible to identify many different cell types in parallel. This in turn\nenables investigation of the spatial cellular architecture in tissue, which is\ncrucial for furthering our understanding of biological processes and diseases.\nHowever, cell typing typically depends on the segmentation of cell nuclei,\nwhich is often done based on images of a DNA stain, such as DAPI. Limiting cell\ndefinition to a nuclear stain makes it fundamentally difficult to determine\naccurate cell borders, and thereby also difficult to assign mRNA molecules to\nthe correct cell. As such, we have developed a computational tool that segments\ncells solely based on the local composition of mRNA molecules. First, a small\nneural network is trained to compute attractive and repulsive edges between\npairs of mRNA molecules. The signed graph is then partitioned by a mutex\nwatershed into components corresponding to different cells. We evaluated our\nmethod on two publicly available datasets and compared it against the current\nstate-of-the-art and older baselines. We conclude that combining neural\nnetworks with combinatorial optimization is a promising approach for cell\nsegmentation of in situ transcriptomics data.",
            "author": [
                "Axel Andersson",
                "Andrea Behanova",
                "Carolina W\u00e4hlby",
                "Filip Malmberg"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-42795-4_13",
                "http://arxiv.org/abs/2312.04181v1",
                "http://arxiv.org/pdf/2312.04181v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04173v1",
            "title": "Contract Wallet Using Emails",
            "updated": "2023-12-07T09:48:25Z",
            "published": "2023-12-07T09:48:25Z",
            "summary": "We proposed a new construction for contract wallets, smart contract\napplications that allow users to control their crypto assets. Users can\nmanipulate their crypto assets by simply sending emails with no need to manage\nkeys. These emails are verified using zero-knowledge proof (ZKP) along with\ntheir attached digital signatures that the sender domain server (SDS) generates\naccording to DomainKeys Identified Mail. Unless the SDS forges the emails, the\ncrypto assets remain secure in the proposed system. Moreover, the existing SDSs\ncan be used as is by outsourcing additional work to a third party that is not\nnecessarily trusted. The system supports various functions to manipulate crypto\nassets. We produced a tool for variable-regex mapping (VRM) that enables\ndevelopers to build a new function without ZKP skills. For example, using the\ntool, we built a demo application where users can exchange crypto assets via\nUniswap only with emails. The published version of this paper is available at\nhttps://doi.org/10.1109/ICBC56567.2023.10174932.",
            "author": [
                "Sora Suegami",
                "Kyohei Shibano"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICBC56567.2023.10174932",
                "http://arxiv.org/abs/2312.04173v1",
                "http://arxiv.org/pdf/2312.04173v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04168v1",
            "title": "Augmentation-Free Dense Contrastive Knowledge Distillation for Efficient\n  Semantic Segmentation",
            "updated": "2023-12-07T09:37:28Z",
            "published": "2023-12-07T09:37:28Z",
            "summary": "In recent years, knowledge distillation methods based on contrastive learning\nhave achieved promising results on image classification and object detection\ntasks. However, in this line of research, we note that less attention is paid\nto semantic segmentation. Existing methods heavily rely on data augmentation\nand memory buffer, which entail high computational resource demands when\napplying them to handle semantic segmentation that requires to preserve\nhigh-resolution feature maps for making dense pixel-wise predictions. In order\nto address this problem, we present Augmentation-free Dense Contrastive\nKnowledge Distillation (Af-DCD), a new contrastive distillation learning\nparadigm to train compact and accurate deep neural networks for semantic\nsegmentation applications. Af-DCD leverages a masked feature mimicking\nstrategy, and formulates a novel contrastive learning loss via taking advantage\nof tactful feature partitions across both channel and spatial dimensions,\nallowing to effectively transfer dense and structured local knowledge learnt by\nthe teacher model to a target student model while maintaining training\nefficiency. Extensive experiments on five mainstream benchmarks with various\nteacher-student network pairs demonstrate the effectiveness of our approach.\nFor instance, the DeepLabV3-Res18|DeepLabV3-MBV2 model trained by Af-DCD\nreaches 77.03%|76.38% mIOU on Cityscapes dataset when choosing DeepLabV3-Res101\nas the teacher, setting new performance records. Besides that, Af-DCD achieves\nan absolute mIOU improvement of 3.26%|3.04%|2.75%|2.30%|1.42% compared with\nindividually trained counterpart on Cityscapes|Pascal\nVOC|Camvid|ADE20K|COCO-Stuff-164K. Code is available at\nhttps://github.com/OSVAI/Af-DCD",
            "author": [
                "Jiawei Fan",
                "Chao Li",
                "Xiaolong Liu",
                "Meina Song",
                "Anbang Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04168v1",
                "http://arxiv.org/pdf/2312.04168v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04166v1",
            "title": "Improving Communication Efficiency of Federated Distillation via\n  Accumulating Local Updates",
            "updated": "2023-12-07T09:36:18Z",
            "published": "2023-12-07T09:36:18Z",
            "summary": "As an emerging federated learning paradigm, federated distillation enables\ncommunication-efficient model training by transmitting only small-scale\nknowledge during the learning process. To further improve the communication\nefficiency of federated distillation, we propose a novel technique, ALU, which\naccumulates multiple rounds of local updates before transferring the knowledge\nto the central server. ALU drastically decreases the frequency of communication\nin federated distillation, thereby significantly reducing the communication\noverhead during the training process. Empirical experiments demonstrate the\nsubstantial effect of ALU in improving the communication efficiency of\nfederated distillation.",
            "author": [
                "Zhiyuan Wu",
                "Sheng Sun",
                "Yuwei Wang",
                "Min Liu",
                "Tian Wen",
                "Wen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04166v1",
                "http://arxiv.org/pdf/2312.04166v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04162v1",
            "title": "Torsion-free abelian groups are faithfully Borel complete and pure\n  embeddability is a complete analytic quasi-order",
            "updated": "2023-12-07T09:24:33Z",
            "published": "2023-12-07T09:24:33Z",
            "summary": "In [9] we proved that the space of countable torsion-free abelian groups is\nBorel complete. In this paper we show that our construction from [9] satisfies\nseveral additional properties of interest. We deduce from this that countable\ntorsion-free abelian groups are faithfully Borel complete, in fact, more\nstrongly, we can $\\mathfrak{L}_{\\omega_1, \\omega}$-interpret countable graphs\nin them. Secondly, we show that the relation of pure embeddability (equiv.,\nelementary embeddability) among countable models of\n$\\mathrm{Th}(\\mathbb{Z}^{(\\omega)})$ is a complete analytic quasi-order.",
            "author": [
                "Gianluca Paolini",
                "Saharon Shelah"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04162v1",
                "http://arxiv.org/pdf/2312.04162v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "03E15, 20K20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04141v1",
            "title": "Distributed Approximate Computing with Constant Locality",
            "updated": "2023-12-07T08:56:35Z",
            "published": "2023-12-07T08:56:35Z",
            "summary": "We consider a distributed coding for computing problem with constant decoding\nlocality, i.e. with a vanishing error probability, any single sample of the\nfunction can be approximately recovered by probing only constant number of\ncompressed bits. We establish an achievable rate region by designing an\nefficient coding scheme. The scheme reduces the required rate by introducing\nauxiliary random variables and supports local decoding at the same time. Then\nwe show the rate region is optimal under mild regularity conditions on source\ndistributions. A coding for computing problem with side information is\nanalogously studied. These results indicate that more rate has to be taken in\norder to achieve lower coding complexity in distributed computing settings.\nMoreover, useful graph characterizations are developed to simplify the\ncomputation of the achievable rate region.",
            "author": [
                "Deheng Yuan",
                "Tao Guo",
                "Zhongyi Huang",
                "Shi Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04141v1",
                "http://arxiv.org/pdf/2312.04141v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04126v1",
            "title": "An Improved Scheduling with Advantage Actor-Critic for Storm Workloads",
            "updated": "2023-12-07T08:29:10Z",
            "published": "2023-12-07T08:29:10Z",
            "summary": "Various resources as the essential elements of data centers, and the\ncompletion time is vital to users. In terms of the persistence, the periodicity\nand the spatial-temporal dependence of stream workload, a new Storm scheduler\nwith Advantage Actor-Critic is proposed to improve resource utilization for\nminimizing the completion time. A new weighted embedding with a Graph Neural\nNetwork is designed to depend on the features of a job comprehensively, which\nincludes the dependence, the types and the positions of tasks in a job. An\nimproved Advantage Actor-Critic integrating task chosen and executor assignment\nis proposed to schedule tasks to executors in order to better resource\nutilization. Then the status of tasks and executors are updated for the next\nscheduling. Compared to existing methods, experimental results show that the\nproposed Storm scheduler improves resource utilization. The completion time is\nreduced by almost 17\\% on the TPC-H data set and reduced by almost 25\\% on the\nAlibaba data set.",
            "author": [
                "Gaoqiang Dong",
                "Jia Wang",
                "Mingjing Wang",
                "Tingting Su"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04126v1",
                "http://arxiv.org/pdf/2312.04126v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04123v1",
            "title": "Approximating the Graph Edit Distance with Compact Neighborhood\n  Representations",
            "updated": "2023-12-07T08:25:00Z",
            "published": "2023-12-07T08:25:00Z",
            "summary": "The graph edit distance is used for comparing graphs in various domains. Due\nto its high computational complexity it is primarily approximated. Widely-used\nheuristics search for an optimal assignment of vertices based on the distance\nbetween local substructures. While faster ones only consider vertices and their\nincident edges, leading to poor accuracy, other approaches require\ncomputationally intense exact distance computations between subgraphs. Our new\nmethod abstracts local substructures to neighborhood trees and compares them\nusing efficient tree matching techniques. This results in a ground distance for\nmapping vertices that yields high quality approximations of the graph edit\ndistance. By limiting the maximum tree height, our method supports steering\nbetween more accurate results and faster execution. We thoroughly analyze the\nrunning time of the tree matching method and propose several techniques to\naccelerate computation in practice. We use compressed tree representations,\nrecognize redundancies by tree canonization and exploit them via caching.\nExperimentally we show that our method provides a significantly improved\ntrade-off between running time and approximation quality compared to existing\nstate-of-the-art approaches.",
            "author": [
                "Franka Bause",
                "Christian Permann",
                "Nils M. Kriege"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04123v1",
                "http://arxiv.org/pdf/2312.04123v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04111v1",
            "title": "Breaking the Entanglement of Homophily and Heterophily in\n  Semi-supervised Node Classification",
            "updated": "2023-12-07T07:54:11Z",
            "published": "2023-12-07T07:54:11Z",
            "summary": "Recently, graph neural networks (GNNs) have shown prominent performance in\nsemi-supervised node classification by leveraging knowledge from the graph\ndatabase. However, most existing GNNs follow the homophily assumption, where\nconnected nodes are more likely to exhibit similar feature distributions and\nthe same labels, and such an assumption has proven to be vulnerable in a\ngrowing number of practical applications. As a supplement, heterophily reflects\ndissimilarity in connected nodes, which has gained significant attention in\ngraph learning. To this end, data engineers aim to develop a powerful GNN model\nthat can ensure performance under both homophily and heterophily. Despite\nnumerous attempts, most existing GNNs struggle to achieve optimal node\nrepresentations due to the constraints of undirected graphs. The neglect of\ndirected edges results in sub-optimal graph representations, thereby hindering\nthe capacity of GNNs. To address this issue, we introduce AMUD, which\nquantifies the relationship between node profiles and topology from a\nstatistical perspective, offering valuable insights for \\underline{A}daptively\n\\underline{M}odeling the natural directed graphs as the \\underline{U}ndirected\nor \\underline{D}irected graph to maximize the benefits from subsequent graph\nlearning. Furthermore, we propose \\underline{A}daptive \\underline{D}irected\n\\underline{P}attern \\underline{A}ggregation (ADPA) as a new directed graph\nlearning paradigm for AMUD. Empirical studies have demonstrated that AMUD\nguides efficient graph learning. Meanwhile, extensive experiments on 14\nbenchmark datasets substantiate the impressive performance of ADPA,\noutperforming baselines by significant margins of 3.96\\%.",
            "author": [
                "Henan Sun",
                "Xunkai Li",
                "Zhengyu Wu",
                "Daohan Su",
                "Rong-Hua Li",
                "Guoren Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04111v1",
                "http://arxiv.org/pdf/2312.04111v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04107v1",
            "title": "Dynamic Quantum Group Key Agreement via Tree Key Graphs",
            "updated": "2023-12-07T07:45:59Z",
            "published": "2023-12-07T07:45:59Z",
            "summary": "Quantum key distribution (QKD) protocols are essential to guarantee\ninformation-theoretic security in quantum communication. Although there was\nsome previous work on quantum group key distribution, they still face many\nchallenges under a ``\\textit{dynamic}'' group communication scenario. In\nparticular, when the group keys need to be updated in real-time for each user\njoining or leaving to ensure secure communication properties, i.e., forward\nconfidentiality and backward confidentiality. However, current protocols\nrequire a large amount of quantum resources to update the group keys, and this\nmakes them impractical for handling large and dynamic communication groups. In\nthis paper, we apply the notion of ``{\\em tree key graph}'' to the quantum key\nagreement and propose two dynamic Quantum Group Key Agreement (QGKA) protocols\nfor a join or leave request in group communications. In addition, we analyze\nthe quantum resource consumption of our proposed protocols. The number of\nqubits required per join or leave only increases logarithmically with the group\nsize. As a result, our proposed protocols are more practical and scalable for\nlarge and dynamic quantum group communications.",
            "author": [
                "Qiang Zhao",
                "Zhuohua Li",
                "John C. S. Lui"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04107v1",
                "http://arxiv.org/pdf/2312.04107v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04095v1",
            "title": "Learn to Unlearn for Deep Neural Networks: Minimizing Unlearning\n  Interference with Gradient Projection",
            "updated": "2023-12-07T07:17:24Z",
            "published": "2023-12-07T07:17:24Z",
            "summary": "Recent data-privacy laws have sparked interest in machine unlearning, which\ninvolves removing the effect of specific training samples from a learnt model\nas if they were never present in the original training dataset. The challenge\nof machine unlearning is to discard information about the ``forget'' data in\nthe learnt model without altering the knowledge about the remaining dataset and\nto do so more efficiently than the naive retraining approach. To achieve this,\nwe adopt a projected-gradient based learning method, named as\nProjected-Gradient Unlearning (PGU), in which the model takes steps in the\northogonal direction to the gradient subspaces deemed unimportant for the\nretaining dataset, so as to its knowledge is preserved. By utilizing Stochastic\nGradient Descent (SGD) to update the model weights, our method can efficiently\nscale to any model and dataset size. We provide empirically evidence to\ndemonstrate that our unlearning method can produce models that behave similar\nto models retrained from scratch across various metrics even when the training\ndataset is no longer accessible. Our code is available at\nhttps://github.com/hnanhtuan/projected_gradient_unlearning.",
            "author": [
                "Tuan Hoang",
                "Santu Rana",
                "Sunil Gupta",
                "Svetha Venkatesh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04095v1",
                "http://arxiv.org/pdf/2312.04095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04092v1",
            "title": "Data stewardship: case studies from North-American, Dutch, and Finnish\n  universities",
            "updated": "2023-12-07T07:06:16Z",
            "published": "2023-12-07T07:06:16Z",
            "summary": "Purpose - As national legislation, federated national services, institutional\npolicies and institutional research service organizations may differ, data\nstewardship transpires differently in higher education institutions across the\nworld. This work seeks to elaborate the picture of different data stewardship\nprograms running in different institutional arrangements and research\nenvironments. Design/methodology/approach - Drawing from autoethnography and\ncase study methods, this study described three distinct data stewardship\nprograms from Purdue University (United States), Delft Technical University\n(Netherlands) and Aalto University (Finland). In addition, this work\ninvestigated the institutional arrangements and national research environments\nof the programs. The focus was on initiatives led by academic libraries or\nsimilar services. Findings - This work demonstrates that data stewardship may\nbe understood differently within different national and institutional contexts.\nThe data stewardship programs differed in terms of roles, organization and\nfunding structures. Moreover, the mesh of policies and legislation,\norganizational structures, and national infrastructures differed. Originality -\nThis work broadens the current literature on data stewardship by not only\nproviding detailed descriptions of three distinct data stewardship programs,\nbut also highlighting how research environments may affect their organization.\nWe argue that the knowledge of institutional and national arrangements is a\ntransversal requirement of data stewardship. Research limitations/implications\n- The data stewardship programs and their contexts develop, and the\ndescriptions presented in this work should be considered as snapshots. Although\nindividual researchers and research groups may undertake data stewardship\nactivities, this study only investigated formalized services.",
            "author": [
                "Antti M. Rousi",
                "Reid I. Boehm",
                "Yan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04092v1",
                "http://arxiv.org/pdf/2312.04092v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04088v1",
            "title": "Efficient Maximum Fair Clique Search over Large Networks",
            "updated": "2023-12-07T06:56:26Z",
            "published": "2023-12-07T06:56:26Z",
            "summary": "Mining cohesive subgraphs in attributed graphs is an essential problem in the\ndomain of graph data analysis. The integration of fairness considerations\nsignificantly fuels interest in models and algorithms for mining fairness-aware\ncohesive subgraphs. Notably, the relative fair clique emerges as a robust\nmodel, ensuring not only comprehensive attribute coverage but also greater\nflexibility in distributing attribute vertices. Motivated by the strength of\nthis model, we for the first time pioneer an investigation into the\nidentification of the maximum relative fair clique in large-scale graphs. We\nintroduce a novel concept of colorful support, which serves as the foundation\nfor two innovative graph reduction techniques. These techniques effectively\nnarrow the graph's size by iteratively removing edges that do not belong to\nrelative fair cliques. Furthermore, a series of upper bounds of the maximum\nrelative fair clique size is proposed by incorporating consideration of vertex\nattributes and colors. The pruning techniques derived from these upper bounds\ncan significantly trim unnecessary search space during the branch-and-bound\nprocedure. Adding to this, we present a heuristic algorithm with a linear time\ncomplexity, employing both a degree-based greedy strategy and a colored\ndegree-based greedy strategy to identify a larger relative fair clique. This\nheuristic algorithm can serve a dual purpose by aiding in branch pruning,\nthereby enhancing overall search efficiency. Extensive experiments conducted on\nsix real-life datasets demonstrate the efficiency, scalability, and\neffectiveness of our algorithms.",
            "author": [
                "Qi Zhang",
                "Rong-Hua Li",
                "Zifan Zheng",
                "Hongchao Qin",
                "Ye Yuan",
                "Guoren Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04088v1",
                "http://arxiv.org/pdf/2312.04088v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04083v1",
            "title": "On the adaptation of in-context learners for system identification",
            "updated": "2023-12-07T06:51:55Z",
            "published": "2023-12-07T06:51:55Z",
            "summary": "In-context system identification aims at constructing meta-models to describe\nclasses of systems, differently from traditional approaches that model single\nsystems. This paradigm facilitates the leveraging of knowledge acquired from\nobserving the behaviour of different, yet related dynamics. This paper\ndiscusses the role of meta-model adaptation. Through numerical examples, we\ndemonstrate how meta-model adaptation can enhance predictive performance in\nthree realistic scenarios: tailoring the meta-model to describe a specific\nsystem rather than a class; extending the meta-model to capture the behaviour\nof systems beyond the initial training class; and recalibrating the model for\nnew prediction tasks. Results highlight the effectiveness of meta-model\nadaptation to achieve a more robust and versatile meta-learning framework for\nsystem identification.",
            "author": [
                "Dario Piga",
                "Filippo Pura",
                "Marco Forgione"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04083v1",
                "http://arxiv.org/pdf/2312.04083v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04078v1",
            "title": "A Review and Taxonomy of Methods for Quantifying Dataset Similarity",
            "updated": "2023-12-07T06:44:14Z",
            "published": "2023-12-07T06:44:14Z",
            "summary": "In statistics and machine learning, measuring the similarity between two or\nmore datasets is important for several purposes. The performance of a\npredictive model on novel datasets, referred to as generalizability, critically\ndepends on how similar the dataset used for fitting the model is to the novel\ndatasets. Exploiting or transferring insights between similar datasets is a key\naspect of meta-learning and transfer-learning. In two-sample testing, it is\nchecked, whether the underlying (multivariate) distributions of two datasets\ncoincide or not.\n  Extremely many approaches for quantifying dataset similarity have been\nproposed in the literature. A structured overview is a crucial first step for\ncomparisons of approaches. We examine more than 100 methods and provide a\ntaxonomy, classifying them into ten classes, including (i) comparisons of\ncumulative distribution functions, density functions, or characteristic\nfunctions, (ii) methods based on multivariate ranks, (iii) discrepancy measures\nfor distributions, (iv) graph-based methods, (v) methods based on inter-point\ndistances, (vi) kernel-based methods, (vii) methods based on binary\nclassification, (viii) distance and similarity measures for datasets, (ix)\ncomparisons based on summary statistics, and (x) different testing approaches.\nHere, we present an extensive review of these methods. We introduce the main\nunderlying ideas, formal definitions, and important properties.",
            "author": [
                "Marieke Stolte",
                "Andrea Bommert",
                "J\u00f6rg Rahnenf\u00fchrer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04078v1",
                "http://arxiv.org/pdf/2312.04078v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "62E99, 62G10, 62H15, 62H30, 05C90"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04076v1",
            "title": "Large Language Models are Good Prompt Learners for Low-Shot Image\n  Classification",
            "updated": "2023-12-07T06:43:34Z",
            "published": "2023-12-07T06:43:34Z",
            "summary": "Low-shot image classification, where training images are limited or\ninaccessible, has benefited from recent progress on pre-trained vision-language\n(VL) models with strong generalizability, e.g. CLIP. Prompt learning methods\nbuilt with VL models generate text features from the class names that only have\nconfined class-specific information. Large Language Models (LLMs), with their\nvast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we\ndiscuss the integration of LLMs to enhance pre-trained VL models, specifically\non low-shot classification. However, the domain gap between language and vision\nblocks the direct application of LLMs. Thus, we propose LLaMP, Large Language\nModels as Prompt learners, that produces adaptive prompts for the CLIP text\nencoder, establishing it as the connecting bridge. Experiments show that,\ncompared with other state-of-the-art prompt learning methods, LLaMP yields\nbetter performance on both zero-shot generalization and few-shot image\nclassification, over a spectrum of 11 datasets.",
            "author": [
                "Zhaoheng Zheng",
                "Jingmin Wei",
                "Xuefeng Hu",
                "Haidong Zhu",
                "Ram Nevatia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04076v1",
                "http://arxiv.org/pdf/2312.04076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04071v1",
            "title": "Synergistic Signals: Exploiting Co-Engagement and Semantic Links via\n  Graph Neural Networks",
            "updated": "2023-12-07T06:29:26Z",
            "published": "2023-12-07T06:29:26Z",
            "summary": "Given a set of candidate entities (e.g. movie titles), the ability to\nidentify similar entities is a core capability of many recommender systems.\nMost often this is achieved by collaborative filtering approaches, i.e. if\nusers co-engage with a pair of entities frequently enough, the embeddings\nshould be similar. However, relying on co-engagement data alone can result in\nlower-quality embeddings for new and unpopular entities. We study this problem\nin the context recommender systems at Netflix. We observe that there is\nabundant semantic information such as genre, content maturity level, themes,\netc. that complements co-engagement signals and provides interpretability in\nsimilarity models. To learn entity similarities from both data sources\nholistically, we propose a novel graph-based approach called SemanticGNN.\nSemanticGNN models entities, semantic concepts, collaborative edges, and\nsemantic edges within a large-scale knowledge graph and conducts representation\nlearning over it. Our key technical contributions are twofold: (1) we develop a\nnovel relation-aware attention graph neural network (GNN) to handle the\nimbalanced distribution of relation types in our graph; (2) to handle web-scale\ngraph data that has millions of nodes and billions of edges, we develop a novel\ndistributed graph training paradigm. The proposed model is successfully\ndeployed within Netflix and empirical experiments indicate it yields up to 35%\nimprovement in performance on similarity judgment tasks.",
            "author": [
                "Zijie Huang",
                "Baolin Li",
                "Hafez Asgharzadeh",
                "Anne Cocos",
                "Lingyi Liu",
                "Evan Cox",
                "Colby Wise",
                "Sudarshan Lamkhede"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04071v1",
                "http://arxiv.org/pdf/2312.04071v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04067v1",
            "title": "MeanCut: A Greedy-Optimized Graph Clustering via Path-based Similarity\n  and Degree Descent Criterion",
            "updated": "2023-12-07T06:19:39Z",
            "published": "2023-12-07T06:19:39Z",
            "summary": "As the most typical graph clustering method, spectral clustering is popular\nand attractive due to the remarkable performance, easy implementation, and\nstrong adaptability. Classical spectral clustering measures the edge weights of\ngraph using pairwise Euclidean-based metric, and solves the optimal graph\npartition by relaxing the constraints of indicator matrix and performing\nLaplacian decomposition. However, Euclidean-based similarity might cause skew\ngraph cuts when handling non-spherical data distributions, and the relaxation\nstrategy introduces information loss. Meanwhile, spectral clustering requires\nspecifying the number of clusters, which is hard to determine without enough\nprior knowledge. In this work, we leverage the path-based similarity to enhance\nintra-cluster associations, and propose MeanCut as the objective function and\ngreedily optimize it in degree descending order for a nondestructive graph\npartition. This algorithm enables the identification of arbitrary shaped\nclusters and is robust to noise. To reduce the computational complexity of\nsimilarity calculation, we transform optimal path search into generating the\nmaximum spanning tree (MST), and develop a fast MST (FastMST) algorithm to\nfurther improve its time-efficiency. Moreover, we define a density gradient\nfactor (DGF) for separating the weakly connected clusters. The validity of our\nalgorithm is demonstrated by testifying on real-world benchmarks and\napplication of face recognition. The source code of MeanCut is available at\nhttps://github.com/ZPGuiGroupWhu/MeanCut-Clustering.",
            "author": [
                "Dehua Peng",
                "Zhipeng Gui",
                "Huayi Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04067v1",
                "http://arxiv.org/pdf/2312.04067v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "I.5.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04066v1",
            "title": "Combining inherent knowledge of vision-language models with unsupervised\n  domain adaptation through self-knowledge distillation",
            "updated": "2023-12-07T06:16:39Z",
            "published": "2023-12-07T06:16:39Z",
            "summary": "Unsupervised domain adaptation (UDA) tries to overcome the tedious work of\nlabeling data by leveraging a labeled source dataset and transferring its\nknowledge to a similar but different target dataset. On the other hand, current\nvision-language models exhibit astonishing zero-shot prediction capabilities.\nIn this work, we combine knowledge gained through UDA with the inherent\nknowledge of vision-language models. In a first step, we generate the zero-shot\npredictions of the source and target dataset using the vision-language model.\nSince zero-shot predictions usually exhibit a large entropy, meaning that the\nclass probabilities are rather evenly distributed, we first adjust the\ndistribution to accentuate the winning probabilities. This is done using both\nsource and target data to keep the relative confidence between source and\ntarget data. We then employ a conventional DA method, to gain the knowledge\nfrom the source dataset, in combination with self-knowledge distillation, to\nmaintain the inherent knowledge of the vision-language model. We further\ncombine our method with a gradual source domain expansion strategy (GSDE) and\nshow that this strategy can also benefit by including zero-shot predictions. We\nconduct experiments and ablation studies on three benchmarks (OfficeHome,\nVisDA, and DomainNet) and outperform state-of-the-art methods. We further show\nin ablation studies the contributions of different parts of our algorithm.",
            "author": [
                "Thomas Westfechtel",
                "Dexuan Zhang",
                "Tatsuya Harada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04066v1",
                "http://arxiv.org/pdf/2312.04066v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04062v1",
            "title": "A Low-Overhead Incorporation-Extrapolation based Few-Shot CSI Feedback\n  Framework for Massive MIMO Systems",
            "updated": "2023-12-07T06:01:47Z",
            "published": "2023-12-07T06:01:47Z",
            "summary": "Accurate channel state information (CSI) is essential for downlink precoding\nat the base station (BS), especially for frequency FDD wideband massive MIMO\nsystems with OFDM. In FDD systems, CSI is attained through CSI feedback from\nthe user equipment (UE). However, large-scale antennas and large number of\nsubcarriers significantly increase CSI feedback overhead. Deep learning-based\nCSI feedback methods have received tremendous attention in recent years due to\ntheir great capability of compressing CSI. Nonetheless, large amounts of\ncollected samples are required to train deep learning models, which is severely\nchallenging in practice. Besides, with the rapidly increasing number of\nantennas and subcarriers, most of these deep learning methods' CSI feedback\noverhead also grow dramatically, owing to their focus on full-dimensional CSI\nfeedback. To address this issue, in this paper, we propose a low-overhead\nIncorporation-Extrapolation based Few-Shot CSI feedback Framework (IEFSF) for\nmassive MIMO systems. To further reduce the feedback overhead, a\nlow-dimensional eigenvector-based CSI matrix is first formed with the\nincorporation process at the UE, and then recovered to the full-dimensional\neigenvector-based CSI matrix at the BS via the extrapolation process. After\nthat, to alleviate the necessity of the extensive collected samples and enable\nfew-shot CSI feedback, we further propose a knowledge-driven data augmentation\nmethod and an artificial intelligence-generated content (AIGC) -based data\naugmentation method by exploiting the domain knowledge of wireless channels and\nby exploiting a novel generative model, respectively. Numerical results\ndemonstrate that the proposed IEFSF can significantly reduce CSI feedback\noverhead by 16 times compared with existing CSI feedback methods while\nmaintaining higher feedback accuracy using only several hundreds of collected\nsamples.",
            "author": [
                "Binggui Zhou",
                "Xi Yang",
                "Jintao Wang",
                "Shaodan Ma",
                "Feifei Gao",
                "Guanghua Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04062v1",
                "http://arxiv.org/pdf/2312.04062v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04055v1",
            "title": "Jointly spatial-temporal representation learning for individual\n  trajectories",
            "updated": "2023-12-07T05:27:24Z",
            "published": "2023-12-07T05:27:24Z",
            "summary": "Individual trajectories, containing substantial information on\nhuman-environment interactions across space and time, is a crucial input for\ngeospatial foundation models (GeoFMs). However, existing attempts, leveraging\ntrajectory data for various applications have overlooked the implicit\nspatial-temporal dependency within trajectories and failed to encode and\nrepresent it in a format friendly to deep learning, posing a challenge in\nobtaining general-purpose trajectory representations. Therefore, this paper\nproposes a spatial-temporal joint representation learning method (ST-GraphRL)\nto formalize learnable spatial-temporal dependencies into trajectory\nrepresentations. The proposed ST-GraphRL consists of three compositions: (i) a\nweighted directed spatial-temporal graph to explicitly construct mobility\ninteractions over both space and time dimensions; (ii) a two-stage jointly\nencoder (i.e., decoupling and fusion) to learn entangled spatial-temporal\ndependencies by independently decomposing and jointly aggregating space and\ntime information; (iii) a decoder guides ST-GraphRL to learn explicit mobility\nregularities by simulating the spatial-temporal distributions of trajectories.\nTested on three real-world human mobility datasets, the proposed ST-GraphRL\noutperformed all the baseline models in predicting movement spatial-temporal\ndistributions and preserving trajectory similarity with high spatial-temporal\ncorrelations. We also explore how spatial-temporal features presented in latent\nspace, validating that ST-GraphRL understands spatial-temporal patterns. This\nmethod is also transferable for general-purpose geospatial data representations\nfor broad downstream tasks, as well advancing GeoFMs developing.",
            "author": [
                "Fei Huang",
                "Jianrong Lv",
                "Yang Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04055v1",
                "http://arxiv.org/pdf/2312.04055v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04052v1",
            "title": "Multimodal Misinformation Detection in a South African Social Media\n  Environment",
            "updated": "2023-12-07T05:20:15Z",
            "published": "2023-12-07T05:20:15Z",
            "summary": "With the constant spread of misinformation on social media networks, a need\nhas arisen to continuously assess the veracity of digital content. This need\nhas inspired numerous research efforts on the development of misinformation\ndetection (MD) models. However, many models do not use all information\navailable to them and existing research contains a lack of relevant datasets to\ntrain the models, specifically within the South African social media\nenvironment. The aim of this paper is to investigate the transferability of\nknowledge of a MD model between different contextual environments. This\nresearch contributes a multimodal MD model capable of functioning in the South\nAfrican social media environment, as well as introduces a South African\nmisinformation dataset. The model makes use of multiple sources of information\nfor misinformation detection, namely: textual and visual elements. It uses\nbidirectional encoder representations from transformers (BERT) as the textual\nencoder and a residual network (ResNet) as the visual encoder. The model is\ntrained and evaluated on the Fakeddit dataset and a South African\nmisinformation dataset. Results show that using South African samples in the\ntraining of the model increases model performance, in a South African\ncontextual environment, and that a multimodal model retains significantly more\nknowledge than both the textual and visual unimodal models. Our study suggests\nthat the performance of a misinformation detection model is influenced by the\ncultural nuances of its operating environment and multimodal models assist in\nthe transferability of knowledge between different contextual environments.\nTherefore, local data should be incorporated into the training process of a\nmisinformation detection model in order to optimize model performance.",
            "author": [
                "Amica De Jager",
                "Vukosi Marivate",
                "Abioudun Modupe"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-49002-6_19",
                "http://arxiv.org/abs/2312.04052v1",
                "http://arxiv.org/pdf/2312.04052v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04044v1",
            "title": "Residual Graph Convolutional Network for Bird's-Eye-View Semantic\n  Segmentation",
            "updated": "2023-12-07T05:04:41Z",
            "published": "2023-12-07T05:04:41Z",
            "summary": "Retrieving spatial information and understanding the semantic information of\nthe surroundings are important for Bird's-Eye-View (BEV) semantic segmentation.\nIn the application of autonomous driving, autonomous vehicles need to be aware\nof their surroundings to drive safely. However, current BEV semantic\nsegmentation techniques, deep Convolutional Neural Networks (CNNs) and\ntransformers, have difficulties in obtaining the global semantic relationships\nof the surroundings at the early layers of the network. In this paper, we\npropose to incorporate a novel Residual Graph Convolutional (RGC) module in\ndeep CNNs to acquire both the global information and the region-level semantic\nrelationship in the multi-view image domain. Specifically, the RGC module\nemploys a non-overlapping graph space projection to efficiently project the\ncomplete BEV information into graph space. It then builds interconnected\nspatial and channel graphs to extract spatial information between each node and\nchannel information within each node (i.e., extract contextual relationships of\nthe global features). Furthermore, it uses a downsample residual process to\nenhance the coordinate feature reuse to maintain the global information. The\nsegmentation data augmentation and alignment module helps to simultaneously\naugment and align BEV features and ground truth to geometrically preserve their\nalignment to achieve better segmentation results. Our experimental results on\nthe nuScenes benchmark dataset demonstrate that the RGC network outperforms\nfour state-of-the-art networks and its four variants in terms of IoU and mIoU.\nThe proposed RGC network achieves a higher mIoU of 3.1% than the best\nstate-of-the-art network, BEVFusion. Code and models will be released.",
            "author": [
                "Qiuxiao Chen",
                "Xiaojun Qi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04044v1",
                "http://arxiv.org/pdf/2312.04044v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04039v1",
            "title": "Irreducible pairings and indecomposable tournaments",
            "updated": "2023-12-07T04:53:21Z",
            "published": "2023-12-07T04:53:21Z",
            "summary": "We only consider finite structures. With every totally ordered set $V$ and a\nsubset $P$ of $\\binom{V}{2}$, we associate the underlying tournament ${\\rm\nInv}(\\underline{V}, P)$ obtained from the transitive tournament\n$\\underline{V}:=(V, \\{(x,y) \\in V \\times V : x < y \\})$ by reversing $P$, i.e.,\nby reversing the arcs $(x,y)$ such that $\\{x,y\\} \\in P$. The subset $P$ is a\npairing (of $\\cup P$) if $|\\cup P| = 2|P|$, a quasi-pairing (of $\\cup P$) if\n$|\\cup P| = 2|P|-1$; it is irreducible if no nontrivial interval of $\\cup P$ is\na union of connected components of the graph $(\\cup P, P)$. In this paper, we\nconsider pairings and quasi-pairings in relation to tournaments. We establish\nclose relationships between irreducibility of pairings (or quasi-pairings) and\nindecomposability of their underlying tournaments under modular decomposition.\nFor example, given a pairing $P$ of a totally ordered set $V$ of size at least\n$6$, the pairing $P$ is irreducible if and only if the tournament ${\\rm\nInv}(\\underline{V}, P)$ is indecomposable. This is a consequence of a more\ngeneral result characterizing indecomposable tournaments obtained from\ntransitive tournaments by reversing pairings. We obtain analogous results in\nthe case of quasi-pairings.",
            "author": [
                "Houmem Belkhechine",
                "Cherifa Ben Salha",
                "Rim Romdhane"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04039v1",
                "http://arxiv.org/pdf/2312.04039v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04032v1",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with\n  Selective Training",
            "updated": "2023-12-07T04:23:36Z",
            "published": "2023-12-07T04:23:36Z",
            "summary": "Fine-tuning pre-trained language models (LMs) has become the de facto\nstandard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to\nrobustness issues, such as adversarial robustness and model calibration.\nSeveral perspectives of robustness for LMs have been studied independently, but\nlacking a unified consideration in multiple perspectives. In this paper, we\npropose Robustifying LMs via Adversarial perturbation with Selective Training\n(RoAST), a simple yet effective fine-tuning technique to enhance the\nmulti-perspective robustness of LMs in a unified way. RoAST effectively\nincorporates two important sources for the model robustness, robustness on the\nperturbed inputs and generalizable knowledge in pre-trained LMs. To be\nspecific, RoAST introduces adversarial perturbation during fine-tuning while\nthe model parameters are selectively updated upon their relative importance to\nminimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by\nincorporating four representative perspectives of model robustness, we\ndemonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning\nmethods on six different types of LMs, which indicates its usefulness in\npractice.",
            "author": [
                "Jaehyung Kim",
                "Yuning Mao",
                "Rui Hou",
                "Hanchao Yu",
                "Davis Liang",
                "Pascale Fung",
                "Qifan Wang",
                "Fuli Feng",
                "Lifu Huang",
                "Madian Khabsa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04032v1",
                "http://arxiv.org/pdf/2312.04032v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04029v1",
            "title": "Improved Face Representation via Joint Label Classification and\n  Supervised Contrastive Clustering",
            "updated": "2023-12-07T03:55:20Z",
            "published": "2023-12-07T03:55:20Z",
            "summary": "Face clustering tasks can learn hierarchical semantic information from\nlarge-scale data, which has the potential to help facilitate face recognition.\nHowever, there are few works on this problem. This paper explores it by\nproposing a joint optimization task of label classification and supervised\ncontrastive clustering to introduce the cluster knowledge to the traditional\nface recognition task in two ways. We first extend ArcFace with a\ncluster-guided angular margin to adjust the within-class feature distribution\naccording to the hard level of face clustering. Secondly, we propose a\nsupervised contrastive clustering approach to pull the features to the cluster\ncenter and propose the cluster-aligning procedure to align the cluster center\nand the learnable class center in the classifier for joint training. Finally,\nextensive qualitative and quantitative experiments on popular facial benchmarks\ndemonstrate the effectiveness of our paradigm and its superiority over the\nexisting approaches to face recognition.",
            "author": [
                "Zhenduo Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04029v1",
                "http://arxiv.org/pdf/2312.04029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04025v1",
            "title": "Moirai: Towards Optimal Placement for Distributed Inference on\n  Heterogeneous Devices",
            "updated": "2023-12-07T03:46:14Z",
            "published": "2023-12-07T03:46:14Z",
            "summary": "The escalating size of Deep Neural Networks (DNNs) has spurred a growing\nresearch interest in hosting and serving DNN models across multiple devices. A\nnumber of studies have been reported to partition a DNN model across devices,\nproviding device placement solutions. The methods appeared in the literature,\nhowever, either suffer from poor placement performance due to the exponential\nsearch space or miss an optimal placement as a consequence of the reduced\nsearch space with limited heuristics. Moreover, these methods have ignored the\nruntime inter-operator optimization of a computation graph when coarsening the\ngraph, which degrades the end-to-end inference performance. This paper presents\nMoirai that better exploits runtime inter-operator fusion in a model to render\na coarsened computation graph, reducing the search space while maintaining the\ninter-operator optimization provided by inference backends. Moirai also\ngeneralizes the device placement algorithm from multiple perspectives by\nconsidering inference constraints and device heterogeneity.Extensive\nexperimental evaluation with 11 large DNNs demonstrates that Moirai outperforms\nthe state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to\n4.28$\\times$ in reduction of the end-to-end inference latency. Moirai code is\nanonymously released at \\url{https://github.com/moirai-placement/moirai}.",
            "author": [
                "Beibei Zhang",
                "Hongwei Zhu",
                "Feng Gao",
                "Zhihui Yang",
                "Sean Xiaoyang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04025v1",
                "http://arxiv.org/pdf/2312.04025v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04016v1",
            "title": "PartDistill: 3D Shape Part Segmentation by Vision-Language Model\n  Distillation",
            "updated": "2023-12-07T03:10:03Z",
            "published": "2023-12-07T03:10:03Z",
            "summary": "This paper proposes a cross-modal distillation framework, PartDistill, which\ntransfers 2D knowledge from vision-language models (VLMs) to facilitate 3D\nshape part segmentation. PartDistill addresses three major challenges in this\ntask: the lack of 3D segmentation in invisible or undetected regions in the 2D\nprojections, inaccurate and inconsistent 2D predictions by VLMs, and the lack\nof knowledge accumulation across different 3D shapes. PartDistill consists of a\nteacher network that uses a VLM to make 2D predictions and a student network\nthat learns from the 2D predictions while extracting geometrical features from\nmultiple 3D shapes to carry out 3D part segmentation. A bi-directional\ndistillation, including forward and backward distillations, is carried out\nwithin the framework, where the former forward distills the 2D predictions to\nthe student network, and the latter improves the quality of the 2D predictions,\nwhich subsequently enhances the final 3D part segmentation. Moreover,\nPartDistill can exploit generative models that facilitate effortless 3D shape\ncreation for generating knowledge sources to be distilled. Through extensive\nexperiments, PartDistill boosts the existing methods with substantial margins\non widely used ShapeNetPart and PartE datasets, by more than 15% and 12% higher\nmIoU scores, respectively.",
            "author": [
                "Ardian Umam",
                "Cheng-Kun Yang",
                "Min-Hung Chen",
                "Jen-Hui Chuang",
                "Yen-Yu Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04016v1",
                "http://arxiv.org/pdf/2312.04016v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.04005v1",
            "title": "KOALA: Self-Attention Matters in Knowledge Distillation of Latent\n  Diffusion Models for Memory-Efficient and Fast Image Synthesis",
            "updated": "2023-12-07T02:46:18Z",
            "published": "2023-12-07T02:46:18Z",
            "summary": "Stable diffusion is the mainstay of the text-to-image (T2I) synthesis in the\ncommunity due to its generation performance and open-source nature. Recently,\nStable Diffusion XL (SDXL), the successor of stable diffusion, has received a\nlot of attention due to its significant performance improvements with a higher\nresolution of 1024x1024 and a larger model. However, its increased computation\ncost and model size require higher-end hardware(e.g., bigger VRAM GPU) for\nend-users, incurring higher costs of operation. To address this problem, in\nthis work, we propose an efficient latent diffusion model for text-to-image\nsynthesis obtained by distilling the knowledge of SDXL. To this end, we first\nperform an in-depth analysis of the denoising U-Net in SDXL, which is the main\nbottleneck of the model, and then design a more efficient U-Net based on the\nanalysis. Secondly, we explore how to effectively distill the generation\ncapability of SDXL into an efficient U-Net and eventually identify four\nessential factors, the core of which is that self-attention is the most\nimportant part. With our efficient U-Net and self-attention-based knowledge\ndistillation strategy, we build our efficient T2I models, called KOALA-1B &\n-700M, while reducing the model size up to 54% and 69% of the original SDXL\nmodel. In particular, the KOALA-700M is more than twice as fast as SDXL while\nstill retaining a decent generation quality. We hope that due to its balanced\nspeed-performance tradeoff, our KOALA models can serve as a cost-effective\nalternative to SDXL in resource-constrained environments.",
            "author": [
                "Youngwan Lee",
                "Kwanyong Park",
                "Yoorhim Cho",
                "Yong-Ju Lee",
                "Sung Ju Hwang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.04005v1",
                "http://arxiv.org/pdf/2312.04005v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03979v1",
            "title": "Node-aware Bi-smoothing: Certified Robustness against Graph Injection\n  Attacks",
            "updated": "2023-12-07T01:24:48Z",
            "published": "2023-12-07T01:24:48Z",
            "summary": "Deep Graph Learning (DGL) has emerged as a crucial technique across various\ndomains. However, recent studies have exposed vulnerabilities in DGL models,\nsuch as susceptibility to evasion and poisoning attacks. While empirical and\nprovable robustness techniques have been developed to defend against graph\nmodification attacks (GMAs), the problem of certified robustness against graph\ninjection attacks (GIAs) remains largely unexplored. To bridge this gap, we\nintroduce the node-aware bi-smoothing framework, which is the first certifiably\nrobust approach for general node classification tasks against GIAs. Notably,\nthe proposed node-aware bi-smoothing scheme is model-agnostic and is applicable\nfor both evasion and poisoning attacks. Through rigorous theoretical analysis,\nwe establish the certifiable conditions of our smoothing scheme. We also\nexplore the practical implications of our node-aware bi-smoothing schemes in\ntwo contexts: as an empirical defense approach against real-world GIAs and in\nthe context of recommendation systems. Furthermore, we extend two\nstate-of-the-art certified robustness frameworks to address node injection\nattacks and compare our approach against them. Extensive evaluations\ndemonstrate the effectiveness of our proposed certificates.",
            "author": [
                "Yuni Lai",
                "Yulin Zhu",
                "Bailin Pan",
                "Kai Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03979v1",
                "http://arxiv.org/pdf/2312.03979v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03970v1",
            "title": "Improving Medical Report Generation with Adapter Tuning and Knowledge\n  Enhancement in Vision-Language Foundation Models",
            "updated": "2023-12-07T01:01:45Z",
            "published": "2023-12-07T01:01:45Z",
            "summary": "Medical report generation demands automatic creation of coherent and precise\ndescriptions for medical images. However, the scarcity of labelled medical\nimage-report pairs poses formidable challenges in developing large-scale neural\nnetworks capable of harnessing the potential of artificial intelligence,\nexemplified by large language models. This study builds upon the\nstate-of-the-art vision-language pre-training and fine-tuning approach, BLIP-2,\nto customize general large-scale foundation models. Integrating adapter tuning\nand a medical knowledge enhancement loss, our model significantly improves\naccuracy and coherence. Validation on the dataset of ImageCLEFmedical 2023\ndemonstrates our model's prowess, achieving the best-averaged results against\nseveral state-of-the-art methods. Significant improvements in ROUGE and CIDEr\nunderscore our method's efficacy, highlighting promising outcomes for the rapid\nmedical-domain adaptation of the vision-language foundation models in\naddressing challenges posed by data scarcity.",
            "author": [
                "Shibin Wu",
                "Bang Yang",
                "Zhiyu Ye",
                "Haoqian Wang",
                "Hairong Zheng",
                "Tong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03970v1",
                "http://arxiv.org/pdf/2312.03970v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03967v1",
            "title": "Test-negative designs with various reasons for testing: statistical bias\n  and solution",
            "updated": "2023-12-07T00:57:47Z",
            "published": "2023-12-07T00:57:47Z",
            "summary": "Test-negative designs are widely used for post-market evaluation of vaccine\neffectiveness. Different from classical test-negative designs where only\nhealthcare-seekers with symptoms are included, recent test-negative designs\nhave involved individuals with various reasons for testing, especially in an\noutbreak setting. While including these data can increase sample size and hence\nimprove precision, concerns have been raised about whether they will introduce\nbias into the current framework of test-negative designs, thereby demanding a\nformal statistical examination of this modified design. In this article, using\nstatistical derivations, causal graphs, and numerical simulations, we show that\nthe standard odds ratio estimator may be biased if various reasons for testing\nare not accounted for. To eliminate this bias, we identify three categories of\nreasons for testing, including symptoms, disease-unrelated reasons, and case\ncontact tracing, and characterize associated statistical properties and\nestimands. Based on our characterization, we propose stratified estimators that\ncan incorporate multiple reasons for testing to achieve consistent estimation\nand improve precision by maximizing the use of data. The performance of our\nproposed method is demonstrated through simulation studies.",
            "author": [
                "Mengxin Yu",
                "Kendrick Qijun Li",
                "Nicholas Jewell",
                "Eric Tchetgen Tchetgen",
                "Dylan Small",
                "Xu Shi",
                "Bingkai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03967v1",
                "http://arxiv.org/pdf/2312.03967v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03965v1",
            "title": "On the formal Peterson subalgebra and its dual",
            "updated": "2023-12-07T00:50:35Z",
            "published": "2023-12-07T00:50:35Z",
            "summary": "In the present notes, we study a generalization of the Peterson subalgebra to\nan oriented (generalized) cohomology theory which we call the formal Peterson\nsubalgebra. Observe that by recent results of Zhong the dual of the formal\nPeterson algebra provides an algebraic model for the oriented cohomology of the\naffine Grassmannian.\n  Our first result shows that the centre of the formal affine Demazure algebra\ngenerates the formal Peterson subalgebra. Our second observation is motivated\nby the Peterson conjecture. We show that a certain localization of the formal\nPeterson subalgebra for the extended Dynkin diagram of type $\\hat A_1$ provides\nan algebraic model for `quantum' oriented cohomology of the projective line.\nOur last result can be viewed as an extension of the previous results on Hopf\nalgebroids of structure algebras of moment graphs to the case of affine root\nsystems. We prove that the dual of the formal Peterson subalgebra (an oriented\ncohomology of the affine Grassmannian) is the $0$th Hochshild homology of the\nformal affine Demazure algebra.",
            "author": [
                "Rui Xiong",
                "Kirill Zainoulline",
                "Changlong Zhong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03965v1",
                "http://arxiv.org/pdf/2312.03965v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "14F43, 14M15, 19L41, 55N22, 14N15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03958v1",
            "title": "On Distributed Nonconvex Optimisation Via Modified ADMM",
            "updated": "2023-12-07T00:20:50Z",
            "published": "2023-12-07T00:20:50Z",
            "summary": "This paper addresses the problem of nonconvex nonsmooth decentralised\noptimisation in multi-agent networks with undirected connected communication\ngraphs. Our contribution lies in introducing an algorithmic framework designed\nfor the distributed minimisation of the sum of a smooth (possibly nonconvex and\nnon-separable) function and a convex (possibly nonsmooth and non-separable)\nregulariser. The proposed algorithm can be seen as a modified version of the\nADMM algorithm where, at each step, an \"inner loop\" needs to be iterated for a\nnumber of iterations. The role of the inner loop is to aggregate and\ndisseminate information across the network. We observe that a naive\ndecentralised approach (one iteration of the inner loop) may not converge. We\nestablish the asymptotic convergence of the proposed algorithm to the set of\nstationary points of the nonconvex problem where the number of iterations of\nthe inner loop increases logarithmically with the step count of the ADMM\nalgorithm. We present numerical results demonstrating the proposed method's\ncorrectness and performance.",
            "author": [
                "Behnam Mafakheri",
                "Jonathan H. Manton",
                "Iman Shames"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03958v1",
                "http://arxiv.org/pdf/2312.03958v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03952v1",
            "title": "Deterministic Creation of Large Photonic Multipartite Entangled States\n  with Group-IV Color Centers in Diamond",
            "updated": "2023-12-06T23:33:22Z",
            "published": "2023-12-06T23:33:22Z",
            "summary": "Measurement-based quantum computation relies on single qubit measurements of\nlarge multipartite entangled states, so-called lattice-graph or cluster states.\nGraph states are also an important resource for quantum communication, where\ntree cluster states are a key resource for one-way quantum repeaters. A\nphotonic realization of this kind of state would inherit many of the benefits\nof photonic platforms, such as very little dephasing due to weak environmental\ninteractions and the well-developed infrastructure to route and measure\nphotonic qubits. In this work, a linear cluster state and GHZ state generation\nscheme is developed for group-IV color centers. In particular, this article\nfocuses on an in-depth investigation of the required control operations,\nincluding the coherent spin and excitation gates. We choose an off-resonant\nRaman scheme for the spin gates, which can be much faster than microwave\ncontrol. We do not rely on a reduced level scheme and use efficient\napproximations to design high-fidelity Raman gates. We benchmark the\nspin-control and excitation scheme using the tin vacancy color center coupled\nto a cavity, assuming a realistic experimental setting. Additionally, the\narticle investigates the fidelities of the Raman and excitation gates in the\npresence of radiative and non-radiative decay mechanisms. Finally, a quality\nmeasure is devised, which emphasizes the importance of fast and high-fidelity\nspin gates in the creation of large entangled photonic states.",
            "author": [
                "Gregor Pieplow",
                "Yannick Strocka",
                "Mariano Isaza-Monsalve",
                "Joseph H. D. Munns",
                "Tim Schr\u00f6der"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03952v1",
                "http://arxiv.org/pdf/2312.03952v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03951v1",
            "title": "Understanding the Role of Optimization in Double Descent",
            "updated": "2023-12-06T23:29:00Z",
            "published": "2023-12-06T23:29:00Z",
            "summary": "The phenomenon of model-wise double descent, where the test error peaks and\nthen reduces as the model size increases, is an interesting topic that has\nattracted the attention of researchers due to the striking observed gap between\ntheory and practice \\citep{Belkin2018ReconcilingMM}. Additionally, while double\ndescent has been observed in various tasks and architectures, the peak of\ndouble descent can sometimes be noticeably absent or diminished, even without\nexplicit regularization, such as weight decay and early stopping. In this\npaper, we investigate this intriguing phenomenon from the optimization\nperspective and propose a simple optimization-based explanation for why double\ndescent sometimes occurs weakly or not at all. To the best of our knowledge, we\nare the first to demonstrate that many disparate factors contributing to\nmodel-wise double descent (initialization, normalization, batch size, learning\nrate, optimization algorithm) are unified from the viewpoint of optimization:\nmodel-wise double descent is observed if and only if the optimizer can find a\nsufficiently low-loss minimum. These factors directly affect the condition\nnumber of the optimization problem or the optimizer and thus affect the final\nminimum found by the optimizer, reducing or increasing the height of the double\ndescent peak. We conduct a series of controlled experiments on random feature\nmodels and two-layer neural networks under various optimization settings,\ndemonstrating this optimization-based unified view. Our results suggest the\nfollowing implication: Double descent is unlikely to be a problem for\nreal-world machine learning setups. Additionally, our results help explain the\ngap between weak double descent peaks in practice and strong peaks observable\nin carefully designed setups.",
            "author": [
                "Chris Yuhao Liu",
                "Jeffrey Flanigan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03951v1",
                "http://arxiv.org/pdf/2312.03951v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03950v1",
            "title": "A Scalable and Generalizable Pathloss Map Prediction",
            "updated": "2023-12-06T23:22:49Z",
            "published": "2023-12-06T23:22:49Z",
            "summary": "Large-scale channel prediction, i.e., estimation of the pathloss from\ngeographical/morphological/building maps, is an essential component of wireless\nnetwork planning. Ray tracing (RT)-based methods have been widely used for many\nyears, but they require significant computational effort that may become\nprohibitive with the increased network densification and/or use of higher\nfrequencies in B5G/6G systems. In this paper, we propose a data-driven,\nmodel-free pathloss map prediction (PMP) method, called PMNet. PMNet uses a\nsupervised learning approach: it is trained on a limited amount of RT (or\nchannel measurement) data and map data. Once trained, PMNet can predict\npathloss over location with high accuracy (an RMSE level of $10^{-2}$) in a few\nmilliseconds. We further extend PMNet by employing transfer learning (TL). TL\nallows PMNet to learn a new network scenario quickly (x5.6 faster training) and\nefficiently (using x4.5 less data) by transferring knowledge from a pre-trained\nmodel, while retaining accuracy. Our results demonstrate that PMNet is a\nscalable and generalizable ML-based PMP method, showing its potential to be\nused in several network optimization applications.",
            "author": [
                "Ju-Hyung Lee",
                "Andreas F. Molisch"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03950v1",
                "http://arxiv.org/pdf/2312.03950v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03940v1",
            "title": "PECANN: Parallel Efficient Clustering with Graph-Based Approximate\n  Nearest Neighbor Search",
            "updated": "2023-12-06T22:43:50Z",
            "published": "2023-12-06T22:43:50Z",
            "summary": "This paper studies density-based clustering of point sets. These methods use\ndense regions of points to detect clusters of arbitrary shapes. In particular,\nwe study variants of density peaks clustering, a popular type of algorithm that\nhas been shown to work well in practice. Our goal is to cluster large\nhigh-dimensional datasets, which are prevalent in practice. Prior solutions are\neither sequential, and cannot scale to large data, or are specialized for\nlow-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a\nsingle framework, PECANN, by abstracting out several key steps common to this\nclass of algorithms. One such key step is to find nearest neighbors that\nsatisfy a predicate function, and one of the main contributions of this paper\nis an efficient way to do this predicate search using graph-based approximate\nnearest neighbor search (ANNS). To provide ample parallelism, we propose a\ndoubling search technique that enables points to find an approximate nearest\nneighbor satisfying the predicate in a small number of rounds. Our technique\ncan be applied to many existing graph-based ANNS algorithms, which can all be\nplugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on\nsynthetic and real-world datasets with up to 1.28 million points and up to 1024\ndimensions on a 30-core machine with two-way hyper-threading. Compared to the\nstate-of-the-art FASTDP algorithm for high-dimensional density peaks\nclustering, which is sequential, our best algorithm is 45x-734x faster while\nachieving competitive ARI scores. Compared to the state-of-the-art parallel\nDPC-based algorithm, which is optimized for low dimensions, we show that PECANN\nis two orders of magnitude faster. As far as we know, our work is the first to\nevaluate DPC variants on large high-dimensional real-world image and text\nembedding datasets.",
            "author": [
                "Shangdi Yu",
                "Joshua Engels",
                "Yihao Huang",
                "Julian Shun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03940v1",
                "http://arxiv.org/pdf/2312.03940v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03933v1",
            "title": "Orbits under Dual Symplectic Transvections",
            "updated": "2023-12-06T22:19:42Z",
            "published": "2023-12-06T22:19:42Z",
            "summary": "Consider an arbitrary field $K$ and a finite-dimensional vector space $X$\nover $K$ equipped with a, possibly degenerate, symplectic form $\\omega$. Given\na spanning subset $S$ of $X$, for each $k$ in $K$ and each vector $s$ in $S$,\nconsider the symplectic transvection mapping a vector $x$ to $x+k\\omega(x,s)s$.\nThe group generated by these transvections has been extensively studied, and\nits orbit structure is known. In this paper, we obtain corresponding results\nfor the orbits of the dual action on $X^\\ast$. As for the non-dual case, the\nanalysis gets harder when the field contains only two elements. For that field,\nthe dual transvection group is equivalent to a game known as the lit-only sigma\ngame, played on a graph. Our results provide a complete solution to the\nreachability problem of that game, previously solved only for some special\ncases.",
            "author": [
                "Jonas Sj\u00f6strand"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03933v1",
                "http://arxiv.org/pdf/2312.03933v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "math.CO",
                "05C50, 05C57, 05C25, 05E18, 20F10, 15A63"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03920v1",
            "title": "Optimizing Closed Payment Networks on the Lightning Network: Dual\n  Central Node Approach",
            "updated": "2023-12-06T21:35:19Z",
            "published": "2023-12-06T21:35:19Z",
            "summary": "The Lightning Network, known for its millisecond settlement speeds and low\ntransaction fees, offers a compelling alternative to traditional payment\nprocessors, which often have higher fees and longer processing times. This is\nparticularly significant for the unbanked population, which lacks access to\nstandard financial services. Our research targets businesses looking to shift\ntheir client to client payment processes, such as B2B invoicing, remittances,\nand cross-border transactions, to the Lightning Network. We compare the\nefficiency of interconnected mesh nodes (complete graph topology) with central\nrouting nodes (star graph topology), with a specific focus on the dual central\nnode approach. This approach introduces features like circular rebalancing,\nredundancy, and a closed network system. Through a basic SimPy model, we assess\nthe network's throughput in a 100 node scenario. While this approach\ncentralizes a technology initially designed for decentralization, it fosters\nbroader enterprise adoption of Bitcoin-based payment networks and encourages\nparticipation in the decentralized financial ecosystem. Our study also\nconsiders the regulatory implications of using central routing nodes, possibly\nclassified as payment processors under Money Transmission Laws (MTL). These\nfindings aim to contribute to the discourse on the Lightning Network's\napplication in business, highlighting its potential to drive shifts in\nfinancial technology towards more decentralized systems.",
            "author": [
                "Jeffy Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03920v1",
                "http://arxiv.org/pdf/2312.03920v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.NI",
                "C.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03906v1",
            "title": "Computing the Volume of a Restricted Independent Set Polytope\n  Deterministically",
            "updated": "2023-12-06T20:59:27Z",
            "published": "2023-12-06T20:59:27Z",
            "summary": "We construct a quasi-polynomial time deterministic approximation algorithm\nfor computing the volume of an independent set polytope with restrictions.\nRandomized polynomial time approximation algorithms for computing the volume of\na convex body have been known now for several decades, but the corresponding\ndeterministic counterparts are not available, and our algorithm is the first of\nthis kind. The class of polytopes for which our algorithm applies arises as\nlinear programming relaxation of the independent set problem with the\nadditional restriction that each variable takes value in the interval\n$[0,1-\\alpha]$ for some $\\alpha<1/2$. (We note that the $\\alpha\\ge 1/2$ case is\ntrivial).\n  We use the correlation decay method for this problem applied to its\nappropriate and natural discretization. The method works provided $\\alpha>\n1/2-O(1/\\Delta^2)$, where $\\Delta$ is the maximum degree of the graph. When\n$\\Delta=3$ (the sparsest non-trivial case), our method works provided\n$0.488<\\alpha<0.5$. Interestingly, the interpolation method, which is based on\nanalyzing complex roots of the associated partition functions, fails even in\nthe trivial case when the underlying graph is a singleton.",
            "author": [
                "David Gamarnik",
                "Devin Smedira"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03906v1",
                "http://arxiv.org/pdf/2312.03906v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM",
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03903v1",
            "title": "Adaptive Dependency Learning Graph Neural Networks",
            "updated": "2023-12-06T20:56:23Z",
            "published": "2023-12-06T20:56:23Z",
            "summary": "Graph Neural Networks (GNN) have recently gained popularity in the\nforecasting domain due to their ability to model complex spatial and temporal\npatterns in tasks such as traffic forecasting and region-based demand\nforecasting. Most of these methods require a predefined graph as input, whereas\nin real-life multivariate time series problems, a well-predefined dependency\ngraph rarely exists. This requirement makes it harder for GNNs to be utilised\nwidely for multivariate forecasting problems in other domains such as retail or\nenergy. In this paper, we propose a hybrid approach combining neural networks\nand statistical structure learning models to self-learn the dependencies and\nconstruct a dynamically changing dependency graph from multivariate data aiming\nto enable the use of GNNs for multivariate forecasting even when a well-defined\ngraph does not exist. The statistical structure modeling in conjunction with\nneural networks provides a well-principled and efficient approach by bringing\nin causal semantics to determine dependencies among the series. Finally, we\ndemonstrate significantly improved performance using our proposed approach on\nreal-world benchmark datasets without a pre-defined dependency graph.",
            "author": [
                "Abishek Sriramulu",
                "Nicolas Fourrier",
                "Christoph Bergmeir"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.ins.2022.12.086",
                "http://arxiv.org/abs/2312.03903v1",
                "http://arxiv.org/pdf/2312.03903v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03900v1",
            "title": "Community Detection in High-Dimensional Graph Ensembles",
            "updated": "2023-12-06T20:50:29Z",
            "published": "2023-12-06T20:50:29Z",
            "summary": "Detecting communities in high-dimensional graphs can be achieved by applying\nrandom matrix theory where the adjacency matrix of the graph is modeled by a\nStochastic Block Model (SBM). However, the SBM makes an unrealistic assumption\nthat the edge probabilities are homogeneous within communities, i.e., the edges\noccur with the same probabilities. The Degree-Corrected SBM is a generalization\nof the SBM that allows these edge probabilities to be different, but existing\nresults from random matrix theory are not directly applicable to this\nheterogeneous model. In this paper, we derive a transformation of the adjacency\nmatrix that eliminates this heterogeneity and preserves the relevant\neigenstructure for community detection. We propose a test based on the extreme\neigenvalues of this transformed matrix and (1) provide a method for controlling\nthe significance level, (2) formulate a conjecture that the test achieves power\none for all positive significance levels in the limit as the number of nodes\napproaches infinity, and (3) provide empirical evidence and theory supporting\nthese claims.",
            "author": [
                "Robert Malinas",
                "Dogyoon Song",
                "Alfred O. Hero III"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03900v1",
                "http://arxiv.org/pdf/2312.03900v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03895v1",
            "title": "HLoOP -- Hyperbolic 2-space Local Outlier Probabilities",
            "updated": "2023-12-06T20:38:39Z",
            "published": "2023-12-06T20:38:39Z",
            "summary": "Hyperbolic geometry has recently garnered considerable attention in machine\nlearning due to its capacity to embed hierarchical graph structures with low\ndistortions for further downstream processing. This paper introduces a simple\nframework to detect local outliers for datasets grounded in hyperbolic 2-space\nreferred to as HLoOP (Hyperbolic Local Outlier Probability). Within a Euclidean\nspace, well-known techniques for local outlier detection are based on the Local\nOutlier Factor (LOF) and its variant, the LoOP (Local Outlier Probability),\nwhich incorporates probabilistic concepts to model the outlier level of a data\nvector. The developed HLoOP combines the idea of finding nearest neighbors,\ndensity-based outlier scoring with a probabilistic, statistically oriented\napproach. Therefore, the method consists in computing the Riemmanian distance\nof a data point to its nearest neighbors following a Gaussian probability\ndensity function expressed in a hyperbolic space. This is achieved by defining\na Gaussian cumulative distribution in this space. The HLoOP algorithm is tested\non the WordNet dataset yielding promising results. Code and data will be made\navailable on request for reproductibility.",
            "author": [
                "Cl\u00e9mence Allietta",
                "Jean-Philippe Condomines",
                "Jean-Yves Tourneret",
                "Emmanuel Lochin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03895v1",
                "http://arxiv.org/pdf/2312.03895v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03865v1",
            "title": "Learning Genomic Sequence Representations using Graph Neural Networks\n  over De Bruijn Graphs",
            "updated": "2023-12-06T19:23:53Z",
            "published": "2023-12-06T19:23:53Z",
            "summary": "The rapid expansion of genomic sequence data calls for new methods to achieve\nrobust sequence representations. Existing techniques often neglect intricate\nstructural details, emphasizing mainly contextual information. To address this,\nwe developed k-mer embeddings that merge contextual and structural string\ninformation by enhancing De Bruijn graphs with structural similarity\nconnections. Subsequently, we crafted a self-supervised method based on\nContrastive Learning that employs a heterogeneous Graph Convolutional Network\nencoder and constructs positive pairs based on node similarities. Our\nembeddings consistently outperform prior techniques for Edit Distance\nApproximation and Closest String Retrieval tasks.",
            "author": [
                "Kacper Kapu\u015bniak",
                "Manuel Burger",
                "Gunnar R\u00e4tsch",
                "Amir Joudaki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03865v1",
                "http://arxiv.org/pdf/2312.03865v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03864v1",
            "title": "Geometry Matching for Multi-Embodiment Grasping",
            "updated": "2023-12-06T19:20:01Z",
            "published": "2023-12-06T19:20:01Z",
            "summary": "Many existing learning-based grasping approaches concentrate on a single\nembodiment, provide limited generalization to higher DoF end-effectors and\ncannot capture a diverse set of grasp modes. We tackle the problem of grasping\nusing multiple embodiments by learning rich geometric representations for both\nobjects and end-effectors using Graph Neural Networks. Our novel method -\nGeoMatch - applies supervised learning on grasping data from multiple\nembodiments, learning end-to-end contact point likelihood maps as well as\nconditional autoregressive predictions of grasps keypoint-by-keypoint. We\ncompare our method against baselines that support multiple embodiments. Our\napproach performs better across three end-effectors, while also producing\ndiverse grasps. Examples, including real robot demos, can be found at\ngeo-match.github.io.",
            "author": [
                "Maria Attarian",
                "Muhammad Adil Asif",
                "Jingzhou Liu",
                "Ruthrash Hari",
                "Animesh Garg",
                "Igor Gilitschenski",
                "Jonathan Tompson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03864v1",
                "http://arxiv.org/pdf/2312.03864v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03859v1",
            "title": "Towards Tight Bounds for the Graph Homomorphism Problem Parameterized by\n  Cutwidth via Asymptotic Rank Parameters",
            "updated": "2023-12-06T19:15:10Z",
            "published": "2023-12-06T19:15:10Z",
            "summary": "A homomorphism from a graph $G$ to a graph $H$ is an edge-preserving mapping\nfrom $V(G)$ to $V(H)$. In the graph homomorphism problem, denoted by $Hom(H)$,\nthe graph $H$ is fixed and we need to determine if there exists a homomorphism\nfrom an instance graph $G$ to $H$. We study the complexity of the problem\nparameterized by the cutwidth of $G$.\n  We aim, for each $H$, for algorithms for $Hom(H)$ running in time $c_H^k\nn^{\\mathcal{O}(1)}$ and matching lower bounds that exclude $c_H^{k \\cdot\no(1)}n^{\\mathcal{O}(1)}$ or $c_H^{k(1-\\Omega(1))}n^{\\mathcal{O}(1)}$ time\nalgorithms under the (Strong) Exponential Time Hypothesis.\n  In the paper we introduce a new parameter that we call $\\mathrm{mimsup}(H)$.\nOur main contribution is strong evidence of a close connection between $c_H$\nand $\\mathrm{mimsup}(H)$:\n  * an information-theoretic argument that the number of states needed in a\nnatural dynamic programming algorithm is at most $\\mathrm{mimsup}(H)^k$,\n  * lower bounds that show that for almost all graphs $H$ indeed we have $c_H\n\\geq \\mathrm{mimsup}(H)$, assuming the (Strong) Exponential-Time Hypothesis,\nand\n  * an algorithm with running time $\\exp ( {\\mathcal{O}( \\mathrm{mimsup}(H)\n\\cdot k \\log k)}) n^{\\mathcal{O}(1)}$.\n  The parameter $\\mathrm{mimsup}(H)$ can be thought of as the $p$-th root of\nthe maximum induced matching number in the graph obtained by multiplying $p$\ncopies of $H$ via certain graph product, where $p$ tends to infinity. It can\nalso be defined as an asymptotic rank parameter of the adjacency matrix of $H$.\nOur results tightly link the parameterized complexity of a problem to such an\nasymptotic rank parameter for the first time.",
            "author": [
                "Carla Groenland",
                "Isja Mannens",
                "Jesper Nederlof",
                "Marta Piecyk",
                "Pawe\u0142 Rz\u0105\u017cewski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03859v1",
                "http://arxiv.org/pdf/2312.03859v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03820v1",
            "title": "The Ambient Space Formalism",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "We present a new formalism to solve the kinematical constraints due to Weyl\ninvariance for CFTs in curved backgrounds and/or non-trivial states, and we\napply it to thermal CFTs and to CFTs on squashed spheres. The ambient space\nformalism is based on constructing a class of geometric objects that are Weyl\ncovariant and identifying them as natural building blocks of correlation\nfunctions. We construct (scalar) $n$-point functions and we illustrate the\nformalism with a detailed computation of 2-point functions. We compare our\nresults for thermal 2-point functions with results that follow from thermal\nOPEs and holographic computations, finding exact agreement. In our holographic\ncomputation we also obtain the OPE coefficient of the leading double-twist\ncontribution, and we discuss how the double-twist coefficients may be computed\nfrom the multi-energy-momentum contributions, given knowledge of the analytic\nstructure of the correlator. The 2-point function for the CFT on squashed\nspheres is a new result. We also discuss the relation of our work to flat\nholography.",
            "author": [
                "Enrico Parisini",
                "Kostas Skenderis",
                "Benjamin Withers"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03820v1",
                "http://arxiv.org/pdf/2312.03820v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc",
                "math.DG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03823v1",
            "title": "High Pileup Particle Tracking with Object Condensation",
            "updated": "2023-12-06T19:00:00Z",
            "published": "2023-12-06T19:00:00Z",
            "summary": "Recent work has demonstrated that graph neural networks (GNNs) can match the\nperformance of traditional algorithms for charged particle tracking while\nimproving scalability to meet the computing challenges posed by the HL-LHC.\nMost GNN tracking algorithms are based on edge classification and identify\ntracks as connected components from an initial graph containing spurious\nconnections. In this talk, we consider an alternative based on object\ncondensation (OC), a multi-objective learning framework designed to cluster\npoints (hits) belonging to an arbitrary number of objects (tracks) and regress\nthe properties of each object. Building on our previous results, we present a\nstreamlined model and show progress toward a one-shot OC tracking algorithm in\na high-pileup environment.",
            "author": [
                "Kilian Lieret",
                "Gage DeZoort",
                "Devdoot Chatterjee",
                "Jian Park",
                "Siqi Miao",
                "Pan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03823v1",
                "http://arxiv.org/pdf/2312.03823v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an",
                "cs.LG",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03691v1",
            "title": "On the Role of Edge Dependency in Graph Generative Models",
            "updated": "2023-12-06T18:54:27Z",
            "published": "2023-12-06T18:54:27Z",
            "summary": "In this work, we introduce a novel evaluation framework for generative models\nof graphs, emphasizing the importance of model-generated graph overlap\n(Chanpuriya et al., 2021) to ensure both accuracy and edge-diversity. We\ndelineate a hierarchy of graph generative models categorized into three levels\nof complexity: edge independent, node independent, and fully dependent models.\nThis hierarchy encapsulates a wide range of prevalent methods. We derive\ntheoretical bounds on the number of triangles and other short-length cycles\nproducible by each level of the hierarchy, contingent on the model overlap. We\nprovide instances demonstrating the asymptotic optimality of our bounds.\nFurthermore, we introduce new generative models for each of the three\nhierarchical levels, leveraging dense subgraph discovery (Gionis & Tsourakakis,\n2015). Our evaluation, conducted on real-world datasets, focuses on assessing\nthe output quality and overlap of our proposed models in comparison to other\npopular models. Our results indicate that our simple, interpretable models\nprovide competitive baselines to popular generative models. Through this\ninvestigation, we aim to propel the advancement of graph generative models by\noffering a structured framework and robust evaluation metrics, thereby\nfacilitating the development of models capable of generating accurate and\nedge-diverse graphs.",
            "author": [
                "Sudhanshu Chanpuriya",
                "Cameron Musco",
                "Konstantinos Sotiropoulos",
                "Charalampos Tsourakakis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03691v1",
                "http://arxiv.org/pdf/2312.03691v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03690v1",
            "title": "Inverse Design of Vitrimeric Polymers by Molecular Dynamics and\n  Generative Modeling",
            "updated": "2023-12-06T18:53:45Z",
            "published": "2023-12-06T18:53:45Z",
            "summary": "Vitrimer is a new class of sustainable polymers with the ability of\nself-healing through rearrangement of dynamic covalent adaptive networks.\nHowever, a limited choice of constituent molecules restricts their property\nspace, prohibiting full realization of their potential applications. Through a\ncombination of molecular dynamics (MD) simulations and machine learning (ML),\nparticularly a novel graph variational autoencoder (VAE) model, we establish a\nmethod for generating novel vitrimers and guide their inverse design based on\ndesired glass transition temperature (Tg). We build the first vitrimer dataset\nof one million and calculate Tg on 8,424 of them by high-throughput MD\nsimulations calibrated by a Gaussian process model. The proposed VAE employs\ndual graph encoders and a latent dimension overlapping scheme which allows for\nindividual representation of multi-component vitrimers. By constructing a\ncontinuous latent space containing necessary information of vitrimers, we\ndemonstrate high accuracy and efficiency of our framework in discovering novel\nvitrimers with desirable Tg beyond the training regime. The proposed vitrimers\nwith reasonable synthesizability cover a wide range of Tg and broaden the\npotential widespread usage of vitrimeric materials.",
            "author": [
                "Yiwen Zheng",
                "Prakash Thakolkaran",
                "Jake A. Smith",
                "Ziheng Lu",
                "Shuxin Zheng",
                "Bichlien H. Nguyen",
                "Siddhant Kumar",
                "Aniruddh Vashisth"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03690v1",
                "http://arxiv.org/pdf/2312.03690v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03688v1",
            "title": "Twin-width of sparse random graphs",
            "updated": "2023-12-06T18:52:18Z",
            "published": "2023-12-06T18:52:18Z",
            "summary": "We show that the twin-width of every $n$-vertex $d$-regular graph is at most\n$n^{\\frac{d-2}{2d-2}+o(1)}$ and that almost all $d$-regular graphs attain this\nbound. More generally, we obtain bounds on the twin-width of sparse\nErd\\H{o}s-Renyi and regular random graphs, complementing the bounds in the\ndenser regime due to Ahn, Chakraborti, Hendrey, Kim and Oum.",
            "author": [
                "Kevin Hendrey",
                "Sergey Norin",
                "Raphael Steiner",
                "J\u00e9r\u00e9mie Turcotte"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03688v1",
                "http://arxiv.org/pdf/2312.03688v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03686v1",
            "title": "Canonization of a random graph by two matrix-vector multiplications",
            "updated": "2023-12-06T18:52:14Z",
            "published": "2023-12-06T18:52:14Z",
            "summary": "We show that a canonical labeling of a random $n$-vertex graph can be\nobtained by assigning to each vertex $x$ the triple $(w_1(x),w_2(x),w_3(x))$,\nwhere $w_k(x)$ is the number of walks of length $k$ starting from $x$. This\ntakes time $O(n^2)$, where $n^2$ is the input size, by using just two\nmatrix-vector multiplications. The linear-time canonization of a random graph\nis the classical result of Babai, Erd\\H{o}s, and Selkow. For this purpose they\nuse the well-known combinatorial color refinement procedure, and we make a\ncomparative analysis of the two algorithmic approaches.",
            "author": [
                "Oleg Verbitsky",
                "Maksim Zhukovskii"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03686v1",
                "http://arxiv.org/pdf/2312.03686v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03682v1",
            "title": "What Planning Problems Can A Relational Neural Network Solve?",
            "updated": "2023-12-06T18:47:28Z",
            "published": "2023-12-06T18:47:28Z",
            "summary": "Goal-conditioned policies are generally understood to be \"feed-forward\"\ncircuits, in the form of neural networks that map from the current state and\nthe goal specification to the next action to take. However, under what\ncircumstances such a policy can be learned and how efficient the policy will be\nare not well understood. In this paper, we present a circuit complexity\nanalysis for relational neural networks (such as graph neural networks and\ntransformers) representing policies for planning problems, by drawing\nconnections with serialized goal regression search (S-GRS). We show that there\nare three general classes of planning problems, in terms of the growth of\ncircuit width and depth as a function of the number of objects and planning\nhorizon, providing constructive proofs. We also illustrate the utility of this\nanalysis for designing neural networks for policy learning.",
            "author": [
                "Jiayuan Mao",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Joshua B. Tenenbaum",
                "Leslie Pack Kaelbling"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03682v1",
                "http://arxiv.org/pdf/2312.03682v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03670v1",
            "title": "Multicolor bipartite Ramsey number of double stars",
            "updated": "2023-12-06T18:35:36Z",
            "published": "2023-12-06T18:35:36Z",
            "summary": "For positive integers $n, m$, the double star $S(n,m)$ is the graph\nconsisting of the disjoint union of two stars $K_{1,n}$ and $K_{1,m}$ together\nwith an edge joining their centers. Finding monochromatic copies of double\nstars in edge-colored complete bipartite graphs has attracted much attention.\nThe $k$-color bipartite Ramsey number of $ S(n,m)$, denoted by\n$r_{bip}(S(n,m);k)$, is the smallest integer $N$ such that, in any $k$-coloring\nof the edges of the complete bipartite graph $K_{N,N}$, there is a\nmonochromatic copy of $S(n,m)$. The study of bipartite Ramsey numbers was\ninitiated in the early 1970s by Faudree and Schelp and, independently, by\nGy\\'arf\\'as and Lehel. The exact value of $r_{bip}(S(n,m);k)$ is only known\nwhen $n=m=1$. Applying the Tur\\'an argument in the bipartite setting, here we\nprove that if $k=2$ and $n\\ge m$, or $k\\ge3$ and $n\\ge 2m$, then \\[\nr_{bip}(S(n,m);k)=kn+1.\\]",
            "author": [
                "Gregory Decamillis",
                "Zi-Xia Song"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03670v1",
                "http://arxiv.org/pdf/2312.03670v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C55, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03668v1",
            "title": "An Integration of Pre-Trained Speech and Language Models for End-to-End\n  Speech Recognition",
            "updated": "2023-12-06T18:34:42Z",
            "published": "2023-12-06T18:34:42Z",
            "summary": "Advances in machine learning have made it possible to perform various text\nand speech processing tasks, including automatic speech recognition (ASR), in\nan end-to-end (E2E) manner. Since typical E2E approaches require large amounts\nof training data and resources, leveraging pre-trained foundation models\ninstead of training from scratch is gaining attention. Although there have been\nattempts to use pre-trained speech and language models in ASR, most of them are\nlimited to using either. This paper explores the potential of integrating a\npre-trained speech representation model with a large language model (LLM) for\nE2E ASR. The proposed model enables E2E ASR by generating text tokens in an\nautoregressive manner via speech representations as speech prompts, taking\nadvantage of the vast knowledge provided by the LLM. Furthermore, the proposed\nmodel can incorporate remarkable developments for LLM utilization, such as\ninference optimization and parameter-efficient domain adaptation. Experimental\nresults show that the proposed model achieves performance comparable to modern\nE2E ASR models.",
            "author": [
                "Yukiya Hono",
                "Koh Mitsuda",
                "Tianyu Zhao",
                "Kentaro Mitsui",
                "Toshiaki Wakatsuki",
                "Kei Sawada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03668v1",
                "http://arxiv.org/pdf/2312.03668v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03664v1",
            "title": "Generative agent-based modeling with actions grounded in physical,\n  social, or digital space using Concordia",
            "updated": "2023-12-06T18:33:50Z",
            "published": "2023-12-06T18:33:50Z",
            "summary": "Agent-based modeling has been around for decades, and applied widely across\nthe social and natural sciences. The scope of this research method is now\npoised to grow dramatically as it absorbs the new affordances provided by Large\nLanguage Models (LLM)s. Generative Agent-Based Models (GABM) are not just\nclassic Agent-Based Models (ABM)s where the agents talk to one another. Rather,\nGABMs are constructed using an LLM to apply common sense to situations, act\n\"reasonably\", recall common semantic knowledge, produce API calls to control\ndigital technologies like apps, and communicate both within the simulation and\nto researchers viewing it from the outside. Here we present Concordia, a\nlibrary to facilitate constructing and working with GABMs. Concordia makes it\neasy to construct language-mediated simulations of physically- or\ndigitally-grounded environments. Concordia agents produce their behavior using\na flexible component system which mediates between two fundamental operations:\nLLM calls and associative memory retrieval. A special agent called the Game\nMaster (GM), which was inspired by tabletop role-playing games, is responsible\nfor simulating the environment where the agents interact. Agents take actions\nby describing what they want to do in natural language. The GM then translates\ntheir actions into appropriate implementations. In a simulated physical world,\nthe GM checks the physical plausibility of agent actions and describes their\neffects. In digital environments simulating technologies such as apps and\nservices, the GM may handle API calls to integrate with external tools such as\ngeneral AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,\nEmail, Search, etc.). Concordia was designed to support a wide array of\napplications both in scientific research and for evaluating performance of real\ndigital services by simulating users and/or generating synthetic data.",
            "author": [
                "Alexander Sasha Vezhnevets",
                "John P. Agapiou",
                "Avia Aharon",
                "Ron Ziv",
                "Jayd Matyas",
                "Edgar A. Du\u00e9\u00f1ez-Guzm\u00e1n",
                "William A. Cunningham",
                "Simon Osindero",
                "Danny Karmon",
                "Joel Z. Leibo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03664v1",
                "http://arxiv.org/pdf/2312.03664v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03663v1",
            "title": "$H$-percolation with a random $H$",
            "updated": "2023-12-06T18:33:00Z",
            "published": "2023-12-06T18:33:00Z",
            "summary": "In $H$-percolation, we start with an Erd\\H{o}s--R\\'enyi graph ${\\mathcal\nG}_{n,p}$ and then iteratively add edges that complete copies of $H$. The\nprocess percolates if all edges missing from ${\\mathcal G}_{n,p}$ are\neventually added. We find the critical threshold $p_c$ when $H={\\mathcal\nG}_{k,1/2}$ is uniformly random, solving a problem of Balogh, Bollob\\'as and\nMorris.",
            "author": [
                "Zsolt Bartha",
                "Brett Kolesnik",
                "Gal Kronenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03663v1",
                "http://arxiv.org/pdf/2312.03663v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR",
                "05C35, 05C80, 05C99, 60K35, 68Q80"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03648v1",
            "title": "Vertex algebras from divisors on Calabi-Yau threefolds",
            "updated": "2023-12-06T18:09:14Z",
            "published": "2023-12-06T18:09:14Z",
            "summary": "We construct vertex algebras $\\mathbb{V}(Y,S)$ from divisors $S$ on toric\nCalabi-Yau threefolds $Y$, satisfying conjectures of Gaiotto-Rapcak and\nFeigin-Gukov, as the kernel of screening operators on lattice vertex algebras\ndetermined by the GKM graph of $Y$ and a filtration on $\\mathcal{O}_S$. We\nprove that there are representations of $\\mathbb{V}(Y,S)$ on the homology\ngroups of various moduli spaces of coherent sheaves on $Y$ supported on $S$\nconstructed in a companion paper with Rapcak, defined by certain Hecke\nmodifications of these sheaves along points and curve classes in the divisor\n$S$. This generalizes the common mathematical formulation of a conjecture of\nAlday-Gaiotto-Tachikawa, the special case in which $Y=\\mathbb{C}^3$ and\n$S=r[\\mathbb{C}^2]$, to toric threefolds and divisors as proposed by\nGaiotto-Rapcak. We outline an approach to the general conjecture and prove many\nspecial cases and partial results using tools developed in the companion paper,\nfollowing the proof of the original conjecture by Schiffmann-Vasserot and its\ngeneralization to divisors in $\\mathbb{C}^3$ by Rapcak-Soibelman-Yang-Zhao.\n  The vertex algebras $\\mathbb{V}(Y,S)$ conjecturally include $W$-superalgebras\n$ W_{f_0,f_1}^\\kappa(\\mathfrak{gl}_{m|n})$ and genus zero class $\\mathcal{S}$\nchiral algebras $\\mathbb{V}^{\\mathcal{S}}_{\\text{Gl}_m;f_1,...,f_k}$, each for\ngeneral nilpotents $f_i$. By definition, this implies the existence of a family\nof compatible free field realizations of these vertex algebras, relevant to\ntheir parabolic induction and inverse quantum Hamiltonian reduction. We prove\nthese conjectures in the examples of lowest non-trivial rank for each case, and\noutline the proof in general for some cases.",
            "author": [
                "Dylan Butson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03648v1",
                "http://arxiv.org/pdf/2312.03648v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "hep-th",
                "math-ph",
                "math.AG",
                "math.MP",
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03643v1",
            "title": "Propagating moments in probabilistic graphical models for decision\n  support systems",
            "updated": "2023-12-06T17:57:21Z",
            "published": "2023-12-06T17:57:21Z",
            "summary": "Probabilistic graphical models are widely used to model complex systems with\nuncertainty. Traditionally, Gaussian directed graphical models are applied for\nanalysis of large networks with continuous variables since they can provide\nconditional and marginal distributions in closed form simplifying the\ninferential task. The Gaussianity and linearity assumptions are often adequate,\nyet can lead to poor performance when dealing with some practical applications.\nIn this paper, we model each variable in graph G as a polynomial regression of\nits parents to capture complex relationships between individual variables and\nwith utility function of polynomial form. Since the marginal posterior\ndistributions of individual variables can become analytically intractable, we\ndevelop a message-passing algorithm to propagate information throughout the\nnetwork solely using moments which enables the expected utility scores to be\ncalculated exactly. We illustrate how the proposed methodology works in a\ndecision problem in energy systems planning.",
            "author": [
                "Victoria Volodina",
                "Nikki Sonenberg",
                "Peter Challenor",
                "Jim Q. Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03643v1",
                "http://arxiv.org/pdf/2312.03643v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.AP",
                "62C10",
                "G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03642v1",
            "title": "Transformer-Powered Surrogates Close the ICF Simulation-Experiment Gap\n  with Extremely Limited Data",
            "updated": "2023-12-06T17:53:06Z",
            "published": "2023-12-06T17:53:06Z",
            "summary": "Recent advances in machine learning, specifically transformer architecture,\nhave led to significant advancements in commercial domains. These powerful\nmodels have demonstrated superior capability to learn complex relationships and\noften generalize better to new data and problems. This paper presents a novel\ntransformer-powered approach for enhancing prediction accuracy in multi-modal\noutput scenarios, where sparse experimental data is supplemented with\nsimulation data. The proposed approach integrates transformer-based\narchitecture with a novel graph-based hyper-parameter optimization technique.\nThe resulting system not only effectively reduces simulation bias, but also\nachieves superior prediction accuracy compared to the prior method. We\ndemonstrate the efficacy of our approach on inertial confinement fusion\nexperiments, where only 10 shots of real-world data are available, as well as\nsynthetic versions of these experiments.",
            "author": [
                "Matthew L. Olson",
                "Shusen Liu",
                "Jayaraman J. Thiagarajan",
                "Bogdan Kustowski",
                "Weng-Keen Wong",
                "Rushil Anirudh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03642v1",
                "http://arxiv.org/pdf/2312.03642v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03636v1",
            "title": "Fed-urlBERT: Client-side Lightweight Federated Transformers for URL\n  Threat Analysis",
            "updated": "2023-12-06T17:31:16Z",
            "published": "2023-12-06T17:31:16Z",
            "summary": "In evolving cyber landscapes, the detection of malicious URLs calls for\ncooperation and knowledge sharing across domains. However, collaboration is\noften hindered by concerns over privacy and business sensitivities. Federated\nlearning addresses these issues by enabling multi-clients collaboration without\ndirect data exchange. Unfortunately, if highly expressive Transformer models\nare used, clients may face intolerable computational burdens, and the exchange\nof weights could quickly deplete network bandwidth. In this paper, we propose\nFed-urlBERT, a federated URL pre-trained model designed to address both privacy\nconcerns and the need for cross-domain collaboration in cybersecurity.\nFed-urlBERT leverages split learning to divide the pre-training model into\nclient and server part, so that the client part takes up less extensive\ncomputation resources and bandwidth. Our appraoch achieves performance\ncomparable to centralized model under both independently and identically\ndistributed (IID) and two non-IID data scenarios. Significantly, our federated\nmodel shows about an 7% decrease in the FPR compared to the centralized model.\nAdditionally, we implement an adaptive local aggregation strategy that\nmitigates heterogeneity among clients, demonstrating promising performance\nimprovements. Overall, our study validates the applicability of the proposed\nTransformer federated learning for URL threat analysis, establishing a\nfoundation for real-world collaborative cybersecurity efforts. The source code\nis accessible at https://github.com/Davidup1/FedURLBERT.",
            "author": [
                "Yujie Li",
                "Yanbin Wang",
                "Haitao Xu",
                "Zhenhao Guo",
                "Fan Zhang",
                "Ruitong Liu",
                "Wenrui Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03636v1",
                "http://arxiv.org/pdf/2312.03636v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03633v1",
            "title": "Not All Large Language Models (LLMs) Succumb to the \"Reversal Curse\": A\n  Comparative Study of Deductive Logical Reasoning in BERT and GPT Models",
            "updated": "2023-12-06T17:29:45Z",
            "published": "2023-12-06T17:29:45Z",
            "summary": "The \"Reversal Curse\" refers to the scenario where auto-regressive decoder\nlarge language models (LLMs), such as ChatGPT, trained on \"A is B\" fail to\nlearn \"B is A\", demonstrating a basic failure of logical deduction. This raises\na red flag in the use of GPT models for certain general tasks such as\nconstructing knowledge graphs, considering their adherence to this symmetric\nprinciple. In our study, we examined a bidirectional LLM, BERT, and found that\nit is immune to the reversal curse. Driven by ongoing efforts to construct\nbiomedical knowledge graphs with LLMs, we also embarked on evaluating more\ncomplex but essential deductive reasoning capabilities. This process included\nfirst training encoder and decoder language models to master the intersection\n($\\cap$) and union ($\\cup$) operations on two sets and then moving on to assess\ntheir capability to infer different combinations of union ($\\cup$) and\nintersection ($\\cap$) operations on three newly created sets. The findings\nshowed that while both encoder and decoder language models, trained for tasks\ninvolving two sets (union/intersection), were proficient in such scenarios,\nthey encountered difficulties when dealing with operations that included three\nsets (various combinations of union and intersection). Our research highlights\nthe distinct characteristics of encoder and decoder models in simple and\ncomplex logical reasoning. In practice, the choice between BERT and GPT should\nbe guided by the specific requirements and nature of the task at hand,\nleveraging their respective strengths in bidirectional context comprehension\nand sequence prediction.",
            "author": [
                "Jingye Yang",
                "Da Wu",
                "Kai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03633v1",
                "http://arxiv.org/pdf/2312.03633v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03613v1",
            "title": "Augmenting optimization-based molecular design with graph neural\n  networks",
            "updated": "2023-12-06T16:56:38Z",
            "published": "2023-12-06T16:56:38Z",
            "summary": "Computer-aided molecular design (CAMD) studies quantitative\nstructure-property relationships and discovers desired molecules using\noptimization algorithms. With the emergence of machine learning models, CAMD\nscore functions may be replaced by various surrogates to automatically learn\nthe structure-property relationships. Due to their outstanding performance on\ngraph domains, graph neural networks (GNNs) have recently appeared frequently\nin CAMD. But using GNNs introduces new optimization challenges. This paper\nformulates GNNs using mixed-integer programming and then integrates this GNN\nformulation into the optimization and machine learning toolkit OMLT. To\ncharacterize and formulate molecules, we inherit the well-established\nmixed-integer optimization formulation for CAMD and propose symmetry-breaking\nconstraints to remove symmetric solutions caused by graph isomorphism. In two\ncase studies, we investigate fragment-based odorant molecular design with more\npractical requirements to test the compatibility and performance of our\napproaches.",
            "author": [
                "Shiqiang Zhang",
                "Juan S. Campos",
                "Christian Feldmann",
                "Frederik Sandfort",
                "Miriam Mathea",
                "Ruth Misener"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03613v1",
                "http://arxiv.org/pdf/2312.03613v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03573v1",
            "title": "Data-driven Wasserstein distributionally robust Nash equilibrium\n  problems",
            "updated": "2023-12-06T16:07:20Z",
            "published": "2023-12-06T16:07:20Z",
            "summary": "We study stochastic Nash equilibrium problems subject to heterogeneous\nuncertainty on the cost functions of the individual agents. In our setting, we\nassume no prior knowledge of the underlying probability distribution of the\nuncertain variables. Adopting a data-driven distributionally robust approach,\nbased on the so-called Wasserstein metric, where each agent constructs their\nown ambiguity set, we provide, under mild assumptions, finite sample guarantees\non the probability that the data-driven distributionally robust Nash\nequilibrium is also robust with respect to the true probability distributions\nwith high confidence. Furthermore, by recasting the game as a distributionally\nrobust variational inequality, under appropriate conditions, we establish\nalmost sure asymptotic convergence of the set of data-driven distributionally\nrobust equilibria to the solution set of the original game. Finally, we recast\nthe distributionally robust Nash game as a finite-dimensional Nash equilibrium\nproblem. We illustrate the proposed distributionally robust reformulation via\nnumerical experiments of stochastic Nash-Cournot games.",
            "author": [
                "George Pantazis",
                "Barbara Franci",
                "Sergio Grammatico"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03573v1",
                "http://arxiv.org/pdf/2312.03573v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03555v1",
            "title": "Enabling Edge Artificial Intelligence via Goal-oriented Deep Neural\n  Network Splitting",
            "updated": "2023-12-06T15:38:53Z",
            "published": "2023-12-06T15:38:53Z",
            "summary": "Deep Neural Network (DNN) splitting is one of the key enablers of edge\nArtificial Intelligence (AI), as it allows end users to pre-process data and\noffload part of the computational burden to nearby Edge Cloud Servers (ECSs).\nThis opens new opportunities and degrees of freedom in balancing energy\nconsumption, delay, accuracy, privacy, and other trustworthiness metrics. In\nthis work, we explore the opportunity of DNN splitting at the edge of 6G\nwireless networks to enable low energy cooperative inference with target delay\nand accuracy with a goal-oriented perspective. Going beyond the current\nliterature, we explore new trade-offs that take into account the accuracy\ndegradation as a function of the Splitting Point (SP) selection and wireless\nchannel conditions. Then, we propose an algorithm that dynamically controls SP\nselection, local computing resources, uplink transmit power and bandwidth\nallocation, in a goal-oriented fashion, to meet a target goal-effectiveness. To\nthe best of our knowledge, this is the first work proposing adaptive SP\nselection on the basis of all learning performance (i.e., energy, delay,\naccuracy), with the aim of guaranteeing the accomplishment of a goal (e.g.,\nminimize the energy consumption under latency and accuracy constraints).\nNumerical results show the advantages of the proposed SP selection and resource\nallocation, to enable energy frugal and effective edge AI.",
            "author": [
                "Francesco Binucci",
                "Mattia Merluzzi",
                "Paolo Banelli",
                "Emilio Calvanese Strinati",
                "Paolo Di Lorenzo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03555v1",
                "http://arxiv.org/pdf/2312.03555v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03535v1",
            "title": "On the geometry of the free factor graph for ${\\rm{Aut}}(F_N)$",
            "updated": "2023-12-06T14:59:28Z",
            "published": "2023-12-06T14:59:28Z",
            "summary": "Let $\\Phi$ be a pseudo-Anosov diffeomorphism of a compact (possibily\nnon-orientable) surface $\\Sigma$ with one boundary component. We show that if\n$b \\in \\pi_1(\\Sigma)$ is the boundary word, $\\phi \\in\n{\\rm{Aut}}(\\pi_1(\\Sigma))$ is a representative of $\\Phi$ fixing $b$, and\n${\\rm{ad}}_b$ denotes conjugation by $b$, then the orbits of $\\langle \\phi,\n{\\rm{ad}}_b \\rangle\\cong\\mathbb{Z}^2$ in the graph of free factors of\n$\\pi_1(\\Sigma)$ are quasi-isometrically embedded. It follows that for $N \\geq\n2$ the free factor graph for ${\\rm{Aut}}(F_N)$ is not hyperbolic, in contrast\nto the ${\\rm{Out}}(F_N)$ case.",
            "author": [
                "Mladen Bestvina",
                "Martin R. Bridson",
                "Richard D. Wade"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03535v1",
                "http://arxiv.org/pdf/2312.03535v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.GR",
                "20F65, 20E05 (Primary) 20E36, 51F30 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03534v1",
            "title": "Validation and benchmarking of quantum annealing technology",
            "updated": "2023-12-06T14:56:45Z",
            "published": "2023-12-06T14:56:45Z",
            "summary": "In this thesis, we focus on the problem of validating and benchmarking\nquantum annealers. To this end, we propose two algorithms for solving\nreal-world problems and test how they perform on the current generation of\nquantum annealers. The first algorithm allows for solving the dynamics of\nquantum systems (or, in fact, any dynamical systems). The second of the\nproposed algorithms is suitable for solving a particular family of railway\ndispatching problems. We assess the performance of those algorithms on the\ncurrent generation of D-Wave quantum annealers with the assistance of two\nnovel, classical strategies for solving an Ising model also presented in the\nthesis. The first, tensor network-based approach is a heuristic algorithm\ntailored for solving instances defined on Chimera-like graphs, thus making it\nideal for providing a baseline with which the results from physical annealers\ncan be compared. The other presented approach is a massively parallel\nimplementation of the exhaustive (brute-force) search through the whole\nsolution space. Although the brute-force approach is limited to moderate\ninstance sizes, it has the advantage of being able to compute the low energy\nspectrum and certify the solutions. Our results suggest that present-day\nquantum annealers are able to solve a subset of the aforementioned problems. In\nparticular, we show that the D-Wave annealers are capable of capturing the\ndynamics of a simple quantum system in a specific regime of parameters, and can\nbe used to obtain good-quality solutions for instances of railway conflict\nmanagement problems. Finally, our findings indicate that the current generation\nof D-Wave annealers is far from perfect. We discuss problem instances for which\nthe annealers failed to find a good or even feasible solution. We also provide,\nwhere possible, a plausible explanation of why some of the presented problems\nmight be hard for the annealers.",
            "author": [
                "Konrad Ja\u0142owiecki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03534v1",
                "http://arxiv.org/pdf/2312.03534v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03798v1",
            "title": "Single Image Reflection Removal with Reflection Intensity Prior\n  Knowledge",
            "updated": "2023-12-06T14:52:11Z",
            "published": "2023-12-06T14:52:11Z",
            "summary": "Single Image Reflection Removal (SIRR) in real-world images is a challenging\ntask due to diverse image degradations occurring on the glass surface during\nlight transmission and reflection. Many existing methods rely on specific prior\nassumptions to resolve the problem. In this paper, we propose a general\nreflection intensity prior that captures the intensity of the reflection\nphenomenon and demonstrate its effectiveness. To learn the reflection intensity\nprior, we introduce the Reflection Prior Extraction Network (RPEN). By\nsegmenting images into regional patches, RPEN learns non-uniform reflection\nprior in an image. We propose Prior-based Reflection Removal Network (PRRN)\nusing a simple transformer U-Net architecture that adapts reflection prior fed\nfrom RPEN. Experimental results on real-world benchmarks demonstrate the\neffectiveness of our approach achieving state-of-the-art accuracy in SIRR.",
            "author": [
                "Dongshen Han",
                "Seungkyu Lee",
                "Chaoning Zhang",
                "Heechan Yoon",
                "Hyukmin Kwon",
                "HyunCheol Kim",
                "HyonGon Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03798v1",
                "http://arxiv.org/pdf/2312.03798v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03795v1",
            "title": "AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and\n  Reconstruction with Canonical Score Distillation",
            "updated": "2023-12-06T14:13:54Z",
            "published": "2023-12-06T14:13:54Z",
            "summary": "Text-to-3D model adaptations have advanced static 3D model quality, but\nsequential 3D model generation, particularly for animatable objects with large\nmotions, is still scarce. Our work proposes AnimatableDreamer, a text-to-4D\ngeneration framework capable of generating diverse categories of non-rigid\nobjects while adhering to the object motions extracted from a monocular video.\nAt its core, AnimatableDreamer is equipped with our novel optimization design\ndubbed Canonical Score Distillation (CSD), which simplifies the generation\ndimension from 4D to 3D by denoising over different frames in the time-varying\ncamera spaces while conducting the distillation process in a unique canonical\nspace shared per video. Concretely, CSD ensures that score gradients\nback-propagate to the canonical space through differentiable warping, hence\nguaranteeing the time-consistent generation and maintaining morphological\nplausibility across different poses. By lifting the 3D generator to 4D with\nwarping functions, AnimatableDreamer offers a novel perspective on non-rigid 3D\nmodel generation and reconstruction. Besides, with inductive knowledge from a\nmulti-view consistent diffusion model, CSD regularizes reconstruction from\nnovel views, thus cyclically enhancing the generation process. Extensive\nexperiments demonstrate the capability of our method in generating\nhigh-flexibility text-guided 3D models from the monocular video, while also\nshowing improved reconstruction performance over typical non-rigid\nreconstruction methods. Project page https://AnimatableDreamer.github.io.",
            "author": [
                "Xinzhou Wang",
                "Yikai Wang",
                "Junliang Ye",
                "Zhengyi Wang",
                "Fuchun Sun",
                "Pengkun Liu",
                "Ling Wang",
                "Kai Sun",
                "Xintong Wang",
                "Bin He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03795v1",
                "http://arxiv.org/pdf/2312.03795v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "I.4.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03488v1",
            "title": "Modeling Aggregate Downwash Forces for Dense Multirotor Flight",
            "updated": "2023-12-06T13:30:49Z",
            "published": "2023-12-06T13:30:49Z",
            "summary": "Dense formation flight with multirotor swarms is a powerful, nature-inspired\nflight regime with numerous applications in the realworld. However, when\nmultirotors fly in close vertical proximity to each other, the propeller\ndownwash from the vehicles can have a destabilising effect on each other.\nUnfortunately, even in a homogeneous team, an accurate model of downwash forces\nfrom one vehicle is unlikely to be sufficient for predicting aggregate forces\nfrom multiple vehicles in formation.\n  In this work, we model the interaction patterns produced by one or more\nvehicles flying in close proximity to an ego-vehicle. We first present an\nexperimental test rig designed to capture 6-DOF exogenic forces acting on a\nmultirotor frame. We then study and characterize these measured forces as a\nfunction of the relative states of two multirotors flying various patterns in\nits vicinity.\n  Our analysis captures strong non-linearities present in the aggregation of\nthese interactions. Then, by modeling the formation as a graph, we present a\nnovel approach for learning the force aggregation function, and contrast it\nagainst simpler linear models. Finally, we explore how our proposed models\ngeneralize when a fourth vehicle is added to the formation.",
            "author": [
                "Jennifer Gielis",
                "Ajay Shankar",
                "Ryan Kortvelesy",
                "Amanda Prorok"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03488v1",
                "http://arxiv.org/pdf/2312.03488v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03485v1",
            "title": "Precision of Individual Shapley Value Explanations",
            "updated": "2023-12-06T13:29:23Z",
            "published": "2023-12-06T13:29:23Z",
            "summary": "Shapley values are extensively used in explainable artificial intelligence\n(XAI) as a framework to explain predictions made by complex machine learning\n(ML) models. In this work, we focus on conditional Shapley values for\npredictive models fitted to tabular data and explain the prediction\n$f(\\boldsymbol{x}^{*})$ for a single observation $\\boldsymbol{x}^{*}$ at the\ntime. Numerous Shapley value estimation methods have been proposed and\nempirically compared on an average basis in the XAI literature. However, less\nfocus has been devoted to analyzing the precision of the Shapley value\nexplanations on an individual basis. We extend our work in Olsen et al. (2023)\nby demonstrating and discussing that the explanations are systematically less\nprecise for observations on the outer region of the training data distribution\nfor all used estimation methods. This is expected from a statistical point of\nview, but to the best of our knowledge, it has not been systematically\naddressed in the Shapley value literature. This is crucial knowledge for\nShapley values practitioners, who should be more careful in applying these\nobservations' corresponding Shapley value explanations.",
            "author": [
                "Lars Henry Berge Olsen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03485v1",
                "http://arxiv.org/pdf/2312.03485v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.AP",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03480v1",
            "title": "AMR Parsing is Far from Solved: GrAPES, the Granular AMR Parsing\n  Evaluation Suite",
            "updated": "2023-12-06T13:19:56Z",
            "published": "2023-12-06T13:19:56Z",
            "summary": "We present the Granular AMR Parsing Evaluation Suite (GrAPES), a challenge\nset for Abstract Meaning Representation (AMR) parsing with accompanying\nevaluation metrics. AMR parsers now obtain high scores on the standard AMR\nevaluation metric Smatch, close to or even above reported inter-annotator\nagreement. But that does not mean that AMR parsing is solved; in fact, human\nevaluation in previous work indicates that current parsers still quite\nfrequently make errors on node labels or graph structure that substantially\ndistort sentence meaning. Here, we provide an evaluation suite that tests AMR\nparsers on a range of phenomena of practical, technical, and linguistic\ninterest. Our 36 categories range from seen and unseen labels, to structural\ngeneralization, to coreference. GrAPES reveals in depth the abilities and\nshortcomings of current AMR parsers.",
            "author": [
                "Jonas Groschwitz",
                "Shay B. Cohen",
                "Lucia Donatelli",
                "Meaghan Fowlie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03480v1",
                "http://arxiv.org/pdf/2312.03480v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "J.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03477v1",
            "title": "From Detection to Action Recognition: An Edge-Based Pipeline for Robot\n  Human Perception",
            "updated": "2023-12-06T13:10:02Z",
            "published": "2023-12-06T13:10:02Z",
            "summary": "Mobile service robots are proving to be increasingly effective in a range of\napplications, such as healthcare, monitoring Activities of Daily Living (ADL),\nand facilitating Ambient Assisted Living (AAL). These robots heavily rely on\nHuman Action Recognition (HAR) to interpret human actions and intentions.\nHowever, for HAR to function effectively on service robots, it requires prior\nknowledge of human presence (human detection) and identification of individuals\nto monitor (human tracking). In this work, we propose an end-to-end pipeline\nthat encompasses the entire process, starting from human detection and\ntracking, leading to action recognition. The pipeline is designed to operate in\nnear real-time while ensuring all stages of processing are performed on the\nedge, reducing the need for centralised computation. To identify the most\nsuitable models for our mobile robot, we conducted a series of experiments\ncomparing state-of-the-art solutions based on both their detection performance\nand efficiency. To evaluate the effectiveness of our proposed pipeline, we\nproposed a dataset comprising daily household activities. By presenting our\nfindings and analysing the results, we demonstrate the efficacy of our approach\nin enabling mobile robots to understand and respond to human behaviour in\nreal-world scenarios relying mainly on the data from their RGB cameras.",
            "author": [
                "Petros Toupas",
                "Georgios Tsamis",
                "Dimitrios Giakoumis",
                "Konstantinos Votis",
                "Dimitrios Tzovaras"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03477v1",
                "http://arxiv.org/pdf/2312.03477v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03461v2",
            "title": "HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian\n  Splatting",
            "updated": "2023-12-07T12:46:07Z",
            "published": "2023-12-06T12:36:53Z",
            "summary": "We have recently seen tremendous progress in photo-real human modeling and\nrendering. Yet, efficiently rendering realistic human performance and\nintegrating it into the rasterization pipeline remains challenging. In this\npaper, we present HiFi4G, an explicit and compact Gaussian-based approach for\nhigh-fidelity human performance rendering from dense footage. Our core\nintuition is to marry the 3D Gaussian representation with non-rigid tracking,\nachieving a compact and compression-friendly representation. We first propose a\ndual-graph mechanism to obtain motion priors, with a coarse deformation graph\nfor effective initialization and a fine-grained Gaussian graph to enforce\nsubsequent constraints. Then, we utilize a 4D Gaussian optimization scheme with\nadaptive spatial-temporal regularizers to effectively balance the non-rigid\nprior and Gaussian updating. We also present a companion compression scheme\nwith residual compensation for immersive experiences on various platforms. It\nachieves a substantial compression rate of approximately 25 times, with less\nthan 2MB of storage per frame. Extensive experiments demonstrate the\neffectiveness of our approach, which significantly outperforms existing\napproaches in terms of optimization speed, rendering quality, and storage\noverhead.",
            "author": [
                "Yuheng Jiang",
                "Zhehao Shen",
                "Penghao Wang",
                "Zhuo Su",
                "Yu Hong",
                "Yingliang Zhang",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03461v2",
                "http://arxiv.org/pdf/2312.03461v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03451v1",
            "title": "An efficient data-based off-policy Q-learning algorithm for optimal\n  output feedback control of linear systems",
            "updated": "2023-12-06T12:21:01Z",
            "published": "2023-12-06T12:21:01Z",
            "summary": "In this paper, we present a Q-learning algorithm to solve the optimal output\nregulation problem for discrete-time LTI systems. This off-policy algorithm\nonly relies on using persistently exciting input-output data, measured offline.\nNo model knowledge or state measurements are needed and the obtained optimal\npolicy only uses past input-output information. Moreover, our formulation of\nthe proposed algorithm renders it computationally efficient. We provide\nconditions that guarantee the convergence of the algorithm to the optimal\nsolution. Finally, the performance of our method is compared to existing\nalgorithms in the literature.",
            "author": [
                "Mohammad Alsalti",
                "Victor G. Lopez",
                "Matthias A. M\u00fcller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03451v1",
                "http://arxiv.org/pdf/2312.03451v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03439v1",
            "title": "The Table of the Structure Constants for the Complex Simple Lie Algebra\n  of Type F_4 and its Application to the Calculation of Commutators in the\n  Chevalley Group of Type F_4 over Fields and Rings",
            "updated": "2023-12-06T11:44:12Z",
            "published": "2023-12-06T11:44:12Z",
            "summary": "This work is the first in a series of papers devoted to constructing tables\nof structure constants for the complex simple Lie algebras and to finding an\nexplicit form of Chevalley commutator formulas.\n  The work consists of three parts. In the first part, expressions are found\nfor the structure constants of the complex simple Lie algebra of type F_4 in\nthe form of functions of structure constants corresponding to extraspecial\npairs of roots. As a consequence, all Chevalley commutator formulas\n[x_r(u),x_s(y)] are calculated when the sum r+s is a root.\n  Further, in the second part, tables of structure constants and Chevalley\ncommutator formulas are given in the special case when all constants\ncorresponding to extraspecial pairs are equal to one.\n  Finally, in the third part, directed and weighted graphs associated with root\nsystems are constructed. It is shown that the elements of the exponent of the\nadjacency matrices of directed graphs are the numbers P_{rs}, where P_{rs} is\nthe number of representations of the root r in the form of a sum of the root s\nand fundamental roots such that any initial segment of the sum is a root. It is\nalso shown that the elements of an exponent of the weight matrix of a weighted\ngraph are the values of sums arising when calculating complex commutators in\nChevalley groups.",
            "author": [
                "Sergey G. Kolesnikov",
                "Anna I. Polovinkina"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03439v1",
                "http://arxiv.org/pdf/2312.03439v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03436v1",
            "title": "Beyond Low Rank: A Graph-Based Propagation Approach to Tensor Completion\n  for Multi-Acquisition Scenarios",
            "updated": "2023-12-06T11:37:25Z",
            "published": "2023-12-06T11:37:25Z",
            "summary": "Tensor completion refers to the problem of recovering the missing, corrupted\nor unobserved entries in data represented by tensors. In this paper, we tackle\nthe tensor completion problem in the scenario in which multiple tensor\nacquisitions are available and do so without placing constraints on the\nunderlying tensor's rank. Whereas previous tensor completion work primarily\nfocuses on low-rank completion methods, we propose a novel graph-based\ndiffusion approach to the problem. Referred to as GraphProp, the method\npropagates observed entries around a graph-based representation of the tensor\nin order to recover the missing entries. A series of experiments have been\nperformed to validate the presented approach, including a\nsynthetically-generated tensor recovery experiment which shows that the method\ncan be used to recover both low and high rank tensor entries. The successful\ntensor completion capabilities of the approach are also demonstrated on a\nreal-world completion problem from the field of multispectral remote sensing\ncompletion. Using data acquired from the Landsat 7 platform, we synthetically\nobscure image sections in order to simulate the scenario in which image\nacquisitions overlap only partially. In these tests, we benchmark against\nalternative tensor completion approaches as well as existing graph signal\nrecovery methods, demonstrating the superior reconstruction performance of our\nmethod versus the state of the art.",
            "author": [
                "Iain Rolland",
                "Sivasakthy Selvakumaran",
                "Andrea Marinoni"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03436v1",
                "http://arxiv.org/pdf/2312.03436v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03435v1",
            "title": "Counting Butterflies in Fully Dynamic Bipartite Graph Streams",
            "updated": "2023-12-06T11:36:15Z",
            "published": "2023-12-06T11:36:15Z",
            "summary": "A bipartite graph extensively models relationships between real-world\nentities of two different types, such as user-product data in e-commerce. Such\ngraph data are inherently becoming more and more streaming, entailing\ncontinuous insertions and deletions of edges. A butterfly (i.e., 2x2 bi-clique)\nis the smallest non-trivial cohesive structure that plays a crucial role.\nCounting such butterfly patterns in streaming bipartite graphs is a core\nproblem in applications such as dense subgraph discovery and anomaly detection.\nYet, existing approximate solutions consider insert-only streams and, thus,\nachieve very low accuracy in fully dynamic bipartite graph streams that involve\nboth insertions and deletions of edges. Adapting them to consider deletions is\nnot trivial either, because different sampling schemes and new accuracy\nanalyses are required. In this paper, we propose Abacus, a novel approximate\nalgorithm that counts butterflies in the presence of both insertions and\ndeletions by utilizing sampling. We prove that Abacus always delivers unbiased\nestimates of low variance. Furthermore, we extend Abacus and devise a parallel\nmini-batch variant, namely, Parabacus, which counts butterflies in parallel.\nParabacus counts butterflies in a load-balanced manner using versioned samples,\nwhich results in significant speedup and is thus ideal for critical\napplications in the streaming environment. We evaluate Abacus/Parabacus using a\ndiverse set of real bipartite graphs and assess its performance in terms of\naccuracy, throughput, and speedup. The results indicate that our proposal is\nthe first capable of efficiently providing accurate butterfly counts in the\nmost generic setting, i.e., a fully dynamic graph streaming environment that\nentails both insertions and deletions. It does so without sacrificing\nthroughput and even improving it with the parallel version.",
            "author": [
                "Serafeim Papadias",
                "Zoi Kaoudi",
                "Varun Pandey",
                "Jorge-Arnulfo Quiane-Ruiz",
                "Volker Markl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03435v1",
                "http://arxiv.org/pdf/2312.03435v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03427v1",
            "title": "Latent State Space Extension for interpretable hybrid mechanistic models",
            "updated": "2023-12-06T11:19:24Z",
            "published": "2023-12-06T11:19:24Z",
            "summary": "Mechanistic growth models play a major role in bioprocess engineering,\ndesign, and control. Their reasonable predictive power and their high level of\ninterpretability make them an essential tool for computer aided engineering\nmethods. Additionally, since they contain knowledge about cell physiology, the\nparameter estimates provide meaningful insights into the metabolism of the\nmicroorganism under study. However, the assumption of time invariance of the\nmodel parameters is often violated in real experiments, limiting their capacity\nto fully explain the observed dynamics. In this work, we propose a framework\nfor identifying such violations and producing insights into misspecified\nmechanisms. The framework achieves this by allowing kinetic and process\nparameters to vary in time. We demonstrate the framework's capabilities by\nfitting a hybrid model based on a simple mechanistic growth model for E. coli\nwith data generated in-silico by a much more complex one and identifying\nmissing kinetics.",
            "author": [
                "Judit Aizpuru",
                "Maxim Borisyak",
                "Peter Neubauer",
                "M. Nicolas Cruz Bournazou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03427v1",
                "http://arxiv.org/pdf/2312.03427v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03415v1",
            "title": "Run LoRA Run: Faster and Lighter LoRA Implementations",
            "updated": "2023-12-06T10:54:34Z",
            "published": "2023-12-06T10:54:34Z",
            "summary": "LoRA is a technique that reduces the number of trainable parameters in a\nneural network by introducing low-rank adapters to linear layers. This\ntechnique is used both for fine-tuning (LoRA, QLoRA) and full train (ReLoRA).\nThis paper presents the RunLoRA framework for efficient implementations of LoRA\nthat significantly improves the speed of neural network training and\nfine-tuning using low-rank adapters. The proposed implementation optimizes the\ncomputation of LoRA operations based on dimensions of corresponding linear\nlayer, layer input dimensions and lora rank by choosing best forward and\nbackward computation graph based on FLOPs and time estimations, resulting in\nfaster training without sacrificing accuracy. The experimental results show up\nto 17% speedup on Llama family of models.",
            "author": [
                "Daria Cherniuk",
                "Aleksandr Mikhalev",
                "Ivan Oseledets"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03415v1",
                "http://arxiv.org/pdf/2312.03415v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03399v1",
            "title": "Connected Dominating Sets in Triangulations",
            "updated": "2023-12-06T10:20:25Z",
            "published": "2023-12-06T10:20:25Z",
            "summary": "We show that every $n$-vertex triangulation has a connected dominating set of\nsize at most $10n/21$. Equivalently, every $n$ vertex triangulation has a\nspanning tree with at least $11n/21$ leaves. Prior to the current work, the\nbest known bounds were $n/2$, which follows from work of Albertson, Berman,\nHutchinson, and Thomassen (J. Graph Theory \\textbf{14}(2):247--258). One\nimmediate consequence of this result is an improved bound for the SEFENOMAP\ngraph drawing problem of Angelini, Evans, Frati, and Gudmundsson (J. Graph\nTheory \\textbf{82}(1):45--64). As a second application, we show that every\n$n$-vertex planar graph has a one-bend non-crossing drawing in which some set\nof at least $11n/21$ vertices is drawn on the $x$-axis.",
            "author": [
                "Prosenjit Bose",
                "Vida Dujmovi\u0107",
                "Hussein Houdrouge",
                "Pat Morin",
                "Saeed Odak"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03399v1",
                "http://arxiv.org/pdf/2312.03399v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03391v1",
            "title": "Action Scene Graphs for Long-Form Understanding of Egocentric Videos",
            "updated": "2023-12-06T10:01:43Z",
            "published": "2023-12-06T10:01:43Z",
            "summary": "We present Egocentric Action Scene Graphs (EASGs), a new representation for\nlong-form understanding of egocentric videos. EASGs extend standard\nmanually-annotated representations of egocentric videos, such as verb-noun\naction labels, by providing a temporally evolving graph-based description of\nthe actions performed by the camera wearer, including interacted objects, their\nrelationships, and how actions unfold in time. Through a novel annotation\nprocedure, we extend the Ego4D dataset by adding manually labeled Egocentric\nAction Scene Graphs offering a rich set of annotations designed for long-from\negocentric video understanding. We hence define the EASG generation task and\nprovide a baseline approach, establishing preliminary benchmarks. Experiments\non two downstream tasks, egocentric action anticipation and egocentric activity\nsummarization, highlight the effectiveness of EASGs for long-form egocentric\nvideo understanding. We will release the dataset and the code to replicate\nexperiments and annotations.",
            "author": [
                "Ivan Rodin",
                "Antonino Furnari",
                "Kyle Min",
                "Subarna Tripathi",
                "Giovanni Maria Farinella"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03391v1",
                "http://arxiv.org/pdf/2312.03391v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03384v1",
            "title": "Domination of subcubic planar graphs with large girth",
            "updated": "2023-12-06T09:50:58Z",
            "published": "2023-12-06T09:50:58Z",
            "summary": "Since Reed conjectured in 1996 that the domination number of a connected\ncubic graph of order $n$ is at most $\\lceil \\frac13 n \\rceil$, the domination\nnumber of cubic graphs has been extensively studied. It is now known that the\nconjecture is false in general, but Henning and Dorbec showed that it holds for\ngraphs with girth at least $9$. Zhu and Wu stated an analogous conjecture for\n2-connected cubic planar graphs.\n  In this paper, we present a new upper bound for the domination number of\nsubcubic planar graphs: if $G$ is a subcubic planar graph with girth at least\n8, then $\\gamma(G) < n_0 + \\frac{3}{4} n_1 + \\frac{11}{20} n_2 + \\frac{7}{20}\nn_3$, where $n_i$ denotes the number of vertices in $G$ of degree $i$, for $i\n\\in \\{0,1,2,3\\}$. We also prove that if $G$ is a subcubic planar graph with\ngirth at least 9, then $\\gamma(G) < n_0 + \\frac{13}{17} n_1 + \\frac{9}{17} n_2\n+ \\frac{6}{17} n_3$.",
            "author": [
                "Eun-Kyung Cho",
                "Eric Culver",
                "Stephen G. Hartke",
                "Vesna Ir\u0161i\u010d"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03384v1",
                "http://arxiv.org/pdf/2312.03384v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03371v1",
            "title": "Understanding Concepts in Graph Signal Processing for Neurophysiological\n  Signal Analysis",
            "updated": "2023-12-06T09:12:29Z",
            "published": "2023-12-06T09:12:29Z",
            "summary": "Multivariate signals, which are measured simultaneously over time and\nacquired by sensor networks, are becoming increasingly common. The emerging\nfield of graph signal processing (GSP) promises to analyse spectral\ncharacteristics of these multivariate signals, while at the same time taking\nthe spatial structure between the time signals into account. A central idea in\nGSP is the graph Fourier transform, which projects a multivariate signal onto\nfrequency-ordered graph Fourier modes, and can therefore be regarded as a\nspatial analog of the temporal Fourier transform. This chapter derives and\ndiscusses key concepts in GSP, with a specific focus on how the various\nconcepts relate to one another. The experimental section focuses on the role of\ngraph frequency in data classification, with applications to neuroimaging. To\naddress the limited sample size of neurophysiological datasets, we introduce a\nminimalist simulation framework that can generate arbitrary amounts of data.\nUsing this artificial data, we find that lower graph frequency signals are less\nsuitable for classifying neurophysiological data as compared to higher graph\nfrequency signals. Finally, we introduce a baseline testing framework for GSP.\nEmploying this framework, our results suggest that GSP applications may\nattenuate spectral characteristics in the signals, highlighting current\nlimitations of GSP for neuroimaging.",
            "author": [
                "Stephan Goerttler",
                "Fei He",
                "Min Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03371v1",
                "http://arxiv.org/pdf/2312.03371v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03360v1",
            "title": "Teaching Specific Scientific Knowledge into Large Language Models\n  through Additional Training",
            "updated": "2023-12-06T08:55:55Z",
            "published": "2023-12-06T08:55:55Z",
            "summary": "Through additional training, we explore embedding specialized scientific\nknowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that\neffective knowledge integration requires reading texts from multiple\nperspectives, especially in instructional formats. We utilize text augmentation\nto tackle the scarcity of specialized texts, including style conversions and\ntranslations. Hyperparameter optimization proves crucial, with different size\nmodels (7b, 13b, and 70b) reasonably undergoing additional training. Validating\nour methods, we construct a dataset of 65,000 scientific papers. Although we\nhave succeeded in partially embedding knowledge, the study highlights the\ncomplexities and limitations of incorporating specialized information into\nLLMs, suggesting areas for further improvement.",
            "author": [
                "Kan Hatakeyama-Sato",
                "Yasuhiko Igarashi",
                "Shun Katakami",
                "Yuta Nabae",
                "Teruaki Hayakawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03360v1",
                "http://arxiv.org/pdf/2312.03360v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03345v1",
            "title": "GraNet: A Multi-Level Graph Network for 6-DoF Grasp Pose Generation in\n  Cluttered Scenes",
            "updated": "2023-12-06T08:36:29Z",
            "published": "2023-12-06T08:36:29Z",
            "summary": "6-DoF object-agnostic grasping in unstructured environments is a critical yet\nchallenging task in robotics. Most current works use non-optimized approaches\nto sample grasp locations and learn spatial features without concerning the\ngrasping task. This paper proposes GraNet, a graph-based grasp pose generation\nframework that translates a point cloud scene into multi-level graphs and\npropagates features through graph neural networks. By building graphs at the\nscene level, object level, and grasp point level, GraNet enhances feature\nembedding at multiple scales while progressively converging to the ideal\ngrasping locations by learning. Our pipeline can thus characterize the spatial\ndistribution of grasps in cluttered scenes, leading to a higher rate of\neffective grasping. Furthermore, we enhance the representation ability of\nscalable graph networks by a structure-aware attention mechanism to exploit\nlocal relations in graphs. Our method achieves state-of-the-art performance on\nthe large-scale GraspNet-1Billion benchmark, especially in grasping unseen\nobjects (+11.62 AP). The real robot experiment shows a high success rate in\ngrasping scattered objects, verifying the effectiveness of the proposed\napproach in unstructured environments.",
            "author": [
                "Haowen Wang",
                "Wanhao Niu",
                "Chungang Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03345v1",
                "http://arxiv.org/pdf/2312.03345v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03327v1",
            "title": "Building Category Graphs Representation with Spatial and Temporal\n  Attention for Visual Navigation",
            "updated": "2023-12-06T07:28:43Z",
            "published": "2023-12-06T07:28:43Z",
            "summary": "Given an object of interest, visual navigation aims to reach the object's\nlocation based on a sequence of partial observations. To this end, an agent\nneeds to 1) learn a piece of certain knowledge about the relations of object\ncategories in the world during training and 2) look for the target object based\non the pre-learned object category relations and its moving trajectory in the\ncurrent unseen environment. In this paper, we propose a Category Relation Graph\n(CRG) to learn the knowledge of object category layout relations and a\nTemporal-Spatial-Region (TSR) attention architecture to perceive the long-term\nspatial-temporal dependencies of objects helping the navigation. We learn prior\nknowledge of object layout, establishing a category relationship graph to\ndeduce the positions of specific objects. Subsequently, we introduced TSR to\ncapture the relationships of objects in temporal, spatial, and regions within\nthe observation trajectories. Specifically, we propose a Temporal attention\nmodule (T) to model the temporal structure of the observation sequence, which\nimplicitly encodes the historical moving or trajectory information. Then, a\nSpatial attention module (S) is used to uncover the spatial context of the\ncurrent observation objects based on the category relation graph and past\nobservations. Last, a Region attention module (R) shifts the attention to the\ntarget-relevant region. Based on the visual representation extracted by our\nmethod, the agent can better perceive the environment and easily learn superior\nnavigation policy. Experiments on AI2-THOR demonstrate our CRG-TSR method\nsignificantly outperforms existing methods regarding both effectiveness and\nefficiency. The code has been included in the supplementary material and will\nbe publicly available.",
            "author": [
                "Xiaobo Hu",
                "Youfang Lin",
                "HeHe Fan",
                "Shuo Wang",
                "Zhihao Wu",
                "Kai Lv"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03327v1",
                "http://arxiv.org/pdf/2312.03327v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03315v1",
            "title": "Impact of parallel code optimization on computer power consumption",
            "updated": "2023-12-06T06:48:16Z",
            "published": "2023-12-06T06:48:16Z",
            "summary": "The increase in performance and power of computing systems requires the wider\nuse of program optimizations. The goal of performing optimizations is not only\nto reduce program runtime, but also to reduce other computer resources\nincluding power consumption. The goal of the study was to evaluate the impact\nof different optimization levels and various optimization strategies on power\nconsumption. In a series of experiments, it was established that the average\npower consumption tends to peak for the programs with optimized source code.\nThe articles also describes the impact of changing computer architecture on\npower consumption graphs. The relationships between the average and median\nvalues of power consumption by example programs are considered. The possibility\nof creating program energy consumption profile for a parallel program is shown.",
            "author": [
                "E. A. Kiselev",
                "P. N. Telegin",
                "A. V. Baranov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03315v1",
                "http://arxiv.org/pdf/2312.03315v1"
            ],
            "primary_category": "cs.MS",
            "category": [
                "cs.MS",
                "cs.DC",
                "68M20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03309v1",
            "title": "Benchmarking Continual Learning from Cognitive Perspectives",
            "updated": "2023-12-06T06:27:27Z",
            "published": "2023-12-06T06:27:27Z",
            "summary": "Continual learning addresses the problem of continuously acquiring and\ntransferring knowledge without catastrophic forgetting of old concepts. While\nhumans achieve continual learning via diverse neurocognitive mechanisms, there\nis a mismatch between cognitive properties and evaluation methods of continual\nlearning models. First, the measurement of continual learning models mostly\nrelies on evaluation metrics at a micro-level, which cannot characterize\ncognitive capacities of the model. Second, the measurement is method-specific,\nemphasizing model strengths in one aspect while obscuring potential weaknesses\nin other respects. To address these issues, we propose to integrate model\ncognitive capacities and evaluation metrics into a unified evaluation paradigm.\nWe first characterize model capacities via desiderata derived from cognitive\nproperties supporting human continual learning. The desiderata concern (1)\nadaptability in varying lengths of task sequence; (2) sensitivity to dynamic\ntask variations; and (3) efficiency in memory usage and training time\nconsumption. Then we design evaluation protocols for each desideratum to assess\ncognitive capacities of recent continual learning models. Experimental results\nshow that no method we consider has satisfied all the desiderata and is still\nfar away from realizing truly continual learning. Although some methods exhibit\nsome degree of adaptability and efficiency, no method is able to identify task\nrelationships when encountering dynamic task variations, or achieve a trade-off\nin learning similarities and differences between tasks. Inspired by these\nresults, we discuss possible factors that influence model performance in these\ndesiderata and provide guidance for the improvement of continual learning\nmodels.",
            "author": [
                "Xiaoqian Liu",
                "Junge Zhang",
                "Mingyi Zhang",
                "Peipei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03309v1",
                "http://arxiv.org/pdf/2312.03309v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03303v1",
            "title": "Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking\n  Technique",
            "updated": "2023-12-06T06:07:50Z",
            "published": "2023-12-06T06:07:50Z",
            "summary": "This paper presents a novel benchmarking framework Dyport for evaluating\nbiomedical hypothesis generation systems. Utilizing curated datasets, our\napproach tests these systems under realistic conditions, enhancing the\nrelevance of our evaluations. We integrate knowledge from the curated databases\ninto a dynamic graph, accompanied by a method to quantify discovery importance.\nThis not only assesses hypothesis accuracy but also their potential impact in\nbiomedical research which significantly extends traditional link prediction\nbenchmarks. Applicability of our benchmarking process is demonstrated on\nseveral link prediction systems applied on biomedical semantic knowledge\ngraphs. Being flexible, our benchmarking system is designed for broad\napplication in hypothesis generation quality verification, aiming to expand the\nscope of scientific discovery within the biomedical research community.\nAvailability and implementation: Dyport framework is fully open-source. All\ncode and datasets are available at: https://github.com/IlyaTyagin/Dyport",
            "author": [
                "Ilya Tyagin",
                "Ilya Safro"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03303v1",
                "http://arxiv.org/pdf/2312.03303v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03300v1",
            "title": "Non-backtracking eigenvector delocalization for random regular graphs",
            "updated": "2023-12-06T05:54:34Z",
            "published": "2023-12-06T05:54:34Z",
            "summary": "The non-backtracking operator of a graph is a powerful tool in spectral graph\ntheory and random matrix theory. Most existing results for the non-backtracking\noperator of a random graph concern only eigenvalues or top eigenvectors. In\nthis paper, we take the first step in analyzing its bulk eigenvector behaviors.\nWe demonstrate that for the non-backtracking operator $B$ of a random\n$d$-regular graph, its eigenvectors corresponding to nontrivial eigenvalues are\ncompletely delocalized with high probability. Additionally, we show complete\ndelocalization for a reduced $2n \\times 2n$ non-backtracking matrix\n$\\widetilde{B}$. By projecting all eigenvalues of $\\widetilde{B}$ onto the real\nline, we obtain an empirical measure that converges weakly in probability to\nthe Kesten-McKay law for fixed $d\\geq 3$ and to a semicircle law as $d\n\\to\\infty$ with $n \\to\\infty$.",
            "author": [
                "Xiangyi Zhu",
                "Yizhe Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03300v1",
                "http://arxiv.org/pdf/2312.03300v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03292v1",
            "title": "Enhancing Molecular Property Prediction via Mixture of Collaborative\n  Experts",
            "updated": "2023-12-06T05:02:10Z",
            "published": "2023-12-06T05:02:10Z",
            "summary": "Molecular Property Prediction (MPP) task involves predicting biochemical\nproperties based on molecular features, such as molecular graph structures,\ncontributing to the discovery of lead compounds in drug development. To address\ndata scarcity and imbalance in MPP, some studies have adopted Graph Neural\nNetworks (GNN) as an encoder to extract commonalities from molecular graphs.\nHowever, these approaches often use a separate predictor for each task,\nneglecting the shared characteristics among predictors corresponding to\ndifferent tasks. In response to this limitation, we introduce the GNN-MoCE\narchitecture. It employs the Mixture of Collaborative Experts (MoCE) as\npredictors, exploiting task commonalities while confronting the homogeneity\nissue in the expert pool and the decision dominance dilemma within the expert\ngroup. To enhance expert diversity for collaboration among all experts, the\nExpert-Specific Projection method is proposed to assign a unique projection\nperspective to each expert. To balance decision-making influence for\ncollaboration within the expert group, the Expert-Specific Loss is presented to\nintegrate individual expert loss into the weighted decision loss of the group\nfor more equitable training. Benefiting from the enhancements of MoCE in expert\ncreation, dynamic expert group formation, and experts' collaboration, our model\ndemonstrates superior performance over traditional methods on 24 MPP datasets,\nespecially in tasks with limited data or high imbalance.",
            "author": [
                "Xu Yao",
                "Shuang Liang",
                "Songqiao Han",
                "Hailiang Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03292v1",
                "http://arxiv.org/pdf/2312.03292v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.MA",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03290v1",
            "title": "Can language agents be alternatives to PPO? A Preliminary Empirical\n  Study On OpenAI Gym",
            "updated": "2023-12-06T04:48:26Z",
            "published": "2023-12-06T04:48:26Z",
            "summary": "The formidable capacity for zero- or few-shot decision-making in language\nagents encourages us to pose a compelling question: Can language agents be\nalternatives to PPO agents in traditional sequential decision-making tasks? To\ninvestigate this, we first take environments collected in OpenAI Gym as our\ntestbeds and ground them to textual environments that construct the TextGym\nsimulator. This allows for straightforward and efficient comparisons between\nPPO agents and language agents, given the widespread adoption of OpenAI Gym. To\nensure a fair and effective benchmarking, we introduce $5$ levels of scenario\nfor accurate domain-knowledge controlling and a unified RL-inspired framework\nfor language agents. Additionally, we propose an innovative\nexplore-exploit-guided language (EXE) agent to solve tasks within TextGym.\nThrough numerical experiments and ablation studies, we extract valuable\ninsights into the decision-making capabilities of language agents and make a\npreliminary evaluation of their potential to be alternatives to PPO in\nclassical sequential decision-making problems. This paper sheds light on the\nperformance of language agents and paves the way for future research in this\nexciting domain. Our code is publicly available\nat~\\url{https://github.com/mail-ecnu/Text-Gym-Agents}.",
            "author": [
                "Junjie Sheng",
                "Zixiao Huang",
                "Chuyun Shen",
                "Wenhao Li",
                "Yun Hua",
                "Bo Jin",
                "Hongyuan Zha",
                "Xiangfeng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03290v1",
                "http://arxiv.org/pdf/2312.03290v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03289v2",
            "title": "Class Incremental Learning for Adversarial Robustness",
            "updated": "2023-12-07T04:21:33Z",
            "published": "2023-12-06T04:38:02Z",
            "summary": "Adversarial training integrates adversarial examples during model training to\nenhance robustness. However, its application in fixed dataset settings differs\nfrom real-world dynamics, where data accumulates incrementally. In this study,\nwe investigate Adversarially Robust Class Incremental Learning (ARCIL), a\nmethod that combines adversarial robustness with incremental learning. We\nobserve that combining incremental learning with naive adversarial training\neasily leads to a loss of robustness. We discover that this is attributed to\nthe disappearance of the flatness of the loss function, a characteristic of\nadversarial training. To address this issue, we propose the Flatness Preserving\nDistillation (FPD) loss that leverages the output difference between\nadversarial and clean examples. Additionally, we introduce the Logit Adjustment\nDistillation (LAD) loss, which adapts the model's knowledge to perform well on\nnew tasks. Experimental results demonstrate the superiority of our method over\napproaches that apply adversarial training to existing incremental learning\nmethods, which provides a strong baseline for incremental learning on\nadversarial robustness in the future. Our method achieves AutoAttack accuracy\nthat is 5.99\\%p, 5.27\\%p, and 3.90\\%p higher on average than the baseline on\nsplit CIFAR-10, CIFAR-100, and Tiny ImageNet, respectively. The code will be\nmade available.",
            "author": [
                "Seungju Cho",
                "Hongsin Lee",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03289v2",
                "http://arxiv.org/pdf/2312.03289v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03288v1",
            "title": "STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention\n  Transformer for Skeleton-based Action Recognition",
            "updated": "2023-12-06T04:36:58Z",
            "published": "2023-12-06T04:36:58Z",
            "summary": "Graph convolutional networks (GCNs) have been widely used and achieved\nremarkable results in skeleton-based action recognition. We think the key to\nskeleton-based action recognition is a skeleton hanging in frames, so we focus\non how the Graph Convolutional Convolution networks learn different topologies\nand effectively aggregate joint features in the global temporal and local\ntemporal. In this work, we propose three Channel-wise Tolopogy Graph\nConvolution based on Channel-wise Topology Refinement Graph Convolution\n(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture\nthe upper-lower body part and hand-foot relationship skeleton features. After\nthat, to capture features of human skeletons changing in frames we design the\nTemporal Attention Transformers to extract skeletons effectively. The Temporal\nAttention Transformers can learn the temporal features of human skeleton\nsequences. Finally, we fuse the temporal features output scale with MLP and\nclassification. We develop a powerful graph convolutional network named Spatial\nTemporal Effective Body-part Cross Attention Transformer which notably\nhigh-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models\nare available at https://github.com/maclong01/STEP-CATFormer",
            "author": [
                "Nguyen Huu Bao Long"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03288v1",
                "http://arxiv.org/pdf/2312.03288v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03286v1",
            "title": "Indirect Gradient Matching for Adversarial Robust Distillation",
            "updated": "2023-12-06T04:32:38Z",
            "published": "2023-12-06T04:32:38Z",
            "summary": "Adversarial training significantly improves adversarial robustness, but\nsuperior performance is primarily attained with large models. This substantial\nperformance gap for smaller models has spurred active research into adversarial\ndistillation (AD) to mitigate the difference. Existing AD methods leverage the\nteacher's logits as a guide. In contrast to these approaches, we aim to\ntransfer another piece of knowledge from the teacher, the input gradient. In\nthis paper, we propose a distillation module termed Indirect Gradient\nDistillation Module (IGDM) that indirectly matches the student's input gradient\nwith that of the teacher. We hypothesize that students can better acquire the\nteacher's knowledge by matching the input gradient. Leveraging the observation\nthat adversarial training renders the model locally linear on the input space,\nwe employ Taylor approximation to effectively align gradients without directly\ncalculating them. Experimental results show that IGDM seamlessly integrates\nwith existing AD methods, significantly enhancing the performance of all AD\nmethods. Particularly, utilizing IGDM on the CIFAR-100 dataset improves the\nAutoAttack accuracy from 28.06% to 30.32% with the ResNet-18 model and from\n26.18% to 29.52% with the MobileNetV2 model when integrated into the SOTA\nmethod without additional data augmentation. The code will be made available.",
            "author": [
                "Hongsin Lee",
                "Seungju Cho",
                "Changick Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03286v1",
                "http://arxiv.org/pdf/2312.03286v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03275v1",
            "title": "VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation",
            "updated": "2023-12-06T04:02:28Z",
            "published": "2023-12-06T04:02:28Z",
            "summary": "Understanding how humans leverage semantic knowledge to navigate unfamiliar\nenvironments and decide where to explore next is pivotal for developing robots\ncapable of human-like search behaviors. We introduce a zero-shot navigation\napproach, Vision-Language Frontier Maps (VLFM), which is inspired by human\nreasoning and designed to navigate towards unseen semantic objects in novel\nenvironments. VLFM builds occupancy maps from depth observations to identify\nfrontiers, and leverages RGB observations and a pre-trained vision-language\nmodel to generate a language-grounded value map. VLFM then uses this map to\nidentify the most promising frontier to explore for finding an instance of a\ngiven target object category. We evaluate VLFM in photo-realistic environments\nfrom the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D)\ndatasets within the Habitat simulator. Remarkably, VLFM achieves\nstate-of-the-art results on all three datasets as measured by success weighted\nby path length (SPL) for the Object Goal Navigation task. Furthermore, we show\nthat VLFM's zero-shot nature enables it to be readily deployed on real-world\nrobots such as the Boston Dynamics Spot mobile manipulation platform. We deploy\nVLFM on Spot and demonstrate its capability to efficiently navigate to target\nobjects within an office building in the real world, without any prior\nknowledge of the environment. The accomplishments of VLFM underscore the\npromising potential of vision-language models in advancing the field of\nsemantic navigation. Videos of real-world deployment can be viewed at\nnaoki.io/vlfm.",
            "author": [
                "Naoki Yokoyama",
                "Sehoon Ha",
                "Dhruv Batra",
                "Jiuguang Wang",
                "Bernadette Bucher"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03275v1",
                "http://arxiv.org/pdf/2312.03275v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03270v1",
            "title": "Geometric Deep Learning Towards the Iterative Classification of\n  Graph-Based Aircraft Thermal Management Systems",
            "updated": "2023-12-06T03:49:13Z",
            "published": "2023-12-06T03:49:13Z",
            "summary": "In this paper, we use graph-based techniques to investigate the use of\ngeometric deep learning (GDL) in the classification and down-selection of\naircraft thermal management systems (TMS). Previous work developed an\nenumerative graph generation procedure using a component catalog with network\nstructure constraints to represent novel aircraft TMSs as graphs. However, as\nwith many enumerative approaches, combinatorial explosion limits its efficacy\nin many real-world problems, particularly when simulations and optimization\nmust be performed on the many (automatically-generated) physics models.\nTherefore, we present an approach that takes the directed graphs representing\naircraft TMSs and use GDL to predict the critical characteristics of the\nremaining graphs. This paper's findings demonstrate that incorporating\nadditional graph-based features enhances performance, achieving an accuracy of\n97% for determining a graph's compilability and simulatability while using only\n5% of the data for training. By applying iterative classification methods, we\nalso successfully segmented the total set of graphs into more specific groups\nwith an average inclusion of 84.7 of the top 100 highest-performing graphs,\nachieved by training on 45% of the data.",
            "author": [
                "Anthony Sirico Jr.",
                "Daniel R Herber"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03270v1",
                "http://arxiv.org/pdf/2312.03270v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03260v1",
            "title": "Connectivity Preserving Hamiltonian Cycles in $k$-Connected Dirac Graphs",
            "updated": "2023-12-06T03:16:44Z",
            "published": "2023-12-06T03:16:44Z",
            "summary": "We show that for $k \\geq 2$, there exists a function $f(k) = O(k)$ such that\nevery $k$-connected graph $G$ of order $n \\geq f(k)$ with minimum degree at\nleast $\\frac{n}{2}$ contains a Hamiltonian cycle $H$ such that $G-E(H)$ is\n$k$-connected. Applying Nash-Williams' result on edge-disjoint Hamiltonian\ncycles, we also show that for $k \\geq 2$ and $\\ell \\geq 2$, there exists a\nfunction $g(k,\\ell) = O(k\\ell)$ such that every $k$-connected graph $G$ of\norder $n \\geq g(k,\\ell)$ with minimum degree at least $\\frac{n}{2}$ contains\n$\\ell$ edge-disjoint Hamiltonian cycles $H_1,H_2,\\ldots,H_\\ell$ such that\n$G-\\cup_{1 \\leq i \\leq \\ell}E(H_i)$ is $k$-connected. As a corollary, we have a\nstatement that refines the result of Nash-Williams for $k$-connected graphs\nwith $k \\leq 8$. Moreover, when the connectivity of $G$ is exactly $k$, a\nsimilar result with an improved lower bound on $n$ can be shown, which does not\ndepend on the result of Nash-Williams.",
            "author": [
                "Toru Hasunuma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03260v1",
                "http://arxiv.org/pdf/2312.03260v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03258v1",
            "title": "On the oriented diameter of near planar triangulations",
            "updated": "2023-12-06T03:12:54Z",
            "published": "2023-12-06T03:12:54Z",
            "summary": "In this paper, we show that the oriented diameter of any $n$-vertex\n$2$-connected near triangulation is at most $\\lceil{\\frac{n}{2}}\\rceil$ (except\nfor seven small exceptions), and the upper bound is tight. This extends a\nresult of Wang et.al. on the oriented diameter of maximal outerplanar graphs,\nand improves an upper bound of $n/2+O(\\sqrt{n})$ on the oriented diameter of\nplanar triangulations by Mondal, Parthiban and Rajasingh.",
            "author": [
                "Yiwei Ge",
                "Xiaonan Liu",
                "Zhiyu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03258v1",
                "http://arxiv.org/pdf/2312.03258v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03257v1",
            "title": "Bayesian Functional Analysis for Untargeted Metabolomics Data with\n  Matching Uncertainty and Small Sample Sizes",
            "updated": "2023-12-06T03:10:16Z",
            "published": "2023-12-06T03:10:16Z",
            "summary": "Untargeted metabolomics based on liquid chromatography-mass spectrometry\ntechnology is quickly gaining widespread application given its ability to\ndepict the global metabolic pattern in biological samples. However, the data is\nnoisy and plagued by the lack of clear identity of data features measured from\nsamples. Multiple potential matchings exist between data features and known\nmetabolites, while the truth can only be one-to-one matches. Some existing\nmethods attempt to reduce the matching uncertainty, but are far from being able\nto remove the uncertainty for most features. The existence of the uncertainty\ncauses major difficulty in downstream functional analysis. To address these\nissues, we develop a novel approach for Bayesian Analysis of Untargeted\nMetabolomics data (BAUM) to integrate previously separate tasks into a single\nframework, including matching uncertainty inference, metabolite selection, and\nfunctional analysis. By incorporating the knowledge graph between variables and\nusing relatively simple assumptions, BAUM can analyze datasets with small\nsample sizes. By allowing different confidence levels of feature-metabolite\nmatching, the method is applicable to datasets in which feature identities are\npartially known. Simulation studies demonstrate that, compared with other\nexisting methods, BAUM achieves better accuracy in selecting important\nmetabolites that tend to be functionally consistent and assigning confidence\nscores to feature-metabolite matches. We analyze a COVID-19 metabolomics\ndataset and a mouse brain metabolomics dataset using BAUM. Even with a very\nsmall sample size of 16 mice per group, BAUM is robust and stable. It finds\npathways that conform to existing knowledge, as well as novel pathways that are\nbiologically plausible.",
            "author": [
                "Guoxuan Ma",
                "Jian Kang",
                "Tianwei Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03257v1",
                "http://arxiv.org/pdf/2312.03257v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03248v1",
            "title": "Customizable Combination of Parameter-Efficient Modules for Multi-Task\n  Learning",
            "updated": "2023-12-06T02:47:56Z",
            "published": "2023-12-06T02:47:56Z",
            "summary": "Modular and composable transfer learning is an emerging direction in the\nfield of Parameter Efficient Fine-Tuning, as it enables neural networks to\nbetter organize various aspects of knowledge, leading to improved cross-task\ngeneralization. In this paper, we introduce a novel approach Customized\nPolytropon C-Poly that combines task-common skills and task-specific skills,\nwhile the skill parameters being highly parameterized using low-rank\ntechniques. Each task is associated with a customizable number of exclusive\nspecialized skills and also benefits from skills shared with peer tasks. A\nskill assignment matrix is jointly learned. To evaluate our approach, we\nconducted extensive experiments on the Super-NaturalInstructions and the\nSuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms\nfully-shared, task-specific, and skill-indistinguishable baselines,\nsignificantly enhancing the sample efficiency in multi-task learning scenarios.",
            "author": [
                "Haowen Wang",
                "Tao Sun",
                "Cong Fan",
                "Jinjie Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03248v1",
                "http://arxiv.org/pdf/2312.03248v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03246v1",
            "title": "On Topological Conditions for Enabling Transient Control in\n  Leader-follower Networks",
            "updated": "2023-12-06T02:45:59Z",
            "published": "2023-12-06T02:45:59Z",
            "summary": "We derive necessary and sufficient conditions for leader-follower multi-agent\nsystems such that we can further apply prescribed performance control to\nachieve the desired formation while satisfying certain transient constraints. A\nleader-follower framework is considered in the sense that a group of agents\nwith external inputs are selected as leaders in order to drive the group of\nfollowers in a way that the entire system can achieve target formation within\ncertain prescribed performance transient bounds. We first derive necessary\nconditions on the leader-follower graph topology under which the target\nformation together with the prescribed performance guarantees can be fulfilled.\nAfterwards, the derived necessary conditions are extended to necessary and\nsufficient conditions for leader-follower formation control under transient\nconstraints. Finally, the proposed results are illustrated with simulation\nexamples.",
            "author": [
                "Fei Chen",
                "Dimos V. Dimarogonas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03246v1",
                "http://arxiv.org/pdf/2312.03246v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.MA",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03236v1",
            "title": "Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets",
            "updated": "2023-12-06T02:16:44Z",
            "published": "2023-12-06T02:16:44Z",
            "summary": "The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of\nhigh-performing subnetworks within a randomly initialized model, discoverable\nthrough pruning a convolutional neural network (CNN) without any weight\ntraining. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH\nfrom CNNs to shallow graph neural networks (GNNs). However, discrepancies\npersist when comparing baseline models with learned dense weights.\nAdditionally, there remains an unexplored area in applying SLTH to deeper GNNs,\nwhich, despite delivering improved accuracy with additional layers, suffer from\nexcessive memory requirements. To address these challenges, this work utilizes\nMulticoated Supermasks (M-Sup), a scalar pruning mask method, and implements it\nin GNNs by proposing a strategy for setting its pruning thresholds adaptively.\nIn the context of deep GNNs, this research uncovers the existence of untrained\nrecurrent networks, which exhibit performance on par with their trained\nfeed-forward counterparts. This paper also introduces the Multi-Stage Folding\nand Unshared Masks methods to expand the search space in terms of both\narchitecture and parameters. Through the evaluation of various datasets,\nincluding the Open Graph Benchmark (OGB), this work establishes a triple-win\nscenario for SLTH-based GNNs: by achieving high sparsity, competitive\nperformance, and high memory efficiency with up to 98.7\\% reduction, it\ndemonstrates suitability for energy-efficient graph processing.",
            "author": [
                "Jiale Yan",
                "Hiroaki Ito",
                "\u00c1ngel L\u00f3pez Garc\u00eda-Arias",
                "Yasuyuki Okoshi",
                "Hikari Otsuka",
                "Kazushi Kawamura",
                "Thiem Van Chu",
                "Masato Motomura"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03236v1",
                "http://arxiv.org/pdf/2312.03236v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03229v1",
            "title": "Attaining Equilibria Using Control Sets",
            "updated": "2023-12-06T01:56:30Z",
            "published": "2023-12-06T01:56:30Z",
            "summary": "Many interactions result in a socially suboptimal equilibrium, or in a\nnon-equilibrium state, from which arriving at an equilibrium through simple\ndynamics can be impossible of too long. Aiming to achieve a certain\nequilibrium, we persuade, bribe, or coerce a group of participants to make them\nact in a way that will motivate the rest of the players to act accordingly to\nthe desired equilibrium. Formally, we ask which subset of the players can adopt\nthe goal equilibrium strategies that will make acting according to the desired\nequilibrium a best response for the other players. We call such a subset a\ndirect control set, prove some connections to strength of equilibrium, and\nstudy the hardness to find such lightest sets, even approximately. We then\nsolve important subcases and provide approximation algorithms, assuming\nmonotonicity. Next, we concentrate on potential games and prove that, while the\nproblem of finding such a set is \\NP-hard, even for constant-factor\napproximation, we can still solve the problem approximately or even precisely\nin relevant special cases. We approximately solve this problem for singleton\npotential games and treat more closely specific potential games, such as\nsymmetric games and coordination games on graphs.",
            "author": [
                "Gleb Polevoy",
                "Jonas Schweichhart"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03229v1",
                "http://arxiv.org/pdf/2312.03229v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "91A10, 91A68",
                "J.4; F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03218v1",
            "title": "Accelerated Gradient Algorithms with Adaptive Subspace Search for\n  Instance-Faster Optimization",
            "updated": "2023-12-06T01:16:10Z",
            "published": "2023-12-06T01:16:10Z",
            "summary": "Gradient-based minimax optimal algorithms have greatly promoted the\ndevelopment of continuous optimization and machine learning. One seminal work\ndue to Yurii Nesterov [Nes83a] established $\\tilde{\\mathcal{O}}(\\sqrt{L/\\mu})$\ngradient complexity for minimizing an $L$-smooth $\\mu$-strongly convex\nobjective. However, an ideal algorithm would adapt to the explicit complexity\nof a particular objective function and incur faster rates for simpler problems,\ntriggering our reconsideration of two defeats of existing optimization modeling\nand analysis. (i) The worst-case optimality is neither the instance optimality\nnor such one in reality. (ii) Traditional $L$-smoothness condition may not be\nthe primary abstraction/characterization for modern practical problems.\n  In this paper, we open up a new way to design and analyze gradient-based\nalgorithms with direct applications in machine learning, including linear\nregression and beyond. We introduce two factors $(\\alpha, \\tau_{\\alpha})$ to\nrefine the description of the degenerated condition of the optimization\nproblems based on the observation that the singular values of Hessian often\ndrop sharply. We design adaptive algorithms that solve simpler problems without\npre-known knowledge with reduced gradient or analogous oracle accesses. The\nalgorithms also improve the state-of-art complexities for several problems in\nmachine learning, thereby solving the open problem of how to design faster\nalgorithms in light of the known complexity lower bounds. Specially, with the\n$\\mathcal{O}(1)$-nuclear norm bounded, we achieve an optimal\n$\\tilde{\\mathcal{O}}(\\mu^{-1/3})$ (v.s. $\\tilde{\\mathcal{O}}(\\mu^{-1/2})$)\ngradient complexity for linear regression. We hope this work could invoke the\nrethinking for understanding the difficulty of modern problems in optimization.",
            "author": [
                "Yuanshi Liu",
                "Hanzhen Zhao",
                "Yang Xu",
                "Pengyun Yue",
                "Cong Fang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03218v1",
                "http://arxiv.org/pdf/2312.03218v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03203v1",
            "title": "Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled\n  Feature Fields",
            "updated": "2023-12-06T00:46:30Z",
            "published": "2023-12-06T00:46:30Z",
            "summary": "3D scene representations have gained immense popularity in recent years.\nMethods that use Neural Radiance fields are versatile for traditional tasks\nsuch as novel view synthesis. In recent times, some work has emerged that aims\nto extend the functionality of NeRF beyond view synthesis, for semantically\naware tasks such as editing and segmentation using 3D feature field\ndistillation from 2D foundation models. However, these methods have two major\nlimitations: (a) they are limited by the rendering speed of NeRF pipelines, and\n(b) implicitly represented feature fields suffer from continuity artifacts\nreducing feature quality. Recently, 3D Gaussian Splatting has shown\nstate-of-the-art performance on real-time radiance field rendering. In this\nwork, we go one step further: in addition to radiance field rendering, we\nenable 3D Gaussian splatting on arbitrary-dimension semantic features via 2D\nfoundation model distillation. This translation is not straightforward: naively\nincorporating feature fields in the 3DGS framework leads to warp-level\ndivergence. We propose architectural and training changes to efficiently avert\nthis problem. Our proposed method is general, and our experiments showcase\nnovel view semantic segmentation, language-guided editing and segment anything\nthrough learning feature fields from state-of-the-art 2D foundation models such\nas SAM and CLIP-LSeg. Across experiments, our distillation method is able to\nprovide comparable or better results, while being significantly faster to both\ntrain and render. Additionally, to the best of our knowledge, we are the first\nmethod to enable point and bounding-box prompting for radiance field\nmanipulation, by leveraging the SAM model. Project website at:\nhttps://feature-3dgs.github.io/",
            "author": [
                "Shijie Zhou",
                "Haoran Chang",
                "Sicheng Jiang",
                "Zhiwen Fan",
                "Zehao Zhu",
                "Dejia Xu",
                "Pradyumna Chari",
                "Suya You",
                "Zhangyang Wang",
                "Achuta Kadambi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03203v1",
                "http://arxiv.org/pdf/2312.03203v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03180v1",
            "title": "Image reconstructions using sparse dictionary representations and\n  implicit, non-negative mappings",
            "updated": "2023-12-05T23:07:21Z",
            "published": "2023-12-05T23:07:21Z",
            "summary": "Many imaging science tasks can be modeled as a discrete linear inverse\nproblem. Solving linear inverse problems is often challenging, with\nill-conditioned operators and potentially non-unique solutions. Embedding prior\nknowledge, such as smoothness, into the solution can overcome these challenges.\nIn this work, we encode prior knowledge using a non-negative patch dictionary,\nwhich effectively learns a basis from a training set of natural images. In this\ndictionary basis, we desire solutions that are non-negative and sparse (i.e.,\ncontain many zero entries). With these constraints, standard methods for\nsolving discrete linear inverse problems are not directly applicable. One such\napproach is the modified residual norm steepest descent (MRNSD), which produces\nnon-negative solutions but does not induce sparsity. In this paper, we provide\ntwo methods based on MRNSD that promote sparsity. In our first method, we add\nan $\\ell_1$-regularization term with a new, optimal step size. In our second\nmethod, we propose a new non-negative, sparsity-promoting mapping of the\nsolution. We compare the performance of our proposed methods on a number of\nnumerical experiments, including deblurring, image completion, computer\ntomography, and superresolution. Our results show that these methods\neffectively solve discrete linear inverse problems with non-negativity and\nsparsity constraints.",
            "author": [
                "Elizabeth Newman",
                "Jack Michael Solomon",
                "Matthias Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03180v1",
                "http://arxiv.org/pdf/2312.03180v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65F10, 65F22",
                "G.1.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03167v1",
            "title": "Adaptive spectral graph wavelets for collaborative filtering",
            "updated": "2023-12-05T22:22:25Z",
            "published": "2023-12-05T22:22:25Z",
            "summary": "Collaborative filtering is a popular approach in recommender systems, whose\nobjective is to provide personalized item suggestions to potential users based\non their purchase or browsing history. However, personalized recommendations\nrequire considerable amount of behavioral data on users, which is usually\nunavailable for new users, giving rise to the cold-start problem. To help\nalleviate this challenging problem, we introduce a spectral graph wavelet\ncollaborative filtering framework for implicit feedback data, where users,\nitems and their interactions are represented as a bipartite graph.\nSpecifically, we first propose an adaptive transfer function by leveraging a\npower transform with the goal of stabilizing the variance of graph frequencies\nin the spectral domain. Then, we design a deep recommendation model for\nefficient learning of low-dimensional embeddings of users and items using\nspectral graph wavelets in an end-to-end fashion. In addition to capturing the\ngraph's local and global structures, our approach yields localization of graph\nsignals in both spatial and spectral domains, and hence not only learns\ndiscriminative representations of users and items, but also promotes the\nrecommendation quality. The effectiveness of our proposed model is demonstrated\nthrough extensive experiments on real-world benchmark datasets, achieving\nbetter recommendation performance compared with strong baseline methods.",
            "author": [
                "Osama Alshareet",
                "A. Ben Hamza"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03167v1",
                "http://arxiv.org/pdf/2312.03167v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03157v1",
            "title": "Failures of the Feynman-Dyson diagrammatic perturbation expansion of\n  propagators",
            "updated": "2023-12-05T21:46:25Z",
            "published": "2023-12-05T21:46:25Z",
            "summary": "Using a general-order many-body Green's-function method for molecules, we\nillustrate numerically three pathological behaviors of the Feynman-Dyson\ndiagrammatic perturbation expansion of one-particle many-body Green's functions\nas electron propagators. First, the perturbation expansion of the\nfrequency-dependent self-energy is nonconvergent at the exact self-energy in\nwide domains of frequency. Second, the Dyson equation with an odd-order\nself-energy has a qualitatively wrong shape and, as a result, most of their\nsatellite roots are complex and nonphysical. Third, the Dyson equation with an\neven-order self-energy has an exponentially increasing number of roots as the\nperturbation order is raised, which quickly exceeds the correct number of\nroots. Infinite partial summation of diagrams by vertex or edge modification\nexacerbates these problems. Not only does the nonconvergence render\nhigher-order perturbation theories useless for satellite roots, but it also\ncalls into question the validity of their combined use with the ans\\\"{a}tze\nrequiring the knowledge of all poles and residues. Such ans\\\"{a}tze include the\nGalitskii-Migdal formula, self-consistent Green's-function methods,\nLuttinger-Ward functional, and some models of the algebraic diagrammatic\nconstruction.",
            "author": [
                "So Hirata",
                "Ireneusz Grabowski",
                "Rodney J. Bartlett"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03157v1",
                "http://arxiv.org/pdf/2312.03157v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math-ph",
                "math.MP",
                "nucl-th",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03149v1",
            "title": "Vertex and edge orbits in nut graphs",
            "updated": "2023-12-05T21:37:25Z",
            "published": "2023-12-05T21:37:25Z",
            "summary": "A nut graph is a simple graph for which the adjacency matrix has a single\nzero eigenvalue such that all non-zero kernel eigenvectors have no zero entry.\nIf the isolated vertex is excluded as trivial, nut graphs have seven or more\nvertices; they are connected, non-bipartite, and have no leaves. It is shown\nthat a nut graph $G$ always has at least one more edge orbit than it has vertex\norbits: $o_e(G) \\geq o_v(G) + 1$, with the obvious corollary that edge\ntransitive nut graphs do not exist. We give infinite familes of\nvertex-transitive nut graphs with two orbits of edges, and infinite families of\nnut graphs with two orbits of vertices and three of edges. Several\nconstructions for nut graphs from smaller starting graphs are known: double\nsubdivision of a bridge, four-fold subdivision of an edge, a construction for\nextrusion of a vertex with preservation of the degree sequence. To these we add\nmultiplier constructions that yield nut graphs from regular (not necessarily\nnut graph) parents. In general, constructions can have different effects on\nautomorphism group and counts of vertex and edge orbits, but in the case where\nthe automorphism group is `preserved', they can be used in a predictable way to\ncontrol vertex and edge orbit numbers.",
            "author": [
                "Nino Ba\u0161i\u0107",
                "Patrick W. Fowler",
                "Toma\u017e Pisanski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03149v1",
                "http://arxiv.org/pdf/2312.03149v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 05C25, 05C75, 05C92"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03147v1",
            "title": "Neural parameter calibration and uncertainty quantification for epidemic\n  forecasting",
            "updated": "2023-12-05T21:34:59Z",
            "published": "2023-12-05T21:34:59Z",
            "summary": "The recent COVID-19 pandemic has thrown the importance of accurately\nforecasting contagion dynamics and learning infection parameters into sharp\nfocus. At the same time, effective policy-making requires knowledge of the\nuncertainty on such predictions, in order, for instance, to be able to ready\nhospitals and intensive care units for a worst-case scenario without needlessly\nwasting resources. In this work, we apply a novel and powerful computational\nmethod to the problem of learning probability densities on contagion parameters\nand providing uncertainty quantification for pandemic projections. Using a\nneural network, we calibrate an ODE model to data of the spread of COVID-19 in\nBerlin in 2020, achieving both a significantly more accurate calibration and\nprediction than Markov-Chain Monte Carlo (MCMC)-based sampling schemes. The\nuncertainties on our predictions provide meaningful confidence intervals e.g.\non infection figures and hospitalisation rates, while training and running the\nneural scheme takes minutes where MCMC takes hours. We show convergence of our\nmethod to the true posterior on a simplified SIR model of epidemics, and also\ndemonstrate our method's learning capabilities on a reduced dataset, where a\ncomplex model is learned from a small number of compartments for which data is\navailable.",
            "author": [
                "Thomas Gaskin",
                "Tim Conrad",
                "Grigorios A. Pavliotis",
                "Christof Sch\u00fctte"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03147v1",
                "http://arxiv.org/pdf/2312.03147v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "physics.soc-ph",
                "49-02, 92-02, 68-02",
                "J.3; G.1.6; I.2.1; G.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03145v1",
            "title": "Maximum likelihood thresholds of Gaussian graphical models and graphical\n  lasso",
            "updated": "2023-12-05T21:29:37Z",
            "published": "2023-12-05T21:29:37Z",
            "summary": "Associated to each graph G is a Gaussian graphical model. Such models are\noften used in high-dimensional settings, i.e. where there are relatively few\ndata points compared to the number of variables. The maximum likelihood\nthreshold of a graph is the minimum number of data points required to fit the\ncorresponding graphical model using maximum likelihood estimation. Graphical\nlasso is a method for selecting and fitting a graphical model. In this project,\nwe ask: when graphical lasso is used to select and fit a graphical model on n\ndata points, how likely is it that n is greater than or equal to the maximum\nlikelihood threshold of the corresponding graph? Our results are a series of\ncomputational experiments.",
            "author": [
                "Daniel Irving Bernstein",
                "Hayden Outlaw"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03145v1",
                "http://arxiv.org/pdf/2312.03145v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03142v1",
            "title": "Central limit theorem for the average closure coefficient",
            "updated": "2023-12-05T21:22:09Z",
            "published": "2023-12-05T21:22:09Z",
            "summary": "Many real-world networks exhibit the phenomenon of edge clustering, which is\ntypically measured by the average clustering coefficient. Recently, an\nalternative measure, the average closure coefficient, is proposed to quantify\nlocal clustering. It is shown that the average closure coefficient possesses a\nnumber of useful properties and can capture complementary information missed by\nthe classical average clustering coefficient. In this paper, we study the\nasymptotic distribution of the average closure coefficient of a heterogeneous\nErd\\\"{o}s-R\\'{e}nyi random graph. We prove that the standardized average\nclosure coefficient converges in distribution to the standard normal\ndistribution. In the Erd\\\"{o}s-R\\'{e}nyi random graph, the variance of the\naverage closure coefficient exhibits the same phase transition phenomenon as\nthe average clustering coefficient.",
            "author": [
                "Mingao Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03142v1",
                "http://arxiv.org/pdf/2312.03142v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03141v1",
            "title": "In-Storage Acceleration of Graph-Traversal-Based Approximate Nearest\n  Neighbor Search",
            "updated": "2023-12-05T21:21:01Z",
            "published": "2023-12-05T21:21:01Z",
            "summary": "Approximate nearest neighbor search (ANNS) is a key retrieval technique for\nvector database and many data center applications, such as person\nre-identification and recommendation systems. Among all the ANNS algorithms,\ngraph-traversal-based ANNS achieves the highest recall rate. However, as the\nsize of dataset increases, the graph may require hundreds of gigabytes of\nmemory, exceeding the main memory capacity of a single workstation node.\nAlthough we can do partitioning and use solid-state drive (SSD) as the backing\nstorage, the limited SSD I/O bandwidth severely degrades the performance of the\nsystem. To address this challenge, we present NDSearch, a near-data processing\n(NDP) solution for ANNS processing. NDSearch consists of a novel in-storage\ncomputing architecture, namely, SEARSSD, that supports the ANNS kernels and\nleverages logic unit (LUN)-level parallelism inside the NAND flash chips.\nNDSearch also includes a processing model that is customized for NDP and\ncooperates with SEARSSD. The processing model enables us to apply a two-level\nscheduling to improve the data locality and exploit the internal bandwidth in\nNDSEARCH, and a speculative searching mechanism to further accelerate the ANNS\nworkload. Our results show that NDSearch improves the throughput by up to\n31.7x, 14.6x, 7.4x, 2.9x over CPU, GPU, a state-of-the-art SmartSSD-only\ndesign, and DeepStore, respectively. NDSEARCH also achieves two\norders-of-magnitude higher energy efficiency than CPU and GPU.",
            "author": [
                "Yitu Wang",
                "Shiyu Li",
                "Qilin Zheng",
                "Linghao Song",
                "Zongwang Li",
                "Andrew Chang",
                "Hai \"Helen\" Li",
                "Yiran Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03141v1",
                "http://arxiv.org/pdf/2312.03141v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03140v1",
            "title": "FlexModel: A Framework for Interpretability of Distributed Large\n  Language Models",
            "updated": "2023-12-05T21:19:33Z",
            "published": "2023-12-05T21:19:33Z",
            "summary": "With the growth of large language models, now incorporating billions of\nparameters, the hardware prerequisites for their training and deployment have\nseen a corresponding increase. Although existing tools facilitate model\nparallelization and distributed training, deeper model interactions, crucial\nfor interpretability and responsible AI techniques, still demand thorough\nknowledge of distributed computing. This often hinders contributions from\nresearchers with machine learning expertise but limited distributed computing\nbackground. Addressing this challenge, we present FlexModel, a software package\nproviding a streamlined interface for engaging with models distributed across\nmulti-GPU and multi-node configurations. The library is compatible with\nexisting model distribution libraries and encapsulates PyTorch models. It\nexposes user-registerable HookFunctions to facilitate straightforward\ninteraction with distributed model internals, bridging the gap between\ndistributed and single-device model paradigms. Primarily, FlexModel enhances\naccessibility by democratizing model interactions and promotes more inclusive\nresearch in the domain of large-scale neural networks. The package is found at\nhttps://github.com/VectorInstitute/flex_model.",
            "author": [
                "Matthew Choi",
                "Muhammad Adil Asif",
                "John Willes",
                "David Emerson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03140v1",
                "http://arxiv.org/pdf/2312.03140v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03768v1",
            "title": "Algoritmo de Contagem Qu\u00e2ntico Aplicado ao Grafo Bipartido Completo",
            "updated": "2023-12-05T21:15:09Z",
            "published": "2023-12-05T21:15:09Z",
            "summary": "Studies on Quantum Computing have been developed since the 1980s, motivating\nresearches on quantum algorithms better than any classical algorithm possible.\nAn example of such algorithms is Grover's algorithm, capable of finding $k$\n(marked) elements in an unordered database with $N$ elements using\n$O(\\sqrt{N/k})$ steps. Grover's algorithm can be interpreted as a quantum walk\nin a complete graph (with loops) containing $N$ vertices from which $k$ are\nmarked. This interpretation motivated search algorithms in other graphs --\ncomplete bipartite graph, grid, and hypercube. Using Grover's algorithm's\nlinear operator, the quantum counting algorithm estimates the value of $k$ with\nan error of $O(\\sqrt{k})$ using $O(\\sqrt{N})$ steps. This work tackles the\nproblem of using the quantum counting algorithm for estimating the value $k$ of\nmarked elements in other graphs; more specifically, the complete bipartite\ngraph. It is concluded that for a particular case, running the proposed\nalgorithm at most $t$ times wields an estimation of $k$ with an error of\n$O(\\sqrt{k})$ using $O(t\\sqrt{N})$ steps and success probability of at least\n$(1 - 2^{-t})8/\\pi^2$.",
            "author": [
                "Gustavo Alves Bezerra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03768v1",
                "http://arxiv.org/pdf/2312.03768v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03114v1",
            "title": "A Note on the Asymptotic Value of the Isoperimetric Number of $J(n,2)$",
            "updated": "2023-12-05T20:19:55Z",
            "published": "2023-12-05T20:19:55Z",
            "summary": "Let $G$ be a graph on $n$ vertices and $S$ a subset of vertices of $G$; the\nboundary of $S$ is the set, $\\partial S$, of edges of $G$ connecting $ S $ to\nits complement in $G$. The isoperimetric number of $G$, is the minimum of\n$\\left| \\partial S \\right|/\\left| S \\right|$ overall $S \\subset V(G)$ of at\nmost $n/2$ vertices. Let $k \\le n$ be positive integers. The Johnson graph is\nthe graph, $J(n,k)$, whose vertices are all the subsets of size $k$ of\n$\\{1,\\dots,n\\}$, two of which are adjacent if their intersection has\ncardinality equal to $k-1$. In this paper we show that the asymptotic value of\nthe isoperimetric number of the Johnson graph $J(n,2)$ is equal to $\n(2-\\sqrt{2})n$.",
            "author": [
                "Ruy Fabila-Monroy",
                "Daniel Gregorio-Longino"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03114v1",
                "http://arxiv.org/pdf/2312.03114v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03113v1",
            "title": "GPU Graph Processing on CXL-Based Microsecond-Latency External Memory",
            "updated": "2023-12-05T20:17:38Z",
            "published": "2023-12-05T20:17:38Z",
            "summary": "In GPU graph analytics, the use of external memory such as the host DRAM and\nsolid-state drives is a cost-effective approach to processing large graphs\nbeyond the capacity of the GPU onboard memory. This paper studies the use of\nCompute Express Link (CXL) memory as alternative external memory for GPU graph\nprocessing in order to see if this emerging memory expansion technology enables\ngraph processing that is as fast as using the host DRAM. Through analysis and\nevaluation using FPGA prototypes, we show that representative GPU graph\ntraversal algorithms involving fine-grained random access can tolerate an\nexternal memory latency of up to a few microseconds introduced by the CXL\ninterface as well as by the underlying memory devices. This insight indicates\nthat microsecond-latency flash memory may be used as CXL memory devices to\nrealize even more cost-effective GPU graph processing while still achieving\nperformance close to using the host DRAM.",
            "author": [
                "Shintaro Sano",
                "Yosuke Bando",
                "Kazuhiro Hiwada",
                "Hirotsugu Kajihara",
                "Tomoya Suzuki",
                "Yu Nakanishi",
                "Daisuke Taki",
                "Akiyuki Kaneko",
                "Tatsuo Shiozawa"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3624062.3624173",
                "http://arxiv.org/abs/2312.03113v1",
                "http://arxiv.org/pdf/2312.03113v1"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03111v1",
            "title": "Parallel Proof-of-Work with DAG-Style Voting and Targeted Reward\n  Discounting",
            "updated": "2023-12-05T20:14:33Z",
            "published": "2023-12-05T20:14:33Z",
            "summary": "We present parallel proof-of-work with DAG-style voting, a novel\nproof-of-work cryptocurrency protocol that, compared to Bitcoin, provides\nbetter consistency guarantees, higher transaction throughput, lower transaction\nconfirmation latency, and higher resilience against incentive attacks. The\nsuperior consistency guarantees follow from implementing parallel\nproof-of-work, a recent consensus scheme that enforces a configurable number of\nproof-of-work votes per block. Our work is inspired by another recent protocol,\nTailstorm, which structures the individual votes as tree and mitigates\nincentive attacks by discounting the mining rewards proportionally to the depth\nof the tree. We propose to structure the votes as a directed acyclic graph\n(DAG) instead of a tree. This allows for a more targeted punishment of\noffending miners and, as we show through a reinforcement learning based attack\nsearch, makes the protocol even more resilient to incentive attacks. An\ninteresting by-product of our analysis is that parallel proof-of-work without\nreward discounting is less resilient to incentive attacks than Bitcoin in some\nrealistic network scenarios.",
            "author": [
                "Patrik Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03111v1",
                "http://arxiv.org/pdf/2312.03111v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03105v1",
            "title": "Improving Automated Algorithm Selection by Advancing Fitness Landscape\n  Analysis",
            "updated": "2023-12-05T19:53:25Z",
            "published": "2023-12-05T19:53:25Z",
            "summary": "Optimization is ubiquitous in our daily lives. In the past, (sub-)optimal\nsolutions to any problem have been derived by trial and error, sheer luck, or\nthe expertise of knowledgeable individuals. In our contemporary age, there\nthankfully exists a plethora of different algorithms that can find solutions\nmore reliably than ever before. Yet, choosing an appropriate algorithm for any\ngiven problem is challenging in itself. The field of automated algorithm\nselection provides various approaches to tackle this latest problem. This is\ndone by delegating the selection of a suitable algorithm for a given problem to\na complex computer model. This computer model is generated through the use of\nArtificial Intelligence. Many of these computer models rely on some sort of\ninformation about the problem to make a reasonable selection. Various methods\nexist to provide this informative input to the computer model in the form of\nnumerical data.\n  In this cumulative dissertation, I propose several improvements to the\ndifferent variants of informative inputs. This in turn enhances and refines the\ncurrent state-of-the-art of automated algorithm selection. Specifically, I\nidentify and address current issues with the existing body of work to\nstrengthen the foundation that future work builds upon. Furthermore, the rise\nof deep learning offers ample opportunities for automated algorithm selection.\nIn several joint works, my colleagues and I developed and evaluated several\ndifferent methods that replace the existing methods to extract an informative\ninput. Lastly, automated algorithm selection approaches have been restricted to\ncertain types of problems. I propose a method to extend the generation of\ninformative inputs to other problem types and provide an outlook on further\npromising research directions.",
            "author": [
                "Raphael Patrick Prager"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03105v1",
                "http://arxiv.org/pdf/2312.03105v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03764v1",
            "title": "Similarity-based Knowledge Transfer for Cross-Domain Reinforcement\n  Learning",
            "updated": "2023-12-05T19:26:01Z",
            "published": "2023-12-05T19:26:01Z",
            "summary": "Transferring knowledge in cross-domain reinforcement learning is a\nchallenging setting in which learning is accelerated by reusing knowledge from\na task with different observation and/or action space. However, it is often\nnecessary to carefully select the source of knowledge for the receiving end to\nbenefit from the transfer process. In this article, we study how to measure the\nsimilarity between cross-domain reinforcement learning tasks to select a source\nof knowledge that will improve the performance of the learning agent. We\ndeveloped a semi-supervised alignment loss to match different spaces with a set\nof encoder-decoders, and use them to measure similarity and transfer policies\nacross tasks. In comparison to prior works, our method does not require data to\nbe aligned, paired or collected by expert policies. Experimental results, on a\nset of varied Mujoco control tasks, show the robustness of our method in\neffectively selecting and transferring knowledge, without the supervision of a\ntailored set of source tasks.",
            "author": [
                "Sergio A. Serrano",
                "Jose Martinez-Carranza",
                "L. Enrique Sucar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03764v1",
                "http://arxiv.org/pdf/2312.03764v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "68T37, 68T42, 68T07, 68T05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03093v1",
            "title": "RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and\n  Editor",
            "updated": "2023-12-05T19:25:38Z",
            "published": "2023-12-05T19:25:38Z",
            "summary": "In this paper, we present RESIN-EDITOR, an interactive event graph visualizer\nand editor designed for analyzing complex events. Our RESIN-EDITOR system\nallows users to render and freely edit hierarchical event graphs extracted from\nmultimedia and multi-document news clusters with guidance from human-curated\nevent schemas. RESIN-EDITOR's unique features include hierarchical graph\nvisualization, comprehensive source tracing, and interactive user editing,\nwhich is more powerful and versatile than existing Information Extraction (IE)\nvisualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in\nwhich our tool is effective in understanding complex events and enhancing\nsystem performance. The source code, a video demonstration, and a live website\nfor RESIN-EDITOR have been made publicly available.",
            "author": [
                "Khanh Duy Nguyen",
                "Zixuan Zhang",
                "Reece Suchocki",
                "Sha Li",
                "Martha Palmer",
                "Susan Brown",
                "Jiawei Han",
                "Heng Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03093v1",
                "http://arxiv.org/pdf/2312.03093v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03092v1",
            "title": "Coloring Groups",
            "updated": "2023-12-05T19:23:16Z",
            "published": "2023-12-05T19:23:16Z",
            "summary": "We introduce coloring groups, which are permutation groups obtained from a\nproper edge coloring of a graph. These groups generalize the generalized toggle\ngroups of Striker (which themselves generalize the toggle groups introduced by\nCameron and Fon-der-Flaass). We present some general results connecting the\nstructure of a coloring group to the structure of its graph coloring, providing\ngraph-theoretic characterizations of the centralizer and primitivity of a\ncoloring group. We apply these results particularly to generalized toggle\ngroups arising from trees as well as coloring groups arising from the\nindependence posets introduced by Thomas and Williams.",
            "author": [
                "Ben Adenbaum",
                "Alexander Wilson"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03092v1",
                "http://arxiv.org/pdf/2312.03092v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR",
                "05E18, 06A75, 05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03088v1",
            "title": "LLMs for Multi-Modal Knowledge Extraction and Analysis in\n  Intelligence/Safety-Critical Applications",
            "updated": "2023-12-05T19:04:50Z",
            "published": "2023-12-05T19:04:50Z",
            "summary": "Large Language Models have seen rapid progress in capability in recent years;\nthis progress has been accelerating and their capabilities, measured by various\nbenchmarks, are beginning to approach those of humans. There is a strong demand\nto use such models in a wide variety of applications but, due to unresolved\nvulnerabilities and limitations, great care needs to be used before applying\nthem to intelligence and safety-critical applications. This paper reviews\nrecent literature related to LLM assessment and vulnerabilities to synthesize\nthe current research landscape and to help understand what advances are most\ncritical to enable use of of these technologies in intelligence and\nsafety-critical applications. The vulnerabilities are broken down into ten\nhigh-level categories and overlaid onto a high-level life cycle of an LLM. Some\ngeneral categories of mitigations are reviewed.",
            "author": [
                "Brett Israelsen",
                "Soumalya Sarkar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03088v1",
                "http://arxiv.org/pdf/2312.03088v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03087v1",
            "title": "Higher-rank dimer models",
            "updated": "2023-12-05T19:03:59Z",
            "published": "2023-12-05T19:03:59Z",
            "summary": "Let $G$ be a bipartite planar graph with edges directed from black to white.\nFor each vertex $v$ let $n_v$ be a positive integer. A multiweb in $G$ is a\nmultigraph with multiplicity $n_v$ at vertex $v$. A connection is a choice of\nlinear maps on edges $\\Phi=\\{\\phi_{bw}\\}_{bw\\in E}$ where $\\phi_{bw}\\in\n\\mathrm{Hom}({\\mathbb R}^{n_b},{\\mathbb R}^{n_w})$. Associated to $\\Phi$ is a\nfunction on multiwebs, the trace $Tr_{\\Phi}$. We define an associated Kasteleyn\nmatrix $K=K(\\Phi)$ in this setting and write $\\det K$ as the sum of traces of\nall multiwebs. This generalizes Kasteleyn's theorem and the result of [Douglas,\nKenyon, Shi: Dimers, webs, and local systems, Trans. AMS 2023].\n  We study connections with positive traces, and define the associated\nprobability measure on multiwebs. By careful choice of connection we can thus\nencode the \"free fermionic\" subvarieties for vertex models such as the\n$6$-vertex model and $20$-vertex models, and in particular give determinantal\nsolutions.\n  We also find for each multiweb system an equivalent scalar system, that is, a\nplanar bipartite graph $H$ and a local measure-preserving mapping from dimer\ncovers of $H$ to multiwebs on $G$. We identify a family of positive connections\nas those whose scalar versions have positive face weights.",
            "author": [
                "Richard Kenyon",
                "Nicholas Ovenhouse"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03087v1",
                "http://arxiv.org/pdf/2312.03087v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "82B20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03052v1",
            "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning\n  into Vision-Language Models",
            "updated": "2023-12-05T18:58:37Z",
            "published": "2023-12-05T18:58:37Z",
            "summary": "Solving complex visual tasks such as \"Who invented the musical instrument on\nthe right?\" involves a composition of skills: understanding space, recognizing\ninstruments, and also retrieving prior knowledge. Recent work shows promise by\ndecomposing such tasks using a large language model (LLM) into an executable\nprogram that invokes specialized vision models. However, generated programs are\nerror-prone: they omit necessary steps, include spurious ones, and are unable\nto recover when the specialized models give incorrect outputs. Moreover, they\nrequire loading multiple models, incurring high latency and computation costs.\nWe propose Visual Program Distillation (VPD), an instruction tuning framework\nthat produces a vision-language model (VLM) capable of solving complex visual\ntasks with a single forward pass. VPD distills the reasoning ability of LLMs by\nusing them to sample multiple candidate programs, which are then executed and\nverified to identify a correct one. It translates each correct program into a\nlanguage description of the reasoning steps, which are then distilled into a\nVLM. Extensive experiments show that VPD improves the VLM's ability to count,\nunderstand spatial relations, and reason compositionally. Our VPD-trained\nPaLI-X outperforms all prior VLMs, achieving state-of-the-art performance\nacross complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE,\nand Hateful Memes. An evaluation with human annotators also confirms that VPD\nimproves model response factuality and consistency. Finally, experiments on\ncontent moderation demonstrate that VPD is also helpful for adaptation to\nreal-world applications with limited data.",
            "author": [
                "Yushi Hu",
                "Otilia Stretcu",
                "Chun-Ta Lu",
                "Krishnamurthy Viswanathan",
                "Kenji Hata",
                "Enming Luo",
                "Ranjay Krishna",
                "Ariel Fuxman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03052v1",
                "http://arxiv.org/pdf/2312.03052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03761v1",
            "title": "Learning High-Dimensional Differential Graphs From Multi-Attribute Data",
            "updated": "2023-12-05T18:54:46Z",
            "published": "2023-12-05T18:54:46Z",
            "summary": "We consider the problem of estimating differences in two Gaussian graphical\nmodels (GGMs) which are known to have similar structure. The GGM structure is\nencoded in its precision (inverse covariance) matrix. In many applications one\nis interested in estimating the difference in two precision matrices to\ncharacterize underlying changes in conditional dependencies of two sets of\ndata. Existing methods for differential graph estimation are based on\nsingle-attribute (SA) models where one associates a scalar random variable with\neach node. In multi-attribute (MA) graphical models, each node represents a\nrandom vector. In this paper, we analyze a group lasso penalized D-trace loss\nfunction approach for differential graph learning from multi-attribute data. An\nalternating direction method of multipliers (ADMM) algorithm is presented to\noptimize the objective function. Theoretical analysis establishing consistency\nin support recovery and estimation in high-dimensional settings is provided.\nNumerical results based on synthetic as well as real data are presented.",
            "author": [
                "Jitendra K Tugnait"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03761v1",
                "http://arxiv.org/pdf/2312.03761v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02964v1",
            "title": "Some locally Kneser graphs",
            "updated": "2023-12-05T18:50:56Z",
            "published": "2023-12-05T18:50:56Z",
            "summary": "The Kneser graph $K(n,d)$ is the graph on the $d$-subsets of an $n$-set,\nadjacent when disjoint. Clearly, $K(n+d,d)$ is locally $K(n,d)$. Hall showed\nfor $n \\ge 3d+1$ that there are no further examples. Here we give other\nexamples of locally $K(n,d)$ graphs for $n = 3d$, and some further sporadic\nexamples. It follows that Hall's bound is best possible.",
            "author": [
                "A. E. Brouwer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02964v1",
                "http://arxiv.org/pdf/2312.02964v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05B30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03050v1",
            "title": "HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation\n  in Video Understanding",
            "updated": "2023-12-05T18:47:19Z",
            "published": "2023-12-05T18:47:19Z",
            "summary": "Visual interactivity understanding within visual scenes presents a\nsignificant challenge in computer vision. Existing methods focus on complex\ninteractivities while leveraging a simple relationship model. These methods,\nhowever, struggle with a diversity of appearance, situation, position,\ninteraction, and relation in videos. This limitation hinders the ability to\nfully comprehend the interplay within the complex visual dynamics of subjects.\nIn this paper, we delve into interactivities understanding within visual\ncontent by deriving scene graph representations from dense interactivities\namong humans and objects. To achieve this goal, we first present a new dataset\ncontaining Appearance-Situation-Position-Interaction-Relation predicates, named\nASPIRe, offering an extensive collection of videos marked by a wide range of\ninteractivities. Then, we propose a new approach named Hierarchical\nInterlacement Graph (HIG), which leverages a unified layer and graph within a\nhierarchical structure to provide deep insights into scene changes across five\ndistinct tasks. Our approach demonstrates superior performance to other methods\nthrough extensive experiments conducted in various scenarios.",
            "author": [
                "Trong-Thuan Nguyen",
                "Pha Nguyen",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03050v1",
                "http://arxiv.org/pdf/2312.03050v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02952v1",
            "title": "Simple evolving random graphs",
            "updated": "2023-12-05T18:38:44Z",
            "published": "2023-12-05T18:38:44Z",
            "summary": "We study the evolution of graphs with $N$ vertices densifying by adding\nedges. Two vertices are chosen randomly, with rate $(2N)^{-1}$, and an edge is\n(i) established if each vertex belongs to a tree; (ii) established with\nprobability $p$ if only one vertex belongs to a tree; (iii) an attempt fails if\nboth vertices belong to unicyclic components. This densification process\ngenerates random graphs with simple components, viz., trees and unicycles. In\nthe $N\\to\\infty$ limit, the fraction of vertices in unicycles vanishes when\n$t\\leq 1$ and becomes positive when $t>1$. A similar phase transition occurs in\nclassical random graphs where all attempts to add an edge are successful. When\n$N\\to\\infty$ and $t<1$, classical random graphs are simple. In the\nsupercritical phase, a classical random graph contains a (complex) giant\ncomponent that eventually engulfs all finite components and continues to\ndensify forever, or until the graph becomes complete if loops and multiple\nedges are forbidden. The evolution of simple random graphs freezes when trees\ndisappear, and only unicycles remain.",
            "author": [
                "P. L. Krapivsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02952v1",
                "http://arxiv.org/pdf/2312.02952v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02941v1",
            "title": "Fast CT anatomic localization algorithm",
            "updated": "2023-12-05T18:09:47Z",
            "published": "2023-12-05T18:09:47Z",
            "summary": "Automatically determining the position of every slice in a CT scan is a basic\nyet powerful capability allowing fast retrieval of region of interest for\nvisual inspection and automated analysis. Unlike conventional localization\napproaches which work at the slice level, we directly localize only a fraction\nof the slices and and then fit a linear model which maps slice index to its\nestimated axial anatomical position based on those slices. The model is then\nused to assign axial position to every slices of the scan. This approach proves\nto be both computationally efficient, with a typical processing time of less\nthan a second per scan (regardless of its size), accurate, with a typical\nmedian localization error of 1 cm, and robust to different noise sources,\nimaging protocols, metal induced artifacts, anatomical deformations etc.\nAnother key element of our approach is the introduction of a mapping confidence\nscore. This score acts as a fail safe mechanism which allows a rejection of\nunreliable localization results in rare cases of anomalous scans. Our algorithm\nsets new State Of The Art results in terms of localization accuracy. It also\noffers a decrease of two orders of magnitude in processing time with respect to\nall published processing times. It was designed to be invariant to various scan\nresolutions, scan protocols, patient orientations, strong artifacts and various\ndeformations and abnormalities. Additionally, our algorithm is the first one to\nthe best of our knowledge which supports the entire body from head to feet and\nis not confined to specific anatomical region. This algorithm was tested on\nthousands of scans and proves to be very reliable and useful as a preprocessing\nstage for many applications.",
            "author": [
                "Amit Oved"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02941v1",
                "http://arxiv.org/pdf/2312.02941v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02916v1",
            "title": "MIND: Multi-Task Incremental Network Distillation",
            "updated": "2023-12-05T17:46:52Z",
            "published": "2023-12-05T17:46:52Z",
            "summary": "The recent surge in pervasive devices generating dynamic data streams has\nunderscored the necessity for learning systems to adapt to data distributional\nshifts continually. To tackle this challenge, the research community has put\nforth a spectrum of methodologies, including the demanding pursuit of\nclass-incremental learning without replay data. In this study, we present MIND,\na parameter isolation method that aims to significantly enhance the performance\nof replay-free solutions and achieve state-of-the-art results on several widely\nstudied datasets. Our approach introduces two main contributions: two\nalternative distillation procedures that significantly improve the efficiency\nof MIND increasing the accumulated knowledge of each sub-network, and the\noptimization of the BachNorm layers across tasks inside the sub-networks.\nOverall, MIND outperforms all the state-of-the-art methods for rehearsal-free\nClass-Incremental learning (with an increment in classification accuracy of\napprox. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.\n+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each\ncontribution to demonstrate its impact on performance improvement. Our results\nshowcase the superior performance of MIND indicating its potential for\naddressing the challenges posed by Class-incremental and Domain-Incremental\nlearning in resource-constrained environments.",
            "author": [
                "Jacopo Bonato",
                "Francesco Pelosin",
                "Luigi Sabetta",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02916v1",
                "http://arxiv.org/pdf/2312.02916v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02911v1",
            "title": "Accretion processes onto black holes: theoretical problems,\n  observational constraints",
            "updated": "2023-12-05T17:36:06Z",
            "published": "2023-12-05T17:36:06Z",
            "summary": "We shortly summarize the standard current knowledge on the structure of the\naccretion flow onto black holes in galactic binary systems and in active\ngalactic nuclei. We stress the similarities and differences between the two\ntypes of systems, and we highlight the complementarity of the data caused by\nthese differences. We highlight some new developments and list the unsolved\nproblems.",
            "author": [
                "Bozena Czerny",
                "Marzena Sniegowska",
                "Agnieszka Janiuk",
                "Bei You"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02911v1",
                "http://arxiv.org/pdf/2312.02911v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02897v1",
            "title": "Perspectives from Naive Participants and Experienced Social Science\n  Researchers on Addressing Embodiment in a Virtual Cyberball Task",
            "updated": "2023-12-05T17:09:59Z",
            "published": "2023-12-05T17:09:59Z",
            "summary": "We describe the design of an immersive virtual Cyberball task that included\navatar customization, and user feedback on this design. We first created a\nprototype of an avatar customization template and added it to a Cyberball\nprototype built in the Unity3D game engine. Then, we conducted in-depth user\ntesting and feedback sessions with 15 Cyberball stakeholders: five naive\nparticipants with no prior knowledge of Cyberball and ten experienced\nresearchers with extensive experience using the Cyberball paradigm. We report\nthe divergent perspectives of the two groups on the following design insights;\ndesigning for intuitive use, inclusivity, and realistic experiences versus\nminimalism. Participant responses shed light on how system design problems may\ncontribute to or perpetuate negative experiences when customizing avatars. They\nalso demonstrate the value of considering multiple stakeholders' feedback in\nthe design process for virtual reality, presenting a more comprehensive view in\ndesigning future Cyberball prototypes and interactive systems for social\nscience research.",
            "author": [
                "Tao Long",
                "Swati Pandita",
                "Andrea Stevenson Won"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02897v1",
                "http://arxiv.org/pdf/2312.02897v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02888v1",
            "title": "Compensation of front-end and modulation delays in phase and ranging\n  measurements for time-delay interferometry",
            "updated": "2023-12-05T17:01:35Z",
            "published": "2023-12-05T17:01:35Z",
            "summary": "In the context of the Laser Interferometer Space Antenna (LISA), the laser\nsubsystems exhibit frequency fluctuations that introduce significant levels of\nnoise into the measurements, surpassing the gravitational wave signal by\nseveral orders of magnitude. Mitigation is achieved via time-shifting\nindividual measurements in a data processing step known as time-delay\ninterferometry (TDI). The suppression performance of TDI relies on accurate\nknowledge and consideration of the delays experienced by the interfering\nlasers. While considerable efforts have been dedicated to the accurate\ndetermination of inter-spacecraft ranging delays, the sources for onboard\ndelays have been either neglected or assumed to be known. Contrary to these\nassumptions, analog delays of the phasemeter front end and the laser modulator\nare not only large but also prone to change with temperature and heterodyne\nfrequency. This motivates our proposal for a novel method enabling a\ncalibration of these delays on-ground and in-space, based on minimal functional\nadditions to the receiver architecture. Specifically, we establish a set of\ncalibration measurements and elucidate how these measurements are utilized in\ndata processing, leading to the mitigation of the delays in the TDI Michelson\nvariables. Following a performance analysis of the calibration measurements,\nproposed calibration scheme is assessed through numerical simulations. We find\nthat in the absence of the calibration scheme, the assumed drifts of the analog\ndelays increase residual laser noise at high frequencies of the LISA\nmeasurement band. A single, on-ground calibration of the analog delays leads to\nan improvement by roughly one order of magnitude, while re-calibration in space\nmay improve performance by yet another order of magnitude. Towards lower\nfrequencies, ranging error is always found to be the limiting factor for which\ncountermeasures are discussed.",
            "author": [
                "Philipp Euringer",
                "Niklas Houba",
                "Gerald Hechenblaikner",
                "Oliver Mandel",
                "Francis Soualle",
                "Walter Fichter"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02888v1",
                "http://arxiv.org/pdf/2312.02888v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02884v1",
            "title": "Last passage percolation and limit theorems in Barak-Erd\u0151s directed\n  random graphs and related models",
            "updated": "2023-12-05T16:56:49Z",
            "published": "2023-12-05T16:56:49Z",
            "summary": "We consider directed random graphs, the prototype of which being the\nBarak-Erd\\H{o}s graph $\\overrightarrow G(\\mathbb Z, p)$, and study the way that\nlong (or heavy, if weights are present) paths grow. This is done by relating\nthe graphs to certain particle systems that we call Infinite Bin Models (IBM).\nA number of limit theorems are shown. The goal of this paper is to present\nresults along with techniques that have been used in this area. In the case of\n$\\overrightarrow G(\\mathbb Z, p)$ the last passage percolation constant $C(p)$\nis studied in great detail. It is shown that $C(p)$ is analytic for $p>0$, has\nan interesting asymptotic expansion at $p=1$ and that $C(p)/p$ converges to $e$\nlike $1/(\\log p)^2$ as $p \\to 0$. The paper includes the study of IBMs as\nmodels on their own as well as their connections to stochastic models of\nbranching processes in continuous or discrete time with selection. Several\nproofs herein are new or simplified versions of published ones. Regenerative\ntechniques are used where possible, exhibiting random sets of vertices over\nwhich the graphs regenerate. When edges have random weights we show how the\nlast passage percolation constants behave and when central limit theorems\nexist. When the underlying vertex set is partially ordered, new phenomena\noccur, e.g., there are relations with last passage Brownian percolation. We\nalso look at weights that may possibly take negative values and study in detail\nsome special cases that require combinatorial/graph theoretic techniques that\nexhibit some interesting non-differentiability properties of the last passage\npercolation constant. We also explain how to approach the problem of estimation\nof last passage percolation constants by means of perfect simulation.",
            "author": [
                "Sergey Foss",
                "Takis Konstantopoulos",
                "Bastien Mallein",
                "Sanjay Ramassamy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02884v1",
                "http://arxiv.org/pdf/2312.02884v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02870v1",
            "title": "Replica analysis of overfitting in regression models for time to event\n  data: the impact of censoring",
            "updated": "2023-12-05T16:39:02Z",
            "published": "2023-12-05T16:39:02Z",
            "summary": "We use statistical mechanics techniques, viz. the replica method, to model\nthe effect of censoring on overfitting in Cox's proportional hazards model, the\ndominant regression method for time-to-event data. In the overfitting regime,\nMaximum Likelihood parameter estimators are known to be biased already for\nsmall values of the ratio of the number of covariates over the number of\nsamples. The inclusion of censoring was avoided in previous overfitting\nanalyses for mathematical convenience, but is vital to make any theory\napplicable to real-world medical data, where censoring is ubiquitous. Upon\nconstructing efficient algorithms for solving the new (and more complex) RS\nequations and comparing the solutions with numerical simulation data, we find\nexcellent agreement, even for large censoring rates. We then address the\npractical problem of using the theory to correct the biased ML estimators\n{without} knowledge of the data-generating distribution. This is achieved via a\nnovel numerical algorithm that self-consistently approximates all relevant\nparameters of the data generating distribution while simultaneously solving the\nRS equations. We investigate numerically the statistics of the corrected\nestimators, and show that the proposed new algorithm indeed succeeds in\nremoving the bias of the ML estimators, for both the association parameters and\nfor the cumulative hazard.",
            "author": [
                "Emanuele Massa",
                "Alexander Mozeika",
                "Anthony Coolen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02870v1",
                "http://arxiv.org/pdf/2312.02870v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cond-mat.dis-nn",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03043v1",
            "title": "Navigating the Synthetic Realm: Harnessing Diffusion-based Models for\n  Laparoscopic Text-to-Image Generation",
            "updated": "2023-12-05T16:20:22Z",
            "published": "2023-12-05T16:20:22Z",
            "summary": "Recent advances in synthetic imaging open up opportunities for obtaining\nadditional data in the field of surgical imaging. This data can provide\nreliable supplements supporting surgical applications and decision-making\nthrough computer vision. Particularly the field of image-guided surgery, such\nas laparoscopic and robotic-assisted surgery, benefits strongly from synthetic\nimage datasets and virtual surgical training methods. Our study presents an\nintuitive approach for generating synthetic laparoscopic images from short text\nprompts using diffusion-based generative models. We demonstrate the usage of\nstate-of-the-art text-to-image architectures in the context of laparoscopic\nimaging with regard to the surgical removal of the gallbladder as an example.\nResults on fidelity and diversity demonstrate that diffusion-based models can\nacquire knowledge about the style and semantics in the field of image-guided\nsurgery. A validation study with a human assessment survey underlines the\nrealistic nature of our synthetic data, as medical personnel detects actual\nimages in a pool with generated images causing a false-positive rate of 66%. In\naddition, the investigation of a state-of-the-art machine learning model to\nrecognize surgical actions indicates enhanced results when trained with\nadditional generated images of up to 5.20%. Overall, the achieved image quality\ncontributes to the usage of computer-generated images in surgical applications\nand enhances its path to maturity.",
            "author": [
                "Simeon Allmendinger",
                "Patrick Hemmer",
                "Moritz Queisner",
                "Igor Sauer",
                "Leopold M\u00fcller",
                "Johannes Jakubik",
                "Michael V\u00f6ssing",
                "Niklas K\u00fchl"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03043v1",
                "http://arxiv.org/pdf/2312.03043v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.AI",
                "cs.CV",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02845v1",
            "title": "A Review of Password-less User Authentication Schemes",
            "updated": "2023-12-05T15:57:40Z",
            "published": "2023-12-05T15:57:40Z",
            "summary": "Since the demise of the password was predicted in 2004, different attempts in\nindustry and academia have been made to create an alternative for the use of\npasswords in authentication, without compromising on security and user\nexperience. This review examines password-less authentication schemes that have\nbeen proposed since after the death knell was placed on passwords in 2004. We\nstart with a brief discussion of the requirements of authentication systems and\nthen identify various password-less authentication proposals to date. We then\nevaluate the truly password-less and practical schemes using a framework that\nexamines authentication credentials based on their impact on user experience,\noverall security, and ease of deployment. The findings of this review observe a\ndifficulty in balancing security with a user experience compared to that of\npasswords in new password-less schemes, providing the opportunity for new\napplied research to leverage existing knowledge and combine technologies and\ntechniques in innovative ways that can address this imbalance.",
            "author": [
                "Tunde Oduguwa",
                "Abdullahi Arabo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02845v1",
                "http://arxiv.org/pdf/2312.02845v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02826v1",
            "title": "Calibrated Adaptive Teacher for Domain Adaptive Intelligent Fault\n  Diagnosis",
            "updated": "2023-12-05T15:19:29Z",
            "published": "2023-12-05T15:19:29Z",
            "summary": "Intelligent Fault Diagnosis (IFD) based on deep learning has proven to be an\neffective and flexible solution, attracting extensive research. Deep neural\nnetworks can learn rich representations from vast amounts of representative\nlabeled data for various applications. In IFD, they achieve high classification\nperformance from signals in an end-to-end manner, without requiring extensive\ndomain knowledge. However, deep learning models usually only perform well on\nthe data distribution they have been trained on. When applied to a different\ndistribution, they may experience performance drops. This is also observed in\nIFD, where assets are often operated in working conditions different from those\nin which labeled data have been collected. Unsupervised domain adaptation (UDA)\ndeals with the scenario where labeled data are available in a source domain,\nand only unlabeled data are available in a target domain, where domains may\ncorrespond to operating conditions. Recent methods rely on training with\nconfident pseudo-labels for target samples. However, the confidence-based\nselection of pseudo-labels is hindered by poorly calibrated confidence\nestimates in the target domain, primarily due to over-confident predictions,\nwhich limits the quality of pseudo-labels and leads to error accumulation. In\nthis paper, we propose a novel UDA method called Calibrated Adaptive Teacher\n(CAT), where we propose to calibrate the predictions of the teacher network\nthroughout the self-training process, leveraging post-hoc calibration\ntechniques. We evaluate CAT on domain-adaptive IFD and perform extensive\nexperiments on the Paderborn benchmark for bearing fault diagnosis under\nvarying operating conditions. Our proposed method achieves state-of-the-art\nperformance on most transfer tasks.",
            "author": [
                "Florent Forest",
                "Olga Fink"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02826v1",
                "http://arxiv.org/pdf/2312.02826v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "eess.SP",
                "stat.ML",
                "68T07, 62H30",
                "I.2.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03038v1",
            "title": "Sample-based Dynamic Hierarchical Transformer with Layer and Head\n  Flexibility via Contextual Bandit",
            "updated": "2023-12-05T15:04:11Z",
            "published": "2023-12-05T15:04:11Z",
            "summary": "Transformer requires a fixed number of layers and heads which makes them\ninflexible to the complexity of individual samples and expensive in training\nand inference. To address this, we propose a sample-based Dynamic Hierarchical\nTransformer (DHT) model whose layers and heads can be dynamically configured\nwith single data samples via solving contextual bandit problems. To determine\nthe number of layers and heads, we use the Uniform Confidence Bound while we\ndeploy combinatorial Thompson Sampling in order to select specific head\ncombinations given their number. Different from previous work that focuses on\ncompressing trained networks for inference only, DHT is not only advantageous\nfor adaptively optimizing the underlying network architecture during training\nbut also has a flexible network for efficient inference. To the best of our\nknowledge, this is the first comprehensive data-driven dynamic transformer\nwithout any additional auxiliary neural networks that implement the dynamic\nsystem. According to the experiment results, we achieve up to 74% computational\nsavings for both training and inference with a minimal loss of accuracy.",
            "author": [
                "Fanfei Meng",
                "Lele Zhang",
                "Yu Chen",
                "Yuxin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03038v1",
                "http://arxiv.org/pdf/2312.03038v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02818v1",
            "title": "Optimally combined incentive for cooperation among interacting agents in\n  population games",
            "updated": "2023-12-05T15:01:57Z",
            "published": "2023-12-05T15:01:57Z",
            "summary": "Combined prosocial incentives, integrating reward for cooperators and\npunishment for defectors, are effective tools to promote cooperation among\ncompeting agents in population games. Existing research concentrated on how to\nadjust reward or punishment, as two mutually exclusive tools, during the\nevolutionary process to achieve the desired proportion of cooperators in the\npopulation, and less attention has been given to exploring a combined\nincentive-based control policy that can steer the system to the full\ncooperation state at the lowest cost. In this work we propose a combined\nincentive scheme in a population of agents whose conflicting interactions are\ndescribed by the prisoner's dilemma game on complete graphs and regular\nnetworks, respectively. By devising an index function for quantifying the\nimplementation cost of the combined incentives, we analytically construct the\noptimally combined incentive protocol by using optimal control theory. By means\nof theoretical analysis, we identify the mathematical conditions, under which\nthe optimally combined incentive scheme requires the minimal amount of cost. In\naddition to numerical calculations, we further perform computer simulations to\nverify our theoretical results and explore their robustness on different types\nof network structures.",
            "author": [
                "Shengxian Wang",
                "Ming Cao",
                "Xiaojie Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02818v1",
                "http://arxiv.org/pdf/2312.02818v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02805v1",
            "title": "Limiting Spectra of inhomogeneous random graphs",
            "updated": "2023-12-05T14:45:40Z",
            "published": "2023-12-05T14:45:40Z",
            "summary": "We consider sparse inhomogeneous Erd\\H{o}s-R\\'enyi random graph ensembles\nwhere edges are connected independently with probability $p_{ij}$. We assume\nthat $p_{ij}= \\varepsilon_N f(w_i, w_j)$ where $(w_i)_{i\\ge 1}$ is a sequence\nof deterministic weights, $f$ is a bounded function and $N\\varepsilon_N\\to\n\\lambda\\in (0,\\infty)$. We characterise the limiting moments in terms of graph\nhomomorphisms and also classify the contributing partitions. We present an\nanalytic way to determine the Stieltjes transform of the limiting measure. The\nconvergence of the empirical distribution function follows from the theory of\nlocal weak convergence in many examples but we do not rely on this theory and\nexploit combinatorial and analytic techniques to derive some interesting\nproperties of the limit. We extend the methods of Khorunzhy et al. (2004) and\nshow that a fixed point equation determines the limiting measure. The limiting\nmeasure crucially depends on $\\lambda$ and it is known that in the homogeneous\ncase, if $\\lambda\\to\\infty$, the measure converges weakly to the semicircular\nlaw (Jung and Lee (2018)). We extend this result of interpolating between the\nsparse and dense regimes to the inhomogeneous setting and show that as\n$\\lambda\\to \\infty$, the measure converges weakly to a measure which is known\nas the operator-valued semicircular law.",
            "author": [
                "Luca Avena",
                "Rajat Subhra Hazra",
                "Nandan Malhotra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02805v1",
                "http://arxiv.org/pdf/2312.02805v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.FA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02798v1",
            "title": "Weakly Supervised Detection of Hallucinations in LLM Activations",
            "updated": "2023-12-05T14:35:11Z",
            "published": "2023-12-05T14:35:11Z",
            "summary": "We propose an auditing method to identify whether a large language model\n(LLM) encodes patterns such as hallucinations in its internal states, which may\npropagate to downstream tasks. We introduce a weakly supervised auditing\ntechnique using a subset scanning approach to detect anomalous patterns in LLM\nactivations from pre-trained models. Importantly, our method does not need\nknowledge of the type of patterns a-priori. Instead, it relies on a reference\ndataset devoid of anomalies during testing. Further, our approach enables the\nidentification of pivotal nodes responsible for encoding these patterns, which\nmay offer crucial insights for fine-tuning specific sub-networks for bias\nmitigation. We introduce two new scanning methods to handle LLM activations for\nanomalous sentences that may deviate from the expected distribution in either\ndirection. Our results confirm prior findings of BERT's limited internal\ncapacity for encoding hallucinations, while OPT appears capable of encoding\nhallucination information internally. Importantly, our scanning approach,\nwithout prior exposure to false statements, performs comparably to a fully\nsupervised out-of-distribution classifier.",
            "author": [
                "Miriam Rateike",
                "Celia Cintas",
                "John Wamburu",
                "Tanya Akumu",
                "Skyler Speakman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02798v1",
                "http://arxiv.org/pdf/2312.02798v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02793v1",
            "title": "Causal flow preserving optimisation of quantum circuits in the\n  ZX-calculus",
            "updated": "2023-12-05T14:24:44Z",
            "published": "2023-12-05T14:24:44Z",
            "summary": "Optimising quantum circuits to minimise resource usage is crucial,\nparticularly in the context of near term hardware which is limited by quantum\nvolume. This paper introduces an optimisation algorithm which aims to minimise\nnon-Clifford gate count and two-qubit gate count by building on\nZX-calculus-based strategies. By translating a circuit into a ZX-diagram it can\nbe simplified before being extracted back into a circuit. I assert that\nsimplifications preserve a graph-theoretic property called causal flow. This\nhas the advantage that qubit lines are well defined throughout, permitting a\ntrivial extraction procedure and in turn enabling the calculation of an\nindividual transformation's impact on the resulting circuit. A general\nprocedure for a decision strategy is introduced, inspired by an existing\nheuristic based method. Both phase teleportation and the neighbour unfusion\nrule are generalised. In particular, allowing unfusion of multiple neighbours\nis shown to lead to significant improvements in optimisation. When run on a set\nof benchmark circuits, the algorithm developed reduces the two-qubit gate count\nby an average of 19.6%, beating both the previous best ZX-based strategy\n(14.2%) and non-ZX strategy (18.9%). This lays a foundation for multiple\navenues of improvement. A particularly effective strategy for optimising QFT\ncircuits is also noted, resulting in exactly one two-qubit gate per\nnon-Clifford gate.",
            "author": [
                "Calum Holker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02793v1",
                "http://arxiv.org/pdf/2312.02793v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02783v1",
            "title": "Large Language Models on Graphs: A Comprehensive Survey",
            "updated": "2023-12-05T14:14:27Z",
            "published": "2023-12-05T14:14:27Z",
            "summary": "Large language models (LLMs), such as ChatGPT and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data are associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data are paired with rich textual\ninformation (e.g., molecules with descriptions). Besides, although LLMs have\nshown their pure text-based reasoning ability, it is underexplored whether such\nability can be generalized to graph scenarios (i.e., graph-based reasoning). In\nthis paper, we provide a systematic review of scenarios and techniques related\nto large language models on graphs. We first summarize potential scenarios of\nadopting LLMs on graphs into three categories, namely pure graphs, text-rich\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we mention the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
            "author": [
                "Bowen Jin",
                "Gang Liu",
                "Chi Han",
                "Meng Jiang",
                "Heng Ji",
                "Jiawei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02783v1",
                "http://arxiv.org/pdf/2312.02783v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03033v1",
            "title": "LiDAR-based Person Re-identification",
            "updated": "2023-12-05T12:44:17Z",
            "published": "2023-12-05T12:44:17Z",
            "summary": "Camera-based person re-identification (ReID) systems have been widely applied\nin the field of public security. However, cameras often lack the perception of\n3D morphological information of human and are susceptible to various\nlimitations, such as inadequate illumination, complex background, and personal\nprivacy. In this paper, we propose a LiDAR-based ReID framework, ReID3D, that\nutilizes pre-training strategy to retrieve features of 3D body shape and\nintroduces Graph-based Complementary Enhancement Encoder for extracting\ncomprehensive features. Due to the lack of LiDAR datasets, we build LReID, the\nfirst LiDAR-based person ReID dataset, which is collected in several outdoor\nscenes with variations in natural conditions. Additionally, we introduce\nLReID-sync, a simulated pedestrian dataset designed for pre-training encoders\nwith tasks of point cloud completion and shape parameter learning. Extensive\nexperiments on LReID show that ReID3D achieves exceptional performance with a\nrank-1 accuracy of 94.0, highlighting the significant potential of LiDAR in\naddressing person ReID tasks. To the best of our knowledge, we are the first to\npropose a solution for LiDAR-based ReID. The code and datasets will be released\nsoon.",
            "author": [
                "Wenxuan Guo",
                "Zhiyu Pan",
                "Yingping Liang",
                "Ziheng Xi",
                "Zhi Chen Zhong",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03033v1",
                "http://arxiv.org/pdf/2312.03033v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02725v1",
            "title": "R3D-SWIN:Use Shifted Window Attention for Single-View 3D Reconstruction",
            "updated": "2023-12-05T12:42:37Z",
            "published": "2023-12-05T12:42:37Z",
            "summary": "Recently, vision transformers have performed well in various computer vision\ntasks, including voxel 3D reconstruction. However, the windows of the vision\ntransformer are not multi-scale, and there is no connection between the\nwindows, which limits the accuracy of voxel 3D reconstruction . Therefore, we\npropose a shifted windows attention voxel 3D reconstruction network. To the\nbest of our knowledge, this is the first work to apply shifted window attention\nto voxel 3D reconstruction. Experimental results on ShapeNet verify our method\nachieves SOTA accuracy in single-view reconstruction.",
            "author": [
                "Chenhuan Li",
                "Meihua Xiao",
                "zehuan li",
                "Mengxi Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02725v1",
                "http://arxiv.org/pdf/2312.02725v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02723v1",
            "title": "Accurate and efficient approximation of large-scale appointment\n  schedules",
            "updated": "2023-12-05T12:37:54Z",
            "published": "2023-12-05T12:37:54Z",
            "summary": "Setting up optimal appointment schedules requires the computation of an\ninherently involved objective function, typically requiring distributional\nknowledge of the clients' waiting times and the server's idle times (as a\nfunction of the appointment times of the individual clients). A frequently used\nidea is to approximate the clients' service times by their phase-type\ncounterpart, thus leading to explicit expressions for the waiting-time and\nidle-time distributions. This method, however, requires the evaluation of the\nmatrix exponential of potentially large matrices, which already becomes\nprohibitively slow from, say, 20 clients on. In this paper we remedy this issue\nby recursively approximating the distributions involved relying on a\ntwo-moments fit. More specifically, we approximate the sojourn time of each of\nthe clients by a low-dimensional phase-type, Weibull or Lognormal random\nvariable with the desired mean and variance. Our computational experiments show\nthat this elementary, yet highly accurate, technique facilitates the evaluation\nof optimal appointment schedules even if the number of clients is large. The\nthree ways to approximate the sojourn-time distribution turn out to be roughly\nequally accurate, except in certain specific regimes, where the low-dimensional\nphase-type fit performs well across all instances considered. As this\nlow-dimensional phase-type fit is by far the fastest of the three alternatives,\nit is the approximation that we recommend.",
            "author": [
                "Ren\u00e9 Bekker",
                "Bharti Bharti",
                "Michel Mandjes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02723v1",
                "http://arxiv.org/pdf/2312.02723v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02720v1",
            "title": "Towards the Inferrence of Structural Similarity of Combinatorial\n  Landscapes",
            "updated": "2023-12-05T12:34:51Z",
            "published": "2023-12-05T12:34:51Z",
            "summary": "One of the most common problem-solving heuristics is by analogy. For a given\nproblem, a solver can be viewed as a strategic walk on its fitness landscape.\nThus if a solver works for one problem instance, we expect it will also be\neffective for other instances whose fitness landscapes essentially share\nstructural similarities with each other. However, due to the black-box nature\nof combinatorial optimization, it is far from trivial to infer such similarity\nin real-world scenarios. To bridge this gap, by using local optima network as a\nproxy of fitness landscapes, this paper proposed to leverage graph data mining\ntechniques to conduct qualitative and quantitative analyses to explore the\nlatent topological structural information embedded in those landscapes. By\nconducting large-scale empirical experiments on three classic combinatorial\noptimization problems, we gain concrete evidence to support the existence of\nstructural similarity between landscapes of the same classes within neighboring\ndimensions. We also interrogated the relationship between landscapes of\ndifferent problem classes.",
            "author": [
                "Mingyu Huang",
                "Ke Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02720v1",
                "http://arxiv.org/pdf/2312.02720v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02717v1",
            "title": "A Graphical Approach to Treatment Effect Estimation with Observational\n  Network Data",
            "updated": "2023-12-05T12:29:09Z",
            "published": "2023-12-05T12:29:09Z",
            "summary": "We propose an easy-to-use adjustment estimator for the effect of a treatment\nbased on observational data from a single (social) network of units. The\napproach allows for interactions among units within the network, called\ninterference, and for observed confounding. We define a simplified causal graph\nthat does not differentiate between units, called generic graph. Using valid\nadjustment sets determined in the generic graph, we can identify the treatment\neffect and build a corresponding estimator. We establish the estimator's\nconsistency and its convergence to a Gaussian limiting distribution at the\nparametric rate under certain regularity conditions that restrict the growth of\ndependencies among units. We empirically verify the theoretical properties of\nour estimator through a simulation study and apply it to estimate the effect of\na strict facial-mask policy on the spread of COVID-19 in Switzerland.",
            "author": [
                "Meta-Lina Spohn",
                "Leonard Henckel",
                "Marloes H. Maathuis"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02717v1",
                "http://arxiv.org/pdf/2312.02717v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02708v1",
            "title": "(Provable) Adversarial Robustness for Group Equivariant Tasks: Graphs,\n  Point Clouds, Molecules, and More",
            "updated": "2023-12-05T12:09:45Z",
            "published": "2023-12-05T12:09:45Z",
            "summary": "A machine learning model is traditionally considered robust if its prediction\nremains (almost) constant under input perturbations with small norm. However,\nreal-world tasks like molecular property prediction or point cloud segmentation\nhave inherent equivariances, such as rotation or permutation equivariance. In\nsuch tasks, even perturbations with large norm do not necessarily change an\ninput's semantic content. Furthermore, there are perturbations for which a\nmodel's prediction explicitly needs to change. For the first time, we propose a\nsound notion of adversarial robustness that accounts for task equivariance. We\nthen demonstrate that provable robustness can be achieved by (1) choosing a\nmodel that matches the task's equivariances (2) certifying traditional\nadversarial robustness. Certification methods are, however, unavailable for\nmany models, such as those with continuous equivariances. We close this gap by\ndeveloping the framework of equivariance-preserving randomized smoothing, which\nenables architecture-agnostic certification. We additionally derive the first\narchitecture-specific graph edit distance certificates, i.e. sound robustness\nguarantees for isomorphism equivariant tasks like node classification. Overall,\na sound notion of robustness is an important prerequisite for future work at\nthe intersection of robust and geometric machine learning.",
            "author": [
                "Jan Schuchardt",
                "Yan Scholten",
                "Stephan G\u00fcnnemann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02708v1",
                "http://arxiv.org/pdf/2312.02708v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02706v1",
            "title": "Large Knowledge Model: Perspectives and Challenges",
            "updated": "2023-12-05T12:07:30Z",
            "published": "2023-12-05T12:07:30Z",
            "summary": "Humankind's understanding of the world is fundamentally linked to our\nperception and cognition, with \\emph{human languages} serving as one of the\nmajor carriers of \\emph{world knowledge}. In this vein, \\emph{Large Language\nModels} (LLMs) like ChatGPT epitomize the pre-training of extensive,\nsequence-based world knowledge into neural networks, facilitating the\nprocessing and manipulation of this knowledge in a parametric space. This\narticle explores large models through the lens of ``knowledge''. We initially\ninvestigate the role of symbolic knowledge such as Knowledge Graphs (KGs) in\nenhancing LLMs, covering aspects like knowledge-augmented language model,\nstructure-inducing pre-training, knowledgeable prompts, structured CoT,\nknowledge editing, semantic tools for LLM and knowledgeable AI agents.\nSubsequently, we examine how LLMs can amplify traditional symbolic knowledge\nbases, encompassing aspects like using LLM as KG builder and controller,\nstructured knowledge pretraining, LLM-enhanced symbolic reasoning, and the\namalgamation of perception with cognition. Considering the intricate nature of\nhuman knowledge, we advocate for the creation of \\emph{Large Knowledge Models}\n(LKM), specifically engineered to manage diversified spectrum of knowledge\nstructures. This ambitious undertaking could entail several key challenges,\nsuch as disentangling knowledge representation from language models,\nrestructuring pre-training with structured knowledge, and building large\ncommonsense models, among others. We finally propose a five-``A'' principle to\ndistinguish the concept of LKM.",
            "author": [
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02706v1",
                "http://arxiv.org/pdf/2312.02706v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02705v1",
            "title": "Unified learning-based lossy and lossless JPEG recompression",
            "updated": "2023-12-05T12:07:27Z",
            "published": "2023-12-05T12:07:27Z",
            "summary": "JPEG is still the most widely used image compression algorithm. Most image\ncompression algorithms only consider uncompressed original image, while\nignoring a large number of already existing JPEG images. Recently, JPEG\nrecompression approaches have been proposed to further reduce the size of JPEG\nfiles. However, those methods only consider JPEG lossless recompression, which\nis just a special case of the rate-distortion theorem. In this paper, we\npropose a unified lossly and lossless JPEG recompression framework, which\nconsists of learned quantization table and Markovian hierarchical variational\nautoencoders. Experiments show that our method can achieve arbitrarily low\ndistortion when the bitrate is close to the upper bound, namely the bitrate of\nthe lossless compression model. To the best of our knowledge, this is the first\nlearned method that bridges the gap between lossy and lossless recompression of\nJPEG images.",
            "author": [
                "Jianghui Zhang",
                "Yuanyuan Wang",
                "Lina Guo",
                "Jixiang Luo",
                "Tongda Xu",
                "Yan Wang",
                "Zhi Wang",
                "Hongwei Qin"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICIP49359.2023.10222354",
                "http://arxiv.org/abs/2312.02705v1",
                "http://arxiv.org/pdf/2312.02705v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02702v1",
            "title": "Neural Sign Actors: A diffusion model for 3D sign language production\n  from text",
            "updated": "2023-12-05T12:04:34Z",
            "published": "2023-12-05T12:04:34Z",
            "summary": "Sign Languages (SL) serve as the predominant mode of communication for the\nDeaf and Hard of Hearing communities. The advent of deep learning has aided\nnumerous methods in SL recognition and translation, achieving remarkable\nresults. However, Sign Language Production (SLP) poses a challenge for the\ncomputer vision community as the motions generated must be realistic and have\nprecise semantic meanings. Most SLP methods rely on 2D data, thus impeding\ntheir ability to attain a necessary level of realism. In this work, we propose\na diffusion-based SLP model trained on a curated large-scale dataset of 4D\nsigning avatars and their corresponding text transcripts. The proposed method\ncan generate dynamic sequences of 3D avatars from an unconstrained domain of\ndiscourse using a diffusion process formed on a novel and anatomically informed\ngraph neural network defined on the SMPL-X body skeleton. Through a series of\nquantitative and qualitative experiments, we show that the proposed method\nconsiderably outperforms previous methods of SLP. We believe that this work\npresents an important and necessary step towards realistic neural sign avatars,\nbridging the communication gap between Deaf and hearing communities. The code,\nmethod and generated data will be made publicly available.",
            "author": [
                "Vasileios Baltatzis",
                "Rolandos Alexandros Potamias",
                "Evangelos Ververas",
                "Guanxiong Sun",
                "Jiankang Deng",
                "Stefanos Zafeiriou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02702v1",
                "http://arxiv.org/pdf/2312.02702v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02691v1",
            "title": "Edge coloring of products of signed graphs",
            "updated": "2023-12-05T11:50:27Z",
            "published": "2023-12-05T11:50:27Z",
            "summary": "In 2020, Behr defined the problem of edge coloring of signed graphs and\nshowed that every signed graph $(G, \\sigma)$ can be colored using exactly\n$\\Delta(G)$ or $\\Delta(G) + 1$ colors, where $\\Delta(G)$ is the maximum degree\nin graph $G$.\n  In this paper, we focus on products of signed graphs. We recall the\ndefinitions of the Cartesian, tensor, strong, and corona products of signed\ngraphs and prove results for them. In particular, we show that $(1)$ the\nCartesian product of $\\Delta$-edge-colorable signed graphs is\n$\\Delta$-edge-colorable, $(2)$ the tensor product of a $\\Delta$-edge-colorable\nsigned graph and a signed tree requires only $\\Delta$ colors and $(3)$ the\ncorona product of almost any two signed graphs is $\\Delta$-edge-colorable. We\nalso prove some results related to the coloring of products of signed paths and\ncycles.",
            "author": [
                "Robert Janczewski",
                "Krzysztof Turowski",
                "Bart\u0142omiej Wr\u00f3blewski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02691v1",
                "http://arxiv.org/pdf/2312.02691v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02687v1",
            "title": "On the Equality of Symbolic and Ordinary Powers of Binomial Edge Ideals",
            "updated": "2023-12-05T11:44:27Z",
            "published": "2023-12-05T11:44:27Z",
            "summary": "In this paper, we investigate whether the symbolic and ordinary powers of a\nbinomial edge ideal $J_{G}$ are equal. We show that the equality\n$J_{G}^{t}=J_{G}^{(t)}$ holds for every $t \\geq 1$ when $|Ass(J_{G})|=2$.\nMoreover, if $G$ is a caterpillar tree, then one has the same equality.\nFinally, we characterize the generalized caterpillar graphs which the equality\nof symbolic and ordinary powers of $J_{G}$ occurs.",
            "author": [
                "Iman Jahani",
                "Shamila Bayati",
                "Farhad Rahmati"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s00574-022-00323-7",
                "http://arxiv.org/abs/2312.02687v1",
                "http://arxiv.org/pdf/2312.02687v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02680v1",
            "title": "The extremal graphs of the minimum negative inertia index",
            "updated": "2023-12-05T11:33:52Z",
            "published": "2023-12-05T11:33:52Z",
            "summary": "Let $G$ be a graph. The numbers of positive, negative and zero eigenvalues\n(including multiplicities) of the adjacency matrix $A(G)$ is denoted by $p(G)$,\n$n(G)$ and $\\eta(G)$, respectively. In general, $n(G)$ (resp. $p(G)$) is called\nthe negative (resp. positive) inertia index of $G$. Suppose that a connected\ngraph $G$ has at least one cycle and let $g$ be the length of the shortest\ncycle in $G$. In this paper, we prove $n(G)\\geq\\lceil \\frac{g}{2}\\rceil-1$.\nFurther, the extremal graphs corresponding to $n(G)=\\lceil \\frac{g}{2}\\rceil-1$\nand $n(G)=\\lceil \\frac{g}{2}\\rceil$ are characterized, respectively.",
            "author": [
                "Songnian Xu",
                "Wenhao Zhen",
                "Dein Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02680v1",
                "http://arxiv.org/pdf/2312.02680v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP",
                "math.CO",
                "O5C50"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02661v1",
            "title": "A Self-Commissioning Edge Computing Method for Data-Driven Anomaly\n  Detection in Power Electronic Systems",
            "updated": "2023-12-05T10:56:25Z",
            "published": "2023-12-05T10:56:25Z",
            "summary": "Ensuring the reliability of power electronic converters is a matter of great\nimportance, and data-driven condition monitoring techniques are cementing\nthemselves as an important tool for this purpose. However, translating methods\nthat work well in controlled lab environments to field applications presents\nsignificant challenges, notably because of the limited diversity and accuracy\nof the lab training data. By enabling the use of field data, online machine\nlearning can be a powerful tool to overcome this problem, but it introduces\nadditional challenges in ensuring the stability and predictability of the\ntraining processes. This work presents an edge computing method that mitigates\nthese shortcomings with minimal additional memory usage, by employing an\nautonomous algorithm that prioritizes the storage of training samples with\nlarger prediction errors. The method is demonstrated on the use case of a\nself-commissioning condition monitoring system, in the form of a thermal\nanomaly detection scheme for a variable frequency motor drive, where the\nalgorithm self-learned to distinguish normal and anomalous operation with\nminimal prior knowledge. The obtained results, based on experimental data, show\na significant improvement in prediction accuracy and training speed, when\ncompared to equivalent models trained online without the proposed data\nselection process.",
            "author": [
                "Pere Izquierdo Gomez",
                "Miguel E. Lopez Gajardo",
                "Nenad Mijatovic",
                "Tomislav Dragicevic"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02661v1",
                "http://arxiv.org/pdf/2312.02661v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02651v1",
            "title": "A locally $5$-arc transitive graph related to ${\\rm PSU}_3(8)$",
            "updated": "2023-12-05T10:42:18Z",
            "published": "2023-12-05T10:42:18Z",
            "summary": "We construct a locally 5-arc transitive $G$-graph $\\Delta$, where $G \\in \\{\n{\\rm PSU}_3(8) \\rtimes C_2, {\\rm PSU}_3(8) \\rtimes C_6\\}$. The corresponding\nvertex stabilizer amalgams are of local characteristic $3$ but are not weak\n$(B,N)$-pairs. They are the first examples of this kind where there exists a\nvertex $z$ such that the extension of $G_z/O_p(G_z^{[1]})$ over\n$O_p(G_z^{[1]})$ is non-split.",
            "author": [
                "John van Bon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02651v1",
                "http://arxiv.org/pdf/2312.02651v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02646v1",
            "title": "SAMSGL: Series-Aligned Multi-Scale Graph Learning for Spatio-Temporal\n  Forecasting",
            "updated": "2023-12-05T10:37:54Z",
            "published": "2023-12-05T10:37:54Z",
            "summary": "Spatio-temporal forecasting in various domains, like traffic prediction and\nweather forecasting, is a challenging endeavor, primarily due to the\ndifficulties in modeling propagation dynamics and capturing high-dimensional\ninteractions among nodes. Despite the significant strides made by graph-based\nnetworks in spatio-temporal forecasting, there remain two pivotal factors\nclosely related to forecasting performance that need further consideration:\ntime delays in propagation dynamics and multi-scale high-dimensional\ninteractions. In this work, we present a Series-Aligned Multi-Scale Graph\nLearning (SAMSGL) framework, aiming to enhance forecasting performance. In\norder to handle time delays in spatial interactions, we propose a\nseries-aligned graph convolution layer to facilitate the aggregation of\nnon-delayed graph signals, thereby mitigating the influence of time delays for\nthe improvement in accuracy. To understand global and local spatio-temporal\ninteractions, we develop a spatio-temporal architecture via multi-scale graph\nlearning, which encompasses two essential components: multi-scale graph\nstructure learning and graph-fully connected (Graph-FC) blocks. The multi-scale\ngraph structure learning includes a global graph structure to learn both\ndelayed and non-delayed node embeddings, as well as a local one to learn node\nvariations influenced by neighboring factors. The Graph-FC blocks\nsynergistically fuse spatial and temporal information to boost prediction\naccuracy. To evaluate the performance of SAMSGL, we conduct experiments on\nmeteorological and traffic forecasting datasets, which demonstrate its\neffectiveness and superiority.",
            "author": [
                "Xiaobei Zou",
                "Luolin Xiong",
                "Yang Tang",
                "Jurgen Kurths"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02646v1",
                "http://arxiv.org/pdf/2312.02646v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02639v1",
            "title": "Dominance complexes, neighborhood complexes and combinatorial Alexander\n  duals",
            "updated": "2023-12-05T10:25:09Z",
            "published": "2023-12-05T10:25:09Z",
            "summary": "We show that the dominance complex $\\mathcal{D}(G)$ of a graph $G$ coincides\nwith the combinatorial Alexander dual of the neighborhood complex\n$\\mathcal{N}(\\overline{G})$ of the complement of $G$. Using this, we obtain a\nrelation between the chromatic number $\\chi(G)$ of $G$ and the homology group\nof $\\mathcal{D}(G)$. We also obtain several known results related to dominance\ncomplexes from well-known facts of neighborhood complexes. After that, we\nsuggest a new method for computing the homology groups of the dominance\ncomplexes, using independence complexes of simple graphs. We show that several\nknown computations of homology groups of dominance complexes can be reduced to\nknown computations of independence complexes. Finally, we determine the\nhomology group of $\\mathcal{D}(P_n \\times P_3)$ by determining the homotopy\ntypes of the independence complex of $P_n \\times P_3 \\times P_2$.",
            "author": [
                "Takahiro Matsushita",
                "Shun Wakatsuki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02639v1",
                "http://arxiv.org/pdf/2312.02639v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02638v1",
            "title": "Synchronization is All You Need: Exocentric-to-Egocentric Transfer for\n  Temporal Action Segmentation with Unlabeled Synchronized Video Pairs",
            "updated": "2023-12-05T10:24:43Z",
            "published": "2023-12-05T10:24:43Z",
            "summary": "We consider the problem of transferring a temporal action segmentation system\ninitially designed for exocentric (fixed) cameras to an egocentric scenario,\nwhere wearable cameras capture video data. The conventional supervised approach\nrequires the collection and labeling of a new set of egocentric videos to adapt\nthe model, which is costly and time-consuming. Instead, we propose a novel\nmethodology which performs the adaptation leveraging existing labeled\nexocentric videos and a new set of unlabeled, synchronized\nexocentric-egocentric video pairs, for which temporal action segmentation\nannotations do not need to be collected. We implement the proposed methodology\nwith an approach based on knowledge distillation, which we investigate both at\nthe feature and model level. To evaluate our approach, we introduce a new\nbenchmark based on the Assembly101 dataset. Results demonstrate the feasibility\nand effectiveness of the proposed method against classic unsupervised domain\nadaptation and temporal sequence alignment approaches. Remarkably, without\nbells and whistles, our best model performs on par with supervised approaches\ntrained on labeled egocentric data, without ever seeing a single egocentric\nlabel, achieving a +15.99% (28.59% vs 12.60%) improvement in the edit score on\nthe Assembly101 dataset compared to a baseline model trained solely on\nexocentric data.",
            "author": [
                "Camillo Quattrocchi",
                "Antonino Furnari",
                "Daniele Di Mauro",
                "Mario Valerio Giuffrida",
                "Giovanni Maria Farinella"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02638v1",
                "http://arxiv.org/pdf/2312.02638v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02627v1",
            "title": "Vertex stabilizers of locally $s$-arc transitive graphs of pushing up\n  type",
            "updated": "2023-12-05T10:08:15Z",
            "published": "2023-12-05T10:08:15Z",
            "summary": "Suppose that $\\Delta$ a thick, locally finite and locally $s$-arc transitive\n$G$-graph with $s \\ge 4$. For a vertex $z$ in $\\Delta$, let $G_z$ be the\nstabilizer of $z$ and $G_z^{[1]}$ be the kernel of the action of $G_z$ on the\nneighbours of $z$. We say $\\Delta$ is of pushing up type provided there exists\na prime $p$ and a $1$-arc $(x,y)$ such that $C_{G_z}(O_p(G_z^{[1]})) \\le\nO_p(G_z^{[1]})$ for $z \\in \\{x,y\\}$ and $O_p(G_x^{[1]}) \\le O_p(G_y^{[1]})$. We\nshow that if $\\Delta$ is of pushing up type, then $O_p(G_x^{[1]})$ is\nelementary abelian and $G_x/G_x^{[1]}\\cong X$ with ${\\rm PSL}_2(p^a)\\le X \\le\n{\\rm P\\Gamma L}_2(p^a)$.",
            "author": [
                "John van Bon",
                "Chris Parker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02627v1",
                "http://arxiv.org/pdf/2312.02627v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20B25"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02622v1",
            "title": "On the Initialization of Graph Neural Networks",
            "updated": "2023-12-05T09:55:49Z",
            "published": "2023-12-05T09:55:49Z",
            "summary": "Graph Neural Networks (GNNs) have displayed considerable promise in graph\nrepresentation learning across various applications. The core learning process\nrequires the initialization of model weight matrices within each GNN layer,\nwhich is typically accomplished via classic initialization methods such as\nXavier initialization. However, these methods were originally motivated to\nstabilize the variance of hidden embeddings and gradients across layers of\nFeedforward Neural Networks (FNNs) and Convolutional Neural Networks (CNNs) to\navoid vanishing gradients and maintain steady information flow. In contrast,\nwithin the GNN context classical initializations disregard the impact of the\ninput graph structure and message passing on variance. In this paper, we\nanalyze the variance of forward and backward propagation across GNN layers and\nshow that the variance instability of GNN initializations comes from the\ncombined effect of the activation function, hidden dimension, graph structure\nand message passing. To better account for these influence factors, we propose\na new initialization method for Variance Instability Reduction within GNN\nOptimization (Virgo), which naturally tends to equate forward and backward\nvariances across successive layers. We conduct comprehensive experiments on 15\ndatasets to show that Virgo can lead to superior model performance and more\nstable variance at initialization on node classification, link prediction and\ngraph classification tasks. Codes are in\nhttps://github.com/LspongebobJH/virgo_icml2023.",
            "author": [
                "Jiahang Li",
                "Yakun Song",
                "Xiang Song",
                "David Paul Wipf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02622v1",
                "http://arxiv.org/pdf/2312.02622v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02619v1",
            "title": "Rethinking and Simplifying Bootstrapped Graph Latents",
            "updated": "2023-12-05T09:49:50Z",
            "published": "2023-12-05T09:49:50Z",
            "summary": "Graph contrastive learning (GCL) has emerged as a representative paradigm in\ngraph self-supervised learning, where negative samples are commonly regarded as\nthe key to preventing model collapse and producing distinguishable\nrepresentations. Recent studies have shown that GCL without negative samples\ncan achieve state-of-the-art performance as well as scalability improvement,\nwith bootstrapped graph latent (BGRL) as a prominent step forward. However,\nBGRL relies on a complex architecture to maintain the ability to scatter\nrepresentations, and the underlying mechanisms enabling the success remain\nlargely unexplored. In this paper, we introduce an instance-level decorrelation\nperspective to tackle the aforementioned issue and leverage it as a springboard\nto reveal the potential unnecessary model complexity within BGRL. Based on our\nfindings, we present SGCL, a simple yet effective GCL framework that utilizes\nthe outputs from two consecutive iterations as positive pairs, eliminating the\nnegative samples. SGCL only requires a single graph augmentation and a single\ngraph encoder without additional parameters. Extensive experiments conducted on\nvarious graph benchmarks demonstrate that SGCL can achieve competitive\nperformance with fewer parameters, lower time and space costs, and significant\nconvergence speedup.",
            "author": [
                "Wangbin Sun",
                "Jintang Li",
                "Liang Chen",
                "Bingzhe Wu",
                "Yatao Bian",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02619v1",
                "http://arxiv.org/pdf/2312.02619v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02601v1",
            "title": "A Neural Receiver for 5G NR Multi-user MIMO",
            "updated": "2023-12-05T09:19:26Z",
            "published": "2023-12-05T09:19:26Z",
            "summary": "We introduce a neural network (NN)-based multiuser multiple-input\nmultiple-output (MU-MIMO) receiver with 5G New Radio (5G NR) physical uplink\nshared channel (PUSCH) compatibility. The NN architecture is based on\nconvolution layers to exploit the time and frequency correlation of the channel\nand a graph neural network (GNN) to handle multiple users. The proposed\narchitecture adapts to an arbitrary number of sub-carriers and supports a\nvarying number of multiple-input multiple-output (MIMO) layers and users\nwithout the need for any retraining. The receiver operates on an entire 5G NR\nslot, i.e., processes the entire received orthogonal frequency division\nmultiplexing (OFDM) time-frequency resource grid by jointly performing channel\nestimation, equalization, and demapping. The proposed architecture operates\nless than 1 dB away from a baseline using linear minimum mean square error\n(LMMSE) channel estimation with K-best detection but benefits from a\nsignificantly lower computational complexity. We show the importance of a\ncarefully designed training process such that the trained receiver is universal\nfor a wide range of different unseen channel conditions. Finally, we\ndemonstrate the results of a hardware-in-the-loop verification based on 3GPP\ncompliant conformance test scenarios.",
            "author": [
                "Sebastian Cammerer",
                "Fay\u00e7al A\u00eft Aoudia",
                "Jakob Hoydis",
                "Andreas Oeldemann",
                "Andreas Roessler",
                "Timo Mayer",
                "Alexander Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02601v1",
                "http://arxiv.org/pdf/2312.02601v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02596v1",
            "title": "TSVR+: Twin support vector regression with privileged information",
            "updated": "2023-12-05T09:15:10Z",
            "published": "2023-12-05T09:15:10Z",
            "summary": "In the realm of machine learning, the data may contain additional attributes,\nknown as privileged information (PI). The main purpose of PI is to assist in\nthe training of the model and then utilize the acquired knowledge to make\npredictions for unseen samples. Support vector regression (SVR) is an effective\nregression model, however, it has a low learning speed due to solving a convex\nquadratic problem (QP) subject to a pair of constraints. In contrast, twin\nsupport vector regression (TSVR) is more efficient than SVR as it solves two\nQPs each subject to one set of constraints. However, TSVR and its variants are\ntrained only on regular features and do not use privileged features for\ntraining. To fill this gap, we introduce a fusion of TSVR with learning using\nprivileged information (LUPI) and propose a novel approach called twin support\nvector regression with privileged information (TSVR+). The regularization terms\nin the proposed TSVR+ capture the essence of statistical learning theory and\nimplement the structural risk minimization principle. We use the successive\noverrelaxation (SOR) technique to solve the optimization problem of the\nproposed TSVR+, which enhances the training efficiency. As far as our knowledge\nextends, the integration of the LUPI concept into twin variants of regression\nmodels is a novel advancement. The numerical experiments conducted on UCI,\nstock and time series data collectively demonstrate the superiority of the\nproposed model.",
            "author": [
                "Anuradha Kumari",
                "M. Tanveer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02596v1",
                "http://arxiv.org/pdf/2312.02596v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02595v2",
            "title": "Optimal Fairness Scheduling for Coded Caching in Multi-AP Multi-antenna\n  WLAN",
            "updated": "2023-12-06T19:41:12Z",
            "published": "2023-12-05T09:12:35Z",
            "summary": "Coded caching (CC) schemes exploit the cumulative cache memory of network\nusers, outperforming traditional uncoded schemes where cache contents are only\nused locally. Interestingly, this CC gain can also be combined with the spatial\nmultiplexing gain of multi-antenna transmissions. In this paper, we extend the\nexisting results of CC-aided data delivery in multi-access point (AP) wireless\nlocal area networks (WLAN) and video streaming applications by assuming\nmulti-antenna transmitters at AP nodes. We present two distinct methods for\nusing the extra resource that multi-antenna transmitters provide. While the\nfirst method tries to reduce the number of interference links in the network\ngraph, the second one aims to remove inter-stream interference so that users\nwith similar cache contents can be served simultaneously. While both methods\nprovide increased throughput, they differ significantly in the underlying\nconcept. Numerical simulations are used to compare the performance of different\nmethods.",
            "author": [
                "Kagan Akcay",
                "MohammadJavad Salehi",
                "Antti T\u00f6lli",
                "Giuseppe Caire"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02595v2",
                "http://arxiv.org/pdf/2312.02595v2"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02592v1",
            "title": "FRAPP\u00c9: A Post-Processing Framework for Group Fairness Regularization",
            "updated": "2023-12-05T09:09:21Z",
            "published": "2023-12-05T09:09:21Z",
            "summary": "Post-processing mitigation techniques for group fairness generally adjust the\ndecision threshold of a base model in order to improve fairness. Methods in\nthis family exhibit several advantages that make them appealing in practice:\npost-processing requires no access to the model training pipeline, is agnostic\nto the base model architecture, and offers a reduced computation cost compared\nto in-processing. Despite these benefits, existing methods face other\nchallenges that limit their applicability: they require knowledge of the\nsensitive attributes at inference time and are oftentimes outperformed by\nin-processing. In this paper, we propose a general framework to transform any\nin-processing method with a penalized objective into a post-processing\nprocedure. The resulting method is specifically designed to overcome the\naforementioned shortcomings of prior post-processing approaches. Furthermore,\nwe show theoretically and through extensive experiments on real-world data that\nthe resulting post-processing method matches or even surpasses the\nfairness-error trade-off offered by the in-processing counterpart.",
            "author": [
                "Alexandru \u0162ifrea",
                "Preethi Lahoti",
                "Ben Packer",
                "Yoni Halpern",
                "Ahmad Beirami",
                "Flavien Prost"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02592v1",
                "http://arxiv.org/pdf/2312.02592v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02585v1",
            "title": "CVE representation to build attack positions graphs",
            "updated": "2023-12-05T08:57:14Z",
            "published": "2023-12-05T08:57:14Z",
            "summary": "In cybersecurity, CVEs (Common Vulnerabilities and Exposures) are publicly\ndisclosed hardware or software vulnerabilities. These vulnerabilities are\ndocumented and listed in the NVD database maintained by the NIST. Knowledge of\nthe CVEs impacting an information system provides a measure of its level of\nsecurity. This article points out that these vulnerabilities should be\ndescribed in greater detail to understand how they could be chained together in\na complete attack scenario. This article presents the first proposal for the\nCAPG format, which is a method for representing a CVE vulnerability, a\ncorresponding exploit, and associated attack positions.",
            "author": [
                "Manuel Poisson",
                "Val\u00e9rie Viet Triem Tong",
                "Gilles Guette",
                "Fr\u00e9d\u00e9ric Guih\u00e9ry",
                "Damien Cr\u00e9milleux"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02585v1",
                "http://arxiv.org/pdf/2312.02585v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03022v1",
            "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
            "updated": "2023-12-05T07:27:08Z",
            "published": "2023-12-05T07:27:08Z",
            "summary": "Knowledge graph construction (KGC) is a multifaceted undertaking involving\nthe extraction of entities, relations, and events. Traditionally, large\nlanguage models (LLMs) have been viewed as solitary task-solving agents in this\ncomplex landscape. However, this paper challenges this paradigm by introducing\na novel framework, CooperKGC. Departing from the conventional approach,\nCooperKGC establishes a collaborative processing network, assembling a KGC\ncollaboration team capable of concurrently addressing entity, relation, and\nevent extraction tasks. Our experiments unequivocally demonstrate that\nfostering collaboration and information interaction among diverse agents within\nCooperKGC yields superior results compared to individual cognitive processes\noperating in isolation. Importantly, our findings reveal that the collaboration\nfacilitated by CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.",
            "author": [
                "Hongbin Ye",
                "Honghao Gui",
                "Aijia Zhang",
                "Tong Liu",
                "Wei Hua",
                "Weiqiang Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03022v1",
                "http://arxiv.org/pdf/2312.03022v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02545v1",
            "title": "Graph Information Bottleneck for Remote Sensing Segmentation",
            "updated": "2023-12-05T07:23:22Z",
            "published": "2023-12-05T07:23:22Z",
            "summary": "Remote sensing segmentation has a wide range of applications in environmental\nprotection, and urban change detection, etc. Despite the success of deep\nlearning-based remote sensing segmentation methods (e.g., CNN and Transformer),\nthey are not flexible enough to model irregular objects. In addition, existing\ngraph contrastive learning methods usually adopt the way of maximizing mutual\ninformation to keep the node representations consistent between different graph\nviews, which may cause the model to learn task-independent redundant\ninformation. To tackle the above problems, this paper treats images as graph\nstructures and introduces a simple contrastive vision GNN (SC-ViG) architecture\nfor remote sensing segmentation. Specifically, we construct a node-masked and\nedge-masked graph view to obtain an optimal graph structure representation,\nwhich can adaptively learn whether to mask nodes and edges. Furthermore, this\npaper innovatively introduces information bottleneck theory into graph\ncontrastive learning to maximize task-related information while minimizing\ntask-independent redundant information. Finally, we replace the convolutional\nmodule in UNet with the SC-ViG module to complete the segmentation and\nclassification tasks of remote sensing images. Extensive experiments on\npublicly available real datasets demonstrate that our method outperforms\nstate-of-the-art remote sensing image segmentation methods.",
            "author": [
                "Yuntao Shou",
                "Wei Ai",
                "Tao Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02545v1",
                "http://arxiv.org/pdf/2312.02545v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02522v1",
            "title": "MASP: Scalable GNN-based Planning for Multi-Agent Navigation",
            "updated": "2023-12-05T06:05:04Z",
            "published": "2023-12-05T06:05:04Z",
            "summary": "We investigate the problem of decentralized multi-agent navigation tasks,\nwhere multiple agents need to reach initially unassigned targets in a limited\ntime. Classical planning-based methods suffer from expensive computation\noverhead at each step and offer limited expressiveness for complex cooperation\nstrategies. In contrast, reinforcement learning (RL) has recently become a\npopular paradigm for addressing this issue. However, RL struggles with low data\nefficiency and cooperation when directly exploring (nearly) optimal policies in\nthe large search space, especially with an increased agent number (e.g., 10+\nagents) or in complex environments (e.g., 3D simulators). In this paper, we\npropose Multi-Agent Scalable GNN-based P lanner (MASP), a goal-conditioned\nhierarchical planner for navigation tasks with a substantial number of agents.\nMASP adopts a hierarchical framework to divide a large search space into\nmultiple smaller spaces, thereby reducing the space complexity and accelerating\ntraining convergence. We also leverage graph neural networks (GNN) to model the\ninteraction between agents and goals, improving goal achievement. Besides, to\nenhance generalization capabilities in scenarios with unseen team sizes, we\ndivide agents into multiple groups, each with a previously trained number of\nagents. The results demonstrate that MASP outperforms classical planning-based\ncompetitors and RL baselines, achieving a nearly 100% success rate with minimal\ntraining data in both multi-agent particle environments (MPE) with 50 agents\nand a quadrotor 3-dimensional environment (OmniDrones) with 20 agents.\nFurthermore, the learned policy showcases zero-shot generalization across\nunseen team sizes.",
            "author": [
                "Xinyi Yang",
                "Xinting Yang",
                "Chao Yu",
                "Jiayu Chen",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02522v1",
                "http://arxiv.org/pdf/2312.02522v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02510v1",
            "title": "Estimation of articulated angle in six-wheeled dump trucks using\n  multiple GNSS receivers for autonomous driving",
            "updated": "2023-12-05T05:28:43Z",
            "published": "2023-12-05T05:28:43Z",
            "summary": "Due to the declining birthrate and aging population, the shortage of labor in\nthe construction industry has become a serious problem, and increasing\nattention has been paid to automation of construction equipment. We focus on\nthe automatic operation of articulated six-wheel dump trucks at construction\nsites. For the automatic operation of the dump trucks, it is important to\nestimate the position and the articulated angle of the dump trucks with high\naccuracy. In this study, we propose a method for estimating the state of a dump\ntruck by using four global navigation satellite systems (GNSSs) installed on an\narticulated dump truck and a graph optimization method that utilizes the\nredundancy of multiple GNSSs. By adding real-time kinematic (RTK)-GNSS\nconstraints and geometric constraints between the four antennas, the proposed\nmethod can robustly estimate the position and articulation angle even in\nenvironments where GNSS satellites are partially blocked. As a result of\nevaluating the accuracy of the proposed method through field tests, it was\nconfirmed that the articulated angle could be estimated with an accuracy of\n0.1$^\\circ$ in an open-sky environment and 0.7$^\\circ$ in a mountainous area\nsimulating an elevation angle of 45$^\\circ$ where GNSS satellites are blocked.",
            "author": [
                "Taro Suzuki",
                "Kazunori Ohno",
                "Syotaro Kojima",
                "Naoto Miyamoto",
                "Takahiro Suzuki",
                "Tomohiro Komatsu",
                "Yukinori Shibata",
                "Kimitaka Asano",
                "Keiji Nagatani"
            ],
            "link": [
                "http://dx.doi.org/10.1080/01691864.2021.1974942",
                "http://arxiv.org/abs/2312.02510v1",
                "http://arxiv.org/pdf/2312.02510v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02498v1",
            "title": "Provable Reinforcement Learning for Networked Control Systems with\n  Stochastic Packet Disordering",
            "updated": "2023-12-05T04:59:28Z",
            "published": "2023-12-05T04:59:28Z",
            "summary": "This paper formulates a stochastic optimal control problem for linear\nnetworked control systems featuring stochastic packet disordering with a unique\nstabilizing solution certified. The problem is solved by proposing\nreinforcement learning algorithms. A measurement method is first presented to\ndeal with PD and calculate the newest control input. The NCSs with stochastic\nPD are modeled as stochastic NCSs. Then, given a cost function, a modified\nalgebraic Riccati equation is derived within the formulation. We propose\noffline policy iteration and value iteration algorithms to solve the MARE\nassociated with provable convergence. These two algorithms require knowledge of\nNCS dynamics and PD probabilities. To release that, we further design online\nmodel-free off-policy and Q-learning algorithms with an online estimation\nmethod for PD probability. Both model-free algorithms solve the optimal control\nproblem using real-time system states, control inputs, and PD probability\nestimates. Simulation results verify the proposed formulation and algorithms at\nlast.",
            "author": [
                "Wenqian Xue",
                "Yi Jiang",
                "Frank L. Lewis",
                "Bosen Lian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02498v1",
                "http://arxiv.org/pdf/2312.02498v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02496v1",
            "title": "MKA: A Scalable Medical Knowledge Assisted Mechanism for Generative\n  Models on Medical Conversation Tasks",
            "updated": "2023-12-05T04:55:54Z",
            "published": "2023-12-05T04:55:54Z",
            "summary": "Using natural language processing (NLP) technologies to develop medical\nchatbots makes the diagnosis of the patient more convenient and efficient,\nwhich is a typical application in healthcare AI. Because of its importance,\nlots of research have been come out. Recently, the neural generative models\nhave shown their impressive ability as the core of chatbot, while it cannot\nscale well when directly applied to medical conversation due to the lack of\nmedical-specific knowledge. To address the limitation, a scalable Medical\nKnowledge Assisted mechanism, MKA, is proposed in this paper. The mechanism\naims to assist general neural generative models to achieve better performance\non the medical conversation task. The medical-specific knowledge graph is\ndesigned within the mechanism, which contains 6 types of medical-related\ninformation, including department, drug, check, symptom, disease, food.\nBesides, the specific token concatenation policy is defined to effectively\ninject medical information into the input data. Evaluation of our method is\ncarried out on two typical medical datasets, MedDG and MedDialog-CN. The\nevaluation results demonstrate that models combined with our mechanism\noutperform original methods in multiple automatic evaluation metrics. Besides,\nMKA-Bert-GPT achieves state-of-the-art performance. The open-sourced codes are\npublic:\nhttps://github.com/LIANGKE23/Knowledge_Assisted_Medical_Dialogue_Generation_Mechanism",
            "author": [
                "Ke Liang",
                "Sifan Wu",
                "Jiayi Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02496v1",
                "http://arxiv.org/pdf/2312.02496v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02473v1",
            "title": "NeutronStream: A Dynamic GNN Training Framework with Sliding Window for\n  Graph Streams",
            "updated": "2023-12-05T03:58:05Z",
            "published": "2023-12-05T03:58:05Z",
            "summary": "Existing Graph Neural Network (GNN) training frameworks have been designed to\nhelp developers easily create performant GNN implementations. However, most\nexisting GNN frameworks assume that the input graphs are static, but ignore\nthat most real-world graphs are constantly evolving. Though many dynamic GNN\nmodels have emerged to learn from evolving graphs, the training process of\nthese dynamic GNNs is dramatically different from traditional GNNs in that it\ncaptures both the spatial and temporal dependencies of graph updates. This\nposes new challenges for designing dynamic GNN training frameworks. First, the\ntraditional batched training method fails to capture real-time structural\nevolution information. Second, the time-dependent nature makes parallel\ntraining hard to design. Third, it lacks system supports for users to\nefficiently implement dynamic GNNs. In this paper, we present NeutronStream, a\nframework for training dynamic GNN models. NeutronStream abstracts the input\ndynamic graph into a chronologically updated stream of events and processes the\nstream with an optimized sliding window to incrementally capture the\nspatial-temporal dependencies of events. Furthermore, NeutronStream provides a\nparallel execution engine to tackle the sequential event processing challenge\nto achieve high performance. NeutronStream also integrates a built-in graph\nstorage structure that supports dynamic updates and provides a set of\neasy-to-use APIs that allow users to express their dynamic GNNs. Our\nexperimental results demonstrate that, compared to state-of-the-art dynamic GNN\nimplementations, NeutronStream achieves speedups ranging from 1.48X to 5.87X\nand an average accuracy improvement of 3.97%.",
            "author": [
                "Chaoyi Chen",
                "Dechao Gao",
                "Yanfeng Zhang",
                "Qiange Wang",
                "Zhenbo Fu",
                "Xuecang Zhang",
                "Junhua Zhu",
                "Yu Gu",
                "Ge Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02473v1",
                "http://arxiv.org/pdf/2312.02473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02471v1",
            "title": "Congestion-aware Distributed Task Offloading in Wireless Multi-hop\n  Networks Using Graph Neural Networks",
            "updated": "2023-12-05T03:46:30Z",
            "published": "2023-12-05T03:46:30Z",
            "summary": "Computational offloading has become an enabling component for edge\nintelligence in mobile and smart devices. Existing offloading schemes mainly\nfocus on mobile devices and servers, while ignoring the potential network\ncongestion caused by tasks from multiple mobile devices, especially in wireless\nmulti-hop networks. To fill this gap, we propose a low-overhead,\ncongestion-aware distributed task offloading scheme by augmenting a distributed\ngreedy framework with graph-based machine learning. In simulated wireless\nmulti-hop networks with 20-110 nodes and a resource allocation scheme based on\nshortest path routing and contention-based link scheduling, our approach is\ndemonstrated to be effective in reducing congestion or unstable queues under\nthe context-agnostic baseline, while improving the execution latency over local\ncomputing.",
            "author": [
                "Zhongyuan Zhao",
                "Jake Perazzone",
                "Gunjan Verma",
                "Santiago Segarra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02471v1",
                "http://arxiv.org/pdf/2312.02471v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "eess.SP",
                "05C90",
                "C.2.1; C.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02470v1",
            "title": "Generator Born from Classifier",
            "updated": "2023-12-05T03:41:17Z",
            "published": "2023-12-05T03:41:17Z",
            "summary": "In this paper, we make a bold attempt toward an ambitious task: given a\npre-trained classifier, we aim to reconstruct an image generator, without\nrelying on any data samples. From a black-box perspective, this challenge seems\nintractable, since it inevitably involves identifying the inverse function for\na classifier, which is, by nature, an information extraction process. As such,\nwe resort to leveraging the knowledge encapsulated within the parameters of the\nneural network. Grounded on the theory of Maximum-Margin Bias of gradient\ndescent, we propose a novel learning paradigm, in which the generator is\ntrained to ensure that the convergence conditions of the network parameters are\nsatisfied over the generated distribution of the samples. Empirical validation\nfrom various image generation tasks substantiates the efficacy of our strategy.",
            "author": [
                "Runpeng Yu",
                "Xinchao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02470v1",
                "http://arxiv.org/pdf/2312.02470v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02448v1",
            "title": "Time-Relative RTK-GNSS: GNSS Loop Closure in Pose Graph Optimization",
            "updated": "2023-12-05T03:00:50Z",
            "published": "2023-12-05T03:00:50Z",
            "summary": "A pose-graph-based optimization technique is widely used to estimate robot\nposes using various sensor measurements from devices such as laser scanners and\ncameras. The global navigation satellite system (GNSS) has recently been used\nto estimate the absolute 3D position of outdoor mobile robots. However, since\nthe accuracy of GNSS single-point positioning is only a few meters, the GNSS is\nnot used for the loop closure of a pose graph. The main purpose of this study\nis to generate a loop closure of a pose graph using a time-relative real-time\nkinematic GNSS (TR-RTK-GNSS) technique. The proposed TR-RTK-GNSS technique uses\ntime-differential carrier phase positioning, which is based on\ncarrier-phase-based differential GNSS with a single GNSS receiver. Unlike a\nconventional RTK-GNSS, we can directly compute the robot's relative position\nusing only a stand-alone GNSS receiver. The initial pose graph is generated\nfrom the accumulated velocity computed from GNSS Doppler measurements. To\nreduce the accumulated error of velocity, we use the TR-RTK-GNSS technique for\nthe loop closure in the graph-based optimization framework. The kinematic\npositioning tests were performed using an unmanned aerial vehicle to confirm\nthe effectiveness of the proposed technique. From the tests, we can estimate\nthe vehicle's trajectory with approximately 3 cm accuracy using only a\nstand-alone GNSS receiver.",
            "author": [
                "Taro Suzuki"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2020.3003861",
                "http://arxiv.org/abs/2312.02448v1",
                "http://arxiv.org/pdf/2312.02448v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02445v1",
            "title": "LLaRA: Aligning Large Language Models with Sequential Recommenders",
            "updated": "2023-12-05T02:53:46Z",
            "published": "2023-12-05T02:53:46Z",
            "summary": "Sequential recommendation aims to predict the subsequent items matching user\npreference based on her/his historical interactions. With the development of\nLarge Language Models (LLMs), there is growing interest in exploring the\npotential of LLMs for sequential recommendation by framing it as a language\nmodeling task. Prior works represent items in the textual prompts using either\nID indexing or text indexing and feed the prompts into LLMs, but falling short\nof either encapsulating comprehensive world knowledge or exhibiting sufficient\nsequential understanding. To harness the complementary strengths of traditional\nrecommenders (which encode user behavioral knowledge) and LLMs (which possess\nworld knowledge about items), we propose LLaRA -- a Large Language and\nRecommendation Assistant framework. Specifically, LLaRA represents items in\nLLM's input prompts using a novel hybrid approach that integrates ID-based item\nembeddings from traditional recommenders with textual item features. Viewing\nthe ``sequential behavior of the user'' as a new modality in recommendation, we\nemploy an adapter to bridge the modality gap between ID embeddings of the\ntraditional recommenders and the input space of LLMs. Furthermore, instead of\ndirectly exposing the hybrid prompt to LLMs, we apply a curriculum learning\napproach to gradually ramp up training complexity. We first warm up the LLM\nwith text-only prompting, which aligns more naturally with the LLM's language\nmodeling capabilities. Thereafter, we progressively transition to hybrid\nprompting, training the adapter to incorporate behavioral knowledge from the\ntraditional sequential recommender into the LLM. Extensive experiments\ndemonstrate the efficacy of LLaRA framework. Our code and data are available at\nhttps://github.com/ljy0ustc/LLaRA .",
            "author": [
                "Jiayi Liao",
                "Sihang Li",
                "Zhengyi Yang",
                "Jiancan Wu",
                "Yancheng Yuan",
                "Xiang Wang",
                "Xiangnan He"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02445v1",
                "http://arxiv.org/pdf/2312.02445v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02441v1",
            "title": "MedDM:LLM-executable clinical guidance tree for clinical decision-making",
            "updated": "2023-12-05T02:44:07Z",
            "published": "2023-12-05T02:44:07Z",
            "summary": "It is becoming increasingly emphasis on the importance of LLM participating\nin clinical diagnosis decision-making. However, the low specialization refers\nto that current medical LLMs can not provide specific medical advice, which are\nmore like a medical Q\\&A. And there is no suitable clinical guidance tree data\nset that can be used directly with LLM. To address this issue, we first propose\nLLM-executavle clinical guidance tree(CGT), which can be directly used by large\nlanguage models, and construct medical diagnostic decision-making dataset\n(MedDM), from flowcharts in clinical practice guidelines. We propose an\napproach to screen flowcharts from medical literature, followed by their\nidentification and conversion into standardized diagnostic decision trees.\nConstructed a knowledge base with 1202 decision trees, which came from 5000\nmedical literature and covered 12 hospital departments, including internal\nmedicine, surgery, psychiatry, and over 500 diseases.Moreover, we propose a\nmethod for reasoning on LLM-executable CGT and a Patient-LLM multi-turn\ndialogue framework.",
            "author": [
                "Binbin Li",
                "Tianxin Meng",
                "Xiaoming Shi",
                "Jie Zhai",
                "Tong Ruan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02441v1",
                "http://arxiv.org/pdf/2312.02441v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02439v2",
            "title": "Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language\n  Models with Creative Humor Generation",
            "updated": "2023-12-06T03:20:29Z",
            "published": "2023-12-05T02:41:57Z",
            "summary": "Chain-of-Thought (CoT) guides large language models (LLMs) to reason\nstep-by-step, and can motivate their logical reasoning ability. While effective\nfor logical tasks, CoT is not conducive to creative problem-solving which often\nrequires out-of-box thoughts and is crucial for innovation advancements. In\nthis paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a\nnon-sequential, creative paradigm involving strong associations and knowledge\nleaps. To this end, we study LLMs on the popular Oogiri game which needs\nparticipants to have good creativity and strong associative thinking for\nresponding unexpectedly and humorously to the given image, text, or both, and\nthus is suitable for LoT study. Then to investigate LLMs' LoT ability in the\nOogiri game, we first build a multimodal and multilingual Oogiri-GO dataset\nwhich contains over 130,000 samples from the Oogiri game, and observe the\ninsufficient LoT ability or failures of most existing LLMs on the Oogiri game.\nAccordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve\nLLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into\nLoT-oriented instruction tuning data to train pretrained LLM for achieving\ncertain LoT humor generation and discrimination abilities. Then CLoT designs an\nexplorative self-refinement that encourages the LLM to generate more creative\nLoT data via exploring parallels between seemingly unrelated concepts and\nselects high-quality data to train itself for self-refinement. CLoT not only\nexcels in humor generation in the Oogiri game but also boosts creative\nabilities in various tasks like cloud guessing game and divergent association\ntask. These findings advance our understanding and offer a pathway to improve\nLLMs' creative capacities for innovative applications across domains. The\ndataset, code, and models will be released online.\nhttps://zhongshsh.github.io/CLoT/.",
            "author": [
                "Shanshan Zhong",
                "Zhongzhan Huang",
                "Shanghua Gao",
                "Wushao Wen",
                "Liang Lin",
                "Marinka Zitnik",
                "Pan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02439v2",
                "http://arxiv.org/pdf/2312.02439v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02433v1",
            "title": "Lenna: Language Enhanced Reasoning Detection Assistant",
            "updated": "2023-12-05T02:19:35Z",
            "published": "2023-12-05T02:19:35Z",
            "summary": "With the fast-paced development of multimodal large language models (MLLMs),\nwe can now converse with AI systems in natural languages to understand images.\nHowever, the reasoning power and world knowledge embedded in the large language\nmodels have been much less investigated and exploited for image perception\ntasks. In this paper, we propose Lenna, a language-enhanced reasoning detection\nassistant, which utilizes the robust multimodal feature representation of\nMLLMs, while preserving location information for detection. This is achieved by\nincorporating an additional <DET> token in the MLLM vocabulary that is free of\nexplicit semantic context but serves as a prompt for the detector to identify\nthe corresponding position. To evaluate the reasoning capability of Lenna, we\nconstruct a ReasonDet dataset to measure its performance on reasoning-based\ndetection. Remarkably, Lenna demonstrates outstanding performance on ReasonDet\nand comes with significantly low training costs. It also incurs minimal\ntransferring overhead when extended to other tasks. Our code and model will be\navailable at https://git.io/Lenna.",
            "author": [
                "Fei Wei",
                "Xinyu Zhang",
                "Ailing Zhang",
                "Bo Zhang",
                "Xiangxiang Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02433v1",
                "http://arxiv.org/pdf/2312.02433v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02424v1",
            "title": "GNSS Odometry: Precise Trajectory Estimation Based on Carrier Phase\n  Cycle Slip Estimation",
            "updated": "2023-12-05T01:58:32Z",
            "published": "2023-12-05T01:58:32Z",
            "summary": "This paper proposes a highly accurate trajectory estimation method for\noutdoor mobile robots using global navigation satellite system (GNSS) time\ndifferences of carrier phase (TDCP) measurements. By using GNSS TDCP, the\nrelative 3D position can be estimated with millimeter precision. However, when\na phenomenon called cycle slip occurs, wherein the carrier phase measurement\njumps and becomes discontinuous, it is impossible to accurately estimate the\nrelative position using TDCP. Although previous studies have eliminated the\neffect of cycle slip using a robust optimization technique, it was difficult to\ncompletely eliminate the effect of outliers. In this paper, we propose a method\nto detect GNSS carrier phase cycle slip, estimate the amount of cycle slip, and\nmodify the observed TDCP to calculate the relative position using the factor\ngraph optimization framework. The estimated relative position acts as a loop\nclosure in graph optimization and contributes to the reduction in the\nintegration error of the relative position. Experiments with an unmanned aerial\nvehicle showed that by modifying the cycle slip using the proposed method, the\nvehicle trajectory could be estimated with an accuracy of 5 to 30 cm using only\na single GNSS receiver, without using any other external data or sensors.",
            "author": [
                "Taro Suzuki"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LRA.2022.3182795",
                "http://arxiv.org/abs/2312.02424v1",
                "http://arxiv.org/pdf/2312.02424v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03016v1",
            "title": "Protein Language Model-Powered 3D Ligand Binding Site Prediction from\n  Protein Sequence",
            "updated": "2023-12-05T01:47:38Z",
            "published": "2023-12-05T01:47:38Z",
            "summary": "Prediction of ligand binding sites of proteins is a fundamental and important\ntask for understanding the function of proteins and screening potential drugs.\nMost existing methods require experimentally determined protein holo-structures\nas input. However, such structures can be unavailable on novel or less-studied\nproteins. To tackle this limitation, we propose LaMPSite, which only takes\nprotein sequences and ligand molecular graphs as input for ligand binding site\npredictions. The protein sequences are used to retrieve residue-level\nembeddings and contact maps from the pre-trained ESM-2 protein language model.\nThe ligand molecular graphs are fed into a graph neural network to compute\natom-level embeddings. Then we compute and update the protein-ligand\ninteraction embedding based on the protein residue-level embeddings and ligand\natom-level embeddings, and the geometric constraints in the inferred protein\ncontact map and ligand distance map. A final pooling on protein-ligand\ninteraction embedding would indicate which residues belong to the binding\nsites. Without any 3D coordinate information of proteins, our proposed model\nachieves competitive performance compared to baseline methods that require 3D\nprotein structures when predicting binding sites. Given that less than 50% of\nproteins have reliable structure information in the current stage, LaMPSite\nwill provide new opportunities for drug discovery.",
            "author": [
                "Shuo Zhang",
                "Lei Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03016v1",
                "http://arxiv.org/pdf/2312.03016v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02419v1",
            "title": "Human Demonstrations are Generalizable Knowledge for Robots",
            "updated": "2023-12-05T01:35:39Z",
            "published": "2023-12-05T01:35:39Z",
            "summary": "Learning from human demonstrations is an emerging trend for designing\nintelligent robotic systems. However, previous methods typically regard videos\nas instructions, simply dividing them into action sequences for robotic\nrepetition, which poses obstacles to generalization to diverse tasks or object\ninstances. In this paper, we propose a different perspective, considering human\ndemonstration videos not as mere instructions, but as a source of knowledge for\nrobots. Motivated by this perspective and the remarkable comprehension and\ngeneralization capabilities exhibited by large language models (LLMs), we\npropose DigKnow, a method that DIstills Generalizable KNOWledge with a\nhierarchical structure. Specifically, DigKnow begins by converting human\ndemonstration video frames into observation knowledge. This knowledge is then\nsubjected to analysis to extract human action knowledge and further distilled\ninto pattern knowledge compassing task and object instances, resulting in the\nacquisition of generalizable knowledge with a hierarchical structure. In\nsettings with different tasks or object instances, DigKnow retrieves relevant\nknowledge for the current task and object instances. Subsequently, the\nLLM-based planner conducts planning based on the retrieved knowledge, and the\npolicy executes actions in line with the plan to achieve the designated task.\nUtilizing the retrieved knowledge, we validate and rectify planning and\nexecution outcomes, resulting in a substantial enhancement of the success rate.\nExperimental results across a range of tasks and scenes demonstrate the\neffectiveness of this approach in facilitating real-world robots to accomplish\ntasks with the knowledge derived from human demonstrations.",
            "author": [
                "Guangyan Chen",
                "Te Cui",
                "Tianxing Zhou",
                "Zicai Peng",
                "Mengxiao Hu",
                "Meiling Wang",
                "Yi Yang",
                "Yufeng Yue"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02419v1",
                "http://arxiv.org/pdf/2312.02419v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02418v1",
            "title": "Decoding Data Quality via Synthetic Corruptions: Embedding-guided\n  Pruning of Code Data",
            "updated": "2023-12-05T01:19:30Z",
            "published": "2023-12-05T01:19:30Z",
            "summary": "Code datasets, often collected from diverse and uncontrolled sources such as\nGitHub, potentially suffer from quality issues, thereby affecting the\nperformance and training efficiency of Large Language Models (LLMs) optimized\nfor code generation. Previous studies demonstrated the benefit of using\nembedding spaces for data pruning, but they mainly focused on duplicate removal\nor increasing variety, and in other modalities, such as images. Our work\nfocuses on using embeddings to identify and remove \"low-quality\" code data.\nFirst, we explore features of \"low-quality\" code in embedding space, through\nthe use of synthetic corruptions. Armed with this knowledge, we devise novel\npruning metrics that operate in embedding space to identify and remove\nlow-quality entries in the Stack dataset. We demonstrate the benefits of this\nsynthetic corruption informed pruning (SCIP) approach on the well-established\nHumanEval and MBPP benchmarks, outperforming existing embedding-based methods.\nImportantly, we achieve up to a 3% performance improvement over no pruning,\nthereby showing the promise of insights from synthetic corruptions for data\npruning.",
            "author": [
                "Yu Yang",
                "Aaditya K. Singh",
                "Mostafa Elhoushi",
                "Anas Mahmoud",
                "Kushal Tirumala",
                "Fabian Gloeckle",
                "Baptiste Rozi\u00e8re",
                "Carole-Jean Wu",
                "Ari S. Morcos",
                "Newsha Ardalani"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02418v1",
                "http://arxiv.org/pdf/2312.02418v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02416v1",
            "title": "Towards Fast and Stable Federated Learning: Confronting Heterogeneity\n  via Knowledge Anchor",
            "updated": "2023-12-05T01:12:56Z",
            "published": "2023-12-05T01:12:56Z",
            "summary": "Federated learning encounters a critical challenge of data heterogeneity,\nadversely affecting the performance and convergence of the federated model.\nVarious approaches have been proposed to address this issue, yet their\neffectiveness is still limited. Recent studies have revealed that the federated\nmodel suffers severe forgetting in local training, leading to global forgetting\nand performance degradation. Although the analysis provides valuable insights,\na comprehensive understanding of the vulnerable classes and their impact\nfactors is yet to be established. In this paper, we aim to bridge this gap by\nsystematically analyzing the forgetting degree of each class during local\ntraining across different communication rounds. Our observations are: (1) Both\nmissing and non-dominant classes suffer similar severe forgetting during local\ntraining, while dominant classes show improvement in performance. (2) When\ndynamically reducing the sample size of a dominant class, catastrophic\nforgetting occurs abruptly when the proportion of its samples is below a\ncertain threshold, indicating that the local model struggles to leverage a few\nsamples of a specific class effectively to prevent forgetting. Motivated by\nthese findings, we propose a novel and straightforward algorithm called\nFederated Knowledge Anchor (FedKA). Assuming that all clients have a single\nshared sample for each class, the knowledge anchor is constructed before each\nlocal training stage by extracting shared samples for missing classes and\nrandomly selecting one sample per class for non-dominant classes. The knowledge\nanchor is then utilized to correct the gradient of each mini-batch towards the\ndirection of preserving the knowledge of the missing and non-dominant classes.\nExtensive experimental results demonstrate that our proposed FedKA achieves\nfast and stable convergence, significantly improving accuracy on popular\nbenchmarks.",
            "author": [
                "Jinqian Chen",
                "Jihua Zhu",
                "Qinghai Zheng"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3612597",
                "http://arxiv.org/abs/2312.02416v1",
                "http://arxiv.org/pdf/2312.02416v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "68T99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03014v1",
            "title": "Foundation Models for Weather and Climate Data Understanding: A\n  Comprehensive Survey",
            "updated": "2023-12-05T01:10:54Z",
            "published": "2023-12-05T01:10:54Z",
            "summary": "As artificial intelligence (AI) continues to rapidly evolve, the realm of\nEarth and atmospheric sciences is increasingly adopting data-driven models,\npowered by progressive developments in deep learning (DL). Specifically, DL\ntechniques are extensively utilized to decode the chaotic and nonlinear aspects\nof Earth systems, and to address climate challenges via understanding weather\nand climate data. Cutting-edge performance on specific tasks within narrower\nspatio-temporal scales has been achieved recently through DL. The rise of large\nmodels, specifically large language models (LLMs), has enabled fine-tuning\nprocesses that yield remarkable outcomes across various downstream tasks,\nthereby propelling the advancement of general AI. However, we are still\nnavigating the initial stages of crafting general AI for weather and climate.\nIn this survey, we offer an exhaustive, timely overview of state-of-the-art AI\nmethodologies specifically engineered for weather and climate data, with a\nspecial focus on time series and text data. Our primary coverage encompasses\nfour critical aspects: types of weather and climate data, principal model\narchitectures, model scopes and applications, and datasets for weather and\nclimate. Furthermore, in relation to the creation and application of foundation\nmodels for weather and climate data understanding, we delve into the field's\nprevailing challenges, offer crucial insights, and propose detailed avenues for\nfuture research. This comprehensive approach equips practitioners with the\nrequisite knowledge to make substantial progress in this domain. Our survey\nencapsulates the most recent breakthroughs in research on large, data-driven\nmodels for weather and climate data understanding, emphasizing robust\nfoundations, current advancements, practical applications, crucial resources,\nand prospective research opportunities.",
            "author": [
                "Shengchao Chen",
                "Guodong Long",
                "Jing Jiang",
                "Dikai Liu",
                "Chengqi Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03014v1",
                "http://arxiv.org/pdf/2312.03014v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02398v1",
            "title": "Heat-Bath and Metropolis Dynamics in Ising-like Models on Directed\n  Regular Random Graphs",
            "updated": "2023-12-04T23:43:38Z",
            "published": "2023-12-04T23:43:38Z",
            "summary": "Using a single-site mean-field approximation (MFA) and Monte Carlo\nsimulations, we examine Ising-like models on directed regular random graphs.\nThe models are directed-network implementations of the Ising model, Ising model\nwith absorbing states, and majority voter models. When these nonequilibrium\nmodels are driven by the heat-bath dynamics, their stationary characteristics,\nsuch as magnetization, are correctly reproduced by MFA as confirmed by Monte\nCarlo simulations. It turns out that MFA reproduces the same result as the\ngenerating functional analysis that is expected to provide the exact\ndescription of such models. We argue that on directed regular random graphs,\nthe neighbors of a given vertex are typically uncorrelated, and that is why MFA\nfor models with heat-bath dynamics provides their exact description. For models\nwith Metropolis dynamics, certain additional correlations become relevant, and\nMFA, which neglects these correlations, is less accurate. Models with heat-bath\ndynamics undergo continuous phase transition, and at the critical point, the\npower-law time decay of the order parameter exhibits the behavior of the Ising\nmean-field universality class. Analogous phase transitions for models with\nMetropolis dynamics are discontinuous.",
            "author": [
                "Adam Lipowski",
                "Antonio L. Ferreira",
                "Dorota Lipowska"
            ],
            "link": [
                "http://dx.doi.org/10.3390/e25121615",
                "http://arxiv.org/abs/2312.02398v1",
                "http://arxiv.org/pdf/2312.02398v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02387v1",
            "title": "Dissecting Medical Referral Mechanisms in Health Services: Role of\n  Physician Professional Networks",
            "updated": "2023-12-04T23:03:09Z",
            "published": "2023-12-04T23:03:09Z",
            "summary": "Medical referrals between primary care physicians (PC) and specialist care\n(SC) physicians profoundly impact patient care regarding quality, satisfaction,\nand cost. This paper investigates the influence of professional networks among\nmedical doctors on referring patients from PC to SC. Using five-year\nconsultation data from a Portuguese private health provider, we conducted\nexploratory data analysis and constructed both professional and referral\nnetworks among physicians. We then apply Graph Neural Network (GNN) models to\nlearn latent representations of the referral network. Our analysis supports the\nhypothesis that doctors' professional social connections can predict medical\nreferrals, potentially enhancing collaboration within organizations and\nimproving healthcare services. This research contributes to dissecting the\nunderlying mechanisms in primary-specialty referrals, thereby providing\nvaluable insights for enhancing patient care and effective healthcare\nmanagement.",
            "author": [
                "Regina de Brito Duarte",
                "Qiwei Han",
                "Claudia Soares"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02387v1",
                "http://arxiv.org/pdf/2312.02387v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02377v1",
            "title": "Clifford Manipulations of Stabilizer States: A graphical rule book for\n  Clifford unitaries and measurements on cluster states, and application to\n  photonic quantum computing",
            "updated": "2023-12-04T22:40:24Z",
            "published": "2023-12-04T22:40:24Z",
            "summary": "Stabilizer states along with Clifford manipulations (unitary transformations\nand measurements) thereof -- despite being efficiently simulable on a classical\ncomputer -- are an important tool in quantum information processing, with\napplications to quantum computing, error correction and networking. Cluster\nstates, defined on a graph, are a special class of stabilizer states that are\ncentral to measurement based quantum computing, all-photonic quantum repeaters,\ndistributed quantum computing, and entanglement distribution in a network. All\ncluster states are local-Clifford equivalent to a stabilizer state. In this\npaper, we review the stabilizer framework, and extend it, by: incorporating\ngeneral stabilizer measurements such as multi-qubit fusions, and providing an\nexplicit procedure -- using Karnaugh maps from Boolean algebra -- for\nconverting arbitrary stabilizer gates into tableau operations of the CHP\nformalism for efficient stabilizer manipulations. Using these tools, we develop\na graphical rule-book and a MATLAB simulator with a graphical user interface\nfor arbitrary stabilizer manipulations of cluster states, a user of which,\ne.g., for research in quantum networks, will not require any background in\nquantum information or the stabilizer framework. We extend our graphical\nrule-book to include dual-rail photonic-qubit cluster state manipulations with\nprobabilistically-heralded linear-optical circuits for various rotated Bell\nmeasurements, i.e., fusions (including new `Type-I' fusions we propose, where\nonly one of the two fused qubits is destructively measured), by incorporating\ngraphical rules for their success and failure modes. Finally, we show how\nstabilizer descriptions of multi-qubit fusions can be mapped to linear optical\ncircuits.",
            "author": [
                "Ashlesha Patil",
                "Saikat Guha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02377v1",
                "http://arxiv.org/pdf/2312.02377v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02372v1",
            "title": "On the Trade-Off between Stability and Representational Capacity in\n  Graph Neural Networks",
            "updated": "2023-12-04T22:07:17Z",
            "published": "2023-12-04T22:07:17Z",
            "summary": "Analyzing the stability of graph neural networks (GNNs) under topological\nperturbations is key to understanding their transferability and the role of\neach architecture component. However, stability has been investigated only for\nparticular architectures, questioning whether it holds for a broader spectrum\nof GNNs or only for a few instances. To answer this question, we study the\nstability of EdgeNet: a general GNN framework that unifies more than twenty\nsolutions including the convolutional and attention-based classes, as well as\ngraph isomorphism networks and hybrid architectures. We prove that all GNNs\nwithin the EdgeNet framework are stable to topological perturbations. By\nstudying the effect of different EdgeNet categories on the stability, we show\nthat GNNs with fewer degrees of freedom in their parameter space, linked to a\nlower representational capacity, are more stable. The key factor yielding this\ntrade-off is the eigenvector misalignment between the EdgeNet parameter\nmatrices and the graph shift operator. For example, graph convolutional neural\nnetworks that assign a single scalar per signal shift (hence, with a perfect\nalignment) are more stable than the more involved node or edge-varying\ncounterparts. Extensive numerical results corroborate our theoretical findings\nand highlight the role of different architecture components in the trade-off.",
            "author": [
                "Zhan Gao",
                "Amanda Prorok",
                "Elvin Isufi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02372v1",
                "http://arxiv.org/pdf/2312.02372v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02357v1",
            "title": "Classification of Minimal Separating Sets of Low Genus Surfaces",
            "updated": "2023-12-04T21:36:28Z",
            "published": "2023-12-04T21:36:28Z",
            "summary": "A minimal separating set in a connected topological space $X$ is a subset $L\n\\subset X$ with the property that $X \\setminus L$ is disconnected, but if\n$L^{\\prime}$ is a proper subset of $L$, then $X \\setminus L^{\\prime}$ is\nconnected. Such sets show up in a variety of contexts. For example, in a wide\nclass of metric spaces, if we choose distinct points p and q, then the set of\npoints x satisfying d(x, p) = d(x, q) is a minimal separating set. In this\npaper we classify which topological graphs can be realized as minimal\nseparating sets in surfaces of low genus. In general the question of whether a\ngraph can be embedded at all in a surface is a difficult one, so our work is\npartly computational. We classify graphs embeddings which are minimal\nseparating in a given genus and write a computer program to find all such\nembeddings and their underlying graphs.",
            "author": [
                "Christopher N. Aagaard",
                "J. J. P. Veerman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02357v1",
                "http://arxiv.org/pdf/2312.02357v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02353v1",
            "title": "Efficient 2D Graph SLAM for Sparse Sensing",
            "updated": "2023-12-04T21:32:33Z",
            "published": "2023-12-04T21:32:33Z",
            "summary": "Simultaneous localization and mapping (SLAM) plays a vital role in mapping\nunknown spaces and aiding autonomous navigation. Virtually all state-of-the-art\nsolutions today for 2D SLAM are designed for dense and accurate sensors such as\nlaser range-finders (LiDARs). However, these sensors are not suitable for\nresource-limited nano robots, which become increasingly capable and ubiquitous\nnowadays, and these robots tend to mount economical and low-power sensors that\ncan only provide sparse and noisy measurements. This introduces a challenging\nproblem called SLAM with sparse sensing. This work addresses the problem by\nadopting the form of the state-of-the-art graph-based SLAM pipeline with a\nnovel frontend and an improvement for loop closing in the backend, both of\nwhich are designed to work with sparse and uncertain range data. Experiments\nshow that the maps constructed by our algorithm have superior quality compared\nto prior works on sparse sensing. Furthermore, our method is capable of running\nin real-time on a modern PC with an average processing time of 1/100th the\ninput interval time.",
            "author": [
                "Hanzhi Zhou",
                "Zichao Hu",
                "Sihang Liu",
                "Samira Khan"
            ],
            "link": [
                "http://dx.doi.org/10.1109/IROS47612.2022.9981200",
                "http://arxiv.org/abs/2312.02353v1",
                "http://arxiv.org/pdf/2312.02353v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02344v1",
            "title": "STEREOFOG -- Computational DeFogging via Image-to-Image Translation on a\n  real-world Dataset",
            "updated": "2023-12-04T21:07:13Z",
            "published": "2023-12-04T21:07:13Z",
            "summary": "Image-to-Image translation (I2I) is a subtype of Machine Learning (ML) that\nhas tremendous potential in applications where two domains of images and the\nneed for translation between the two exist, such as the removal of fog. For\nexample, this could be useful for autonomous vehicles, which currently struggle\nwith adverse weather conditions like fog. However, datasets for I2I tasks are\nnot abundant and typically hard to acquire. Here, we introduce STEREOFOG, a\ndataset comprised of $10,067$ paired fogged and clear images, captured using a\ncustom-built device, with the purpose of exploring I2I's potential in this\ndomain. It is the only real-world dataset of this kind to the best of our\nknowledge. Furthermore, we apply and optimize the pix2pix I2I ML framework to\nthis dataset. With the final model achieving an average Complex\nWavelet-Structural Similarity (CW-SSIM) score of $0.76$, we prove the\ntechnique's suitability for the problem.",
            "author": [
                "Anton Pollak",
                "Rajesh Menon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02344v1",
                "http://arxiv.org/pdf/2312.02344v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02339v1",
            "title": "Expressive Sign Equivariant Networks for Spectral Geometric Learning",
            "updated": "2023-12-04T20:48:18Z",
            "published": "2023-12-04T20:48:18Z",
            "summary": "Recent work has shown the utility of developing machine learning models that\nrespect the structure and symmetries of eigenvectors. These works promote sign\ninvariance, since for any eigenvector v the negation -v is also an eigenvector.\nHowever, we show that sign invariance is theoretically limited for tasks such\nas building orthogonally equivariant models and learning node positional\nencodings for link prediction in graphs. In this work, we demonstrate the\nbenefits of sign equivariance for these tasks. To obtain these benefits, we\ndevelop novel sign equivariant neural network architectures. Our models are\nbased on a new analytic characterization of sign equivariant polynomials and\nthus inherit provable expressiveness properties. Controlled synthetic\nexperiments show that our networks can achieve the theoretically predicted\nbenefits of sign equivariant models. Code is available at\nhttps://github.com/cptq/Sign-Equivariant-Nets.",
            "author": [
                "Derek Lim",
                "Joshua Robinson",
                "Stefanie Jegelka",
                "Haggai Maron"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02339v1",
                "http://arxiv.org/pdf/2312.02339v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02334v1",
            "title": "An Evaluation Framework for Mapping News Headlines to Event Classes in a\n  Knowledge Graph",
            "updated": "2023-12-04T20:42:26Z",
            "published": "2023-12-04T20:42:26Z",
            "summary": "Mapping ongoing news headlines to event-related classes in a rich knowledge\nbase can be an important component in a knowledge-based event analysis and\nforecasting solution. In this paper, we present a methodology for creating a\nbenchmark dataset of news headlines mapped to event classes in Wikidata, and\nresources for the evaluation of methods that perform the mapping. We use the\ndataset to study two classes of unsupervised methods for this task: 1)\nadaptations of classic entity linking methods, and 2) methods that treat the\nproblem as a zero-shot text classification problem. For the first approach, we\nevaluate off-the-shelf entity linking systems. For the second approach, we\nexplore a) pre-trained natural language inference (NLI) models, and b)\npre-trained large generative language models. We present the results of our\nevaluation, lessons learned, and directions for future work. The dataset and\nscripts for evaluation are made publicly available.",
            "author": [
                "Steve Fonin Mbouadeu",
                "Martin Lorenzo",
                "Ken Barker",
                "Oktie Hassanzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02334v1",
                "http://arxiv.org/pdf/2312.02334v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02332v1",
            "title": "Connected Components in Linear Work and Near-Optimal Time",
            "updated": "2023-12-04T20:33:54Z",
            "published": "2023-12-04T20:33:54Z",
            "summary": "Computing the connected components of a graph is a fundamental problem in\nalgorithmic graph theory. A major question in this area is whether we can\ncompute connected components in $o(\\log n)$ parallel time. Recent works showed\nan affirmative answer in the Massively Parallel Computation (MPC) model for a\nwide class of graphs. Specifically, Behnezhad et al. (FOCS'19) showed that\nconnected components can be computed in $O(\\log d + \\log \\log n)$ rounds in the\nMPC model. More recently, Liu et al. (SPAA'20) showed that the same result can\nbe achieved in the standard PRAM model but their result incurs $\\Theta((m+n)\n\\cdot (\\log d + \\log \\log n))$ work which is sub-optimal.\n  In this paper, we show that for graphs that contain \\emph{well-connected}\ncomponents, we can compute connected components on a PRAM in sub-logarithmic\nparallel time with \\emph{optimal}, i.e., $O(m+n)$ total work. Specifically, our\nalgorithm achieves $O(\\log(1/\\lambda) + \\log \\log n)$ parallel time with high\nprobability, where $\\lambda$ is the minimum spectral gap of any connected\ncomponent in the input graph. The algorithm requires no prior knowledge on\n$\\lambda$.\n  Additionally, based on the \\textsc{2-Cycle} Conjecture we provide a time\nlower bound of $\\Omega(\\log(1/\\lambda))$ for solving connected components on a\nPRAM with $O(m+n)$ total memory when $\\lambda \\le (1/\\log n)^c$, giving\nconditional optimality to the running time of our algorithm as a parameter of\n$\\lambda$.",
            "author": [
                "Alireza Farhadi",
                "S. Cliff Liu",
                "Elaine Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02332v1",
                "http://arxiv.org/pdf/2312.02332v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02327v1",
            "title": "FLea: Improving federated learning on scarce and label-skewed data via\n  privacy-preserving feature augmentation",
            "updated": "2023-12-04T20:24:09Z",
            "published": "2023-12-04T20:24:09Z",
            "summary": "Learning a global model by abstracting the knowledge, distributed across\nmultiple clients, without aggregating the raw data is the primary goal of\nFederated Learning (FL). Typically, this works in rounds alternating between\nparallel local training at several clients, followed by model aggregation at a\nserver. We found that existing FL methods under-perform when local datasets are\nsmall and present severe label skew as these lead to over-fitting and local\nmodel bias. This is a realistic setting in many real-world applications. To\naddress the problem, we propose \\textit{FLea}, a unified framework that tackles\nover-fitting and local bias by encouraging clients to exchange\nprivacy-protected features to aid local training. The features refer to\nactivations from an intermediate layer of the model, which are obfuscated\nbefore being shared with other clients to protect sensitive information in the\ndata. \\textit{FLea} leverages a novel way of combining local and shared\nfeatures as augmentations to enhance local model learning. Our extensive\nexperiments demonstrate that \\textit{FLea} outperforms the start-of-the-art FL\nmethods, sharing only model parameters, by up to $17.6\\%$, and FL methods that\nshare data augmentations by up to $6.3\\%$, while reducing the privacy\nvulnerability associated with shared data augmentations.",
            "author": [
                "Tong Xia",
                "Abhirup Ghosh",
                "Cecilia Mascolo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02327v1",
                "http://arxiv.org/pdf/2312.02327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02317v1",
            "title": "GNN2R: Weakly-Supervised Rationale-Providing Question Answering over\n  Knowledge Graphs",
            "updated": "2023-12-04T19:58:07Z",
            "published": "2023-12-04T19:58:07Z",
            "summary": "Most current methods for multi-hop question answering (QA) over knowledge\ngraphs (KGs) only provide final conclusive answers without explanations, such\nas a set of KG entities that is difficult for normal users to review and\ncomprehend. This issue severely limits the application of KG-based QA in\nreal-world scenarios. However, it is non-trivial to solve due to two\nchallenges: First, annotations of reasoning chains of multi-hop questions,\nwhich could serve as supervision for explanation generation, are usually\nlacking. Second, it is difficult to maintain high efficiency when explicit KG\ntriples need to be retrieved to generate explanations. In this paper, we\npropose a novel Graph Neural Network-based Two-Step Reasoning model (GNN2R) to\nsolve this issue. GNN2R can provide both final answers and reasoning subgraphs\nas a rationale behind final answers efficiently with only weak supervision that\nis available through question-final answer pairs. We extensively evaluated\nGNN2R with detailed analyses in experiments. The results demonstrate that, in\nterms of effectiveness, efficiency, and quality of generated explanations,\nGNN2R outperforms existing state-of-the-art methods that are applicable to this\ntask. Our code and pre-trained models are available at\nhttps://github.com/ruijie-wang-uzh/GNN2R.",
            "author": [
                "Ruijie Wang",
                "Luca Rossetto",
                "Michael Cochez",
                "Abraham Bernstein"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02317v1",
                "http://arxiv.org/pdf/2312.02317v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03756v1",
            "title": "LineConGraphs: Line Conversation Graphs for Effective Emotion\n  Recognition using Graph Neural Networks",
            "updated": "2023-12-04T19:36:58Z",
            "published": "2023-12-04T19:36:58Z",
            "summary": "Emotion Recognition in Conversations (ERC) is a critical aspect of affective\ncomputing, and it has many practical applications in healthcare, education,\nchatbots, and social media platforms. Earlier approaches for ERC analysis\ninvolved modeling both speaker and long-term contextual information using graph\nneural network architectures. However, it is ideal to deploy\nspeaker-independent models for real-world applications. Additionally, long\ncontext windows can potentially create confusion in recognizing the emotion of\nan utterance in a conversation. To overcome these limitations, we propose novel\nline conversation graph convolutional network (LineConGCN) and graph attention\n(LineConGAT) models for ERC analysis. These models are speaker-independent and\nbuilt using a graph construction strategy for conversations -- line\nconversation graphs (LineConGraphs). The conversational context in\nLineConGraphs is short-term -- limited to one previous and future utterance,\nand speaker information is not part of the graph. We evaluate the performance\nof our proposed models on two benchmark datasets, IEMOCAP and MELD, and show\nthat our LineConGAT model outperforms the state-of-the-art methods with an\nF1-score of 64.58% and 76.50%. Moreover, we demonstrate that embedding\nsentiment shift information into line conversation graphs further enhances the\nERC performance in the case of GCN models.",
            "author": [
                "Gokul S Krishnan",
                "Sarala Padi",
                "Craig S. Greenberg",
                "Balaraman Ravindran",
                "Dinesh Manoch",
                "Ram D. Sriram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03756v1",
                "http://arxiv.org/pdf/2312.03756v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02266v1",
            "title": "Hierarchical Cross-entropy Loss for Classification of Astrophysical\n  Transients",
            "updated": "2023-12-04T19:00:01Z",
            "published": "2023-12-04T19:00:01Z",
            "summary": "Astrophysical transient phenomena are traditionally classified\nspectroscopically in a hierarchical taxonomy; however, this graph structure is\ncurrently not utilized in neural net-based photometric classifiers for\ntime-domain astrophysics. Instead, independent classifiers are trained for\ndifferent tiers of classified data, and events are excluded if they fall\noutside of these well-defined but flat classification schemes. Here, we\nintroduce a weighted hierarchical cross-entropy objective function for\nclassification of astrophysical transients. Our method allows users to directly\nbuild and use physics- or observationally-motivated tree-based taxonomies. Our\nweighted hierarchical cross-entropy loss directly uses this graph to accurately\nclassify all targets into any node of the tree, re-weighting imbalanced\nclasses. We test our novel loss on a set of variable stars and extragalactic\ntransients from the Zwicky Transient Facility, showing that we can achieve\nsimilar performance to fine-tuned classifiers with the advantage of notably\nmore flexibility in downstream classification tasks.",
            "author": [
                "V. Ashley Villar",
                "Kaylee de Soto",
                "Alex Gagliano"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02266v1",
                "http://arxiv.org/pdf/2312.02266v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02145v1",
            "title": "Repurposing Diffusion-Based Image Generators for Monocular Depth\n  Estimation",
            "updated": "2023-12-04T18:59:13Z",
            "published": "2023-12-04T18:59:13Z",
            "summary": "Monocular depth estimation is a fundamental computer vision task. Recovering\n3D depth from a single image is geometrically ill-posed and requires scene\nunderstanding, so it is not surprising that the rise of deep learning has led\nto a breakthrough. The impressive progress of monocular depth estimators has\nmirrored the growth in model capacity, from relatively modest CNNs to large\nTransformer architectures. Still, monocular depth estimators tend to struggle\nwhen presented with images with unfamiliar content and layout, since their\nknowledge of the visual world is restricted by the data seen during training,\nand challenged by zero-shot generalization to new domains. This motivates us to\nexplore whether the extensive priors captured in recent generative diffusion\nmodels can enable better, more generalizable depth estimation. We introduce\nMarigold, a method for affine-invariant monocular depth estimation that is\nderived from Stable Diffusion and retains its rich prior knowledge. The\nestimator can be fine-tuned in a couple of days on a single GPU using only\nsynthetic training data. It delivers state-of-the-art performance across a wide\nrange of datasets, including over 20% performance gains in specific cases.\nProject page: https://marigoldmonodepth.github.io.",
            "author": [
                "Bingxin Ke",
                "Anton Obukhov",
                "Shengyu Huang",
                "Nando Metzger",
                "Rodrigo Caye Daudt",
                "Konrad Schindler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02145v1",
                "http://arxiv.org/pdf/2312.02145v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02132v1",
            "title": "Hot PATE: Private Aggregation of Distributions for Diverse Task",
            "updated": "2023-12-04T18:54:34Z",
            "published": "2023-12-04T18:54:34Z",
            "summary": "The Private Aggregation of Teacher Ensembles (PATE)\nframework~\\cite{PapernotAEGT:ICLR2017} is a versatile approach to\nprivacy-preserving machine learning. In PATE, teacher models are trained on\ndistinct portions of sensitive data, and their predictions are privately\naggregated to label new training examples for a student model.\n  Until now, PATE has primarily been explored with classification-like tasks,\nwhere each example possesses a ground-truth label, and knowledge is transferred\nto the student by labeling public examples. Generative AI models, however,\nexcel in open ended \\emph{diverse} tasks with multiple valid responses and\nscenarios that may not align with traditional labeled examples. Furthermore,\nthe knowledge of models is often encapsulated in the response distribution\nitself and may be transferred from teachers to student in a more fluid way. We\npropose \\emph{hot PATE}, tailored for the diverse setting. In hot PATE, each\nteacher model produces a response distribution and the aggregation method must\npreserve both privacy and diversity of responses. We demonstrate, analytically\nand empirically, that hot PATE achieves privacy-utility tradeoffs that are\ncomparable to, and in diverse settings, significantly surpass, the baseline\n``cold'' PATE.",
            "author": [
                "Edith Cohen",
                "Xin Lyu",
                "Jelani Nelson",
                "Tamas Sarlos",
                "Uri Stemmer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02132v1",
                "http://arxiv.org/pdf/2312.02132v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02130v1",
            "title": "Fundamental Physics Opportunities with the Next-Generation Event Horizon\n  Telescope",
            "updated": "2023-12-04T18:53:55Z",
            "published": "2023-12-04T18:53:55Z",
            "summary": "The Event Horizon Telescope (EHT) Collaboration recently published the first\nimages of the supermassive black holes in the cores of the Messier 87 and Milky\nWay galaxies. These observations have provided a new means to study\nsupermassive black holes and probe physical processes occurring in the\nstrong-field regime. We review the prospects of future observations and\ntheoretical studies of supermassive black hole systems with the next-generation\nEvent Horizon Telescope (ngEHT), which will greatly enhance the capabilities of\nthe existing EHT array. These enhancements will open up several previously\ninaccessible avenues of investigation, thereby providing important new insights\ninto the properties of supermassive black holes and their environments. This\nreview describes the current state of knowledge for five key science cases,\nsummarising the unique challenges and opportunities for fundamental physics\ninvestigations that the ngEHT will enable.",
            "author": [
                "Dimitry Ayzenberg",
                "Lindy Blackburn",
                "Richard Brito",
                "Silke Britzen",
                "Avery E. Broderick",
                "Ra\u00fal Carballo-Rubio",
                "Vitor Cardoso",
                "Andrew Chael",
                "Koushik Chatterjee",
                "Yifan Chen",
                "Pedro V. P. Cunha",
                "Hooman Davoudiasl",
                "Peter B. Denton",
                "Sheperd S. Doeleman",
                "Astrid Eichhorn",
                "Marshall Eubanks",
                "Yun Fang",
                "Arianna Foschi",
                "Christian M. Fromm",
                "Peter Galison",
                "Sushant G. Ghosh",
                "Roman Gold",
                "Leonid I. Gurvits",
                "Shahar Hadar",
                "Aaron Held",
                "Janice Houston",
                "Yichao Hu",
                "Michael D. Johnson",
                "Prashant Kocherlakota",
                "Priyamvada Natarajan",
                "H\u00e9ctor Olivares",
                "Daniel Palumbo",
                "Dominic W. Pesce",
                "Surjeet Rajendran",
                "Rittick Roy",
                "Saurabh",
                "Lijing Shao",
                "Shammi Tahura",
                "Aditya Tamar",
                "Paul Tiede",
                "Fr\u00e9d\u00e9ric H. Vincent",
                "Luca Visinelli",
                "Zhiren Wang",
                "Maciek Wielgus",
                "Xiao Xue",
                "Kadri Yakut",
                "Huan Yang",
                "Ziri Younsi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02130v1",
                "http://arxiv.org/pdf/2312.02130v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM",
                "gr-qc",
                "hep-ph",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02128v1",
            "title": "Can we truly transfer an actor's genuine happiness to avatars? An\n  investigation into virtual, real, posed and spontaneous faces",
            "updated": "2023-12-04T18:53:42Z",
            "published": "2023-12-04T18:53:42Z",
            "summary": "A look is worth a thousand words is a popular phrase. And why is a simple\nlook enough to portray our feelings about something or someone? Behind this\nquestion are the theoretical foundations of the field of psychology regarding\nsocial cognition and the studies of psychologist Paul Ekman. Facial\nexpressions, as a form of non-verbal communication, are the primary way to\ntransmit emotions between human beings. The set of movements and expressions of\nfacial muscles that convey some emotional state of the individual to their\nobservers are targets of studies in many areas. Our research aims to evaluate\nEkman's action units in datasets of real human faces, posed and spontaneous,\nand virtual human faces resulting from transferring real faces into Computer\nGraphics faces. In addition, we also conducted a case study with specific movie\ncharacters, such as SheHulk and Genius. We intend to find differences and\nsimilarities in facial expressions between real and CG datasets, posed and\nspontaneous faces, and also to consider the actors' genders in the videos. This\ninvestigation can help several areas of knowledge, whether using real or\nvirtual human beings, in education, health, entertainment, games, security, and\neven legal matters. Our results indicate that AU intensities are greater for\nposed than spontaneous datasets, regardless of gender. Furthermore, there is a\nsmoothing of intensity up to 80 percent for AU6 and 45 percent for AU12 when a\nreal face is transformed into CG.",
            "author": [
                "Vitor Miguel Xavier Peres",
                "Greice Pinho Dal Molin",
                "Soraia Raupp Musse"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3631085.3631231",
                "http://arxiv.org/abs/2312.02128v1",
                "http://arxiv.org/pdf/2312.02128v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02114v1",
            "title": "Transitions of Solutions and Their Efficiency",
            "updated": "2023-12-04T18:47:13Z",
            "published": "2023-12-04T18:47:13Z",
            "summary": "We broaden the basis of non-cooperative game theory by considering\nmiscoordination on a solution concept. For any solution concept, we extend the\nsolution set of a strategic-form game to a transition set. This set contains\nprofiles where various agents simultaneously follow different solutions,\ne.g.~different Nash equilibria. This models the fact that in practice,\ncomplicated agents are rarely perfectly coordinated on the same equilibrium. We\ndefine two efficiency measures, called the price of transition anarchy and\nstability, and bound them. We also refine the notion of transition to the\nnotion of limited transition, where only a limited number of solutions is\nsimultaneously played, and to stable transitions, which allow for only minor\nlack of coordination. We compare the above mentioned efficiency measures and\nbound the efficiency of transitions in important cases, including the important\ncases of constant-sum and potential games, which span the set of finite games\nwith the same number of strategies for each agent. We also prove tight\nefficiency bounds for routing games and coordination games on graphs. Finally,\nwe study algorithms to find the transition degree required to make a given\nprofile a transition, or to render all the profiles transitions. We conclude\nthat for the sake of efficiency, it is crucial to avoid uncoordinated\ntransitions, besides certain cases, such as constant-sum games, identical\nutility games, some types of routing games, limited transitions in potential\ngames, and stable transitions in coordination games.",
            "author": [
                "Gleb Polevoy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02114v1",
                "http://arxiv.org/pdf/2312.02114v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.DM",
                "91A10, 91A14, 91A28,",
                "J.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02111v2",
            "title": "TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology",
            "updated": "2023-12-05T12:18:25Z",
            "published": "2023-12-04T18:43:45Z",
            "summary": "Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.",
            "author": [
                "Lucas Farndale",
                "Robert Insall",
                "Ke Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02111v2",
                "http://arxiv.org/pdf/2312.02111v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02103v1",
            "title": "Learning Pseudo-Labeler beyond Noun Concepts for Open-Vocabulary Object\n  Detection",
            "updated": "2023-12-04T18:29:03Z",
            "published": "2023-12-04T18:29:03Z",
            "summary": "Open-vocabulary object detection (OVOD) has recently gained significant\nattention as a crucial step toward achieving human-like visual intelligence.\nExisting OVOD methods extend target vocabulary from pre-defined categories to\nopen-world by transferring knowledge of arbitrary concepts from vision-language\npre-training models to the detectors. While previous methods have shown\nremarkable successes, they suffer from indirect supervision or limited\ntransferable concepts. In this paper, we propose a simple yet effective method\nto directly learn region-text alignment for arbitrary concepts. Specifically,\nthe proposed method aims to learn arbitrary image-to-text mapping for\npseudo-labeling of arbitrary concepts, named Pseudo-Labeling for Arbitrary\nConcepts (PLAC). The proposed method shows competitive performance on the\nstandard OVOD benchmark for noun concepts and a large improvement on referring\nexpression comprehension benchmark for arbitrary concepts.",
            "author": [
                "Sunghun Kang",
                "Junbum Cha",
                "Jonghwan Mun",
                "Byungseok Roh",
                "Chang D. Yoo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02103v1",
                "http://arxiv.org/pdf/2312.02103v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02073v1",
            "title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding\n  with Fakepedia",
            "updated": "2023-12-04T17:35:42Z",
            "published": "2023-12-04T17:35:42Z",
            "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nstoring and recalling factual knowledge, but also in adapting to novel\nin-context information. Yet, the mechanisms underlying their in-context\ngrounding remain unknown, especially in situations where in-context information\ncontradicts factual knowledge embedded in the parameters. This is critical for\nretrieval-augmented generation methods, which enrich the context with\nup-to-date information, hoping that grounding can rectify the outdated\nparametric knowledge. In this study, we introduce Fakepedia, a counterfactual\ndataset designed to evaluate grounding abilities when the parametric knowledge\nclashes with the in-context information. We benchmark various LLMs with\nFakepedia and discover that GPT-4-turbo has a strong preference for its\nparametric knowledge. Mistral-7B, on the contrary, is the model that most\nrobustly chooses the grounded answer. Then, we conduct causal mediation\nanalysis on LLM components when answering Fakepedia queries. We demonstrate\nthat inspection of the computational graph alone can predict LLM grounding with\n92.8% accuracy, especially because few MLPs in the Transformer can predict\nnon-grounded behavior. Our results, together with existing findings about\nfactual recall mechanisms, provide a coherent narrative of how grounding and\nfactual recall mechanisms interact within LLMs.",
            "author": [
                "Giovanni Monea",
                "Maxime Peyrard",
                "Martin Josifoski",
                "Vishrav Chaudhary",
                "Jason Eisner",
                "Emre K\u0131c\u0131man",
                "Hamid Palangi",
                "Barun Patra",
                "Robert West"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02073v1",
                "http://arxiv.org/pdf/2312.02073v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02052v1",
            "title": "DUCK: Distance-based Unlearning via Centroid Kinematics",
            "updated": "2023-12-04T17:10:25Z",
            "published": "2023-12-04T17:10:25Z",
            "summary": "Machine Unlearning is rising as a new field, driven by the pressing necessity\nof ensuring privacy in modern artificial intelligence models. This technique\nprimarily aims to eradicate any residual influence of a specific subset of data\nfrom the knowledge acquired by a neural model during its training. This work\nintroduces a novel unlearning algorithm, denoted as Distance-based Unlearning\nvia Centroid Kinematics (DUCK), which employs metric learning to guide the\nremoval of samples matching the nearest incorrect centroid in the embedding\nspace. Evaluation of the algorithm's performance is conducted across various\nbenchmark datasets in two distinct scenarios, class removal, and homogeneous\nsampling removal, obtaining state-of-the-art performance. We introduce a novel\nmetric, called Adaptive Unlearning Score (AUS), encompassing not only the\nefficacy of the unlearning process in forgetting target data but also\nquantifying the performance loss relative to the original model. Moreover, we\npropose a novel membership inference attack to assess the algorithm's capacity\nto erase previously acquired knowledge, designed to be adaptable to future\nmethodologies.",
            "author": [
                "Marco Cotogni",
                "Jacopo Bonato",
                "Luigi Sabetta",
                "Francesco Pelosin",
                "Alessandro Nicolosi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02052v1",
                "http://arxiv.org/pdf/2312.02052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02048v1",
            "title": "Isomorphism for Tournaments of Small Twin Width",
            "updated": "2023-12-04T17:02:59Z",
            "published": "2023-12-04T17:02:59Z",
            "summary": "We prove that isomorphism of tournaments of twin width at most $k$ can be\ndecided in time $k^{O(\\log k)}n^{O(1)}$. This implies that the isomorphism\nproblem for classes of tournaments of bounded or moderately growing twin width\nis in polynomial time. By comparison, there are classes of undirected graphs of\nbounded twin width that are isomorphism complete, that is, the isomorphism\nproblem for the classes is as hard as the general graph isomorphism problem.\nTwin width is a graph parameter that has been introduced only recently (Bonnet\net al., FOCS 2020), but has received a lot of attention in structural graph\ntheory since then. On directed graphs, it is functionally smaller than clique\nwidth. We prove that on tournaments (but not on general directed graphs) it is\nalso functionally smaller than directed tree width (and thus, the same also\nholds for cut width and directed path width). Hence, our result implies that\ntournament isomorphism testing is also fixed-parameter tractable when\nparameterized by any of these parameters.\n  Our isomorphism algorithm heavily employs group-theoretic techniques. This\nseems to be necessary: as a second main result, we show that the combinatorial\nWeisfeiler-Leman algorithm does not decide isomorphism of tournaments of twin\nwidth at most 35 if its dimension is $o(\\sqrt n)$. (Throughout this abstract,\n$n$ is the order of the input graphs.)",
            "author": [
                "Martin Grohe",
                "Daniel Neuen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02048v1",
                "http://arxiv.org/pdf/2312.02048v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02037v1",
            "title": "GFS: Graph-based Feature Synthesis for Prediction over Relational\n  Databases",
            "updated": "2023-12-04T16:54:40Z",
            "published": "2023-12-04T16:54:40Z",
            "summary": "Relational databases are extensively utilized in a variety of modern\ninformation system applications, and they always carry valuable data patterns.\nThere are a huge number of data mining or machine learning tasks conducted on\nrelational databases. However, it is worth noting that there are limited\nmachine learning models specifically designed for relational databases, as most\nmodels are primarily tailored for single table settings. Consequently, the\nprevalent approach for training machine learning models on data stored in\nrelational databases involves performing feature engineering to merge the data\nfrom multiple tables into a single table and subsequently applying single table\nmodels. This approach not only requires significant effort in feature\nengineering but also destroys the inherent relational structure present in the\ndata. To address these challenges, we propose a novel framework called\nGraph-based Feature Synthesis (GFS). GFS formulates the relational database as\na heterogeneous graph, thereby preserving the relational structure within the\ndata. By leveraging the inductive bias from single table models, GFS\neffectively captures the intricate relationships inherent in each table.\nAdditionally, the whole framework eliminates the need for manual feature\nengineering. In the extensive experiment over four real-world multi-table\nrelational databases, GFS outperforms previous methods designed for relational\ndatabases, demonstrating its superior performance.",
            "author": [
                "Han Zhang",
                "Quan Gan",
                "David Wipf",
                "Weinan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02037v1",
                "http://arxiv.org/pdf/2312.02037v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02028v1",
            "title": "Every $d(d+1)$-connected graph is globally rigid in $\\mathbb{R}^d$",
            "updated": "2023-12-04T16:50:27Z",
            "published": "2023-12-04T16:50:27Z",
            "summary": "Using a probabilistic method, we prove that $d(d+1)$-connected graphs are\nrigid in $\\mathbb{R}^d$, a conjecture of Lov\\'asz and Yemini. Then, using\nrecent results on weakly globally linked pairs, we modify our argument to prove\nthat $d(d+1)$-connected graphs are globally rigid, too, a conjecture of\nConnelly, Jord\\'an and Whiteley. The constant $d(d+1)$ is best possible.",
            "author": [
                "Soma Vill\u00e1nyi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02028v1",
                "http://arxiv.org/pdf/2312.02028v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02016v1",
            "title": "Combinatorial Disjunctive Constraints for Obstacle Avoidance in Path\n  Planning",
            "updated": "2023-12-04T16:38:43Z",
            "published": "2023-12-04T16:38:43Z",
            "summary": "We present a new approach for modeling avoidance constraints in 2D\nenvironments, in which waypoints are assigned to obstacle-free polyhedral\nregions. Constraints of this form are often formulated as mixed-integer\nprogramming (MIP) problems employing big-M techniques -- however, these are\ngenerally not the strongest formulations possible with respect to the MIP's\nconvex relaxation (so called ideal formulations), potentially resulting in\nlarger computational burden. We instead model obstacle avoidance as\ncombinatorial disjunctive constraints and leverage the independent branching\nscheme to construct small, ideal formulations. As our approach requires a\nbiclique cover for an associated graph, we exploit the structure of this class\nof graphs to develop a fast subroutine for obtaining biclique covers in\npolynomial time. We also contribute an open-source Julia library named\nClutteredEnvPathOpt to facilitate computational experiments of MIP formulations\nfor obstacle avoidance. Experiments have shown our formulation is more compact\nand remains competitive on a number of instances compared with standard big-M\ntechniques, for which solvers possess highly optimized procedures.",
            "author": [
                "Raul Garcia",
                "Illya V. Hicks",
                "Joey Huchette"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02016v1",
                "http://arxiv.org/pdf/2312.02016v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01994v1",
            "title": "A Generative Self-Supervised Framework using Functional Connectivity in\n  fMRI Data",
            "updated": "2023-12-04T16:14:43Z",
            "published": "2023-12-04T16:14:43Z",
            "summary": "Deep neural networks trained on Functional Connectivity (FC) networks\nextracted from functional Magnetic Resonance Imaging (fMRI) data have gained\npopularity due to the increasing availability of data and advances in model\narchitectures, including Graph Neural Network (GNN). Recent research on the\napplication of GNN to FC suggests that exploiting the time-varying properties\nof the FC could significantly improve the accuracy and interpretability of the\nmodel prediction. However, the high cost of acquiring high-quality fMRI data\nand corresponding phenotypic labels poses a hurdle to their application in\nreal-world settings, such that a model na\\\"ively trained in a supervised\nfashion can suffer from insufficient performance or a lack of generalization on\na small number of data. In addition, most Self-Supervised Learning (SSL)\napproaches for GNNs to date adopt a contrastive strategy, which tends to lose\nappropriate semantic information when the graph structure is perturbed or does\nnot leverage both spatial and temporal information simultaneously. In light of\nthese challenges, we propose a generative SSL approach that is tailored to\neffectively harness spatio-temporal information within dynamic FC. Our\nempirical results, experimented with large-scale (>50,000) fMRI datasets,\ndemonstrate that our approach learns valuable representations and enables the\nconstruction of accurate and robust models when fine-tuned for downstream\ntasks.",
            "author": [
                "Jungwon Choi",
                "Seongho Keum",
                "EungGu Yun",
                "Byung-Hoon Kim",
                "Juho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01994v1",
                "http://arxiv.org/pdf/2312.01994v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "eess.IV",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01982v1",
            "title": "Stability and Approximations for Decorated Reeb Spaces",
            "updated": "2023-12-04T15:51:00Z",
            "published": "2023-12-04T15:51:00Z",
            "summary": "Given a map $f:X \\to M$ from a topological space $X$ to a metric space $M$, a\ndecorated Reeb space consists of the Reeb space, together with an attribution\nfunction whose values recover geometric information lost during the\nconstruction of the Reeb space. For example, when $M=\\mathbb{R}$ is the real\nline, the Reeb space is the well-known Reeb graph, and the attributions may\nconsist of persistence diagrams summarizing the level set topology of $f$. In\nthis paper, we introduce decorated Reeb spaces in various flavors and prove\nthat our constructions are Gromov-Hausdorff stable. We also provide results on\napproximating decorated Reeb spaces from finite samples and leverage these to\ndevelop a computational framework for applying these constructions to point\ncloud data.",
            "author": [
                "Justin Curry",
                "Washington Mio",
                "Tom Needham",
                "Osman Berat Okutan",
                "Florian Russold"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01982v1",
                "http://arxiv.org/pdf/2312.01982v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "cs.CG",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01970v1",
            "title": "CaRL: Cascade Reinforcement Learning with State Space Splitting for\n  O-RAN based Traffic Steering",
            "updated": "2023-12-04T15:33:00Z",
            "published": "2023-12-04T15:33:00Z",
            "summary": "The Open Radio Access Network (O-RAN) architecture empowers intelligent and\nautomated optimization of the RAN through applications deployed on the RAN\nIntelligent Controller (RIC) platform, enabling capabilities beyond what is\nachievable with traditional RAN solutions. Within this paradigm, Traffic\nSteering (TS) emerges as a pivotal RIC application that focuses on optimizing\ncell-level mobility settings in near-real-time, aiming to significantly improve\nnetwork spectral efficiency. In this paper, we design a novel TS algorithm\nbased on a Cascade Reinforcement Learning (CaRL) framework. We propose state\nspace factorization and policy decomposition to reduce the need for large\nmodels and well-labeled datasets. For each sub-state space, an RL sub-policy\nwill be trained to learn an optimized mapping onto the action space. To apply\nCaRL on new network regions, we propose a knowledge transfer approach to\ninitialize a new sub-policy based on knowledge learned by the trained policies.\nTo evaluate CaRL, we build a data-driven and scalable RIC digital twin (DT)\nthat is modeled using important real-world data, including network\nconfiguration, user geo-distribution, and traffic demand, among others, from a\ntier-1 mobile operator in the US. We evaluate CaRL on two DT scenarios\nrepresenting two network clusters in two different cities and compare its\nperformance with the business-as-usual (BAU) policy and other competing\noptimization approaches using heuristic and Q-table algorithms. Benchmarking\nresults show that CaRL performs the best and improves the average\ncluster-aggregated downlink throughput over the BAU policy by 24% and 18% in\nthese two scenarios, respectively.",
            "author": [
                "Chuanneng Sun",
                "Yu Zhou",
                "Gueyoung Jung",
                "Tuyen Xuan Tran",
                "Dario Pompili"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01970v1",
                "http://arxiv.org/pdf/2312.01970v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.SY",
                "eess.SY",
                "C.2.3; I.2.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01954v1",
            "title": "Zero- and Few-Shots Knowledge Graph Triplet Extraction with Large\n  Language Models",
            "updated": "2023-12-04T15:12:04Z",
            "published": "2023-12-04T15:12:04Z",
            "summary": "In this work, we tested the Triplet Extraction (TE) capabilities of a variety\nof Large Language Models (LLMs) of different sizes in the Zero- and Few-Shots\nsettings. In detail, we proposed a pipeline that dynamically gathers contextual\ninformation from a Knowledge Base (KB), both in the form of context triplets\nand of (sentence, triplets) pairs as examples, and provides it to the LLM\nthrough a prompt. The additional context allowed the LLMs to be competitive\nwith all the older fully trained baselines based on the Bidirectional Long\nShort-Term Memory (BiLSTM) Network architecture. We further conducted a\ndetailed analysis of the quality of the gathered KB context, finding it to be\nstrongly correlated with the final TE performance of the model. In contrast,\nthe size of the model appeared to only logarithmically improve the TE\ncapabilities of the LLMs.",
            "author": [
                "Andrea Papaluca",
                "Daniel Krefl",
                "Sergio Mendez Rodriguez",
                "Artem Lensky",
                "Hanna Suominen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01954v1",
                "http://arxiv.org/pdf/2312.01954v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01946v1",
            "title": "The evaluation of graphs on surfaces for state-sum models with defects",
            "updated": "2023-12-04T15:01:37Z",
            "published": "2023-12-04T15:01:37Z",
            "summary": "The evaluation of graphs on 2-spheres is a central ingredient of the\nTuraev-Viro construction of three-dimensional topological field theories. In\nthis article, we introduce a class of graphs, called extruded graphs, that is\nrelevant for the Turaev-Viro construction with general defect configurations\ninvolving defects of various dimensions. We define the evaluation of extruded\ngraphs and show that it is invariant under a set of moves. This ensures the\ncomputability and uniqueness of our evaluation.",
            "author": [
                "Julian Farnsteiner",
                "Christoph Schweigert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01946v1",
                "http://arxiv.org/pdf/2312.01946v1"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA",
                "math-ph",
                "math.CT",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01939v1",
            "title": "Foundations for Transfer in Reinforcement Learning: A Taxonomy of\n  Knowledge Modalities",
            "updated": "2023-12-04T14:55:58Z",
            "published": "2023-12-04T14:55:58Z",
            "summary": "Contemporary artificial intelligence systems exhibit rapidly growing\nabilities accompanied by the growth of required resources, expansive datasets\nand corresponding investments into computing infrastructure. Although earlier\nsuccesses predominantly focus on constrained settings, recent strides in\nfundamental research and applications aspire to create increasingly general\nsystems. This evolving landscape presents a dual panorama of opportunities and\nchallenges in refining the generalisation and transfer of knowledge - the\nextraction from existing sources and adaptation as a comprehensive foundation\nfor tackling new problems. Within the domain of reinforcement learning (RL),\nthe representation of knowledge manifests through various modalities, including\ndynamics and reward models, value functions, policies, and the original data.\nThis taxonomy systematically targets these modalities and frames its discussion\nbased on their inherent properties and alignment with different objectives and\nmechanisms for transfer. Where possible, we aim to provide coarse guidance\ndelineating approaches which address requirements such as limiting environment\ninteractions, maximising computational efficiency, and enhancing generalisation\nacross varying axes of change. Finally, we analyse reasons contributing to the\nprevalence or scarcity of specific forms of transfer, the inherent potential\nbehind pushing these frontiers, and underscore the significance of\ntransitioning from designed to learned transfer.",
            "author": [
                "Markus Wulfmeier",
                "Arunkumar Byravan",
                "Sarah Bechtle",
                "Karol Hausman",
                "Nicolas Heess"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01939v1",
                "http://arxiv.org/pdf/2312.01939v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01935v1",
            "title": "A Note on the 2-Colored Rectilinear Crossing Number of Random Point Sets\n  in the Unit Square",
            "updated": "2023-12-04T14:49:55Z",
            "published": "2023-12-04T14:49:55Z",
            "summary": "Let $S$ be a set of four points chosen independently, uniformly at random\nfrom a square. Join every pair of points of $S$ with a straight line segment.\nColor these edges red if they have positive slope and blue, otherwise. We show\nthat the probability that $S$ defines a pair of crossing edges of the same\ncolor is equal to $1/4$. This is connected to a recent result of Aichholzer et\nal. [GD 2019] who showed that by 2-colouring the edges of a geometric graph and\ncounting monochromatic crossings instead of crossings, the number of crossings\ncan be more than halfed. Our result shows that for the described random\ndrawings, there is a coloring of the edges such that the number of\nmonochromatic crossings is in expectation $\\frac{1}{2}-\\frac{7}{50}$ of the\ntotal number of crossings.",
            "author": [
                "Sergio Cabello",
                "\u00c9va Czabarka",
                "Ruy Fabila-Monroy",
                "Yuya Higashikawa",
                "Raimund Seidel",
                "L\u00e1szl\u00f3 Sz\u00e9kely",
                "Josef Tkadlec",
                "Alexandra Wesolek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01935v1",
                "http://arxiv.org/pdf/2312.01935v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01921v1",
            "title": "A Machine Learning Approach Towards SKILL Code Autocompletion",
            "updated": "2023-12-04T14:29:28Z",
            "published": "2023-12-04T14:29:28Z",
            "summary": "As Moore's Law continues to increase the complexity of electronic systems,\nElectronic Design Automation (EDA) must advance to meet global demand. An\nimportant example of an EDA technology is SKILL, a scripting language used to\ncustomize and extend EDA software. Recently, code generation models using the\ntransformer architecture have achieved impressive results in academic settings\nand have even been used in commercial developer tools to improve developer\nproductivity. To the best of our knowledge, this study is the first to apply\ntransformers to SKILL code autocompletion towards improving the productivity of\nhardware design engineers. In this study, a novel, data-efficient methodology\nfor generating SKILL code is proposed and experimentally validated. More\nspecifically, we propose a novel methodology for (i) creating a high-quality\nSKILL dataset with both unlabeled and labeled data, (ii) a training strategy\nwhere T5 models pre-trained on general programming language code are fine-tuned\non our custom SKILL dataset using unsupervised and supervised learning, and\n(iii) evaluating synthesized SKILL code. We show that models trained using the\nproposed methodology outperform baselines in terms of human-judgment score and\nBLEU score. A major challenge faced was the extremely small amount of available\nSKILL code data that can be used to train a transformer model to generate SKILL\ncode. Despite our validated improvements, the extremely small dataset available\nto us was still not enough to train a model that can reliably autocomplete\nSKILL code. We discuss this and other limitations as well as future work that\ncould address these limitations.",
            "author": [
                "Enrique Dehaerne",
                "Bappaditya Dey",
                "Wannes Meert"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01921v1",
                "http://arxiv.org/pdf/2312.01921v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL",
                "cs.PL",
                "I.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01916v1",
            "title": "PEACE: Prototype lEarning Augmented transferable framework for\n  Cross-domain rEcommendation",
            "updated": "2023-12-04T14:20:16Z",
            "published": "2023-12-04T14:20:16Z",
            "summary": "To help merchants/customers to provide/access a variety of services through\nminiapps, online service platforms have occupied a critical position in the\neffective content delivery, in which how to recommend items in the new domain\nlaunched by the service provider for customers has become more urgent. However,\nthe non-negligible gap between the source and diversified target domains poses\na considerable challenge to cross-domain recommendation systems, which often\nleads to performance bottlenecks in industrial settings. While entity graphs\nhave the potential to serve as a bridge between domains, rudimentary\nutilization still fail to distill useful knowledge and even induce the negative\ntransfer issue. To this end, we propose PEACE, a Prototype lEarning Augmented\ntransferable framework for Cross-domain rEcommendation. For domain gap\nbridging, PEACE is built upon a multi-interest and entity-oriented pre-training\narchitecture which could not only benefit the learning of generalized knowledge\nin a multi-granularity manner, but also help leverage more structural\ninformation in the entity graph. Then, we bring the prototype learning into the\npre-training over source domains, so that representations of users and items\nare greatly improved by the contrastive prototype learning module and the\nprototype enhanced attention mechanism for adaptive knowledge utilization. To\nease the pressure of online serving, PEACE is carefully deployed in a\nlightweight manner, and significant performance improvements are observed in\nboth online and offline environments.",
            "author": [
                "Chunjing Gan",
                "Bo Huang",
                "Binbin Hu",
                "Jian Ma",
                "Ziqi Liu",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Guannan Zhang",
                "Wenliang Zhong"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01916v1",
                "http://arxiv.org/pdf/2312.01916v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01901v1",
            "title": "Packing and covering a given directed graph in a directed graph",
            "updated": "2023-12-04T13:58:23Z",
            "published": "2023-12-04T13:58:23Z",
            "summary": "For every fixed $k \\ge 4$, it is proved that if an $n$-vertex directed graph\nhas at most $t$ pairwise arc-disjoint directed $k$-cycles, then there exists a\nset of at most $\\frac{2}{3}kt+ o(n^2)$ arcs that meets all directed $k$-cycles\nand that the set of $k$-cycles admits a fractional cover of value at most\n$\\frac{2}{3}kt$. It is also proved that the ratio $\\frac{2}{3}k$ cannot be\nimproved to a constant smaller than $\\frac{k}{2}$. For $k=5$ the constant\n$2k/3$ is improved to $25/8$ and for $k=3$ it was recently shown by Cooper et\nal. that the constant can be taken to be $9/5$. The result implies a\ndeterministic polynomial time $\\frac{2}{3}k$-approximation algorithm for the\ndirected $k$-cycle cover problem, improving upon a previous\n$(k{-}1)$-approximation algorithm of Kortsarz et al.\n  More generally, for every directed graph $H$ we introduce a graph parameter\n$f(H)$ for which it is proved that if an $n$-vertex directed graph has at most\n$t$ pairwise arc-disjoint $H$-copies, then there exists a set of at most\n$f(H)t+ o(n^2)$ arcs that meets all $H$-copies and that the set of $H$-copies\nadmits a fractional cover of value at most $f(H)t$. It is shown that for almost\nall $H$ it holds that $f(H) \\approx |E(H)|/2$ and that for every $k$-vertex\ntournament $H$ it holds that $f(H) \\le \\lfloor k^2/4 \\rfloor$.",
            "author": [
                "Raphael Yuster"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01901v1",
                "http://arxiv.org/pdf/2312.01901v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C20, 05C35, 05C38, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01888v1",
            "title": "Highly Accelerated Weighted MMSE Algorithms for Designing Precoders in\n  FDD Systems with Incomplete CSI",
            "updated": "2023-12-04T13:42:10Z",
            "published": "2023-12-04T13:42:10Z",
            "summary": "In this work, we derive a lower bound on the training-based achievable\ndownlink (DL) sum rate (SR) of a multi-user multiple-input-single-output (MISO)\nsystem operating in frequency-division-duplex (FDD) mode. Assuming linear\nminimum mean square error (LMMSE) channel estimation is used, we establish a\nconnection of the derived lower bound on the signal-to-interference-noise-ratio\n(SINR) to an average MSE that allows to reformulate the SR maximization problem\nas the minimization of the augmented weighted average MSE (AWAMSE). We propose\nan iterative precoder design with three alternating steps, all given in closed\nform, drastically reducing the computation time. We show numerically the\neffectiveness of the proposed approach in challenging scenarios with limited\nchannel knowledge, i.e., we consider scenarios with a very limited number of\npilots. We additionally propose a more efficient version of the well-known\nstochastic iterative WMMSE (SIWMMSE) approach, where the precoder update is\ngiven in closed form.",
            "author": [
                "Donia Ben Amor",
                "Michael Joham",
                "Wolfgang Utschick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01888v1",
                "http://arxiv.org/pdf/2312.01888v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01886v1",
            "title": "InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models",
            "updated": "2023-12-04T13:40:05Z",
            "published": "2023-12-04T13:40:05Z",
            "summary": "Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.",
            "author": [
                "Xunguang Wang",
                "Zhenlan Ji",
                "Pingchuan Ma",
                "Zongjie Li",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01886v1",
                "http://arxiv.org/pdf/2312.01886v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01880v1",
            "title": "Testing popularity in linear time via maximum matching",
            "updated": "2023-12-04T13:23:19Z",
            "published": "2023-12-04T13:23:19Z",
            "summary": "Popularity is an approach in mechanism design to find fair structures in a\ngraph, based on the votes of the nodes. Popular matchings are the relaxation of\nstable matchings: given a graph G=(V,E) with strict preferences on the\nneighbors of the nodes, a matching M is popular if there is no other matching\nM' such that the number of nodes preferring M' is more than those preferring M.\nThis paper considers the popularity testing problem, when the task is to decide\nwhether a given matching is popular or not. Previous algorithms applied\nreductions to maximum weight matchings. We give a new algorithm for testing\npopularity by reducing the problem to maximum matching testing, thus attaining\na linear running time O(|E|).\n  Linear programming-based characterization of popularity is often applied for\nproving the popularity of a certain matching. As a consequence of our algorithm\nwe derive a more structured dual witness than previous ones. Based on this\nresult we give a combinatorial characterization of fractional popular\nmatchings, which are a special class of popular matchings.",
            "author": [
                "Erika B\u00e9rczi-Kov\u00e1cs",
                "Kata Kosztol\u00e1nyi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01880v1",
                "http://arxiv.org/pdf/2312.01880v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01878v1",
            "title": "HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning",
            "updated": "2023-12-04T13:20:15Z",
            "published": "2023-12-04T13:20:15Z",
            "summary": "Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.",
            "author": [
                "Xingtong Yu",
                "Zemin Liu",
                "Yuan Fang",
                "Xinming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01878v1",
                "http://arxiv.org/pdf/2312.01878v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01858v1",
            "title": "Evaluating Dependencies in Fact Editing for Language Models: Specificity\n  and Implication Awareness",
            "updated": "2023-12-04T12:45:30Z",
            "published": "2023-12-04T12:45:30Z",
            "summary": "The potential of using a large language model (LLM) as a knowledge base (KB)\nhas sparked significant interest. To manage the knowledge acquired by LLMs, we\nneed to ensure that the editing of learned facts respects internal logical\nconstraints, which are known as dependency of knowledge. Existing work on\nediting LLMs has partially addressed the issue of dependency, when the editing\nof a fact should apply to its lexical variations without disrupting irrelevant\nones. However, they neglect the dependency between a fact and its logical\nimplications. We propose an evaluation protocol with an accompanying\nquestion-answering dataset, DepEdit, that provides a comprehensive assessment\nof the editing process considering the above notions of dependency. Our\nprotocol involves setting up a controlled environment in which we edit facts\nand monitor their impact on LLMs, along with their implications based on\nIf-Then rules. Extensive experiments on DepEdit show that existing knowledge\nediting methods are sensitive to the surface form of knowledge, and that they\nhave limited performance in inferring the implications of edited facts.",
            "author": [
                "Zichao Li",
                "Ines Arous",
                "Siva Reddy",
                "Jackie C. K. Cheung"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01858v1",
                "http://arxiv.org/pdf/2312.01858v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01837v1",
            "title": "Prompting Disentangled Embeddings for Knowledge Graph Completion with\n  Pre-trained Language Model",
            "updated": "2023-12-04T12:20:25Z",
            "published": "2023-12-04T12:20:25Z",
            "summary": "Both graph structures and textual information play a critical role in\nKnowledge Graph Completion (KGC). With the success of Pre-trained Language\nModels (PLMs) such as BERT, they have been applied for text encoding for KGC.\nHowever, the current methods mostly prefer to fine-tune PLMs, leading to huge\ntraining costs and limited scalability to larger PLMs. In contrast, we propose\nto utilize prompts and perform KGC on a frozen PLM with only the prompts\ntrained. Accordingly, we propose a new KGC method named PDKGC with two prompts\n-- a hard task prompt which is to adapt the KGC task to the PLM pre-training\ntask of token prediction, and a disentangled structure prompt which learns\ndisentangled graph representation so as to enable the PLM to combine more\nrelevant structure knowledge with the text information. With the two prompts,\nPDKGC builds a textual predictor and a structural predictor, respectively, and\ntheir combination leads to more comprehensive entity prediction. Solid\nevaluation on two widely used KGC datasets has shown that PDKGC often\noutperforms the baselines including the state-of-the-art, and its components\nare all effective. Our codes and data are available at\nhttps://github.com/genggengcss/PDKGC.",
            "author": [
                "Yuxia Geng",
                "Jiaoyan Chen",
                "Yuhang Zeng",
                "Zhuo Chen",
                "Wen Zhang",
                "Jeff Z. Pan",
                "Yuxiang Wang",
                "Xiaoliang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01837v1",
                "http://arxiv.org/pdf/2312.01837v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01828v1",
            "title": "Hajnal--M\u00e1t\u00e9 graphs, Cohen reals, and disjoint type guessing",
            "updated": "2023-12-04T12:03:31Z",
            "published": "2023-12-04T12:03:31Z",
            "summary": "A Hajnal--M\\'{a}t\\'{e} graph is an uncountably chromatic graph on $\\omega_1$\nsatisfying a certain natural sparseness condition. We investigate\nHajnal-M\\'{a}t\\'{e} graphs and generalizations thereof, focusing on the\nexistence of Hajnal-M\\'{a}t\\'{e} graphs in models resulting from adding a\nsingle Cohen real. In particular, answering a question of D\\'{a}niel Soukup, we\nshow that such models necessarily contain triangle-free Hajnal-M\\'{a}t\\'{e}\ngraphs. In the process, we isolate a weakening of club guessing called\n\\emph{disjoint type guessing} that we feel is of interest in its own right. We\nshow that disjoint type guessing is independent of $\\mathsf{ZFC}$ and, if\ndisjoint type guessing holds in the ground model, then the forcing extension by\na single Cohen real contains Hajnal-M\\'{a}t\\'{e} graphs $G$ such that the\nchromatic numbers of finite subgraphs of $G$ grow arbitrarily slowly.",
            "author": [
                "Chris Lambie-Hanson",
                "D\u00e1vid Uhrik"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01828v1",
                "http://arxiv.org/pdf/2312.01828v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.CO",
                "03E05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02243v1",
            "title": "FlowHON: Representing Flow Fields Using Higher-Order Networks",
            "updated": "2023-12-04T11:50:25Z",
            "published": "2023-12-04T11:50:25Z",
            "summary": "Flow fields are often partitioned into data blocks for massively parallel\ncomputation and analysis based on blockwise relationships. However, most of the\nprevious techniques only consider the first-order dependencies among blocks,\nwhich is insufficient in describing complex flow patterns. In this work, we\npresent FlowHON, an approach to construct higher-order networks (HONs) from\nflow fields. FlowHON captures the inherent higher-order dependencies in flow\nfields as nodes and estimates the transitions among them as edges. We formulate\nthe HON construction as an optimization problem with three linear\ntransformations. The first two layers correspond to the node generation and the\nthird one corresponds to edge estimation. Our formulation allows the node\ngeneration and edge estimation to be solved in a unified framework. With\nFlowHON, the rich set of traditional graph algorithms can be applied without\nany modification to analyze flow fields, while leveraging the higher-order\ninformation to understand the inherent structure and manage flow data for\nefficiency. We demonstrate the effectiveness of FlowHON using a series of\ndownstream tasks, including estimating the density of particles during tracing,\npartitioning flow fields for data management, and understanding flow fields\nusing the node-link diagram representation of networks.",
            "author": [
                "Nan Chen",
                "Zhihong Li",
                "Jun Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02243v1",
                "http://arxiv.org/pdf/2312.02243v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01812v1",
            "title": "Emergence of innovations in networked populations with reputation-driven\n  interactions",
            "updated": "2023-12-04T11:33:25Z",
            "published": "2023-12-04T11:33:25Z",
            "summary": "In this work we analyze how reputation-based interactions influence the\nemergence of innovations. To do so, we make use of a dynamic model that mimics\nthe discovery process by which, at each time step, a pair of individuals meet\nand merge their knowledge to eventually result in a novel technology of higher\nvalue. The way in which these pairs are brought together is found to be crucial\nfor achieving the highest technological level. Our results show that when the\ninfluence of reputation is weak or moderate, it induces an acceleration of the\ndiscovery process with respect to the neutral case (purely random coupling).\nHowever, an excess of reputation is clearly detrimental, because it leads to an\nexcessive concentration of knowledge in a small set of people, which prevents a\ndiversification of the technologies discovered and, in addition, leads to\nsocieties in which a majority of individuals lack technical capabilities.",
            "author": [
                "Pablo Gallarta-S\u00e1enz",
                "Hugo P\u00e9rez-Mart\u00ednez",
                "Jes\u00fas G\u00f3mez-Garde\u00f1es"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01812v1",
                "http://arxiv.org/pdf/2312.01812v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01804v1",
            "title": "Minimizing Maximum Dissatisfaction in the Allocation of Indivisible\n  Items under a Common Preference Graph",
            "updated": "2023-12-04T11:03:28Z",
            "published": "2023-12-04T11:03:28Z",
            "summary": "We consider the task of allocating indivisible items to agents, when the\nagents' preferences over the items are identical. The preferences are captured\nby means of a directed acyclic graph, with vertices representing items and an\nedge $(a,b)$ meaning that each of the agents prefers item $a$ over item $b$.\nThe dissatisfaction of an agent is measured by the number of items that the\nagent does not receive and also does not receive any more preferred item. The\naim is to allocate the items to the agents in a fair way, i.e., to minimize the\nmaximum dissatisfaction among the agents. We study the status of computational\ncomplexity of that problem and establish the following dichotomy: the problem\nis NP-hard for the case of at least three agents, even on fairly restricted\ngraphs, but polynomially solvable for two agents. We also provide several\npolynomial-time results with respect to different underlying graph structures,\nsuch as graphs of width at most two and tree-like structures such as stars and\nmatchings. These findings are complemented with fixed parameter tractability\nresults related to path modules and independent set modules. Techniques\nemployed in the paper include bottleneck assignment problem, greedy algorithm,\ndynamic programming, maximum network flow, and integer linear programming.",
            "author": [
                "Nina Chiarelli",
                "Cl\u00e9ment Dallard",
                "Andreas Darmann",
                "Stefan Lendl",
                "Martin Milani\u010d",
                "Peter Mur\u0161i\u010d",
                "Ulrich Pferschy"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01804v1",
                "http://arxiv.org/pdf/2312.01804v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "91B32, 90C27, 90C47, 68Q25, 05C85, 05C20, 68Q27"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01803v1",
            "title": "Expected hitting time estimates on finite graphs",
            "updated": "2023-12-04T10:58:25Z",
            "published": "2023-12-04T10:58:25Z",
            "summary": "The expected hitting time from vertex $a$ to vertex $b$, $H(a,b)$, is the\nexpected value of the time it takes a random walk starting at $a$ to reach $b$.\nIn this paper, we give estimates for $H(a,b)$ when the distance between $a$ and\n$b$ is comparable to the diameter of the graph, and the graph satisfies a\nHarnack condition. We show that, in such cases, $H(a,b)$ can be estimated in\nterms of the volumes of balls around $b$. Using our results, we estimate\n$H(a,b)$ on various graphs, such as rectangular tori, some convex traces in\n$\\mathbb{Z}^d$, and fractal graphs. Our proofs use heat kernel estimates.",
            "author": [
                "Laurent Saloff-Coste",
                "Yuwen Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01803v1",
                "http://arxiv.org/pdf/2312.01803v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02240v1",
            "title": "Contrastive Learning-Based Spectral Knowledge Distillation for\n  Multi-Modality and Missing Modality Scenarios in Semantic Segmentation",
            "updated": "2023-12-04T10:27:09Z",
            "published": "2023-12-04T10:27:09Z",
            "summary": "Improving the performance of semantic segmentation models using multispectral\ninformation is crucial, especially for environments with low-light and adverse\nconditions. Multi-modal fusion techniques pursue either the learning of\ncross-modality features to generate a fused image or engage in knowledge\ndistillation but address multimodal and missing modality scenarios as distinct\nissues, which is not an optimal approach for multi-sensor models. To address\nthis, a novel multi-modal fusion approach called CSK-Net is proposed, which\nuses a contrastive learning-based spectral knowledge distillation technique\nalong with an automatic mixed feature exchange mechanism for semantic\nsegmentation in optical (EO) and infrared (IR) images. The distillation scheme\nextracts detailed textures from the optical images and distills them into the\noptical branch of CSK-Net. The model encoder consists of shared convolution\nweights with separate batch norm (BN) layers for both modalities, to capture\nthe multi-spectral information from different modalities of the same objects. A\nNovel Gated Spectral Unit (GSU) and mixed feature exchange strategy are\nproposed to increase the correlation of modality-shared information and\ndecrease the modality-specific information during the distillation process.\nComprehensive experiments show that CSK-Net surpasses state-of-the-art models\nin multi-modal tasks and for missing modalities when exclusively utilizing IR\ndata for inference across three public benchmarking datasets. For missing\nmodality scenarios, the performance increase is achieved without additional\ncomputational costs compared to the baseline segmentation models.",
            "author": [
                "Aniruddh Sikdar",
                "Jayant Teotia",
                "Suresh Sundaram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02240v1",
                "http://arxiv.org/pdf/2312.02240v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01772v1",
            "title": "Momentum scale calibration of the LHCb spectrometer",
            "updated": "2023-12-04T09:56:46Z",
            "published": "2023-12-04T09:56:46Z",
            "summary": "For accurate determination of particle masses accurate knowledge of the\nmomentum scale of the detectors is crucial. The procedure used to calibrate the\nmomentum scale of the LHCb spectrometer is described and illustrated using the\nperformance obtained with an integrated luminosity of $1.6~ fb^{-1}$ collected\nduring 2016 in $pp$ running. The procedure uses large samples of $J/\\psi\n\\rightarrow \\mu^+ \\mu^-$ and $B^+ \\rightarrow J/\\psi K^+$ decays and leads to a\nrelative accuracy of $3 \\times 10^{-4}$ on the momentum scale.",
            "author": [
                "LHCb collaboration",
                "R. Aaij",
                "A. S. W. Abdelmotteleb",
                "C. Abellan Beteta",
                "F. Abudin\u00e9n",
                "T. Ackernley",
                "B. Adeva",
                "M. Adinolfi",
                "P. Adlarson",
                "C. Agapopoulou",
                "C. A. Aidala",
                "Z. Ajaltouni",
                "S. Akar",
                "K. Akiba",
                "P. Albicocco",
                "J. Albrecht",
                "F. Alessio",
                "M. Alexander",
                "A. Alfonso Albero",
                "Z. Aliouche",
                "P. Alvarez Cartelle",
                "R. Amalric",
                "S. Amato",
                "J. L. Amey",
                "Y. Amhis",
                "L. An",
                "L. Anderlini",
                "M. Andersson",
                "A. Andreianov",
                "P. Andreola",
                "M. Andreotti",
                "D. Andreou",
                "A. Anelli",
                "D. Ao",
                "F. Archilli",
                "M. Argenton",
                "S. Arguedas Cuendis",
                "A. Artamonov",
                "M. Artuso",
                "E. Aslanides",
                "M. Atzeni",
                "B. Audurier",
                "D. Bacher",
                "I. Bachiller Perea",
                "S. Bachmann",
                "M. Bachmayer",
                "J. J. Back",
                "P. Baladron Rodriguez",
                "V. Balagura",
                "W. Baldini",
                "J. Baptista de Souza Leite",
                "M. Barbetti",
                "I. R. Barbosa",
                "R. J. Barlow",
                "S. Barsuk",
                "W. Barter",
                "M. Bartolini",
                "J. Bartz",
                "F. Baryshnikov",
                "J. M. Basels",
                "G. Bassi",
                "B. Batsukh",
                "A. Battig",
                "A. Bay",
                "A. Beck",
                "M. Becker",
                "F. Bedeschi",
                "I. B. Bediaga",
                "A. Beiter",
                "S. Belin",
                "V. Bellee",
                "K. Belous",
                "I. Belov",
                "I. Belyaev",
                "G. Benane",
                "G. Bencivenni",
                "E. Ben-Haim",
                "A. Berezhnoy",
                "R. Bernet",
                "S. Bernet Andres",
                "C. Bertella",
                "A. Bertolin",
                "C. Betancourt",
                "F. Betti",
                "J. Bex",
                "Ia. Bezshyiko",
                "J. Bhom",
                "M. S. Bieker",
                "N. V. Biesuz",
                "P. Billoir",
                "A. Biolchini",
                "M. Birch",
                "F. C. R. Bishop",
                "A. Bitadze",
                "A. Bizzeti",
                "M. P. Blago",
                "T. Blake",
                "F. Blanc",
                "J. E. Blank",
                "S. Blusk",
                "D. Bobulska",
                "V. Bocharnikov",
                "J. A. Boelhauve",
                "O. Boente Garcia",
                "T. Boettcher",
                "A. Bohare",
                "A. Boldyrev",
                "C. S. Bolognani",
                "R. Bolzonella",
                "N. Bondar",
                "F. Borgato",
                "S. Borghi",
                "M. Borsato",
                "J. T. Borsuk",
                "S. A. Bouchiba",
                "T. J. V. Bowcock",
                "A. Boyer",
                "C. Bozzi",
                "M. J. Bradley",
                "A. Brea Rodriguez",
                "N. Breer",
                "J. Brodzicka",
                "A. Brossa Gonzalo",
                "J. Brown",
                "D. Brundu",
                "A. Buonaura",
                "L. Buonincontri",
                "A. T. Burke",
                "C. Burr",
                "A. Bursche",
                "A. Butkevich",
                "J. S. Butter",
                "J. Buytaert",
                "W. Byczynski",
                "S. Cadeddu",
                "H. Cai",
                "R. Calabrese",
                "L. Calefice",
                "S. Cali",
                "M. Calvi",
                "M. Calvo Gomez",
                "J. Cambon Bouzas",
                "P. Campana",
                "D. H. Campora Perez",
                "A. F. Campoverde Quezada",
                "S. Capelli",
                "L. Capriotti",
                "R. Caravaca-Mora",
                "A. Carbone",
                "L. Carcedo Salgado",
                "R. Cardinale",
                "A. Cardini",
                "P. Carniti",
                "L. Carus",
                "A. Casais Vidal",
                "R. Caspary",
                "G. Casse",
                "J. Castro Godinez",
                "M. Cattaneo",
                "G. Cavallero",
                "V. Cavallini",
                "S. Celani",
                "J. Cerasoli",
                "D. Cervenkov",
                "S. Cesare",
                "A. J. Chadwick",
                "I. Chahrour",
                "M. Charles",
                "Ph. Charpentier",
                "C. A. Chavez Barajas",
                "M. Chefdeville",
                "C. Chen",
                "S. Chen",
                "Z. Chen",
                "A. Chernov",
                "S. Chernyshenko",
                "V. Chobanova",
                "S. Cholak",
                "M. Chrzaszcz",
                "A. Chubykin",
                "V. Chulikov",
                "P. Ciambrone",
                "M. F. Cicala",
                "X. Cid Vidal",
                "G. Ciezarek",
                "P. Cifra",
                "P. E. L. Clarke",
                "M. Clemencic",
                "H. V. Cliff",
                "J. Closier",
                "J. L. Cobbledick",
                "C. Cocha Toapaxi",
                "V. Coco",
                "J. Cogan",
                "E. Cogneras",
                "L. Cojocariu",
                "P. Collins",
                "T. Colombo",
                "A. Comerma-Montells",
                "L. Congedo",
                "A. Contu",
                "N. Cooke",
                "I. Corredoira",
                "A. Correia",
                "G. Corti",
                "J. J. Cottee Meldrum",
                "B. Couturier",
                "D. C. Craik",
                "M. Cruz Torres",
                "E. Curras Rivera",
                "R. Currie",
                "C. L. Da Silva",
                "S. Dadabaev",
                "L. Dai",
                "X. Dai",
                "E. Dall'Occo",
                "J. Dalseno",
                "C. D'Ambrosio",
                "J. Daniel",
                "A. Danilina",
                "P. d'Argent",
                "A. Davidson",
                "J. E. Davies",
                "A. Davis",
                "O. De Aguiar Francisco",
                "C. De Angelis",
                "J. de Boer",
                "K. De Bruyn",
                "S. De Capua",
                "M. De Cian",
                "U. De Freitas Carneiro Da Graca",
                "E. De Lucia",
                "J. M. De Miranda",
                "L. De Paula",
                "M. De Serio",
                "D. De Simone",
                "P. De Simone",
                "F. De Vellis",
                "J. A. de Vries",
                "F. Debernardis",
                "D. Decamp",
                "V. Dedu",
                "L. Del Buono",
                "B. Delaney",
                "H. -P. Dembinski",
                "J. Deng",
                "V. Denysenko",
                "O. Deschamps",
                "F. Dettori",
                "B. Dey",
                "P. Di Nezza",
                "I. Diachkov",
                "S. Didenko",
                "S. Ding",
                "V. Dobishuk",
                "A. D. Docheva",
                "A. Dolmatov",
                "C. Dong",
                "A. M. Donohoe",
                "F. Dordei",
                "A. C. dos Reis",
                "L. Douglas",
                "A. G. Downes",
                "W. Duan",
                "P. Duda",
                "M. W. Dudek",
                "L. Dufour",
                "V. Duk",
                "P. Durante",
                "M. M. Duras",
                "J. M. Durham",
                "A. Dziurda",
                "A. Dzyuba",
                "S. Easo",
                "E. Eckstein",
                "U. Egede",
                "A. Egorychev",
                "V. Egorychev",
                "C. Eirea Orro",
                "S. Eisenhardt",
                "E. Ejopu",
                "S. Ek-In",
                "L. Eklund",
                "M. Elashri",
                "J. Ellbracht",
                "S. Ely",
                "A. Ene",
                "E. Epple",
                "S. Escher",
                "J. Eschle",
                "S. Esen",
                "T. Evans",
                "F. Fabiano",
                "L. N. Falcao",
                "Y. Fan",
                "B. Fang",
                "L. Fantini",
                "M. Faria",
                "K. Farmer",
                "D. Fazzini",
                "L. Felkowski",
                "M. Feng",
                "M. Feo",
                "M. Fernandez Gomez",
                "A. D. Fernez",
                "F. Ferrari",
                "F. Ferreira Rodrigues",
                "S. Ferreres Sole",
                "M. Ferrillo",
                "M. Ferro-Luzzi",
                "S. Filippov",
                "R. A. Fini",
                "M. Fiorini",
                "K. M. Fischer",
                "D. S. Fitzgerald",
                "C. Fitzpatrick",
                "F. Fleuret",
                "M. Fontana",
                "L. F. Foreman",
                "R. Forty",
                "D. Foulds-Holt",
                "M. Franco Sevilla",
                "M. Frank",
                "E. Franzoso",
                "G. Frau",
                "C. Frei",
                "D. A. Friday",
                "L. Frontini",
                "J. Fu",
                "Q. Fuehring",
                "Y. Fujii",
                "T. Fulghesu",
                "E. Gabriel",
                "G. Galati",
                "M. D. Galati",
                "A. Gallas Torreira",
                "D. Galli",
                "S. Gambetta",
                "M. Gandelman",
                "P. Gandini",
                "H. Gao",
                "R. Gao",
                "Y. Gao",
                "Y. Gao",
                "Y. Gao",
                "M. Garau",
                "L. M. Garcia Martin",
                "P. Garcia Moreno",
                "J. Garc\u00eda Pardi\u00f1as",
                "B. Garcia Plana",
                "K. G. Garg",
                "L. Garrido",
                "C. Gaspar",
                "R. E. Geertsema",
                "L. L. Gerken",
                "E. Gersabeck",
                "M. Gersabeck",
                "T. Gershon",
                "Z. Ghorbanimoghaddam",
                "L. Giambastiani",
                "F. I. Giasemis",
                "V. Gibson",
                "H. K. Giemza",
                "A. L. Gilman",
                "M. Giovannetti",
                "A. Giovent\u00f9",
                "P. Gironella Gironell",
                "C. Giugliano",
                "M. A. Giza",
                "E. L. Gkougkousis",
                "F. C. Glaser",
                "V. V. Gligorov",
                "C. G\u00f6bel",
                "E. Golobardes",
                "D. Golubkov",
                "A. Golutvin",
                "A. Gomes",
                "S. Gomez Fernandez",
                "F. Goncalves Abrantes",
                "M. Goncerz",
                "G. Gong",
                "J. A. Gooding",
                "I. V. Gorelov",
                "C. Gotti",
                "J. P. Grabowski",
                "L. A. Granado Cardoso",
                "E. Graug\u00e9s",
                "E. Graverini",
                "L. Grazette",
                "G. Graziani",
                "A. T. Grecu",
                "L. M. Greeven",
                "N. A. Grieser",
                "L. Grillo",
                "S. Gromov",
                "C. Gu",
                "M. Guarise",
                "M. Guittiere",
                "V. Guliaeva",
                "P. A. G\u00fcnther",
                "A. -K. Guseinov",
                "E. Gushchin",
                "Y. Guz",
                "T. Gys",
                "K. Habermann",
                "T. Hadavizadeh",
                "C. Hadjivasiliou",
                "G. Haefeli",
                "C. Haen",
                "J. Haimberger",
                "M. Hajheidari",
                "M. M. Halvorsen",
                "P. M. Hamilton",
                "J. Hammerich",
                "Q. Han",
                "X. Han",
                "S. Hansmann-Menzemer",
                "L. Hao",
                "N. Harnew",
                "T. Harrison",
                "M. Hartmann",
                "J. He",
                "K. Heijhoff",
                "F. Hemmer",
                "C. Henderson",
                "R. D. L. Henderson",
                "A. M. Hennequin",
                "K. Hennessy",
                "L. Henry",
                "J. Herd",
                "P. Herrero Gascon",
                "J. Heuel",
                "A. Hicheur",
                "G. Hijano Mendizabal",
                "D. Hill",
                "S. E. Hollitt",
                "J. Horswill",
                "R. Hou",
                "Y. Hou",
                "N. Howarth",
                "J. Hu",
                "J. Hu",
                "W. Hu",
                "X. Hu",
                "W. Huang",
                "W. Hulsbergen",
                "R. J. Hunter",
                "M. Hushchyn",
                "D. Hutchcroft",
                "D. Ilin",
                "P. Ilten",
                "A. Inglessi",
                "A. Iniukhin",
                "A. Ishteev",
                "K. Ivshin",
                "R. Jacobsson",
                "H. Jage",
                "S. J. Jaimes Elles",
                "S. Jakobsen",
                "E. Jans",
                "B. K. Jashal",
                "A. Jawahery",
                "V. Jevtic",
                "E. Jiang",
                "X. Jiang",
                "Y. Jiang",
                "Y. J. Jiang",
                "M. John",
                "D. Johnson",
                "C. R. Jones",
                "T. P. Jones",
                "S. Joshi",
                "B. Jost",
                "N. Jurik",
                "I. Juszczak",
                "D. Kaminaris",
                "S. Kandybei",
                "Y. Kang",
                "M. Karacson",
                "D. Karpenkov",
                "M. Karpov",
                "A. M. Kauniskangas",
                "J. W. Kautz",
                "F. Keizer",
                "D. M. Keller",
                "M. Kenzie",
                "T. Ketel",
                "B. Khanji",
                "A. Kharisova",
                "S. Kholodenko",
                "G. Khreich",
                "T. Kirn",
                "V. S. Kirsebom",
                "O. Kitouni",
                "S. Klaver",
                "N. Kleijne",
                "K. Klimaszewski",
                "M. R. Kmiec",
                "S. Koliiev",
                "L. Kolk",
                "A. Konoplyannikov",
                "P. Kopciewicz",
                "P. Koppenburg",
                "M. Korolev",
                "I. Kostiuk",
                "O. Kot",
                "S. Kotriakhova",
                "A. Kozachuk",
                "P. Kravchenko",
                "L. Kravchuk",
                "M. Kreps",
                "S. Kretzschmar",
                "P. Krokovny",
                "W. Krupa",
                "W. Krzemien",
                "J. Kubat",
                "S. Kubis",
                "W. Kucewicz",
                "M. Kucharczyk",
                "V. Kudryavtsev",
                "E. Kulikova",
                "A. Kupsc",
                "B. K. Kutsenko",
                "D. Lacarrere",
                "A. Lai",
                "A. Lampis",
                "D. Lancierini",
                "C. Landesa Gomez",
                "J. J. Lane",
                "R. Lane",
                "C. Langenbruch",
                "J. Langer",
                "O. Lantwin",
                "T. Latham",
                "F. Lazzari",
                "C. Lazzeroni",
                "R. Le Gac",
                "S. H. Lee",
                "R. Lef\u00e8vre",
                "A. Leflat",
                "S. Legotin",
                "M. Lehuraux",
                "O. Leroy",
                "T. Lesiak",
                "B. Leverington",
                "A. Li",
                "H. Li",
                "K. Li",
                "L. Li",
                "P. Li",
                "P. -R. Li",
                "S. Li",
                "T. Li",
                "T. Li",
                "Y. Li",
                "Y. Li",
                "Z. Li",
                "Z. Lian",
                "X. Liang",
                "C. Lin",
                "T. Lin",
                "R. Lindner",
                "V. Lisovskyi",
                "R. Litvinov",
                "F. L. Liu",
                "G. Liu",
                "K. Liu",
                "Q. Liu",
                "S. Liu",
                "Y. Liu",
                "Y. Liu",
                "Y. L. Liu",
                "A. Lobo Salvia",
                "A. Loi",
                "J. Lomba Castro",
                "T. Long",
                "J. H. Lopes",
                "A. Lopez Huertas",
                "S. L\u00f3pez Soli\u00f1o",
                "G. H. Lovell",
                "C. Lucarelli",
                "D. Lucchesi",
                "S. Luchuk",
                "M. Lucio Martinez",
                "V. Lukashenko",
                "Y. Luo",
                "A. Lupato",
                "E. Luppi",
                "K. Lynch",
                "X. -R. Lyu",
                "G. M. Ma",
                "R. Ma",
                "S. Maccolini",
                "F. Machefert",
                "F. Maciuc",
                "B. M. Mack",
                "I. Mackay",
                "L. M. Mackey",
                "L. R. Madhan Mohan",
                "M. M. Madurai",
                "A. Maevskiy",
                "D. Magdalinski",
                "D. Maisuzenko",
                "M. W. Majewski",
                "J. J. Malczewski",
                "S. Malde",
                "B. Malecki",
                "L. Malentacca",
                "A. Malinin",
                "T. Maltsev",
                "G. Manca",
                "G. Mancinelli",
                "C. Mancuso",
                "R. Manera Escalero",
                "D. Manuzzi",
                "D. Marangotto",
                "J. F. Marchand",
                "R. Marchevski",
                "U. Marconi",
                "S. Mariani",
                "C. Marin Benito",
                "J. Marks",
                "A. M. Marshall",
                "P. J. Marshall",
                "G. Martelli",
                "G. Martellotti",
                "L. Martinazzoli",
                "M. Martinelli",
                "D. Martinez Santos",
                "F. Martinez Vidal",
                "A. Massafferri",
                "M. Materok",
                "R. Matev",
                "A. Mathad",
                "V. Matiunin",
                "C. Matteuzzi",
                "K. R. Mattioli",
                "A. Mauri",
                "E. Maurice",
                "J. Mauricio",
                "P. Mayencourt",
                "M. Mazurek",
                "M. McCann",
                "L. Mcconnell",
                "T. H. McGrath",
                "N. T. McHugh",
                "A. McNab",
                "R. McNulty",
                "B. Meadows",
                "G. Meier",
                "D. Melnychuk",
                "M. Merk",
                "A. Merli",
                "L. Meyer Garcia",
                "D. Miao",
                "H. Miao",
                "M. Mikhasenko",
                "D. A. Milanes",
                "A. Minotti",
                "E. Minucci",
                "T. Miralles",
                "S. E. Mitchell",
                "B. Mitreska",
                "D. S. Mitzel",
                "A. Modak",
                "A. M\u00f6dden",
                "R. A. Mohammed",
                "R. D. Moise",
                "S. Mokhnenko",
                "T. Momb\u00e4cher",
                "M. Monk",
                "I. A. Monroy",
                "S. Monteil",
                "A. Morcillo Gomez",
                "G. Morello",
                "M. J. Morello",
                "M. P. Morgenthaler",
                "A. B. Morris",
                "A. G. Morris",
                "R. Mountain",
                "H. Mu",
                "Z. M. Mu",
                "E. Muhammad",
                "F. Muheim",
                "M. Mulder",
                "K. M\u00fcller",
                "F. M\u0169noz-Rojas",
                "R. Murta",
                "P. Naik",
                "T. Nakada",
                "R. Nandakumar",
                "T. Nanut",
                "I. Nasteva",
                "M. Needham",
                "N. Neri",
                "S. Neubert",
                "N. Neufeld",
                "P. Neustroev",
                "R. Newcombe",
                "J. Nicolini",
                "D. Nicotra",
                "E. M. Niel",
                "N. Nikitin",
                "P. Nogga",
                "N. S. Nolte",
                "C. Normand",
                "J. Novoa Fernandez",
                "G. Nowak",
                "C. Nunez",
                "H. N. Nur",
                "A. Oblakowska-Mucha",
                "V. Obraztsov",
                "T. Oeser",
                "S. Okamura",
                "R. Oldeman",
                "F. Oliva",
                "M. Olocco",
                "C. J. G. Onderwater",
                "R. H. O'Neil",
                "J. M. Otalora Goicochea",
                "T. Ovsiannikova",
                "P. Owen",
                "A. Oyanguren",
                "O. Ozcelik",
                "K. O. Padeken",
                "B. Pagare",
                "P. R. Pais",
                "T. Pajero",
                "A. Palano",
                "M. Palutan",
                "G. Panshin",
                "L. Paolucci",
                "A. Papanestis",
                "M. Pappagallo",
                "L. L. Pappalardo",
                "C. Pappenheimer",
                "C. Parkes",
                "B. Passalacqua",
                "G. Passaleva",
                "D. Passaro",
                "A. Pastore",
                "M. Patel",
                "J. Patoc",
                "C. Patrignani",
                "C. J. Pawley",
                "A. Pellegrino",
                "M. Pepe Altarelli",
                "S. Perazzini",
                "D. Pereima",
                "A. Pereiro Castro",
                "P. Perret",
                "A. Perro",
                "K. Petridis",
                "A. Petrolini",
                "S. Petrucci",
                "J. P. Pfaller",
                "H. Pham",
                "L. Pica",
                "M. Piccini",
                "B. Pietrzyk",
                "G. Pietrzyk",
                "D. Pinci",
                "F. Pisani",
                "M. Pizzichemi",
                "V. Placinta",
                "M. Plo Casasus",
                "F. Polci",
                "M. Poli Lener",
                "A. Poluektov",
                "N. Polukhina",
                "I. Polyakov",
                "E. Polycarpo",
                "S. Ponce",
                "D. Popov",
                "S. Poslavskii",
                "K. Prasanth",
                "C. Prouve",
                "V. Pugatch",
                "G. Punzi",
                "W. Qian",
                "N. Qin",
                "S. Qu",
                "R. Quagliani",
                "R. I. Rabadan Trejo",
                "J. H. Rademacker",
                "M. Rama",
                "M. Ram\u00edrez Garc\u00eda",
                "M. Ramos Pernas",
                "M. S. Rangel",
                "F. Ratnikov",
                "G. Raven",
                "M. Rebollo De Miguel",
                "F. Redi",
                "J. Reich",
                "F. Reiss",
                "Z. Ren",
                "P. K. Resmi",
                "R. Ribatti",
                "G. R. Ricart",
                "D. Riccardi",
                "S. Ricciardi",
                "K. Richardson",
                "M. Richardson-Slipper",
                "K. Rinnert",
                "P. Robbe",
                "G. Robertson",
                "E. Rodrigues",
                "E. Rodriguez Fernandez",
                "J. A. Rodriguez Lopez",
                "E. Rodriguez Rodriguez",
                "A. Rogovskiy",
                "D. L. Rolf",
                "A. Rollings",
                "P. Roloff",
                "V. Romanovskiy",
                "M. Romero Lamas",
                "A. Romero Vidal",
                "G. Romolini",
                "F. Ronchetti",
                "M. Rotondo",
                "S. R. Roy",
                "M. S. Rudolph",
                "T. Ruf",
                "M. Ruiz Diaz",
                "R. A. Ruiz Fernandez",
                "J. Ruiz Vidal",
                "A. Ryzhikov",
                "J. Ryzka",
                "J. J. Saborido Silva",
                "R. Sadek",
                "N. Sagidova",
                "N. Sahoo",
                "B. Saitta",
                "M. Salomoni",
                "C. Sanchez Gras",
                "I. Sanderswood",
                "R. Santacesaria",
                "C. Santamarina Rios",
                "M. Santimaria",
                "L. Santoro",
                "E. Santovetti",
                "A. Saputi",
                "D. Saranin",
                "G. Sarpis",
                "M. Sarpis",
                "A. Sarti",
                "C. Satriano",
                "A. Satta",
                "M. Saur",
                "D. Savrina",
                "H. Sazak",
                "L. G. Scantlebury Smead",
                "A. Scarabotto",
                "S. Schael",
                "S. Scherl",
                "A. M. Schertz",
                "M. Schiller",
                "H. Schindler",
                "M. Schmelling",
                "B. Schmidt",
                "S. Schmitt",
                "H. Schmitz",
                "O. Schneider",
                "A. Schopper",
                "N. Schulte",
                "S. Schulte",
                "M. H. Schune",
                "R. Schwemmer",
                "G. Schwering",
                "B. Sciascia",
                "A. Sciuccati",
                "S. Sellam",
                "A. Semennikov",
                "M. Senghi Soares",
                "A. Sergi",
                "N. Serra",
                "L. Sestini",
                "A. Seuthe",
                "Y. Shang",
                "D. M. Shangase",
                "M. Shapkin",
                "R. S. Sharma",
                "I. Shchemerov",
                "L. Shchutska",
                "T. Shears",
                "L. Shekhtman",
                "Z. Shen",
                "S. Sheng",
                "V. Shevchenko",
                "B. Shi",
                "E. B. Shields",
                "Y. Shimizu",
                "E. Shmanin",
                "R. Shorkin",
                "J. D. Shupperd",
                "R. Silva Coutinho",
                "G. Simi",
                "S. Simone",
                "N. Skidmore",
                "R. Skuza",
                "T. Skwarnicki",
                "M. W. Slater",
                "J. C. Smallwood",
                "E. Smith",
                "K. Smith",
                "M. Smith",
                "A. Snoch",
                "L. Soares Lavra",
                "M. D. Sokoloff",
                "F. J. P. Soler",
                "A. Solomin",
                "A. Solovev",
                "I. Solovyev",
                "R. Song",
                "Y. Song",
                "Y. Song",
                "Y. S. Song",
                "F. L. Souza De Almeida",
                "B. Souza De Paula",
                "E. Spadaro Norella",
                "E. Spedicato",
                "J. G. Speer",
                "E. Spiridenkov",
                "P. Spradlin",
                "V. Sriskaran",
                "F. Stagni",
                "M. Stahl",
                "S. Stahl",
                "S. Stanislaus",
                "E. N. Stein",
                "O. Steinkamp",
                "O. Stenyakin",
                "H. Stevens",
                "D. Strekalina",
                "Y. Su",
                "F. Suljik",
                "J. Sun",
                "L. Sun",
                "Y. Sun",
                "P. N. Swallow",
                "F. Swystun",
                "A. Szabelski",
                "T. Szumlak",
                "M. Szymanski",
                "Y. Tan",
                "S. Taneja",
                "M. D. Tat",
                "A. Terentev",
                "F. Terzuoli",
                "F. Teubert",
                "E. Thomas",
                "D. J. D. Thompson",
                "H. Tilquin",
                "V. Tisserand",
                "S. T'Jampens",
                "M. Tobin",
                "L. Tomassetti",
                "G. Tonani",
                "X. Tong",
                "D. Torres Machado",
                "L. Toscano",
                "D. Y. Tou",
                "C. Trippl",
                "G. Tuci",
                "N. Tuning",
                "L. H. Uecker",
                "A. Ukleja",
                "D. J. Unverzagt",
                "E. Ursov",
                "A. Usachov",
                "A. Ustyuzhanin",
                "U. Uwer",
                "V. Vagnoni",
                "A. Valassi",
                "G. Valenti",
                "N. Valls Canudas",
                "H. Van Hecke",
                "E. van Herwijnen",
                "C. B. Van Hulse",
                "R. Van Laak",
                "M. van Veghel",
                "R. Vazquez Gomez",
                "P. Vazquez Regueiro",
                "C. V\u00e1zquez Sierra",
                "S. Vecchi",
                "J. J. Velthuis",
                "M. Veltri",
                "A. Venkateswaran",
                "M. Vesterinen",
                "M. Vieites Diaz",
                "X. Vilasis-Cardona",
                "E. Vilella Figueras",
                "A. Villa",
                "P. Vincent",
                "F. C. Volle",
                "D. vom Bruch",
                "V. Vorobyev",
                "N. Voropaev",
                "K. Vos",
                "G. Vouters",
                "C. Vrahas",
                "J. Walsh",
                "E. J. Walton",
                "G. Wan",
                "C. Wang",
                "G. Wang",
                "J. Wang",
                "J. Wang",
                "J. Wang",
                "J. Wang",
                "M. Wang",
                "N. W. Wang",
                "R. Wang",
                "X. Wang",
                "X. W. Wang",
                "Y. Wang",
                "Z. Wang",
                "Z. Wang",
                "Z. Wang",
                "J. A. Ward",
                "M. Waterlaat",
                "N. K. Watson",
                "D. Websdale",
                "Y. Wei",
                "B. D. C. Westhenry",
                "D. J. White",
                "M. Whitehead",
                "A. R. Wiederhold",
                "D. Wiedner",
                "G. Wilkinson",
                "M. K. Wilkinson",
                "M. Williams",
                "M. R. J. Williams",
                "R. Williams",
                "F. F. Wilson",
                "W. Wislicki",
                "M. Witek",
                "L. Witola",
                "C. P. Wong",
                "G. Wormser",
                "S. A. Wotton",
                "H. Wu",
                "J. Wu",
                "Y. Wu",
                "K. Wyllie",
                "S. Xian",
                "Z. Xiang",
                "Y. Xie",
                "A. Xu",
                "J. Xu",
                "L. Xu",
                "L. Xu",
                "M. Xu",
                "Z. Xu",
                "Z. Xu",
                "Z. Xu",
                "D. Yang",
                "S. Yang",
                "X. Yang",
                "Y. Yang",
                "Z. Yang",
                "Z. Yang",
                "V. Yeroshenko",
                "H. Yeung",
                "H. Yin",
                "C. Y. Yu",
                "J. Yu",
                "X. Yuan",
                "E. Zaffaroni",
                "M. Zavertyaev",
                "M. Zdybal",
                "M. Zeng",
                "C. Zhang",
                "D. Zhang",
                "J. Zhang",
                "L. Zhang",
                "S. Zhang",
                "S. Zhang",
                "Y. Zhang",
                "Y. Z. Zhang",
                "Y. Zhao",
                "A. Zharkova",
                "A. Zhelezov",
                "X. Z. Zheng",
                "Y. Zheng",
                "T. Zhou",
                "X. Zhou",
                "Y. Zhou",
                "V. Zhovkovska",
                "L. Z. Zhu",
                "X. Zhu",
                "X. Zhu",
                "V. Zhukov",
                "J. Zhuo",
                "Q. Zou",
                "D. Zuliani",
                "G. Zunica"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01772v1",
                "http://arxiv.org/pdf/2312.01772v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01768v1",
            "title": "Localizing and Assessing Node Significance in Default Mode Network using\n  Sub-Community Detection in Mild Cognitive Impairment",
            "updated": "2023-12-04T09:43:05Z",
            "published": "2023-12-04T09:43:05Z",
            "summary": "Our study aims to utilize fMRI to identify the affected brain regions within\nthe Default Mode Network (DMN) in subjects with Mild Cognitive Impairment\n(MCI), using a novel Node Significance Score (NSS). We construct\nsubject-specific DMN graphs by employing partial correlation of Regions of\nInterest (ROIs) that make-up the DMN. For the DMN graph, ROIs are the nodes and\nedges are determined based on partial correlation. Four popular community\ndetection algorithms (Clique Percolation Method (CPM), Louvain algorithm,\nGreedy Modularity and Leading Eigenvectors) are applied to determine the\nlargest sub-community. NSS ratings are derived for each node, considering (I)\nfrequency in the largest sub-community within a class across all subjects and\n(II) occurrence in the largest sub-community according to all four methods.\nAfter computing the NSS of each ROI in both healthy and MCI subjects, we\nquantify the score disparity to identify nodes most impacted by MCI. The\nresults reveal a disparity exceeding 20% for 10 DMN nodes, maximally for PCC\nand Fusiform, showing 45.69% and 43.08% disparity. This aligns with existing\nmedical literature, additionally providing a quantitative measure that enables\nthe ordering of the affected ROIs. These findings offer valuable insights and\ncould lead to treatment strategies aggressively targeting the affected nodes.",
            "author": [
                "Ameiy Acharya",
                "Chakka Sai Pradeep",
                "Neelam Sinha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01768v1",
                "http://arxiv.org/pdf/2312.01768v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01739v1",
            "title": "Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network\n  Structure Learning",
            "updated": "2023-12-04T09:03:06Z",
            "published": "2023-12-04T09:03:06Z",
            "summary": "Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have\nbecome increasingly vital in representing complex stochastic processes in\nvarious domains such as gene expression analysis, healthcare, and traffic\nprediction. Structure learning of DBNs from data is challenging, particularly\nfor datasets with thousands of variables. Most current algorithms for DBN\nstructure learning are adaptations from those used in static Bayesian Networks\n(BNs), and are typically focused on small-scale problems. In order to solve\nlarge-scale problems while taking full advantage of existing algorithms, this\npaper introduces a novel divide-and-conquer strategy, originally developed for\nstatic BNs, and adapts it for large-scale DBN structure learning. In this work,\nwe specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a\nspecial class of DBNs. Furthermore, we leverage the prior knowledge of 2-TBNs\nto enhance the performance of the strategy we introduce. Our approach\nsignificantly improves the scalability and accuracy of 2-TBN structure\nlearning. Experimental results demonstrate the effectiveness of our method,\nshowing substantial improvements over existing algorithms in both computational\nefficiency and structure learning accuracy. On problem instances with more than\n1,000 variables, our approach improves two accuracy metrics by 74.45% and\n110.94% on average , respectively, while reducing runtime by 93.65% on average.",
            "author": [
                "Hui Ouyang",
                "Cheng Chen",
                "Ke Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01739v1",
                "http://arxiv.org/pdf/2312.01739v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01734v1",
            "title": "Effective Adapter for Face Recognition in the Wild",
            "updated": "2023-12-04T08:55:46Z",
            "published": "2023-12-04T08:55:46Z",
            "summary": "In this paper, we tackle the challenge of face recognition in the wild, where\nimages often suffer from low quality and real-world distortions. Traditional\nheuristic approaches-either training models directly on these degraded images\nor their enhanced counterparts using face restoration techniques-have proven\nineffective, primarily due to the degradation of facial features and the\ndiscrepancy in image domains. To overcome these issues, we propose an effective\nadapter for augmenting existing face recognition models trained on high-quality\nfacial datasets. The key of our adapter is to process both the unrefined and\nthe enhanced images by two similar structures where one is fixed and the other\ntrainable. Such design can confer two benefits. First, the dual-input system\nminimizes the domain gap while providing varied perspectives for the face\nrecognition model, where the enhanced image can be regarded as a complex\nnon-linear transformation of the original one by the restoration model. Second,\nboth two similar structures can be initialized by the pre-trained models\nwithout dropping the past knowledge. The extensive experiments in zero-shot\nsettings show the effectiveness of our method by surpassing baselines of about\n3%, 4%, and 7% in three datasets. Our code will be publicly available at\nhttps://github.com/liuyunhaozz/FaceAdapter/.",
            "author": [
                "Yunhao Liu",
                "Lu Qi",
                "Yu-Ju Tsai",
                "Xiangtai Li",
                "Kelvin C. K. Chan",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01734v1",
                "http://arxiv.org/pdf/2312.01734v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01729v1",
            "title": "EdgeConvFormer: Dynamic Graph CNN and Transformer based Anomaly\n  Detection in Multivariate Time Series",
            "updated": "2023-12-04T08:38:54Z",
            "published": "2023-12-04T08:38:54Z",
            "summary": "Transformer-based models for anomaly detection in multivariate time series\ncan benefit from the self-attention mechanism due to its advantage in modeling\nlong-term dependencies. However, Transformer-based anomaly detection models\nhave problems such as a large amount of data being required for training,\nstandard positional encoding is not suitable for multivariate time series data,\nand the interdependence between time series is not considered. To address these\nlimitations, we propose a novel anomaly detection method, named EdgeConvFormer,\nwhich integrates Time2vec embedding, stacked dynamic graph CNN, and Transformer\nto extract global and local spatial-time information. This design of\nEdgeConvFormer empowers it with decomposition capacities for complex time\nseries, progressive spatiotemporal correlation discovery between time series,\nand representation aggregation of multi-scale features. Experiments demonstrate\nthat EdgeConvFormer can learn the spatial-temporal correlations from\nmultivariate time series data and achieve better anomaly detection performance\nthan the state-of-the-art approaches on many real-world datasets of different\nscales.",
            "author": [
                "Jie Liu",
                "Qilin Li",
                "Senjian An",
                "Bradley Ezard",
                "Ling Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01729v1",
                "http://arxiv.org/pdf/2312.01729v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01728v1",
            "title": "ImputeFormer: Graph Transformers for Generalizable Spatiotemporal\n  Imputation",
            "updated": "2023-12-04T08:35:31Z",
            "published": "2023-12-04T08:35:31Z",
            "summary": "This paper focuses on the multivariate time series imputation problem using\ndeep neural architectures. The ubiquitous issue of missing data in both\nscientific and engineering tasks necessitates the development of an effective\nand general imputation model. Leveraging the wisdom and expertise garnered from\nlow-rank imputation methods, we power the canonical Transformers with three key\nknowledge-driven enhancements, including projected temporal attention, global\nadaptive graph convolution, and Fourier imputation loss. These task-agnostic\ninductive biases exploit the inherent structures of incomplete time series, and\nthus make our model versatile for a variety of imputation problems. We\ndemonstrate its superiority in terms of accuracy, efficiency, and flexibility\non heterogeneous datasets, including traffic speed, traffic volume, solar\nenergy, smart metering, and air quality. Comprehensive case studies are\nperformed to further strengthen the interpretability. Promising empirical\nresults provide strong conviction that incorporating time series primitives,\nsuch as low-rank properties, can substantially facilitate the development of a\ngeneralizable model to approach a wide range of spatiotemporal imputation\nproblems.",
            "author": [
                "Tong Nie",
                "Guoyang Qin",
                "Yuewen Mei",
                "Jian Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01728v1",
                "http://arxiv.org/pdf/2312.01728v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01726v1",
            "title": "Simultaneous Alignment and Surface Regression Using Hybrid 2D-3D\n  Networks for 3D Coherent Layer Segmentation of Retinal OCT Images with Full\n  and Sparse Annotations",
            "updated": "2023-12-04T08:32:31Z",
            "published": "2023-12-04T08:32:31Z",
            "summary": "Layer segmentation is important to quantitative analysis of retinal optical\ncoherence tomography (OCT). Recently, deep learning based methods have been\ndeveloped to automate this task and yield remarkable performance. However, due\nto the large spatial gap and potential mismatch between the B-scans of an OCT\nvolume, all of them were based on 2D segmentation of individual B-scans, which\nmay lose the continuity and diagnostic information of the retinal layers in 3D\nspace. Besides, most of these methods required dense annotation of the OCT\nvolumes, which is labor-intensive and expertise-demanding. This work presents a\nnovel framework based on hybrid 2D-3D convolutional neural networks (CNNs) to\nobtain continuous 3D retinal layer surfaces from OCT volumes, which works well\nwith both full and sparse annotations. The 2D features of individual B-scans\nare extracted by an encoder consisting of 2D convolutions. These 2D features\nare then used to produce the alignment displacement vectors and layer\nsegmentation by two 3D decoders coupled via a spatial transformer module. Two\nlosses are proposed to utilize the retinal layers' natural property of being\nsmooth for B-scan alignment and layer segmentation, respectively, and are the\nkey to the semi-supervised learning with sparse annotation. The entire\nframework is trained end-to-end. To the best of our knowledge, this is the\nfirst work that attempts 3D retinal layer segmentation in volumetric OCT images\nbased on CNNs. Experiments on a synthetic dataset and three public clinical\ndatasets show that our framework can effectively align the B-scans for\npotential motion correction, and achieves superior performance to\nstate-of-the-art 2D deep learning methods in terms of both layer segmentation\naccuracy and cross-B-scan 3D continuity in both fully and semi-supervised\nsettings, thus offering more clinical values than previous works.",
            "author": [
                "Hong Liu",
                "Dong Wei",
                "Donghuan Lu",
                "Xiaoying Tang",
                "Liansheng Wang",
                "Yefeng Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01726v1",
                "http://arxiv.org/pdf/2312.01726v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01725v1",
            "title": "StableVITON: Learning Semantic Correspondence with Latent Diffusion\n  Model for Virtual Try-On",
            "updated": "2023-12-04T08:27:59Z",
            "published": "2023-12-04T08:27:59Z",
            "summary": "Given a clothing image and a person image, an image-based virtual try-on aims\nto generate a customized image that appears natural and accurately reflects the\ncharacteristics of the clothing image. In this work, we aim to expand the\napplicability of the pre-trained diffusion model so that it can be utilized\nindependently for the virtual try-on task.The main challenge is to preserve the\nclothing details while effectively utilizing the robust generative capability\nof the pre-trained model. In order to tackle these issues, we propose\nStableVITON, learning the semantic correspondence between the clothing and the\nhuman body within the latent space of the pre-trained diffusion model in an\nend-to-end manner. Our proposed zero cross-attention blocks not only preserve\nthe clothing details by learning the semantic correspondence but also generate\nhigh-fidelity images by utilizing the inherent knowledge of the pre-trained\nmodel in the warping process. Through our proposed novel attention total\nvariation loss and applying augmentation, we achieve the sharp attention map,\nresulting in a more precise representation of clothing details. StableVITON\noutperforms the baselines in qualitative and quantitative evaluation, showing\npromising quality in arbitrary person images. Our code is available at\nhttps://github.com/rlawjdghek/StableVITON.",
            "author": [
                "Jeongho Kim",
                "Gyojung Gu",
                "Minho Park",
                "Sunghyun Park",
                "Jaegul Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01725v1",
                "http://arxiv.org/pdf/2312.01725v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03004v1",
            "title": "Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning",
            "updated": "2023-12-04T08:23:09Z",
            "published": "2023-12-04T08:23:09Z",
            "summary": "Temporal Knowledge Graph (TKG) reasoning that forecasts future events based\non historical snapshots distributed over timestamps is denoted as extrapolation\nand has gained significant attention. Owing to its extreme versatility and\nvariation in spatial and temporal correlations, TKG reasoning presents a\nchallenging task, demanding efficient capture of concurrent structures and\nevolutional interactions among facts. While existing methods have made strides\nin this direction, they still fall short of harnessing the diverse forms of\nintrinsic expressive semantics of TKGs, which encompass entity correlations\nacross multiple timestamps and periodicity of temporal information. This\nlimitation constrains their ability to thoroughly reflect historical\ndependencies and future trends. In response to these drawbacks, this paper\nproposes an innovative reasoning approach that focuses on Learning Multi-graph\nStructure (LMS). Concretely, it comprises three distinct modules concentrating\non multiple aspects of graph structure knowledge within TKGs, including\nconcurrent and evolutional patterns along timestamps, query-specific\ncorrelations across timestamps, and semantic dependencies of timestamps, which\ncapture TKG features from various perspectives. Besides, LMS incorporates an\nadaptive gate for merging entity representations both along and across\ntimestamps effectively. Moreover, it integrates timestamp semantics into graph\nattention calculations and time-aware decoders, in order to impose temporal\nconstraints on events and narrow down prediction scopes with historical\nstatistics. Extensive experimental results on five event-based benchmark\ndatasets demonstrate that LMS outperforms state-of-the-art extrapolation\nmodels, indicating the superiority of modeling a multi-graph perspective for\nTKG reasoning.",
            "author": [
                "Jinchuan Zhang",
                "Bei Hui",
                "Chong Mu",
                "Ling Tian"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03004v1",
                "http://arxiv.org/pdf/2312.03004v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01721v1",
            "title": "The Self-Loop Paradox: Investigating the Impact of Self-Loops on Graph\n  Neural Networks",
            "updated": "2023-12-04T08:23:00Z",
            "published": "2023-12-04T08:23:00Z",
            "summary": "Many Graph Neural Networks (GNNs) add self-loops to a graph to include\nfeature information about a node itself at each layer. However, if the GNN\nconsists of more than one layer, this information can return to its origin via\ncycles in the graph topology. Intuition suggests that this \"backflow\" of\ninformation should be larger in graphs with self-loops compared to graphs\nwithout. In this work, we counter this intuition and show that for certain GNN\narchitectures, the information a node gains from itself can be smaller in\ngraphs with self-loops compared to the same graphs without. We adopt an\nanalytical approach for the study of statistical graph ensembles with a given\ndegree sequence and show that this phenomenon, which we call the self-loop\nparadox, can depend both on the number of GNN layers $k$ and whether $k$ is\neven or odd. We experimentally validate our theoretical findings in a synthetic\nnode classification task and investigate its practical relevance in 23\nreal-world graphs.",
            "author": [
                "Moritz Lampert",
                "Ingo Scholtes"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01721v1",
                "http://arxiv.org/pdf/2312.01721v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02236v1",
            "title": "Rethinking Adversarial Training with Neural Tangent Kernel",
            "updated": "2023-12-04T08:06:59Z",
            "published": "2023-12-04T08:06:59Z",
            "summary": "Adversarial training (AT) is an important and attractive topic in deep\nlearning security, exhibiting mysteries and odd properties. Recent studies of\nneural network training dynamics based on Neural Tangent Kernel (NTK) make it\npossible to reacquaint AT and deeply analyze its properties. In this paper, we\nperform an in-depth investigation of AT process and properties with NTK, such\nas NTK evolution. We uncover three new findings that are missed in previous\nworks. First, we disclose the impact of data normalization on AT and the\nimportance of unbiased estimators in batch normalization layers. Second, we\nexperimentally explore the kernel dynamics and propose more time-saving AT\nmethods. Third, we study the spectrum feature inside the kernel to address the\ncatastrophic overfitting problem. To the best of our knowledge, it is the first\nwork leveraging the observations of kernel dynamics to improve existing AT\nmethods.",
            "author": [
                "Guanlin Li",
                "Han Qiu",
                "Shangwei Guo",
                "Jiwei Li",
                "Tianwei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02236v1",
                "http://arxiv.org/pdf/2312.02236v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01697v2",
            "title": "Hulk: A Universal Knowledge Translator for Human-Centric Tasks",
            "updated": "2023-12-05T05:37:25Z",
            "published": "2023-12-04T07:36:04Z",
            "summary": "Human-centric perception tasks, e.g., human mesh recovery, pedestrian\ndetection, skeleton-based action recognition, and pose estimation, have wide\nindustrial applications, such as metaverse and sports analysis. There is a\nrecent surge to develop human-centric foundation models that can benefit a\nbroad range of human-centric perception tasks. While many human-centric\nfoundation models have achieved success, most of them only excel in 2D vision\ntasks or require extensive fine-tuning for practical deployment in real-world\nscenarios. These limitations severely restrict their usability across various\ndownstream tasks and situations. To tackle these problems, we present Hulk, the\nfirst multimodal human-centric generalist model, capable of addressing most of\nthe mainstream tasks simultaneously without task-specific finetuning, covering\n2D vision, 3D vision, skeleton-based, and vision-language tasks. The key to\nachieving this is condensing various task-specific heads into two general\nheads, one for discrete representations, e.g., languages, and the other for\ncontinuous representations, e.g., location coordinates. The outputs of two\nheads can be further stacked into four distinct input and output modalities.\nThis uniform representation enables Hulk to treat human-centric tasks as\nmodality translation, integrating knowledge across a wide range of tasks. To\nvalidate the effectiveness of our proposed method, we conduct comprehensive\nexperiments on 11 benchmarks across 8 human-centric tasks. Experimental results\nsurpass previous methods substantially, demonstrating the superiority of our\nproposed method. The code will be available on\nhttps://github.com/OpenGVLab/HumanBench.",
            "author": [
                "Yizhou Wang",
                "Yixuan Wu",
                "Shixiang Tang",
                "Weizhen He",
                "Xun Guo",
                "Feng Zhu",
                "Lei Bai",
                "Rui Zhao",
                "Jian Wu",
                "Tong He",
                "Wanli Ouyang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01697v2",
                "http://arxiv.org/pdf/2312.01697v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01691v1",
            "title": "Estimating Coronal Mass Ejection Mass and Kinetic Energy by Fusion of\n  Multiple Deep-learning Models",
            "updated": "2023-12-04T07:25:55Z",
            "published": "2023-12-04T07:25:55Z",
            "summary": "Coronal mass ejections (CMEs) are massive solar eruptions, which have a\nsignificant impact on Earth. In this paper, we propose a new method, called\nDeepCME, to estimate two properties of CMEs, namely, CME mass and kinetic\nenergy. Being able to estimate these properties helps better understand CME\ndynamics. Our study is based on the CME catalog maintained at the Coordinated\nData Analysis Workshops (CDAW) Data Center, which contains all CMEs manually\nidentified since 1996 using the Large Angle and Spectrometric Coronagraph\n(LASCO) on board the Solar and Heliospheric Observatory (SOHO). We use LASCO C2\ndata in the period between January 1996 and December 2020 to train, validate\nand test DeepCME through 10-fold cross validation. The DeepCME method is a\nfusion of three deep learning models, including ResNet, InceptionNet, and\nInceptionResNet. Our fusion model extracts features from LASCO C2 images,\neffectively combining the learning capabilities of the three component models\nto jointly estimate the mass and kinetic energy of CMEs. Experimental results\nshow that the fusion model yields a mean relative error (MRE) of 0.013 (0.009,\nrespectively) compared to the MRE of 0.019 (0.017, respectively) of the best\ncomponent model InceptionResNet (InceptionNet, respectively) in estimating the\nCME mass (kinetic energy, respectively). To our knowledge, this is the first\ntime that deep learning has been used for CME mass and kinetic energy\nestimations.",
            "author": [
                "Khalid A. Alobaid",
                "Yasser Abduallah",
                "Jason T. L. Wang",
                "Haimin Wang",
                "Shen Fan",
                "Jialiang Li",
                "Huseyin Cavus",
                "Vasyl Yurchyshyn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01691v1",
                "http://arxiv.org/pdf/2312.01691v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "cs.LG",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01688v1",
            "title": "Tab-Attention: Self-Attention-based Stacked Generalization for\n  Imbalanced Credit Default Prediction",
            "updated": "2023-12-04T07:23:21Z",
            "published": "2023-12-04T07:23:21Z",
            "summary": "Accurately credit default prediction faces challenges due to imbalanced data\nand low correlation between features and labels. Existing default prediction\nstudies on the basis of gradient boosting decision trees (GBDT), deep learning\ntechniques, and feature selection strategies can have varying degrees of\nsuccess depending on the specific task. Motivated by this, we propose\nTab-Attention, a novel self-attention-based stacked generalization method for\ncredit default prediction. This approach ensembles the potential proprietary\nknowledge contributions from multi-view feature spaces, to cope with low\nfeature correlation and imbalance. We organize multi-view feature spaces\naccording to the latent linear or nonlinear strengths between features and\nlabels. Meanwhile, the f1 score assists the model in imbalance training to find\nthe optimal state for identifying minority default samples. Our Tab-Attention\nachieves superior Recall_1 and f1_1 of default intention recognition than\nexisting GBDT-based models and advanced deep learning by about 32.92% and\n16.05% on average, respectively, while maintaining outstanding overall\nperformance and prediction performance for non-default samples. The proposed\nmethod could ensemble essential knowledge through the self-attention mechanism,\nwhich is of great significance for a more robust future prediction system.",
            "author": [
                "Yandan Tan",
                "Hongbin Zhu",
                "JieWu",
                "Hongfeng Chai"
            ],
            "link": [
                "http://dx.doi.org/10.3233/FAIA230532",
                "http://arxiv.org/abs/2312.01688v1",
                "http://arxiv.org/pdf/2312.01688v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01681v1",
            "title": "Malicious Lateral Movement in 5G Core With Network Slicing And Its\n  Detection",
            "updated": "2023-12-04T07:09:33Z",
            "published": "2023-12-04T07:09:33Z",
            "summary": "5G networks are susceptible to cyber attacks due to reasons such as\nimplementation issues and vulnerabilities in 3GPP standard specifications. In\nthis work, we propose lateral movement strategies in a 5G Core (5GC) with\nnetwork slicing enabled, as part of a larger attack campaign by well-resourced\nadversaries such as APT groups. Further, we present 5GLatte, a system to detect\nsuch malicious lateral movement. 5GLatte operates on a host-container access\ngraph built using host/NF container logs collected from the 5GC. Paths inferred\nfrom the access graph are scored based on selected filtering criteria and\nsubsequently presented as input to a threshold-based anomaly detection\nalgorithm to reveal malicious lateral movement paths. We evaluate 5GLatte on a\ndataset containing attack campaigns (based on MITRE ATT&CK and FiGHT\nframeworks) launched in a 5G test environment which shows that compared to\nother lateral movement detectors based on state-of-the-art, it can achieve\nhigher true positive rates with similar false positive rates.",
            "author": [
                "Ayush Kumar",
                "Vrizlynn L. L. Thing"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01681v1",
                "http://arxiv.org/pdf/2312.01681v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01678v2",
            "title": "Jellyfish: A Large Language Model for Data Preprocessing",
            "updated": "2023-12-05T18:02:46Z",
            "published": "2023-12-04T07:01:54Z",
            "summary": "In this paper, we present Jellyfish, an open-source LLM as a universal task\nsolver for DP. Built on the Llama 2 13B model, Jellyfish is instruction-tuned\nwith the datasets of several typical DP tasks including error detection, data\nimputation, schema matching, and entity matching, and delivers generalizability\nto other tasks. Remarkably, Jellyfish can operate on a local, single, and\nlow-priced GPU with its 13 billion parameters, ensuring data security and\nenabling further tuning. Its proficiency in understanding natural language\nallows users to manually craft instructions for DP tasks. Unlike many existing\nmethods that heavily rely on prior knowledge, Jellyfish acquires domain\nknowledge during its tuning process and integrates optional knowledge injection\nduring inference. A distinctive feature of Jellyfish is its interpreter, which\nelucidates its output decisions. To construct Jellyfish, we develop a series of\npre-tuning and DP-tuning techniques. Jellyfish is equipped with an instance\nserializer, which automatically translates raw data into model prompts, and a\nknowledge injector, which optionally introduces task- and dataset-specific\nknowledge to enhance DP performance. Our evaluation of Jellyfish, using a range\nof real datasets, shows its competitiveness compared to state-of-the-art\nmethods and its strong generalizability to unseen tasks. Jellyfish's\nperformance rivals that of GPT series models, and its interpreter offers\nenhanced reasoning capabilities compared to GPT-3.5. Furthermore, our\nevaluation highlights the effectiveness of the techniques employed in\nconstructing Jellyfish. Our model is available at Hugging Face:\nhttps://huggingface.co/NECOUDBFM/Jellyfish .",
            "author": [
                "Haochen Zhang",
                "Yuyang Dong",
                "Chuan Xiao",
                "Masafumi Oyamada"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01678v2",
                "http://arxiv.org/pdf/2312.01678v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01676v1",
            "title": "Controllability of a second-order impulsive neutral differential\n  equation via resolvent operator technique",
            "updated": "2023-12-04T06:54:38Z",
            "published": "2023-12-04T06:54:38Z",
            "summary": "This paper uses the resolvent operator technique to investigate second-order\nnon-autonomous neutral integrodifferential equations with impulsive conditions\nin a Banach space. We study the existence of a mild solution and the system's\napproximate controllability. The semigroup and resolvent operator theory, graph\nnorm, and Krasnoselskii's fixed point theorem are used to demonstrate the\nresults. Finally, we present our findings with an example.",
            "author": [
                "Asma Afreen",
                "Abdur Raheem",
                "Areefa Khatoon"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01676v1",
                "http://arxiv.org/pdf/2312.01676v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02232v1",
            "title": "HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with\n  Diverse Poses",
            "updated": "2023-12-04T06:37:11Z",
            "published": "2023-12-04T06:37:11Z",
            "summary": "We present HumanNeRF-SE, which can synthesize diverse novel pose images with\nsimple input. Previous HumanNeRF studies require large neural networks to fit\nthe human appearance and prior knowledge. Subsequent methods build upon this\napproach with some improvements. Instead, we reconstruct this approach,\ncombining explicit and implicit human representations with both general and\nspecific mapping processes. Our key insight is that explicit shape can filter\nthe information used to fit implicit representation, and frozen general mapping\ncombined with point-specific mapping can effectively avoid overfitting and\nimprove pose generalization performance. Our explicit and implicit human\nrepresent combination architecture is extremely effective. This is reflected in\nour model's ability to synthesize images under arbitrary poses with few-shot\ninput and increase the speed of synthesizing images by 15 times through a\nreduction in computational complexity without using any existing acceleration\nmodules. Compared to the state-of-the-art HumanNeRF studies, HumanNeRF-SE\nachieves better performance with fewer learnable parameters and less training\ntime (see Figure 1).",
            "author": [
                "Caoyuan Ma",
                "Yu-Lun Liu",
                "Zhixiang Wang",
                "Wu Liu",
                "Xinchen Liu",
                "Zheng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02232v1",
                "http://arxiv.org/pdf/2312.02232v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01669v1",
            "title": "Analyze Drivers' Intervention Behavior During Autonomous Driving -- A\n  VR-incorporated Approach",
            "updated": "2023-12-04T06:36:57Z",
            "published": "2023-12-04T06:36:57Z",
            "summary": "Given the rapid advance in ITS technologies, future mobility is pointing to\nvehicular autonomy. However, there is still a long way before full automation,\nand human intervention is required. This work sheds light on understanding\nhuman drivers' intervention behavior involved in the operation of autonomous\nvehicles (AVs) and utilizes this knowledge to improve the perception of\ncritical driving scenarios. Experiment environments were implemented where the\nvirtual reality (VR) and traffic micro-simulation are integrated, and tests\nwere carried out under typical and diverse traffic scenes. Performance\nindicators such as the probability of intervention, accident rates are defined\nand used to quantify and compare the risk levels. By offering novel insights\ninto drivers' intervention behavior, this work will help improve the\nperformances of the automated control under similar scenarios. Furthermore,\nsuch an integrated and immersive tool for autonomous driving studies will be\nvaluable for research on human-to-automation trust. To the best knowledge of\nthe authors, this work is among the pioneer works making efforts into such\ntypes of tools.",
            "author": [
                "Zheng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01669v1",
                "http://arxiv.org/pdf/2312.01669v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01652v1",
            "title": "On the Expressive Power of Behavior Structure",
            "updated": "2023-12-04T06:11:21Z",
            "published": "2023-12-04T06:11:21Z",
            "summary": "Efforts toward a comprehensive description of behavior have indeed\nfacilitated the development of representation-based approaches that utilize\ndeep learning to capture behavioral information. As behavior complexity\nincreases, the expressive power of these models reaches a bottleneck. We coin\nthe term ``behavioral molecular structure\" and propose a new model called the\nBehavioral Molecular Structure (BMS). The model characterizes behaviors at the\natomic level, analogizes behavioral attributes to atoms, and concretizes\ninterrelations at the granularity of atoms using graphs. Here, we design three\ndifferent downstream tasks to test the performance of the BMS model on public\ndatasets. Additionally, we provide a preliminary theoretical analysis\ndemonstrating that the BMS model can offer effective expressiveness for complex\nbehaviors.",
            "author": [
                "Cheng Wang",
                "Hangyu Zhu",
                "Yuhang Lin",
                "Changjun Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01652v1",
                "http://arxiv.org/pdf/2312.01652v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01643v1",
            "title": "Enriching meta-analyses through scoping review, bibliometrics, and\n  alternative impact metrics: Visualizing study characteristics, hidden risk of\n  bias, societal influence, and research translation",
            "updated": "2023-12-04T05:50:48Z",
            "published": "2023-12-04T05:50:48Z",
            "summary": "We present a framework consisting of three approaches that can enhance\nmeta-analyses: 1) scoping reviews (evidence map), 2) bibliometrics, and 3)\nalternative impact metrics. These three \"enrichment\" approaches facilitate the\nresearch synthesis of both quantitative and qualitative evidence, along with\nacademic and non-academic influences. While the meta-analysis yields\nquantitative insights (e.g., overall estimates), the enrichment analyses\nprovide user-friendly summaries of qualitative information on the evidence\nbase. Scoping reviews can visualize study characteristics, unravelling\nknowledge gaps and methodological differences. Bibliometric analysis offers a\nvisual assessment of the non-independent evidence, such as hyper-dominant\nauthors and countries, and funding sources, potentially informing the risk of\nbias. Impact metric analysis employs alternative metrics to gauge societal\ninfluence and research translation (e.g., policy and patent citations) of\nstudies in the meta-analysis. To illustrate the application of this framework,\nwe provide sample visualizations and R code.",
            "author": [
                "Yefeng Yang",
                "Malgorzata Lagisz",
                "Shinichi Nakagawa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01643v1",
                "http://arxiv.org/pdf/2312.01643v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01639v1",
            "title": "On the Effectiveness of Large Language Models in Domain-Specific Code\n  Generation",
            "updated": "2023-12-04T05:41:02Z",
            "published": "2023-12-04T05:41:02Z",
            "summary": "Large language models (LLMs) such as ChatGPT have shown remarkable\ncapabilities in code generation. Despite their great success, their\neffectiveness within particular domains (e.g., web development) necessitates\nfurther evaluation. In this study, we conduct an empirical study of\ndomain-specific code generation with LLMs. We demonstrate that LLMs exhibit\nsub-optimal performance in generating domain-specific code, due to their\nlimited proficiency in utilizing domain-specific libraries. We further observe\nthat incorporating API knowledge as prompts can empower LLMs to generate more\nprofessional code. Based on these findings, we further investigate how to\nefficiently incorporate API knowledge into the code generation process. We\nexperiment with three strategies for incorporating domain knowledge, namely,\nexternal knowledge inquirer, chain-of-thought prompting, and chain-of-thought\nfine-tuning. We refer to these strategies as a new code generation approach\ncalled DomCoder. Experimental results show that all strategies of DomCoder lead\nto improvement in the effectiveness of domain-specific code generation under\ncertain settings. The results also show that there is still ample room for\nfurther improvement, based on which we suggest possible future works.",
            "author": [
                "Meng Chen",
                "Hongyu Zhang",
                "Chengcheng Wan",
                "Zhao Wei",
                "Yong Xu",
                "Juhong Wang",
                "Xiaodong Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01639v1",
                "http://arxiv.org/pdf/2312.01639v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01617v1",
            "title": "Heroes: Lightweight Federated Learning with Neural Composition and\n  Adaptive Local Update in Heterogeneous Edge Networks",
            "updated": "2023-12-04T04:14:50Z",
            "published": "2023-12-04T04:14:50Z",
            "summary": "Federated Learning (FL) enables distributed clients to collaboratively train\nmodels without exposing their private data. However, it is difficult to\nimplement efficient FL due to limited resources. Most existing works compress\nthe transmitted gradients or prune the global model to reduce the resource\ncost, but leave the compressed or pruned parameters under-optimized, which\ndegrades the training performance. To address this issue, the neural\ncomposition technique constructs size-adjustable models by composing low-rank\ntensors, allowing every parameter in the global model to learn the knowledge\nfrom all clients. Nevertheless, some tensors can only be optimized by a small\nfraction of clients, thus the global model may get insufficient training,\nleading to a long completion time, especially in heterogeneous edge scenarios.\nTo this end, we enhance the neural composition technique, enabling all\nparameters to be fully trained. Further, we propose a lightweight FL framework,\ncalled Heroes, with enhanced neural composition and adaptive local update. A\ngreedy-based algorithm is designed to adaptively assign the proper tensors and\nlocal update frequencies for participating clients according to their\nheterogeneous capabilities and resource budgets. Extensive experiments\ndemonstrate that Heroes can reduce traffic consumption by about 72.05\\% and\nprovide up to 2.97$\\times$ speedup compared to the baselines.",
            "author": [
                "Jiaming Yan",
                "Jianchun Liu",
                "Shilong Wang",
                "Hongli Xu",
                "Haifeng Liu",
                "Jianhua Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01617v1",
                "http://arxiv.org/pdf/2312.01617v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01613v1",
            "title": "The Local Cohomology Modules of the Binomial Edge Ideals of the\n  Complements of Connected Graphs of Girth at Least 5",
            "updated": "2023-12-04T04:08:42Z",
            "published": "2023-12-04T04:08:42Z",
            "summary": "We calculate the local cohomology modules of the binomial edge ideals of the\ncomplements of connected graphs of girth at least 5 using the poset and tools\nintroduced by \\`Alvarez Montaner in arXiv:1901.08645. We then compute the\nregularity of these binomial edge ideals.",
            "author": [
                "David Williams"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01613v1",
                "http://arxiv.org/pdf/2312.01613v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "13C70 (Primary) 13D45, 13F65 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01612v1",
            "title": "xNeuSM: Explainable Neural Subgraph Matching with Graph Learnable\n  Multi-hop Attention Networks",
            "updated": "2023-12-04T04:03:30Z",
            "published": "2023-12-04T04:03:30Z",
            "summary": "Subgraph matching is a challenging problem with a wide range of applications\nin database systems, biochemistry, and cognitive science. It involves\ndetermining whether a given query graph is present within a larger target\ngraph. Traditional graph-matching algorithms provide precise results but face\nchallenges in large graph instances due to the NP-complete problem, limiting\ntheir practical applicability. In contrast, recent neural network-based\napproximations offer more scalable solutions, but often lack interpretable node\ncorrespondences. To address these limitations, this article presents xNeuSM:\nExplainable Neural Subgraph Matching which introduces Graph Learnable Multi-hop\nAttention Networks (GLeMA) that adaptively learns the parameters governing the\nattention factor decay for each node across hops rather than relying on fixed\nhyperparameters. We provide a theoretical analysis establishing error bounds\nfor GLeMA's approximation of multi-hop attention as a function of the number of\nhops. Additionally, we prove that learning distinct attention decay factors for\neach node leads to a correct approximation of multi-hop attention. Empirical\nevaluation on real-world datasets shows that xNeuSM achieves substantial\nimprovements in prediction accuracy of up to 34% compared to approximate\nbaselines and, notably, at least a seven-fold faster query time than exact\nalgorithms. The source code of our implementation is available at\nhttps://github.com/martinakaduc/xNeuSM.",
            "author": [
                "Duc Q. Nguyen",
                "Thanh Toan Nguyen",
                "Tho quan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01612v1",
                "http://arxiv.org/pdf/2312.01612v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01610v1",
            "title": "Accelerated Parallel Magnetic Resonance Imaging with Compressed Sensing\n  using Structured Sparsity",
            "updated": "2023-12-04T04:01:13Z",
            "published": "2023-12-04T04:01:13Z",
            "summary": "Compressed sensing is an imaging paradigm that allows one to invert an\nunderdetermined linear system by imposing the a priori knowledge that the\nsought after solution is sparse (i.e., mostly zeros). Previous works have shown\nthat if one also knows something about the sparsity pattern (the locations\nwhere non-zero entries exist), one can take advantage of this structure to\nimprove the quality of the result. A significant application of compressed\nsensing is magnetic resonance imaging (MRI), where samples are acquired in the\nFourier domain. Compressed sensing allows one to reconstruct a high-quality\nimage with fewer samples which can be collected with a faster scan. This\nincreases the robustness of MRI to patient motion since less motion is possible\nduring the shorter scan. Parallel imaging, where multiple coils are used to\ngather data, is another an more ubiquitously used method for accelerating MRI.\nExisting combinations of these acceleration methods, such as Sparse SENSE,\nyield high quality images with an even shorter scan time than either technique\nalone. In this work, we show how to modify Sparse SENSE with structured\nsparsity to reconstruct a high quality image with even fewer samples.",
            "author": [
                "Nicholas Dwork",
                "Erin K. Englund"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01610v1",
                "http://arxiv.org/pdf/2312.01610v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02230v1",
            "title": "A Simple and Scalable Representation for Graph Generation",
            "updated": "2023-12-04T03:43:26Z",
            "published": "2023-12-04T03:43:26Z",
            "summary": "Recently, there has been a surge of interest in employing neural networks for\ngraph generation, a fundamental statistical learning problem with critical\napplications like molecule design and community analysis. However, most\napproaches encounter significant limitations when generating large-scale\ngraphs. This is due to their requirement to output the full adjacency matrices\nwhose size grows quadratically with the number of nodes. In response to this\nchallenge, we introduce a new, simple, and scalable graph representation named\ngap encoded edge list (GEEL) that has a small representation size that aligns\nwith the number of edges. In addition, GEEL significantly reduces the\nvocabulary size by incorporating the gap encoding and bandwidth restriction\nschemes. GEEL can be autoregressively generated with the incorporation of node\npositional encoding, and we further extend GEEL to deal with attributed graphs\nby designing a new grammar. Our findings reveal that the adoption of this\ncompact representation not only enhances scalability but also bolsters\nperformance by simplifying the graph generation process. We conduct a\ncomprehensive evaluation across ten non-attributed and two molecular graph\ngeneration tasks, demonstrating the effectiveness of GEEL.",
            "author": [
                "Yunhui Jang",
                "Seul Lee",
                "Sungsoo Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02230v1",
                "http://arxiv.org/pdf/2312.02230v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01607v1",
            "title": "Monte Carlo Experiments of Network Effects in Randomized Controlled\n  Trials",
            "updated": "2023-12-04T03:41:17Z",
            "published": "2023-12-04T03:41:17Z",
            "summary": "I run Monte Carlo simulations of content production over random\nWatts-Strogatz graphs to show various effects relevant to modeling and\nunderstanding Randomized Controlled Trials on social networks: the network\neffect, spillover effect, experiment dampening effect, intrinsic dampening\neffect, clustering effect, degree distribution effect and the experiment size\neffect. I will also define some simple metrics to measure their strength. When\nrunning experiments these potentially unexpected effects must be understood and\ncontrolled for in some manner, such as modeling the underlying graph structure\nto establish a baseline.",
            "author": [
                "M\u00e1rton Trencs\u00e9ni"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01607v1",
                "http://arxiv.org/pdf/2312.01607v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01601v1",
            "title": "Local-Global History-aware Contrastive Learning for Temporal Knowledge\n  Graph Reasoning",
            "updated": "2023-12-04T03:27:01Z",
            "published": "2023-12-04T03:27:01Z",
            "summary": "Temporal knowledge graphs (TKGs) have been identified as a promising approach\nto represent the dynamics of facts along the timeline. The extrapolation of TKG\nis to predict unknowable facts happening in the future, holding significant\npractical value across diverse fields. Most extrapolation studies in TKGs focus\non modeling global historical fact repeating and cyclic patterns, as well as\nlocal historical adjacent fact evolution patterns, showing promising\nperformance in predicting future unknown facts. Yet, existing methods still\nface two major challenges: (1) They usually neglect the importance of\nhistorical information in KG snapshots related to the queries when encoding the\nlocal and global historical information; (2) They exhibit weak anti-noise\ncapabilities, which hinders their performance when the inputs are contaminated\nwith noise.To this end, we propose a novel \\blue{Lo}cal-\\blue{g}lobal\nhistory-aware \\blue{C}ontrastive \\blue{L}earning model (\\blue{LogCL}) for TKG\nreasoning, which adopts contrastive learning to better guide the fusion of\nlocal and global historical information and enhance the ability to resist\ninterference. Specifically, for the first challenge, LogCL proposes an\nentity-aware attention mechanism applied to the local and global historical\nfacts encoder, which captures the key historical information related to\nqueries. For the latter issue, LogCL designs four historical query contrast\npatterns, effectively improving the robustness of the model. The experimental\nresults on four benchmark datasets demonstrate that LogCL delivers better and\nmore robust performance than the state-of-the-art baselines.",
            "author": [
                "Wei Chen",
                "Huaiyu Wan",
                "Yuting Wu",
                "Shuyuan Zhao",
                "Jiayaqi Cheng",
                "Yuxin Li",
                "Youfang Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01601v1",
                "http://arxiv.org/pdf/2312.01601v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01585v1",
            "title": "OCGEC: One-class Graph Embedding Classification for DNN Backdoor\n  Detection",
            "updated": "2023-12-04T02:48:40Z",
            "published": "2023-12-04T02:48:40Z",
            "summary": "Deep neural networks (DNNs) have been found vulnerable to backdoor attacks,\nraising security concerns about their deployment in mission-critical\napplications. There are various approaches to detect backdoor attacks, however\nthey all make certain assumptions about the target attack to be detected and\nrequire equal and huge numbers of clean and backdoor samples for training,\nwhich renders these detection methods quite limiting in real-world\ncircumstances.\n  This study proposes a novel one-class classification framework called\nOne-class Graph Embedding Classification (OCGEC) that uses GNNs for model-level\nbackdoor detection with only a little amount of clean data. First, we train\nthousands of tiny models as raw datasets from a small number of clean datasets.\nFollowing that, we design a ingenious model-to-graph method for converting the\nmodel's structural details and weight features into graph data. We then\npre-train a generative self-supervised graph autoencoder (GAE) to better learn\nthe features of benign models in order to detect backdoor models without\nknowing the attack strategy. After that, we dynamically combine the GAE and\none-class classifier optimization goals to form classification boundaries that\ndistinguish backdoor models from benign models.\n  Our OCGEC combines the powerful representation capabilities of graph neural\nnetworks with the utility of one-class classification techniques in the field\nof anomaly detection. In comparison to other baselines, it achieves AUC scores\nof more than 98% on a number of tasks, which far exceeds existing methods for\ndetection even when they rely on a huge number of positive and negative\nsamples. Our pioneering application of graphic scenarios for generic backdoor\ndetection can provide new insights that can be used to improve other backdoor\ndefense tasks. Code is available at https://github.com/jhy549/OCGEC.",
            "author": [
                "Haoyu Jiang",
                "Haiyang Yu",
                "Nan Li",
                "Ping Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01585v1",
                "http://arxiv.org/pdf/2312.01585v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02226v1",
            "title": "Generating Action-conditioned Prompts for Open-vocabulary Video Action\n  Recognition",
            "updated": "2023-12-04T02:31:38Z",
            "published": "2023-12-04T02:31:38Z",
            "summary": "Exploring open-vocabulary video action recognition is a promising venture,\nwhich aims to recognize previously unseen actions within any arbitrary set of\ncategories. Existing methods typically adapt pretrained image-text models to\nthe video domain, capitalizing on their inherent strengths in generalization. A\ncommon thread among such methods is the augmentation of visual embeddings with\ntemporal information to improve the recognition of seen actions. Yet, they\ncompromise with standard less-informative action descriptions, thus faltering\nwhen confronted with novel actions. Drawing inspiration from human cognitive\nprocesses, we argue that augmenting text embeddings with human prior knowledge\nis pivotal for open-vocabulary video action recognition. To realize this, we\ninnovatively blend video models with Large Language Models (LLMs) to devise\nAction-conditioned Prompts. Specifically, we harness the knowledge in LLMs to\nproduce a set of descriptive sentences that contain distinctive features for\nidentifying given actions. Building upon this foundation, we further introduce\na multi-modal action knowledge alignment mechanism to align concepts in video\nand textual knowledge encapsulated within the prompts. Extensive experiments on\nvarious video benchmarks, including zero-shot, few-shot, and base-to-novel\ngeneralization settings, demonstrate that our method not only sets new SOTA\nperformance but also possesses excellent interpretability.",
            "author": [
                "Chengyou Jia",
                "Minnan Luo",
                "Xiaojun Chang",
                "Zhuohang Dang",
                "Mingfei Han",
                "Mengmeng Wang",
                "Guang Dai",
                "Sizhe Dang",
                "Jingdong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02226v1",
                "http://arxiv.org/pdf/2312.02226v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01560v1",
            "title": "RaftGP: Random Fast Graph Partitioning",
            "updated": "2023-12-04T01:16:14Z",
            "published": "2023-12-04T01:16:14Z",
            "summary": "Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides the node set of a graph into densely-connected blocks. Following\nprior work on the IEEE HPEC Graph Challenge benchmark and recent advances in\ngraph machine learning, we propose a novel RAndom FasT Graph Partitioning\n(RaftGP) method based on an efficient graph embedding scheme. It uses the\nGaussian random projection to extract community-preserving features from\nclassic GP objectives. These features are fed into a graph neural network (GNN)\nto derive low-dimensional node embeddings. Surprisingly, our experiments\ndemonstrate that a randomly initialized GNN even without training is enough for\nRaftGP to derive informative community-preserving embeddings and support\nhigh-quality GP. To enable the derived embeddings to tackle GP, we introduce a\nhierarchical model selection algorithm that simultaneously determines the\nnumber of blocks and the corresponding GP result. We evaluate RaftGP on the\nGraph Challenge benchmark and compare the performance with five baselines,\nwhere our method can achieve a better trade-off between quality and efficiency.\nIn particular, compared to the baseline algorithm of the IEEE HPEC Graph\nChallenge, our method is 6.68x -- 23.9x faster on graphs with 1E3 -- 5E4 nodes\nand at least 64.5x faster on larger (1E5 node) graphs on which the baseline\ntakes more than 1E4 seconds. Our method achieves better accuracy on all test\ncases. We also develop a new graph generator to address some limitations of the\noriginal generator in the benchmark.",
            "author": [
                "Yu Gao",
                "Meng Qin",
                "Yibin Ding",
                "Li Zeng",
                "Chaorui Zhang",
                "Weixi Zhang",
                "Wei Han",
                "Rongqian Zhao",
                "Bo Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01560v1",
                "http://arxiv.org/pdf/2312.01560v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01559v1",
            "title": "Axisymmetric Virtual Elements For Problems of Elasticity and Plasticity",
            "updated": "2023-12-04T01:12:31Z",
            "published": "2023-12-04T01:12:31Z",
            "summary": "The virtual element method (VEM) allows discretization of elasticity and\nplasticity problems with polygons in 2D and polyhedrals in 3D. The polygons\n(and polyhedrals) can have an arbitrary number of sides and can be concave or\nconvex. These features, among others, are attractive for meshing complex\ngeometries. However, to the author's knowledge axisymmetric virtual elements\nhave not appeared before in the literature. Hence, in this work a novel first\norder consistent axisymmetric virtual element method is applied to problems of\nelasticity and plasticity. The VEM specific implementation details and\nadjustments needed to solve axisymmetric simulations are presented.\nRepresentative benchmark problems including pressure vessels and circular\nplates are illustrated. Examples also show that problems of near\nincompressibility are solved successfully. Consequently, this research\ndemonstrates that the axisymmetric VEM formulation successfully solves certain\nclasses of solid mechanics problems. The work concludes with a discussion of\nresults for the current formulation and future research directions.",
            "author": [
                "Louie L. Yaw"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01559v1",
                "http://arxiv.org/pdf/2312.01559v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02225v1",
            "title": "Digital Histopathology with Graph Neural Networks: Concepts and\n  Explanations for Clinicians",
            "updated": "2023-12-04T00:20:50Z",
            "published": "2023-12-04T00:20:50Z",
            "summary": "To address the challenge of the ``black-box\" nature of deep learning in\nmedical settings, we combine GCExplainer - an automated concept discovery\nsolution - along with Logic Explained Networks to provide global explanations\nfor Graph Neural Networks. We demonstrate this using a generally applicable\ngraph construction and classification pipeline, involving panoptic segmentation\nwith HoVer-Net and cancer prediction with Graph Convolution Networks. By\ntraining on H&E slides of breast cancer, we show promising results in offering\nexplainable and trustworthy AI tools for clinicians.",
            "author": [
                "Alessandro Farace di Villaforesta",
                "Lucie Charlotte Magister",
                "Pietro Barbiero",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02225v1",
                "http://arxiv.org/pdf/2312.02225v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01544v1",
            "title": "KEEC: Embed to Control on An Equivariant Geometry",
            "updated": "2023-12-04T00:11:27Z",
            "published": "2023-12-04T00:11:27Z",
            "summary": "This paper investigates how representation learning can enable optimal\ncontrol in unknown and complex dynamics, such as chaotic and non-linear\nsystems, without relying on prior domain knowledge of the dynamics. The core\nidea is to establish an equivariant geometry that is diffeomorphic to the\nmanifold defined by a dynamical system and to perform optimal control within\nthis corresponding geometry, which is a non-trivial task. To address this\nchallenge, Koopman Embed to Equivariant Control (KEEC) is introduced for model\nlearning and control. Inspired by Lie theory, KEEC begins by learning a\nnon-linear dynamical system defined on a manifold and embedding trajectories\ninto a Lie group. Subsequently, KEEC formulates an equivariant value function\nequation in reinforcement learning on the equivariant geometry, ensuring an\ninvariant effect as the value function on the original manifold. By deriving\nanalytical-form optimal actions on the equivariant value function, KEEC\ntheoretically achieves quadratic convergence for the optimal equivariant value\nfunction by leveraging the differential information on the equivariant\ngeometry. The effectiveness of KEEC is demonstrated in challenging dynamical\nsystems, including chaotic ones like Lorenz-63. Notably, our findings indicate\nthat isometric and isomorphic loss functions, ensuring the compactness and\nsmoothness of geometry, outperform loss functions without these properties.",
            "author": [
                "Xiaoyuan Cheng",
                "Yiming Yang",
                "Wei Jiang",
                "Yukun Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01544v1",
                "http://arxiv.org/pdf/2312.01544v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01539v1",
            "title": "Combinatorics of $(m,n)$-Word Lattices",
            "updated": "2023-12-03T23:36:36Z",
            "published": "2023-12-03T23:36:36Z",
            "summary": "We study the $(m,n)$-word lattices recently introduced by V. Pilaud and D.\nPoliakova in their study of generalized Hochschild polytopes. We prove that\nthese lattices are extremal and constructable by interval doublings. Moreover,\nwe describe further combinatorial properties of these lattices, such as their\ncardinality, their canonical join representations and their Galois graphs.",
            "author": [
                "Henri M\u00fchle"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01539v1",
                "http://arxiv.org/pdf/2312.01539v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "06D75, 05E99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01538v1",
            "title": "Recurrent Distance-Encoding Neural Networks for Graph Representation\n  Learning",
            "updated": "2023-12-03T23:36:16Z",
            "published": "2023-12-03T23:36:16Z",
            "summary": "Graph neural networks based on iterative one-hop message passing have been\nshown to struggle in harnessing information from distant nodes effectively.\nConversely, graph transformers allow each node to attend to all other nodes\ndirectly, but suffer from high computational complexity and have to rely on\nad-hoc positional encoding to bake in the graph inductive bias. In this paper,\nwe propose a new architecture to reconcile these challenges. Our approach stems\nfrom the recent breakthroughs in long-range modeling provided by deep\nstate-space models on sequential data: for a given target node, our model\naggregates other nodes by their shortest distances to the target and uses a\nparallelizable linear recurrent network over the chain of distances to provide\na natural encoding of its neighborhood structure. With no need for positional\nencoding, we empirically show that the performance of our model is highly\ncompetitive compared with that of state-of-the-art graph transformers on\nvarious benchmarks, at a drastically reduced computational complexity. In\naddition, we show that our model is theoretically more expressive than one-hop\nmessage passing neural networks.",
            "author": [
                "Yuhui Ding",
                "Antonio Orvieto",
                "Bobby He",
                "Thomas Hofmann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01538v1",
                "http://arxiv.org/pdf/2312.01538v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01537v1",
            "title": "Unlocking the Potential of Federated Learning: The Symphony of Dataset\n  Distillation via Deep Generative Latents",
            "updated": "2023-12-03T23:30:48Z",
            "published": "2023-12-03T23:30:48Z",
            "summary": "Data heterogeneity presents significant challenges for federated learning\n(FL). Recently, dataset distillation techniques have been introduced, and\nperformed at the client level, to attempt to mitigate some of these challenges.\nIn this paper, we propose a highly efficient FL dataset distillation framework\non the server side, significantly reducing both the computational and\ncommunication demands on local devices while enhancing the clients' privacy.\nUnlike previous strategies that perform dataset distillation on local devices\nand upload synthetic data to the server, our technique enables the server to\nleverage prior knowledge from pre-trained deep generative models to synthesize\nessential data representations from a heterogeneous model architecture. This\nprocess allows local devices to train smaller surrogate models while enabling\nthe training of a larger global model on the server, effectively minimizing\nresource utilization. We substantiate our claim with a theoretical analysis,\ndemonstrating the asymptotic resemblance of the process to the hypothetical\nideal of completely centralized training on a heterogeneous dataset. Empirical\nevidence from our comprehensive experiments indicates our method's superiority,\ndelivering an accuracy enhancement of up to 40% over non-dataset-distillation\ntechniques in highly heterogeneous FL contexts, and surpassing existing\ndataset-distillation methods by 18%. In addition to the high accuracy, our\nframework converges faster than the baselines because rather than the server\ntrains on several sets of heterogeneous data distributions, it trains on a\nmulti-modal distribution. Our code is available at\nhttps://github.com/FedDG23/FedDG-main.git",
            "author": [
                "Yuqi Jia",
                "Saeed Vahidian",
                "Jingwei Sun",
                "Jianyi Zhang",
                "Vyacheslav Kungurtsev",
                "Neil Zhenqiang Gong",
                "Yiran Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01537v1",
                "http://arxiv.org/pdf/2312.01537v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01529v2",
            "title": "T3D: Towards 3D Medical Image Understanding through Vision-Language\n  Pre-training",
            "updated": "2023-12-05T09:01:07Z",
            "published": "2023-12-03T23:03:22Z",
            "summary": "Expert annotation of 3D medical image for downstream analysis is\nresource-intensive, posing challenges in clinical applications. Visual\nself-supervised learning (vSSL), though effective for learning visual\ninvariance, neglects the incorporation of domain knowledge from medicine. To\nincorporate medical knowledge into visual representation learning,\nvision-language pre-training (VLP) has shown promising results in 2D image.\nHowever, existing VLP approaches become generally impractical when applied to\nhigh-resolution 3D medical images due to GPU hardware constraints and the\npotential loss of critical details caused by downsampling, which is the\nintuitive solution to hardware constraints. To address the above limitations,\nwe introduce T3D, the first VLP framework designed for high-resolution 3D\nmedical images. T3D incorporates two text-informed pretext tasks:\n(\\lowerromannumeral{1}) text-informed contrastive learning;\n(\\lowerromannumeral{2}) text-informed image restoration. These tasks focus on\nlearning 3D visual representations from high-resolution 3D medical images and\nintegrating clinical knowledge from radiology reports, without distorting\ninformation through forced alignment of downsampled volumes with detailed\nanatomical text. Trained on a newly curated large-scale dataset of 3D medical\nimages and radiology reports, T3D significantly outperforms current vSSL\nmethods in tasks like organ and tumor segmentation, as well as disease\nclassification. This underlines T3D's potential in representation learning for\n3D medical image analysis. All data and code will be available upon acceptance.",
            "author": [
                "Che Liu",
                "Cheng Ouyang",
                "Yinda Chen",
                "Cesar C\u00e9sar Quilodr\u00e1n-Casas",
                "Lei Ma",
                "Jie Fu",
                "Yike Guo",
                "Anand Shah",
                "Wenjia Bai",
                "Rossella Arcucci"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01529v2",
                "http://arxiv.org/pdf/2312.01529v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01524v1",
            "title": "Code Swarm: A Code Generation Tool Based on the Automatic Derivation of\n  Transformation Rule Set",
            "updated": "2023-12-03T22:47:42Z",
            "published": "2023-12-03T22:47:42Z",
            "summary": "Automatic generation of software code from system design models remains an\nactively explored research area for the past several years. A number of tools\nare currently available to facilitate and automate the task of generating code\nfrom software models. To the best of our knowledge, existing software tools\nrely on an explicitly defined transformation rule set to perform the\nmodel-to-code transformation process. In this paper, we introduce a novel tool\nnamed Code Swarm, abbreviated as CodS, that automatically generates\nimplementation code from system design models by utilizing a swarm-based\napproach. Specifically, CodS is capable of generating Java code from the class\nand state models of the software system by making use of the previously solved\nmodel-to-code transformation examples. Our tool enables the designers to\nspecify behavioural actions in the input models using the Action Specification\nLanguage (ASL). We use an industrial case study of the Elevator Control System\n(ECS) to perform the experimental validation of our tool. Our results indicate\nthat the code generated by CodS is correct and consistent with the input design\nmodels. CodS performs the process of automatic code generation without taking\nthe explicit transformation rule set or languages metamodels information as\ninput, which distinguishes it from all the existing automatic code generation\ntools.",
            "author": [
                "Hina Mahmood",
                "Atif Aftab Jilani",
                "Abdul Rauf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01524v1",
                "http://arxiv.org/pdf/2312.01524v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01516v1",
            "title": "Quantum properties of $\\mathcal F$-cographs",
            "updated": "2023-12-03T22:12:46Z",
            "published": "2023-12-03T22:12:46Z",
            "summary": "We initiate a systematic study of quantum properties of finite graphs,\nnamely, quantum asymmetry, quantum symmetry, and quantum isomorphism. We define\nthe Schmidt alternative for a family of graphs, which reveals to be a useful\ntool for computing quantum automorphism groups of graphs, as well as the notion\nof tractable families. After showing that quantum isomorphic graphs have\nquantum isomorphic connected components, we show that the families of cographs\nand forests are tractable, thus completely solving the aforementioned problems\nfor these families. Using general results on tractable families, we compute for\nthe first time the quantum automorphism groups of cographs, forests, and\ntree-cographs, a strictly proper superclass of cographs and forests, as well as\n$\\mathcal G_5$-cographs, a proper superclass of cographs distinct from\ntree-cographs. Doing so, we extend to the noncommutative setting a theorem of\nJordan.",
            "author": [
                "Paul Meunier"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01516v1",
                "http://arxiv.org/pdf/2312.01516v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "math.CO",
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02224v1",
            "title": "Tracing Hyperparameter Dependencies for Model Parsing via Learnable\n  Graph Pooling Network",
            "updated": "2023-12-03T22:05:05Z",
            "published": "2023-12-03T22:05:05Z",
            "summary": "Model Parsing defines the research task of predicting hyperparameters of the\ngenerative model (GM), given a generated image as input. Since a diverse set of\nhyperparameters is jointly employed by the generative model, and dependencies\noften exist among them, it is crucial to learn these hyperparameter\ndependencies for the improved model parsing performance. To explore such\nimportant dependencies, we propose a novel model parsing method called\nLearnable Graph Pooling Network (LGPN). Specifically, we transform model\nparsing into a graph node classification task, using graph nodes and edges to\nrepresent hyperparameters and their dependencies, respectively. Furthermore,\nLGPN incorporates a learnable pooling-unpooling mechanism tailored to model\nparsing, which adaptively learns hyperparameter dependencies of GMs used to\ngenerate the input image. We also extend our proposed method to CNN-generated\nimage detection and coordinate attacks detection. Empirically, we achieve\nstate-of-the-art results in model parsing and its extended applications,\nshowing the effectiveness of our method. Our source code are available.",
            "author": [
                "Xiao Guo",
                "Vishal Asnani",
                "Sijia Liu",
                "Xiaoming Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02224v1",
                "http://arxiv.org/pdf/2312.02224v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01502v1",
            "title": "Normed Spaces for Graph Embedding",
            "updated": "2023-12-03T20:21:08Z",
            "published": "2023-12-03T20:21:08Z",
            "summary": "Theoretical results from discrete geometry suggest that normed spaces can\nabstractly embed finite metric spaces with surprisingly low theoretical bounds\non distortion in low dimensions. In this paper, inspired by this theoretical\ninsight, we highlight normed spaces as a more flexible and computationally\nefficient alternative to several popular Riemannian manifolds for learning\ngraph embeddings. Normed space embeddings significantly outperform several\npopular manifolds on a large range of synthetic and real-world graph\nreconstruction benchmark datasets while requiring significantly fewer\ncomputational resources. We also empirically verify the superiority of normed\nspace embeddings on growing families of graphs associated with negative, zero,\nand positive curvature, further reinforcing the flexibility of normed spaces in\ncapturing diverse graph structures as graph sizes increase. Lastly, we\ndemonstrate the utility of normed space embeddings on two applied graph\nembedding tasks, namely, link prediction and recommender systems. Our work\nhighlights the potential of normed spaces for geometric graph representation\nlearning, raises new research questions, and offers a valuable tool for\nexperimental mathematics in the field of finite metric space embeddings. We\nmake our code and data publically available.",
            "author": [
                "Diaaeldin Taha",
                "Wei Zhao",
                "J. Maxwell Riestenberg",
                "Michael Strube"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01502v1",
                "http://arxiv.org/pdf/2312.01502v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01498v1",
            "title": "Learning Neural Traffic Rules",
            "updated": "2023-12-03T20:06:43Z",
            "published": "2023-12-03T20:06:43Z",
            "summary": "Extensive research has been devoted to the field of multi-agent navigation.\nRecently, there has been remarkable progress attributed to the emergence of\nlearning-based techniques with substantially elevated intelligence and realism.\nNonetheless, prevailing learned models face limitations in terms of scalability\nand effectiveness, primarily due to their agent-centric nature, i.e., the\nlearned neural policy is individually deployed on each agent. Inspired by the\nefficiency observed in real-world traffic networks, we present an\nenvironment-centric navigation policy. Our method learns a set of traffic rules\nto coordinate a vast group of unintelligent agents that possess only basic\ncollision-avoidance capabilities. Our method segments the environment into\ndistinct blocks and parameterizes the traffic rule using a Graph Recurrent\nNeural Network (GRNN) over the block network. Each GRNN node is trained to\nmodulate the velocities of agents as they traverse through. Using either\nImitation Learning (IL) or Reinforcement Learning (RL) schemes, we demonstrate\nthe efficacy of our neural traffic rules in resolving agent congestion, closely\nresembling real-world traffic regulations. Our method handles up to $240$\nagents at real-time and generalizes across diverse agent and environment\nconfigurations.",
            "author": [
                "Xuan Zhang",
                "Xifeng Gao",
                "Kui Wu",
                "Zherong Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01498v1",
                "http://arxiv.org/pdf/2312.01498v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01467v1",
            "title": "Online Dominating Set and Coloring for Geometric Intersection Graphs",
            "updated": "2023-12-03T17:41:47Z",
            "published": "2023-12-03T17:41:47Z",
            "summary": "We present online deterministic algorithms for minimum coloring and minimum\ndominating set problems in the context of geometric intersection graphs. We\nconsider a graph parameter: the independent kissing number $\\zeta$, which is a\nnumber equal to `the size of the largest induced star in the graph $-1$'. For a\ngraph with an independent kissing number at most $\\zeta$, we show that the\nfamous greedy algorithm achieves an optimal competitive ratio of $\\zeta$ for\nthe minimum dominating set and the minimum independent dominating set problems.\nHowever, for the minimum connected dominating set problem, we obtain a\ncompetitive ratio of at most $2\\zeta$. To complement this, we prove that for\nthe minimum connected dominating set problem, any deterministic online\nalgorithm has a competitive ratio of at least $2(\\zeta-1)$ for the geometric\nintersection graph of translates of a convex object in $\\mathbb{R}^2$. Next,\nfor the minimum coloring problem, we obtain algorithms having a competitive\nratio of $O\\left({\\zeta'}{\\log m}\\right)$ for geometric intersection graphs of\nbounded scaled $\\alpha$-fat objects in $\\mathbb{R}^d$ having widths in the\ninterval $[1,m]$, where $\\zeta'$ is the independent kissing number of the\ngeometric intersection graph of bounded scaled $\\alpha$-fat objects having\nwidths in the interval $[1,2]$. Finally, we investigate the value of $\\zeta$\nfor geometric intersection graphs of various families of geometric objects.",
            "author": [
                "Minati De",
                "Sambhav Khurana",
                "Satyam Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01467v1",
                "http://arxiv.org/pdf/2312.01467v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01466v1",
            "title": "Indivisibility for Classes of Graphs",
            "updated": "2023-12-03T17:28:11Z",
            "published": "2023-12-03T17:28:11Z",
            "summary": "We examine indivisibility for classes of graphs. We show that the class of\nhereditarily $\\alpha$-sparse graphs is indivisible if and only if $\\alpha > 2$.\nAdditionally, we show that the following classes of graphs are indivisible:\nperfect graphs, cographs, and chordal graphs, and the following classes of\ngraphs are not indivisible: threshold graphs, split graphs, and\ndistance-hereditary graphs.",
            "author": [
                "Vince Guingona",
                "Felix Nusbaum",
                "Zain Padamsee",
                "Miriam Parnes",
                "Christian Pippin",
                "Ava Zinman"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01466v1",
                "http://arxiv.org/pdf/2312.01466v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.LO",
                "03C98, 05C15, 05C55"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01456v1",
            "title": "Compositional Policy Learning in Stochastic Control Systems with Formal\n  Guarantees",
            "updated": "2023-12-03T17:04:18Z",
            "published": "2023-12-03T17:04:18Z",
            "summary": "Reinforcement learning has shown promising results in learning neural network\npolicies for complicated control tasks. However, the lack of formal guarantees\nabout the behavior of such policies remains an impediment to their deployment.\nWe propose a novel method for learning a composition of neural network policies\nin stochastic environments, along with a formal certificate which guarantees\nthat a specification over the policy's behavior is satisfied with the desired\nprobability. Unlike prior work on verifiable RL, our approach leverages the\ncompositional nature of logical specifications provided in SpectRL, to learn\nover graphs of probabilistic reach-avoid specifications. The formal guarantees\nare provided by learning neural network policies together with reach-avoid\nsupermartingales (RASM) for the graph's sub-tasks and then composing them into\na global policy. We also derive a tighter lower bound compared to previous work\non the probability of reach-avoidance implied by a RASM, which is required to\nfind a compositional policy with an acceptable probabilistic threshold for\ncomplex tasks with multiple edge policies. We implement a prototype of our\napproach and evaluate it on a Stochastic Nine Rooms environment.",
            "author": [
                "\u0110or\u0111e \u017dikeli\u0107",
                "Mathias Lechner",
                "Abhinav Verma",
                "Krishnendu Chatterjee",
                "Thomas A. Henzinger"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01456v1",
                "http://arxiv.org/pdf/2312.01456v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01454v2",
            "title": "D-Bot: Database Diagnosis System using Large Language Models",
            "updated": "2023-12-06T02:53:11Z",
            "published": "2023-12-03T16:58:10Z",
            "summary": "Database administrators (DBAs) play an important role in managing,\nmaintaining and optimizing database systems. However, it is hard and tedious\nfor DBAs to manage a large number of databases and give timely response\n(waiting for hours is intolerable in many online cases). In addition, existing\nempirical methods only support limited diagnosis scenarios, which are also\nlabor-intensive to update the diagnosis rules for database version updates.\nRecently large language models (LLMs) have shown great potential in various\nfields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can\nautomatically acquire knowledge from diagnosis documents, and generate\nreasonable and well-founded diagnosis report (i.e., identifying the root causes\nand solutions) within acceptable time (e.g., under 10 minutes compared to hours\nby a DBA). The techniques in D-Bot include (i) offline knowledge extraction\nfrom documents, (ii) automatic prompt generation (e.g., knowledge matching,\ntool retrieval), (iii) root cause analysis using tree search algorithm, and\n(iv) collaborative mechanism for complex anomalies with multiple root causes.\nWe verify D-Bot on real benchmarks (including 539 anomalies of six typical\napplications), and the results show that D-Bot can effectively analyze the root\ncauses of unseen anomalies and significantly outperforms traditional methods\nand vanilla models like GPT-4.",
            "author": [
                "Xuanhe Zhou",
                "Guoliang Li",
                "Zhaoyan Sun",
                "Zhiyuan Liu",
                "Weize Chen",
                "Jianming Wu",
                "Jiesi Liu",
                "Ruohang Feng",
                "Guoyang Zeng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01454v2",
                "http://arxiv.org/pdf/2312.01454v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01450v1",
            "title": "Foveation in the Era of Deep Learning",
            "updated": "2023-12-03T16:48:09Z",
            "published": "2023-12-03T16:48:09Z",
            "summary": "In this paper, we tackle the challenge of actively attending to visual scenes\nusing a foveated sensor. We introduce an end-to-end differentiable foveated\nactive vision architecture that leverages a graph convolutional network to\nprocess foveated images, and a simple yet effective formulation for foveated\nimage sampling. Our model learns to iteratively attend to regions of the image\nrelevant for classification. We conduct detailed experiments on a variety of\nimage datasets, comparing the performance of our method with previous\napproaches to foveated vision while measuring how the impact of different\nchoices, such as the degree of foveation, and the number of fixations the\nnetwork performs, affect object recognition performance. We find that our model\noutperforms a state-of-the-art CNN and foveated vision architectures of\ncomparable parameters and a given pixel or computation budget",
            "author": [
                "George Killick",
                "Paul Henderson",
                "Paul Siebert",
                "Gerardo Aragon-Camarasa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01450v1",
                "http://arxiv.org/pdf/2312.01450v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "I.2.10; I.5.1; I.4.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01448v1",
            "title": "Driven transparent quantum graphs",
            "updated": "2023-12-03T16:34:29Z",
            "published": "2023-12-03T16:34:29Z",
            "summary": "In this paper, we discuss the concept of quantum graphs with transparent\nvertices by considering the case where the graph interacts with an external\ntime-independent field. In particular, we address the problem of transparent\nboundary conditions for quantum graphs, building on previous work on\ntransparent boundary conditions for the stationary Schrodinger equation on a\nline. Physically relevant constraints making the vertex transparent under\nboundary conditions in the form of (weight) continuity and Kirchhoff rules are\nderived using two methods, the scattering approach and transparent boundary\nconditions for the time-independent Schrodinger equation. The latter is derived\nby extending the transparent boundary condition concept to the time-independent\nSchrodinger equation on driven quantum graphs. We also discuss how the\neigenvalues and eigenfunctions of a quantum graph are influenced not only by\nits topology, but also by the shape(type) of a potential when an external field\nis involved.",
            "author": [
                "J. R. Yusupov",
                "M. Ehrhardt",
                "Kh. Sh. Matyokubov",
                "D. U. Matrasulov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01448v1",
                "http://arxiv.org/pdf/2312.01448v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.NA",
                "math-ph",
                "math.MP",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01437v1",
            "title": "On the Bessel solution of Kepler's equation",
            "updated": "2023-12-03T16:05:52Z",
            "published": "2023-12-03T16:05:52Z",
            "summary": "Since its introduction in 1650, Kepler's equation never ceases to exert its\ncharm on mathematicians, scientists, engineers: a huge number of different\nsolution strategies have been conceived and implemented over five centuries.\nAmong them, that originally proposed by J. L. Lagrange and later by F. W.\nBessel, continues to reveal a source of mathematical treasures. Here, the\nBessel solution of Kepler's equation will be explored from a new perspective,\noffered by the theory of {Stieltjes series}. In particular, it will be proved\nthat a complex Kapteyn series, directly obtained by the Bessel expansion, is\nStieltjes. From this mathematical result, a new (up to our knowledge) integral\nrepresentation of the KE solution will be achieved. A few considerations about\npossible extensions of our results to more general classes of Kapteyn series\nare also presented.",
            "author": [
                "Riccardo Borghi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01437v1",
                "http://arxiv.org/pdf/2312.01437v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01424v1",
            "title": "Batch Hop-Constrained s-t Simple Path Query Processing in Large Graphs",
            "updated": "2023-12-03T15:05:00Z",
            "published": "2023-12-03T15:05:00Z",
            "summary": "Hop-constrained s-t simple path (HC-s-t path) enumeration is a fundamental\nproblem in graph analysis. Existing solutions for this problem focus on\noptimizing the processing performance of a single query. However, in practice,\nit is more often that multiple HC-s-t path queries are issued simultaneously\nand processed as a batch. Therefore, we study the problem of batch HC-s-t path\nquery processing in this paper and aim to compute the results of all queries\nconcurrently and efficiently as a batch. To achieve this goal, we first propose\nthe concept of HC-s path query which can precisely characterize the common\ncomputation among different queries.We then devise a two-phase HC-s path query\ndetection algorithm to identify the common HC-s path queries for the given\nHC-s-t path queries. Based on the detected HC-s path queries, we further devise\nan efficient HC-s-t path enumeration algorithm in which the common computation\nrepresented by HC-s path queries are effectively shared. We conduct extensive\nexperiments on real-world graphs and the experimental results demonstrate that\nour proposed algorithm is efficient and scalable regarding processing multiple\nHC-s-t path queries in large graphs at billion-scale.",
            "author": [
                "Kongzhang Hao",
                "Long Yuan",
                "Xuemin Lin",
                "Wenjie Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01424v1",
                "http://arxiv.org/pdf/2312.01424v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01403v1",
            "title": "OplixNet: Towards Area-Efficient Optical Split-Complex Networks with\n  Real-to-Complex Data Assignment and Knowledge Distillation",
            "updated": "2023-12-03T14:06:20Z",
            "published": "2023-12-03T14:06:20Z",
            "summary": "Having the potential for high speed, high throughput, and low energy cost,\noptical neural networks (ONNs) have emerged as a promising candidate for\naccelerating deep learning tasks. In conventional ONNs, light amplitudes are\nmodulated at the input and detected at the output. However, the light phases\nare still ignored in conventional structures, although they can also carry\ninformation for computing. To address this issue, in this paper, we propose a\nframework called OplixNet to compress the areas of ONNs by modulating input\nimage data into the amplitudes and phase parts of light signals. The input and\noutput parts of the ONNs are redesigned to make full use of both amplitude and\nphase information. Moreover, mutual learning across different ONN structures is\nintroduced to maintain the accuracy. Experimental results demonstrate that the\nproposed framework significantly reduces the areas of ONNs with the accuracy\nwithin an acceptable range. For instance, 75.03% area is reduced with a 0.33%\naccuracy decrease on fully connected neural network (FCNN) and 74.88% area is\nreduced with a 2.38% accuracy decrease on ResNet-32.",
            "author": [
                "Ruidi Qiu",
                "Amro Eldebiky",
                "Grace Li Zhang",
                "Xunzhao Yin",
                "Cheng Zhuo",
                "Ulf Schlichtmann",
                "Bing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01403v1",
                "http://arxiv.org/pdf/2312.01403v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01384v2",
            "title": "A Tight Lower Bound for 3-Coloring Grids in the Online-LOCAL Model",
            "updated": "2023-12-05T12:47:21Z",
            "published": "2023-12-03T13:14:37Z",
            "summary": "Recently, Akbari, Eslami, Lievonen, Melnyk, S\\\"{a}rkij\\\"{a}rvi, and Suomela\n(ICALP 2023) studied the locality of graph problems in distributed, sequential,\ndynamic, and online settings from a unified point of view. They designed a\nnovel $O(\\log n)$-locality algorithm for proper 3-coloring bipartite graphs in\nthe $\\mathsf{Online}$-$\\mathsf{LOCAL}$ model. In this work, we show the\noptimality of the algorithm by demonstrating a tight $\\Omega(\\log n)$ locality\nlower bound which holds even on grids. Moreover, we show a higher\n$\\Omega(\\sqrt{n})$ lower bound for 3-coloring toroidal and cylindrical grids.",
            "author": [
                "Yi-Jun Chang",
                "Gopinath Mishra",
                "Thuan Hung Nguyen",
                "Mingyang Yang",
                "Yu-Cheng Yeh"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01384v2",
                "http://arxiv.org/pdf/2312.01384v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01381v1",
            "title": "Language-driven All-in-one Adverse Weather Removal",
            "updated": "2023-12-03T13:05:54Z",
            "published": "2023-12-03T13:05:54Z",
            "summary": "All-in-one (AiO) frameworks restore various adverse weather degradations with\na single set of networks jointly. To handle various weather conditions, an AiO\nframework is expected to adaptively learn weather-specific knowledge for\ndifferent degradations and shared knowledge for common patterns. However,\nexisting methods: 1) rely on extra supervision signals, which are usually\nunknown in real-world applications; 2) employ fixed network structures, which\nrestrict the diversity of weather-specific knowledge. In this paper, we propose\na Language-driven Restoration framework (LDR) to alleviate the aforementioned\nissues. First, we leverage the power of pre-trained vision-language (PVL)\nmodels to enrich the diversity of weather-specific knowledge by reasoning about\nthe occurrence, type, and severity of degradation, generating description-based\ndegradation priors. Then, with the guidance of degradation prior, we sparsely\nselect restoration experts from a candidate list dynamically based on a\nMixture-of-Experts (MoE) structure. This enables us to adaptively learn the\nweather-specific and shared knowledge to handle various weather conditions\n(e.g., unknown or mixed weather). Experiments on extensive restoration\nscenarios show our superior performance (see Fig. 1). The source code will be\nmade available.",
            "author": [
                "Hao Yang",
                "Liyuan Pan",
                "Yan Yang",
                "Wei Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01381v1",
                "http://arxiv.org/pdf/2312.01381v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01378v1",
            "title": "The reachability homology of a directed graph",
            "updated": "2023-12-03T12:56:58Z",
            "published": "2023-12-03T12:56:58Z",
            "summary": "The last decade has seen the development of path homology and magnitude\nhomology -- two homology theories of directed graphs, each satisfying classic\nproperties such as Kunneth and Mayer-Vietoris theorems. Recent work of Asao has\nshown that magnitude homology and path homology are related, appearing in\ndifferent pages of a certain spectral sequence. Here we study the target of\nthat spectral sequence, which we call reachability homology. We prove that it\nsatisfies appropriate homotopy invariance, Kunneth, excision, and\nMayer-Vietoris theorems, these all being stronger than the corresponding\nproperties for either magnitude or path homology.",
            "author": [
                "Richard Hepworth",
                "Emily Roff"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01378v1",
                "http://arxiv.org/pdf/2312.01378v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.CO",
                "math.CT",
                "18G90, 05C25 (Primary) 05C31, 05C38, 18G35 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02719v1",
            "title": "A Conditional Denoising Diffusion Probabilistic Model for Point Cloud\n  Upsampling",
            "updated": "2023-12-03T12:41:41Z",
            "published": "2023-12-03T12:41:41Z",
            "summary": "Point cloud upsampling (PCU) enriches the representation of raw point clouds,\nsignificantly improving the performance in downstream tasks such as\nclassification and reconstruction. Most of the existing point cloud upsampling\nmethods focus on sparse point cloud feature extraction and upsampling module\ndesign. In a different way, we dive deeper into directly modelling the gradient\nof data distribution from dense point clouds. In this paper, we proposed a\nconditional denoising diffusion probability model (DDPM) for point cloud\nupsampling, called PUDM. Specifically, PUDM treats the sparse point cloud as a\ncondition, and iteratively learns the transformation relationship between the\ndense point cloud and the noise. Simultaneously, PUDM aligns with a dual\nmapping paradigm to further improve the discernment of point features. In this\ncontext, PUDM enables learning complex geometry details in the ground truth\nthrough the dominant features, while avoiding an additional upsampling module\ndesign. Furthermore, to generate high-quality arbitrary-scale point clouds\nduring inference, PUDM exploits the prior knowledge of the scale between sparse\npoint clouds and dense point clouds during training by parameterizing a rate\nfactor. Moreover, PUDM exhibits strong noise robustness in experimental\nresults. In the quantitative and qualitative evaluations on PU1K and PUGAN,\nPUDM significantly outperformed existing methods in terms of Chamfer Distance\n(CD) and Hausdorff Distance (HD), achieving state of the art (SOTA)\nperformance.",
            "author": [
                "Wentao Qu",
                "Yuantian Shao",
                "Lingwu Meng",
                "Xiaoshui Huang",
                "Liang Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02719v1",
                "http://arxiv.org/pdf/2312.02719v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01367v1",
            "title": "DiFace: Cross-Modal Face Recognition through Controlled Diffusion",
            "updated": "2023-12-03T12:28:52Z",
            "published": "2023-12-03T12:28:52Z",
            "summary": "Diffusion probabilistic models (DPMs) have exhibited exceptional proficiency\nin generating visual media of outstanding quality and realism. Nonetheless,\ntheir potential in non-generative domains, such as face recognition, has yet to\nbe thoroughly investigated. Meanwhile, despite the extensive development of\nmulti-modal face recognition methods, their emphasis has predominantly centered\non visual modalities. In this context, face recognition through textual\ndescription presents a unique and promising solution that not only transcends\nthe limitations from application scenarios but also expands the potential for\nresearch in the field of cross-modal face recognition. It is regrettable that\nthis avenue remains unexplored and underutilized, a consequence from the\nchallenges mainly associated with three aspects: 1) the intrinsic imprecision\nof verbal descriptions; 2) the significant gaps between texts and images; and\n3) the immense hurdle posed by insufficient databases.To tackle this problem,\nwe present DiFace, a solution that effectively achieves face recognition via\ntext through a controllable diffusion process, by establishing its theoretical\nconnection with probability transport. Our approach not only unleashes the\npotential of DPMs across a broader spectrum of tasks but also achieves, to the\nbest of our knowledge, a significant accuracy in text-to-image face recognition\nfor the first time, as demonstrated by our experiments on verification and\nidentification.",
            "author": [
                "Bowen Sun",
                "Shibao Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01367v1",
                "http://arxiv.org/pdf/2312.01367v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01353v1",
            "title": "The minimum number of detours in graphs",
            "updated": "2023-12-03T11:16:29Z",
            "published": "2023-12-03T11:16:29Z",
            "summary": "A longest path in a graph is called a detour. It is easy to see that a\nconnected graph of minimum degree at least $2$ and order at least $4$ has at\nleast $4$ detours. We prove that if the number of detours in such a graph of\norder at least $9$ is odd, then it is at least $9,$ and this lower bound can be\nattained for every order. Thus the possibilities $3,$ $5$ and $7$ are excluded.\nTwo open problems are posed.",
            "author": [
                "Xingzhi Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01353v1",
                "http://arxiv.org/pdf/2312.01353v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C30, 05C35, 05C38"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01342v1",
            "title": "Graph Coordinates and Conventional Neural Networks -- An Alternative for\n  Graph Neural Networks",
            "updated": "2023-12-03T10:14:10Z",
            "published": "2023-12-03T10:14:10Z",
            "summary": "Graph-based data present unique challenges and opportunities for machine\nlearning. Graph Neural Networks (GNNs), and especially those algorithms that\ncapture graph topology through message passing for neighborhood aggregation,\nhave been a leading solution. However, these networks often require substantial\ncomputational resources and may not optimally leverage the information\ncontained in the graph's topology, particularly for large-scale or complex\ngraphs. We propose Topology Coordinate Neural Network (TCNN) and Directional\nVirtual Coordinate Neural Network (DVCNN) as novel and efficient alternatives\nto message passing GNNs, that directly leverage the graph's topology,\nsidestepping the computational challenges presented by competing algorithms.\nOur proposed methods can be viewed as a reprise of classic techniques for graph\nembedding for neural network feature engineering, but they are novel in that\nour embedding techniques leverage ideas in Graph Coordinates (GC) that are\nlacking in current practice. Experimental results, benchmarked against the Open\nGraph Benchmark Leaderboard, demonstrate that TCNN and DVCNN achieve\ncompetitive or superior performance to message passing GNNs. For similar levels\nof accuracy and ROC-AUC, TCNN and DVCNN need far fewer trainable parameters\nthan contenders of the OGBN Leaderboard. The proposed TCNN architecture\nrequires fewer parameters than any neural network method currently listed in\nthe OGBN Leaderboard for both OGBN-Proteins and OGBN-Products datasets.\nConversely, our methods achieve higher performance for a similar number of\ntrainable parameters. By providing an efficient and effective alternative to\nmessage passing GNNs, our work expands the toolbox of techniques for\ngraph-based machine learning.",
            "author": [
                "Zheyi Qin",
                "Randy Paffenroth",
                "Anura P. Jayasumana"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01342v1",
                "http://arxiv.org/pdf/2312.01342v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01338v1",
            "title": "Enhancing and Adapting in the Clinic: Source-free Unsupervised Domain\n  Adaptation for Medical Image Enhancement",
            "updated": "2023-12-03T10:01:59Z",
            "published": "2023-12-03T10:01:59Z",
            "summary": "Medical imaging provides many valuable clues involving anatomical structure\nand pathological characteristics. However, image degradation is a common issue\nin clinical practice, which can adversely impact the observation and diagnosis\nby physicians and algorithms. Although extensive enhancement models have been\ndeveloped, these models require a well pre-training before deployment, while\nfailing to take advantage of the potential value of inference data after\ndeployment. In this paper, we raise an algorithm for source-free unsupervised\ndomain adaptive medical image enhancement (SAME), which adapts and optimizes\nenhancement models using test data in the inference phase. A\nstructure-preserving enhancement network is first constructed to learn a robust\nsource model from synthesized training data. Then a teacher-student model is\ninitialized with the source model and conducts source-free unsupervised domain\nadaptation (SFUDA) by knowledge distillation with the test data. Additionally,\na pseudo-label picker is developed to boost the knowledge distillation of\nenhancement tasks. Experiments were implemented on ten datasets from three\nmedical image modalities to validate the advantage of the proposed algorithm,\nand setting analysis and ablation studies were also carried out to interpret\nthe effectiveness of SAME. The remarkable enhancement performance and benefits\nfor downstream tasks demonstrate the potential and generalizability of SAME.\nThe code is available at\nhttps://github.com/liamheng/Annotation-free-Medical-Image-Enhancement.",
            "author": [
                "Heng Li",
                "Ziqin Lin",
                "Zhongxi Qiu",
                "Zinan Li",
                "Huazhu Fu",
                "Yan Hu",
                "Jiang Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMI.2023.3335651",
                "http://arxiv.org/abs/2312.01338v1",
                "http://arxiv.org/pdf/2312.01338v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01314v1",
            "title": "NLEBench+NorGLM: A Comprehensive Empirical Analysis and Benchmark\n  Dataset for Generative Language Models in Norwegian",
            "updated": "2023-12-03T08:09:45Z",
            "published": "2023-12-03T08:09:45Z",
            "summary": "Recent advancements in Generative Language Models (GLMs) have transformed\nNatural Language Processing (NLP) by showcasing the effectiveness of the\n\"pre-train, prompt, and predict\" paradigm in utilizing pre-trained GLM\nknowledge for diverse applications. Despite their potential, these capabilities\nlack adequate quantitative characterization due to the absence of comprehensive\nbenchmarks, particularly for low-resource languages. Existing low-resource\nbenchmarks focus on discriminative language models like BERT, neglecting the\nevaluation of generative language models. Moreover, current benchmarks often\noverlook measuring generalization performance across multiple tasks, a crucial\nmetric for GLMs.\n  To bridge these gaps, we introduce NLEBench, a comprehensive benchmark\ntailored for evaluating natural language generation capabilities in Norwegian,\na low-resource language. We use Norwegian as a case study to explore whether\ncurrent GLMs and benchmarks in mainstream languages like English can reveal the\nunique characteristics of underrepresented languages. NLEBench encompasses a\nsuite of real-world NLP tasks ranging from news storytelling, summarization,\nopen-domain conversation, natural language understanding, instruction\nfine-tuning, toxicity and bias evaluation, to self-curated Chain-of-Thought\ninvestigation. It features two high-quality, human-annotated datasets: an\ninstruction dataset covering traditional Norwegian cultures, idioms, slang, and\nspecial expressions, and a document-grounded multi-label dataset for topic\nclassification, question answering, and summarization. This paper also\nintroduces foundational Norwegian Generative Language Models (NorGLMs)\ndeveloped with diverse parameter scales and Transformer-based architectures.\nSystematic evaluations on the proposed benchmark suite provide insights into\nthe capabilities and scalability of NorGLMs across various downstream tasks.",
            "author": [
                "Peng Liu",
                "Lemei Zhang",
                "Terje Nissen Farup",
                "Even W. Lauvrak",
                "Jon Espen Ingvaldsen",
                "Simen Eide",
                "Jon Atle Gulla",
                "Zhirong Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01314v1",
                "http://arxiv.org/pdf/2312.01314v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01308v1",
            "title": "Bridging Background Knowledge Gaps in Translation with Automatic\n  Explicitation",
            "updated": "2023-12-03T07:24:12Z",
            "published": "2023-12-03T07:24:12Z",
            "summary": "Translations help people understand content written in another language.\nHowever, even correct literal translations do not fulfill that goal when people\nlack the necessary background to understand them. Professional translators\nincorporate explicitations to explain the missing context by considering\ncultural differences between source and target audiences. Despite its potential\nto help users, NLP research on explicitation is limited because of the dearth\nof adequate evaluation methods. This work introduces techniques for\nautomatically generating explicitations, motivated by WikiExpl: a dataset that\nwe collect from Wikipedia and annotate with human translators. The resulting\nexplicitations are useful as they help answer questions more accurately in a\nmultilingual question answering framework.",
            "author": [
                "HyoJung Han",
                "Jordan Lee Boyd-Graber",
                "Marine Carpuat"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01308v1",
                "http://arxiv.org/pdf/2312.01308v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01299v1",
            "title": "Robust Non-parametric Knowledge-based Diffusion Least Mean Squares over\n  Adaptive Networks",
            "updated": "2023-12-03T06:18:59Z",
            "published": "2023-12-03T06:18:59Z",
            "summary": "The present study proposes incorporating non-parametric knowledge into the\ndiffusion least-mean-squares algorithm in the framework of a maximum a\nposteriori (MAP) estimation. The proposed algorithm leads to a robust\nestimation of an unknown parameter vector in a group of cooperative estimators.\nUtilizing kernel density estimation and buffering some intermediate\nestimations, the prior distribution and conditional likelihood of the\nparameters vector in each node are calculated. Pseudo Huber loss function is\nused for designing the likelihood function. Also, an error thresholding\nfunction is defined to reduce the computational overhead as well as more\nrelaxation against noise, which stops the update every time an error is less\nthan a predefined threshold. The performance of the proposed algorithm is\nexamined in the stationary and non-stationary scenarios in the presence of\nGaussian and non-Gaussian noise. Results show the robustness of the proposed\nalgorithm in the presence of different noise types.",
            "author": [
                "Soheil Ashkezari-Toussi",
                "Hadi sadoghi-Yazdi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01299v1",
                "http://arxiv.org/pdf/2312.01299v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01295v1",
            "title": "Two-stage dynamic creative optimization under sparse ambiguous samples\n  for e-commerce advertising",
            "updated": "2023-12-03T06:02:27Z",
            "published": "2023-12-03T06:02:27Z",
            "summary": "Ad creative is one of the main mediums for e-commerce advertising. In our\napproach we decouple this dynamic creative optimization into two stages, a\ncascaded structure that can trade off between effectiveness and efficiency. In\nthe first stage, we train an automatic creative optimization architecture based\non autoco to simulate complex interactions between creative elements. Although\nwe obtained the ranking of different creatives under a sku, because we bucketed\nand merged historical data according to periods, this confuses the ctr\ndiversity of the same ad creatives on different days and weakens the ability to\nseparate ambiguous samples. Therefore, we propose a transformer-based rerank\nmodel. With the help of the rank model, we propose a distillation method to\nlearn the relative order of ideas and extract the ranking knowledge to guide\nthe rerank learning. The creative order soft labels under each sku are\ngenerated by the rank model to alleviate the dilemma that a large number of\nunder-represented creatives cannot obtain real labels. Through the knowledge\ndiffusion of rerank, the ambiguous samples are associated with the positive and\nnegative samples. Cascade rerank and autoco to output the estimated value of\nthe synthetic ad image. In the second stage, we designed a bandit model, and\nthe bandit selected one of the output ad of the first stage for timely\ndelivery. Experimental results show that our method can outperform competing\nbaselines in terms of sctr. Online A/B testing shows that our method improves\nctr by 10% compared to the baseline.",
            "author": [
                "Guandong Li",
                "Xian Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01295v1",
                "http://arxiv.org/pdf/2312.01295v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01291v1",
            "title": "Opportunities for Retrieval and Tool Augmented Large Language Models in\n  Scientific Facilities",
            "updated": "2023-12-03T05:32:26Z",
            "published": "2023-12-03T05:32:26Z",
            "summary": "Upgrades to advanced scientific user facilities such as next-generation x-ray\nlight sources, nanoscience centers, and neutron facilities are revolutionizing\nour understanding of materials across the spectrum of the physical sciences,\nfrom life sciences to microelectronics. However, these facility and instrument\nupgrades come with a significant increase in complexity. Driven by more\nexacting scientific needs, instruments and experiments become more intricate\neach year. This increased operational complexity makes it ever more challenging\nfor domain scientists to design experiments that effectively leverage the\ncapabilities of and operate on these advanced instruments. Large language\nmodels (LLMs) can perform complex information retrieval, assist in\nknowledge-intensive tasks across applications, and provide guidance on tool\nusage. Using x-ray light sources, leadership computing, and nanoscience centers\nas representative examples, we describe preliminary experiments with a\nContext-Aware Language Model for Science (CALMS) to assist scientists with\ninstrument operations and complex experimentation. With the ability to retrieve\nrelevant information from facility documentation, CALMS can answer simple\nquestions on scientific capabilities and other operational procedures. With the\nability to interface with software tools and experimental hardware, CALMS can\nconversationally operate scientific instruments. By making information more\naccessible and acting on user needs, LLMs could expand and diversify scientific\nfacilities' users and accelerate scientific output.",
            "author": [
                "Michael H. Prince",
                "Henry Chan",
                "Aikaterini Vriza",
                "Tao Zhou",
                "Varuni K. Sastry",
                "Matthew T. Dearing",
                "Ross J. Harder",
                "Rama K. Vasudevan",
                "Mathew J. Cherukara"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01291v1",
                "http://arxiv.org/pdf/2312.01291v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cond-mat.mtrl-sci",
                "physics.acc-ph",
                "physics.app-ph",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01276v1",
            "title": "Running cognitive evaluations on large language models: The do's and the\n  don'ts",
            "updated": "2023-12-03T04:28:19Z",
            "published": "2023-12-03T04:28:19Z",
            "summary": "In this paper, I describe methodological considerations for studies that aim\nto evaluate the cognitive capacities of large language models (LLMs) using\nlanguage-based behavioral assessments. Drawing on three case studies from the\nliterature (a commonsense knowledge benchmark, a theory of mind evaluation, and\na test of syntactic agreement), I describe common pitfalls that might arise\nwhen applying a cognitive test to an LLM. I then list 10 do's and don'ts that\nshould help design high-quality cognitive evaluations for AI systems. I\nconclude by discussing four areas where the do's and don'ts are currently under\nactive discussion -- prompt sensitivity, cultural and linguistic diversity,\nusing LLMs as research assistants, and running evaluations on open vs. closed\nLLMs. Overall, the goal of the paper is to contribute to the broader discussion\nof best practices in the rapidly growing field of AI Psychology.",
            "author": [
                "Anna A. Ivanova"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01276v1",
                "http://arxiv.org/pdf/2312.01276v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01262v1",
            "title": "A Review and A Robust Framework of Data-Efficient 3D Scene Parsing with\n  Traditional/Learned 3D Descriptors",
            "updated": "2023-12-03T02:51:54Z",
            "published": "2023-12-03T02:51:54Z",
            "summary": "Existing state-of-the-art 3D point cloud understanding methods merely perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework that simultaneously solves the downstream high-level\nunderstanding tasks including both segmentation and detection, especially when\nlabels are extremely limited. This work presents a general and simple framework\nto tackle point cloud understanding when labels are limited. The first\ncontribution is that we have done extensive methodology comparisons of\ntraditional and learned 3D descriptors for the task of weakly supervised 3D\nscene understanding, and validated that our adapted traditional PFH-based 3D\ndescriptors show excellent generalization ability across different domains. The\nsecond contribution is that we proposed a learning-based region merging\nstrategy based on the affinity provided by both the traditional/learned 3D\ndescriptors and learned semantics. The merging process takes both low-level\ngeometric and high-level semantic feature correlations into consideration.\nExperimental results demonstrate that our framework has the best performance\namong the three most important weakly supervised point clouds understanding\ntasks including semantic segmentation, instance segmentation, and object\ndetection even when very limited number of points are labeled. Our method,\ntermed Region Merging 3D (RM3D), has superior performance on ScanNet\ndata-efficient learning online benchmarks and other four large-scale 3D\nunderstanding benchmarks under various experimental settings, outperforming\ncurrent arts by a margin for various 3D understanding tasks without complicated\nlearning strategies such as active learning.",
            "author": [
                "Kangcheng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01262v1",
                "http://arxiv.org/pdf/2312.01262v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02208v1",
            "title": "A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing",
            "updated": "2023-12-03T02:38:51Z",
            "published": "2023-12-03T02:38:51Z",
            "summary": "Existing state-of-the-art 3D point clouds understanding methods only perform\nwell in a fully supervised manner. To the best of our knowledge, there exists\nno unified framework which simultaneously solves the downstream high-level\nunderstanding tasks, especially when labels are extremely limited. This work\npresents a general and simple framework to tackle point clouds understanding\nwhen labels are limited. We propose a novel unsupervised region expansion based\nclustering method for generating clusters. More importantly, we innovatively\npropose to learn to merge the over-divided clusters based on the local\nlow-level geometric property similarities and the learned high-level feature\nsimilarities supervised by weak labels. Hence, the true weak labels guide\npseudo labels merging taking both geometric and semantic feature correlations\ninto consideration. Finally, the self-supervised reconstruction and data\naugmentation optimization modules are proposed to guide the propagation of\nlabels among semantically similar points within a scene. Experimental Results\ndemonstrate that our framework has the best performance among the three most\nimportant weakly supervised point clouds understanding tasks including semantic\nsegmentation, instance segmentation, and object detection even when limited\npoints are labeled, under the data-efficient settings for the large-scale 3D\nsemantic scene parsing. The developed techniques have postentials to be applied\nto downstream tasks for better representations in robotic manipulation and\nrobotic autonomous navigation. Codes and models are publicly available at:\nhttps://github.com/KangchengLiu.",
            "author": [
                "Kangcheng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02208v1",
                "http://arxiv.org/pdf/2312.02208v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02206v1",
            "title": "Axiomatic Preference Modeling for Longform Question Answering",
            "updated": "2023-12-02T23:11:41Z",
            "published": "2023-12-02T23:11:41Z",
            "summary": "The remarkable abilities of large language models (LLMs) like GPT-4 partially\nstem from post-training processes like Reinforcement Learning from Human\nFeedback (RLHF) involving human preferences encoded in a reward model. However,\nthese reward models (RMs) often lack direct knowledge of why, or under what\nprinciples, the preferences annotations were made. In this study, we identify\nprinciples that guide RMs to better align with human preferences, and then\ndevelop an axiomatic framework to generate a rich variety of preference signals\nto uphold them. We use these axiomatic signals to train a model for scoring\nanswers to longform questions. Our approach yields a Preference Model with only\nabout 220M parameters that agrees with gold human-annotated preference labels\nmore often than GPT-4. The contributions of this work include: training a\nstandalone preference model that can score human- and LLM-generated answers on\nthe same scale; developing an axiomatic framework for generating training data\npairs tailored to certain principles; and showing that a small amount of\naxiomatic signals can help small models outperform GPT-4 in preference scoring.\nWe release our model on huggingface:\nhttps://huggingface.co/corbyrosset/axiomatic_preference_model",
            "author": [
                "Corby Rosset",
                "Guoqing Zheng",
                "Victor Dibia",
                "Ahmed Awadallah",
                "Paul Bennett"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02206v1",
                "http://arxiv.org/pdf/2312.02206v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01243v1",
            "title": "Boolean inverse semigroups and their type monoids",
            "updated": "2023-12-02T22:58:28Z",
            "published": "2023-12-02T22:58:28Z",
            "summary": "This is an expository paper which provides a quick introduction to Boolean\ninverse semigroups and their type monoids, with the emphasis on techniques and\ninsights of the theory, and also treats the connection of the type monoid\n${\\mathrm{Typ}}(S)$ of a Boolean inverse semigroup $S$ with the monoid\n$V(K\\langle S\\rangle)$ of the ring $K\\langle S\\rangle$ assigned to $S$. We give\noriginal direct and simple proofs of some known results, such as the structure\nof semisimple Boolean inverse semigroups, the presentation of the type monoid\nby generalized rook matrices. We also prove that the type monoid of the tight\nBooleanization of a graph inverse semigroup is isomorphic to the graph monoid\nof this semigroup.",
            "author": [
                "Ganna Kudryavtseva"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01243v1",
                "http://arxiv.org/pdf/2312.01243v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01239v2",
            "title": "Motion Informed Needle Segmentation in Ultrasound Images",
            "updated": "2023-12-05T04:14:18Z",
            "published": "2023-12-02T22:25:24Z",
            "summary": "Segmenting a moving needle in ultrasound images is challenging due to the\npresence of artifacts, noise, and needle occlusion. This task becomes even more\ndemanding in scenarios where data availability is limited. Convolutional Neural\nNetworks (CNNs) have been successful in many computer vision applications, but\nstruggle to accurately segment needles without considering their motion. In\nthis paper, we present a novel approach for needle segmentation that combines\nclassical Kalman Filter (KF) techniques with data-driven learning,\nincorporating both needle features and needle motion. Our method offers two key\ncontributions. First, we propose a compatible framework that seamlessly\nintegrates into commonly used encoder-decoder style architectures. Second, we\ndemonstrate superior performance compared to recent state-of-the-art needle\nsegmentation models using our novel convolutional neural network (CNN) based\nKF-inspired block, achieving a 15\\% reduction in pixel-wise needle tip error\nand an 8\\% reduction in length error. Third, to our knowledge we are the first\nto implement a learnable filter to incorporate non-linear needle motion for\nimproving needle segmentation.",
            "author": [
                "Raghavv Goel",
                "Cecilia Morales",
                "Manpreet Singh",
                "Artur Dubrawski",
                "John Galeotti",
                "Howie Choset"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01239v2",
                "http://arxiv.org/pdf/2312.01239v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01228v1",
            "title": "Mixed-Integer Optimisation of Graph Neural Networks for Computer-Aided\n  Molecular Design",
            "updated": "2023-12-02T21:10:18Z",
            "published": "2023-12-02T21:10:18Z",
            "summary": "ReLU neural networks have been modelled as constraints in mixed integer\nlinear programming (MILP), enabling surrogate-based optimisation in various\ndomains and efficient solution of machine learning certification problems.\nHowever, previous works are mostly limited to MLPs. Graph neural networks\n(GNNs) can learn from non-euclidean data structures such as molecular\nstructures efficiently and are thus highly relevant to computer-aided molecular\ndesign (CAMD). We propose a bilinear formulation for ReLU Graph Convolutional\nNeural Networks and a MILP formulation for ReLU GraphSAGE models. These\nformulations enable solving optimisation problems with trained GNNs embedded to\nglobal optimality. We apply our optimization approach to an illustrative CAMD\ncase study where the formulations of the trained GNNs are used to design\nmolecules with optimal boiling points.",
            "author": [
                "Tom McDonald",
                "Calvin Tsay",
                "Artur M. Schweidtmann",
                "Neil Yorke-Smith"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01228v1",
                "http://arxiv.org/pdf/2312.01228v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.NE",
                "90C11",
                "G.1.6; I.2.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01219v1",
            "title": "A hierarchical event correlation model for real time threat detection\n  and response",
            "updated": "2023-12-02T20:07:40Z",
            "published": "2023-12-02T20:07:40Z",
            "summary": "Intrusion detection systems perform post-compromise detection of security\nbreaches whenever preventive measures such as firewalls do not avert an attack.\nHowever, these systems raise a vast number of alerts that must be analysed and\ntriaged by security analysts. This process is largely manual, tedious and\ntime-consuming. Alert correlation is a technique that tries to reduce the\nnumber of intrusion alerts by aggregating those that are related in some way.\nHowever, the correlation is performed outside the IDS through third-party\nsystems and tools, after the high volume of alerts has already been raised.\nThese other third-party systems add to the complexity of security operations.\nIn this paper, we build on the very researched area of correlation techniques\nby developing a novel hierarchical event correlation model that promises to\nreduce the number of alerts issued by an Intrusion Detection System. This is\nachieved by correlating the events before the IDS classifies them. The proposed\nmodel takes the best of features from similarity and graph-based correlation\ntechniques to deliver an ensemble capability not possible by either approach\nseparately. Further, we propose a correlation process for correlation of events\nrather than alerts as is the case in current art. We further develop our own\ncorrelation and clustering algorithm which is tailor-made to the correlation\nand clustering of network event data. The model is implemented as a proof of\nconcept with experiments run on the DARPA 99 Intrusion detection set. The\ncorrelation achieved 87 percent data reduction through aggregation, producing\nnearly 21000 clusters in about 30 seconds.",
            "author": [
                "Herbert Maosa",
                "Karim Ouazzane",
                "Mohamed Chahine Ghanem"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01219v1",
                "http://arxiv.org/pdf/2312.01219v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01217v1",
            "title": "Understanding Opinions Towards Climate Change on Social Media",
            "updated": "2023-12-02T20:02:34Z",
            "published": "2023-12-02T20:02:34Z",
            "summary": "Social media platforms such as Twitter (now known as X) have revolutionized\nhow the public engage with important societal and political topics. Recently,\nclimate change discussions on social media became a catalyst for political\npolarization and the spreading of misinformation. In this work, we aim to\nunderstand how real world events influence the opinions of individuals towards\nclimate change related topics on social media. To this end, we extracted and\nanalyzed a dataset of 13.6 millions tweets sent by 3.6 million users from 2006\nto 2019. Then, we construct a temporal graph from the user-user mentions\nnetwork and utilize the Louvain community detection algorithm to analyze the\nchanges in community structure around Conference of the Parties on Climate\nChange~(COP) events. Next, we also apply tools from the Natural Language\nProcessing literature to perform sentiment analysis and topic modeling on the\ntweets. Our work acts as a first step towards understanding the evolution of\npro-climate change communities around COP events. Answering these questions\nhelps us understand how to raise people's awareness towards climate change thus\nhopefully calling on more individuals to join the collaborative effort in\nslowing down climate change.",
            "author": [
                "Yashaswi Pupneja",
                "Joseph Zou",
                "Sacha L\u00e9vy",
                "Shenyang Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01217v1",
                "http://arxiv.org/pdf/2312.01217v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01202v1",
            "title": "From Voices to Validity: Leveraging Large Language Models (LLMs) for\n  Textual Analysis of Policy Stakeholder Interviews",
            "updated": "2023-12-02T18:55:14Z",
            "published": "2023-12-02T18:55:14Z",
            "summary": "Obtaining stakeholders' diverse experiences and opinions about current policy\nin a timely manner is crucial for policymakers to identify strengths and gaps\nin resource allocation, thereby supporting effective policy design and\nimplementation. However, manually coding even moderately sized interview texts\nor open-ended survey responses from stakeholders can often be labor-intensive\nand time-consuming. This study explores the integration of Large Language\nModels (LLMs)--like GPT-4--with human expertise to enhance text analysis of\nstakeholder interviews regarding K-12 education policy within one U.S. state.\nEmploying a mixed-methods approach, human experts developed a codebook and\ncoding processes as informed by domain knowledge and unsupervised topic\nmodeling results. They then designed prompts to guide GPT-4 analysis and\niteratively evaluate different prompts' performances. This combined\nhuman-computer method enabled nuanced thematic and sentiment analysis. Results\nreveal that while GPT-4 thematic coding aligned with human coding by 77.89% at\nspecific themes, expanding to broader themes increased congruence to 96.02%,\nsurpassing traditional Natural Language Processing (NLP) methods by over 25%.\nAdditionally, GPT-4 is more closely matched to expert sentiment analysis than\nlexicon-based methods. Findings from quantitative measures and qualitative\nreviews underscore the complementary roles of human domain expertise and\nautomated analysis as LLMs offer new perspectives and coding consistency. The\nhuman-computer interactive approach enhances efficiency, validity, and\ninterpretability of educational policy research.",
            "author": [
                "Alex Liu",
                "Min Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01202v1",
                "http://arxiv.org/pdf/2312.01202v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01188v1",
            "title": "Efficient Expansion and Gradient Based Task Inference for Replay Free\n  Incremental Learning",
            "updated": "2023-12-02T17:28:52Z",
            "published": "2023-12-02T17:28:52Z",
            "summary": "This paper proposes a simple but highly efficient expansion-based model for\ncontinual learning. The recent feature transformation, masking and\nfactorization-based methods are efficient, but they grow the model only over\nthe global or shared parameter. Therefore, these approaches do not fully\nutilize the previously learned information because the same task-specific\nparameter forgets the earlier knowledge. Thus, these approaches show limited\ntransfer learning ability. Moreover, most of these models have constant\nparameter growth for all tasks, irrespective of the task complexity. Our work\nproposes a simple filter and channel expansion based method that grows the\nmodel over the previous task parameters and not just over the global parameter.\nTherefore, it fully utilizes all the previously learned information without\nforgetting, which results in better knowledge transfer. The growth rate in our\nproposed model is a function of task complexity; therefore for a simple task,\nthe model has a smaller parameter growth while for complex tasks, the model\nrequires more parameters to adapt to the current task. Recent expansion based\nmodels show promising results for task incremental learning (TIL). However, for\nclass incremental learning (CIL), prediction of task id is a crucial challenge;\nhence, their results degrade rapidly as the number of tasks increase. In this\nwork, we propose a robust task prediction method that leverages entropy\nweighted data augmentations and the models gradient using pseudo labels. We\nevaluate our model on various datasets and architectures in the TIL, CIL and\ngenerative continual learning settings. The proposed approach shows\nstate-of-the-art results in all these settings. Our extensive ablation studies\nshow the efficacy of the proposed components.",
            "author": [
                "Soumya Roy",
                "Vinay K Verma",
                "Deepak Gupta"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01188v1",
                "http://arxiv.org/pdf/2312.01188v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01166v1",
            "title": "Enhanced spatial modeling on linear networks using Gaussian\n  Whittle-Mat\u00e9rn fields",
            "updated": "2023-12-02T16:22:02Z",
            "published": "2023-12-02T16:22:02Z",
            "summary": "Spatial statistics is traditionally based on stationary models on\n$\\mathbb{R^d}$ like Mat\\'ern fields. The adaptation of traditional spatial\nstatistical methods, originally designed for stationary models in Euclidean\nspaces, to effectively model phenomena on linear networks such as stream\nsystems and urban road networks is challenging. The current study aims to\nanalyze the incidence of traffic accidents on road networks using three\ndifferent methodologies and compare the model performance for each methodology.\nInitially, we analyzed the application of spatial triangulation precisely on\nroad networks instead of traditional continuous regions. However, this approach\nposed challenges in areas with complex boundaries, leading to the emergence of\nartificial spatial dependencies. To address this, we applied an alternative\ncomputational method to construct nonstationary barrier models. Finally, we\nexplored a recently proposed class of Gaussian processes on compact metric\ngraphs, the Whittle-Mat\\'ern fields, defined by a fractional SPDE on the metric\ngraph. The latter fields are a natural extension of Gaussian fields with\nMat\\'ern covariance functions on Euclidean domains to non-Euclidean metric\ngraph settings. A ten-year period (2010-2019) of daily traffic-accident records\nfrom Barcelona, Spain have been used to evaluate the three models referred\nabove. While comparing model performance we observed that the Whittle-Mat\\'ern\nfields defined directly on the network outperformed the network triangulation\nand barrier models. Due to their flexibility, the Whittle-Mat\\'ern fields can\nbe applied to a wide range of environmental problems on linear networks such as\nspatio-temporal modeling of water contamination in stream networks or modeling\nair quality or accidents on urban road networks.",
            "author": [
                "Somnath Chaudhuri",
                "Maria A. Barcel\u00f3",
                "Pablo Juan",
                "Diego Varga",
                "David Bolin",
                "Haavard Rue",
                "Marc Saez"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01166v1",
                "http://arxiv.org/pdf/2312.01166v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01163v1",
            "title": "A New Learning Paradigm for Foundation Model-based Remote Sensing Change\n  Detection",
            "updated": "2023-12-02T15:57:17Z",
            "published": "2023-12-02T15:57:17Z",
            "summary": "Change detection (CD) is a critical task to observe and analyze dynamic\nprocesses of land cover. Although numerous deep learning-based CD models have\nperformed excellently, their further performance improvements are constrained\nby the limited knowledge extracted from the given labelled data. On the other\nhand, the foundation models that emerged recently contain a huge amount of\nknowledge by scaling up across data modalities and proxy tasks. In this paper,\nwe propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation\nmodel-based CD adaptation framework aiming to extract the knowledge of\nfoundation models for CD. The proposed BAN contains three parts, i.e. frozen\nfoundation model (e.g., CLIP), bitemporal adapter branch (Bi-TAB), and bridging\nmodules between them. Specifically, the Bi-TAB can be either an existing\narbitrary CD model or some hand-crafted stacked blocks. The bridging modules\nare designed to align the general features with the task/domain-specific\nfeatures and inject the selected general knowledge into the Bi-TAB. To our\nknowledge, this is the first universal framework to adapt the foundation model\nto the CD task. Extensive experiments show the effectiveness of our BAN in\nimproving the performance of existing CD methods (e.g., up to 4.08\\% IoU\nimprovement) with only a few additional learnable parameters. More importantly,\nthese successful practices show us the potential of foundation models for\nremote sensing CD. The code is available at \\url{https://github.com/likyoo/BAN}\nand will be supported in our Open-CD \\url{https://github.com/likyoo/open-cd}.",
            "author": [
                "Kaiyu Li",
                "Xiangyong Cao",
                "Deyu Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01163v1",
                "http://arxiv.org/pdf/2312.01163v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01160v1",
            "title": "Graph characterization of the annihilator ideals of Leavitt path\n  algebras",
            "updated": "2023-12-02T15:42:25Z",
            "published": "2023-12-02T15:42:25Z",
            "summary": "If $E$ is a graph and $K$ is a field, we consider an ideal $I$ of the Leavitt\npath algebra $L_K(E)$ of $E$ over $K$. We describe the admissible pair\ncorresponding to the smallest graded ideal which contains $I$ where the grading\nin question is the natural grading of $L_K(E)$ by $\\mathbb Z$. Using this\ndescription, we show that the right and the left annihilators of $I$ are equal\n(which can be somewhat surprising given that $I$ may not be self-adjoint). In\nparticular, we establish that both annihilators correspond to the same\nadmissible pair and its description produces the characterization from the\ntitle. Then, we turn to the property that the right (equivalently left)\nannihilator of any ideal is a direct summand and recall that a unital ring with\nthis property is said to be quasi-Baer. We exhibit a condition on $E$ which is\nequivalent to unital $L_K(E)$ having this property.",
            "author": [
                "Lia Vas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01160v1",
                "http://arxiv.org/pdf/2312.01160v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "16S88, 16D25, 16D70"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01149v1",
            "title": "Disjoint Dominating and 2-Dominating Sets in Graphs: Hardness and\n  Approximation results",
            "updated": "2023-12-02T14:35:43Z",
            "published": "2023-12-02T14:35:43Z",
            "summary": "A set $D \\subseteq V$ of a graph $G=(V, E)$ is a dominating set of $G$ if\neach vertex $v\\in V\\setminus D$ is adjacent to at least one vertex in $D,$\nwhereas a set $D_2\\subseteq V$ is a $2$-dominating (double dominating) set of\n$G$ if each vertex $v\\in V \\setminus D_2$ is adjacent to at least two vertices\nin $D_2.$ A graph $G$ is a $DD_2$-graph if there exists a pair ($D, D_2$) of\ndominating set and $2$-dominating set of $G$ which are disjoint. In this paper,\nwe solve some open problems posed by M.Miotk, J.~Topp and P.{\\.Z}yli{\\'n}ski\n(Disjoint dominating and 2-dominating sets in graphs, Discrete Optimization,\n35:100553, 2020) by giving approximation algorithms for the problem of\ndetermining a minimal spanning $DD_2$-graph of minimum size (Min-$DD_2$) with\nan approximation ratio of $3$; a minimal spanning $DD_2$-graph of maximum size\n(Max-$DD_2$) with an approximation ratio of $3$; and for the problem of adding\nminimum number of edges to a graph $G$ to make it a $DD_2$-graph\n(Min-to-$DD_2$) with an $O(\\log n)$ approximation ratio. Furthermore, we prove\nthat Min-$DD_2$ and Max-$DD_2$ are APX-complete for graphs with maximum degree\n$4$. We also show that Min-$DD_2$ and Max-$DD_2$ are approximable within a\nfactor of $1.8$ and $1.5$ respectively, for any $3$-regular graph. Finally, we\nshow the inapproximability result of Max-Min-to-$DD_2$ for bipartite graphs,\nthat this problem can not be approximated within $n^{\\frac{1}{6}-\\varepsilon}$\nfor any $\\varepsilon >0,$ unless P=NP.",
            "author": [
                "Soumyashree Rana",
                "Sounaka Mishra",
                "Bhawani Sankar Panda"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01149v1",
                "http://arxiv.org/pdf/2312.01149v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01148v1",
            "title": "Has Anything Changed? 3D Change Detection by 2D Segmentation Masks",
            "updated": "2023-12-02T14:30:23Z",
            "published": "2023-12-02T14:30:23Z",
            "summary": "As capturing devices become common, 3D scans of interior spaces are acquired\non a daily basis. Through scene comparison over time, information about objects\nin the scene and their changes is inferred. This information is important for\nrobots and AR and VR devices, in order to operate in an immersive virtual\nexperience. We thus propose an unsupervised object discovery method that\nidentifies added, moved, or removed objects without any prior knowledge of what\nobjects exist in the scene. We model this problem as a combination of a 3D\nchange detection and a 2D segmentation task. Our algorithm leverages generic 2D\nsegmentation masks to refine an initial but incomplete set of 3D change\ndetections. The initial changes, acquired through render-and-compare likely\ncorrespond to movable objects. The incomplete detections are refined through\ngraph optimization, distilling the information of the 2D segmentation masks in\nthe 3D space. Experiments on the 3Rscan dataset prove that our method\noutperforms competitive baselines, with SoTA results.",
            "author": [
                "Aikaterini Adam",
                "Konstantinos Karantzalos",
                "Lazaros Grammatikopoulos",
                "Torsten Sattler"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01148v1",
                "http://arxiv.org/pdf/2312.01148v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01147v1",
            "title": "On the knowledge production function",
            "updated": "2023-12-02T14:17:33Z",
            "published": "2023-12-02T14:17:33Z",
            "summary": "Knowledge amount is an integral indicator of the humanitarian and\ntechnological development of society. The rate of knowledge production depends\non population size and knowledge amount. Formalizing this relationship, we lead\nto an equation for knowledge production that connects population and\ninformation dynamics. This equation uses the representation of per capita\nworking efficiency in knowledge production as a function of knowledge amount.\nWe explore this function in detail and verify it on empirical material that\nincludes global demographic and information data. Knowledge can be represented\nin different forms such as patents, scientific and technical journal articles,\nand books of any genre. Knowledge is stored in various types of devices, which\ntogether form a global informational storage. Storage capacity is increasing\nrapidly as digital technology advances. The model is also applied to the\ndescription of this process. The model is in good agreement with the literature\ndata. The study performed allows us to deepen our understanding of the dynamics\nof civilization.",
            "author": [
                "Boris M. Dolgonosov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01147v1",
                "http://arxiv.org/pdf/2312.01147v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01141v1",
            "title": "On the Moser's Bernstein Theorem",
            "updated": "2023-12-02T13:59:25Z",
            "published": "2023-12-02T13:59:25Z",
            "summary": "In this paper, we prove the following version of the famous Bernstein's\ntheorem: Let $X\\subset \\mathbb R^{n+k}$ be a closed and connected set with\nHausdorff dimension $n$. Assume that $X$ satisfies the monotonicity formula at\n$p\\in X$. Then, the following statements are equivalent: (1) $X$ is an affine\nlinear subspace; (2) $X$ is a definable set that is Lipschitz regular at\ninfinity and its geometric tangent cone at infinity, $C(X,\\infty)$, is a linear\nsubspace; (3) $X$ is a definable set, blow-spherical regular at infinity and\n$C(X,\\infty)$ is a linear subspace; (4) $X$ is an LNE at infinity definable set\nand $C(X,\\infty)$ is a linear subspace.\n  As a consequence, we obtain a proof of the following generalization of\nBernstein's theorem: Let $X\\subset \\mathbb R^{n+1}$ be a closed and connected\nset with Hasdorff dimension $n$. Assume that $X$ satisfies the monotonicity\nformula at $p\\in X$ and there are compact sets $K\\subset \\mathbb{R}^n$ and\n$\\tilde K\\subset \\mathbb{R}^{n+1}$ such that $X\\setminus \\tilde K$ is a minimal\nhypersurface that is the graph of a $C^2$ smooth function $u\\colon\n\\mathbb{R}^{n}\\setminus K\\to \\mathbb{R}$. Assume that $u$ has bounded\nderivative whenever $n>7$. Then $X$ is a hyperplane. Several other consequences\nare also presented.",
            "author": [
                "Jos\u00e9 Edson Sampaio",
                "Euripedes Carvalho da Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01141v1",
                "http://arxiv.org/pdf/2312.01141v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "math.AG",
                "math.CV",
                "53A10, 53C42, 14J17, 14P10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01137v1",
            "title": "Fast and Robust Sparsity-Aware Block Diagonal Representation",
            "updated": "2023-12-02T13:44:27Z",
            "published": "2023-12-02T13:44:27Z",
            "summary": "The block diagonal structure of an affinity matrix is a commonly desired\nproperty in cluster analysis because it represents clusters of feature vectors\nby non-zero coefficients that are concentrated in blocks. However, recovering a\nblock diagonal affinity matrix is challenging in real-world applications, in\nwhich the data may be subject to outliers and heavy-tailed noise that obscure\nthe hidden cluster structure. To address this issue, we first analyze the\neffect of different fundamental outlier types in graph-based cluster analysis.\nA key idea that simplifies the analysis is to introduce a vector that\nrepresents a block diagonal matrix as a piece-wise linear function of the\nsimilarity coefficients that form the affinity matrix. We reformulate the\nproblem as a robust piece-wise linear fitting problem and propose a Fast and\nRobust Sparsity-Aware Block Diagonal Representation (FRS-BDR) method, which\njointly estimates cluster memberships and the number of blocks. Comprehensive\nexperiments on a variety of real-world applications demonstrate the\neffectiveness of FRS-BDR in terms of clustering accuracy, robustness against\ncorrupted features, computation time and cluster enumeration performance.",
            "author": [
                "Aylin Tastan",
                "Michael Muma",
                "Abdelhak M. Zoubir"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01137v1",
                "http://arxiv.org/pdf/2312.01137v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03751v1",
            "title": "Which linguistic cues make people fall for fake news? A comparison of\n  cognitive and affective processing",
            "updated": "2023-12-02T11:06:14Z",
            "published": "2023-12-02T11:06:14Z",
            "summary": "Fake news on social media has large, negative implications for society.\nHowever, little is known about what linguistic cues make people fall for fake\nnews and, hence, how to design effective countermeasures for social media. In\nthis study, we seek to understand which linguistic cues make people fall for\nfake news. Linguistic cues (e.g., adverbs, personal pronouns, positive emotion\nwords, negative emotion words) are important characteristics of any text and\nalso affect how people process real vs. fake news. Specifically, we compare the\nrole of linguistic cues across both cognitive processing (related to careful\nthinking) and affective processing (related to unconscious automatic\nevaluations). To this end, we performed a within-subject experiment where we\ncollected neurophysiological measurements of 42 subjects while these read a\nsample of 40 real and fake news articles. During our experiment, we measured\ncognitive processing through eye fixations, and affective processing in situ\nthrough heart rate variability. We find that users engage more in cognitive\nprocessing for longer fake news articles, while affective processing is more\npronounced for fake news written in analytic words. To the best of our\nknowledge, this is the first work studying the role of linguistic cues in fake\nnews processing. Altogether, our findings have important implications for\ndesigning online platforms that encourage users to engage in careful thinking\nand thus prevent them from falling for fake news.",
            "author": [
                "Bernhard Lutz",
                "Marc Adam",
                "Stefan Feuerriegel",
                "Nicolas Pr\u00f6llochs",
                "Dirk Neumann"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03751v1",
                "http://arxiv.org/pdf/2312.03751v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01105v1",
            "title": "S2P3: Self-Supervised Polarimetric Pose Prediction",
            "updated": "2023-12-02T10:46:40Z",
            "published": "2023-12-02T10:46:40Z",
            "summary": "This paper proposes the first self-supervised 6D object pose prediction from\nmultimodal RGB+polarimetric images. The novel training paradigm comprises 1) a\nphysical model to extract geometric information of polarized light, 2) a\nteacher-student knowledge distillation scheme and 3) a self-supervised loss\nformulation through differentiable rendering and an invertible physical\nconstraint. Both networks leverage the physical properties of polarized light\nto learn robust geometric representations by encoding shape priors and\npolarization characteristics derived from our physical model. Geometric\npseudo-labels from the teacher support the student network without the need for\nannotated real data. Dense appearance and geometric information of objects are\nobtained through a differentiable renderer with the predicted pose for\nself-supervised direct coupling. The student network additionally features our\nproposed invertible formulation of the physical shape priors that enables\nend-to-end self-supervised training through physical constraints of derived\npolarization characteristics compared against polarimetric input images. We\nspecifically focus on photometrically challenging objects with texture-less or\nreflective surfaces and transparent materials for which the most prominent\nperformance gain is reported.",
            "author": [
                "Patrick Ruhkamp",
                "Daoyi Gao",
                "Nassir Navab",
                "Benjamin Busam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01105v1",
                "http://arxiv.org/pdf/2312.01105v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01099v1",
            "title": "Rethinking Multiple Instance Learning for Whole Slide Image\n  Classification: A Bag-Level Classifier is a Good Instance-Level Teacher",
            "updated": "2023-12-02T10:16:03Z",
            "published": "2023-12-02T10:16:03Z",
            "summary": "Multiple Instance Learning (MIL) has demonstrated promise in Whole Slide\nImage (WSI) classification. However, a major challenge persists due to the high\ncomputational cost associated with processing these gigapixel images. Existing\nmethods generally adopt a two-stage approach, comprising a non-learnable\nfeature embedding stage and a classifier training stage. Though it can greatly\nreduce the memory consumption by using a fixed feature embedder pre-trained on\nother domains, such scheme also results in a disparity between the two stages,\nleading to suboptimal classification accuracy. To address this issue, we\npropose that a bag-level classifier can be a good instance-level teacher. Based\non this idea, we design Iteratively Coupled Multiple Instance Learning (ICMIL)\nto couple the embedder and the bag classifier at a low cost. ICMIL initially\nfix the patch embedder to train the bag classifier, followed by fixing the bag\nclassifier to fine-tune the patch embedder. The refined embedder can then\ngenerate better representations in return, leading to a more accurate\nclassifier for the next iteration. To realize more flexible and more effective\nembedder fine-tuning, we also introduce a teacher-student framework to\nefficiently distill the category knowledge in the bag classifier to help the\ninstance-level embedder fine-tuning. Thorough experiments were conducted on\nfour distinct datasets to validate the effectiveness of ICMIL. The experimental\nresults consistently demonstrate that our method significantly improves the\nperformance of existing MIL backbones, achieving state-of-the-art results. The\ncode is available at: https://github.com/Dootmaan/ICMIL/tree/confidence_based",
            "author": [
                "Hongyi Wang",
                "Luyang Luo",
                "Fang Wang",
                "Ruofeng Tong",
                "Yen-Wei Chen",
                "Hongjie Hu",
                "Lanfen Lin",
                "Hao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01099v1",
                "http://arxiv.org/pdf/2312.01099v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01093v1",
            "title": "Predicting Postoperative Nausea And Vomiting Using Machine Learning: A\n  Model Development and Validation Study",
            "updated": "2023-12-02T09:51:49Z",
            "published": "2023-12-02T09:51:49Z",
            "summary": "Background: Postoperative nausea and vomiting (PONV) is a frequently observed\ncomplication in patients undergoing surgery under general anesthesia. Moreover,\nit is a frequent cause of distress and dissatisfaction during the early\npostoperative period. The tools used for predicting PONV at present have not\nyielded satisfactory results. Therefore, prognostic tools for the prediction of\nearly and delayed PONV were developed in this study with the aim of achieving\nsatisfactory predictive performance.\n  Methods: The retrospective data of adult patients admitted to the\npost-anesthesia care unit after undergoing surgical procedures under general\nanesthesia at the Sheba Medical Center, Israel, between September 1, 2018, and\nSeptember 1, 2023, were used in this study. An ensemble model of machine\nlearning algorithms trained on the data of 54848 patients was developed. The\nk-fold cross-validation method was used followed by splitting the data to train\nand test sets that optimally preserve the sociodemographic features of the\npatients, such as age, sex, and smoking habits, using the Bee Colony algorithm.\n  Findings: Among the 54848 patients, early and delayed PONV were observed in\n2706 (4.93%) and 8218 (14.98%) patients, respectively. The proposed PONV\nprediction tools could correctly predict early and delayed PONV in 84.0% and\n77.3% of cases, respectively, outperforming the second-best PONV prediction\ntool (Koivuranta score) by 13.4% and 12.9%, respectively. Feature importance\nanalysis revealed that the performance of the proposed prediction tools aligned\nwith previous clinical knowledge, indicating their utility.\n  Interpretation: The machine learning-based tools developed in this study\nenabled improved PONV prediction, thereby facilitating personalized care and\nimproved patient outcomes.",
            "author": [
                "Maxim Glebov",
                "Teddy Lazebnik",
                "Boris Orkin",
                "Haim Berkenstadt",
                "Svetlana Bunimovich-Mendrazitsky"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01093v1",
                "http://arxiv.org/pdf/2312.01093v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01065v1",
            "title": "Scholarly Knowledge Graph Construction from Published Software Packages",
            "updated": "2023-12-02T08:28:08Z",
            "published": "2023-12-02T08:28:08Z",
            "summary": "The value of structured scholarly knowledge for research and society at large\nis well understood, but producing scholarly knowledge (i.e., knowledge\ntraditionally published in articles) in structured form remains a challenge. We\npropose an approach for automatically extracting scholarly knowledge from\npublished software packages by static analysis of their metadata and contents\n(scripts and data) and populating a scholarly knowledge graph with the\nextracted knowledge. Our approach is based on mining scientific software\npackages linked to article publications by extracting metadata and analyzing\nthe Abstract Syntax Tree (AST) of the source code to obtain information about\nthe used and produced data as well as operations performed on data. The\nresulting knowledge graph includes articles, software packages metadata, and\ncomputational techniques applied to input data utilized as materials in\nresearch work. The knowledge graph also includes the results reported as\nscholarly knowledge in articles.",
            "author": [
                "Muhammad Haris",
                "S\u00f6ren Auer",
                "Markus Stocker"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01065v1",
                "http://arxiv.org/pdf/2312.01065v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01053v1",
            "title": "End-to-End Speech-to-Text Translation: A Survey",
            "updated": "2023-12-02T07:40:32Z",
            "published": "2023-12-02T07:40:32Z",
            "summary": "Speech-to-text translation pertains to the task of converting speech signals\nin a language to text in another language. It finds its application in various\ndomains, such as hands-free communication, dictation, video lecture\ntranscription, and translation, to name a few. Automatic Speech Recognition\n(ASR), as well as Machine Translation(MT) models, play crucial roles in\ntraditional ST translation, enabling the conversion of spoken language in its\noriginal form to written text and facilitating seamless cross-lingual\ncommunication. ASR recognizes spoken words, while MT translates the transcribed\ntext into the target language. Such disintegrated models suffer from cascaded\nerror propagation and high resource and training costs. As a result,\nresearchers have been exploring end-to-end (E2E) models for ST translation.\nHowever, to our knowledge, there is no comprehensive review of existing works\non E2E ST. The present survey, therefore, discusses the work in this direction.\nOur attempt has been to provide a comprehensive review of models employed,\nmetrics, and datasets used for ST tasks, providing challenges and future\nresearch direction with new insights. We believe this review will be helpful to\nresearchers working on various applications of ST models.",
            "author": [
                "Nivedita Sethiya",
                "Chandresh Kumar Maurya"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01053v1",
                "http://arxiv.org/pdf/2312.01053v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02191v1",
            "title": "Prompt Tuning for Zero-shot Compositional Learning",
            "updated": "2023-12-02T07:32:24Z",
            "published": "2023-12-02T07:32:24Z",
            "summary": "Open World Compositional Zero-Shot Learning (OW-CZSL) is known to be an\nextremely challenging task, which aims to recognize unseen compositions formed\nfrom seen attributes and objects without any prior assumption of the output\nspace. In order to achieve this goal, a model has to be \"smart\" and\n\"knowledgeable\". To be smart, a model should be good at reasoning the\ninteractions between attributes and objects from the seen compositions. While\n\"knowledgeable\" means the model owns \"common sense\" to the open world that can\n\"foresee\" some features of the unseen compositions. Most previous work focuses\non the \"smart\" part, while few of them provided an effective solution to\nachieve the \"knowledgeable\" goal. In this paper, we proposed a framework named\nMulti-Modal Prompt Tuning (MMPT) to inherit the \"knowledgeable\" property from\nthe large pre-trained vision-language model. Extensive experiments show that\nour proposed MMPT obtains new state-of-the-art results in OW-CZSL task. On the\nUT-Zappos dataset, MMPT pushes the AUC score to $29.8$, while the previous best\nscore is $26.5$. On the more challenging MIT-States dataset, the AUC score of\nMMPT is 1.5 times better than the current state-of-the-art.",
            "author": [
                "Lingyu Zhang",
                "Ting Hua",
                "Yilin Shen",
                "Hongxia Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02191v1",
                "http://arxiv.org/pdf/2312.02191v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01044v1",
            "title": "Large Language Models Are Zero-Shot Text Classifiers",
            "updated": "2023-12-02T06:33:23Z",
            "published": "2023-12-02T06:33:23Z",
            "summary": "Retrained large language models (LLMs) have become extensively used across\nvarious sub-disciplines of natural language processing (NLP). In NLP, text\nclassification problems have garnered considerable focus, but still faced with\nsome limitations related to expensive computational cost, time consumption, and\nrobust performance to unseen classes. With the proposal of chain of thought\nprompting (CoT), LLMs can be implemented using zero-shot learning (ZSL) with\nthe step by step reasoning prompts, instead of conventional question and answer\nformats. The zero-shot LLMs in the text classification problems can alleviate\nthese limitations by directly utilizing pretrained models to predict both seen\nand unseen classes. Our research primarily validates the capability of GPT\nmodels in text classification. We focus on effectively utilizing prompt\nstrategies to various text classification scenarios. Besides, we compare the\nperformance of zero shot LLMs with other state of the art text classification\nmethods, including traditional machine learning methods, deep learning methods,\nand ZSL methods. Experimental results demonstrate that the performance of LLMs\nunderscores their effectiveness as zero-shot text classifiers in three of the\nfour datasets analyzed. The proficiency is especially advantageous for small\nbusinesses or teams that may not have extensive knowledge in text\nclassification.",
            "author": [
                "Zhiqiang Wang",
                "Yiran Pang",
                "Yanbin Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01044v1",
                "http://arxiv.org/pdf/2312.01044v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01040v1",
            "title": "From Beginner to Expert: Modeling Medical Knowledge into General LLMs",
            "updated": "2023-12-02T05:54:06Z",
            "published": "2023-12-02T05:54:06Z",
            "summary": "Recently, large language model (LLM) based artificial intelligence (AI)\nsystems have demonstrated remarkable capabilities in natural language\nunderstanding and generation. However, these models face a significant\nchallenge when it comes to sensitive applications, such as reasoning over\nmedical knowledge and answering medical questions in a physician-like manner.\nPrior studies attempted to overcome this challenge by increasing the model size\n(>100B) to learn more general medical knowledge, while there is still room for\nimprovement in LLMs with smaller-scale model sizes (<100B). In this work, we\nstart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a\nmedical beginner towards a medical expert (called AntGLM-Med-10B), which\nleverages a 3-stage optimization procedure, \\textit{i.e.}, general medical\nknowledge injection, medical domain instruction tuning, and specific medical\ntask adaptation. Our contributions are threefold: (1) We specifically\ninvestigate how to adapt a pre-trained general LLM in medical domain,\nespecially for a specific medical task. (2) We collect and construct\nlarge-scale medical datasets for each stage of the optimization process. These\ndatasets encompass various data types and tasks, such as question-answering,\nmedical reasoning, multi-choice questions, and medical conversations. (3)\nSpecifically for multi-choice questions in the medical domain, we propose a\nnovel Verification-of-Choice approach for prompting engineering, which\nsignificantly enhances the reasoning ability of LLMs. Remarkably, by combining\nthe above approaches, our AntGLM-Med-10B model can outperform the most of LLMs\non PubMedQA, including both general and medical LLMs, even when these LLMs have\nlarger model size.",
            "author": [
                "Qiang Li",
                "Xiaoyan Yang",
                "Haowen Wang",
                "Qin Wang",
                "Lei Liu",
                "Junjie Wang",
                "Yang Zhang",
                "Mingyuan Chu",
                "Sen Hu",
                "Yicheng Chen",
                "Yue Shen",
                "Cong Fan",
                "Wangshu Zhang",
                "Teng Xu",
                "Jinjie Gu",
                "Jing Zheng",
                "Guannan Zhang Ant Group"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01040v1",
                "http://arxiv.org/pdf/2312.01040v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01037v1",
            "title": "Eliciting Latent Knowledge from Quirky Language Models",
            "updated": "2023-12-02T05:47:22Z",
            "published": "2023-12-02T05:47:22Z",
            "summary": "Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's\nactivations which robustly track the true state of the world, even when the\nnetwork's overt output is false or misleading. To further ELK research, we\nintroduce a suite of \"quirky\" language models that are LoRA finetuned to make\nsystematic errors when answering math questions if and only if the keyword\n\"Bob\" is present in the prompt. We demonstrate that simple probing methods can\nelicit the model's latent knowledge of the correct answer in these contexts,\neven for problems harder than those the probe was trained on. We then compare\nELK probing methods and find that a simple difference-in-means classifier\ngeneralizes best. We also find that a mechanistic anomaly detection approach\ncan flag untruthful behavior with upwards of 99% AUROC. Our results show\npromise for eliciting superhuman knowledge from capable models, and we aim to\nfacilitate future research that expands on our findings, employing more diverse\nand challenging datasets.",
            "author": [
                "Alex Mallen",
                "Nora Belrose"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01037v1",
                "http://arxiv.org/pdf/2312.01037v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01028v1",
            "title": "A structure theorem for pseudo-segments and its applications",
            "updated": "2023-12-02T04:37:15Z",
            "published": "2023-12-02T04:37:15Z",
            "summary": "We prove a far-reaching strengthening of Szemer\\'edi's regularity lemma for\nintersection graphs of pseudo-segments. It shows that the vertex set of such a\ngraph can be partitioned into a bounded number of parts of roughly the same\nsize such that almost all bipartite graphs between different pairs of parts are\ncomplete or empty. We use this to get an improved bound on disjoint edges in\nsimple topological graphs, showing that every $n$-vertex simple topological\ngraph with no $k$ pairwise disjoint edges has at most $n(\\log n)^{O(\\log k)}$\nedges.",
            "author": [
                "Jacob Fox",
                "Janos Pach",
                "Andrew Suk"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01028v1",
                "http://arxiv.org/pdf/2312.01028v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01025v1",
            "title": "Adding Domain Knowledge to Query-Driven Learned Databases",
            "updated": "2023-12-02T04:21:29Z",
            "published": "2023-12-02T04:21:29Z",
            "summary": "In recent years, \\emph{learned cardinality estimation} has emerged as an\nalternative to traditional query optimization methods: by training machine\nlearning models over observed query performance, learned cardinality estimation\ntechniques can accurately predict query cardinalities and costs -- accounting\nfor skew, correlated predicates, and many other factors that traditional\nmethods struggle to capture. However, query-driven learned cardinality\nestimators are dependent on sample workloads, requiring vast amounts of labeled\nqueries. Further, we show that state-of-the-art query-driven techniques can\nmake significant and unpredictable errors on queries that are outside the\ndistribution of their training set. We show that these out-of-distribution\nerrors can be mitigated by incorporating the \\emph{domain knowledge} used in\ntraditional query optimizers: \\emph{constraints} on values and cardinalities\n(e.g., based on key-foreign-key relationships, range predicates, and more\ngenerally on inclusion and functional dependencies). We develop methods for\n\\emph{semi-supervised} query-driven learned query optimization, based on\nconstraints, and we experimentally demonstrate that such techniques can\nincrease a learned query optimizer's accuracy in cardinality estimation, reduce\nthe reliance on massive labeled queries, and improve the robustness of query\nend-to-end performance.",
            "author": [
                "Peizhi Wu",
                "Ryan Marcus",
                "Zachary G. Ives"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01025v1",
                "http://arxiv.org/pdf/2312.01025v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.01006v1",
            "title": "Dual-Teacher De-biasing Distillation Framework for Multi-domain Fake\n  News Detection",
            "updated": "2023-12-02T02:53:45Z",
            "published": "2023-12-02T02:53:45Z",
            "summary": "Multi-domain fake news detection aims to identify whether various news from\ndifferent domains is real or fake and has become urgent and important. However,\nexisting methods are dedicated to improving the overall performance of fake\nnews detection, ignoring the fact that unbalanced data leads to disparate\ntreatment for different domains, i.e., the domain bias problem. To solve this\nproblem, we propose the Dual-Teacher De-biasing Distillation framework (DTDBD)\nto mitigate bias across different domains. Following the knowledge distillation\nmethods, DTDBD adopts a teacher-student structure, where pre-trained large\nteachers instruct a student model. In particular, the DTDBD consists of an\nunbiased teacher and a clean teacher that jointly guide the student model in\nmitigating domain bias and maintaining performance. For the unbiased teacher,\nwe introduce an adversarial de-biasing distillation loss to instruct the\nstudent model in learning unbiased domain knowledge. For the clean teacher, we\ndesign domain knowledge distillation loss, which effectively incentivizes the\nstudent model to focus on representing domain features while maintaining\nperformance. Moreover, we present a momentum-based dynamic adjustment algorithm\nto trade off the effects of two teachers. Extensive experiments on Chinese and\nEnglish datasets show that the proposed method substantially outperforms the\nstate-of-the-art baseline methods in terms of bias metrics while guaranteeing\ncompetitive performance.",
            "author": [
                "Jiayang Li",
                "Xuan Feng",
                "Tianlong Gu",
                "Liang Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.01006v1",
                "http://arxiv.org/pdf/2312.01006v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00997v1",
            "title": "Scaling Whole-Chip QAOA for Higher-Order Ising Spin Glass Models on\n  Heavy-Hex Graphs",
            "updated": "2023-12-02T01:47:05Z",
            "published": "2023-12-02T01:47:05Z",
            "summary": "We show through numerical simulation that the Quantum Alternating Operator\nAnsatz (QAOA) for higher-order, random-coefficient, heavy-hex compatible spin\nglass Ising models has strong parameter concentration across problem sizes from\n$16$ up to $127$ qubits for $p=1$ up to $p=5$, which allows for\nstraight-forward transfer learning of QAOA angles on instance sizes where\nexhaustive grid-search is prohibitive even for $p>1$. We use Matrix Product\nState (MPS) simulation at different bond dimensions to obtain confidence in\nthese results, and we obtain the optimal solutions to these combinatorial\noptimization problems using CPLEX. In order to assess the ability of current\nnoisy quantum hardware to exploit such parameter concentration, we execute\nshort-depth QAOA circuits (with a CNOT depth of 6 per $p$, resulting in\ncircuits which contain $1420$ two qubit gates for $127$ qubit $p=5$ QAOA) on\n$100$ higher-order (cubic term) Ising models on IBM quantum superconducting\nprocessors with $16, 27, 127$ qubits using QAOA angles learned from a single\n$16$-qubit instance. We show that (i) the best quantum processors generally\nfind lower energy solutions up to $p=3$ for 27 qubit systems and up to $p=2$\nfor 127 qubit systems and are overcome by noise at higher values of $p$, (ii)\nthe best quantum processors find mean energies that are about a factor of two\noff from the noise-free numerical simulation results. Additional insights from\nour experiments are that large performance differences exist among different\nquantum processors even of the same generation and that dynamical decoupling\nsignificantly improve performance for some, but decrease performance for other\nquantum processors. Lastly we show $p=1$ QAOA angle mean energy landscapes\ncomputed using up to a $414$ qubit quantum computer, showing that the mean QAOA\nenergy landscapes remain very similar as the problem size changes.",
            "author": [
                "Elijah Pelofske",
                "Andreas B\u00e4rtschi",
                "Lukasz Cincio",
                "John Golden",
                "Stephan Eidenbenz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00997v1",
                "http://arxiv.org/pdf/2312.00997v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.dis-nn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00979v1",
            "title": "Recoloring some hereditary graph classes",
            "updated": "2023-12-02T00:26:36Z",
            "published": "2023-12-02T00:26:36Z",
            "summary": "The reconfiguration graph of the $k$-colorings, denoted $R_k(G)$, is the\ngraph whose vertices are the $k$-colorings of $G$ and two colorings are\nadjacent in $R_k(G)$ if they differ in color on exactly one vertex. A graph $G$\nis said to be recolorable if $R_{\\ell}(G)$ is connected for all $\\ell\\geq\n\\chi(G)$+1. In this paper, we study the recolorability of several graph classes\nrestricted by forbidden induced subgraphs. We prove some properties of a\nvertex-minimal graph $G$ which is not recolorable. We show that every\n(triangle, $H$)-free graph is recolorable if and only if every (paw, $H$)-free\ngraph is recolorable. Every graph in the class of $(2K_2,\\ H)$-free graphs,\nwhere $H$ is a 4-vertex graph except $P_4$ or $P_3$+$P_1$, is recolorable if\n$H$ is either a triangle, paw, claw, or a diamond. Furthermore, we prove that\nthe class of ($P_5$, $C_5$, house, co-banner)-free graphs are recolorable.",
            "author": [
                "Manoj Belavadi",
                "Kathie Cameron"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00979v1",
                "http://arxiv.org/pdf/2312.00979v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02188v1",
            "title": "Video Summarization: Towards Entity-Aware Captions",
            "updated": "2023-12-01T23:56:00Z",
            "published": "2023-12-01T23:56:00Z",
            "summary": "Existing popular video captioning benchmarks and models deal with generic\ncaptions devoid of specific person, place or organization named entities. In\ncontrast, news videos present a challenging setting where the caption requires\nsuch named entities for meaningful summarization. As such, we propose the task\nof summarizing news video directly to entity-aware captions. We also release a\nlarge-scale dataset, VIEWS (VIdeo NEWS), to support research on this task.\nFurther, we propose a method that augments visual information from videos with\ncontext retrieved from external world knowledge to generate entity-aware\ncaptions. We demonstrate the effectiveness of our approach on three video\ncaptioning models. We also show that our approach generalizes to existing news\nimage captions dataset. With all the extensive experiments and insights, we\nbelieve we establish a solid basis for future research on this challenging\ntask.",
            "author": [
                "Hammad A. Ayyubi",
                "Tianqi Liu",
                "Arsha Nagrani",
                "Xudong Lin",
                "Mingda Zhang",
                "Anurag Arnab",
                "Feng Han",
                "Yukun Zhu",
                "Jialu Liu",
                "Shih-Fu Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02188v1",
                "http://arxiv.org/pdf/2312.02188v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00968v1",
            "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of\n  Low-rank Experts",
            "updated": "2023-12-01T23:04:27Z",
            "published": "2023-12-01T23:04:27Z",
            "summary": "Large multi-modal models (LMMs) exhibit remarkable performance across\nnumerous tasks. However, generalist LMMs often suffer from performance\ndegradation when tuned over a large collection of tasks. Recent research\nsuggests that Mixture of Experts (MoE) architectures are useful for instruction\ntuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost\nof replicating and storing the expert models severely limits the number of\nexperts we can use. We propose Omni-SMoLA, an architecture that uses the Soft\nMoE approach to (softly) mix many multimodal low rank experts, and avoids\nintroducing a significant number of new parameters compared to conventional MoE\nmodels. The core intuition here is that the large model provides a foundational\nbackbone, while different lightweight experts residually learn specialized\nknowledge, either per-modality or multimodally. Extensive experiments\ndemonstrate that the SMoLA approach helps improve the generalist performance\nacross a broad range of generative vision-and-language tasks, achieving new\nSoTA generalist performance that often matches or outperforms single\nspecialized LMM baselines, as well as new SoTA specialist performance.",
            "author": [
                "Jialin Wu",
                "Xia Hu",
                "Yaqing Wang",
                "Bo Pang",
                "Radu Soricut"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00968v1",
                "http://arxiv.org/pdf/2312.00968v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00966v1",
            "title": "Spectral Temporal Contrastive Learning",
            "updated": "2023-12-01T22:48:52Z",
            "published": "2023-12-01T22:48:52Z",
            "summary": "Learning useful data representations without requiring labels is a\ncornerstone of modern deep learning. Self-supervised learning methods,\nparticularly contrastive learning (CL), have proven successful by leveraging\ndata augmentations to define positive pairs. This success has prompted a number\nof theoretical studies to better understand CL and investigate theoretical\nbounds for downstream linear probing tasks. This work is concerned with the\ntemporal contrastive learning (TCL) setting where the sequential structure of\nthe data is used instead to define positive pairs, which is more commonly used\nin RL and robotics contexts. In this paper, we adapt recent work on Spectral CL\nto formulate Spectral Temporal Contrastive Learning (STCL). We discuss a\npopulation loss based on a state graph derived from a time-homogeneous\nreversible Markov chain with uniform stationary distribution. The STCL loss\nenables to connect the linear probing performance to the spectral properties of\nthe graph, and can be estimated by considering previously observed data\nsequences as an ensemble of MCMC chains.",
            "author": [
                "Sacha Morin",
                "Somjit Nath",
                "Samira Ebrahimi Kahou",
                "Guy Wolf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00966v1",
                "http://arxiv.org/pdf/2312.00966v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00964v1",
            "title": "Permutation Entropy for Signal Analysis",
            "updated": "2023-12-01T22:42:02Z",
            "published": "2023-12-01T22:42:02Z",
            "summary": "Shannon Entropy is the preeminent tool for measuring the level of uncertainty\n(and conversely, information content) in a random variable. In the field of\ncommunications, entropy can be used to express the information content of given\nsignals (represented as time series) by considering random variables which\nsample from specified subsequences. In this paper, we will discuss how an\nentropy variant, the \\textit{permutation entropy} can be used to study and\nclassify radio frequency signals in a noisy environment. The permutation\nentropy is the entropy of the random variable which samples occurrences of\npermutation patterns from time series given a fixed window length, making it a\nfunction of the distribution of permutation patterns. Since the permutation\nentropy is a function of the relative order of data, it is (global) amplitude\nagnostic and thus allows for comparison between signals at different scales.\nThis article is intended to describe a permutation patterns approach to a data\ndriven problem in radio frequency communications research, and includes a\nprimer on all non-permutation pattern specific background. An empirical\nanalysis of the methods herein on radio frequency data is included. No prior\nknowledge of signals analysis is assumed, and permutation pattern specific\nnotation will be included. This article serves as a self-contained introduction\nto the relationship between permutation patterns, entropy, and signals analysis\nfor studying radio frequency signals and includes results on a classification\ntask.",
            "author": [
                "Bill Kay",
                "Audun Myers",
                "Thad Boydston",
                "Emily Ellwein",
                "Cameron Mackenzie",
                "Erik Lentz"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00964v1",
                "http://arxiv.org/pdf/2312.00964v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.DM",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00960v1",
            "title": "The Cost of Compression: Investigating the Impact of Compression on\n  Parametric Knowledge in Language Models",
            "updated": "2023-12-01T22:27:12Z",
            "published": "2023-12-01T22:27:12Z",
            "summary": "Compressing large language models (LLMs), often consisting of billions of\nparameters, provides faster inference, smaller memory footprints, and enables\nlocal deployment. Two standard compression techniques are pruning and\nquantization, with the former eliminating redundant connections in model layers\nand the latter representing model parameters with fewer bits. The key tradeoff\nis between the degree of compression and the impact on the quality of the\ncompressed model. Existing research on LLM compression primarily focuses on\nperformance in terms of general metrics like perplexity or downstream task\naccuracy. More fine-grained metrics, such as those measuring parametric\nknowledge, remain significantly underexplored. To help bridge this gap, we\npresent a comprehensive analysis across multiple model families (ENCODER,\nENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order\nto systematically quantify the effect of commonly employed compression\ntechniques on model performance. A particular focus is on tradeoffs involving\nparametric knowledge, with the goal of providing practitioners with practical\ninsights to help make informed decisions on compression. We release our\ncodebase1 to enable further research.",
            "author": [
                "Satya Sai Srinath Namburi",
                "Makesh Sreedhar",
                "Srinath Srinivasan",
                "Frederic Sala"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00960v1",
                "http://arxiv.org/pdf/2312.00960v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00928v1",
            "title": "The Hat Guessing Number of Cactus Graphs and Cycles",
            "updated": "2023-12-01T21:08:02Z",
            "published": "2023-12-01T21:08:02Z",
            "summary": "We study the hat guessing game on graphs. In this game, a player is placed on\neach vertex $v$ of a graph $G$ and assigned a colored hat from $h(v)$ possible\ncolors. Each player makes a deterministic guess on their hat color based on the\ncolors assigned to the players on neighboring vertices, and the players win if\nat least one player correctly guesses his assigned color. If there exists a\nstrategy that ensures at least one player guesses correctly for every possible\nassignment of colors, the game defined by $\\langle G,h\\rangle$ is called\nwinning. The hat guessing number of $G$ is the largest integer $q$ so that if\n$h(v)=q$ for all $v\\in G$ then $\\langle G,h\\rangle$ is winning.\n  In this note, we determine whether $\\langle G,h\\rangle $ is winning for any\n$h$ whenever $G$ is a cycle, resolving a conjecture of Kokhas and Latyshev in\nthe affirmative and extending it. We then use this result to determine the hat\nguessing number of every cactus graph, graphs in which every pair of cycles\nshare at most one vertex.",
            "author": [
                "Jeremy Chizewer",
                "I. M. J. McInnis",
                "Mehrdad Sohrabi",
                "Shriya Kaistha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00928v1",
                "http://arxiv.org/pdf/2312.00928v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C57"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00922v1",
            "title": "List majority edge-colorings of graphs",
            "updated": "2023-12-01T20:51:55Z",
            "published": "2023-12-01T20:51:55Z",
            "summary": "A majority edge-coloring of a graph without pendant edges is a coloring of\nits edges such that, for every vertex $v$ and every color $\\alpha$, there are\nat most as many edges incident to $v$ colored with $\\alpha$ as with all other\ncolors. We extend some known results for finite graphs to infinite graphs,\nmostly in the list setting. In particular, we prove that every infinite graph\nwithout pendant edges has a majority edge-coloring from lists of size $4$.\nAnother interesting result states that every infinite graph without vertices of\nfinite odd degrees admits a majority edge-coloring from lists of size $2$. We\nformulate two conjectures. As a consequence of our results, we prove that line\ngraphs of any cardinality admit majority vertex-colorings from lists of size 2,\nthus confirming the Unfriendly Partition Conjecture for line graphs.",
            "author": [
                "Rafa\u0142 Kalinowski",
                "Monika Pil\u015bniak",
                "Marcin Stawiski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00922v1",
                "http://arxiv.org/pdf/2312.00922v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00894v1",
            "title": "Leveraging Large Language Models to Improve REST API Testing",
            "updated": "2023-12-01T19:53:23Z",
            "published": "2023-12-01T19:53:23Z",
            "summary": "The widespread adoption of REST APIs, coupled with their growing complexity\nand size, has led to the need for automated REST API testing tools. Current\ntesting tools focus on the structured data in REST API specifications but often\nneglect valuable insights available in unstructured natural-language\ndescriptions in the specifications, which leads to suboptimal test coverage.\nRecently, to address this gap, researchers have developed techniques that\nextract rules from these human-readable descriptions and query knowledge bases\nto derive meaningful input values. However, these techniques are limited in the\ntypes of rules they can extract and can produce inaccurate results. This paper\npresents RESTGPT, an innovative approach that leverages the power and intrinsic\ncontext-awareness of Large Language Models (LLMs) to improve REST API testing.\nRESTGPT takes as input an API specification, extracts machine-interpretable\nrules, and generates example parameter values from natural-language\ndescriptions in the specification. It then augments the original specification\nwith these rules and values. Our preliminary evaluation suggests that RESTGPT\noutperforms existing techniques in both rule extraction and value generation.\nGiven these encouraging results, we outline future research directions for\nleveraging LLMs more broadly for improving REST API testing.",
            "author": [
                "Myeongsoo Kim",
                "Tyler Stennett",
                "Dhruv Shah",
                "Saurabh Sinha",
                "Alessandro Orso"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00894v1",
                "http://arxiv.org/pdf/2312.00894v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00874v1",
            "title": "Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs\n  in Language Pretraining",
            "updated": "2023-12-01T19:03:38Z",
            "published": "2023-12-01T19:03:38Z",
            "summary": "The knowledge graph is a structure to store and represent knowledge, and\nrecent studies have discussed its capability to assist language models for\nvarious applications. Some variations of knowledge graphs aim to record\narguments and their relations for computational argumentation tasks. However,\nmany must simplify semantic types to fit specific schemas, thus losing\nflexibility and expression ability. In this paper, we propose the Hierarchical\nArgumentation Graph (Hi-ArG), a new structure to organize arguments. We also\nintroduce two approaches to exploit Hi-ArG, including a text-graph multi-modal\nmodel GreaseArG and a new pre-training framework augmented with graph\ninformation. Experiments on two argumentation tasks have shown that after\nfurther pre-training and fine-tuning, GreaseArG supersedes same-scale language\nmodels on these tasks, while incorporating graph information during further\npre-training can also improve the performance of vanilla language models. Code\nfor this paper is available at https://github.com/ljcleo/Hi-ArG .",
            "author": [
                "Jingcong Liang",
                "Rong Ye",
                "Meng Han",
                "Qi Zhang",
                "Ruofei Lai",
                "Xinyu Zhang",
                "Zhao Cao",
                "Xuanjing Huang",
                "Zhongyu Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00874v1",
                "http://arxiv.org/pdf/2312.00874v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00785v1",
            "title": "Sequential Modeling Enables Scalable Learning for Large Vision Models",
            "updated": "2023-12-01T18:59:57Z",
            "published": "2023-12-01T18:59:57Z",
            "summary": "We introduce a novel sequential modeling approach which enables learning a\nLarge Vision Model (LVM) without making use of any linguistic data. To do this,\nwe define a common format, \"visual sentences\", in which we can represent raw\nimages and videos as well as annotated data sources such as semantic\nsegmentations and depth reconstructions without needing any meta-knowledge\nbeyond the pixels. Once this wide variety of visual data (comprising 420\nbillion tokens) is represented as sequences, the model can be trained to\nminimize a cross-entropy loss for next token prediction. By training across\nvarious scales of model architecture and data diversity, we provide empirical\nevidence that our models scale effectively. Many different vision tasks can be\nsolved by designing suitable visual prompts at test time.",
            "author": [
                "Yutong Bai",
                "Xinyang Geng",
                "Karttikeya Mangalam",
                "Amir Bar",
                "Alan Yuille",
                "Trevor Darrell",
                "Jitendra Malik",
                "Alexei A Efros"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00785v1",
                "http://arxiv.org/pdf/2312.00785v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00778v1",
            "title": "MorpheuS: Neural Dynamic 360\u00b0 Surface Reconstruction from Monocular\n  RGB-D Video",
            "updated": "2023-12-01T18:55:53Z",
            "published": "2023-12-01T18:55:53Z",
            "summary": "Neural rendering has demonstrated remarkable success in dynamic scene\nreconstruction. Thanks to the expressiveness of neural representations, prior\nworks can accurately capture the motion and achieve high-fidelity\nreconstruction of the target object. Despite this, real-world video scenarios\noften feature large unobserved regions where neural representations struggle to\nachieve realistic completion. To tackle this challenge, we introduce MorpheuS,\na framework for dynamic 360{\\deg} surface reconstruction from a casually\ncaptured RGB-D video. Our approach models the target scene as a canonical field\nthat encodes its geometry and appearance, in conjunction with a deformation\nfield that warps points from the current frame to the canonical space. We\nleverage a view-dependent diffusion prior and distill knowledge from it to\nachieve realistic completion of unobserved regions. Experimental results on\nvarious real-world and synthetic datasets show that our method can achieve\nhigh-fidelity 360{\\deg} surface reconstruction of a deformable object from a\nmonocular RGB-D video.",
            "author": [
                "Hengyi Wang",
                "Jingwen Wang",
                "Lourdes Agapito"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00778v1",
                "http://arxiv.org/pdf/2312.00778v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00774v1",
            "title": "Context Retrieval via Normalized Contextual Latent Interaction for\n  Conversational Agent",
            "updated": "2023-12-01T18:53:51Z",
            "published": "2023-12-01T18:53:51Z",
            "summary": "Conversational agents leveraging AI, particularly deep learning, are emerging\nin both academic research and real-world applications. However, these\napplications still face challenges, including disrespecting knowledge and\nfacts, not personalizing to user preferences, and enormous demand for\ncomputational resources during training and inference. Recent research efforts\nhave been focused on addressing these challenges from various aspects,\nincluding supplementing various types of auxiliary information to the\nconversational agents. However, existing methods are still not able to\neffectively and efficiently exploit relevant information from these auxiliary\nsupplements to further unleash the power of the conversational agents and the\nlanguage models they use. In this paper, we present a novel method, PK-NCLI,\nthat is able to accurately and efficiently identify relevant auxiliary\ninformation to improve the quality of conversational responses by learning the\nrelevance among persona, chat history, and knowledge background through\nlow-level normalized contextual latent interaction. Our experimental results\nindicate that PK-NCLI outperforms the state-of-the-art method, PK-FoCus, by\n47.80%/30.61%/24.14% in terms of perplexity, knowledge grounding, and training\nefficiency, respectively, and maintained the same level of persona grounding\nperformance. We also provide a detailed analysis of how different factors,\nincluding language model choices and trade-offs on training weights, would\naffect the performance of PK-NCLI.",
            "author": [
                "Junfeng Liu",
                "Zhuocheng Mei",
                "Kewen Peng",
                "Ranga Raju Vatsavai"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00774v1",
                "http://arxiv.org/pdf/2312.00774v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00862v1",
            "title": "From Spin States to Socially Integrated Ising Models: Proposed\n  Applications of Graph States, Stabilizer States, Toric States to Opinion\n  Dynamics",
            "updated": "2023-12-01T18:19:43Z",
            "published": "2023-12-01T18:19:43Z",
            "summary": "Recent research has developed the Ising model from physics, especially\nstatistical mechanics, and it plays an important role in quantum computing,\nespecially quantum annealing and quantum Monte Carlo methods. The model has\nalso been used in opinion dynamics as a powerful tool for simulating social\ninteractions and opinion formation processes. Individual opinions and\npreferences correspond to spin states, and social pressure and communication\ndynamics are modeled through interactions between spins. Quantum computing\nmakes it possible to efficiently simulate these interactions and analyze more\ncomplex social networks.Recent research has incorporated concepts from quantum\ninformation theory such as Graph State, Stabilizer State, and Surface Code (or\nToric Code) into models of opinion dynamics. The incorporation of these\nconcepts allows for a more detailed analysis of the process of opinion\nformation and the dynamics of social networks. The concepts lie at the\nintersection of graph theory and quantum theory, and the use of Graph State in\nopinion dynamics can represent the interdependence of opinions and networks of\ninfluence among individuals. It helps to represent the local stability of\nopinions and the mechanisms for correcting misunderstandings within a social\nnetwork. It allows us to understand how individual opinions are subject to\nsocial pressures and cultural influences and how they change over\ntime.Incorporating these quantum theory concepts into opinion dynamics allows\nfor a deeper understanding of social interactions and opinion formation\nprocesses. Moreover, these concepts can provide new insights not only in the\nsocial sciences, but also in fields as diverse as political science, economics,\nmarketing, and urban planning.",
            "author": [
                "Yasuko Kawahata"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00862v1",
                "http://arxiv.org/pdf/2312.00862v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00741v1",
            "title": "Crystal: Enhancing Blockchain Mining Transparency with Quorum\n  Certificate",
            "updated": "2023-12-01T17:24:28Z",
            "published": "2023-12-01T17:24:28Z",
            "summary": "Researchers have discovered a series of theoretical attacks against Bitcoin's\nNakamoto consensus; the most damaging ones are selfish mining, double-spending,\nand consistency delay attacks. These attacks have one common cause: block\nwithholding. This paper proposes Crystal, which leverages quorum certificates\nto resist block withholding misbehavior. Crystal continuously elects committees\nfrom miners and requires each block to have a quorum certificate, i.e., a set\nof signatures issued by members of its committee. Consequently, an attacker has\nto publish its blocks to obtain quorum certificates, rendering block\nwithholding impossible. To build Crystal, we design a novel two-round committee\nelection in a Sybil-resistant, unpredictable and non-interactive way, and a\nreward mechanism to incentivize miners to follow the protocol. Our analysis and\nevaluations show that Crystal can significantly mitigate selfish mining and\ndouble-spending attacks. For example, in Bitcoin, an attacker with 30% of the\ntotal computation power will succeed in double-spending attacks with a\nprobability of 15.6% to break the 6-confirmation rule; however, in Crystal, the\nsuccess probability for the same attacker falls to 0.62%. We provide formal\nend-to-end safety proofs for Crystal, ensuring no unknown attacks will be\nintroduced. To the best of our knowledge, Crystal is the first protocol that\nprevents selfish mining and double-spending attacks while providing safety\nproof.",
            "author": [
                "Jianyu Niu",
                "Fangyu Gai",
                "Runchao Han",
                "Ren Zhang",
                "Yinqian Zhang",
                "Chen Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00741v1",
                "http://arxiv.org/pdf/2312.00741v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00859v1",
            "title": "Random Walks Performed by Topologically-Specific Agents on Complex\n  Networks",
            "updated": "2023-12-01T17:09:44Z",
            "published": "2023-12-01T17:09:44Z",
            "summary": "Random walks by single-node agents have been systematically conducted on\nvarious types of complex networks in order to investigate how their topologies\ncan affect the dynamics of the agents. However, by fitting any network node,\nthese agents do not engage in topological interactions with the network. In the\npresent work, we describe random walks on complex networks performed by agents\nthat are actually small graphs. These agents can only occupy admissible\nportions of the network onto which they fit topologically, hence their name\nbeing taken as topologically-specific agents. These agents are also allowed to\nmove to adjacent subgraphs in the network, which have each node adjacent to the\noriginal respective node of the agent. Two types of random walks are considered\nhere: uniformly random and influenced by an external field. The performance of\nthe random walks performed by three types of topologically-specific agents is\nstudied respectively to the obtained coverage considering three types of\ncomplex networks (geometrical, Erd\\H{o}s-R\\'enyi, and Barab\\'asi-Albert). The\nnumber of nodes displaced at each random walk step is also obtained and\nanalyzed. Several interesting results are reported and discussed, including the\nfact that, despite its intrinsic node degree heterogeneity, Barab\\'asi-Albert\nnetworks tend to allow relatively smooth and effective coverage by all the\nconsidered topologically-specific agents. Erd\\H{o}s-R\\'enyi networks were also\nfound to yield large dispersions of node coverage. In addition, the triangle\nagent was found to allow more effective random walks respectively to any of the\nthree considered networks.",
            "author": [
                "Alexandre Benatti",
                "Luciano da F. Costa"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00859v1",
                "http://arxiv.org/pdf/2312.00859v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00710v2",
            "title": "SpaCE: The Spatial Confounding Environment",
            "updated": "2023-12-06T02:00:53Z",
            "published": "2023-12-01T16:42:57Z",
            "summary": "Spatial confounding poses a significant challenge in scientific studies\ninvolving spatial data, where unobserved spatial variables can influence both\ntreatment and outcome, possibly leading to spurious associations. To address\nthis problem, we introduce SpaCE: The Spatial Confounding Environment, the\nfirst toolkit to provide realistic benchmark datasets and tools for\nsystematically evaluating causal inference methods designed to alleviate\nspatial confounding. Each dataset includes training data, true counterfactuals,\na spatial graph with coordinates, and smoothness and confounding scores\ncharacterizing the effect of a missing spatial confounder. It also includes\nrealistic semi-synthetic outcomes and counterfactuals, generated using\nstate-of-the-art machine learning ensembles, following best practices for\ncausal inference benchmarks. The datasets cover real treatment and covariates\nfrom diverse domains, including climate, health and social sciences. SpaCE\nfacilitates an automated end-to-end pipeline, simplifying data loading,\nexperimental setup, and evaluating machine learning and causal inference\nmodels. The SpaCE project provides several dozens of datasets of diverse sizes\nand spatial complexity. It is publicly available as a Python package,\nencouraging community feedback and contributions.",
            "author": [
                "Mauricio Tec",
                "Ana Trisovic",
                "Michelle Audirac",
                "Sophie Woodward",
                "Jie Kate Hu",
                "Naeem Khoshnevis",
                "Francesca Dominici"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00710v2",
                "http://arxiv.org/pdf/2312.00710v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00701v1",
            "title": "Rigidity Results for large displacement quotients of mapping class\n  groups",
            "updated": "2023-12-01T16:36:26Z",
            "published": "2023-12-01T16:36:26Z",
            "summary": "We consider quotients of mapping class groups of orientable, finite type\nsurfaces by subgroups whose action on the curve graph has large displacement.\nThis class includes quotients by the normal closure of a pseudo-Anosov element,\nthe mapping class group itself, and in view of forthcoming work of Abbott,\nBerlyne, Ng, and Rasmussen, also random quotients. First, we show that every\nautomorphism of the corresponding quotient of the curve graph is induced by a\nmapping class, thus generalising Ivanov's Theorem about automorphisms of the\ncurve graph. Then we use this to prove quasi-isometric rigidity under\nadditional assumptions, satisfied by all aforementioned quotients. In the\nprocess, we clarify a proof of quasi-isometric rigidity of mapping class groups\nby Behrstock, Hagen, and Sisto. Finally, we show that the outer automorphisms\ngroups of our quotients, as well as their abstract commensurators, are \"the\nsmallest possible\".",
            "author": [
                "Giorgio Mangioni"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00701v1",
                "http://arxiv.org/pdf/2312.00701v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00700v1",
            "title": "GIFT: Generative Interpretable Fine-Tuning Transformers",
            "updated": "2023-12-01T16:33:57Z",
            "published": "2023-12-01T16:33:57Z",
            "summary": "We present GIFT (Generative Interpretable Fine-tuning Transformers) for\nfine-tuning pretrained (often large) Transformer models at downstream tasks in\na parameter-efficient way with built-in interpretability. Our GIFT is a deep\nparameter-residual learning method, which addresses two problems in fine-tuning\na pretrained Transformer model: Where to apply the parameter-efficient\nfine-tuning (PEFT) to be extremely lightweight yet sufficiently expressive, and\nHow to learn the PEFT to better exploit the knowledge of the pretrained model\nin a direct way? For the former, we select the final projection (linear) layer\nin the multi-head self-attention of a Transformer model, and verify its\neffectiveness. For the latter, in contrast to the prior art that directly\nintroduce new model parameters (often in low-rank approximation form) to be\nlearned in fine-tuning with downstream data, we propose a method for learning\nto generate the fine-tuning parameters. Our GIFT is a hyper-Transformer which\ntake as input the pretrained parameters of the projection layer to generate its\nfine-tuning parameters using a proposed Parameter-to-Cluster Attention (PaCa).\nThe PaCa results in a simple clustering-based forward explainer that plays the\nrole of semantic segmentation in testing. In experiments, our proposed GIFT is\ntested on the VTAB benchmark and the fine-grained visual classification (FGVC)\nbenchmark. It obtains significantly better performance than the prior art. Our\ncode is available at https://github.com/savadikarc/gift",
            "author": [
                "Chinmay Savadikar",
                "Xi Song",
                "Tianfu Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00700v1",
                "http://arxiv.org/pdf/2312.00700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00663v1",
            "title": "Generalized Label-Efficient 3D Scene Parsing via Hierarchical Feature\n  Aligned Pre-Training and Region-Aware Fine-tuning",
            "updated": "2023-12-01T15:47:04Z",
            "published": "2023-12-01T15:47:04Z",
            "summary": "Deep neural network models have achieved remarkable progress in 3D scene\nunderstanding while trained in the closed-set setting and with full labels.\nHowever, the major bottleneck for current 3D recognition approaches is that\nthey do not have the capacity to recognize any unseen novel classes beyond the\ntraining categories in diverse kinds of real-world applications. In the\nmeantime, current state-of-the-art 3D scene understanding approaches primarily\nrequire high-quality labels to train neural networks, which merely perform well\nin a fully supervised manner. This work presents a generalized and simple\nframework for dealing with 3D scene understanding when the labeled scenes are\nquite limited. To extract knowledge for novel categories from the pre-trained\nvision-language models, we propose a hierarchical feature-aligned pre-training\nand knowledge distillation strategy to extract and distill meaningful\ninformation from large-scale vision-language models, which helps benefit the\nopen-vocabulary scene understanding tasks. To leverage the boundary\ninformation, we propose a novel energy-based loss with boundary awareness\nbenefiting from the region-level boundary predictions. To encourage latent\ninstance discrimination and to guarantee efficiency, we propose the\nunsupervised region-level semantic contrastive learning scheme for point\nclouds, using confident predictions of the neural network to discriminate the\nintermediate feature embeddings at multiple stages. Extensive experiments with\nboth indoor and outdoor scenes demonstrated the effectiveness of our approach\nin both data-efficient learning and open-world few-shot learning. All codes,\nmodels, and data are made publicly available at:\nhttps://drive.google.com/drive/folders/1M58V-PtR8DBEwD296zJkNg_m2qq-MTAP?usp=sharing.",
            "author": [
                "Kangcheng Liu",
                "Yong-Jin Liu",
                "Kai Tang",
                "Ming Liu",
                "Baoquan Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00663v1",
                "http://arxiv.org/pdf/2312.00663v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00660v1",
            "title": "Resource-constrained knowledge diffusion processes inspired by human\n  peer learning",
            "updated": "2023-12-01T15:39:24Z",
            "published": "2023-12-01T15:39:24Z",
            "summary": "We consider a setting where a population of artificial learners is given, and\nthe objective is to optimize aggregate measures of performance, under\nconstraints on training resources. The problem is motivated by the study of\npeer learning in human educational systems. In this context, we study natural\nknowledge diffusion processes in networks of interacting artificial learners.\nBy `natural', we mean processes that reflect human peer learning where the\nstudents' internal state and learning process is mostly opaque, and the main\ndegree of freedom lies in the formation of peer learning groups by a\ncoordinator who can potentially evaluate the learners before assigning them to\npeer groups. Among else, we empirically show that such processes indeed make\neffective use of the training resources, and enable the design of modular\nneural models that have the capacity to generalize without being prone to\noverfitting noisy labels.",
            "author": [
                "Ehsan Beikihassan",
                "Amy K. Hoover",
                "Ioannis Koutis",
                "Ali Parviz",
                "Niloofar Aghaieabiane"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00660v1",
                "http://arxiv.org/pdf/2312.00660v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00641v1",
            "title": "Near mass-shell double boxes",
            "updated": "2023-12-01T15:04:48Z",
            "published": "2023-12-01T15:04:48Z",
            "summary": "Two-loop multi-leg form factors in off-shell kinematics require knowledge of\nplanar and nonplanar double box Feynman diagrams with massless internal\npropagators. These are complicated functions of Mandelstam variables and\nexternal particle virtualities. The latter serve as regulators of infrared\ndivergences, thus making these observables finite in four space-time\ndimensions. In this paper, we use the method of canonical differential\nequations for calculation of (non)planar double box integrals in the near\nmass-shell kinematical regime, i.e., where virtualities of external particles\nare much smaller than the Mandelstam variables involved. We deduce a basis of\nmaster integrals with uniform transcendental weight based on the analysis of\nleading singularities by means of the Baikov representation as well as an array\nof complementary techniques. We dub the former asymptotically canonical since\nit is valid in the near mass-shell limit of interest. We iteratively solve\nresulting differential equations up to weight four in terms of multiple\npolylogarithms.",
            "author": [
                "A. V. Belitsky",
                "V. A. Smirnov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00641v1",
                "http://arxiv.org/pdf/2312.00641v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00639v1",
            "title": "EvE: Exploiting Generative Priors for Radiance Field Enrichment",
            "updated": "2023-12-01T14:59:43Z",
            "published": "2023-12-01T14:59:43Z",
            "summary": "Modeling large-scale scenes from unconstrained image collections in-the-wild\nhas proven to be a major challenge in computer vision. Existing methods\ntackling in-the-wild neural rendering operate in a closed-world setting, where\nknowledge is limited to a scene's captured images within a training set. We\npropose EvE, which is, to the best of our knowledge, the first method\nleveraging generative priors to improve in-the-wild scene modeling. We employ\npre-trained generative networks to enrich K-Planes representations with\nextrinsic knowledge. To this end, we define an alternating training procedure\nto conduct optimization guidance of K-Planes trained on the training set. We\ncarry out extensive experiments and verify the merit of our method on synthetic\ndata as well as real tourism photo collections. EvE enhances rendered scenes\nwith richer details and outperforms the state of the art on the task of novel\nview synthesis in-the-wild. Our project page can be found at\nhttps://eve-nvs.github.io .",
            "author": [
                "Karim Kassab",
                "Antoine Schnepf",
                "Jean-Yves Franceschi",
                "Laurent Caraffa",
                "Jeremie Mary",
                "Val\u00e9rie Gouet-Brunet"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00639v1",
                "http://arxiv.org/pdf/2312.00639v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00633v1",
            "title": "Towards Efficient 3D Object Detection in Bird's-Eye-View Space for\n  Autonomous Driving: A Convolutional-Only Approach",
            "updated": "2023-12-01T14:52:59Z",
            "published": "2023-12-01T14:52:59Z",
            "summary": "3D object detection in Bird's-Eye-View (BEV) space has recently emerged as a\nprevalent approach in the field of autonomous driving. Despite the demonstrated\nimprovements in accuracy and velocity estimation compared to perspective view\nmethods, the deployment of BEV-based techniques in real-world autonomous\nvehicles remains challenging. This is primarily due to their reliance on\nvision-transformer (ViT) based architectures, which introduce quadratic\ncomplexity with respect to the input resolution. To address this issue, we\npropose an efficient BEV-based 3D detection framework called BEVENet, which\nleverages a convolutional-only architectural design to circumvent the\nlimitations of ViT models while maintaining the effectiveness of BEV-based\nmethods. Our experiments show that BEVENet is 3$\\times$ faster than\ncontemporary state-of-the-art (SOTA) approaches on the NuScenes challenge,\nachieving a mean average precision (mAP) of 0.456 and a nuScenes detection\nscore (NDS) of 0.555 on the NuScenes validation dataset, with an inference\nspeed of 47.6 frames per second. To the best of our knowledge, this study\nstands as the first to achieve such significant efficiency improvements for\nBEV-based methods, highlighting their enhanced feasibility for real-world\nautonomous driving applications.",
            "author": [
                "Yuxin Li",
                "Qiang Han",
                "Mengying Yu",
                "Yuxin Jiang",
                "Chaikiat Yeo",
                "Yiheng Li",
                "Zihang Huang",
                "Nini Liu",
                "Hsuanhan Chen",
                "Xiaojun Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00633v1",
                "http://arxiv.org/pdf/2312.00633v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00629v1",
            "title": "The Ecosystem of Trust (EoT): Enabling effective deployment of\n  autonomous systems through collaborative and trusted ecosystems",
            "updated": "2023-12-01T14:47:36Z",
            "published": "2023-12-01T14:47:36Z",
            "summary": "Ecosystems are ubiquitous but trust within them is not guaranteed. Trust is\nparamount because stakeholders within an ecosystem must collaborate to achieve\ntheir objectives. With the twin transitions, digital transformation to go in\nparallel with green transition, accelerating the deployment of autonomous\nsystems, trust has become even more critical to ensure that the deployed\ntechnology creates value. To address this need, we propose an ecosystem of\ntrust approach to support deployment of technology by enabling trust among and\nbetween stakeholders, technologies and infrastructures, institutions and\ngovernance, and the artificial and natural environments in an ecosystem. The\napproach can help the stakeholders in the ecosystem to create, deliver, and\nreceive value by addressing their concerns and aligning their objectives. We\npresent an autonomous, zero emission ferry as a real world use case to\ndemonstrate the approach from a stakeholder perspective. We argue that\nassurance, defined as grounds for justified confidence originated from evidence\nand knowledge, is a prerequisite to enable the approach. Assurance provides\nevidence and knowledge that are collected, analysed, and communicated in a\nsystematic, targeted, and meaningful way. Assurance can enable the approach to\nhelp successfully deploy technology by ensuring that risk is managed, trust is\nshared, and value is created.",
            "author": [
                "Jon Arne Glomsrud",
                "Tita Alissa Bach"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00629v1",
                "http://arxiv.org/pdf/2312.00629v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00852v1",
            "title": "Beyond First-Order Tweedie: Solving Inverse Problems using Latent\n  Diffusion",
            "updated": "2023-12-01T14:36:24Z",
            "published": "2023-12-01T14:36:24Z",
            "summary": "Sampling from the posterior distribution poses a major computational\nchallenge in solving inverse problems using latent diffusion models. Common\nmethods rely on Tweedie's first-order moments, which are known to induce a\nquality-limiting bias. Existing second-order approximations are impractical due\nto prohibitive computational costs, making standard reverse diffusion processes\nintractable for posterior sampling. This paper introduces Second-order Tweedie\nsampler from Surrogate Loss (STSL), a novel sampler that offers efficiency\ncomparable to first-order Tweedie with a tractable reverse process using\nsecond-order approximation. Our theoretical results reveal that the\nsecond-order approximation is lower bounded by our surrogate loss that only\nrequires $O(1)$ compute using the trace of the Hessian, and by the lower bound\nwe derive a new drift term to make the reverse process tractable. Our method\nsurpasses SoTA solvers PSLD and P2L, achieving 4X and 8X reduction in neural\nfunction evaluations, respectively, while notably enhancing sampling quality on\nFFHQ, ImageNet, and COCO benchmarks. In addition, we show STSL extends to\ntext-guided image editing and addresses residual distortions present from\ncorrupted images in leading text-guided image editing methods. To our best\nknowledge, this is the first work to offer an efficient second-order\napproximation in solving inverse problems using latent diffusion and editing\nreal-world images with corruptions.",
            "author": [
                "Litu Rout",
                "Yujia Chen",
                "Abhishek Kumar",
                "Constantine Caramanis",
                "Sanjay Shakkottai",
                "Wen-Sheng Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00852v1",
                "http://arxiv.org/pdf/2312.00852v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00620v1",
            "title": "Extremal graphs without long paths and a given graph",
            "updated": "2023-12-01T14:33:26Z",
            "published": "2023-12-01T14:33:26Z",
            "summary": "For a family of graphs $\\mathcal{F}$, the Tur\\'{a}n number\n$ex(n,\\mathcal{F})$ is the maximum number of edges in an $n$-vertex graph\ncontaining no member of $\\mathcal{F}$ as a subgraph. The maximum number of\nedges in an $n$-vertex connected graph containing no member of $\\mathcal{F}$ as\na subgraph is denoted by $ex_{conn}(n,\\mathcal{F})$. Let $P_k$ be the path on\n$k$ vertices and $H$ be a graph with chromatic number more than $2$. Katona and\nXiao [Extremal graphs without long paths and large cliques, European J.\nCombin., 2023 103807] posed the following conjecture: Suppose that the\nchromatic number of $H$ is more than $2$. Then\n$ex\\big(n,\\{H,P_k\\}\\big)=n\\max\\big\\{\\big\\lfloor\n\\frac{k}{2}\\big\\rfloor-1,\\frac{ex(k-1,H)}{k-1}\\big\\}+O_k(1)$. In this paper, we\ndetermine the exact value of $ex_{conn}\\big(n,\\{P_k,H\\}\\big)$ for sufficiently\nlarge $n$. Moreover, we obtain asymptotical result for\n$ex\\big(n,\\{P_k,H\\}\\big)$, which solves the conjecture proposed by Katona and\nXiao.",
            "author": [
                "Yichong Liu",
                "Liying Kang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00620v1",
                "http://arxiv.org/pdf/2312.00620v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00601v1",
            "title": "Online Graph Coloring with Predictions",
            "updated": "2023-12-01T14:07:10Z",
            "published": "2023-12-01T14:07:10Z",
            "summary": "We introduce learning augmented algorithms to the online graph coloring\nproblem. Although the simple greedy algorithm FirstFit is known to perform\npoorly in the worst case, we are able to establish a relationship between the\nstructure of any input graph $G$ that is revealed online and the number of\ncolors that FirstFit uses for $G$. Based on this relationship, we propose an\nonline coloring algorithm FirstFitPredictions that extends FirstFit while\nmaking use of machine learned predictions. We show that FirstFitPredictions is\nboth \\emph{consistent} and \\emph{smooth}. Moreover, we develop a novel\nframework for combining online algorithms at runtime specifically for the\nonline graph coloring problem. Finally, we show how this framework can be used\nto robustify by combining it with any classical online coloring algorithm (that\ndisregards the predictions).",
            "author": [
                "Antonios Antoniadis",
                "Hajo Broersma",
                "Yang Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00601v1",
                "http://arxiv.org/pdf/2312.00601v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00600v1",
            "title": "Improving Plasticity in Online Continual Learning via Collaborative\n  Learning",
            "updated": "2023-12-01T14:06:28Z",
            "published": "2023-12-01T14:06:28Z",
            "summary": "Online Continual Learning (CL) solves the problem of learning the\never-emerging new classification tasks from a continuous data stream. Unlike\nits offline counterpart, in online CL, the training data can only be seen once.\nMost existing online CL research regards catastrophic forgetting (i.e., model\nstability) as almost the only challenge. In this paper, we argue that the\nmodel's capability to acquire new knowledge (i.e., model plasticity) is another\nchallenge in online CL. While replay-based strategies have been shown to be\neffective in alleviating catastrophic forgetting, there is a notable gap in\nresearch attention toward improving model plasticity. To this end, we propose\nCollaborative Continual Learning (CCL), a collaborative learning based strategy\nto improve the model's capability in acquiring new concepts. Additionally, we\nintroduce Distillation Chain (DC), a novel collaborative learning scheme to\nboost the training of the models. We adapted CCL-DC to existing representative\nonline CL works. Extensive experiments demonstrate that even if the learners\nare well-trained with state-of-the-art online CL methods, our strategy can\nstill improve model plasticity dramatically, and thereby improve the overall\nperformance by a large margin.",
            "author": [
                "Maorong Wang",
                "Nicolas Michel",
                "Ling Xiao",
                "Toshihiko Yamasaki"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00600v1",
                "http://arxiv.org/pdf/2312.00600v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00591v1",
            "title": "Less is More: Learning Reference Knowledge Using No-Reference Image\n  Quality Assessment",
            "updated": "2023-12-01T13:56:01Z",
            "published": "2023-12-01T13:56:01Z",
            "summary": "Image Quality Assessment (IQA) with reference images have achieved great\nsuccess by imitating the human vision system, in which the image quality is\neffectively assessed by comparing the query image with its pristine reference\nimage. However, for the images in the wild, it is quite difficult to access\naccurate reference images. We argue that it is possible to learn reference\nknowledge under the No-Reference Image Quality Assessment (NR-IQA) setting,\nwhich is effective and efficient empirically. Concretely, by innovatively\nintroducing a novel feature distillation method in IQA, we propose a new\nframework to learn comparative knowledge from non-aligned reference images. And\nthen, to achieve fast convergence and avoid overfitting, we further propose an\ninductive bias regularization. Such a framework not only solves the congenital\ndefects of NR-IQA but also improves the feature extraction framework, enabling\nit to express more abundant quality information. Surprisingly, our method\nutilizes less input while obtaining a more significant improvement compared to\nthe teacher models. Extensive experiments on eight standard NR-IQA datasets\ndemonstrate the superior performance to the state-of-the-art NR-IQA methods,\ni.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs.\n0.661 in LIVEFB).",
            "author": [
                "Xudong Li",
                "Jingyuan Zheng",
                "Xiawu Zheng",
                "Runze Hu",
                "Enwei Zhang",
                "Yuting Gao",
                "Yunhang Shen",
                "Ke Li",
                "Yutao Liu",
                "Pingyang Dai",
                "Yan Zhang",
                "Rongrong Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00591v1",
                "http://arxiv.org/pdf/2312.00591v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00582v1",
            "title": "Design Patterns for Machine Learning Based Systems with\n  Human-in-the-Loop",
            "updated": "2023-12-01T13:46:38Z",
            "published": "2023-12-01T13:46:38Z",
            "summary": "The development and deployment of systems using supervised machine learning\n(ML) remain challenging: mainly due to the limited reliability of prediction\nmodels and the lack of knowledge on how to effectively integrate human\nintelligence into automated decision-making. Humans involvement in the ML\nprocess is a promising and powerful paradigm to overcome the limitations of\npure automated predictions and improve the applicability of ML in practice. We\ncompile a catalog of design patterns to guide developers select and implement\nsuitable human-in-the-loop (HiL) solutions. Our catalog takes into\nconsideration key requirements as the cost of human involvement and model\nretraining. It includes four training patterns, four deployment patterns, and\ntwo orthogonal cooperation patterns.",
            "author": [
                "Jakob Smedegaard Andersen",
                "Walid Maalej"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00582v1",
                "http://arxiv.org/pdf/2312.00582v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00575v1",
            "title": "Instruction-tuning Aligns LLMs to the Human Brain",
            "updated": "2023-12-01T13:31:02Z",
            "published": "2023-12-01T13:31:02Z",
            "summary": "Instruction-tuning is a widely adopted method of finetuning that enables\nlarge language models (LLMs) to generate output that more closely resembles\nhuman responses to natural language queries, in many cases leading to\nhuman-level performance on diverse testbeds. However, it remains unclear\nwhether instruction-tuning truly makes LLMs more similar to how humans process\nlanguage. We investigate the effect of instruction-tuning on LLM-human\nsimilarity in two ways: (1) brain alignment, the similarity of LLM internal\nrepresentations to neural activity in the human language system, and (2)\nbehavioral alignment, the similarity of LLM and human behavior on a reading\ntask. We assess 25 vanilla and instruction-tuned LLMs across three datasets\ninvolving humans reading naturalistic stories and sentences. We discover that\ninstruction-tuning generally enhances brain alignment by an average of 6%, but\ndoes not have a similar effect on behavioral alignment. To identify the factors\nunderlying LLM-brain alignment, we compute correlations between the brain\nalignment of LLMs and various model properties, such as model size, various\nproblem-solving abilities, and performance on tasks requiring world knowledge\nspanning various domains. Notably, we find a strong positive correlation\nbetween brain alignment and model size (r = 0.95), as well as performance on\ntasks requiring world knowledge (r = 0.81). Our results demonstrate that\ninstruction-tuning LLMs improves both world knowledge representations and brain\nalignment, suggesting that mechanisms that encode world knowledge in LLMs also\nimprove representational alignment to the human brain.",
            "author": [
                "Khai Loong Aw",
                "Syrielle Montariol",
                "Badr AlKhamissi",
                "Martin Schrimpf",
                "Antoine Bosselut"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00575v1",
                "http://arxiv.org/pdf/2312.00575v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00555v1",
            "title": "Dense, irregular, yet always graphic $3$-uniform hypergraph degree\n  sequences",
            "updated": "2023-12-01T13:00:45Z",
            "published": "2023-12-01T13:00:45Z",
            "summary": "A $3$-uniform hypergraph is a generalization of simple graphs where each\nhyperedge is a subset of vertices of size $3$. The degree of a vertex in a\nhypergraph is the number of hyperedges incident with it. The degree sequence of\na hypergraph is the sequence of the degrees of its vertices. The degree\nsequence problem for $3$-uniform hypergraphs is to decide if a $3$-uniform\nhypergraph exists with a prescribed degree sequence. Such a hypergraph is\ncalled a realization. Recently, Deza \\emph{et al.} proved that the degree\nsequence problem for $3$-uniform hypergraphs is NP-complete. Some special cases\nare easy; however, polynomial algorithms have been known so far only for some\nvery restricted degree sequences. The main result of our research is the\nfollowing. If all degrees are between $\\frac{2n^2}{63}+O(n)$ and\n$\\frac{5n^2}{63}-O(n)$ in a degree sequence $D$, further, the number of\nvertices is at least $45$, and the degree sum can be divided by $3$, then $D$\nhas a $3$-uniform hypergraph realization. Our proof is constructive and in\nfact, it constructs a hypergraph realization in polynomial time for any degree\nsequence satisfying the properties mentioned above. To our knowledge, this is\nthe first polynomial running time algorithm to construct a $3$-uniform\nhypergraph realization of a highly irregular and dense degree sequence.",
            "author": [
                "Runze Li",
                "Istvan Miklos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00555v1",
                "http://arxiv.org/pdf/2312.00555v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C65, 05C07, 05C85"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00553v1",
            "title": "A Spatio-Temporal Graph Convolutional Network for Gesture Recognition\n  from High-Density Electromyography",
            "updated": "2023-12-01T13:00:41Z",
            "published": "2023-12-01T13:00:41Z",
            "summary": "Accurate hand gesture prediction is crucial for effective upper-limb\nprosthetic limbs control. As the high flexibility and multiple degrees of\nfreedom exhibited by human hands, there has been a growing interest in\nintegrating deep networks with high-density surface electromyography (HD-sEMG)\ngrids to enhance gesture recognition capabilities. However, many existing\nmethods fall short in fully exploit the specific spatial topology and temporal\ndependencies present in HD-sEMG data. Additionally, these studies are often\nlimited number of gestures and lack generality. Hence, this study introduces a\nnovel gesture recognition method, named STGCN-GR, which leverages\nspatio-temporal graph convolution networks for HD-sEMG-based human-machine\ninterfaces. Firstly, we construct muscle networks based on functional\nconnectivity between channels, creating a graph representation of HD-sEMG\nrecordings. Subsequently, a temporal convolution module is applied to capture\nthe temporal dependences in the HD-sEMG series and a spatial graph convolution\nmodule is employed to effectively learn the intrinsic spatial topology\ninformation among distinct HD-sEMG channels. We evaluate our proposed model on\na public HD-sEMG dataset comprising a substantial number of gestures (i.e.,\n65). Our results demonstrate the remarkable capability of the STGCN-GR method,\nachieving an impressive accuracy of 91.07% in predicting gestures, which\nsurpasses state-of-the-art deep learning methods applied to the same dataset.",
            "author": [
                "Wenjuan Zhong",
                "Yuyang Zhang",
                "Peiwen Fu",
                "Wenxuan Xiong",
                "Mingming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00553v1",
                "http://arxiv.org/pdf/2312.00553v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00552v1",
            "title": "Improving Unsupervised Relation Extraction by Augmenting Diverse\n  Sentence Pairs",
            "updated": "2023-12-01T12:59:32Z",
            "published": "2023-12-01T12:59:32Z",
            "summary": "Unsupervised relation extraction (URE) aims to extract relations between\nnamed entities from raw text without requiring manual annotations or\npre-existing knowledge bases. In recent studies of URE, researchers put a\nnotable emphasis on contrastive learning strategies for acquiring relation\nrepresentations. However, these studies often overlook two important aspects:\nthe inclusion of diverse positive pairs for contrastive learning and the\nexploration of appropriate loss functions. In this paper, we propose AugURE\nwith both within-sentence pairs augmentation and augmentation through\ncross-sentence pairs extraction to increase the diversity of positive pairs and\nstrengthen the discriminative power of contrastive learning. We also identify\nthe limitation of noise-contrastive estimation (NCE) loss for relation\nrepresentation learning and propose to apply margin loss for sentence pairs.\nExperiments on NYT-FB and TACRED datasets demonstrate that the proposed\nrelation representation learning and a simple K-Means clustering achieves\nstate-of-the-art performance.",
            "author": [
                "Qing Wang",
                "Kang Zhou",
                "Qiao Qiao",
                "Yuepei Li",
                "Qi Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00552v1",
                "http://arxiv.org/pdf/2312.00552v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00540v1",
            "title": "Target-agnostic Source-free Domain Adaptation for Regression Tasks",
            "updated": "2023-12-01T12:35:18Z",
            "published": "2023-12-01T12:35:18Z",
            "summary": "Unsupervised domain adaptation (UDA) seeks to bridge the domain gap between\nthe target and source using unlabeled target data. Source-free UDA removes the\nrequirement for labeled source data at the target to preserve data privacy and\nstorage. However, work on source-free UDA assumes knowledge of domain gap\ndistribution, and hence is limited to either target-aware or classification\ntask. To overcome it, we propose TASFAR, a novel target-agnostic source-free\ndomain adaptation approach for regression tasks. Using prediction confidence,\nTASFAR estimates a label density map as the target label distribution, which is\nthen used to calibrate the source model on the target domain. We have conducted\nextensive experiments on four regression tasks with various domain gaps,\nnamely, pedestrian dead reckoning for different users, image-based people\ncounting in different scenes, housing-price prediction at different districts,\nand taxi-trip duration prediction from different departure points. TASFAR is\nshown to substantially outperform the state-of-the-art source-free UDA\napproaches by averagely reducing 22% errors for the four tasks and achieve\nnotably comparable accuracy as source-based UDA without using source data.",
            "author": [
                "Tianlang He",
                "Zhiqiu Xia",
                "Jierun Chen",
                "Haoliang Li",
                "S. -H. Gary Chan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00540v1",
                "http://arxiv.org/pdf/2312.00540v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00509v1",
            "title": "Bayesian causal discovery from unknown general interventions",
            "updated": "2023-12-01T11:30:51Z",
            "published": "2023-12-01T11:30:51Z",
            "summary": "We consider the problem of learning causal Directed Acyclic Graphs (DAGs)\nusing combinations of observational and interventional experimental data.\nCurrent methods tailored to this setting assume that interventions either\ndestroy parent-child relations of the intervened (target) nodes or only alter\nsuch relations without modifying the parent sets, even when the intervention\ntargets are unknown. We relax this assumption by proposing a Bayesian method\nfor causal discovery from general interventions, which allow for modifications\nof the parent sets of the unknown targets. Even in this framework, DAGs and\ngeneral interventions may be identifiable only up to some equivalence classes.\nWe provide graphical characterizations of such interventional Markov\nequivalence and devise compatible priors for Bayesian inference that guarantee\nscore equivalence of indistinguishable structures. We then develop a Markov\nChain Monte Carlo (MCMC) scheme to approximate the posterior distribution over\nDAGs, intervention targets and induced parent sets. Finally, we evaluate the\nproposed methodology on both simulated and real protein expression data.",
            "author": [
                "Alessandro Mascaro",
                "Federico Castelletti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00509v1",
                "http://arxiv.org/pdf/2312.00509v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00503v1",
            "title": "Tree universality in positional games",
            "updated": "2023-12-01T11:06:38Z",
            "published": "2023-12-01T11:06:38Z",
            "summary": "In this paper we consider positional games where the winning sets are tree\nuniversal graphs. Specifically, we show that in the unbiased Maker-Breaker game\non the complete graph $K_n$, Maker has a strategy to occupy a graph which\ncontains copies of all spanning trees with maximum degree at most $cn/\\log(n)$,\nfor a suitable constant $c$ and $n$ being large enough. We also prove an\nanalogous result for Waiter-Client games. Both of our results show that the\nbuilding player can play at least as good as suggested by the random graph\nintuition. Moreover, they improve on a special case of earlier results by\nJohannsen, Krivelevich, and Samotij as well as Han and Yang for Maker-Breaker\ngames.",
            "author": [
                "Grzegorz Adamski",
                "Sylwia Antoniuk",
                "Ma\u0142gorzata Bednarska-Bzd\u0119ga",
                "Dennis Clemens",
                "Fabian Hamann",
                "Yannick Mogge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00503v1",
                "http://arxiv.org/pdf/2312.00503v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C05, 91A24"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00485v1",
            "title": "Backbone-based Dynamic Graph Spatio-Temporal Network for Epidemic\n  Forecasting",
            "updated": "2023-12-01T10:34:03Z",
            "published": "2023-12-01T10:34:03Z",
            "summary": "Accurate epidemic forecasting is a critical task in controlling disease\ntransmission. Many deep learning-based models focus only on static or dynamic\ngraphs when constructing spatial information, ignoring their relationship.\nAdditionally, these models often rely on recurrent structures, which can lead\nto error accumulation and computational time consumption. To address the\naforementioned problems, we propose a novel model called Backbone-based Dynamic\nGraph Spatio-Temporal Network (BDGSTN). Intuitively, the continuous and smooth\nchanges in graph structure, make adjacent graph structures share a basic\npattern. To capture this property, we use adaptive methods to generate static\nbackbone graphs containing the primary information and temporal models to\ngenerate dynamic temporal graphs of epidemic data, fusing them to generate a\nbackbone-based dynamic graph. To overcome potential limitations associated with\nrecurrent structures, we introduce a linear model DLinear to handle temporal\ndependencies and combine it with dynamic graph convolution for epidemic\nforecasting. Extensive experiments on two datasets demonstrate that BDGSTN\noutperforms baseline models and ablation comparison further verifies the\neffectiveness of model components. Furthermore, we analyze and measure the\nsignificance of backbone and temporal graphs by using information metrics from\ndifferent aspects. Finally, we compare model parameter volume and training time\nto confirm the superior complexity and efficiency of BDGSTN.",
            "author": [
                "Junkai Mao",
                "Yuexing Han",
                "Gouhei Tanaka",
                "Bing Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00485v1",
                "http://arxiv.org/pdf/2312.00485v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00476v1",
            "title": "Self-Supervised Learning of Spatial Acoustic Representation with\n  Cross-Channel Signal Reconstruction and Multi-Channel Conformer",
            "updated": "2023-12-01T10:16:02Z",
            "published": "2023-12-01T10:16:02Z",
            "summary": "Supervised learning methods have shown effectiveness in estimating spatial\nacoustic parameters such as time difference of arrival, direct-to-reverberant\nratio and reverberation time. However, they still suffer from the\nsimulation-to-reality generalization problem due to the mismatch between\nsimulated and real-world acoustic characteristics and the deficiency of\nannotated real-world data. To this end, this work proposes a self-supervised\nmethod that takes full advantage of unlabeled data for spatial acoustic\nparameter estimation. First, a new pretext task, i.e. cross-channel signal\nreconstruction (CCSR), is designed to learn a universal spatial acoustic\nrepresentation from unlabeled multi-channel microphone signals. We mask partial\nsignals of one channel and ask the model to reconstruct them, which makes it\npossible to learn spatial acoustic information from unmasked signals and\nextract source information from the other microphone channel. An\nencoder-decoder structure is used to disentangle the two kinds of information.\nBy fine-tuning the pre-trained spatial encoder with a small annotated dataset,\nthis encoder can be used to estimate spatial acoustic parameters. Second, a\nnovel multi-channel audio Conformer (MC-Conformer) is adopted as the encoder\nmodel architecture, which is suitable for both the pretext and downstream\ntasks. It is carefully designed to be able to capture the local and global\ncharacteristics of spatial acoustics exhibited in the time-frequency domain.\nExperimental results of five acoustic parameter estimation tasks on both\nsimulated and real-world data show the effectiveness of the proposed method. To\nthe best of our knowledge, this is the first self-supervised learning method in\nthe field of spatial acoustic representation learning and multi-channel audio\nsignal processing.",
            "author": [
                "Bing Yang",
                "Xiaofei Li"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00476v1",
                "http://arxiv.org/pdf/2312.00476v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00460v1",
            "title": "A large family of strongly regular graphs with small Weisfeiler-Leman\n  dimension",
            "updated": "2023-12-01T09:54:10Z",
            "published": "2023-12-01T09:54:10Z",
            "summary": "In 2002, D. Fon-Der-Flaass constructed a prolific family of strongly regular\ngraphs. In this paper, we prove that for infinitely many natural numbers $n$,\nthis family contains $n^{\\Omega(n^{2/3})}$ strongly regular $n$-vertex graphs\n$X$ with the same parameters, which satisfy the following condition: an\nisomorphism between $X$ and any other graph can be verified by the\n$4$-dimensional Weisfeiler-Leman algorithm.",
            "author": [
                "Jinzhuan Cai",
                "Jin Guo",
                "Alexander L. Gavrilyuk",
                "Ilia Ponomarenko"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00460v1",
                "http://arxiv.org/pdf/2312.00460v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00456v1",
            "title": "Auto-encoding GPS data to reveal individual and collective behaviour",
            "updated": "2023-12-01T09:41:40Z",
            "published": "2023-12-01T09:41:40Z",
            "summary": "We propose an innovative and generic methodology to analyse individual and\ncollective behaviour through individual trajectory data. The work is motivated\nby the analysis of GPS trajectories of fishing vessels collected from\nregulatory tracking data in the context of marine biodiversity conservation and\necosystem-based fisheries management. We build a low-dimensional latent\nrepresentation of trajectories using convolutional neural networks as\nnon-linear mapping. This is done by training a conditional variational\nauto-encoder taking into account covariates. The posterior distributions of the\nlatent representations can be linked to the characteristics of the actual\ntrajectories. The latent distributions of the trajectories are compared with\nthe Bhattacharyya coefficient, which is well-suited for comparing\ndistributions. Using this coefficient, we analyse the variation of the\nindividual behaviour of each vessel during time. For collective behaviour\nanalysis, we build proximity graphs and use an extension of the stochastic\nblock model for multiple networks. This model results in a clustering of the\nindividuals based on their set of trajectories. The application to French\nfishing vessels enables us to obtain groups of vessels whose individual and\ncollective behaviours exhibit spatio-temporal patterns over the period\n2014-2018.",
            "author": [
                "Saint-Clair Chabert-Liddell",
                "Nicolas Bez",
                "Pierre Gloaguen",
                "Sophie Donnet",
                "St\u00e9phanie Mah\u00e9vas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00456v1",
                "http://arxiv.org/pdf/2312.00456v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00442v1",
            "title": "BPMS for management: a systematic literature review",
            "updated": "2023-12-01T09:19:50Z",
            "published": "2023-12-01T09:19:50Z",
            "summary": "The aim of this paper is to carry out a systematic analysis of the literature\nto show the state of the art of Business Processes Management Systems (BPMS).\nBPMS represents a technology that automates business processes connecting users\nwith their tasks. For this, a systematic review of the literature of the last\nten years was carried out, using scientific papers indexed in the main\ndatabases of the knowledge area. The papers generated by the search were later\nanalysed and filtered. Among the findings of this study, the academic interest\nand the multidisciplinary nature of the subject, as this type of studies have\nbeen identified in different areas of knowledge. Our research is a starting\npoint for future research eager to develop a more robust theory and broaden the\ninterest of the subject due its economic impact on process management.",
            "author": [
                "Alicia Martin-Navarro",
                "Maria Paula Lechuga Sancho",
                "Jose Aurelio Medina-Garrido"
            ],
            "link": [
                "http://dx.doi.org/10.3989/redc.2018.3.1532",
                "http://arxiv.org/abs/2312.00442v1",
                "http://arxiv.org/pdf/2312.00442v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00435v1",
            "title": "Enhancing Image Captioning with Neural Models",
            "updated": "2023-12-01T09:06:56Z",
            "published": "2023-12-01T09:06:56Z",
            "summary": "This research explores the realm of neural image captioning using deep\nlearning models. The study investigates the performance of different neural\narchitecture configurations, focusing on the inject architecture, and proposes\na novel quality metric for evaluating caption generation. Through extensive\nexperimentation and analysis, this work sheds light on the challenges and\nopportunities in image captioning, providing insights into model behavior and\noverfitting. The results reveal that while the merge models exhibit a larger\nvocabulary and higher ROUGE scores, the inject architecture generates relevant\nand concise image captions. The study also highlights the importance of\nrefining training data and optimizing hyperparameters for improved model\nperformance. This research contributes to the growing body of knowledge in\nneural image captioning and encourages further exploration in the field,\nemphasizing the democratization of artificial intelligence.",
            "author": [
                "Pooja Bhatnagar",
                "Sai Mrunaal",
                "Sachin Kamnure"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00435v1",
                "http://arxiv.org/pdf/2312.00435v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00430v1",
            "title": "Every Elementary Graph is Chromatic Choosable",
            "updated": "2023-12-01T08:54:21Z",
            "published": "2023-12-01T08:54:21Z",
            "summary": "Elementary graphs are graphs whose edges can be colored using two colors in\nsuch a way that the edges in any induced $P_3$ get distinct colors. They\nconstitute a subclass of the class of claw-free perfect graphs. In this paper,\nwe show that for any elementary graph, its list chromatic number and chromatic\nnumber are equal.",
            "author": [
                "Nandana K Vasudevan",
                "K Somasundaram",
                "J Geetha"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00430v1",
                "http://arxiv.org/pdf/2312.00430v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15, 05C17"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00429v1",
            "title": "Polygraphs: From Rewriting to Higher Categories",
            "updated": "2023-12-01T08:52:39Z",
            "published": "2023-12-01T08:52:39Z",
            "summary": "Polygraphs are a higher-dimensional generalization of the notion of directed\ngraph. Based on those as unifying concept, this monograph on polygraphs\nrevisits the theory of rewriting in the context of strict higher categories,\nadopting the abstract point of view offered by homotopical algebra. The first\nhalf explores the theory of polygraphs in low dimensions and its applications\nto the computation of the coherence of algebraic structures. It is meant to be\nprogressive, with little requirements on the background of the reader, apart\nfrom basic category theory, and is illustrated with algorithmic computations on\nalgebraic structures. The second half introduces and studies the general notion\nof n-polygraph, dealing with the homotopy theory of those. It constructs the\nfolk model structure on the category of strict higher categories and exhibits\npolygraphs as cofibrant objects. This allows extending to higher dimensional\nstructures the coherence results developed in the first half.",
            "author": [
                "Dimitri Ara",
                "Albert Burroni",
                "Yves Guiraud",
                "Philippe Malbos",
                "Fran\u00e7ois M\u00e9tayer",
                "Samuel Mimram"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00429v1",
                "http://arxiv.org/pdf/2312.00429v1"
            ],
            "primary_category": "math.CT",
            "category": [
                "math.CT",
                "cs.LO",
                "18N30 (Primary) 18-00, 18C10, 18N40, 68Q42 (Secondary)",
                "F.4.2; A.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00421v1",
            "title": "A Semi-Tensor Product based Circuit Simulation for SAT-sweeping",
            "updated": "2023-12-01T08:44:36Z",
            "published": "2023-12-01T08:44:36Z",
            "summary": "In recent years, circuit simulators and Boolean satisfiability (SAT) solvers\nhave been tightly integrated to provide efficient logic synthesis and\nverification. Circuit simulation can generate highly expressive simulation\npatterns that can either enumerate or filter out most candidates for synthesis.\nSubsequently, SAT solvers are employed to check those that remain, thereby\nmaking the logic synthesis process more efficient. This paper introduces a\nnovel circuit simulator of k-input lookup table (k-LUT) networks, based on\nsemi-tensor product (STP). STP-based simulators use computation of logic\nmatrices, the primitives of logic networks, as opposed to relying on bitwise\nlogic operations for simulation of k-LUT networks. Experimental results show\nthat our STP-based simulator reduces the runtime by an average of 7.2x.\nFurthermore, we integrate this proposed simulator into a SAT-sweeping engine\nknown as SAT sweeper. Through a combination of structural hashing, simulation,\nand SAT queries, SAT sweeper simplifies logic networks by systematically\nmerging graph vertices from input to output. To enhance the efficiency, we used\nSTP-based exhaustive simulation, which significantly reduces the number of\nfalse equivalence class candidates, thereby improving the computational\nefficiency by reducing the number of SAT calls required. When compared to the\nSOTA SAT sweeper, our method demonstrates an average 35% runtime reduction.",
            "author": [
                "Hongyang Pan",
                "Ruibing Zhang",
                "Yinshui Xia",
                "Lunyao Wang",
                "Fan Yang",
                "Xuan Zeng",
                "Zhufei Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00421v1",
                "http://arxiv.org/pdf/2312.00421v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00418v1",
            "title": "On 2-bisections and monochromatic edges in claw-free cubic multigraphs",
            "updated": "2023-12-01T08:43:43Z",
            "published": "2023-12-01T08:43:43Z",
            "summary": "A $k$-bisection of a multigraph $G$ is a partition of its vertex set into two\nparts of the same cardinality such that every component of each part has at\nmost $k$ vertices. Cui and Liu shown that every claw-free cubic multigraph\ncontains a $2$-bisection, while Eom and Ozeki constructed specific\n$2$-bisections with bounded number of monochromatic edges. Their bound is the\nbest possible for claw-free cubic simple graphs. In this note, we extend the\nlatter result to the larger family of claw-free cubic multigraphs",
            "author": [
                "Federico Romaniello"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00418v1",
                "http://arxiv.org/pdf/2312.00418v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00401v1",
            "title": "VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video\n  Internet of Things",
            "updated": "2023-12-01T07:50:53Z",
            "published": "2023-12-01T07:50:53Z",
            "summary": "Video Internet of Things (VIoT) has shown full potential in collecting an\nunprecedented volume of video data. Learning to schedule perceiving models and\nanalyzing the collected videos intelligently will be potential sparks for VIoT.\nIn this paper, to address the challenges posed by the fine-grained and\ninterrelated vision tool usage of VIoT, we build VIoTGPT, the framework based\non LLMs to correctly interact with humans, query knowledge videos, and invoke\nvision models to accomplish complicated tasks. To support VIoTGPT and related\nfuture works, we meticulously crafted the training dataset and established\nbenchmarks involving 11 representative vision models across three categories\nbased on semi-automatic annotations. To guide LLM to act as the intelligent\nagent towards intelligent VIoT, we resort to ReAct instruction tuning based on\nthe collected VIoT dataset to learn the tool capability. Quantitative and\nqualitative experimental results and analyses demonstrate the effectiveness of\nVIoTGPT.",
            "author": [
                "Yaoyao Zhong",
                "Mengshi Qi",
                "Rui Wang",
                "Yuhan Qiu",
                "Yang Zhang",
                "Huadong Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00401v1",
                "http://arxiv.org/pdf/2312.00401v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00396v1",
            "title": "GFN-SR: Symbolic Regression with Generative Flow Networks",
            "updated": "2023-12-01T07:38:05Z",
            "published": "2023-12-01T07:38:05Z",
            "summary": "Symbolic regression (SR) is an area of interpretable machine learning that\naims to identify mathematical expressions, often composed of simple functions,\nthat best fit in a given set of covariates $X$ and response $y$. In recent\nyears, deep symbolic regression (DSR) has emerged as a popular method in the\nfield by leveraging deep reinforcement learning to solve the complicated\ncombinatorial search problem. In this work, we propose an alternative framework\n(GFN-SR) to approach SR with deep learning. We model the construction of an\nexpression tree as traversing through a directed acyclic graph (DAG) so that\nGFlowNet can learn a stochastic policy to generate such trees sequentially.\nEnhanced with an adaptive reward baseline, our method is capable of generating\na diverse set of best-fitting expressions. Notably, we observe that GFN-SR\noutperforms other SR algorithms in noisy data regimes, owing to its ability to\nlearn a distribution of rewards over a space of candidate solutions.",
            "author": [
                "Sida Li",
                "Ioana Marinescu",
                "Sebastian Musslick"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00396v1",
                "http://arxiv.org/pdf/2312.00396v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00391v1",
            "title": "Inverse-Optimization-Based Uncertainty Set for Robust Linear\n  Optimization",
            "updated": "2023-12-01T07:24:37Z",
            "published": "2023-12-01T07:24:37Z",
            "summary": "We consider solving linear optimization (LO) problems with uncertain\nobjective coefficients. For such problems, we often employ robust optimization\n(RO) approaches by introducing an uncertainty set for the unknown coefficients.\nTypical RO approaches require observations or prior knowledge of the unknown\ncoefficient to define an appropriate uncertainty set. However, such information\nmay not always be available in practice. In this study, we propose a novel\nuncertainty set for robust linear optimization (RLO) problems without prior\nknowledge of the unknown coefficients. Instead, we assume to have data of known\nconstraint parameters and corresponding optimal solutions. Specifically, we\nderive an explicit form of the uncertainty set as a polytope by applying\ntechniques of inverse optimization (IO). We prove that the RLO problem with the\nproposed uncertainty set can be equivalently reformulated as an LO problem.\nNumerical experiments show that the RO approach with the proposed uncertainty\nset outperforms classical IO in terms of performance stability.",
            "author": [
                "Ayaka Ueta",
                "Mirai Tanaka",
                "Ken Kobayashi",
                "Kazuhide Nakata"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00391v1",
                "http://arxiv.org/pdf/2312.00391v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00383v1",
            "title": "On the automorphism group of a distance-regular graph",
            "updated": "2023-12-01T07:13:28Z",
            "published": "2023-12-01T07:13:28Z",
            "summary": "The motion of a graph is the minimal degree of its full automorphism group.\nBabai conjectured that the motion of a primitive distance-regular graph on $n$\nvertices of diameter greater than two is at least $n/C$ for some universal\nconstant $C > 0$, unless the graph is a Johnson or Hamming graph. We prove that\nthe motion of a distance-regular graph of diameter $d \\geq 3$ on $n$ vertices\nis at least $Cn/(\\log n)^6$ for some universal constant $C > 0$, unless it is a\nJohnson, a Hamming or a crown graph. This follows using an improvement of an\nearlier result by Kivva who gave a lower bound on motion of the form $n/c_d$,\nwhere $c_d$ depends exponentially on $d$. As a corollary we derive a\nquasipolynomial upper bound for the automorphism group of a primitive\ndistance-regular graph acting edge-transitively on the graph and on its\ndistance-2 graph. The proofs use elementary combinatorial arguments and do not\ndepend on the classification of finite simple groups.",
            "author": [
                "L\u00e1szl\u00f3 Pyber",
                "Saveliy V. Skresanov"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00383v1",
                "http://arxiv.org/pdf/2312.00383v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.GR",
                "05E30 (Primary), 20B25 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00368v1",
            "title": "The upper bound of the spectral radius for the hypergraphs without\n  Berge-graphs",
            "updated": "2023-12-01T06:18:28Z",
            "published": "2023-12-01T06:18:28Z",
            "summary": "The spectral analogue of the Tur\\'{a}n type problem for hypergraphs is to\ndetermine the maximum spectral radius for the hypergraphs of order $n$ that do\nnot contain a given hypergraph. For the hypergraphs among the set of the\nconnected linear $3$-uniform hypergraphs on $n$ vertices without the\nBerge-$C_l$, we present two upper bounds for their spectral radius and\n$\\alpha$-spectral radius, which are related to $n$,$l$ and $\\alpha$, where\n$C_l$ is a cycle of length $l$ with $l\\geqslant 5$, $n\\geqslant 3$ and $0\n\\leqslant \\alpha<1$. Let $B_s$ be an $s$-book with $s\\geqslant2$ and $K_{s,t}$\nbe a complete bipartite graph with two parts of size $s$ and $t$, respectively,\nwhere $s,t \\geqslant 1$. For the hypergraphs among the set of the connected\nlinear $k$-uniform hypergraphs on $n$ vertices without the Berge-$\\{B_s,\nK_{2,t}\\}$, we derive two upper bounds for their spectral radius and\n$\\alpha$-spectral radius, which depend on $n$, $k$, $s$, and $\\alpha$, where\n$n$,$k\\geqslant 3$,$s\\geqslant 2$,$1\\leqslant t\\leqslant\n\\frac{1}{2}(6k^2-15k+10)(s-1)+1$, and $0\\leqslant \\alpha <1$.",
            "author": [
                "Wen-Huan Wang",
                "Lou-Jun Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00368v1",
                "http://arxiv.org/pdf/2312.00368v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00356v1",
            "title": "Transfer learning for predicting source terms of principal component\n  transport in chemically reactive flow",
            "updated": "2023-12-01T05:18:35Z",
            "published": "2023-12-01T05:18:35Z",
            "summary": "The objective of this study is to evaluate whether the number of requisite\ntraining samples can be reduced with the use of various transfer learning\nmodels for predicting, for example, the chemical source terms of the\ndata-driven reduced-order model that represents the homogeneous ignition\nprocess of a hydrogen/air mixture. Principal component analysis is applied to\nreduce the dimensionality of the hydrogen/air mixture in composition space.\nArtificial neural networks (ANNs) are used to tabulate the reaction rates of\nprincipal components, and subsequently, a system of ordinary differential\nequations is solved. As the number of training samples decreases at the target\ntask (i.e.,for T0 > 1000 K and various phi), the reduced-order model fails to\npredict the ignition evolution of a hydrogen/air mixture. Three transfer\nlearning strategies are then applied to the training of the ANN model with a\nsparse dataset. The performance of the reduced-order model with a sparse\ndataset is found to be remarkably enhanced if the training of the ANN model is\nrestricted by a regularization term that controls the degree of knowledge\ntransfer from source to target tasks. To this end, a novel transfer learning\nmethod is introduced, parameter control via partial initialization and\nregularization (PaPIR), whereby the amount of knowledge transferred is\nsystemically adjusted for the initialization and regularization of the ANN\nmodel in the target task. It is found that an additional performance gain can\nbe achieved by changing the initialization scheme of the ANN model in the\ntarget task when the task similarity between source and target tasks is\nrelatively low.",
            "author": [
                "Ki Sung Jung",
                "Tarek Echekki",
                "Jacqueline H. Chen",
                "Mohammad Khalil"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00356v1",
                "http://arxiv.org/pdf/2312.00356v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00353v1",
            "title": "On Exploring the Reasoning Capability of Large Language Models with\n  Knowledge Graphs",
            "updated": "2023-12-01T05:08:47Z",
            "published": "2023-12-01T05:08:47Z",
            "summary": "This paper examines the capacity of LLMs to reason with knowledge graphs\nusing their internal knowledge graph, i.e., the knowledge graph they learned\nduring pre-training. Two research questions are formulated to investigate the\naccuracy of LLMs in recalling information from pre-training knowledge graphs\nand their ability to infer knowledge graph relations from context. To address\nthese questions, we employ LLMs to perform four distinct knowledge graph\nreasoning tasks. Furthermore, we identify two types of hallucinations that may\noccur during knowledge reasoning with LLMs: content and ontology hallucination.\nOur experimental results demonstrate that LLMs can successfully tackle both\nsimple and complex knowledge graph reasoning tasks from their own memory, as\nwell as infer from input context.",
            "author": [
                "Pei-Chi Lo",
                "Yi-Hang Tsai",
                "Ee-Peng Lim",
                "San-Yih Hwang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00353v1",
                "http://arxiv.org/pdf/2312.00353v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00351v2",
            "title": "Manipulating the Label Space for In-Context Classification",
            "updated": "2023-12-06T04:19:04Z",
            "published": "2023-12-01T04:57:20Z",
            "summary": "After pre-training by generating the next word conditional on previous words,\nthe Language Model (LM) acquires the ability of In-Context Learning (ICL) that\ncan learn a new task conditional on the context of the given in-context\nexamples (ICEs). Similarly, visually-conditioned Language Modelling is also\nused to train Vision-Language Models (VLMs) with ICL ability. However, such\nVLMs typically exhibit weaker classification abilities compared to contrastive\nlearning-based models like CLIP, since the Language Modelling objective does\nnot directly contrast whether an object is paired with a text. To improve the\nICL of classification, using more ICEs to provide more knowledge is a\nstraightforward way. However, this may largely increase the selection time, and\nmore importantly, the inclusion of additional in-context images tends to extend\nthe length of the in-context sequence beyond the processing capacity of a VLM.\nTo alleviate these limitations, we propose to manipulate the label space of\neach ICE to increase its knowledge density, allowing for fewer ICEs to convey\nas much information as a larger set would. Specifically, we propose two\nstrategies which are Label Distribution Enhancement and Visual Descriptions\nEnhancement to improve In-context classification performance on diverse\ndatasets, including the classic ImageNet and more fine-grained datasets like\nCUB-200. Specifically, using our approach on ImageNet, we increase accuracy\nfrom 74.70\\% in a 4-shot setting to 76.21\\% with just 2 shots. surpassing CLIP\nby 0.67\\%. On CUB-200, our method raises 1-shot accuracy from 48.86\\% to\n69.05\\%, 12.15\\% higher than CLIP. The code is given in\nhttps://anonymous.4open.science/r/MLS_ICC.",
            "author": [
                "Haokun Chen",
                "Xu Yang",
                "Yuhang Huang",
                "Zihan Wu",
                "Jing Wang",
                "Xin Geng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00351v2",
                "http://arxiv.org/pdf/2312.00351v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00347v1",
            "title": "RTQ: Rethinking Video-language Understanding Based on Image-text Model",
            "updated": "2023-12-01T04:51:01Z",
            "published": "2023-12-01T04:51:01Z",
            "summary": "Recent advancements in video-language understanding have been established on\nthe foundation of image-text models, resulting in promising outcomes due to the\nshared knowledge between images and videos. However, video-language\nunderstanding presents unique challenges due to the inclusion of highly complex\nsemantic details, which result in information redundancy, temporal dependency,\nand scene complexity. Current techniques have only partially tackled these\nissues, and our quantitative analysis indicates that some of these methods are\ncomplementary. In light of this, we propose a novel framework called RTQ\n(Refine, Temporal model, and Query), which addresses these challenges\nsimultaneously. The approach involves refining redundant information within\nframes, modeling temporal relations among frames, and querying task-specific\ninformation from the videos. Remarkably, our model demonstrates outstanding\nperformance even in the absence of video-language pre-training, and the results\nare comparable with or superior to those achieved by state-of-the-art\npre-training methods.",
            "author": [
                "Xiao Wang",
                "Yaoyu Li",
                "Tian Gan",
                "Zheng Zhang",
                "Jingjing Lv",
                "Liqiang Nie"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3612152",
                "http://arxiv.org/abs/2312.00347v1",
                "http://arxiv.org/pdf/2312.00347v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00345v1",
            "title": "IEEE 802.11be Network Throughput Optimization with Multi-Link Operation\n  and AP Coordination",
            "updated": "2023-12-01T04:41:57Z",
            "published": "2023-12-01T04:41:57Z",
            "summary": "IEEE 802.11be (Wi-Fi 7) introduces a new concept called multi-link operation\n(MLO) which allows multiple Wi-Fi interfaces in different bands (2.4, 5, and 6\nGHz) to work together to increase network throughput, reduce latency, and\nimprove spectrum reuse efficiency in dense overlapping networks. To make the\nmost of MLO, a new intelligent resource allocation is needed. This paper\nproposes a model to align MLO and access point (AP) coordination in 11be. To\nmaximize network throughput, a network topology optimization problem is\nformulated for MLO with AP coordination, which is solved by exploiting the\ntotally unimodular property of the bipartite graph formed by the connection\nbetween AP and station (STA) in Wi-Fi networks. Subsequently, a proportional\nfairness algorithm is applied for radio link allocation, network throughput\noptimization considering the channel condition, and the fairness of the\nmulti-link device (MLD) data rate. The performance of the proposed algorithm on\ntwo main MLO implementations - multi-mink multi-radio (MLMR) with simultaneous\ntransmission and reception (STR), and the interplay between multiple nodes\nemploying them are evaluated through cross-layer (PHY-MAC) data rate simulation\nwith PHY abstraction.",
            "author": [
                "Lyutianyang Zhang",
                "Hao Yin",
                "Sumit Roy",
                "Liu Cao",
                "Xiangyu Gao",
                "Vanlin Sathya"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00345v1",
                "http://arxiv.org/pdf/2312.00345v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00343v1",
            "title": "OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline",
            "updated": "2023-12-01T04:35:47Z",
            "published": "2023-12-01T04:35:47Z",
            "summary": "Stereo matching, a pivotal technique in computer vision, plays a crucial role\nin robotics, autonomous navigation, and augmented reality. Despite the\ndevelopment of numerous impressive methods in recent years, replicating their\nresults and determining the most suitable architecture for practical\napplication remains challenging. Addressing this gap, our paper introduces a\ncomprehensive benchmark focusing on practical applicability rather than solely\non performance enhancement. Specifically, we develop a flexible and efficient\nstereo matching codebase, called OpenStereo. OpenStereo includes training and\ninference codes of more than 12 network models, making it, to our knowledge,\nthe most complete stereo matching toolbox available. Based on OpenStereo, we\nconducted experiments on the SceneFlow dataset and have achieved or surpassed\nthe performance metrics reported in the original paper. Additionally, we\nconduct an in-depth revisitation of recent developments in stereo matching\nthrough ablative experiments. These investigations inspired the creation of\nStereoBase, a simple yet strong baseline model. Our extensive comparative\nanalyses of StereoBase against numerous contemporary stereo matching methods on\nthe SceneFlow dataset demonstrate its remarkably strong performance. The source\ncode is available at https://github.com/XiandaGuo/OpenStereo.",
            "author": [
                "Xianda Guo",
                "Juntao Lu",
                "Chenming Zhang",
                "Yiqi Wang",
                "Yiqun Duan",
                "Tian Yang",
                "Zheng Zhu",
                "Long Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00343v1",
                "http://arxiv.org/pdf/2312.00343v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00336v1",
            "title": "Hypergraph Node Representation Learning with One-Stage Message Passing",
            "updated": "2023-12-01T04:10:00Z",
            "published": "2023-12-01T04:10:00Z",
            "summary": "Hypergraphs as an expressive and general structure have attracted\nconsiderable attention from various research domains. Most existing hypergraph\nnode representation learning techniques are based on graph neural networks, and\nthus adopt the two-stage message passing paradigm (i.e. node -> hyperedge ->\nnode). This paradigm only focuses on local information propagation and does not\neffectively take into account global information, resulting in less optimal\nrepresentations. Our theoretical analysis of representative two-stage message\npassing methods shows that, mathematically, they model different ways of local\nmessage passing through hyperedges, and can be unified into one-stage message\npassing (i.e. node -> node). However, they still only model local information.\nMotivated by this theoretical analysis, we propose a novel one-stage message\npassing paradigm to model both global and local information propagation for\nhypergraphs. We integrate this paradigm into HGraphormer, a Transformer-based\nframework for hypergraph node representation learning. HGraphormer injects the\nhypergraph structure information (local information) into Transformers (global\ninformation) by combining the attention matrix and hypergraph Laplacian.\nExtensive experiments demonstrate that HGraphormer outperforms recent\nhypergraph learning methods on five representative benchmark datasets on the\nsemi-supervised hypernode classification task, setting new state-of-the-art\nperformance, with accuracy improvements between 2.52% and 6.70%. Our code and\ndatasets are available.",
            "author": [
                "Shilin Qu",
                "Weiqing Wang",
                "Yuan-Fang Li",
                "Xin Zhou",
                "Fajie Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00336v1",
                "http://arxiv.org/pdf/2312.00336v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00334v1",
            "title": "UAV-Aided Lifelong Learning for AoI and Energy Optimization in\n  Non-Stationary IoT Networks",
            "updated": "2023-12-01T04:06:45Z",
            "published": "2023-12-01T04:06:45Z",
            "summary": "In this paper, a novel joint energy and age of information (AoI) optimization\nframework for IoT devices in a non-stationary environment is presented. In\nparticular, IoT devices that are distributed in the real-world are required to\nefficiently utilize their computing resources so as to balance the freshness of\ntheir data and their energy consumption. To optimize the performance of IoT\ndevices in such a dynamic setting, a novel lifelong reinforcement learning (RL)\nsolution that enables IoT devices to continuously adapt their policies to each\nnewly encountered environment is proposed. Given that IoT devices have limited\nenergy and computing resources, an unmanned aerial vehicle (UAV) is leveraged\nto visit the IoT devices and update the policy of each device sequentially. As\nsuch, the UAV is exploited as a mobile learning agent that can learn a shared\nknowledge base with a feature base in its training phase, and feature sets of a\nzero-shot learning method in its testing phase, to generalize between the\nenvironments. To optimize the trajectory and flying velocity of the UAV, an\nactor-critic network is leveraged so as to minimize the UAV energy consumption.\nSimulation results show that the proposed lifelong RL solution can outperform\nthe state-of-art benchmarks by enhancing the balanced cost of IoT devices by\n$8.3\\%$ when incorporating warm-start policies for unseen environments. In\naddition, our solution achieves up to $49.38\\%$ reduction in terms of energy\nconsumption by the UAV in comparison to the random flying strategy.",
            "author": [
                "Zhenzhen Gong",
                "Omar Hashash",
                "Yingze Wang",
                "Qimei Cui",
                "Wei Ni",
                "Walid Saad",
                "Kei Sakaguchi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00334v1",
                "http://arxiv.org/pdf/2312.00334v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00332v1",
            "title": "Matching Weak Informative Ontologies",
            "updated": "2023-12-01T03:56:29Z",
            "published": "2023-12-01T03:56:29Z",
            "summary": "Most existing ontology matching methods utilize the literal information to\ndiscover alignments. However, some literal information in ontologies may be\nopaque and some ontologies may not have sufficient literal information. In this\npaper, these ontologies are named as weak informative ontologies (WIOs) and it\nis challenging for existing methods to matching WIOs. On one hand, string-based\nand linguistic-based matching methods cannot work well for WIOs. On the other\nhand, some matching methods use external resources to improve their\nperformance, but collecting and processing external resources is still\ntime-consuming. To address this issue, this paper proposes a practical method\nfor matching WIOs by employing the ontology structure information to discover\nalignments. First, the semantic subgraphs are extracted from the ontology graph\nto capture the precise meanings of ontology elements. Then, a new similarity\npropagation model is designed for matching WIOs. Meanwhile, in order to avoid\nmeaningless propagation, the similarity propagation is constrained by semantic\nsubgraphs and other conditions. Consequently, the similarity propagation model\nensures a balance between efficiency and quality during matching. Finally, the\nsimilarity propagation model uses a few credible alignments as seeds to find\nmore alignments, and some useful strategies are adopted to improve the\nperformance. This matching method for WIOs has been implemented in the ontology\nmatching system Lily. Experimental results on public OAEI benchmark datasets\ndemonstrate that Lily significantly outperforms most of the state-of-the-art\nworks in both WIO matching tasks and general ontology matching tasks. In\nparticular, Lily increases the recall by a large margin, while it still obtains\nhigh precision of matching results.",
            "author": [
                "Peng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00332v1",
                "http://arxiv.org/pdf/2312.00332v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00326v1",
            "title": "Agent-OM: Leveraging Large Language Models for Ontology Matching",
            "updated": "2023-12-01T03:44:54Z",
            "published": "2023-12-01T03:44:54Z",
            "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM-based agents\nhave become revolutionary in data engineering and have been applied creatively\nin various domains, their potential for OM remains underexplored. This study\nintroduces a novel agent-powered LLM-based design paradigm for OM systems. With\nthoughtful consideration of several specific challenges to leverage LLMs for\nOM, we propose a generic framework, namely Agent-OM, consisting of two Siamese\nagents for retrieval and matching, with a set of simple prompt-based OM tools.\nOur framework is implemented in a proof-of-concept system. Evaluations of three\nOntology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM\nsystems show that our system can achieve very close results to the best\nlong-standing performance on simple OM tasks and significantly improve the\nperformance on complex and few-shot OM tasks.",
            "author": [
                "Zhangcheng Qiang",
                "Weiqing Wang",
                "Kerry Taylor"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00326v1",
                "http://arxiv.org/pdf/2312.00326v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00323v1",
            "title": "Liouville theory and the Weil-Petersson geometry of moduli space:\n  bordered, conic, and higher genus surfaces",
            "updated": "2023-12-01T03:36:19Z",
            "published": "2023-12-01T03:36:19Z",
            "summary": "Two-dimensional conformal field theory is a powerful tool to understand the\ngeometry of surfaces. Here, we study Liouville conformal field theory in the\nclassical (large central charge) limit, where it encodes the geometry of the\nmoduli space of Riemann surfaces. Generalizing previous work, we employ this to\nstudy moduli spaces of higher genus surfaces, surfaces with boundaries, and\nsurfaces with cone points. In each case, the knowledge of classical conformal\nblocks provides an extremely efficient approximation to the Weil-Petersson\nmetric on moduli space. We find detailed agreement with analytic results for\nvolumes and geodesic lengths on moduli space.",
            "author": [
                "Kale Colville",
                "Sarah M. Harrison",
                "Alexander Maloney",
                "Keivan Namjou"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00323v1",
                "http://arxiv.org/pdf/2312.00323v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00312v1",
            "title": "Segment Anything Model-guided Collaborative Learning Network for\n  Scribble-supervised Polyp Segmentation",
            "updated": "2023-12-01T03:07:13Z",
            "published": "2023-12-01T03:07:13Z",
            "summary": "Polyp segmentation plays a vital role in accurately locating polyps at an\nearly stage, which holds significant clinical importance for the prevention of\ncolorectal cancer. Various polyp segmentation methods have been developed using\nfully-supervised deep learning techniques. However, pixel-wise annotation for\npolyp images by physicians during the diagnosis is both time-consuming and\nexpensive. Moreover, visual foundation models such as the Segment Anything\nModel (SAM) have shown remarkable performance. Nevertheless, directly applying\nSAM to medical segmentation may not produce satisfactory results due to the\ninherent absence of medical knowledge. In this paper, we propose a novel\nSAM-guided Collaborative Learning Network (SAM-CLNet) for scribble-supervised\npolyp segmentation, enabling a collaborative learning process between our\nsegmentation network and SAM to boost the model performance. Specifically, we\nfirst propose a Cross-level Enhancement and Aggregation Network (CEA-Net) for\nweakly-supervised polyp segmentation. Within CEA-Net, we propose a Cross-level\nEnhancement Module (CEM) that integrates the adjacent features to enhance the\nrepresentation capabilities of different resolution features. Additionally, a\nFeature Aggregation Module (FAM) is employed to capture richer features across\nmultiple levels. Moreover, we present a box-augmentation strategy that combines\nthe segmentation maps generated by CEA-Net with scribble annotations to create\nmore precise prompts. These prompts are then fed into SAM, generating\nsegmentation SAM-guided masks, which can provide additional supervision to\ntrain CEA-Net effectively. Furthermore, we present an Image-level Filtering\nMechanism to filter out unreliable SAM-guided masks. Extensive experimental\nresults show that our SAM-CLNet outperforms state-of-the-art weakly-supervised\nsegmentation methods.",
            "author": [
                "Yiming Zhao",
                "Tao Zhou",
                "Yunqi Gu",
                "Yi Zhou",
                "Yizhe Zhang",
                "Ye Wu",
                "Huazhu Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00312v1",
                "http://arxiv.org/pdf/2312.00312v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00308v1",
            "title": "A knowledge-based data-driven (KBDD) framework for all-day\n  identification of cloud types using satellite remote sensing",
            "updated": "2023-12-01T02:58:27Z",
            "published": "2023-12-01T02:58:27Z",
            "summary": "Cloud types, as a type of meteorological data, are of particular significance\nfor evaluating changes in rainfall, heatwaves, water resources, floods and\ndroughts, food security and vegetation cover, as well as land use. In order to\neffectively utilize high-resolution geostationary observations, a\nknowledge-based data-driven (KBDD) framework for all-day identification of\ncloud types based on spectral information from Himawari-8/9 satellite sensors\nis designed. And a novel, simple and efficient network, named CldNet, is\nproposed. Compared with widely used semantic segmentation networks, including\nSegNet, PSPNet, DeepLabV3+, UNet, and ResUnet, our proposed model CldNet with\nan accuracy of 80.89+-2.18% is state-of-the-art in identifying cloud types and\nhas increased by 32%, 46%, 22%, 2%, and 39%, respectively. With the assistance\nof auxiliary information (e.g., satellite zenith/azimuth angle, solar\nzenith/azimuth angle), the accuracy of CldNet-W using visible and near-infrared\nbands and CldNet-O not using visible and near-infrared bands on the test\ndataset is 82.23+-2.14% and 73.21+-2.02%, respectively. Meanwhile, the total\nparameters of CldNet are only 0.46M, making it easy for edge deployment. More\nimportantly, the trained CldNet without any fine-tuning can predict cloud types\nwith higher spatial resolution using satellite spectral data with spatial\nresolution 0.02{\\deg}*0.02{\\deg}, which indicates that CldNet possesses a\nstrong generalization ability. In aggregate, the KBDD framework using CldNet is\na highly effective cloud-type identification system capable of providing a\nhigh-fidelity, all-day, spatiotemporal cloud-type database for many climate\nassessment fields.",
            "author": [
                "Longfeng Nie",
                "Yuntian Chen",
                "Mengge Du",
                "Changqi Sun",
                "Dongxiao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00308v1",
                "http://arxiv.org/pdf/2312.00308v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00840v1",
            "title": "Towards Redundancy-Free Sub-networks in Continual Learning",
            "updated": "2023-12-01T02:29:52Z",
            "published": "2023-12-01T02:29:52Z",
            "summary": "Catastrophic Forgetting (CF) is a prominent issue in continual learning.\nParameter isolation addresses this challenge by masking a sub-network for each\ntask to mitigate interference with old tasks. However, these sub-networks are\nconstructed relying on weight magnitude, which does not necessarily correspond\nto the importance of weights, resulting in maintaining unimportant weights and\nconstructing redundant sub-networks. To overcome this limitation, inspired by\ninformation bottleneck, which removes redundancy between adjacent network\nlayers, we propose \\textbf{\\underline{I}nformation \\underline{B}ottleneck\n\\underline{M}asked sub-network (IBM)} to eliminate redundancy within\nsub-networks. Specifically, IBM accumulates valuable information into essential\nweights to construct redundancy-free sub-networks, not only effectively\nmitigating CF by freezing the sub-networks but also facilitating new tasks\ntraining through the transfer of valuable knowledge. Additionally, IBM\ndecomposes hidden representations to automate the construction process and make\nit flexible. Extensive experiments demonstrate that IBM consistently\noutperforms state-of-the-art methods. Notably, IBM surpasses the\nstate-of-the-art parameter isolation method with a 70\\% reduction in the number\nof parameters within sub-networks and an 80\\% decrease in training time.",
            "author": [
                "Cheng Chen",
                "Jingkuan Song",
                "LianLi Gao",
                "Heng Tao Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00840v1",
                "http://arxiv.org/pdf/2312.00840v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03749v1",
            "title": "Conceptual Engineering Using Large Language Models",
            "updated": "2023-12-01T01:58:16Z",
            "published": "2023-12-01T01:58:16Z",
            "summary": "We describe a method, based on Jennifer Nado's definition of classification\nprocedures as targets of conceptual engineering, that implements such\nprocedures using a large language model. We then apply this method using data\nfrom the Wikidata knowledge graph to evaluate concept definitions from two\nparadigmatic conceptual engineering projects: the International Astronomical\nUnion's redefinition of PLANET and Haslanger's ameliorative analysis of WOMAN.\nWe discuss implications of this work for the theory and practice of conceptual\nengineering. The code and data can be found on GitHub.",
            "author": [
                "Bradley P. Allen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03749v1",
                "http://arxiv.org/pdf/2312.03749v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CY",
                "I.2.7; I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00279v1",
            "title": "Age-Based Scheduling for Mobile Edge Computing: A Deep Reinforcement\n  Learning Approach",
            "updated": "2023-12-01T01:30:49Z",
            "published": "2023-12-01T01:30:49Z",
            "summary": "With the rapid development of Mobile Edge Computing (MEC), various real-time\napplications have been deployed to benefit people's daily lives. The\nperformance of these applications relies heavily on the freshness of collected\nenvironmental information, which can be quantified by its Age of Information\n(AoI). In the traditional definition of AoI, it is assumed that the status\ninformation can be actively sampled and directly used. However, for many\nMEC-enabled applications, the desired status information is updated in an\nevent-driven manner and necessitates data processing. To better serve these\napplications, we propose a new definition of AoI and, based on the redefined\nAoI, we formulate an online AoI minimization problem for MEC systems. Notably,\nthe problem can be interpreted as a Markov Decision Process (MDP), thus\nenabling its solution through Reinforcement Learning (RL) algorithms.\nNevertheless, the traditional RL algorithms are designed for MDPs with\ncompletely unknown system dynamics and hence usually suffer long convergence\ntimes. To accelerate the learning process, we introduce Post-Decision States\n(PDSs) to exploit the partial knowledge of the system's dynamics. We also\ncombine PDSs with deep RL to further improve the algorithm's applicability,\nscalability, and robustness. Numerical results demonstrate that our algorithm\noutperforms the benchmarks under various scenarios.",
            "author": [
                "Xingqiu He",
                "Chaoqun You",
                "Tony Q. S. Quek"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00279v1",
                "http://arxiv.org/pdf/2312.00279v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00268v1",
            "title": "Academic competitions",
            "updated": "2023-12-01T01:01:04Z",
            "published": "2023-12-01T01:01:04Z",
            "summary": "Academic challenges comprise effective means for (i) advancing the state of\nthe art, (ii) putting in the spotlight of a scientific community specific\ntopics and problems, as well as (iii) closing the gap for under represented\ncommunities in terms of accessing and participating in the shaping of research\nfields. Competitions can be traced back for centuries and their achievements\nhave had great influence in our modern world. Recently, they (re)gained\npopularity, with the overwhelming amounts of data that is being generated in\ndifferent domains, as well as the need of pushing the barriers of existing\nmethods, and available tools to handle such data. This chapter provides a\nsurvey of academic challenges in the context of machine learning and related\nfields. We review the most influential competitions in the last few years and\nanalyze challenges per area of knowledge. The aims of scientific challenges,\ntheir goals, major achievements and expectations for the next few years are\nreviewed.",
            "author": [
                "Hugo Jair Escalante",
                "Aleksandra Kruchinina"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00268v1",
                "http://arxiv.org/pdf/2312.00268v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00835v1",
            "title": "Normal Frequency method for finding the exixtense of maximum clique of\n  the clique complex",
            "updated": "2023-12-01T00:48:23Z",
            "published": "2023-12-01T00:48:23Z",
            "summary": "Determining the existence of $k$-clique in the arbitrary graph is one of the\nNP problems. We suggest a novel way to determine the existence of $k$-clique in\nthe clique complex $G$ under specific conditions, by using the normal mode and\neigenvector of the Laplace matrix of $G$.",
            "author": [
                "Youngik Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00835v1",
                "http://arxiv.org/pdf/2312.00835v1"
            ],
            "primary_category": "physics.gen-ph",
            "category": [
                "physics.gen-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00262v1",
            "title": "Augmented Kinesthetic Teaching: Enhancing Task Execution Efficiency\n  through Intuitive Human Instructions",
            "updated": "2023-12-01T00:30:31Z",
            "published": "2023-12-01T00:30:31Z",
            "summary": "In this paper, we present a complete and efficient implementation of a\nknowledge-sharing augmented kinesthetic teaching approach for efficient task\nexecution in robotics. Our augmented kinesthetic teaching method integrates\nintuitive human feedback, including verbal, gesture, gaze, and physical\nguidance, to facilitate the extraction of multiple layers of task information\nincluding control type, attention direction, input and output type, action\nstate change trigger, etc., enhancing the adaptability and autonomy of robots\nduring task execution. We propose an efficient Programming by Demonstration\n(PbD) framework for users with limited technical experience to teach the robot\nin an intuitive manner. The proposed framework provides an interface for such\nusers to teach customized tasks using high-level commands, with the goal of\nachieving a smoother teaching experience and task execution. This is\ndemonstrated with the sample task of pouring water.",
            "author": [
                "Cheng Tang",
                "Jiaming Zhong",
                "Yue Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00262v1",
                "http://arxiv.org/pdf/2312.00262v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00245v1",
            "title": "SPAM: Secure & Private Aircraft Management",
            "updated": "2023-11-30T23:16:45Z",
            "published": "2023-11-30T23:16:45Z",
            "summary": "With the rising use of aircrafts for operations ranging from disaster-relief\nto warfare, there is a growing risk of adversarial attacks. Malicious entities\noften only require the location of the aircraft for these attacks. Current\nsatellite-aircraft communication and tracking protocols put aircrafts at risk\nif the satellite is compromised, due to computation being done in plaintext. In\nthis work, we present \\texttt{SPAM}, a private, secure, and accurate system\nthat allows satellites to efficiently manage and maintain tracking angles for\naircraft fleets without learning aircrafts' locations. \\texttt{SPAM} is built\nupon multi-party computation and zero-knowledge proofs to guarantee privacy and\nhigh efficiency. While catered towards aircrafts, \\texttt{SPAM}'s\nzero-knowledge fleet management can be easily extended to the IoT, with very\nlittle overhead.",
            "author": [
                "Yaman Jandali",
                "Nojan Sheybani",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00245v1",
                "http://arxiv.org/pdf/2312.00245v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00240v1",
            "title": "Betti graphs and atomization of Puiseux monoids",
            "updated": "2023-11-30T22:57:05Z",
            "published": "2023-11-30T22:57:05Z",
            "summary": "Let $M$ be a Puiseux monoid, that is, a monoid consisting of nonnegative\nrationals (under addition). A nonzero element of $M$ is called an atom if its\nonly decomposition as a sum of two elements in $M$ is the trivial decomposition\n(i.e., one of the summands is $0$), while a nonzero element $b \\in M$ is called\natomic if it can be expressed as a sum of finitely many atoms allowing\nrepetitions: this formal sum of atoms is called an (additive) factorization of\n$b$. The monoid $M$ is called atomic if every nonzero element of $M$ is atomic.\nIn this paper, we study factorizations in atomic Puiseux monoids through the\nlens of their associated Betti graphs. The Betti graph of $b \\in M$ is the\ngraph whose vertices are the factorizations of $b$ with edges between\nfactorizations that share at least one atom. Betti graphs have been useful in\nthe literature to understand several factorization invariants in the more\ngeneral class of atomic monoids.",
            "author": [
                "Scott T. Chapman",
                "Joshua Jang",
                "Jason Mao",
                "Skyler Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00240v1",
                "http://arxiv.org/pdf/2312.00240v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "Primary: 13F15, 13A05, Secondary: 20M13, 13F05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00238v1",
            "title": "Self-similarity of Communities of the ABCD Model",
            "updated": "2023-11-30T22:52:39Z",
            "published": "2023-11-30T22:52:39Z",
            "summary": "The Artificial Benchmark for Community Detection (ABCD) graph is a random\ngraph model with community structure and power-law distribution for both\ndegrees and community sizes. The model generates graphs similar to the\nwell-known LFR model but it is faster and can be investigated analytically.\n  In this paper, we show that the ABCD model exhibits some interesting\nself-similar behaviour, namely, the degree distribution of ground-truth\ncommunities is asymptotically the same as the degree distribution of the whole\ngraph (appropriately normalized based on their sizes). As a result, we can not\nonly estimate the number of edges induced by each community but also the number\nof self-loops and multi-edges generated during the process. Understanding these\nquantities is important as (a) rewiring self-loops and multi-edges to keep the\ngraph simple is an expensive part of the algorithm, and (b) every rewiring\ncauses the underlying configuration models to deviate slightly from uniform\nsimple graphs on their corresponding degree sequences.",
            "author": [
                "Jordan Barrett",
                "Bogumil Kaminski",
                "Pawel Pralat",
                "Francois Theberge"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00238v1",
                "http://arxiv.org/pdf/2312.00238v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.DM",
                "cs.LG",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00237v1",
            "title": "Negotiated Representations to Prevent Forgetting in Machine Learning\n  Applications",
            "updated": "2023-11-30T22:43:50Z",
            "published": "2023-11-30T22:43:50Z",
            "summary": "Catastrophic forgetting is a significant challenge in the field of machine\nlearning, particularly in neural networks. When a neural network learns to\nperform well on a new task, it often forgets its previously acquired knowledge\nor experiences. This phenomenon occurs because the network adjusts its weights\nand connections to minimize the loss on the new task, which can inadvertently\noverwrite or disrupt the representations that were crucial for the previous\ntasks. As a result, the the performance of the network on earlier tasks\ndeteriorates, limiting its ability to learn and adapt to a sequence of tasks.\nIn this paper, we propose a novel method for preventing catastrophic forgetting\nin machine learning applications, specifically focusing on neural networks. Our\napproach aims to preserve the knowledge of the network across multiple tasks\nwhile still allowing it to learn new information effectively. We demonstrate\nthe effectiveness of our method by conducting experiments on various benchmark\ndatasets, including Split MNIST, Split CIFAR10, Split Fashion MNIST, and Split\nCIFAR100. These datasets are created by dividing the original datasets into\nseparate, non overlapping tasks, simulating a continual learning scenario where\nthe model needs to learn multiple tasks sequentially without forgetting the\nprevious ones. Our proposed method tackles the catastrophic forgetting problem\nby incorporating negotiated representations into the learning process, which\nallows the model to maintain a balance between retaining past experiences and\nadapting to new tasks. By evaluating our method on these challenging datasets,\nwe aim to showcase its potential for addressing catastrophic forgetting and\nimproving the performance of neural networks in continual learning settings.",
            "author": [
                "Nuri Korhan",
                "Ceren \u00d6ner"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00237v1",
                "http://arxiv.org/pdf/2312.00237v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00236v1",
            "title": "Brainformer: Modeling MRI Brain Functions to Machine Vision",
            "updated": "2023-11-30T22:39:23Z",
            "published": "2023-11-30T22:39:23Z",
            "summary": "\"Perception is reality\". Human perception plays a vital role in forming\nbeliefs and understanding reality. Exploring how the human brain works in the\nvisual system facilitates bridging the gap between human visual perception and\ncomputer vision models. However, neuroscientists study the brain via\nNeuroimaging, i.e., Functional Magnetic Resonance Imaging (fMRI), to discover\nthe brain's functions. These approaches face interpretation challenges where\nfMRI data can be complex and require expertise. Therefore, neuroscientists make\ninferences about cognitive processes based on patterns of brain activities,\nwhich can lead to potential misinterpretation or limited functional\nunderstanding. In this work, we first present a simple yet effective\nBrainformer approach, a novel Transformer-based framework, to analyze the\npatterns of fMRI in the human perception system from the machine learning\nperspective. Secondly, we introduce a novel mechanism incorporating fMRI, which\nrepresents the human brain activities, as the supervision for the machine\nvision model. This work also introduces a novel perspective on transferring\nknowledge from human perception to neural networks. Through our experiments, we\ndemonstrated that by leveraging fMRI information, the machine vision model can\nachieve potential results compared to the current State-of-the-art methods in\nvarious image recognition tasks.",
            "author": [
                "Xuan-Bac Nguyen",
                "Xin Li",
                "Samee U. Khan",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00236v1",
                "http://arxiv.org/pdf/2312.00236v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00234v1",
            "title": "Deep Equilibrium Based Neural Operators for Steady-State PDEs",
            "updated": "2023-11-30T22:34:57Z",
            "published": "2023-11-30T22:34:57Z",
            "summary": "Data-driven machine learning approaches are being increasingly used to solve\npartial differential equations (PDEs). They have shown particularly striking\nsuccesses when training an operator, which takes as input a PDE in some family,\nand outputs its solution. However, the architectural design space, especially\ngiven structural knowledge of the PDE family of interest, is still poorly\nunderstood. We seek to remedy this gap by studying the benefits of weight-tied\nneural network architectures for steady-state PDEs. To achieve this, we first\ndemonstrate that the solution of most steady-state PDEs can be expressed as a\nfixed point of a non-linear operator. Motivated by this observation, we propose\nFNO-DEQ, a deep equilibrium variant of the FNO architecture that directly\nsolves for the solution of a steady-state PDE as the infinite-depth fixed point\nof an implicit operator layer using a black-box root solver and differentiates\nanalytically through this fixed point resulting in $\\mathcal{O}(1)$ training\nmemory. Our experiments indicate that FNO-DEQ-based architectures outperform\nFNO-based baselines with $4\\times$ the number of parameters in predicting the\nsolution to steady-state PDEs such as Darcy Flow and steady-state\nincompressible Navier-Stokes. Finally, we show FNO-DEQ is more robust when\ntrained with datasets with more noisy observations than the FNO-based\nbaselines, demonstrating the benefits of using appropriate inductive biases in\narchitectural design for different neural network based PDE solvers. Further,\nwe show a universal approximation result that demonstrates that FNO-DEQ can\napproximate the solution to any steady-state PDE that can be written as a fixed\npoint equation.",
            "author": [
                "Tanya Marwah",
                "Ashwini Pokle",
                "J. Zico Kolter",
                "Zachary C. Lipton",
                "Jianfeng Lu",
                "Andrej Risteski"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00234v1",
                "http://arxiv.org/pdf/2312.00234v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00232v1",
            "title": "Uncertainty in Graph Contrastive Learning with Bayesian Neural Networks",
            "updated": "2023-11-30T22:32:24Z",
            "published": "2023-11-30T22:32:24Z",
            "summary": "Graph contrastive learning has shown great promise when labeled data is\nscarce, but large unlabeled datasets are available. However, it often does not\ntake uncertainty estimation into account. We show that a variational Bayesian\nneural network approach can be used to improve not only the uncertainty\nestimates but also the downstream performance on semi-supervised\nnode-classification tasks. Moreover, we propose a new measure of uncertainty\nfor contrastive learning, that is based on the disagreement in likelihood due\nto different positive samples.",
            "author": [
                "Alexander M\u00f6llers",
                "Alexander Immer",
                "Elvin Isufi",
                "Vincent Fortuin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00232v1",
                "http://arxiv.org/pdf/2312.00232v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00189v1",
            "title": "HeTriNet: Heterogeneous Graph Triplet Attention Network for\n  Drug-Target-Disease Interaction",
            "updated": "2023-11-30T20:55:57Z",
            "published": "2023-11-30T20:55:57Z",
            "summary": "Modeling the interactions between drugs, targets, and diseases is paramount\nin drug discovery and has significant implications for precision medicine and\npersonalized treatments. Current approaches frequently consider drug-target or\ndrug-disease interactions individually, ignoring the interdependencies among\nall three entities. Within human metabolic systems, drugs interact with protein\ntargets in cells, influencing target activities and subsequently impacting\nbiological pathways to promote healthy functions and treat diseases. Moving\nbeyond binary relationships and exploring tighter triple relationships is\nessential to understanding drugs' mechanism of action (MoAs). Moreover,\nidentifying the heterogeneity of drugs, targets, and diseases, along with their\ndistinct characteristics, is critical to model these complex interactions\nappropriately. To address these challenges, we effectively model the\ninterconnectedness of all entities in a heterogeneous graph and develop a novel\nHeterogeneous Graph Triplet Attention Network (\\texttt{HeTriNet}).\n\\texttt{HeTriNet} introduces a novel triplet attention mechanism within this\nheterogeneous graph structure. Beyond pairwise attention as the importance of\nan entity for the other one, we define triplet attention to model the\nimportance of pairs for entities in the drug-target-disease triplet prediction\nproblem. Experimental results on real-world datasets show that\n\\texttt{HeTriNet} outperforms several baselines, demonstrating its remarkable\nproficiency in uncovering novel drug-target-disease relationships.",
            "author": [
                "Farhan Tanvir",
                "Khaled Mohammed Saifuddin",
                "Tanvir Hossain",
                "Arunkumar Bagavathi",
                "Esra Akbas"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00189v1",
                "http://arxiv.org/pdf/2312.00189v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00183v1",
            "title": "RNA-KG: An ontology-based knowledge graph for representing interactions\n  involving RNA molecules",
            "updated": "2023-11-30T20:46:20Z",
            "published": "2023-11-30T20:46:20Z",
            "summary": "The \"RNA world\" represents a novel frontier for the study of fundamental\nbiological processes and human diseases and is paving the way for the\ndevelopment of new drugs tailored to the patient's biomolecular\ncharacteristics. Although scientific data about coding and non-coding RNA\nmolecules are continuously produced and available from public repositories,\nthey are scattered across different databases and a centralized, uniform, and\nsemantically consistent representation of the \"RNA world\" is still lacking. We\npropose RNA-KG, a knowledge graph encompassing biological knowledge about RNAs\ngathered from more than 50 public databases, integrating functional\nrelationships with genes, proteins, and chemicals and ontologically grounded\nbiomedical concepts. To develop RNA-KG, we first identified, pre-processed, and\ncharacterized each data source; next, we built a meta-graph that provides an\nontological description of the KG by representing all the bio-molecular\nentities and medical concepts of interest in this domain, as well as the types\nof interactions connecting them. Finally, we leveraged an instance-based\nsemantically abstracted knowledge model to specify the ontological alignment\naccording to which RNA-KG was generated. RNA-KG can be downloaded in different\nformats and also queried by a SPARQL endpoint. A thorough topological analysis\nof the resulting heterogeneous graph provides further insights into the\ncharacteristics of the \"RNA world\". RNA-KG can be both directly explored and\nvisualized, and/or analyzed by applying computational methods to infer\nbio-medical knowledge from its heterogeneous nodes and edges. The resource can\nbe easily updated with new experimental data, and specific views of the overall\nKG can be extracted according to the bio-medical problem to be studied.",
            "author": [
                "Emanuele Cavalleri",
                "Alberto Cabri",
                "Mauricio Soto-Gomez",
                "Sara Bonfitto",
                "Paolo Perlasca",
                "Jessica Gliozzo",
                "Tiffany J. Callahan",
                "Justin Reese",
                "Peter N Robinson",
                "Elena Casiraghi",
                "Giorgio Valentini",
                "Marco Mesiti"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00183v1",
                "http://arxiv.org/pdf/2312.00183v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00174v1",
            "title": "Compression of end-to-end non-autoregressive image-to-speech system for\n  low-resourced devices",
            "updated": "2023-11-30T20:13:10Z",
            "published": "2023-11-30T20:13:10Z",
            "summary": "People with visual impairments have difficulty accessing touchscreen-enabled\npersonal computing devices like mobile phones and laptops. The image-to-speech\n(ITS) systems can assist them in mitigating this problem, but their huge model\nsize makes it extremely hard to be deployed on low-resourced embedded devices.\nIn this paper, we aim to overcome this challenge by developing an efficient\nendto-end neural architecture for generating audio from tiny segments of\ndisplay content on low-resource devices. We introduced a vision\ntransformers-based image encoder and utilized knowledge distillation to\ncompress the model from 6.1 million to 2.46 million parameters. Human and\nautomatic evaluation results show that our approach leads to a very minimal\ndrop in performance and can speed up the inference time by 22%.",
            "author": [
                "Gokul Srinivasagan",
                "Michael Deisher",
                "Munir Georges"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00174v1",
                "http://arxiv.org/pdf/2312.00174v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00166v1",
            "title": "Causes and consequences of dispersal in biodiverse spatially structured\n  systems: what is old and what is new?",
            "updated": "2023-11-30T19:57:01Z",
            "published": "2023-11-30T19:57:01Z",
            "summary": "Dispersal is a well recognized driver of ecological and evolutionary\ndynamics, and simultaneously an evolving trait. Dispersal evolution has\ntraditionally been studied in single-species metapopulations so that it remains\nunclear how dispersal evolves in spatially structured communities and food\nwebs. Since most natural systems are biodiverse and spatially structured, and\nthus affected by dispersal and its evolution, this knowledge gap should be\nbridged.\n  Here we discuss whether knowledge established in single-species systems holds\nin spatially structured multispecies systems and highlight generally valid and\nfundamental principles. Most biotic interactions form the ecological theatre\nfor the evolutionary dispersal play because interactions mediate patterns of\nfitness expectations in space and time. While this allows for a simple\ntransposition of certain known drivers to a multispecies context, other drivers\nmay require more complex transpositions, or might not be transferred. We\ndiscuss an important quantitative modulator of dispersal evolution in the\nincreased trait dimensionality of biodiverse meta-systems and an additional\ndriver in co-dispersal.\n  We speculate that scale and selection pressure mismatches due to\nco-dispersal, together with increased trait dimensionality may lead to slower\nand more \"diffuse\" evolution in biodiverse meta-systems. Open questions and\npotential consequences in both ecological and evolutionary terms call for more\ninvestigation.",
            "author": [
                "Emanuel A. Fronhofer",
                "Dries Bonte",
                "Elvire Bestion",
                "Julien Cote",
                "Jhelam N. Deshpande",
                "Alison B. Duncan",
                "Thomas Hovestadt",
                "Oliver Kaltz",
                "Sally Keith",
                "Hanna Kokko",
                "Delphine Legrand",
                "Sarthak P. Malusare",
                "Thomas Parmentier",
                "Camille Saade",
                "Nicolas Schtickzelle",
                "Giacomo Zilio",
                "Fran\u00e7ois Massol"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00166v1",
                "http://arxiv.org/pdf/2312.00166v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00827v1",
            "title": "A Unified Framework for Connecting Noise Modeling to Boost Noise\n  Detection",
            "updated": "2023-11-30T19:24:47Z",
            "published": "2023-11-30T19:24:47Z",
            "summary": "Noisy labels can impair model performance, making the study of learning with\nnoisy labels an important topic. Two conventional approaches are noise modeling\nand noise detection. However, these two methods are typically studied\nindependently, and there has been limited work on their collaboration. In this\nwork, we explore the integration of these two approaches, proposing an\ninterconnected structure with three crucial blocks: noise modeling, source\nknowledge identification, and enhanced noise detection using noise\nsource-knowledge-integration methods. This collaboration structure offers\nadvantages such as discriminating hard negatives and preserving genuinely clean\nlabels that might be suspiciously noisy. Our experiments on four datasets,\nfeaturing three types of noise and different combinations of each block,\ndemonstrate the efficacy of these components' collaboration. Our collaborative\nstructure methods achieve up to a 10% increase in top-1 classification accuracy\nin synthesized noise datasets and 3-5% in real-world noisy datasets. The\nresults also suggest that these components make distinct contributions to\noverall performance across various noise scenarios. These findings provide\nvaluable insights for designing noisy label learning methods customized for\nspecific noise scenarios in the future. Our code is accessible to the public.",
            "author": [
                "Siqi Wang",
                "Chau Pham",
                "Bryan A. Plummer"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00827v1",
                "http://arxiv.org/pdf/2312.00827v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00135v1",
            "title": "Conformal graphs as twisted partition functions",
            "updated": "2023-11-30T19:00:12Z",
            "published": "2023-11-30T19:00:12Z",
            "summary": "We show that a class of $L$-loop conformal ladder graphs correspond to\ntwisted partition functions of free massive complex scalars in $d=2L+1$\ndimensions. The graphs arise as four-point functions in certain two- and\nfour-dimensional conformal fishnet models. The twisted thermal two-point\nfunction of the scalars is a generator of such conformal graphs for all loops.\nWe argue that this correspondence is seeded by a system of two decoupled\nharmonic oscillators twisted by an imaginary chemical potential. We find a\nnumber of algebraic and differential relations among the conformal graphs which\nmirror the underlying free dynamics.",
            "author": [
                "Manthos Karydas",
                "Songyuan Li",
                "Anastasios C. Petkou",
                "Matthieu Vilatte"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00135v1",
                "http://arxiv.org/pdf/2312.00135v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18841v1",
            "title": "On the maximum black hole mass at solar metallicity",
            "updated": "2023-11-30T18:59:58Z",
            "published": "2023-11-30T18:59:58Z",
            "summary": "Recently, models obtained with MESA and Genec detailed evolutionary codes\nindicated that black holes formed at solar metallicity (Z = 0.014) may reach 35\nM$_\\odot$ or even higher masses. We perform a replication study to assess the\nvalidity of these results. We use MESA and Genec to calculate a suite of\nmassive stellar models at solar metallicity. In our calculations we employ\nupdated physics important for massive star evolution (moderate rotation, high\novershooting, magnetic angular momentum transport). The key feature of our\nmodels is a new prescription for stellar winds for massive stars that updates\nsignificantly previous calculations. We find a maximum BH mass of 28 M$_\\odot$\nat Z = 0.014. The most massive BHs are predicted to form from stars with\ninitial mass $M_{\\rm zams}\\sim$40 M$_\\odot$ and for stars with $M_{\\rm zams}$\nabove 200 M$_\\odot$. The lower mass BHs found in our study mostly result from\nthe updated wind mass loss prescriptions. While we acknowledge the inherent\nuncertainties in stellar evolution modelling, our study underscores the\nimportance of employing the most up-to-date knowledge of key physics (e.g.,\nstellar wind mass loss rates) in BH mass predictions.",
            "author": [
                "Amedeo Romagnolo",
                "Alex C. Gormaz-Matamala",
                "Krzysztof Belczynski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18841v1",
                "http://arxiv.org/pdf/2311.18841v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00093v1",
            "title": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs",
            "updated": "2023-11-30T18:59:58Z",
            "published": "2023-11-30T18:59:58Z",
            "summary": "As pretrained text-to-image diffusion models become increasingly powerful,\nrecent efforts have been made to distill knowledge from these text-to-image\npretrained models for optimizing a text-guided 3D model. Most of the existing\nmethods generate a holistic 3D model from a plain text input. This can be\nproblematic when the text describes a complex scene with multiple objects,\nbecause the vectorized text embeddings are inherently unable to capture a\ncomplex description with multiple entities and relationships. Holistic 3D\nmodeling of the entire scene further prevents accurate grounding of text\nentities and concepts. To address this limitation, we propose GraphDreamer, a\nnovel framework to generate compositional 3D scenes from scene graphs, where\nobjects are represented as nodes and their interactions as edges. By exploiting\nnode and edge information in scene graphs, our method makes better use of the\npretrained text-to-image diffusion model and is able to fully disentangle\ndifferent objects without image-level supervision. To facilitate modeling of\nobject-wise relationships, we use signed distance fields as representation and\nimpose a constraint to avoid inter-penetration of objects. To avoid manual\nscene graph creation, we design a text prompt for ChatGPT to generate scene\ngraphs based on text inputs. We conduct both qualitative and quantitative\nexperiments to validate the effectiveness of GraphDreamer in generating\nhigh-fidelity compositional 3D scenes with disentangled object entities.",
            "author": [
                "Gege Gao",
                "Weiyang Liu",
                "Anpei Chen",
                "Andreas Geiger",
                "Bernhard Sch\u00f6lkopf"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00093v1",
                "http://arxiv.org/pdf/2312.00093v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18836v1",
            "title": "PoseGPT: Chatting about 3D Human Pose",
            "updated": "2023-11-30T18:59:52Z",
            "published": "2023-11-30T18:59:52Z",
            "summary": "We introduce PoseGPT, a framework employing Large Language Models (LLMs) to\nunderstand and reason about 3D human poses from images or textual descriptions.\nOur work is motivated by the human ability to intuitively understand postures\nfrom a single image or a brief description, a process that intertwines image\ninterpretation, world knowledge, and an understanding of body language.\nTraditional human pose estimation methods, whether image-based or text-based,\noften lack holistic scene comprehension and nuanced reasoning, leading to a\ndisconnect between visual data and its real-world implications. PoseGPT\naddresses these limitations by embedding SMPL poses as a distinct signal token\nwithin a multi-modal LLM, enabling direct generation of 3D body poses from both\ntextual and visual inputs. This approach not only simplifies pose prediction\nbut also empowers LLMs to apply their world knowledge in reasoning about human\nposes, fostering two advanced tasks: speculative pose generation and reasoning\nabout pose estimation. These tasks involve reasoning about humans to generate\n3D poses from subtle text queries, possibly accompanied by images. We establish\nbenchmarks for these tasks, moving beyond traditional 3D pose generation and\nestimation methods. Our results show that PoseGPT outperforms existing\nmultimodal LLMs and task-sepcific methods on these newly proposed tasks.\nFurthermore, PoseGPT's ability to understand and generate 3D human poses based\non complex reasoning opens new directions in human pose analysis.",
            "author": [
                "Yao Feng",
                "Jing Lin",
                "Sai Kumar Dwivedi",
                "Yu Sun",
                "Priyanka Patel",
                "Michael J. Black"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18836v1",
                "http://arxiv.org/pdf/2311.18836v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18824v1",
            "title": "An Adaptive Framework for Generalizing Network Traffic Prediction\n  towards Uncertain Environments",
            "updated": "2023-11-30T18:58:38Z",
            "published": "2023-11-30T18:58:38Z",
            "summary": "We have developed a new framework using time-series analysis for dynamically\nassigning mobile network traffic prediction models in previously unseen\nwireless environments. Our framework selectively employs learned behaviors,\noutperforming any single model with over a 50% improvement relative to current\nstudies. More importantly, it surpasses traditional approaches without needing\nprior knowledge of a cell. While this paper focuses on network traffic\nprediction using our adaptive forecasting framework, this framework can also be\napplied to other machine learning applications in uncertain environments.\n  The framework begins with unsupervised clustering of time-series data to\nidentify unique trends and seasonal patterns. Subsequently, we apply supervised\nlearning for traffic volume prediction within each cluster. This specialization\ntowards specific traffic behaviors occurs without penalties from spatial and\ntemporal variations. Finally, the framework adaptively assigns trained models\nto new, previously unseen cells. By analyzing real-time measurements of a cell,\nour framework intelligently selects the most suitable cluster for that cell at\nany given time, with cluster assignment dynamically adjusting to\nspatio-temporal fluctuations.",
            "author": [
                "Alexander Downey",
                "Evren Tuna",
                "Alkan Soysal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18824v1",
                "http://arxiv.org/pdf/2311.18824v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18823v1",
            "title": "Initializing Models with Larger Ones",
            "updated": "2023-11-30T18:58:26Z",
            "published": "2023-11-30T18:58:26Z",
            "summary": "Weight initialization plays an important role in neural network training.\nWidely used initialization methods are proposed and evaluated for networks that\nare trained from scratch. However, the growing number of pretrained models now\noffers new opportunities for tackling this classical problem of weight\ninitialization. In this work, we introduce weight selection, a method for\ninitializing smaller models by selecting a subset of weights from a pretrained\nlarger model. This enables the transfer of knowledge from pretrained weights to\nsmaller models. Our experiments demonstrate that weight selection can\nsignificantly enhance the performance of small models and reduce their training\ntime. Notably, it can also be used together with knowledge distillation. Weight\nselection offers a new approach to leverage the power of pretrained models in\nresource-constrained settings, and we hope it can be a useful tool for training\nsmall models in the large-model era. Code is available at\nhttps://github.com/OscarXZQ/weight-selection.",
            "author": [
                "Zhiqiu Xu",
                "Yanjie Chen",
                "Kirill Vishniakov",
                "Yida Yin",
                "Zhiqiang Shen",
                "Trevor Darrell",
                "Lingjie Liu",
                "Zhuang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18823v1",
                "http://arxiv.org/pdf/2311.18823v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18803v2",
            "title": "BioCLIP: A Vision Foundation Model for the Tree of Life",
            "updated": "2023-12-04T16:13:21Z",
            "published": "2023-11-30T18:49:43Z",
            "summary": "Images of the natural world, collected by a variety of cameras, from drones\nto individual phones, are increasingly abundant sources of biological\ninformation. There is an explosion of computational methods and tools,\nparticularly computer vision, for extracting biologically relevant information\nfrom images for science and conservation. Yet most of these are bespoke\napproaches designed for a specific task and are not easily adaptable or\nextendable to new questions, contexts, and datasets. A vision model for general\norganismal biology questions on images is of timely need. To approach this, we\ncurate and release TreeOfLife-10M, the largest and most diverse ML-ready\ndataset of biology images. We then develop BioCLIP, a foundation model for the\ntree of life, leveraging the unique properties of biology captured by\nTreeOfLife-10M, namely the abundance and variety of images of plants, animals,\nand fungi, together with the availability of rich structured biological\nknowledge. We rigorously benchmark our approach on diverse fine-grained biology\nclassification tasks, and find that BioCLIP consistently and substantially\noutperforms existing baselines (by 17% to 20% absolute). Intrinsic evaluation\nreveals that BioCLIP has learned a hierarchical representation conforming to\nthe tree of life, shedding light on its strong generalizability. Our code,\nmodels and data will be made available at\nhttps://github.com/Imageomics/bioclip.",
            "author": [
                "Samuel Stevens",
                "Jiaman Wu",
                "Matthew J Thompson",
                "Elizabeth G Campolongo",
                "Chan Hee Song",
                "David Edward Carlyn",
                "Li Dong",
                "Wasila M Dahdul",
                "Charles Stewart",
                "Tanya Berger-Wolf",
                "Wei-Lun Chao",
                "Yu Su"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18803v2",
                "http://arxiv.org/pdf/2311.18803v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18797v2",
            "title": "$\u03b5$-Uniform Mixing in Discrete Quantum Walks",
            "updated": "2023-12-03T17:20:29Z",
            "published": "2023-11-30T18:43:38Z",
            "summary": "We study whether a discrete quantum walk can get arbitrarily close to a state\nwhose entries have the same absolute value over all the arcs, given that the\nwalk starts with a uniform superposition of the outgoing arcs of some vertex.\nWe characterize this phenomenon on non-bipartite graphs using the adjacency\nspectrum of the graph; in particular, if this happens in some association\nscheme and the state we get arbitrarily close to ``respects the neighborhood\",\nthen it happens regardless of the initial vertex, and the adjacency algebra of\nthe graph contains a real (regular) Hadamard matrix. We then find infinite\nfamilies of primitive strongly regular graphs that admit this phenomenon.\n  We also derive some results on a strengthening of this phenomenon called\nsimultaneous $\\epsilon$-uniform mixing, which enables local $\\epsilon$-uniform\nmixing at every vertex.",
            "author": [
                "Hanmeng Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18797v2",
                "http://arxiv.org/pdf/2311.18797v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18786v1",
            "title": "Slow graph bootstrap percolation II: Accelerating properties",
            "updated": "2023-11-30T18:36:44Z",
            "published": "2023-11-30T18:36:44Z",
            "summary": "For a graph $H$ and an $n$-vertex graph $G$, the $H$-bootstrap process on $G$\nis the process which starts with $G$ and, at every time step, adds any missing\nedges on the vertices of $G$ that complete a copy of $H$. This process\neventually stabilises and we are interested in the extremal question raised by\nBollob\\'as, of determining the maximum running time (number of time steps\nbefore stabilising) of this process, over all possible choices of $n$-vertex\ngraph $G$. In this paper, we initiate a systematic study of the asymptotics of\nthis parameter, denoted $M_H(n)$, and its dependence on properties of the graph\n$H$. Our focus is on $H$ which define relatively fast bootstrap processes, that\nis, with $M_H(n)$ being at most linear in $n$. We study the graph class of\ntrees, showing that one can bound $M_T(n)$ by a quadratic function in $v(T)$\nfor all trees $T$ and all $n$. We then go on to explore the relationship\nbetween the running time of the $H$-process and the minimum vertex degree and\nconnectivity of $H$.",
            "author": [
                "David Fabian",
                "Patrick Morris",
                "Tibor Szab\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18786v1",
                "http://arxiv.org/pdf/2311.18786v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03747v1",
            "title": "Classifying patient voice in social media data using neural networks: A\n  comparison of AI models on different data sources and therapeutic domains",
            "updated": "2023-11-30T18:35:24Z",
            "published": "2023-11-30T18:35:24Z",
            "summary": "It is essential that healthcare professionals and members of the healthcare\ncommunity can access and easily understand patient experiences in the real\nworld, so that care standards can be improved and driven towards personalised\ndrug treatment. Social media platforms and message boards are deemed suitable\nsources of patient experience information, as patients have been observed to\ndiscuss and exchange knowledge, look for and provide support online. This paper\ntests the hypothesis that not all online patient experience information can be\ntreated and collected in the same way, as a result of the inherent differences\nin the way individuals talk about their journeys, in different therapeutic\ndomains and or data sources.\n  We used linguistic analysis to understand and identify similarities between\ndatasets, across patient language, between data sources (Reddit, SocialGist)\nand therapeutic domains (cardiovascular, oncology, immunology, neurology). We\ndetected common vocabulary used by patients in the same therapeutic domain\nacross data sources, except for immunology patients, who use unique vocabulary\nbetween the two data sources, and compared to all other datasets. We combined\nlinguistically similar datasets to train classifiers (CNN, transformer) to\naccurately identify patient experience posts from social media, a task we refer\nto as patient voice classification. The cardiovascular and neurology\ntransformer classifiers perform the best in their respective comparisons for\nthe Reddit data source, achieving F1-scores of 0.865 and 1.0 respectively. The\noverall best performing classifier is the transformer classifier trained on all\ndata collected for this experiment, achieving F1-scores ranging between 0.863\nand 0.995 across all therapeutic domain and data source specific test datasets.",
            "author": [
                "Giorgos Lysandrou",
                "Roma English Owen",
                "Vanja Popovic",
                "Grant Le Brun",
                "Beatrice Alex",
                "Elizabeth A. L. Fairley"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03747v1",
                "http://arxiv.org/pdf/2312.03747v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18761v1",
            "title": "Can training neural language models on a curriculum with developmentally\n  plausible data improve alignment with human reading behavior?",
            "updated": "2023-11-30T18:03:58Z",
            "published": "2023-11-30T18:03:58Z",
            "summary": "The use of neural language models to model human behavior has met with mixed\nsuccess. While some work has found that the surprisal estimates from these\nmodels can be used to predict a wide range of human neural and behavioral\nresponses, other work studying more complex syntactic phenomena has found that\nthese surprisal estimates generate incorrect behavioral predictions. This paper\nexplores the extent to which the misalignment between empirical and\nmodel-predicted behavior can be minimized by training models on more\ndevelopmentally plausible data, such as in the BabyLM Challenge. We trained\nteacher language models on the BabyLM \"strict-small\" dataset and used sentence\nlevel surprisal estimates from these teacher models to create a curriculum. We\nfound tentative evidence that our curriculum made it easier for models to\nacquire linguistic knowledge from the training data: on the subset of tasks in\nthe BabyLM challenge suite evaluating models' grammatical knowledge of English,\nmodels first trained on the BabyLM data curriculum and then on a few randomly\nordered training epochs performed slightly better than models trained on\nrandomly ordered epochs alone. This improved linguistic knowledge acquisition\ndid not result in better alignment with human reading behavior, however: models\ntrained on the BabyLM dataset (with or without a curriculum) generated\npredictions that were as misaligned with human behavior as models trained on\nlarger less curated datasets. This suggests that training on developmentally\nplausible datasets alone is likely insufficient to generate language models\ncapable of accurately predicting human language processing.",
            "author": [
                "Aryaman Chobey",
                "Oliver Smith",
                "Anzi Wang",
                "Grusha Prasad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18761v1",
                "http://arxiv.org/pdf/2311.18761v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18760v1",
            "title": "TaskBench: Benchmarking Large Language Models for Task Automation",
            "updated": "2023-11-30T18:02:44Z",
            "published": "2023-11-30T18:02:44Z",
            "summary": "Recently, the incredible progress of large language models (LLMs) has ignited\nthe spark of task automation, which decomposes the complex tasks described by\nuser instructions into sub-tasks, and invokes external tools to execute them,\nand plays a central role in autonomous agents. However, there lacks a\nsystematic and standardized benchmark to foster the development of LLMs in task\nautomation. To this end, we introduce TaskBench to evaluate the capability of\nLLMs in task automation. Specifically, task automation can be formulated into\nthree critical stages: task decomposition, tool invocation, and parameter\nprediction to fulfill user intent. This complexity makes data collection and\nevaluation more challenging compared to common NLP tasks. To generate\nhigh-quality evaluation datasets, we introduce the concept of Tool Graph to\nrepresent the decomposed tasks in user intent, and adopt a back-instruct method\nto simulate user instruction and annotations. Furthermore, we propose TaskEval\nto evaluate the capability of LLMs from different aspects, including task\ndecomposition, tool invocation, and parameter prediction. Experimental results\ndemonstrate that TaskBench can effectively reflects the capability of LLMs in\ntask automation. Benefiting from the mixture of automated data construction and\nhuman verification, TaskBench achieves a high consistency compared to the human\nevaluation, which can be utilized as a comprehensive and faithful benchmark for\nLLM-based autonomous agents.",
            "author": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Wenqi Zhang",
                "Kan Ren",
                "Siyu Yuan",
                "Weiming Lu",
                "Dongsheng Li",
                "Yueting Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18760v1",
                "http://arxiv.org/pdf/2311.18760v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18753v1",
            "title": "A note on extremal constructions for the Erd\u0151s--Rademacher problem",
            "updated": "2023-11-30T17:54:13Z",
            "published": "2023-11-30T17:54:13Z",
            "summary": "For given positive integers $r\\ge 3$, $n$ and $e\\le \\binom{n}{2}$, the famous\nErd\\H{o}s--Rademacher problem asks for the minimum number of $r$-cliques in a\ngraph with $n$ vertices and $e$ edges. A conjecture of Lov\\'{a}sz and\nSimonovits from the 1970s states that, for every $r\\ge 3$, if $n$ is\nsufficiently large then, for every $e\\le \\binom{n}{2}$, at least one extremal\ngraph can be obtained from a complete partite graph by adding a triangle-free\ngraph into one part.\n  In this note, we explicitly write the minimum number of $r$-cliques predicted\nby the above conjecture. Also, we describe what we believe to be the set of\nextremal graphs for any $r\\ge 4$ and all large $n$, amending the previous\nconjecture of Pikhurko and Razborov.",
            "author": [
                "Xizhi Liu",
                "Oleg Pikhurko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18753v1",
                "http://arxiv.org/pdf/2311.18753v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18740v1",
            "title": "First-Order Model Checking on Monadically Stable Graph Classes",
            "updated": "2023-11-30T17:38:00Z",
            "published": "2023-11-30T17:38:00Z",
            "summary": "A graph class $\\mathscr{C}$ is called monadically stable if one cannot\ninterpret, in first-order logic, arbitrary large linear orders in colored\ngraphs from $\\mathscr{C}$. We prove that the model checking problem for\nfirst-order logic is fixed-parameter tractable on every monadically stable\ngraph class. This extends the results of [Grohe, Kreutzer, and Siebertz; J. ACM\n'17] for nowhere dense classes and of [Dreier, M\\\"ahlmann, and Siebertz; STOC\n'23] for structurally nowhere dense classes to all monadically stable classes.\n  As a complementary hardness result, we prove that for every hereditary graph\nclass $\\mathscr{C}$ that is edge-stable (excludes some half-graph as a\nsemi-induced subgraph) but not monadically stable, first-order model checking\nis $\\mathrm{AW}[*]$-hard on $\\mathscr{C}$, and $\\mathrm{W}[1]$-hard when\nrestricted to existential sentences. This confirms, in the special case of\nedge-stable classes, an on-going conjecture that the notion of monadic NIP\ndelimits the tractability of first-order model checking on hereditary classes\nof graphs.\n  For our tractability result, we first prove that monadically stable graph\nclasses have almost linear neighborhood complexity. Using this, we construct\nsparse neighborhood covers for monadically stable classes, which provides the\nmissing ingredient for the algorithm of [Dreier, M\\\"ahlmann, and Siebertz; STOC\n'23]. The key component of this construction is the usage of orders with low\ncrossing number [Welzl; SoCG '88], a tool from the area of range queries.\n  For our hardness result, we prove a new characterization of monadically\nstable graph classes in terms of forbidden induced subgraphs. We then use this\ncharacterization to show that in hereditary classes that are edge-stable but\nnot monadically stable, one can effectively interpret the class of all graphs\nusing only existential formulas.",
            "author": [
                "Jan Dreier",
                "Ioannis Eleftheriadis",
                "Nikolas M\u00e4hlmann",
                "Rose McCarty",
                "Micha\u0142 Pilipczuk",
                "Szymon Toru\u0144czyk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18740v1",
                "http://arxiv.org/pdf/2311.18740v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.DM",
                "cs.DS",
                "math.CO",
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18724v1",
            "title": "Routing-Guided Learned Product Quantization for Graph-Based Approximate\n  Nearest Neighbor Search",
            "updated": "2023-11-30T17:22:55Z",
            "published": "2023-11-30T17:22:55Z",
            "summary": "Given a vector dataset $\\mathcal{X}$, a query vector $\\vec{x}_q$, graph-based\nApproximate Nearest Neighbor Search (ANNS) aims to build a proximity graph (PG)\nas an index of $\\mathcal{X}$ and approximately return vectors with minimum\ndistances to $\\vec{x}_q$ by searching over the PG index. It suffers from the\nlarge-scale $\\mathcal{X}$ because a PG with full vectors is too large to fit\ninto the memory, e.g., a billion-scale $\\mathcal{X}$ in 128 dimensions would\nconsume nearly 600 GB memory. To solve this, Product Quantization (PQ)\nintegrated graph-based ANNS is proposed to reduce the memory usage, using\nsmaller compact codes of quantized vectors in memory instead of the large\noriginal vectors. Existing PQ methods do not consider the important routing\nfeatures of PG, resulting in low-quality quantized vectors that affect the\nANNS's effectiveness. In this paper, we present an end-to-end Routing-guided\nlearned Product Quantization (RPQ) for graph-based ANNS. It consists of (1) a\n\\textit{differentiable quantizer} used to make the standard discrete PQ\ndifferentiable to suit for back-propagation of end-to-end learning, (2) a\n\\textit{sampling-based feature extractor} used to extract neighborhood and\nrouting features of a PG, and (3) a \\textit{multi-feature joint training\nmodule} with two types of feature-aware losses to continuously optimize the\ndifferentiable quantizer. As a result, the inherent features of a PG would be\nembedded into the learned PQ, generating high-quality quantized vectors.\nMoreover, we integrate our RPQ with the state-of-the-art DiskANN and existing\npopular PGs to improve their performance. Comprehensive experiments on\nreal-world large-scale datasets (from 1M to 1B) demonstrate RPQ's superiority,\ne.g., 1.7$\\times$-4.2$\\times$ improvement on QPS at the same recall@10 of 95\\%.",
            "author": [
                "Qiang Yue",
                "Xiaoliang Xu",
                "Yuxiang Wang",
                "Yikun Tao",
                "Xuliyuan Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18724v1",
                "http://arxiv.org/pdf/2311.18724v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18694v1",
            "title": "Balancing Summarization and Change Detection in Graph Streams",
            "updated": "2023-11-30T16:39:46Z",
            "published": "2023-11-30T16:39:46Z",
            "summary": "This study addresses the issue of balancing graph summarization and graph\nchange detection. Graph summarization compresses large-scale graphs into a\nsmaller scale. However, the question remains: To what extent should the\noriginal graph be compressed? This problem is solved from the perspective of\ngraph change detection, aiming to detect statistically significant changes\nusing a stream of summary graphs. If the compression rate is extremely high,\nimportant changes can be ignored, whereas if the compression rate is extremely\nlow, false alarms may increase with more memory. This implies that there is a\ntrade-off between compression rate in graph summarization and accuracy in\nchange detection. We propose a novel quantitative methodology to balance this\ntrade-off to simultaneously realize reliable graph summarization and change\ndetection. We introduce a probabilistic structure of hierarchical latent\nvariable model into a graph, thereby designing a parameterized summary graph on\nthe basis of the minimum description length principle. The parameter specifying\nthe summary graph is then optimized so that the accuracy of change detection is\nguaranteed to suppress Type I error probability (probability of raising false\nalarms) to be less than a given confidence level. First, we provide a\ntheoretical framework for connecting graph summarization with change detection.\nThen, we empirically demonstrate its effectiveness on synthetic and real\ndatasets.",
            "author": [
                "Shintaro Fukushima",
                "Kenji Yamanishi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18694v1",
                "http://arxiv.org/pdf/2311.18694v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18674v2",
            "title": "Scalable and Lightweight Post-Quantum Authentication for Internet of\n  Things",
            "updated": "2023-12-01T19:53:35Z",
            "published": "2023-11-30T16:20:50Z",
            "summary": "Internet of Things (IoT) applications are composed of massive quantities of\nresource-limited devices that collect sensitive data with long-term operational\nand security requirements. With the threat of emerging quantum computers,\nPost-Quantum Cryptography (PQC) is a critical requirement for IoTs. In\nparticular, digital signatures offer scalable authentication with\nnon-repudiation and are an essential tool for IoTs. However, as seen in NIST\nPQC standardization, post-quantum signatures are extremely costly for\nresource-limited IoTs. Hence, there is a significant need for quantum-safe\nsignatures that respect the processing, memory, and bandwidth limitations of\nIoTs. In this paper, we created a new lightweight quantum-safe digital\nsignature referred to as INFinity-HORS (INF-HORS), which is (to the best of our\nknowledge) the first signer-optimal hash-based signature with (polynomially)\nunbounded signing capability. INF-HORS enables a verifier to non-interactively\nconstruct one-time public keys from a master public key via encrypted function\nevaluations. This strategy avoids the performance bottleneck of hash-based\nstandards (e.g., SPHINCS+) by eliminating hyper-tree structures. It also does\nnot require a trusted party or non-colliding servers to distribute public keys.\nOur performance analysis confirms that INF-HORS is magnitudes of times more\nsigner computation efficient than selected NIST PQC schemes (e.g., SPHINCS+,\nDilithium, Falcon) with a small memory footprint.",
            "author": [
                "Attila A. Yavuz",
                "Saleh Darzi",
                "Saif E. Nouma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18674v2",
                "http://arxiv.org/pdf/2311.18674v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18672v1",
            "title": "A Comparison Between Invariant and Equivariant Classical and Quantum\n  Graph Neural Networks",
            "updated": "2023-11-30T16:19:13Z",
            "published": "2023-11-30T16:19:13Z",
            "summary": "Machine learning algorithms are heavily relied on to understand the vast\namounts of data from high-energy particle collisions at the CERN Large Hadron\nCollider (LHC). The data from such collision events can naturally be\nrepresented with graph structures. Therefore, deep geometric methods, such as\ngraph neural networks (GNNs), have been leveraged for various data analysis\ntasks in high-energy physics. One typical task is jet tagging, where jets are\nviewed as point clouds with distinct features and edge connections between\ntheir constituent particles. The increasing size and complexity of the LHC\nparticle datasets, as well as the computational models used for their analysis,\ngreatly motivate the development of alternative fast and efficient\ncomputational paradigms such as quantum computation. In addition, to enhance\nthe validity and robustness of deep networks, one can leverage the fundamental\nsymmetries present in the data through the use of invariant inputs and\nequivariant layers. In this paper, we perform a fair and comprehensive\ncomparison between classical graph neural networks (GNNs) and equivariant graph\nneural networks (EGNNs) and their quantum counterparts: quantum graph neural\nnetworks (QGNNs) and equivariant quantum graph neural networks (EQGNN). The\nfour architectures were benchmarked on a binary classification task to classify\nthe parton-level particle initiating the jet. Based on their AUC scores, the\nquantum networks were shown to outperform the classical networks. However,\nseeing the computational advantage of the quantum networks in practice may have\nto wait for the further development of quantum technology and its associated\nAPIs.",
            "author": [
                "Roy T. Forestano",
                "Mar\u00e7al Comajoan Cara",
                "Gopal Ramesh Dahale",
                "Zhongtian Dong",
                "Sergei Gleyzer",
                "Daniel Justice",
                "Kyoungchul Kong",
                "Tom Magorsch",
                "Konstantin T. Matchev",
                "Katia Matcheva",
                "Eyup B. Unlu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18672v1",
                "http://arxiv.org/pdf/2311.18672v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG",
                "hep-ph",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18671v1",
            "title": "Solution to an open problem on the closeness of graphs",
            "updated": "2023-11-30T16:18:12Z",
            "published": "2023-11-30T16:18:12Z",
            "summary": "A network can be analyzed by means of many graph theoretical parameters. In\nthe context of networks analysis, closeness is a structural metric that\nevaluates a node's significance inside a network. A cactus is a connected graph\nin which any block is either a cut edge or a cycle. This paper analyzes the\ncloseness of cacti, we determine the unique graph that minimizes the closeness\nover all cacti with fixed numbers of vertices and cycles, which solves an open\nproblem proposed by Poklukar \\& \\v{Z}erovnik [Fundam. Inform. 167 (2019)\n219--234].",
            "author": [
                "Fazal Hayat",
                "Shou-Jun Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18671v1",
                "http://arxiv.org/pdf/2311.18671v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "68R10, 05C12, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18666v1",
            "title": "Action Recognition in Video Recordings from Gynecologic Laparoscopy",
            "updated": "2023-11-30T16:15:46Z",
            "published": "2023-11-30T16:15:46Z",
            "summary": "Action recognition is a prerequisite for many applications in laparoscopic\nvideo analysis including but not limited to surgical training, operation room\nplanning, follow-up surgery preparation, post-operative surgical assessment,\nand surgical outcome estimation. However, automatic action recognition in\nlaparoscopic surgeries involves numerous challenges such as (I) cross-action\nand intra-action duration variation, (II) relevant content distortion due to\nsmoke, blood accumulation, fast camera motions, organ movements, object\nocclusion, and (III) surgical scene variations due to different illuminations\nand viewpoints. Besides, action annotations in laparoscopy surgeries are\nlimited and expensive due to requiring expert knowledge. In this study, we\ndesign and evaluate a CNN-RNN architecture as well as a customized\ntraining-inference framework to deal with the mentioned challenges in\nlaparoscopic surgery action recognition. Using stacked recurrent layers, our\nproposed network takes advantage of inter-frame dependencies to negate the\nnegative effect of content distortion and variation in action recognition.\nFurthermore, our proposed frame sampling strategy effectively manages the\nduration variations in surgical actions to enable action recognition with high\ntemporal resolution. Our extensive experiments confirm the superiority of our\nproposed method in action recognition compared to static CNNs.",
            "author": [
                "Sahar Nasirihaghighi",
                "Negin Ghamsarian",
                "Daniela Stefanics",
                "Klaus Schoeffmann",
                "Heinrich Husslein"
            ],
            "link": [
                "http://dx.doi.org/10.1109/CBMS58004.2023.00187",
                "http://arxiv.org/abs/2311.18666v1",
                "http://arxiv.org/pdf/2311.18666v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18662v2",
            "title": "Solving the Team Orienteering Problem with Transformers",
            "updated": "2023-12-01T09:48:02Z",
            "published": "2023-11-30T16:10:35Z",
            "summary": "Route planning for a fleet of vehicles is an important task in applications\nsuch as package delivery, surveillance, or transportation. This problem is\nusually modeled as a Combinatorial Optimization problem named as Team\nOrienteering Problem. The most popular Team Orienteering Problem solvers are\nmainly based on either linear programming, which provides accurate solutions by\nemploying a large computation time that grows with the size of the problem, or\nheuristic methods, which usually find suboptimal solutions in a shorter amount\nof time. In this paper, a multi-agent route planning system capable of solving\nthe Team Orienteering Problem in a very fast and accurate manner is presented.\nThe proposed system is based on a centralized Transformer neural network that\ncan learn to encode the scenario (modeled as a graph) and the context of the\nagents to provide fast and accurate solutions. Several experiments have been\nperformed to demonstrate that the presented system can outperform most of the\nstate-of-the-art works in terms of computation speed. In addition, the code is\npublicly available at http://gti.ssr.upm.es/data.",
            "author": [
                "Daniel Fuertes",
                "Carlos R. del-Blanco",
                "Fernando Jaureguizar",
                "Narciso Garc\u00eda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18662v2",
                "http://arxiv.org/pdf/2311.18662v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18658v1",
            "title": "ArcMMLU: A Library and Information Science Benchmark for Large Language\n  Models",
            "updated": "2023-11-30T16:08:04Z",
            "published": "2023-11-30T16:08:04Z",
            "summary": "In light of the rapidly evolving capabilities of large language models\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\nbenchmarks to accurately assess their capabilities. In response to this need,\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\nArchival Science, Data Science, Library Science, and Information Science.\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\nquestions for the compilation of ArcMMLU. This extensive compilation can\nreflect the diverse nature of the LIS domain and offer a robust foundation for\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\nnotable performance gap, suggesting substantial headroom for refinement in LLM\ncapabilities within the LIS domain. Further analysis explores the effectiveness\nof few-shot examples on model performance and highlights challenging questions\nwhere models consistently underperform, providing valuable insights for\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\nthe Chinese LIS domain and paves the way for future development of LLMs\ntailored to this specialized area.",
            "author": [
                "Shitou Zhang",
                "Zuchao Li",
                "Xingshen Liu",
                "Liming Yang",
                "Ping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18658v1",
                "http://arxiv.org/pdf/2311.18658v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18649v1",
            "title": "Simple Semantic-Aided Few-Shot Learning",
            "updated": "2023-11-30T15:57:34Z",
            "published": "2023-11-30T15:57:34Z",
            "summary": "Learning from a limited amount of data, namely Few-Shot Learning, stands out\nas a challenging computer vision task. Several works exploit semantics and\ndesign complicated semantic fusion mechanisms to compensate for rare\nrepresentative features within restricted data. However, relying on naive\nsemantics such as class names introduces biases due to their brevity, while\nacquiring extensive semantics from external knowledge takes a huge time and\neffort. This limitation severely constrains the potential of semantics in\nfew-shot learning. In this paper, we design an automatic way called Semantic\nEvolution to generate high-quality semantics. The incorporation of high-quality\nsemantics alleviates the need for complex network structures and learning\nalgorithms used in previous works. Hence, we employ a simple two-layer network\ntermed Semantic Alignment Network to transform semantics and visual features\ninto robust class prototypes with rich discriminative features for few-shot\nclassification. The experimental results show our framework outperforms all\nprevious methods on five benchmarks, demonstrating a simple network with\nhigh-quality semantics can beat intricate multi-modal modules on few-shot\nclassification tasks.",
            "author": [
                "Hai Zhang",
                "Junzhe Xu",
                "Shanlin Jiang",
                "Zhenan He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18649v1",
                "http://arxiv.org/pdf/2311.18649v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18645v1",
            "title": "Stochastic Vision Transformers with Wasserstein Distance-Aware Attention",
            "updated": "2023-11-30T15:53:37Z",
            "published": "2023-11-30T15:53:37Z",
            "summary": "Self-supervised learning is one of the most promising approaches to acquiring\nknowledge from limited labeled data. Despite the substantial advancements made\nin recent years, self-supervised models have posed a challenge to\npractitioners, as they do not readily provide insight into the model's\nconfidence and uncertainty. Tackling this issue is no simple feat, primarily\ndue to the complexity involved in implementing techniques that can make use of\nthe latent representations learned during pre-training without relying on\nexplicit labels. Motivated by this, we introduce a new stochastic vision\ntransformer that integrates uncertainty and distance awareness into\nself-supervised learning (SSL) pipelines. Instead of the conventional\ndeterministic vector embedding, our novel stochastic vision transformer encodes\nimage patches into elliptical Gaussian distributional embeddings. Notably, the\nattention matrices of these stochastic representational embeddings are computed\nusing Wasserstein distance-based attention, effectively capitalizing on the\ndistributional nature of these embeddings. Additionally, we propose a\nregularization term based on Wasserstein distance for both pre-training and\nfine-tuning processes, thereby incorporating distance awareness into latent\nrepresentations. We perform extensive experiments across different tasks such\nas in-distribution generalization, out-of-distribution detection, dataset\ncorruption, semi-supervised settings, and transfer learning to other datasets\nand tasks. Our proposed method achieves superior accuracy and calibration,\nsurpassing the self-supervised baseline in a wide range of experiments on a\nvariety of datasets.",
            "author": [
                "Franciskus Xaverius Erick",
                "Mina Rezaei",
                "Johanna Paula M\u00fcller",
                "Bernhard Kainz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18645v1",
                "http://arxiv.org/pdf/2311.18645v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18641v1",
            "title": "CrimeGAT: Leveraging Graph Attention Networks for Enhanced Predictive\n  Policing in Criminal Networks",
            "updated": "2023-11-30T15:47:59Z",
            "published": "2023-11-30T15:47:59Z",
            "summary": "In this paper, we present CrimeGAT, a novel application of Graph Attention\nNetworks (GATs) for predictive policing in criminal networks. Criminal networks\npose unique challenges for predictive analytics due to their complex structure,\nmulti-relational links, and dynamic behavior. Traditional methods often fail to\ncapture these complexities, leading to suboptimal predictions. To address these\nchallenges, we propose the use of GATs, which can effectively leverage both\nnode features and graph structure to make predictions. Our proposed CrimeGAT\nmodel integrates attention mechanisms to weigh the importance of a node's\nneighbors, thereby capturing the local and global structures of criminal\nnetworks. We formulate the problem as learning a function that maps node\nfeatures and graph structure to a prediction of future criminal activity. The\nexperimental results on real-world datasets demonstrate that CrimeGAT\nout-performs conventional methods in predicting criminal activities, thereby\nproviding a powerful tool for law enforcement agencies to proactively deploy\nresources. Furthermore, the interpretable nature of the attentionmechanism\ninGATs offers insights into the key players and relationships in criminal\nnetworks. This research opens new avenues for applying deep learning techniques\nin the Aeld of predictive policing and criminal network analysis.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18641v1",
                "http://arxiv.org/pdf/2311.18641v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18612v1",
            "title": "Cancer-Net PCa-Gen: Synthesis of Realistic Prostate Diffusion Weighted\n  Imaging Data via Anatomic-Conditional Controlled Latent Diffusion",
            "updated": "2023-11-30T15:11:03Z",
            "published": "2023-11-30T15:11:03Z",
            "summary": "In Canada, prostate cancer is the most common form of cancer in men and\naccounted for 20% of new cancer cases for this demographic in 2022. Due to\nrecent successes in leveraging machine learning for clinical decision support,\nthere has been significant interest in the development of deep neural networks\nfor prostate cancer diagnosis, prognosis, and treatment planning using\ndiffusion weighted imaging (DWI) data. A major challenge hindering widespread\nadoption in clinical use is poor generalization of such networks due to\nscarcity of large-scale, diverse, balanced prostate imaging datasets for\ntraining such networks. In this study, we explore the efficacy of latent\ndiffusion for generating realistic prostate DWI data through the introduction\nof an anatomic-conditional controlled latent diffusion strategy. To the best of\nthe authors' knowledge, this is the first study to leverage conditioning for\nsynthesis of prostate cancer imaging. Experimental results show that the\nproposed strategy, which we call Cancer-Net PCa-Gen, enhances synthesis of\ndiverse prostate images through controllable tumour locations and better\nanatomical and textural fidelity. These crucial features make it well-suited\nfor augmenting real patient data, enabling neural networks to be trained on a\nmore diverse and comprehensive data distribution. The Cancer-Net PCa-Gen\nframework and sample images have been made publicly available at\nhttps://www.kaggle.com/datasets/deetsadi/cancer-net-pca-gen-dataset as a part\nof a global open-source initiative dedicated to accelerating advancement in\nmachine learning to aid clinicians in the fight against cancer.",
            "author": [
                "Aditya Sridhar",
                "Chi-en Amy Tai",
                "Hayden Gunraj",
                "Yuhao Chen",
                "Alexander Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18612v1",
                "http://arxiv.org/pdf/2311.18612v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18604v1",
            "title": "Barwise Music Structure Analysis with the Correlation Block-Matching\n  Segmentation Algorithm",
            "updated": "2023-11-30T15:00:25Z",
            "published": "2023-11-30T15:00:25Z",
            "summary": "Music Structure Analysis (MSA) is a Music Information Retrieval task\nconsisting of representing a song in a simplified, organized manner by breaking\nit down into sections typically corresponding to ``chorus'', ``verse'',\n``solo'', etc. In this work, we extend an MSA algorithm called the Correlation\nBlock-Matching (CBM) algorithm introduced by (Marmoret et al., 2020, 2022b).\nThe CBM algorithm is a dynamic programming algorithm that segments\nself-similarity matrices, which are a standard description used in MSA and in\nnumerous other applications. In this work, self-similarity matrices are\ncomputed from the feature representation of an audio signal and time is sampled\nat the bar-scale. This study examines three different standard similarity\nfunctions for the computation of self-similarity matrices. Results show that,\nin optimal conditions, the proposed algorithm achieves a level of performance\nwhich is competitive with supervised state-of-the-art methods while only\nrequiring knowledge of bar positions. In addition, the algorithm is made\nopen-source and is highly customizable.",
            "author": [
                "Axel Marmoret",
                "J\u00e9r\u00e9my E. Cohen",
                "Fr\u00e9d\u00e9ric Bimbot"
            ],
            "link": [
                "http://dx.doi.org/10.5334/tismir.167",
                "http://arxiv.org/abs/2311.18604v1",
                "http://arxiv.org/pdf/2311.18604v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.IR",
                "eess.AS",
                "H.5.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18588v1",
            "title": "Optimizing ZX-Diagrams with Deep Reinforcement Learning",
            "updated": "2023-11-30T14:29:18Z",
            "published": "2023-11-30T14:29:18Z",
            "summary": "ZX-diagrams are a powerful graphical language for the description of quantum\nprocesses with applications in fundamental quantum mechanics, quantum circuit\noptimization, tensor network simulation, and many more. The utility of\nZX-diagrams relies on a set of local transformation rules that can be applied\nto them without changing the underlying quantum process they describe. These\nrules can be exploited to optimize the structure of ZX-diagrams for a range of\napplications. However, finding an optimal sequence of transformation rules is\ngenerally an open problem. In this work, we bring together ZX-diagrams with\nreinforcement learning, a machine learning technique designed to discover an\noptimal sequence of actions in a decision-making problem and show that a\ntrained reinforcement learning agent can significantly outperform other\noptimization techniques like a greedy strategy or simulated annealing. The use\nof graph neural networks to encode the policy of the agent enables\ngeneralization to diagrams much bigger than seen during the training phase.",
            "author": [
                "Maximilian N\u00e4gele",
                "Florian Marquardt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18588v1",
                "http://arxiv.org/pdf/2311.18588v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18574v1",
            "title": "Multi-scale Iterative Refinement towards Robust and Versatile Molecular\n  Docking",
            "updated": "2023-11-30T14:09:20Z",
            "published": "2023-11-30T14:09:20Z",
            "summary": "Molecular docking is a key computational tool utilized to predict the binding\nconformations of small molecules to protein targets, which is fundamental in\nthe design of novel drugs. Despite recent advancements in geometric deep\nlearning-based approaches leading to improvements in blind docking efficiency,\nthese methods have encountered notable challenges, such as limited\ngeneralization performance on unseen proteins, the inability to concurrently\naddress the settings of blind docking and site-specific docking, and the\nfrequent occurrence of physical implausibilities such as inter-molecular steric\nclash. In this study, we introduce DeltaDock, a robust and versatile framework\ndesigned for efficient molecular docking to overcome these challenges.\nDeltaDock operates in a two-step process: rapid initial complex structures\nsampling followed by multi-scale iterative refinement of the initial\nstructures. In the initial stage, to sample accurate structures with high\nefficiency, we develop a ligand-dependent binding site prediction model founded\non large protein models and graph neural networks. This model is then paired\nwith GPU-accelerated sampling algorithms. The sampled structures are updated\nusing a multi-scale iterative refinement module that captures both\nprotein-ligand atom-atom interactions and residue-atom interactions in the\nfollowing stage. Distinct from previous geometric deep learning methods that\nare conditioned on the blind docking setting, DeltaDock demonstrates superior\nperformance in both blind docking and site-specific docking settings.\nComprehensive experimental results reveal that DeltaDock consistently surpasses\nbaseline methods in terms of docking accuracy. Furthermore, it displays\nremarkable generalization capabilities and proficiency for predicting\nphysically valid structures, thereby attesting to its robustness and\nreliability in various scenarios.",
            "author": [
                "Jiaxian Yan",
                "Zaixi Zhang",
                "Kai Zhang",
                "Qi Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18574v1",
                "http://arxiv.org/pdf/2311.18574v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18566v1",
            "title": "Unconditionally Secure Commitments with Quantum Auxiliary Inputs",
            "updated": "2023-11-30T13:57:30Z",
            "published": "2023-11-30T13:57:30Z",
            "summary": "We show the following unconditional results on quantum commitments in two\nrelated yet different models:\n  1. We revisit the notion of quantum auxiliary-input commitments introduced by\nChailloux, Kerenidis, and Rosgen (Comput. Complex. 2016) where both the\ncommitter and receiver take the same quantum state, which is determined by the\nsecurity parameter, as quantum auxiliary inputs. We show that\ncomputationally-hiding and statistically-binding quantum auxiliary-input\ncommitments exist unconditionally, i.e., without relying on any unproven\nassumption, while Chailloux et al. assumed a complexity-theoretic assumption,\n${\\bf QIP}\\not\\subseteq{\\bf QMA}$. On the other hand, we observe that achieving\nboth statistical hiding and statistical binding at the same time is impossible\neven in the quantum auxiliary-input setting. To the best of our knowledge, this\nis the first example of unconditionally proving computational security of any\nform of (classical or quantum) commitments for which statistical security is\nimpossible. As intermediate steps toward our construction, we introduce and\nunconditionally construct post-quantum sparse pseudorandom distributions and\nquantum auxiliary-input EFI pairs which may be of independent interest.\n  2. We introduce a new model which we call the common reference quantum state\n(CRQS) model where both the committer and receiver take the same quantum state\nthat is randomly sampled by an efficient setup algorithm. We unconditionally\nprove that there exist statistically hiding and statistically binding\ncommitments in the CRQS model, circumventing the impossibility in the plain\nmodel.\n  We also discuss their applications to zero-knowledge proofs, oblivious\ntransfers, and multi-party computations.",
            "author": [
                "Tomoyuki Morimae",
                "Barak Nehoran",
                "Takashi Yamakawa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18566v1",
                "http://arxiv.org/pdf/2311.18566v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18559v1",
            "title": "FediOS: Decoupling Orthogonal Subspaces for Personalization in\n  Feature-skew Federated Learning",
            "updated": "2023-11-30T13:50:38Z",
            "published": "2023-11-30T13:50:38Z",
            "summary": "Personalized federated learning (pFL) enables collaborative training among\nmultiple clients to enhance the capability of customized local models. In pFL,\nclients may have heterogeneous (also known as non-IID) data, which poses a key\nchallenge in how to decouple the data knowledge into generic knowledge for\nglobal sharing and personalized knowledge for preserving local personalization.\nA typical way of pFL focuses on label distribution skew, and they adopt a\ndecoupling scheme where the model is split into a common feature extractor and\ntwo prediction heads (generic and personalized). However, such a decoupling\nscheme cannot solve the essential problem of feature skew heterogeneity,\nbecause a common feature extractor cannot decouple the generic and personalized\nfeatures. Therefore, in this paper, we rethink the architecture decoupling\ndesign for feature-skew pFL and propose an effective pFL method called FediOS.\nIn FediOS, we reformulate the decoupling into two feature extractors (generic\nand personalized) and one shared prediction head. Orthogonal projections are\nused for clients to map the generic features into one common subspace and\nscatter the personalized features into different subspaces to achieve\ndecoupling for them. In addition, a shared prediction head is trained to\nbalance the importance of generic and personalized features during inference.\nExtensive experiments on four vision datasets demonstrate our method reaches\nstate-of-the-art pFL performances under feature skew heterogeneity.",
            "author": [
                "Lingzhi Gao",
                "Zexi Li",
                "Yang Lu",
                "Chao Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18559v1",
                "http://arxiv.org/pdf/2311.18559v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18558v1",
            "title": "Learning Radio Environments by Differentiable Ray Tracing",
            "updated": "2023-11-30T13:50:21Z",
            "published": "2023-11-30T13:50:21Z",
            "summary": "Ray tracing (RT) is instrumental in 6G research in order to generate\nspatially-consistent and environment-specific channel impulse responses (CIRs).\nWhile acquiring accurate scene geometries is now relatively straightforward,\ndetermining material characteristics requires precise calibration using channel\nmeasurements. We therefore introduce a novel gradient-based calibration method,\ncomplemented by differentiable parametrizations of material properties,\nscattering and antenna patterns. Our method seamlessly integrates with\ndifferentiable ray tracers that enable the computation of derivatives of CIRs\nwith respect to these parameters. Essentially, we approach field computation as\na large computational graph wherein parameters are trainable akin to weights of\na neural network (NN). We have validated our method using both synthetic data\nand real-world indoor channel measurements, employing a distributed\nmultiple-input multiple-output (MIMO) channel sounder.",
            "author": [
                "Jakob Hoydis",
                "Fay\u00e7al A\u00eft Aoudia",
                "Sebastian Cammerer",
                "Florian Euchner",
                "Merlin Nimier-David",
                "Stephan ten Brink",
                "Alexander Keller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18558v1",
                "http://arxiv.org/pdf/2311.18558v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18553v1",
            "title": "Heterogeneous Graph-based Trajectory Prediction using Local Map Context\n  and Social Interactions",
            "updated": "2023-11-30T13:46:05Z",
            "published": "2023-11-30T13:46:05Z",
            "summary": "Precisely predicting the future trajectories of surrounding traffic\nparticipants is a crucial but challenging problem in autonomous driving, due to\ncomplex interactions between traffic agents, map context and traffic rules.\nVector-based approaches have recently shown to achieve among the best\nperformances on trajectory prediction benchmarks. These methods model simple\ninteractions between traffic agents but don't distinguish between relation-type\nand attributes like their distance along the road. Furthermore, they represent\nlanes only by sequences of vectors representing center lines and ignore context\ninformation like lane dividers and other road elements. We present a novel\napproach for vector-based trajectory prediction that addresses these\nshortcomings by leveraging three crucial sources of information: First, we\nmodel interactions between traffic agents by a semantic scene graph, that\naccounts for the nature and important features of their relation. Second, we\nextract agent-centric image-based map features to model the local map context.\nFinally, we generate anchor paths to enforce the policy in multi-modal\nprediction to permitted trajectories only. Each of these three enhancements\nshows advantages over the baseline model HoliGraph.",
            "author": [
                "Daniel Grimm",
                "Maximilian Zipfl",
                "Felix Hertlein",
                "Alexander Naumann",
                "J\u00fcrgen L\u00fcttin",
                "Steffen Thoma",
                "Stefan Schmid",
                "Lavdim Halilaj",
                "Achim Rettinger",
                "J. Marius Z\u00f6llner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18553v1",
                "http://arxiv.org/pdf/2311.18553v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00096v1",
            "title": "OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for\n  General Video Recognition",
            "updated": "2023-11-30T13:32:43Z",
            "published": "2023-11-30T13:32:43Z",
            "summary": "Due to the resource-intensive nature of training vision-language models on\nexpansive video data, a majority of studies have centered on adapting\npre-trained image-language models to the video domain. Dominant pipelines\npropose to tackle the visual discrepancies with additional temporal learners\nwhile overlooking the substantial discrepancy for web-scaled descriptive\nnarratives and concise action category names, leading to less distinct semantic\nspace and potential performance limitations. In this work, we prioritize the\nrefinement of text knowledge to facilitate generalizable video recognition. To\naddress the limitations of the less distinct semantic space of category names,\nwe prompt a large language model (LLM) to augment action class names into\nSpatio-Temporal Descriptors thus bridging the textual discrepancy and serving\nas a knowledge base for general recognition. Moreover, to assign the best\ndescriptors with different video instances, we propose Optimal Descriptor\nSolver, forming the video recognition problem as solving the optimal matching\nflow across frame-level representations and descriptors. Comprehensive\nevaluations in zero-shot, few-shot, and fully supervised video recognition\nhighlight the effectiveness of our approach. Our best model achieves a\nstate-of-the-art zero-shot accuracy of 75.1% on Kinetics-600.",
            "author": [
                "Tongjia Chen",
                "Hongshan Yu",
                "Zhengeng Yang",
                "Zechuan Li",
                "Wei Sun",
                "Chen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00096v1",
                "http://arxiv.org/pdf/2312.00096v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18543v1",
            "title": "CrimeGraphNet: Link Prediction in Criminal Networks with Graph\n  Convolutional Networks",
            "updated": "2023-11-30T13:25:46Z",
            "published": "2023-11-30T13:25:46Z",
            "summary": "In this paper, we introduce CrimeGraphNet, a novel approach for link\nprediction in criminal networks utilizingGraph Convolutional Networks (GCNs).\nCriminal networks are intricate and dynamic, with covert links that are\nchallenging to uncover. Accurate prediction of these links can aid in proactive\ncrime prevention and investigation. Existing methods often fail to capture the\ncomplex interconnections in such networks. They also struggle in scenarios\nwhere only limited labeled data is available for training. To address these\nchallenges, we propose CrimeGraphNet, which leverages the power of GCNs for\nlink prediction in these networks. The GCNmodel effectively captures\ntopological features and node characteristics, making it well-suited for this\ntask. We evaluate CrimeGraphNet on several real-world criminal network\ndatasets. Our results demonstrate that CrimeGraphNet outperforms existing\nmethods in terms of prediction accuracy, robustness, and computational\nefAciency. Furthermore, our approach enables the extraction of meaningful\ninsights from the predicted links, thereby contributing to a better\nunderstanding of the underlying criminal activities. Overall, CrimeGraphNet\nrepresents a signiAcant step forward in the use of deep learning for criminal\nnetwork analysis.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18543v1",
                "http://arxiv.org/pdf/2311.18543v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18539v2",
            "title": "Bridging Both Worlds in Semantics and Time: Domain Knowledge Based\n  Analysis and Correlation of Industrial Process Attacks",
            "updated": "2023-12-03T18:02:14Z",
            "published": "2023-11-30T13:21:13Z",
            "summary": "Modern industrial control systems (ICS) attacks infect supervisory control\nand data acquisition (SCADA) hosts to stealthily alter industrial processes,\ncausing damage. To detect attacks with low false alarms, recent work detects\nattacks in both SCADA and process data. Unfortunately, this led to the same\nproblem - disjointed (false) alerts, due to the semantic and time gap in SCADA\nand process behavior, i.e., SCADA execution does not map to process dynamics\nnor evolve at similar time scales. We propose BRIDGE to analyze and correlate\nSCADA and industrial process attacks using domain knowledge to bridge their\nunique semantic and time evolution. This enables operators to tie malicious\nSCADA operations to their adverse process effects, which reduces false alarms\nand improves attack understanding. BRIDGE (i) identifies process constraints\nviolations in SCADA by measuring actuation dependencies in SCADA\nprocess-control, and (ii) detects malicious SCADA effects in processes via a\nphysics-informed neural network that embeds generic knowledge of inertial\nprocess dynamics. BRIDGE then dynamically aligns both analysis (i and ii) in a\ntime-window that adjusts their time evolution based on process inertial delays.\nWe applied BRIDGE to 11 diverse real-world industrial processes, and adaptive\nattacks inspired by past events. BRIDGE correlated 98.3% of attacks with 0.8%\nfalse positives (FP), compared to 78.3% detection accuracy and 13.7% FP of\nrecent work.",
            "author": [
                "Moses Ike",
                "Kandy Phan",
                "Anwesh Badapanda",
                "Matthew Landen",
                "Keaton Sadoski",
                "Wanda Guo",
                "Asfahan Shah",
                "Saman Zonouz",
                "Wenke Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18539v2",
                "http://arxiv.org/pdf/2311.18539v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00095v1",
            "title": "Textual-Knowledge-Guided Numerical Feature Discovery Method for Power\n  Demand Forecasting",
            "updated": "2023-11-30T13:17:22Z",
            "published": "2023-11-30T13:17:22Z",
            "summary": "Power demand forecasting is a crucial and challenging task for new power\nsystem and integrated energy system. However, as public feature databases and\nthe theoretical mechanism of power demand changes are unavailable, the known\nfeatures of power demand fluctuation are much limited. Recently, multimodal\nlearning approaches have shown great vitality in machine learning and AIGC. In\nthis paper, we interact two modal data and propose a textual-knowledge-guided\nnumerical feature discovery (TKNFD) method for short-term power demand\nforecasting. TKNFD extensively accumulates qualitative textual knowledge,\nexpands it into a candidate feature-type set, collects numerical data of these\nfeatures, and eventually builds four-dimensional multivariate source-tracking\ndatabases (4DM-STDs). Next, TKNFD presents a two-level quantitative feature\nidentification strategy independent of forecasting models, finds 43-48\nfeatures, and systematically analyses feature contribution and dependency\ncorrelation. Benchmark experiments in two different regions around the world\ndemonstrate that the forecasting accuracy of TKNFD-discovered features reliably\noutperforms that of SoTA feature schemes by 16.84% to 36.36% MAPE. In\nparticular, TKNFD reveals many unknown features, especially several dominant\nfeatures in the unknown energy and astronomical dimensions, which extend the\nknowledge on the origin of strong randomness and non-linearity in power demand\nfluctuation. Besides, 4DM-STDs can serve as public baseline databases.",
            "author": [
                "Zifan Ning",
                "Min Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00095v1",
                "http://arxiv.org/pdf/2312.00095v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18533v1",
            "title": "A knowledge-driven framework for synthesizing designs from modular\n  components",
            "updated": "2023-11-30T13:16:47Z",
            "published": "2023-11-30T13:16:47Z",
            "summary": "Creating a design from modular components necessitates three steps: Acquiring\nknowledge about available components, conceiving an abstract design concept,\nand implementing that concept in a concrete design. The third step entails many\nrepetitive and menial tasks, such as inserting parts and creating joints\nbetween them. Especially when comparing and implementing design alternatives,\nthis issue is compounded. We propose a use-case agnostic knowledge-driven\nframework to automate the implementation step. In particular, the framework\ncatalogues the acquired knowledge and the design concept, as well as utilizes\nCombinatory Logic Synthesis to synthesize concrete design alternatives. This\nminimizes the effort required to create designs, allowing the design space to\nbe thoroughly explored. We implemented the framework as a plugin for the CAD\nsoftware Autodesk Fusion 360. We conducted a case study in which robotic arms\nwere synthesized from a set of 28 modular components. Based on the case study,\nthe applicability of the framework is analyzed and discussed.",
            "author": [
                "Constantin Chaumet",
                "Jakob Rehof",
                "Thomas Schuster"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18533v1",
                "http://arxiv.org/pdf/2311.18533v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SE",
                "J.6; F.4.1; D.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18529v1",
            "title": "Applying an Evolutionary Algorithm to Minimize Teleportation Costs in\n  Distributed Quantum Computing",
            "updated": "2023-11-30T13:10:28Z",
            "published": "2023-11-30T13:10:28Z",
            "summary": "By connecting multiple quantum computers (QCs) through classical and quantum\nchannels, a quantum communication network can be formed. This gives rise to new\napplications such as blind quantum computing, distributed quantum computing and\nquantum key distribution. In distributed quantum computing, QCs collectively\nperform a quantum computation. As each device only executes a sub-circuit with\nfewer qubits than required by the complete circuit, a number of small QCs can\nbe used in combination to execute a large quantum circuit that a single QC\ncould not solve on its own. However, communication between QCs may still occur.\nDepending on the connectivity of the circuit, qubits must be teleported to\ndifferent QCs in the network, adding overhead to the actual computation; thus,\nit is crucial to minimize the number of teleportations. In this paper, we\npropose an evolutionary algorithm for this problem. More specifically, the\nalgorithm assigns qubits to QCs in the network for each time step of the\ncircuit such that the overall teleportation cost is minimized. Moreover,\nnetwork-specific constraints such as the capacity of each QC in the network can\nbe taken into account. We run experiments on random as well as benchmarking\ncircuits and give an outline on how this method can be adjusted to be\nincorporated into more realistic network settings as well as in compilers for\ndistributed quantum computing. Our results show that an evolutionary algorithm\nis well suited for this problem when compared to the graph partitioning\napproach as it delivers better results while simultaneously allows the easy\nintegration and consideration of various problem-specific constraints.",
            "author": [
                "Leo S\u00fcnkel",
                "Manik Dawar",
                "Thomas Gabor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18529v1",
                "http://arxiv.org/pdf/2311.18529v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18527v1",
            "title": "InfoFlowNet: A Multi-head Attention-based Self-supervised Learning Model\n  with Surrogate Approach for Uncovering Brain Effective Connectivity",
            "updated": "2023-11-30T13:06:04Z",
            "published": "2023-11-30T13:06:04Z",
            "summary": "Deciphering brain network topology can enhance the depth of neuroscientific\nknowledge and facilitate the development of neural engineering methods.\nEffective connectivity, a measure of brain network dynamics, is particularly\nuseful for investigating the directional influences among different brain\nregions. In this study, we introduce a novel brain causal inference model named\nInfoFlowNet, which leverages the self-attention mechanism to capture\nassociations among electroencephalogram (EEG) time series. The proposed method\nestimates the magnitude of directional information flow (dIF) among EEG\nprocesses by measuring the loss of model inference resulting from the shuffling\nof the time order of the original time series. To evaluate the feasibility of\nInfoFlowNet, we conducted experiments using a synthetic time series and two EEG\ndatasets. The results demonstrate that InfoFlowNet can extract time-varying\ncausal relationships among processes, reflected in the fluctuation of dIF\nvalues. Compared with the Granger causality model and temporal causal discovery\nframework, InfoFlowNet can identify more significant causal edges underlying\nEEG processes while maintaining an acceptable computation time. Our work\ndemonstrates the potential of InfoFlowNet for analyzing effective connectivity\nin EEG data. The findings highlight the importance of effective connectivity in\nunderstanding the complex dynamics of the brain network.",
            "author": [
                "Chun-Hsiang Chuang",
                "Shao-Xun Fang",
                "Chih-Sheng Huang",
                "Weiping Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18527v1",
                "http://arxiv.org/pdf/2311.18527v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18526v1",
            "title": "HOT: Higher-Order Dynamic Graph Representation Learning with Efficient\n  Transformers",
            "updated": "2023-11-30T13:05:39Z",
            "published": "2023-11-30T13:05:39Z",
            "summary": "Many graph representation learning (GRL) problems are dynamic, with millions\nof edges added or removed per second. A fundamental workload in this setting is\ndynamic link prediction: using a history of graph updates to predict whether a\ngiven pair of vertices will become connected. Recent schemes for link\nprediction in such dynamic settings employ Transformers, modeling individual\ngraph updates as single tokens. In this work, we propose HOT: a model that\nenhances this line of works by harnessing higher-order (HO) graph structures;\nspecifically, k-hop neighbors and more general subgraphs containing a given\npair of vertices. Harnessing such HO structures by encoding them into the\nattention matrix of the underlying Transformer results in higher accuracy of\nlink prediction outcomes, but at the expense of increased memory pressure. To\nalleviate this, we resort to a recent class of schemes that impose hierarchy on\nthe attention matrix, significantly reducing memory footprint. The final design\noffers a sweetspot between high accuracy and low memory utilization. HOT\noutperforms other dynamic GRL schemes, for example achieving 9%, 7%, and 15%\nhigher accuracy than - respectively - DyGFormer, TGN, and GraphMixer, for the\nMOOC dataset. Our design can be seamlessly extended towards other dynamic GRL\nworkloads.",
            "author": [
                "Maciej Besta",
                "Afonso Claudino Catarino",
                "Lukas Gianinazzi",
                "Nils Blach",
                "Piotr Nyczyk",
                "Hubert Niewiadomski",
                "Torsten Hoefler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18526v1",
                "http://arxiv.org/pdf/2311.18526v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18525v1",
            "title": "Detecting Anomalous Network Communication Patterns Using Graph\n  Convolutional Networks",
            "updated": "2023-11-30T13:03:49Z",
            "published": "2023-11-30T13:03:49Z",
            "summary": "To protect an organizations' endpoints from sophisticated cyberattacks,\nadvanced detection methods are required. In this research, we present\nGCNetOmaly: a graph convolutional network (GCN)-based variational autoencoder\n(VAE) anomaly detector trained on data that include connection events among\ninternal and external machines. As input, the proposed GCN-based VAE model\nreceives two matrices: (i) the normalized adjacency matrix, which represents\nthe connections among the machines, and (ii) the feature matrix, which includes\nvarious features (demographic, statistical, process-related, and Node2vec\nstructural features) that are used to profile the individual nodes/machines.\nAfter training the model on data collected for a predefined time window, the\nmodel is applied on the same data; the reconstruction score obtained by the\nmodel for a given machine then serves as the machine's anomaly score.\nGCNetOmaly was evaluated on real, large-scale data logged by Carbon Black EDR\nfrom a large financial organization's automated teller machines (ATMs) as well\nas communication with Active Directory (AD) servers in two setups: unsupervised\nand supervised. The results of our evaluation demonstrate GCNetOmaly's\neffectiveness in detecting anomalous behavior of machines on unsupervised data.",
            "author": [
                "Yizhak Vaisman",
                "Gilad Katz",
                "Yuval Elovici",
                "Asaf Shabtai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18525v1",
                "http://arxiv.org/pdf/2311.18525v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18518v1",
            "title": "Color-Emotion Associations in Art: Fuzzy Approach",
            "updated": "2023-11-30T12:49:11Z",
            "published": "2023-11-30T12:49:11Z",
            "summary": "Art objects can evoke certain emotions. Color is a fundamental element of\nvisual art and plays a significant role in how art is perceived. This paper\nintroduces a novel approach to classifying emotions in art using Fuzzy Sets. We\nemploy a fuzzy approach because it aligns well with human judgments' imprecise\nand subjective nature. Extensive fuzzy colors (n=120) and a broad emotional\nspectrum (n=10) allow for a more human-consistent and context-aware exploration\nof emotions inherent in paintings. First, we introduce the fuzzy color\nrepresentation model. Then, at the fuzzification stage, we process the Wiki Art\nDataset of paintings tagged with emotions, extracting fuzzy dominant colors\nlinked to specific emotions. This results in fuzzy color distributions for ten\nemotions. Finally, we convert them back to a crisp domain, obtaining a\nknowledge base of color-emotion associations in primary colors. Our findings\nreveal strong associations between specific emotions and colors; for instance,\ngratitude strongly correlates with green, brown, and orange. Other noteworthy\nassociations include brown and anger, orange with shame, yellow with happiness,\nand gray with fear. Using these associations and Jaccard similarity, we can\nfind the emotions in the arbitrary untagged image. We conducted a 2AFC\nexperiment involving human subjects to evaluate the proposed method. The\naverage hit rate of 0.77 indicates a significant correlation between the\nmethod's predictions and human perception. The proposed method is simple to\nadapt to art painting retrieval systems. The study contributes to the\ntheoretical understanding of color-emotion associations in art, offering\nvaluable insights for various practical applications besides art, like\nmarketing, design, and psychology.",
            "author": [
                "Muragul Muratbekova",
                "Pakizar Shamoi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18518v1",
                "http://arxiv.org/pdf/2311.18518v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18498v1",
            "title": "Data-Agnostic Model Poisoning against Federated Learning: A Graph\n  Autoencoder Approach",
            "updated": "2023-11-30T12:19:10Z",
            "published": "2023-11-30T12:19:10Z",
            "summary": "This paper proposes a novel, data-agnostic, model poisoning attack on\nFederated Learning (FL), by designing a new adversarial graph autoencoder\n(GAE)-based framework. The attack requires no knowledge of FL training data and\nachieves both effectiveness and undetectability. By listening to the benign\nlocal models and the global model, the attacker extracts the graph structural\ncorrelations among the benign local models and the training data features\nsubstantiating the models. The attacker then adversarially regenerates the\ngraph structural correlations while maximizing the FL training loss, and\nsubsequently generates malicious local models using the adversarial graph\nstructure and the training data features of the benign ones. A new algorithm is\ndesigned to iteratively train the malicious local models using GAE and\nsub-gradient descent. The convergence of FL under attack is rigorously proved,\nwith a considerably large optimality gap. Experiments show that the FL accuracy\ndrops gradually under the proposed attack and existing defense mechanisms fail\nto detect it. The attack can give rise to an infection across all benign\ndevices, making it a serious threat to FL.",
            "author": [
                "Kai Li",
                "Jingjing Zheng",
                "Xin Yuan",
                "Wei Ni",
                "Ozgur B. Akan",
                "H. Vincent Poor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18498v1",
                "http://arxiv.org/pdf/2311.18498v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18496v1",
            "title": "Accurate Segmentation of Optic Disc And Cup from Multiple Pseudo-labels\n  by Noise-Aware Learning",
            "updated": "2023-11-30T12:17:16Z",
            "published": "2023-11-30T12:17:16Z",
            "summary": "Optic disc and cup segmentation play a crucial role in automating the\nscreening and diagnosis of optic glaucoma. While data-driven convolutional\nneural networks (CNNs) show promise in this area, the inherent ambiguity of\nsegmenting object and background boundaries in the task of optic disc and cup\nsegmentation leads to noisy annotations that impact model performance. To\naddress this, we propose an innovative label-denoising method of Multiple\nPseudo-labels Noise-aware Network (MPNN) for accurate optic disc and cup\nsegmentation. Specifically, the Multiple Pseudo-labels Generation and Guided\nDenoising (MPGGD) module generates pseudo-labels by multiple different\ninitialization networks trained on true labels, and the pixel-level consensus\ninformation extracted from these pseudo-labels guides to differentiate clean\npixels from noisy pixels. The training framework of the MPNN is constructed by\na teacher-student architecture to learn segmentation from clean pixels and\nnoisy pixels. Particularly, such a framework adeptly leverages (i) reliable and\nfundamental insights from clean pixels and (ii) the supplementary knowledge\nwithin noisy pixels via multiple perturbation-based unsupervised consistency.\nCompared to other label-denoising methods, comprehensive experimental results\non the RIGA dataset demonstrate our method's excellent performance and\nsignificant denoising ability.",
            "author": [
                "Tengjin Weng",
                "Yang Shen",
                "Zhidong Zhao",
                "Zhiming Cheng",
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18496v1",
                "http://arxiv.org/pdf/2311.18496v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18486v1",
            "title": "New Perspectives on the Evaluation of Link Prediction Algorithms for\n  Dynamic Graphs",
            "updated": "2023-11-30T11:57:07Z",
            "published": "2023-11-30T11:57:07Z",
            "summary": "There is a fast-growing body of research on predicting future links in\ndynamic networks, with many new algorithms. Some benchmark data exists, and\nperformance evaluations commonly rely on comparing the scores of observed\nnetwork events (positives) with those of randomly generated ones (negatives).\nThese evaluation measures depend on both the predictive ability of the model\nand, crucially, the type of negative samples used. Besides, as generally the\ncase with temporal data, prediction quality may vary over time. This creates a\ncomplex evaluation space. In this work, we catalog the possibilities for\nnegative sampling and introduce novel visualization methods that can yield\ninsight into prediction performance and the dynamics of temporal networks. We\nleverage these visualization tools to investigate the effect of negative\nsampling on the predictive performance, at the node and edge level. We validate\nempirically, on datasets extracted from recent benchmarks that the error is\ntypically not evenly distributed across different data segments. Finally, we\nargue that such visualization tools can serve as powerful guides to evaluate\ndynamic link prediction methods at different levels.",
            "author": [
                "Rapha\u00ebl Romero",
                "Tijl De Bie",
                "Jefrey Lijffijt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18486v1",
                "http://arxiv.org/pdf/2311.18486v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18473v1",
            "title": "DGMem: Learning Visual Navigation Policy without Any Labels by Dynamic\n  Graph Memory",
            "updated": "2023-11-30T11:29:58Z",
            "published": "2023-11-30T11:29:58Z",
            "summary": "In recent years, learning-based approaches have demonstrated significant\npromise in addressing intricate navigation tasks. Traditional methods for\ntraining deep neural network navigation policies rely on meticulously designed\nreward functions or extensive teleoperation datasets as navigation\ndemonstrations. However, the former is often confined to simulated\nenvironments, and the latter demands substantial human labor, making it a\ntime-consuming process. Our vision is for robots to autonomously learn\nnavigation skills and adapt their behaviors to environmental changes without\nany human intervention. In this work, we discuss the self-supervised navigation\nproblem and present Dynamic Graph Memory (DGMem), which facilitates training\nonly with on-board observations. With the help of DGMem, agents can actively\nexplore their surroundings, autonomously acquiring a comprehensive navigation\npolicy in a data-efficient manner without external feedback. Our method is\nevaluated in photorealistic 3D indoor scenes, and empirical studies demonstrate\nthe effectiveness of DGMem.",
            "author": [
                "Wenzhe Cai",
                "Teng Wang",
                "Guangran Cheng",
                "Lele Xu",
                "Changyin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18473v1",
                "http://arxiv.org/pdf/2311.18473v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18460v1",
            "title": "Causal Fairness under Unobserved Confounding: A Neural Sensitivity\n  Framework",
            "updated": "2023-11-30T11:11:26Z",
            "published": "2023-11-30T11:11:26Z",
            "summary": "Fairness for machine learning predictions is widely required in practice for\nlegal, ethical, and societal reasons. Existing work typically focuses on\nsettings without unobserved confounding, even though unobserved confounding can\nlead to severe violations of causal fairness and, thus, unfair predictions. In\nthis work, we analyze the sensitivity of causal fairness to unobserved\nconfounding. Our contributions are three-fold. First, we derive bounds for\ncausal fairness metrics under different sources of unobserved confounding. This\nenables practitioners to examine the sensitivity of their machine learning\nmodels to unobserved confounding in fairness-critical applications. Second, we\npropose a novel neural framework for learning fair predictions, which allows us\nto offer worst-case guarantees of the extent to which causal fairness can be\nviolated due to unobserved confounding. Third, we demonstrate the effectiveness\nof our framework in a series of experiments, including a real-world case study\nabout predicting prison sentences. To the best of our knowledge, ours is the\nfirst work to study causal fairness under unobserved confounding. To this end,\nour work is of direct practical value as a refutation strategy to ensure the\nfairness of predictions in high-stakes applications.",
            "author": [
                "Maresa Schr\u00f6der",
                "Dennis Frauen",
                "Stefan Feuerriegel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18460v1",
                "http://arxiv.org/pdf/2311.18460v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18452v1",
            "title": "Developer Experiences with a Contextualized AI Coding Assistant:\n  Usability, Expectations, and Outcomes",
            "updated": "2023-11-30T10:52:28Z",
            "published": "2023-11-30T10:52:28Z",
            "summary": "In the rapidly advancing field of artificial intelligence, software\ndevelopment has emerged as a key area of innovation. Despite the plethora of\ngeneral-purpose AI assistants available, their effectiveness diminishes in\ncomplex, domain-specific scenarios. Noting this limitation, both the academic\ncommunity and industry players are relying on contextualized coding AI\nassistants. These assistants surpass general-purpose AI tools by integrating\nproprietary, domain-specific knowledge, offering precise and relevant\nsolutions. Our study focuses on the initial experiences of 62 participants who\nused a contextualized coding AI assistant -- named StackSpot AI -- in a\ncontrolled setting. According to the participants, the assistants' use resulted\nin significant time savings, easier access to documentation, and the generation\nof accurate codes for internal APIs. However, challenges associated with the\nknowledge sources necessary to make the coding assistant access more contextual\ninformation as well as variable responses and limitations in handling complex\ncodes were observed. The study's findings, detailing both the benefits and\nchallenges of contextualized AI assistants, underscore their potential to\nrevolutionize software development practices, while also highlighting areas for\nfurther refinement.",
            "author": [
                "Gustavo Pinto",
                "Cleidson de Souza",
                "Thayssa Rocha",
                "Igor Steinmacher",
                "Alberto de Souza",
                "Edward Monteiro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18452v1",
                "http://arxiv.org/pdf/2311.18452v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18451v1",
            "title": "How Much Is Hidden in the NAS Benchmarks? Few-Shot Adaptation of a NAS\n  Predictor",
            "updated": "2023-11-30T10:51:46Z",
            "published": "2023-11-30T10:51:46Z",
            "summary": "Neural architecture search has proven to be a powerful approach to designing\nand refining neural networks, often boosting their performance and efficiency\nover manually-designed variations, but comes with computational overhead. While\nthere has been a considerable amount of research focused on lowering the cost\nof NAS for mainstream tasks, such as image classification, a lot of those\nimprovements stem from the fact that those tasks are well-studied in the\nbroader context. Consequently, applicability of NAS to emerging and\nunder-represented domains is still associated with a relatively high cost\nand/or uncertainty about the achievable gains. To address this issue, we turn\nour focus towards the recent growth of publicly available NAS benchmarks in an\nattempt to extract general NAS knowledge, transferable across different tasks\nand search spaces. We borrow from the rich field of meta-learning for few-shot\nadaptation and carefully study applicability of those methods to NAS, with a\nspecial focus on the relationship between task-level correlation (domain shift)\nand predictor transferability; which we deem critical for improving NAS on\ndiverse tasks. In our experiments, we use 6 NAS benchmarks in conjunction,\nspanning in total 16 NAS settings -- our meta-learning approach not only shows\nsuperior (or matching) performance in the cross-validation experiments but also\nsuccessful extrapolation to a new search space and tasks.",
            "author": [
                "Hrushikesh Loya",
                "\u0141ukasz Dudziak",
                "Abhinav Mehrotra",
                "Royson Lee",
                "Javier Fernandez-Marques",
                "Nicholas D. Lane",
                "Hongkai Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18451v1",
                "http://arxiv.org/pdf/2311.18451v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18424v1",
            "title": "Multiple Disciplinary Data Work Practices in Artificial Intelligence\n  Research: a Healthcare Case Study in the UK",
            "updated": "2023-11-30T10:19:33Z",
            "published": "2023-11-30T10:19:33Z",
            "summary": "Developing artificial intelligence (AI) tools for healthcare is a multiple\ndisciplinary effort, bringing data scientists, clinicians, patients and other\ndisciplines together. In this paper, we explore the AI development workflow and\nhow participants navigate the challenges and tensions of sharing and generating\nknowledge across disciplines. Through an inductive thematic analysis of 13\nsemi-structured interviews with participants in a large research consortia, our\nfindings suggest that multiple disciplinarity heavily impacts work practices.\nParticipants faced challenges to learn the languages of other disciplines and\nneeded to adapt the tools used for sharing and communicating with their\naudience, particularly those from a clinical or patient perspective. Large\nhealth datasets also posed certain restrictions on work practices. We\nidentified meetings as a key platform for facilitating exchanges between\ndisciplines and allowing for the blending and creation of knowledge. Finally,\nwe discuss design implications for data science and collaborative tools, and\nrecommendations for future research.",
            "author": [
                "Rafael Henkin",
                "Elizabeth Remfry",
                "Duncan J. Reynolds",
                "Megan Clinch",
                "Michael R. Barnes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18424v1",
                "http://arxiv.org/pdf/2311.18424v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18408v1",
            "title": "Formal conjugacy growth in graph products II",
            "updated": "2023-11-30T09:58:26Z",
            "published": "2023-11-30T09:58:26Z",
            "summary": "In this paper we give an algorithm for computing the conjugacy growth series\nfor a right-angled Artin group, based on a natural language of minimal length\nconjugacy representatives. In addition, we provide a further language of unique\nconjugacy geodesic representatives of the conjugacy classes for a graph product\nof groups. The conjugacy representatives and growth series here provide an\nalternate viewpoint, and are more amenable to computational experiments\ncompared to those in our previous paper.\n  Examples of applications of this algorithm for right-angled Artin groups are\nprovided, as well as computations of conjugacy geodesic growth growth series\nwith respect to the standard generating sets.",
            "author": [
                "Laura Ciobanu",
                "Susan Hermiller",
                "Valentin Mercier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18408v1",
                "http://arxiv.org/pdf/2311.18408v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20F69, 20F65, 68Q45"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18397v1",
            "title": "IAG: Induction-Augmented Generation Framework for Answering Reasoning\n  Questions",
            "updated": "2023-11-30T09:48:51Z",
            "published": "2023-11-30T09:48:51Z",
            "summary": "Retrieval-Augmented Generation (RAG), by incorporating external knowledge\nwith parametric memory of language models, has become the state-of-the-art\narchitecture for open-domain QA tasks. However, common knowledge bases are\ninherently constrained by limited coverage and noisy information, making\nretrieval-based approaches inadequate to answer implicit reasoning questions.\nIn this paper, we propose an Induction-Augmented Generation (IAG) framework\nthat utilizes inductive knowledge along with the retrieved documents for\nimplicit reasoning. We leverage large language models (LLMs) for deriving such\nknowledge via a novel prompting method based on inductive reasoning patterns.\nOn top of this, we implement two versions of IAG named IAG-GPT and IAG-Student,\nrespectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for\nanswer prediction, while IAG-Student gets rid of dependencies on GPT service at\ninference time by incorporating a student inductor model. The inductor is\nfirstly trained via knowledge distillation and further optimized by\nback-propagating the generator feedback via differentiable beam scores.\nExperimental results show that IAG outperforms RAG baselines as well as ChatGPT\non two Open-Domain QA tasks. Notably, our best models have won the first place\nin the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA\n(since Jan 8, 2023).",
            "author": [
                "Zhebin Zhang",
                "Xinyu Zhang",
                "Yuanhang Ren",
                "Saijiang Shi",
                "Meng Han",
                "Yongkang Wu",
                "Ruofei Lai",
                "Zhao Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18397v1",
                "http://arxiv.org/pdf/2311.18397v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18392v1",
            "title": "Augmented Reality Technology in Teaching about Physics: A systematic\n  review of opportunities and challenges",
            "updated": "2023-11-30T09:35:36Z",
            "published": "2023-11-30T09:35:36Z",
            "summary": "The use of augmented reality (AR) allows for the integration of digital\ninformation onto our perception of the physical world. In this article, we\npresent a comprehensive review of previously published literature on the\nimplementation of augmented reality in physics education, at the school and the\nuniversity level. Our review includes an analysis of 96 papers from the Scopus\nand Eric databases, all of which were published between January 1st, 2012 and\nJanuary 1st, 2023. We evaluated how AR has been used for facilitating learning\nabout physics. Potential AR-based learning activities for different physics\ntopics have been summarized and opportunities, as well as challenges associated\nwith AR-based learning of physics have been reported. It has been shown that AR\ntechnologies may facilitate physics learning by: providing complementary\nvisualizations, optimizing cognitive load, allowing for haptic learning,\nreducing task completion time and promoting collaborative inquiry. The\npotential disadvantages of using AR in physics teaching are mainly related to\nthe shortcomings of software and hardware technologies (e.g., camera freeze,\nvisualization delay) and extraneous cognitive load (e.g., paying more attention\nto secondary details than to constructing target knowledge).",
            "author": [
                "A. Vidak",
                "I. Movre \u0160api\u0107",
                "V. Me\u0161i\u0107",
                "V. Gomzi"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1361-6404/ad0e84",
                "http://arxiv.org/abs/2311.18392v1",
                "http://arxiv.org/pdf/2311.18392v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18379v1",
            "title": "Room temperature polariton condensation from Whispering gallery modes in\n  CsPbBr3 microplatelets",
            "updated": "2023-11-30T09:21:20Z",
            "published": "2023-11-30T09:21:20Z",
            "summary": "Room temperature (RT) polariton condensate holds exceptional promise for\nrevolutionizing various fields of science and technology, encompassing\noptoelectronics devices to quantum information processing. Using perovskite\nmaterials like all-inorganic CsPbBr3 single crystal provides additional\nadvantages, such as ease of synthesis, cost-effectiveness, and compatibility\nwith existing semiconductor technologies. In this work, we show the formation\nof whispering gallery modes (WGM) in CsPbBr3 single crystals with controlled\ngeometry, synthesized using a lowcost and efficient capillary bridge method.\nThrough the implementation of microplatelets geometry, we achieve enhanced\noptical properties and performance thanks to the presence of sharp edges and a\nuniform surface, effectively avoiding non-radiative scattering losses caused by\ndefects. This allows us not only to observe strong light matter coupling and\nformation of whispering gallery polaritons, but also to demonstrate the onset\nof polariton condensation at RT. This investigation not only contributes to the\nadvancement of our knowledge concerning the exceptional optical properties of\nperovskite-based polariton systems, but also unveils prospects for the\nexploration of WGM polariton condensation within the framework of a 3D\nperovskite-based platform, working at RT. The unique characteristics of\npolariton condensate, including low excitation thresholds and ultrafast\ndynamics, open up unique opportunities for advancements in photonics and\noptoelectronics devices.",
            "author": [
                "Laura Polimeno",
                "Annalisa Coriolano",
                "Rosanna Mastria",
                "Francesco Todisco",
                "Milena De Giorgi",
                "Antonio Fieramosca",
                "Marco Pugliese",
                "Carmela T. Prontera",
                "Aurora Rizzo",
                "Luisa De Marco",
                "Dario Ballarini",
                "Giuseppe Gigli",
                "Daniele Sanvitto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18379v1",
                "http://arxiv.org/pdf/2311.18379v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18371v1",
            "title": "Hidden Categories: a New Perspective on Lewin's Generalized Interval\n  Systems and Klumpenhouwer Networks",
            "updated": "2023-11-30T09:12:02Z",
            "published": "2023-11-30T09:12:02Z",
            "summary": "In this work we provide a categorical formalization of several constructions\nfound in transformational music theory. We first revisit David Lewin's original\ntheoretical construction of Generalized Interval Systems (GIS) to show that it\nimplicitly defines categories. When all the conditions in Lewin's definition\nare fullfilled, such categories coincide with the category of elements\n$\\int_\\mathbf{G} S$ for the group action $S \\colon \\mathbf{G} \\to\n\\mathbf{Sets}$ implied by the GIS structure. By focusing on the role played by\ncategories of elements in such a context, we reformulate previous definitions\nof transformational networks in a $\\mathbf{Cat}$-based diagrammatical\nperspective, and present a new definition of transformational networks (called\nCT-Nets) in general musical categories. We show incidently how such an approach\nprovides a bridge between algebraic, geometrical and graph-theoretical\napproaches in transformational music analysis. We end with a discussion on the\nnew perspectives opened by such a formalization of transformational theory, in\nparticular with respect to $\\mathbf{Rel}$-based transformational networks which\noccur in well-known music-theoretical constructions such as Douthett's and\nSteinbach's Cube Dance.",
            "author": [
                "Alexandre Popoff",
                "Moreno Andreatta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18371v1",
                "http://arxiv.org/pdf/2311.18371v1"
            ],
            "primary_category": "math.CT",
            "category": [
                "math.CT",
                "math.GR",
                "00A65"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18365v1",
            "title": "Fully Dynamic Algorithms for Euclidean Steiner Tree",
            "updated": "2023-11-30T09:04:33Z",
            "published": "2023-11-30T09:04:33Z",
            "summary": "The Euclidean Steiner tree problem asks to find a min-cost metric graph that\nconnects a given set of \\emph{terminal} points $X$ in $\\mathbb{R}^d$, possibly\nusing points not in $X$ which are called Steiner points. Even though\nnear-linear time $(1 + \\epsilon)$-approximation was obtained in the offline\nsetting in seminal works of Arora and Mitchell, efficient dynamic algorithms\nfor Steiner tree is still open. We give the first algorithm that (implicitly)\nmaintains a $(1 + \\epsilon)$-approximate solution which is accessed via a set\nof tree traversal queries, subject to point insertion and deletions, with\namortized update and query time $O(\\poly\\log n)$ with high probability. Our\napproach is based on an Arora-style geometric dynamic programming, and our main\ntechnical contribution is to maintain the DP subproblems in the dynamic setting\nefficiently. We also need to augment the DP subproblems to support the tree\ntraversal queries.",
            "author": [
                "T-H. Hubert Chan",
                "Gramoz Goranci",
                "Shaofeng H. -C. Jiang",
                "Bo Wang",
                "Quan Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18365v1",
                "http://arxiv.org/pdf/2311.18365v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18358v1",
            "title": "TIDE: Test Time Few Shot Object Detection",
            "updated": "2023-11-30T09:00:44Z",
            "published": "2023-11-30T09:00:44Z",
            "summary": "Few-shot object detection (FSOD) aims to extract semantic knowledge from\nlimited object instances of novel categories within a target domain. Recent\nadvances in FSOD focus on fine-tuning the base model based on a few objects via\nmeta-learning or data augmentation. Despite their success, the majority of them\nare grounded with parametric readjustment to generalize on novel objects, which\nface considerable challenges in Industry 5.0, such as (i) a certain amount of\nfine-tuning time is required, and (ii) the parameters of the constructed model\nbeing unavailable due to the privilege protection, making the fine-tuning fail.\nSuch constraints naturally limit its application in scenarios with real-time\nconfiguration requirements or within black-box settings. To tackle the\nchallenges mentioned above, we formalize a novel FSOD task, referred to as Test\nTIme Few Shot DEtection (TIDE), where the model is un-tuned in the\nconfiguration procedure. To that end, we introduce an asymmetric architecture\nfor learning a support-instance-guided dynamic category classifier. Further, a\ncross-attention module and a multi-scale resizer are provided to enhance the\nmodel performance. Experimental results on multiple few-shot object detection\nplatforms reveal that the proposed TIDE significantly outperforms existing\ncontemporary methods. The implementation codes are available at\nhttps://github.com/deku-0621/TIDE",
            "author": [
                "Weikai Li",
                "Hongfeng Wei",
                "Yanlai Wu",
                "Jie Yang",
                "Yudi Ruan",
                "Yuan Li",
                "Ying Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18358v1",
                "http://arxiv.org/pdf/2311.18358v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18356v1",
            "title": "Towards Comparable Active Learning",
            "updated": "2023-11-30T08:54:32Z",
            "published": "2023-11-30T08:54:32Z",
            "summary": "Active Learning has received significant attention in the field of machine\nlearning for its potential in selecting the most informative samples for\nlabeling, thereby reducing data annotation costs. However, we show that the\nreported lifts in recent literature generalize poorly to other domains leading\nto an inconclusive landscape in Active Learning research. Furthermore, we\nhighlight overlooked problems for reproducing AL experiments that can lead to\nunfair comparisons and increased variance in the results. This paper addresses\nthese issues by providing an Active Learning framework for a fair comparison of\nalgorithms across different tasks and domains, as well as a fast and performant\noracle algorithm for evaluation. To the best of our knowledge, we propose the\nfirst AL benchmark that tests algorithms in 3 major domains: Tabular, Image,\nand Text. We report empirical results for 6 widely used algorithms on 7\nreal-world and 2 synthetic datasets and aggregate them into a domain-specific\nranking of AL algorithms.",
            "author": [
                "Thorben Werner",
                "Johannes Burchert",
                "Lars Schmidt-Thieme"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18356v1",
                "http://arxiv.org/pdf/2311.18356v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18355v1",
            "title": "Guided Demonstrations Using Automated Excuse Generation",
            "updated": "2023-11-30T08:45:19Z",
            "published": "2023-11-30T08:45:19Z",
            "summary": "Teaching task-level directives to robots via demonstration is a popular tool\nto expand the robot's capabilities to interact with its environment. While\ncurrent learning from demonstration systems primarily focuses on abstracting\nthe task-level knowledge to the robot, these systems lack the ability to\nunderstand which part of the task can be already solved given the robot's prior\nknowledge. Therefore, instead of only requiring demonstrations of the missing\npieces, these systems will require a demonstration of the complete task, which\nis cumbersome, repetitive, and can discourage people from helping the robot by\nperforming the demonstrations. Therefore, we propose to use the notion of\n\"excuses\" to identify the smallest change in the robot state that makes a task,\ncurrently not solvable by the robot, solvable -- as a means to solicit more\ntargeted demonstrations from a human. These excuses are generated automatically\nusing combinatorial search over possible changes that can be made to the\nrobot's state and choosing the minimum changes that make it solvable. These\nexcuses then serve as guidance for the demonstrator who can use it to decide\nwhat to demonstrate to the robot in order to make this requested change\npossible, thereby making the original task solvable for the robot without\nhaving to demonstrate it in its entirety. By working with symbolic state\ndescriptions, the excuses can be directly communicated and intuitively\nunderstood by a human demonstrator. We show empirically and in a user study\nthat the use of excuses reduces the demonstration time by 54% and leads to a\n74% reduction in demonstration size.",
            "author": [
                "Maximilian Diehl",
                "Tathagata Chakraborti",
                "Karinne Ramirez-Amaro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18355v1",
                "http://arxiv.org/pdf/2311.18355v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18340v1",
            "title": "Neuromorphic Incremental on-chip Learning with Hebbian Weight\n  Consolidation",
            "updated": "2023-11-30T08:19:03Z",
            "published": "2023-11-30T08:19:03Z",
            "summary": "As next-generation implantable brain-machine interfaces become pervasive on\nedge device, incrementally learning new tasks in bio-plasticity ways is\nurgently demanded for Neuromorphic chips. Due to the inherent characteristics\nof its structure, spiking neural networks are naturally well-suited for\nBMI-chips. Here we propose Hebbian Weight Consolidation, as well as an on-chip\nlearning framework. HWC selectively masks synapse modifications for previous\ntasks, retaining them to store new knowledge from subsequent tasks while\npreserving the old knowledge. Leveraging the bio-plasticity of dendritic\nspines, the intrinsic self-organizing nature of Hebbian Weight Consolidation\naligns naturally with the incremental learning paradigm, facilitating robust\nlearning outcomes. By reading out spikes layer by layer and performing\nback-propagation on the external micro-controller unit, MLoC can efficiently\naccomplish on-chip learning. Experiments show that our HWC algorithm up to\n23.19% outperforms lower bound that without incremental learning algorithm,\nparticularly in more challenging monkey behavior decoding scenarios. Taking\ninto account on-chip computing on Synsense Speck 2e chip, our proposed\nalgorithm exhibits an improvement of 11.06%. This study demonstrates the\nfeasibility of employing incremental learning for high-performance neural\nsignal decoding in next-generation brain-machine interfaces.",
            "author": [
                "Zifan Ning",
                "Chaojin Chen",
                "Xiang Cheng",
                "Wangzi Yao",
                "Tielin Zhang",
                "Bo Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18340v1",
                "http://arxiv.org/pdf/2311.18340v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00821v1",
            "title": "Global well-posedness of a two dimensional wave-Klein-Gordon system with\n  small non-compactly supported data",
            "updated": "2023-11-30T08:11:36Z",
            "published": "2023-11-30T08:11:36Z",
            "summary": "In this paper we are interested in the coupled wave and Klein-Gordon\nequations in $\\mathbb{R}^+\\times\\mathbb{R}^2$. We want to establish the global\nwell-posedness of such system by showing the uniform boundedness of the energy\nfor the global solution without any compactness assumptions on the initial\ndata. In addition, we also demonstrate the pointwise asymptotic behavior of the\nsolution pair. In order to achieve that we apply a modified Alinhac's ghost\nweight method together with a newly developed normal-form framework to remedy\nthe lack of the space-time scaling vector field. Finally we show the global\nsolutions scatter linearly strongly for the Klein-Gordon field $\\phi$ and\nweakly for the wave field $n$ as $t\\to+\\infty$. To our best knowledge such\nscattering phenomenon is novel in the literature.",
            "author": [
                "Xinyu Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00821v1",
                "http://arxiv.org/pdf/2312.00821v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00089v1",
            "title": "Constant Sum Partition of $\\{1,2,...,n\\}$ Into Subsets With Prescribed\n  Orders",
            "updated": "2023-11-30T07:52:29Z",
            "published": "2023-11-30T07:52:29Z",
            "summary": "Studies on partition of $I_n$ = $\\{1, 2, . . . , n\\}$ into subsets $S_1, S_2,\n. . . , S_x$ so far considered with prescribed sum of the elements in each\nsubset. In this paper, we study constant sum partitions $\\{S_1,S_2,...,S_x\\}$\nof $I_n$ with prescribed $|S_i|$, $1 \\leq i \\leq x$. Theorem \\ref{thm 2.3} is\nthe main result which gives a necessary and sufficient condition for a\npartition set $\\{S_1,S_2,\\ldots, S_x\\}$ of $I_n$ with prescribed $|S_i|$ to be\na constant sum partition of $I_n$, $1 \\leq i \\leq x$ and $n > x \\geq 2$. We\nstate its applications in graph theory and also define {\\em constant sum\npartition permutation} or {\\em magic partition permutation} of $I_n$. A\npartition $\\{S_1,S_2,\\cdots,S_x\\}$ of $I_n$ is a {\\em constant sum partition of\n$I_n$} if $\\sum_{j\\in S_i}{j}$ is a constant for every $i$, $1 \\leq i \\leq x$.",
            "author": [
                "V. Vilfred Kamalappan",
                "Sajidha P"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00089v1",
                "http://arxiv.org/pdf/2312.00089v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "11P81, 05C70, 05C78"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18324v1",
            "title": "Low-rank optimization on Tucker tensor varieties",
            "updated": "2023-11-30T07:49:24Z",
            "published": "2023-11-30T07:49:24Z",
            "summary": "In the realm of tensor optimization, low-rank tensor decomposition,\nparticularly Tucker decomposition, stands as a pivotal technique for reducing\nthe number of parameters and for saving storage. We embark on an exploration of\nTucker tensor varieties -- the set of tensors with bounded Tucker rank -- in\nwhich the geometry is notably more intricate than the well-explored geometry of\nmatrix varieties. We give an explicit parametrization of the tangent cone of\nTucker tensor varieties and leverage its geometry to develop provable\ngradient-related line-search methods for optimization on Tucker tensor\nvarieties. The search directions are computed from approximate projections of\nantigradient onto the tangent cone, which circumvents the calculation of\nintractable metric projections. To the best of our knowledge, this is the first\nwork concerning geometry and optimization on Tucker tensor varieties. In\npractice, low-rank tensor optimization suffers from the difficulty of choosing\na reliable rank parameter. To this end, we incorporate the established geometry\nand propose a Tucker rank-adaptive method that is capable of identifying an\nappropriate rank during iterations while the convergence is also guaranteed.\nNumerical experiments on tensor completion with synthetic and real-world\ndatasets reveal that the proposed methods are in favor of recovering\nperformance over other state-of-the-art methods. Moreover, the rank-adaptive\nmethod performs the best across various rank parameter selections and is indeed\nable to find an appropriate rank.",
            "author": [
                "Bin Gao",
                "Renfeng Peng",
                "Ya-xiang Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18324v1",
                "http://arxiv.org/pdf/2311.18324v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.NA",
                "math.NA",
                "15A69, 65K05, 65F30, 90C30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18316v1",
            "title": "Learning for Semantic Knowledge Base-Guided Online Feature Transmission\n  in Dynamic Channels",
            "updated": "2023-11-30T07:35:56Z",
            "published": "2023-11-30T07:35:56Z",
            "summary": "With the proliferation of edge computing, efficient AI inference on edge\ndevices has become essential for intelligent applications such as autonomous\nvehicles and VR/AR. In this context, we address the problem of efficient remote\nobject recognition by optimizing feature transmission between mobile devices\nand edge servers. We propose an online optimization framework to address the\nchallenge of dynamic channel conditions and device mobility in an end-to-end\ncommunication system. Our approach builds upon existing methods by leveraging a\nsemantic knowledge base to drive multi-level feature transmission, accounting\nfor temporal factors and dynamic elements throughout the transmission process.\nTo solve the online optimization problem, we design a novel soft\nactor-critic-based deep reinforcement learning system with a carefully designed\nreward function for real-time decision-making, overcoming the optimization\ndifficulty of the NP-hard problem and achieving the minimization of semantic\nloss while respecting latency constraints. Numerical results showcase the\nsuperiority of our approach compared to traditional greedy methods under\nvarious system setups.",
            "author": [
                "Xiangyu Gao",
                "Yaping Sun",
                "Dongyu Wei",
                "Xiaodong Xu",
                "Hao Chen",
                "Hao Yin",
                "Shuguang Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18316v1",
                "http://arxiv.org/pdf/2311.18316v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00086v1",
            "title": "Star colouring and locally constrained graph homomorphisms",
            "updated": "2023-11-30T07:25:05Z",
            "published": "2023-11-30T07:25:05Z",
            "summary": "Dvo\\v{r}\\'ak, Mohar and \\v{S}\\'amal (J. Graph Theory, 2013) proved that for\nevery 3-regular graph $G$, the line graph of $G$ is 4-star colourable if and\nonly if $G$ admits a locally bijective homomorphism to the cube $Q_3$. We\ngeneralise this result as follows: for $p\\geq 2$, a $K_{1,p+1}$-free\n$2p$-regular graph $G$ admits a $(p + 2)$-star colouring if and only if $G$\nadmits a locally bijective homomorphism to a fixed $2p$-regular graph named\n$G_{2p}$. We also prove the following: (i) for $p\\geq 2$, a $2p$-regular graph\n$G$ admits a $(p + 2)$-star colouring if and only if $G$ has an orientation\n$\\vec{G}$ that admits an out-neighbourhood bijective homomorphism to a fixed\norientation $\\vec{G_{2p}}$ of $G2p$; (ii) for every 3-regular graph $G$, the\nline graph of $G$ is 4-star colourable if and only if $G$ is bipartite and\ndistance-two 4-colourable; and (iii) it is NP-complete to check whether a\nplanar 4-regular 3-connected graph is 4-star colourable.",
            "author": [
                "Shalu M. A.",
                "Cyriac Antony"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00086v1",
                "http://arxiv.org/pdf/2312.00086v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18303v1",
            "title": "OmniMotionGPT: Animal Motion Generation with Limited Data",
            "updated": "2023-11-30T07:14:00Z",
            "published": "2023-11-30T07:14:00Z",
            "summary": "Our paper aims to generate diverse and realistic animal motion sequences from\ntextual descriptions, without a large-scale animal text-motion dataset. While\nthe task of text-driven human motion synthesis is already extensively studied\nand benchmarked, it remains challenging to transfer this success to other\nskeleton structures with limited data. In this work, we design a model\narchitecture that imitates Generative Pretraining Transformer (GPT), utilizing\nprior knowledge learned from human data to the animal domain. We jointly train\nmotion autoencoders for both animal and human motions and at the same time\noptimize through the similarity scores among human motion encoding, animal\nmotion encoding, and text CLIP embedding. Presenting the first solution to this\nproblem, we are able to generate animal motions with high diversity and\nfidelity, quantitatively and qualitatively outperforming the results of\ntraining human motion generation baselines on animal data. Additionally, we\nintroduce AnimalML3D, the first text-animal motion dataset with 1240 animation\nsequences spanning 36 different animal identities. We hope this dataset would\nmediate the data scarcity problem in text-driven animal motion generation,\nproviding a new playground for the research community.",
            "author": [
                "Zhangsihao Yang",
                "Mingyuan Zhou",
                "Mengyi Shan",
                "Bingbing Wen",
                "Ziwei Xuan",
                "Mitch Hill",
                "Junjie Bai",
                "Guo-Jun Qi",
                "Yalin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18303v1",
                "http://arxiv.org/pdf/2311.18303v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18301v1",
            "title": "Rainbow common graphs must be forests",
            "updated": "2023-11-30T07:11:47Z",
            "published": "2023-11-30T07:11:47Z",
            "summary": "We study the rainbow version of the graph commonness property: a graph $H$ is\n$r$-rainbow common if the number of rainbow copies of $H$ (where all edges have\ndistinct colors) in an $r$-coloring of edges of $K_n$ is maximized\nasymptotically by independently coloring each edge uniformly at random. $H$ is\n\\emph{$r$-rainbow uncommon} otherwise. We show that if $H$ has a cycle, then it\nis $r$-rainbow uncommon for every $r$ at least the number of edges of $H$. This\ngeneralizes a result of Erd\\H{o}s and Hajnal, and proves a conjecture of De\nSilva, Si, Tait, Tun\\c{c}bilek, Yang, and Young.",
            "author": [
                "Yihang Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18301v1",
                "http://arxiv.org/pdf/2311.18301v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18300v1",
            "title": "Multi-label Annotation for Visual Multi-Task Learning Models",
            "updated": "2023-11-30T07:10:13Z",
            "published": "2023-11-30T07:10:13Z",
            "summary": "Deep learning requires large amounts of data, and a well-defined pipeline for\nlabeling and augmentation. Current solutions support numerous computer vision\ntasks with dedicated annotation types and formats, such as bounding boxes,\npolygons, and key points. These annotations can be combined into a single data\nformat to benefit approaches such as multi-task models. However, to our\nknowledge, no available labeling tool supports the export functionality for a\ncombined benchmark format, and no augmentation library supports transformations\nfor the combination of all. In this work, these functionalities are presented,\nwith visual data annotation and augmentation to train a multi-task model\n(object detection, segmentation, and key point extraction). The tools are\ndemonstrated in two robot perception use cases.",
            "author": [
                "G. Sharma",
                "A. Angleraud",
                "R. Pieters"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18300v1",
                "http://arxiv.org/pdf/2311.18300v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18295v1",
            "title": "Almost-Linear Time Algorithms for Incremental Graphs: Cycle Detection,\n  SCCs, $s$-$t$ Shortest Path, and Minimum-Cost Flow",
            "updated": "2023-11-30T06:58:37Z",
            "published": "2023-11-30T06:58:37Z",
            "summary": "We give the first almost-linear time algorithms for several problems in\nincremental graphs including cycle detection, strongly connected component\nmaintenance, $s$-$t$ shortest path, maximum flow, and minimum-cost flow. To\nsolve these problems, we give a deterministic data structure that returns a\n$m^{o(1)}$-approximate minimum-ratio cycle in fully dynamic graphs in amortized\n$m^{o(1)}$ time per update. Combining this with the interior point method\nframework of Brand-Liu-Sidford (STOC 2023) gives the first almost-linear time\nalgorithm for deciding the first update in an incremental graph after which the\ncost of the minimum-cost flow attains value at most some given threshold $F$.\nBy rather direct reductions to minimum-cost flow, we are then able to solve the\nproblems in incremental graphs mentioned above.\n  At a high level, our algorithm dynamizes the $\\ell_1$ oblivious routing of\nRozho\\v{n}-Grunau-Haeupler-Zuzic-Li (STOC 2022), and develops a method to\nextract an approximate minimum ratio cycle from the structure of the oblivious\nrouting. To maintain the oblivious routing, we use tools from concurrent work\nof Kyng-Meierhans-Probst Gutenberg which designed vertex sparsifiers for\nshortest paths, in order to maintain a sparse neighborhood cover in fully\ndynamic graphs.\n  To find a cycle, we first show that an approximate minimum ratio cycle can be\nrepresented as a fundamental cycle on a small set of trees resulting from the\noblivious routing. Then, we find a cycle whose quality is comparable to the\nbest tree cycle. This final cycle query step involves vertex and edge\nsparsification procedures reminiscent of previous works, but crucially requires\na more powerful dynamic spanner which can handle far more edge insertions. We\nbuild such a spanner via a construction that hearkens back to the classic\ngreedy spanner algorithm.",
            "author": [
                "Li Chen",
                "Rasmus Kyng",
                "Yang P. Liu",
                "Simon Meierhans",
                "Maximilian Probst Gutenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18295v1",
                "http://arxiv.org/pdf/2311.18295v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18284v1",
            "title": "The Complement of the Djokovic-Winkler Relation",
            "updated": "2023-11-30T06:44:11Z",
            "published": "2023-11-30T06:44:11Z",
            "summary": "The Djokovi\\'{c}-Winkler relation $\\Theta$ is a binary relation defined on\nthe edge set of a given graph that is based on the distances of certain\nvertices and which plays a prominent role in graph theory. In this paper, we\nexplore the relatively uncharted ``reflexive complement'' $\\overline\\Theta$ of\n$\\Theta$, where $(e,f)\\in \\overline\\Theta$ if and only if $e=f$ or $(e,f)\\notin\n\\Theta$ for edges $e$ and $f$. We establish the relationship between\n$\\overline\\Theta$ and the set $\\Delta_{ef}$, comprising the distances between\nthe vertices of $e$ and $f$ and shed some light on the intricacies of its\ntransitive closure $\\overline\\Theta^*$. Notably, we demonstrate that\n$\\overline\\Theta^*$ exhibits multiple equivalence classes only within a\nrestricted subclass of complete multipartite graphs. In addition, we\ncharacterize non-trivial relations $R$ that coincide with $\\overline\\Theta$ as\nthose where the graph representation is disconnected, with each connected\ncomponent being the (join of) Cartesian product of complete graphs. The latter\nresults imply, somewhat surprisingly, that knowledge about the distances\nbetween vertices is not required to determine $\\overline\\Theta^*$. Moreover,\n$\\overline\\Theta^*$ has either exactly one or three equivalence classes.",
            "author": [
                "Marc Hellmuth",
                "Bruno J. Schmidt",
                "Guillaume E. Scholz",
                "Sandhya Thekkumpadan Puthiyaveedu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18284v1",
                "http://arxiv.org/pdf/2311.18284v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18281v1",
            "title": "Utilizing Radiomic Feature Analysis For Automated MRI Keypoint\n  Detection: Enhancing Graph Applications",
            "updated": "2023-11-30T06:37:02Z",
            "published": "2023-11-30T06:37:02Z",
            "summary": "Graph neural networks (GNNs) present a promising alternative to CNNs and\ntransformers in certain image processing applications due to their\nparameter-efficiency in modeling spatial relationships. Currently, a major area\nof research involves the converting non-graph input data for GNN-based models,\nnotably in scenarios where the data originates from images. One approach\ninvolves converting images into nodes by identifying significant keypoints\nwithin them. Super-Retina, a semi-supervised technique, has been utilized for\ndetecting keypoints in retinal images. However, its limitations lie in the\ndependency on a small initial set of ground truth keypoints, which is\nprogressively expanded to detect more keypoints. Having encountered\ndifficulties in detecting consistent initial keypoints in brain images using\nSIFT and LoFTR, we proposed a new approach: radiomic feature-based keypoint\ndetection. Demonstrating the anatomical significance of the detected keypoints\nwas achieved by showcasing their efficacy in improving registration processes\nguided by these keypoints. Subsequently, these keypoints were employed as the\nground truth for the keypoint detection method (LK-SuperRetina). Furthermore,\nthe study showcases the application of GNNs in image matching, highlighting\ntheir superior performance in terms of both the number of good matches and\nconfidence scores. This research sets the stage for expanding GNN applications\ninto various other applications, including but not limited to image\nclassification, segmentation, and registration.",
            "author": [
                "Sahar Almahfouz Nasser",
                "Shashwat Pathak",
                "Keshav Singhal",
                "Mohit Meena",
                "Nihar Gupte",
                "Ananya Chinmaya",
                "Prateek Garg",
                "Amit Sethi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18281v1",
                "http://arxiv.org/pdf/2311.18281v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18273v1",
            "title": "HKUST at SemEval-2023 Task 1: Visual Word Sense Disambiguation with\n  Context Augmentation and Visual Assistance",
            "updated": "2023-11-30T06:23:15Z",
            "published": "2023-11-30T06:23:15Z",
            "summary": "Visual Word Sense Disambiguation (VWSD) is a multi-modal task that aims to\nselect, among a batch of candidate images, the one that best entails the target\nword's meaning within a limited context. In this paper, we propose a\nmulti-modal retrieval framework that maximally leverages pretrained\nVision-Language models, as well as open knowledge bases and datasets. Our\nsystem consists of the following key components: (1) Gloss matching: a\npretrained bi-encoder model is used to match contexts with proper senses of the\ntarget words; (2) Prompting: matched glosses and other textual information,\nsuch as synonyms, are incorporated using a prompting template; (3) Image\nretrieval: semantically matching images are retrieved from large open datasets\nusing prompts as queries; (4) Modality fusion: contextual information from\ndifferent modalities are fused and used for prediction. Although our system\ndoes not produce the most competitive results at SemEval-2023 Task 1, we are\nstill able to beat nearly half of the teams. More importantly, our experiments\nreveal acute insights for the field of Word Sense Disambiguation (WSD) and\nmulti-modal learning. Our code is available on GitHub.",
            "author": [
                "Zhuohao Yin",
                "Xin Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18273v1",
                "http://arxiv.org/pdf/2311.18273v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18251v1",
            "title": "Can Large Language Models Be Good Companions? An LLM-Based Eyewear\n  System with Conversational Common Ground",
            "updated": "2023-11-30T04:59:34Z",
            "published": "2023-11-30T04:59:34Z",
            "summary": "Developing chatbots as personal companions has long been a goal of artificial\nintelligence researchers. Recent advances in Large Language Models (LLMs) have\ndelivered a practical solution for endowing chatbots with anthropomorphic\nlanguage capabilities. However, it takes more than LLMs to enable chatbots that\ncan act as companions. Humans use their understanding of individual\npersonalities to drive conversations. Chatbots also require this capability to\nenable human-like companionship. They should act based on personalized,\nreal-time, and time-evolving knowledge of their owner. We define such essential\nknowledge as the \\textit{common ground} between chatbots and their owners, and\nwe propose to build a common-ground-aware dialogue system from an LLM-based\nmodule, named \\textit{OS-1}, to enable chatbot companionship. Hosted by\neyewear, OS-1 can sense the visual and audio signals the user receives and\nextract real-time contextual semantics. Those semantics are categorized and\nrecorded to formulate historical contexts from which the user's profile is\ndistilled and evolves over time, i.e., OS-1 gradually learns about its user.\nOS-1 combines knowledge from real-time semantics, historical contexts, and\nuser-specific profiles to produce a common-ground-aware prompt input into the\nLLM module. The LLM's output is converted to audio, spoken to the wearer when\nappropriate.We conduct laboratory and in-field studies to assess OS-1's ability\nto build common ground between the chatbot and its user. The technical\nfeasibility and capabilities of the system are also evaluated. OS-1, with its\ncommon-ground awareness, can significantly improve user satisfaction and\npotentially lead to downstream tasks such as personal emotional support and\nassistance.",
            "author": [
                "Zhenyu Xu",
                "Hailin Xu",
                "Zhouyang Lu",
                "Yingying Zhao",
                "Rui Zhu",
                "Yujiang Wang",
                "Mingzhi Dong",
                "Yuhu Chang",
                "Qin Lv",
                "Robert P. Dick",
                "Fan Yang",
                "Tun Lu",
                "Ning Gu",
                "Li Shang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18251v1",
                "http://arxiv.org/pdf/2311.18251v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18250v2",
            "title": "Feasibility Analysis of In-Band Coexistence in Dense LEO Satellite\n  Communication Systems",
            "updated": "2023-12-01T18:40:22Z",
            "published": "2023-11-30T04:58:26Z",
            "summary": "This work provides a rigorous methodology for assessing the feasibility of\nspectrum sharing between large low-earth orbit (LEO) satellite constellations.\nFor concreteness, we focus on the existing Starlink system and the\nsoon-to-be-launched Kuiper system, which is prohibited from inflicting\nexcessive interference onto the incumbent Starlink ground users. We carefully\nmodel and study the potential downlink interference between the two systems and\ninvestigate how strategic satellite selection may be used by Kuiper to serve\nits ground users while also protecting Starlink ground users. We then extend\nthis notion of satellite selection to the case where Kuiper has limited\nknowledge of Starlink's serving satellite. Our findings reveal that there is\nalways the potential for very high and extremely low interference, depending on\nwhich Starlink and Kuiper satellites are being used to serve their users.\nConsequently, we show that Kuiper can protect Starlink ground users with high\nprobability, by strategically selecting which of its satellites are used to\nserve its ground users. Simultaneously, Kuiper is capable of delivering\nnear-maximal downlink SINR to its own ground users. This highlights a feasible\nroute to the coexistence of two dense LEO satellite systems, even in scenarios\nwhere one system has limited knowledge of the other's serving satellites.",
            "author": [
                "Eunsun Kim",
                "Ian P. Roberts",
                "Jeffrey G. Andrews"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18250v2",
                "http://arxiv.org/pdf/2311.18250v2"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00819v1",
            "title": "Large Language Models for Travel Behavior Prediction",
            "updated": "2023-11-30T04:35:55Z",
            "published": "2023-11-30T04:35:55Z",
            "summary": "Travel behavior prediction is a fundamental task in transportation demand\nmanagement. The conventional methods for travel behavior prediction rely on\nnumerical data to construct mathematical models and calibrate model parameters\nto represent human preferences. Recent advancement in large language models\n(LLMs) has shown great reasoning abilities to solve complex problems. In this\nstudy, we propose to use LLMs to predict travel behavior with prompt\nengineering without data-based parameter learning. Specifically, we carefully\ndesign our prompts that include 1) task description, 2) travel characteristics,\n3) individual attributes, and 4) guides of thinking with domain knowledge, and\nask the LLMs to predict an individual's travel behavior and explain the\nresults. We select the travel mode choice task as a case study. Results show\nthat, though no training samples are provided, LLM-based predictions have\ncompetitive accuracy and F1-score as canonical supervised learning methods such\nas multinomial logit, random forest, and neural networks. LLMs can also output\nreasons that support their prediction. However, though in most of the cases,\nthe output explanations are reasonable, we still observe cases that violate\nlogic or with hallucinations.",
            "author": [
                "Baichuan Mo",
                "Hanyong Xu",
                "Dingyi Zhuang",
                "Ruoyun Ma",
                "Xiaotong Guo",
                "Jinhua Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00819v1",
                "http://arxiv.org/pdf/2312.00819v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18243v1",
            "title": "DKiS: Decay weight invertible image steganography with private key",
            "updated": "2023-11-30T04:21:10Z",
            "published": "2023-11-30T04:21:10Z",
            "summary": "Image steganography, the practice of concealing information within another\nimage, traditionally faces security challenges when its methods become publicly\nknown. To counteract this, we introduce a novel private key-based image\nsteganography technique. This approach ensures the security of hidden\ninformation, requiring a corresponding private key for access, irrespective of\nthe public knowledge of the steganography method. We present experimental\nevidence demonstrating our method's effectiveness, showcasing its real-world\napplicability. Additionally, we identified a critical challenge in the\ninvertible image steganography process: the transfer of non-essential, or\n`garbage', information from the secret to the host pipeline. To address this,\nwe introduced the decay weight to control the information transfer, filtering\nout irrelevant data and enhancing the performance of image steganography. Our\ncode is publicly accessible at https://github.com/yanghangAI/DKiS, and a\npractical demonstration is available at http://yanghang.site/hidekey.",
            "author": [
                "Hang Yang",
                "Yitian Xu",
                "Xuhua Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18243v1",
                "http://arxiv.org/pdf/2311.18243v1"
            ],
            "primary_category": "cs.MM",
            "category": [
                "cs.MM",
                "cs.CR",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18237v1",
            "title": "Label-efficient Training of Small Task-specific Models by Leveraging\n  Vision Foundation Models",
            "updated": "2023-11-30T04:07:44Z",
            "published": "2023-11-30T04:07:44Z",
            "summary": "Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit\nimpressive performance on various downstream tasks, especially with limited\nlabeled target data. However, due to their high memory and compute\nrequirements, these models cannot be deployed in resource constrained settings.\nThis raises an important question: How can we utilize the knowledge from a\nlarge VFM to train a small task-specific model for a new target task with\nlimited labeled training data? In this work, we answer this question by\nproposing a simple and highly effective task-oriented knowledge transfer\napproach to leverage pretrained VFMs for effective training of small\ntask-specific models. Our experimental results on four target tasks under\nlimited labeled data settings show that the proposed knowledge transfer\napproach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining\nand supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively.\nWe also show that the dataset used for transferring knowledge has a significant\neffect on the final target task performance, and propose an image\nretrieval-based approach for curating effective transfer sets.",
            "author": [
                "Raviteja Vemulapalli",
                "Hadi Pouransari",
                "Fartash Faghri",
                "Sachin Mehta",
                "Mehrdad Farajtabar",
                "Mohammad Rastegari",
                "Oncel Tuzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18237v1",
                "http://arxiv.org/pdf/2311.18237v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18231v1",
            "title": "TCP:Textual-based Class-aware Prompt tuning for Visual-Language Model",
            "updated": "2023-11-30T03:59:23Z",
            "published": "2023-11-30T03:59:23Z",
            "summary": "Prompt tuning represents a valuable technique for adapting pre-trained\nvisual-language models (VLM) to various downstream tasks. Recent advancements\nin CoOp-based methods propose a set of learnable domain-shared or\nimage-conditional textual tokens to facilitate the generation of task-specific\ntextual classifiers. However, those textual tokens have a limited\ngeneralization ability regarding unseen domains, as they cannot dynamically\nadjust to the distribution of testing classes. To tackle this issue, we present\na novel Textual-based Class-aware Prompt tuning(TCP) that explicitly\nincorporates prior knowledge about classes to enhance their discriminability.\nThe critical concept of TCP involves leveraging Textual Knowledge Embedding\n(TKE) to map the high generalizability of class-level textual knowledge into\nclass-aware textual tokens. By seamlessly integrating these class-aware prompts\ninto the Text Encoder, a dynamic class-aware classifier is generated to enhance\ndiscriminability for unseen domains. During inference, TKE dynamically\ngenerates class-aware prompts related to the unseen classes. Comprehensive\nevaluations demonstrate that TKE serves as a plug-and-play module effortlessly\ncombinable with existing methods. Furthermore, TCP consistently achieves\nsuperior performance while demanding less training time.",
            "author": [
                "Hantao Yao",
                "Rui Zhang",
                "Changsheng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18231v1",
                "http://arxiv.org/pdf/2311.18231v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18225v1",
            "title": "Harnessing graph state resources for robust quantum magnetometry under\n  noise",
            "updated": "2023-11-30T03:38:35Z",
            "published": "2023-11-30T03:38:35Z",
            "summary": "Precise measurement of magnetic fields is essential for various applications,\nsuch as fundamental physics, space exploration, and biophysics. Although recent\nprogress in quantum engineering has assisted in creating advanced quantum\nmagnetometers, there are still ongoing challenges in improving their efficiency\nand noise resistance. This study focuses on using symmetric graph state\nresources for quantum magnetometry to enhance measurement precision by\nanalyzing the estimation theory under Markovian and non-Markovian noise models.\nThe results show a significant improvement in estimating both single and\nmultiple Larmor frequencies. In single Larmor frequency estimation, the quantum\nFisher information spans a spectrum from the standard quantum limit to the\nHeisenberg limit within a periodic range of the Larmor frequency, and in the\ncase of multiple Larmor frequencies, it can exceed the standard quantum limit\nfor both Markovian and non-Markovian noise. This study highlights the potential\nof graph state-based methods for improving magnetic field measurements under\nnoisy environments.",
            "author": [
                "Phu Trong Nguyen",
                "Trung Kien Le",
                "Hung Q. Nguyen",
                "Le Bin Ho"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18225v1",
                "http://arxiv.org/pdf/2311.18225v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18219v1",
            "title": "FoldExplorer: Fast and Accurate Protein Structure Search with\n  Sequence-Enhanced Graph Embedding",
            "updated": "2023-11-30T03:29:20Z",
            "published": "2023-11-30T03:29:20Z",
            "summary": "The advent of highly accurate protein structure prediction methods has fueled\nan exponential expansion of the protein structure database. Consequently, there\nis a rising demand for rapid and precise structural homolog search. Traditional\nalignment-based methods are dedicated to precise comparisons between pairs,\nexhibiting high accuracy. However, their sluggish processing speed is no longer\nadequate for managing the current massive volume of data. In response to this\nchallenge, we propose a novel deep-learning approach FoldExplorer. It harnesses\nthe powerful capabilities of graph attention neural networks and protein large\nlanguage models for protein structures and sequences data processing to\ngenerate embeddings for protein structures. The structural embeddings can be\nused for fast and accurate protein search. The embeddings also provide insights\ninto the protein space. FoldExplorer demonstrates a substantial performance\nimprovement of 5% to 8% over the current state-of-the-art algorithm on the\nbenchmark datasets. Meanwhile, FoldExplorer does not compromise on search speed\nand excels particularly in searching on a large-scale dataset.",
            "author": [
                "Yuan Liu",
                "Hong-Bin Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18219v1",
                "http://arxiv.org/pdf/2311.18219v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18218v1",
            "title": "Computing the Bounds of the Number of Reticulations in a Tree-Child\n  Network That Displays a Set of Trees",
            "updated": "2023-11-30T03:22:32Z",
            "published": "2023-11-30T03:22:32Z",
            "summary": "Phylogenetic network is an evolutionary model that uses a rooted directed\nacyclic graph (instead of a tree) to model an evolutionary history of species\nin which reticulate events (e.g., hybrid speciation or horizontal gene\ntransfer) occurred. Tree-child network is a kind of phylogenetic network with\nstructural constraints. Existing approaches for tree-child network\nreconstruction can be slow for large data. In this paper, we present several\ncomputational approaches for bounding from below the number of reticulations in\na tree-child network that displays a given set of rooted binary phylogenetic\ntrees. In addition, we also present some theoretical results on bounding from\nabove the number of reticulations. Through simulation, we demonstrate that the\nnew lower bounds on the reticulation number for tree-child networks can\npractically be computed for large tree data. The bounds can provide estimates\nof reticulation for relatively large data.",
            "author": [
                "Yufeng Wu",
                "Louxin Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18218v1",
                "http://arxiv.org/pdf/2311.18218v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "05C30",
                "J.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18214v1",
            "title": "Perception of Misalignment States for Sky Survey Telescopes with the\n  Digital Twin and the Deep Neural Networks",
            "updated": "2023-11-30T03:16:27Z",
            "published": "2023-11-30T03:16:27Z",
            "summary": "Sky survey telescopes play a critical role in modern astronomy, but\nmisalignment of their optical elements can introduce significant variations in\npoint spread functions, leading to reduced data quality. To address this, we\nneed a method to obtain misalignment states, aiding in the reconstruction of\naccurate point spread functions for data processing methods or facilitating\nadjustments of optical components for improved image quality. Since sky survey\ntelescopes consist of many optical elements, they result in a vast array of\npotential misalignment states, some of which are intricately coupled, posing\ndetection challenges. However, by continuously adjusting the misalignment\nstates of optical elements, we can disentangle coupled states. Based on this\nprinciple, we propose a deep neural network to extract misalignment states from\ncontinuously varying point spread functions in different field of views. To\nensure sufficient and diverse training data, we recommend employing a digital\ntwin to obtain data for neural network training. Additionally, we introduce the\nstate graph to store misalignment data and explore complex relationships\nbetween misalignment states and corresponding point spread functions, guiding\nthe generation of training data from experiments. Once trained, the neural\nnetwork estimates misalignment states from observation data, regardless of the\nimpacts caused by atmospheric turbulence, noise, and limited spatial sampling\nrates in the detector. The method proposed in this paper could be used to\nprovide prior information for the active optics system and the optical system\nalignment.",
            "author": [
                "Miao Zhang",
                "Peng Jia",
                "Zhengyang Li",
                "Wennan Xiang",
                "Jiameng Lv",
                "Rui Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18214v1",
                "http://arxiv.org/pdf/2311.18214v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.GA",
                "astro-ph.SR",
                "cs.CV",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18182v1",
            "title": "PEOPLEx: PEdestrian Opportunistic Positioning LEveraging IMU, UWB, BLE\n  and WiFi",
            "updated": "2023-11-30T01:57:00Z",
            "published": "2023-11-30T01:57:00Z",
            "summary": "This paper advances the field of pedestrian localization by introducing a\nunifying framework for opportunistic positioning based on nonlinear factor\ngraph optimization. While many existing approaches assume constant availability\nof one or multiple sensing signals, our methodology employs IMU-based\npedestrian inertial navigation as the backbone for sensor fusion,\nopportunistically integrating Ultra-Wideband (UWB), Bluetooth Low Energy (BLE),\nand WiFi signals when they are available in the environment. The proposed\nPEOPLEx framework is designed to incorporate sensing data as it becomes\navailable, operating without any prior knowledge about the environment (e.g.\nanchor locations, radio frequency maps, etc.). Our contributions are twofold:\n1) we introduce an opportunistic multi-sensor and real-time pedestrian\npositioning framework fusing the available sensor measurements; 2) we develop\nnovel factors for adaptive scaling and coarse loop closures, significantly\nimproving the precision of indoor positioning. Experimental validation confirms\nthat our approach achieves accurate localization estimates in real indoor\nscenarios using commercial smartphones.",
            "author": [
                "Pierre-Yves Lajoie",
                "Bobak Hamed Baghi",
                "Sachini Herath",
                "Francois Hogan",
                "Xue Liu",
                "Gregory Dudek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18182v1",
                "http://arxiv.org/pdf/2311.18182v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18177v1",
            "title": "An Effective Universal Polynomial Basis for Spectral Graph Neural\n  Networks",
            "updated": "2023-11-30T01:48:42Z",
            "published": "2023-11-30T01:48:42Z",
            "summary": "Spectral Graph Neural Networks (GNNs), also referred to as graph filters have\ngained increasing prevalence for heterophily graphs. Optimal graph filters rely\non Laplacian eigendecomposition for Fourier transform. In an attempt to avert\nthe prohibitive computations, numerous polynomial filters by leveraging\ndistinct polynomials have been proposed to approximate the desired graph\nfilters. However, polynomials in the majority of polynomial filters are\npredefined and remain fixed across all graphs, failing to accommodate the\ndiverse heterophily degrees across different graphs. To tackle this issue, we\nfirst investigate the correlation between polynomial bases of desired graph\nfilters and the degrees of graph heterophily via a thorough theoretical\nanalysis. Afterward, we develop an adaptive heterophily basis by incorporating\ngraph heterophily degrees. Subsequently, we integrate this heterophily basis\nwith the homophily basis, creating a universal polynomial basis UniBasis. In\nconsequence, we devise a general polynomial filter UniFilter. Comprehensive\nexperiments on both real-world and synthetic datasets with varying heterophily\ndegrees significantly support the superiority of UniFilter, demonstrating the\neffectiveness and generality of UniBasis, as well as its promising capability\nas a new method for graph analysis.",
            "author": [
                "Keke Huang",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18177v1",
                "http://arxiv.org/pdf/2311.18177v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00078v1",
            "title": "Enhancing Cross-domain Click-Through Rate Prediction via Explicit\n  Feature Augmentation",
            "updated": "2023-11-30T01:42:11Z",
            "published": "2023-11-30T01:42:11Z",
            "summary": "Cross-domain CTR (CDCTR) prediction is an important research topic that\nstudies how to leverage meaningful data from a related domain to help CTR\nprediction in target domain. Most existing CDCTR works design implicit ways to\ntransfer knowledge across domains such as parameter-sharing that regularizes\nthe model training in target domain. More effectively, recent researchers\npropose explicit techniques to extract user interest knowledge and transfer\nthis knowledge to target domain. However, the proposed method mainly faces two\nissues: 1) it usually requires a super domain, i.e. an extremely large source\ndomain, to cover most users or items of target domain, and 2) the extracted\nuser interest knowledge is static no matter what the context is in target\ndomain. These limitations motivate us to develop a more flexible and efficient\ntechnique to explicitly transfer knowledge. In this work, we propose a\ncross-domain augmentation network (CDAnet) being able to perform explicit\nknowledge transfer between two domains. Specifically, CDAnet contains a\ndesigned translation network and an augmentation network which are trained\nsequentially. The translation network computes latent features from two domains\nand learns meaningful cross-domain knowledge of each input in target domain by\nusing a designed cross-supervised feature translator. Later the augmentation\nnetwork employs the explicit cross-domain knowledge as augmented information to\nboost the target domain CTR prediction. Through extensive experiments on two\npublic benchmarks and one industrial production dataset, we show CDAnet can\nlearn meaningful translated features and largely improve the performance of CTR\nprediction. CDAnet has been conducted online A/B test in image2product\nretrieval at Taobao app, bringing an absolute 0.11 point CTR improvement, a\nrelative 0.64% deal growth and a relative 1.26% GMV increase.",
            "author": [
                "Xu Chen",
                "Zida Cheng",
                "Jiangchao Yao",
                "Chen Ju",
                "Weilin Huang",
                "Jinsong Lan",
                "Xiaoyi Zeng",
                "Shuai Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00078v1",
                "http://arxiv.org/pdf/2312.00078v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18160v1",
            "title": "A Compilation Scheme for Suppressing Crosstalk and Decoherence in\n  Superconducting Quantum Chips with Tunable Coupling and Tunable Qubits",
            "updated": "2023-11-30T00:31:03Z",
            "published": "2023-11-30T00:31:03Z",
            "summary": "Quantum computing faces challenges such as crosstalk and decoherence, which\ndegrade the performance and reliability of quantum algorithms. Crosstalk is the\nunwanted interaction between qubits when they are operated in parallel, and\ndecoherence is the loss of quantum coherence due to the interaction with the\nenvironment. Both crosstalk and decoherence can reduce the fidelity of quantum\ngates, which are the basic operations in quantum computing.\n  In this paper, we propose an optimized qubit mapping and gate scheduling\nscheme for crosstalk mitigation and decoherence suppression in quantum\ncomputing. Our scheme consists of two steps: (1) selecting the qubit mapping\nscheme according to the crosstalk and decoherence noise, and (2) optimizing the\ngate timing using the maximum independent set problem in graph theory. Our\nscheme is based on a pulse compensation technique that can cut off the\ncrosstalk in both adiabatic gate systems and tunable coupler and tunable qubit\nsystems. We evaluate our scheme on tunnabl qubit tunnable coupler system and\nshow that it can significantly improve the fidelity of quantum algorithm\nexecution. Compared with the existing crosstalk-aware compilation schemes, our\nscheme can more effectively reduce the decoherence noise while suppressing\ncrosstalk. Moreover, our scheme has a polynomial algorithm complexity with\nrespect to the circuit size.",
            "author": [
                "Bin-han Lu",
                "Yu-chun Wu",
                "Peng Wang",
                "Guo-ping Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18160v1",
                "http://arxiv.org/pdf/2311.18160v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18149v1",
            "title": "STF: Spatial Temporal Fusion for Trajectory Prediction",
            "updated": "2023-11-29T23:31:40Z",
            "published": "2023-11-29T23:31:40Z",
            "summary": "Trajectory prediction is a challenging task that aims to predict the future\ntrajectory of vehicles or pedestrians over a short time horizon based on their\nhistorical positions. The main reason is that the trajectory is a kind of\ncomplex data, including spatial and temporal information, which is crucial for\naccurate prediction. Intuitively, the more information the model can capture,\nthe more precise the future trajectory can be predicted. However, previous\nworks based on deep learning methods processed spatial and temporal information\nseparately, leading to inadequate spatial information capture, which means they\nfailed to capture the complete spatial information. Therefore, it is of\nsignificance to capture information more fully and effectively on vehicle\ninteractions. In this study, we introduced an integrated 3D graph that\nincorporates both spatial and temporal edges. Based on this, we proposed the\nintegrated 3D graph, which considers the cross-time interaction information. In\nspecific, we design a Spatial-Temporal Fusion (STF) model including Multi-layer\nperceptions (MLP) and Graph Attention (GAT) to capture the spatial and temporal\ninformation historical trajectories simultaneously on the 3D graph. Our\nexperiment on the ApolloScape Trajectory Datasets shows that the proposed STF\noutperforms several baseline methods, especially on the long-time-horizon\ntrajectory prediction.",
            "author": [
                "Pengqian Han",
                "Partha Roop",
                "Jiamou Liu",
                "Tianzhe Bao",
                "Yifei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18149v1",
                "http://arxiv.org/pdf/2311.18149v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18143v1",
            "title": "Pretty good fractional revival via magnetic fields: theory and examples",
            "updated": "2023-11-29T23:10:28Z",
            "published": "2023-11-29T23:10:28Z",
            "summary": "We develop the theory of pretty good quantum fractional revival in arbitrary\nsized subsets of a graph, including the theory for fractional cospectrality of\nsubsets of arbitrary size. We use this theory to give conditions under which a\nmagnetic field can induce pretty good fractional revival, and give several\nexamples.",
            "author": [
                "Whitney Drazen",
                "Mark Kempton",
                "Gabor Lippner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18143v1",
                "http://arxiv.org/pdf/2311.18143v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "quant-ph",
                "05C50, 81P45"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18141v1",
            "title": "RDMA-Based Algorithms for Sparse Matrix Multiplication on GPUs",
            "updated": "2023-11-29T23:08:09Z",
            "published": "2023-11-29T23:08:09Z",
            "summary": "Sparse matrix multiplication is an important kernel for large-scale graph\nprocessing and other data-intensive applications. In this paper, we implement\nvarious asynchronous, RDMA-based sparse times dense (SpMM) and sparse times\nsparse (SpGEMM) algorithms, evaluating their performance running in a\ndistributed memory setting on GPUs. Our RDMA-based implementations use the\nNVSHMEM communication library for direct, asynchronous one-sided communication\nbetween GPUs. We compare our asynchronous implementations to state-of-the-art\nbulk synchronous GPU libraries as well as a CUDA-aware MPI implementation of\nthe SUMMA algorithm. We find that asynchronous RDMA-based implementations are\nable to offer favorable performance compared to bulk synchronous\nimplementations, while also allowing for the straightforward implementation of\nnovel work stealing algorithms.",
            "author": [
                "Benjamin Brock",
                "Ayd\u0131n Bulu\u00e7",
                "Katherine Yelick"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18141v1",
                "http://arxiv.org/pdf/2311.18141v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18138v1",
            "title": "Algorithmic Persuasion Through Simulation: Information Design in the Age\n  of Generative AI",
            "updated": "2023-11-29T23:01:33Z",
            "published": "2023-11-29T23:01:33Z",
            "summary": "How can an informed sender persuade a receiver, having only limited\ninformation about the receiver's beliefs? Motivated by research showing\ngenerative AI can simulate economic agents, we initiate the study of\ninformation design with an oracle. We assume the sender can learn more about\nthe receiver by querying this oracle, e.g., by simulating the receiver's\nbehavior. Aside from AI motivations such as general-purpose Large Language\nModels (LLMs) and problem-specific machine learning models, alternate\nmotivations include customer surveys and querying a small pool of live users.\n  Specifically, we study Bayesian Persuasion where the sender has a\nsecond-order prior over the receiver's beliefs. After a fixed number of queries\nto an oracle to refine this prior, the sender commits to an information\nstructure. Upon receiving the message, the receiver takes a payoff-relevant\naction maximizing her expected utility given her posterior beliefs. We design\npolynomial-time querying algorithms that optimize the sender's expected utility\nin this Bayesian Persuasion game. As a technical contribution, we show that\nqueries form partitions of the space of receiver beliefs that can be used to\nquantify the sender's knowledge.",
            "author": [
                "Keegan Harris",
                "Nicole Immorlica",
                "Brendan Lucier",
                "Aleksandrs Slivkins"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18138v1",
                "http://arxiv.org/pdf/2311.18138v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.AI",
                "econ.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00074v1",
            "title": "Inference using a composite-likelihood approximation for stochastic\n  metapopulation model of disease spread",
            "updated": "2023-11-29T22:52:26Z",
            "published": "2023-11-29T22:52:26Z",
            "summary": "Spatio-temporal pathogen spread is often partially observed at the\nmetapopulation scale. Available data correspond to proxies and are incomplete,\ncensored and heterogeneous. Moreover, representing such biological systems\noften leads to complex stochastic models. Such complexity together with data\ncharacteristics make the analysis of these systems a challenge. Our objective\nwas to develop a new inference procedure to estimate key parameters of\nstochastic metapopulation models of animal disease spread from longitudinal and\nspatial datasets, while accurately accounting for characteristics of census\ndata. We applied our procedure to provide new knowledge on the regional spread\nof \\emph{Mycobacterium avium} subsp. \\emph{paratuberculosis} (\\emph{Map}),\nwhich causes bovine paratuberculosis, a worldwide endemic disease. \\emph{Map}\nspread between herds through trade movements was modeled with a stochastic\nmechanistic model. Comprehensive data from 2005 to 2013 on cattle movements in\n12,857 dairy herds in Brittany (western France) and partial data on animal\ninfection status in 2,278 herds sampled from 2007 to 2013 were used. Inference\nwas performed using a new criterion based on a Monte-Carlo approximation of a\ncomposite likelihood, coupled to a numerical optimization algorithm\n(Nelder-Mead Simplex-like). Our criterion showed a clear superiority to\nalternative ones in identifying the right parameter values, as assessed by an\nempirical identifiability on simulated data. Point estimates and profile\nlikelihoods allowed us to establish the initial state of the system, identify\nthe risk of pathogen introduction from outside the metapopulation, and confirm\nthe assumption of the low sensitivity of the diagnostic test. Our inference\nprocedure could easily be applied to other spatio-temporal infection dynamics,\nespecially when ABC-like methods face challenges in defining relevant summary\nstatistics.",
            "author": [
                "Ga\u00ebl Beaun\u00e9e",
                "Pauline Ezanno",
                "Alain Joly",
                "Pierre Nicolas",
                "Elisabeta Vergu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00074v1",
                "http://arxiv.org/pdf/2312.00074v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "q-bio.QM",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18115v1",
            "title": "Understanding the Effects of Using Parsons Problems to Scaffold Code\n  Writing for Students with Varying CS Self-Efficacy Levels",
            "updated": "2023-11-29T22:02:46Z",
            "published": "2023-11-29T22:02:46Z",
            "summary": "Introductory programming courses aim to teach students to write code\nindependently. However, transitioning from studying worked examples to\ngenerating their own code is often difficult and frustrating for students,\nespecially those with lower CS self-efficacy in general. Therefore, we\ninvestigated the impact of using Parsons problems as a code-writing scaffold\nfor students with varying levels of CS self-efficacy. Parsons problems are\nprogramming tasks where students arrange mixed-up code blocks in the correct\norder. We conducted a between-subjects study with undergraduate students (N=89)\non a topic where students have limited code-writing expertise. Students were\nrandomly assigned to one of two conditions. Students in one condition practiced\nwriting code without any scaffolding, while students in the other condition\nwere provided with scaffolding in the form of an equivalent Parsons problem. We\nfound that, for students with low CS self-efficacy levels, those who received\nscaffolding achieved significantly higher practice performance and in-practice\nproblem-solving efficiency compared to those without any scaffolding.\nFurthermore, when given Parsons problems as scaffolding during practice,\nstudents with lower CS self-efficacy were more likely to solve them. In\naddition, students with higher pre-practice knowledge on the topic were more\nlikely to effectively use the Parsons scaffolding. This study provides evidence\nfor the benefits of using Parsons problems to scaffold students' write-code\nactivities. It also has implications for optimizing the Parsons scaffolding\nexperience for students, including providing personalized and adaptive Parsons\nproblems based on the student's current problem-solving status.",
            "author": [
                "Xinying Hou",
                "Barbara J. Ericson",
                "Xu Wang"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3631802.3631832",
                "http://arxiv.org/abs/2311.18115v1",
                "http://arxiv.org/pdf/2311.18115v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18090v1",
            "title": "Fleming-Viot helps speed up variational quantum algorithms in the\n  presence of barren plateaus",
            "updated": "2023-11-29T21:18:23Z",
            "published": "2023-11-29T21:18:23Z",
            "summary": "Inspired by the Fleming-Viot stochastic process, we propose a variant of\nVariational Quantum Algorithms benefitting from a parallel implementation of\nthe classical step of learning parameters of a given variational form, with the\naim of avoiding regions of the parameter space known as barren plateaus. In the\nFleming-Viot tradition, parallel searches are called particles. In our proposed\napproach, the search by a Fleming-Viot particle is stopped when it encounters a\nregion where the gradient is too small or noisy. The stopped particle continues\nthe search after being regenerated at another potentially more interesting\nlocation of the parameter space, biasing the exploration away from barren\nplateaus. We analyze the behavior of the Fleming-Viot particles from a\ntheoretical standpoint, backed up with numerical experiments on synthetic\nproblems as well as on selected instances of the Max-Cut problem on graphs,\nwhich show that our method performs better than plain-vanilla variants when\nthere are large barren plateaus.",
            "author": [
                "Daniel Mastropietro",
                "Georgios Korpas",
                "Vyacheslav Kungurtsev",
                "Jakub Marecek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18090v1",
                "http://arxiv.org/pdf/2311.18090v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00068v1",
            "title": "GLiDR: Topologically Regularized Graph Generative Network for Sparse\n  LiDAR Point Clouds",
            "updated": "2023-11-29T20:59:00Z",
            "published": "2023-11-29T20:59:00Z",
            "summary": "Sparse LiDAR point clouds cause severe loss of detail of static structures\nand reduce the density of static points available for navigation. Reduced\ndensity can be detrimental to navigation under several scenarios. We observe\nthat despite high sparsity, in most cases, the global topology of LiDAR\noutlining the static structures can be inferred. We utilize this property to\nobtain a backbone skeleton of a static LiDAR scan in the form of a single\nconnected component that is a proxy to its global topology. We utilize the\nbackbone to augment new points along static structures to overcome sparsity.\nNewly introduced points could correspond to existing static structures or to\nstatic points that were earlier obstructed by dynamic objects. To the best of\nour knowledge, we are the first to use this strategy for sparse LiDAR point\nclouds. Existing solutions close to our approach fail to identify and preserve\nthe global static LiDAR topology and generate sub-optimal points. We propose\nGLiDR, a Graph Generative network that is topologically regularized using\n0-dimensional Persistent Homology (PH) constraints. This enables GLiDR to\nintroduce newer static points along a topologically consistent global static\nLiDAR backbone. GLiDR generates precise static points using 32x sparser dynamic\nscans and performs better than the baselines across three datasets. The newly\nintroduced static points allow GLiDR to outperform LiDAR-based navigation using\nSLAM in several settings. GLiDR generates a valuable byproduct - an accurate\nbinary segmentation mask of static and dynamic objects that is helpful for\nnavigation planning and safety in constrained environments.",
            "author": [
                "Prashant Kumar",
                "Kshitij Madhav Bhat",
                "Vedang Bhupesh Shenvi Nadkarni",
                "Prem Kalra"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00068v1",
                "http://arxiv.org/pdf/2312.00068v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18076v1",
            "title": "A Nystr\u00f6m method with missing distances",
            "updated": "2023-11-29T20:43:49Z",
            "published": "2023-11-29T20:43:49Z",
            "summary": "We study the problem of determining the configuration of $n$ points, referred\nto as mobile nodes, by utilizing pairwise distances to $m$ fixed points known\nas anchor nodes. In the standard setting, we have information about the\ndistances between anchors (anchor-anchor) and between anchors and mobile nodes\n(anchor-mobile), but the distances between mobile nodes (mobile-mobile) are not\nknown. For this setup, the Nystr\\\"om method is a viable technique for\nestimating the positions of the mobile nodes. This study focuses on the setting\nwhere the anchor-mobile block of the distance matrix contains only partial\ndistance information. First, we establish a relationship between the columns of\nthe anchor-mobile block in the distance matrix and the columns of the\ncorresponding block in the Gram matrix via a graph Laplacian. Exploiting this\nconnection, we introduce a novel sampling model that frames the position\nestimation problem as low-rank recovery of an inner product matrix, given a\nsubset of its expansion coefficients in a special non-orthogonal basis. This\nbasis and its dual basis--the central elements of our model--are explicitly\nderived. Our analysis is grounded in a specific centering of the points that is\nunique to the Nystr\\\"om method. With this in mind, we extend previous work in\nEuclidean distance geometry by providing a general dual basis approach for\npoints centered anywhere.",
            "author": [
                "Samuel Lichtenberg",
                "Abiy Tasissa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18076v1",
                "http://arxiv.org/pdf/2311.18076v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.LG",
                "cs.RO",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18074v1",
            "title": "Game Projection and Robustness for Game-Theoretic Autonomous Driving",
            "updated": "2023-11-29T20:40:02Z",
            "published": "2023-11-29T20:40:02Z",
            "summary": "Game-theoretic approaches are envisioned to bring human-like reasoning skills\nand decision-making processes for autonomous vehicles (AVs). However,\nchallenges including game complexity and incomplete information still remain to\nbe addressed before they can be sufficiently practical for real-world use. Game\ncomplexity refers to the difficulties of solving a multi-player game, which\ninclude solution existence, algorithm convergence, and scalability. To address\nthese difficulties, a potential game based framework was developed in our\nrecent work. However, conditions on cost function design need to be enforced to\nmake the game a potential game. This paper relaxes the conditions and makes the\npotential game approach applicable to more general scenarios, even including\nthe ones that cannot be molded as a potential game. Incomplete information\nrefers to the ego vehicle's lack of knowledge of other traffic agents' cost\nfunctions. Cost function deviations between the ego vehicle estimated/learned\nother agents' cost functions and their actual ones are often inevitable. This\nmotivates us to study the robustness of a game-theoretic solution. This paper\ndefines the robustness margin of a game solution as the maximum magnitude of\ncost function deviations that can be accommodated in a game without changing\nthe optimality of the game solution. With this definition, closed-form\nrobustness margins are derived. Numerical studies using highway lane-changing\nscenarios are reported.",
            "author": [
                "Mushuang Liu",
                "H. Eric Tseng",
                "Dimitar Filev",
                "Anouck Girard",
                "Ilya Kolmanovsky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18074v1",
                "http://arxiv.org/pdf/2311.18074v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18057v1",
            "title": "Non Linear Software Documentation with Interactive Code Examples",
            "updated": "2023-11-29T20:08:46Z",
            "published": "2023-11-29T20:08:46Z",
            "summary": "Documentation enables sharing knowledge between the developers of a\ntechnology and its users. Creating quality documents, however, is challenging:\nDocuments must satisfy the needs of a large audience without being overwhelming\nfor individuals. We address this challenge with a new document format, named\nCasdoc. Casdoc documents are interactive resources centered around code\nexamples for programmers. Explanations of the code elements are presented as\nannotations that the readers reveal based on their needs. We evaluated Casdoc\nin a field study with over 300 participants who used 126 documents as part of a\nsoftware design course. The majority of participants adopted Casdoc instead of\na baseline format during the study. We observed that interactive documents can\ncontain more information than static documents without being distracting to\nreaders. We also gathered insights into five aspects of Casdoc that can be\napplied to other formats, and propose five guidelines to improve navigability\nin online documents.",
            "author": [
                "Mathieu Nassif",
                "Martin P. Robillard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18057v1",
                "http://arxiv.org/pdf/2311.18057v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00066v1",
            "title": "Exploring Factors Affecting Pedestrian Crash Severity Using TabNet: A\n  Deep Learning Approach",
            "updated": "2023-11-29T19:44:52Z",
            "published": "2023-11-29T19:44:52Z",
            "summary": "This study presents the first investigation of pedestrian crash severity\nusing the TabNet model, a novel tabular deep learning method exceptionally\nsuited for analyzing the tabular data inherent in transportation safety\nresearch. Through the application of TabNet to a comprehensive dataset from\nUtah covering the years 2010 to 2022, we uncover intricate factors contributing\nto pedestrian crash severity. The TabNet model, capitalizing on its\ncompatibility with structured data, demonstrates remarkable predictive\naccuracy, eclipsing that of traditional models. It identifies critical\nvariables, such as pedestrian age, involvement in left or right turns, lighting\nconditions, and alcohol consumption, which significantly influence crash\noutcomes. The utilization of SHapley Additive exPlanations (SHAP) enhances our\nability to interpret the TabNet model's predictions, ensuring transparency and\nunderstandability in our deep learning approach. The insights derived from our\nanalysis provide a valuable compass for transportation safety engineers and\npolicymakers, enabling the identification of pivotal factors that affect\npedestrian crash severity. Such knowledge is instrumental in formulating\nprecise, data-driven interventions aimed at bolstering pedestrian safety across\ndiverse urban and rural settings.",
            "author": [
                "Amir Rafe",
                "Patrick A. Singleton"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00066v1",
                "http://arxiv.org/pdf/2312.00066v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00065v2",
            "title": "Unsupervised Keypoints from Pretrained Diffusion Models",
            "updated": "2023-12-05T19:36:01Z",
            "published": "2023-11-29T19:43:38Z",
            "summary": "Unsupervised learning of keypoints and landmarks has seen significant\nprogress with the help of modern neural network architectures, but performance\nis yet to match the supervised counterpart, making their practicability\nquestionable. We leverage the emergent knowledge within text-to-image diffusion\nmodels, towards more robust unsupervised keypoints. Our core idea is to find\ntext embeddings that would cause the generative model to consistently attend to\ncompact regions in images (i.e. keypoints). To do so, we simply optimize the\ntext embedding such that the cross-attention maps within the denoising network\nare localized as Gaussians with small standard deviations. We validate our\nperformance on multiple datasets: the CelebA, CUB-200-2011, Tai-Chi-HD,\nDeepFashion, and Human3.6m datasets. We achieve significantly improved\naccuracy, sometimes even outperforming supervised ones, particularly for data\nthat is non-aligned and less curated. Our code is publicly available and can be\nfound through our project page: https://ubc-vision.github.io/StableKeypoints/",
            "author": [
                "Eric Hedlin",
                "Gopal Sharma",
                "Shweta Mahajan",
                "Xingzhe He",
                "Hossam Isack",
                "Abhishek Kar Helge Rhodin",
                "Andrea Tagliasacchi",
                "Kwang Moo Yi"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00065v2",
                "http://arxiv.org/pdf/2312.00065v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18044v1",
            "title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of\n  Promises and Challenges",
            "updated": "2023-11-29T19:40:10Z",
            "published": "2023-11-29T19:40:10Z",
            "summary": "Transfer learning is a conceptually-enticing paradigm in pursuit of truly\nintelligent embodied agents. The core concept -- reusing prior knowledge to\nlearn in and from novel situations -- is successfully leveraged by humans to\nhandle novel situations. In recent years, transfer learning has received\nrenewed interest from the community from different perspectives, including\nimitation learning, domain adaptation, and transfer of experience from\nsimulation to the real world, among others. In this paper, we unify the concept\nof transfer learning in robotics and provide the first taxonomy of its kind\nconsidering the key concepts of robot, task, and environment. Through a review\nof the promises and challenges in the field, we identify the need of\ntransferring at different abstraction levels, the need of quantifying the\ntransfer gap and the quality of transfer, as well as the dangers of negative\ntransfer. Via this position paper, we hope to channel the effort of the\ncommunity towards the most significant roadblocks to realize the full potential\nof transfer learning in robotics.",
            "author": [
                "No\u00e9mie Jaquier",
                "Michael C. Welle",
                "Andrej Gams",
                "Kunpeng Yao",
                "Bernardo Fichera",
                "Aude Billard",
                "Ale\u0161 Ude",
                "Tamim Asfour",
                "Danica Kragi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18044v1",
                "http://arxiv.org/pdf/2311.18044v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18030v1",
            "title": "On topological states and secular equations for quantum-graph\n  eigenvalues",
            "updated": "2023-11-29T19:13:54Z",
            "published": "2023-11-29T19:13:54Z",
            "summary": "Quantum graphs without interaction which contain equilateral cycles have\n``topological'' eigenstates that are supported on the cycles (by direct\nconstruction), but such states do not correspond to zeroes of one of the two\nvariants of the secular equation for quantum graphs. Here we completely\ncharacterize quantum graph states, even with interactions, and, using an\nIhara-style theorem, we elucidate the role of topological states in the\nspectral analysis of quantum graph Hamiltonians.\n  This work is dedicated to E.B. Davies on the occasion of his 80th birthday\nand in honor of his important contributions to the theory of quantum graphs,\ne.g., \\cite{DaExLi,Da13} and of his broad and influential work on spectral\ntheory, e.g., \\cite{Da89,Da95}.",
            "author": [
                "Evans M. Harrell II",
                "Anna V. Maltsev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18030v1",
                "http://arxiv.org/pdf/2311.18030v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.18027v1",
            "title": "Enhancing Data-Assimilation in CFD using Graph Neural Networks",
            "updated": "2023-11-29T19:11:40Z",
            "published": "2023-11-29T19:11:40Z",
            "summary": "We present a novel machine learning approach for data assimilation applied in\nfluid mechanics, based on adjoint-optimization augmented by Graph Neural\nNetworks (GNNs) models. We consider as baseline the Reynolds-Averaged\nNavier-Stokes (RANS) equations, where the unknown is the meanflow and a closure\nmodel based on the Reynolds-stress tensor is required for correctly computing\nthe solution. An end-to-end process is cast; first, we train a GNN model for\nthe closure term. Second, the GNN model is introduced in the training process\nof data assimilation, where the RANS equations act as a physics constraint for\na consistent prediction. We obtain our results using direct numerical\nsimulations based on a Finite Element Method (FEM) solver; a two-fold interface\nbetween the GNN model and the solver allows the GNN's predictions to be\nincorporated into post-processing steps of the FEM analysis. The proposed\nscheme provides an excellent reconstruction of the meanflow without any\nfeatures selection; preliminary results show promising generalization\nproperties over unseen flow configurations.",
            "author": [
                "Michele Quattromini",
                "Michele Alessandro Bucci",
                "Stefania Cherubini",
                "Onofrio Semeraro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.18027v1",
                "http://arxiv.org/pdf/2311.18027v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17987v1",
            "title": "Closing the dark photon window to thermal dark matter",
            "updated": "2023-11-29T19:00:00Z",
            "published": "2023-11-29T19:00:00Z",
            "summary": "The nature of dark matter remains a central question in particle physics,\ncosmology, and astrophysics. The prevailing hypothesis postulates that dark\nmatter consists of particles that interact only weakly with Standard Model\nparticles. However, the knowledge of dark matter properties beyond these\ninteractions is limited. This study explores a scenario involving a dark photon\nas a mediator between dark matter and the Standard Model, akin to the photon's\nrole in electromagnetism. Recent cosmological and experimental evidence impose\nconstraints on this scenario, focusing on results from direct detection\nexperiments such as PICO-60, XENON-1T, and PANDAX-4T. The results reveal severe\nconstraints, effectively closing the window for laboratory searches for dark\nphotons as mediators between the Standard Model and the dark sector (dark\nelectrons) in the secluded dark matter scenario. The findings underscore the\nneed for alternative explanations and offer fresh perspectives on the ongoing\nquest to understand dark matter and its interactions since they are nearly\nindependent of the dark electron fraction content for the total dark matter.\nThis analysis significantly narrows down the parameter space for thermal dark\nmatter scenarios involving a dark photon portal, reinforcing the urgency of\nexploring alternative models and designing new experiments to unravel the\nmysteries surrounding the nature of dark matter.",
            "author": [
                "Leon M. G. de la Vega",
                "R. Ferro-Hernandez",
                "A. Garc\u00eda-Viltres",
                "Eduardo Peinado",
                "E. V\u00e1zquez-J\u00e1uregui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17987v1",
                "http://arxiv.org/pdf/2311.17987v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17991v1",
            "title": "A model of quantum gravity on a noisy quantum computer",
            "updated": "2023-11-29T19:00:00Z",
            "published": "2023-11-29T19:00:00Z",
            "summary": "We study the Sachdev-Ye-Kitaev (SYK) model -- an important toy model for\nquantum gravity on IBM's superconducting qubit quantum computers. By using a\ngraph-coloring algorithm to minimize the number of commuting clusters of terms\nin the qubitized Hamiltonian, we find the circuit complexity of the time\nevolution using the first-order Lie product formula for $N$ Majorana fermions\nis $\\mathcal{O}(N^5 J^{2}t^2/\\epsilon)$ where $J$ is the dimensionful coupling\nparameter, $t$ is the evolution time, and $\\epsilon$ is the desired accuracy.\nThis complexity is a significant improvement over existing result in the\nliterature. With this improved resource requirement, we perform the time\nevolution for $N=6, 8$ using up to 300 two-qubit gates and perform different\nerror mitigation schemes on the noisy hardware results. We find good agreement\nwith the results obtained using exact diagonalization on classical computers\nand noiseless simulators. In particular, we compute the return probability to\nthe vacuum state after time $t$ and out-of-time order correlators (OTOC) which\nis a standard method of quantifying the chaotic nature of quantum many-body\nsystems.",
            "author": [
                "Muhammad Asaduzzaman",
                "Raghav G. Jha",
                "Bharath Sambasivam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17991v1",
                "http://arxiv.org/pdf/2311.17991v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "hep-lat",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17914v1",
            "title": "Reconstruction of electromagnetic showers in calorimeters using Deep\n  Learning",
            "updated": "2023-11-29T18:59:05Z",
            "published": "2023-11-29T18:59:05Z",
            "summary": "The precise reconstruction of properties of photons and electrons in modern\nhigh energy physics detectors, such as the CMS or Atlas experiments, plays a\ncrucial role in numerous physics results. Conventional geometrical algorithms\nare used to reconstruct the energy and position of these particles from the\nshowers they induce in the electromagnetic calorimeter. Despite their accuracy\nand efficiency, these methods still suffer from several limitations, such as\nlow-energy background and limited capacity to reconstruct close-by particles.\nThis paper introduces an innovative machine-learning technique to measure the\nenergy and position of photons and electrons based on convolutional and graph\nneural networks, taking the geometry of the CMS electromagnetic calorimeter as\nan example. The developed network demonstrates a significant improvement in\nresolution both for photon energy and position predictions compared to the\nalgorithm used in CMS. Notably, one of the main advantages of this new approach\nis its ability to better distinguish between multiple close-by electromagnetic\nshowers.",
            "author": [
                "Polina Simkina",
                "Fabrice Couderc",
                "Julie Malcl\u00e8s",
                "Mehmet \u00d6zg\u00fcr Sahin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17914v1",
                "http://arxiv.org/pdf/2311.17914v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17911v1",
            "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models\n  via Over-Trust Penalty and Retrospection-Allocation",
            "updated": "2023-11-29T18:57:07Z",
            "published": "2023-11-29T18:57:07Z",
            "summary": "Hallucination, posed as a pervasive challenge of multi-modal large language\nmodels (MLLMs), has significantly impeded their real-world usage that demands\nprecise judgment. Existing methods mitigate this issue with either training\nwith specific designed data or inferencing with external knowledge from other\nsources, incurring inevitable additional costs. In this paper, we present\nOPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a\nRetrospection-Allocation strategy, serving as a nearly free lunch to alleviate\nthe hallucination issue without additional data, knowledge, or training. Our\napproach begins with an interesting observation that, most hallucinations are\nclosely tied to the knowledge aggregation patterns manifested in the\nself-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a\nfew summary tokens, but not all the previous tokens. Such partial over-trust\ninclination results in the neglecting of image tokens and describes the image\ncontent with hallucination. Statistically, we observe an 80%$\\sim$95%\nco-currency rate between hallucination contents and such knowledge aggregation\npatterns. Based on the observation, OPERA introduces a penalty term on the\nmodel logits during the beam-search decoding to mitigate the over-trust issue,\nalong with a rollback strategy that retrospects the presence of summary tokens\nin the previously generated tokens, and re-allocate the token selection if\nnecessary. With extensive experiments, OPERA shows significant\nhallucination-mitigating performance on different MLLMs and metrics, proving\nits effectiveness and generality. Our code is available at:\nhttps://github.com/shikiw/OPERA.",
            "author": [
                "Qidong Huang",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Bin Wang",
                "Conghui He",
                "Jiaqi Wang",
                "Dahua Lin",
                "Weiming Zhang",
                "Nenghai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17911v1",
                "http://arxiv.org/pdf/2311.17911v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17901v1",
            "title": "SODA: Bottleneck Diffusion Models for Representation Learning",
            "updated": "2023-11-29T18:53:34Z",
            "published": "2023-11-29T18:53:34Z",
            "summary": "We introduce SODA, a self-supervised diffusion model, designed for\nrepresentation learning. The model incorporates an image encoder, which\ndistills a source view into a compact representation, that, in turn, guides the\ngeneration of related novel views. We show that by imposing a tight bottleneck\nbetween the encoder and a denoising decoder, and leveraging novel view\nsynthesis as a self-supervised objective, we can turn diffusion models into\nstrong representation learners, capable of capturing visual semantics in an\nunsupervised manner. To the best of our knowledge, SODA is the first diffusion\nmodel to succeed at ImageNet linear-probe classification, and, at the same\ntime, it accomplishes reconstruction, editing and synthesis tasks across a wide\nrange of datasets. Further investigation reveals the disentangled nature of its\nemergent latent space, that serves as an effective interface to control and\nmanipulate the model's produced images. All in all, we aim to shed light on the\nexciting and promising potential of diffusion models, not only for image\ngeneration, but also for learning rich and robust representations.",
            "author": [
                "Drew A. Hudson",
                "Daniel Zoran",
                "Mateusz Malinowski",
                "Andrew K. Lampinen",
                "Andrew Jaegle",
                "James L. McClelland",
                "Loic Matthey",
                "Felix Hill",
                "Alexander Lerchner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17901v1",
                "http://arxiv.org/pdf/2311.17901v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17898v2",
            "title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis",
            "updated": "2023-11-30T18:59:01Z",
            "published": "2023-11-29T18:51:46Z",
            "summary": "Hallucinations and unfaithful synthesis due to inaccurate prompts with\ninsufficient semantic details are widely observed in multimodal generative\nmodels. A prevalent strategy to align multiple modalities is to fine-tune the\ngenerator with a large number of annotated text-image pairs. However, such a\nprocedure is labor-consuming and resource-draining. The key question we ask is:\ncan we enhance the quality and faithfulness of text-driven generative models\nbeyond extensive text-image pair annotations? To address this question, we\npropose Knowledge Pursuit Prompting (KPP), a zero-shot framework that\niteratively incorporates external knowledge to help generators produce reliable\nvisual content. Instead of training generators to handle generic prompts, KPP\nemploys a recursive knowledge query process to gather informative external\nfacts from the knowledge base, instructs a language model to compress the\nacquired knowledge for prompt refinement, and utilizes text-driven generators\nfor visual synthesis. The entire process is zero-shot, without accessing the\narchitectures and parameters of generative models. We evaluate the framework\nacross multiple text-driven generative tasks (image, 3D rendering, and video)\non datasets of different domains. We further demonstrate the extensibility and\nadaptability of KPP through varying foundation model bases and instructions.\nOur results show that KPP is capable of generating faithful and semantically\nrich content across diverse visual domains, offering a promising solution to\nimprove multimodal generative models.",
            "author": [
                "Jinqi Luo",
                "Kwan Ho Ryan Chan",
                "Dimitris Dimos",
                "Ren\u00e9 Vidal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17898v2",
                "http://arxiv.org/pdf/2311.17898v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17891v1",
            "title": "Pose Anything: A Graph-Based Approach for Category-Agnostic Pose\n  Estimation",
            "updated": "2023-11-29T18:44:12Z",
            "published": "2023-11-29T18:44:12Z",
            "summary": "Traditional 2D pose estimation models are limited by their category-specific\ndesign, making them suitable only for predefined object categories. This\nrestriction becomes particularly challenging when dealing with novel objects\ndue to the lack of relevant training data.\n  To address this limitation, category-agnostic pose estimation (CAPE) was\nintroduced. CAPE aims to enable keypoint localization for arbitrary object\ncategories using a single model, requiring minimal support images with\nannotated keypoints. This approach not only enables object pose generation\nbased on arbitrary keypoint definitions but also significantly reduces the\nassociated costs, paving the way for versatile and adaptable pose estimation\napplications.\n  We present a novel approach to CAPE that leverages the inherent geometrical\nrelations between keypoints through a newly designed Graph Transformer Decoder.\nBy capturing and incorporating this crucial structural information, our method\nenhances the accuracy of keypoint localization, marking a significant departure\nfrom conventional CAPE techniques that treat keypoints as isolated entities.\n  We validate our approach on the MP-100 benchmark, a comprehensive dataset\ncomprising over 20,000 images spanning more than 100 categories. Our method\noutperforms the prior state-of-the-art by substantial margins, achieving\nremarkable improvements of 2.16% and 1.82% under 1-shot and 5-shot settings,\nrespectively. Furthermore, our method's end-to-end training demonstrates both\nscalability and efficiency compared to previous CAPE approaches.",
            "author": [
                "Or Hirschorn",
                "Shai Avidan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17891v1",
                "http://arxiv.org/pdf/2311.17891v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17867v3",
            "title": "A Class of Directed Acyclic Graphs with Mixed Data Types in Mediation\n  Analysis",
            "updated": "2023-12-04T16:08:42Z",
            "published": "2023-11-29T18:14:27Z",
            "summary": "We propose a unified class of generalized structural equation models (GSEMs)\nwith data of mixed types in mediation analysis, including continuous,\ncategorical, and count variables. Such models extend substantially the\nclassical linear structural equation model to accommodate many data types\narising from the application of mediation analysis. Invoking the hierarchical\nmodeling approach, we specify GSEMs by a copula joint distribution of outcome\nvariable, mediator and exposure variable, in which marginal distributions are\nbuilt upon generalized linear models (GLMs) with confounding factors. We\ndiscuss the identifiability conditions for the causal mediation effects in the\ncounterfactual paradigm as well as the issue of mediation leakage, and develop\nan asymptotically efficient profile maximum likelihood estimation and inference\nfor two key mediation estimands, natural direct effect and natural indirect\neffect, in different scenarios of mixed data types. The proposed new\nmethodology is illustrated by a motivating epidemiological study that aims to\ninvestigate whether the tempo of reaching infancy BMI peak (delay or on time),\nan important early life growth milestone, may mediate the association between\nprenatal exposure to phthalates and pubertal health outcomes.",
            "author": [
                "Wei Hao",
                "Canyi Chen",
                "Peter X. -K. Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17867v3",
                "http://arxiv.org/pdf/2311.17867v3"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17860v1",
            "title": "On the Verification of the Correctness of a Subgraph Construction\n  Algorithm",
            "updated": "2023-11-29T18:05:45Z",
            "published": "2023-11-29T18:05:45Z",
            "summary": "We automatically verify the crucial steps in the original proof of\ncorrectness of an algorithm which, given a geometric graph satisfying certain\nadditional properties removes edges in a systematic way for producing a\nconnected graph in which edges do not (geometrically) intersect. The challenge\nin this case is representing and reasoning about geometric properties of graphs\nin the Euclidean plane, about their vertices and edges, and about connectivity.\nFor modelling the geometric aspects, we use an axiomatization of plane\ngeometry; for representing the graph structure we use additional predicates;\nfor representing certain classes of paths in geometric graphs we use linked\nlists.",
            "author": [
                "Lucas B\u00f6ltz",
                "Viorica Sofronie-Stokkermans",
                "Hannes Frey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17860v1",
                "http://arxiv.org/pdf/2311.17860v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.IT",
                "cs.NI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17856v1",
            "title": "Leveraging Graph Diffusion Models for Network Refinement Tasks",
            "updated": "2023-11-29T18:02:29Z",
            "published": "2023-11-29T18:02:29Z",
            "summary": "Most real-world networks are noisy and incomplete samples from an unknown\ntarget distribution. Refining them by correcting corruptions or inferring\nunobserved regions typically improves downstream performance. Inspired by the\nimpressive generative capabilities that have been used to correct corruptions\nin images, and the similarities between \"in-painting\" and filling in missing\nnodes and edges conditioned on the observed graph, we propose a novel graph\ngenerative framework, SGDM, which is based on subgraph diffusion. Our framework\nnot only improves the scalability and fidelity of graph diffusion models, but\nalso leverages the reverse process to perform novel, conditional generation\ntasks. In particular, through extensive empirical analysis and a set of novel\nmetrics, we demonstrate that our proposed model effectively supports the\nfollowing refinement tasks for partially observable networks: T1: denoising\nextraneous subgraphs, T2: expanding existing subgraphs and T3: performing\n\"style\" transfer by regenerating a particular subgraph to match the\ncharacteristics of a different node or subgraph.",
            "author": [
                "Puja Trivedi",
                "Ryan Rossi",
                "David Arbour",
                "Tong Yu",
                "Franck Dernoncourt",
                "Sungchul Kim",
                "Nedim Lipka",
                "Namyong Park",
                "Nesreen K. Ahmed",
                "Danai Koutra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17856v1",
                "http://arxiv.org/pdf/2311.17856v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17853v2",
            "title": "On the Adversarial Robustness of Graph Contrastive Learning Methods",
            "updated": "2023-11-30T19:03:33Z",
            "published": "2023-11-29T17:59:18Z",
            "summary": "Contrastive learning (CL) has emerged as a powerful framework for learning\nrepresentations of images and text in a self-supervised manner while enhancing\nmodel robustness against adversarial attacks. More recently, researchers have\nextended the principles of contrastive learning to graph-structured data,\ngiving birth to the field of graph contrastive learning (GCL). However, whether\nGCL methods can deliver the same advantages in adversarial robustness as their\ncounterparts in the image and text domains remains an open question. In this\npaper, we introduce a comprehensive robustness evaluation protocol tailored to\nassess the robustness of GCL models. We subject these models to adaptive\nadversarial attacks targeting the graph structure, specifically in the evasion\nscenario. We evaluate node and graph classification tasks using diverse\nreal-world datasets and attack strategies. With our work, we aim to offer\ninsights into the robustness of GCL methods and hope to open avenues for\npotential future research directions.",
            "author": [
                "Filippo Guerranti",
                "Zinuo Yi",
                "Anna Starovoit",
                "Rafiq Kamel",
                "Simon Geisler",
                "Stephan G\u00fcnnemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17853v2",
                "http://arxiv.org/pdf/2311.17853v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17847v1",
            "title": "FastSample: Accelerating Distributed Graph Neural Network Training for\n  Billion-Scale Graphs",
            "updated": "2023-11-29T17:49:48Z",
            "published": "2023-11-29T17:49:48Z",
            "summary": "Training Graph Neural Networks(GNNs) on a large monolithic graph presents\nunique challenges as the graph cannot fit within a single machine and it cannot\nbe decomposed into smaller disconnected components. Distributed sampling-based\ntraining distributes the graph across multiple machines and trains the GNN on\nsmall parts of the graph that are randomly sampled every training iteration. We\nshow that in a distributed environment, the sampling overhead is a significant\ncomponent of the training time for large-scale graphs. We propose FastSample\nwhich is composed of two synergistic techniques that greatly reduce the\ndistributed sampling time: 1)a new graph partitioning method that eliminates\nmost of the communication rounds in distributed sampling , 2)a novel highly\noptimized sampling kernel that reduces memory movement during sampling. We test\nFastSample on large-scale graph benchmarks and show that FastSample speeds up\ndistributed sampling-based GNN training by up to 2x with no loss in accuracy.",
            "author": [
                "Hesham Mostafa",
                "Adam Grabowski",
                "Md Asadullah Turja",
                "Juan Cervino",
                "Alejandro Ribeiro",
                "Nageen Himayat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17847v1",
                "http://arxiv.org/pdf/2311.17847v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17842v1",
            "title": "Look Before You Leap: Unveiling the Power of GPT-4V in Robotic\n  Vision-Language Planning",
            "updated": "2023-11-29T17:46:25Z",
            "published": "2023-11-29T17:46:25Z",
            "summary": "In this study, we are interested in imbuing robots with the capability of\nphysically-grounded task planning. Recent advancements have shown that large\nlanguage models (LLMs) possess extensive knowledge useful in robotic tasks,\nespecially in reasoning and planning. However, LLMs are constrained by their\nlack of world grounding and dependence on external affordance models to\nperceive environmental information, which cannot jointly reason with LLMs. We\nargue that a task planner should be an inherently grounded, unified multimodal\nsystem. To this end, we introduce Robotic Vision-Language Planning (ViLa), a\nnovel approach for long-horizon robotic planning that leverages vision-language\nmodels (VLMs) to generate a sequence of actionable steps. ViLa directly\nintegrates perceptual data into its reasoning and planning process, enabling a\nprofound understanding of commonsense knowledge in the visual world, including\nspatial layouts and object attributes. It also supports flexible multimodal\ngoal specification and naturally incorporates visual feedback. Our extensive\nevaluation, conducted in both real-robot and simulated environments,\ndemonstrates ViLa's superiority over existing LLM-based planners, highlighting\nits effectiveness in a wide array of open-world manipulation tasks.",
            "author": [
                "Yingdong Hu",
                "Fanqi Lin",
                "Tong Zhang",
                "Li Yi",
                "Yang Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17842v1",
                "http://arxiv.org/pdf/2311.17842v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17841v1",
            "title": "Fast list-decoding of univariate multiplicity and folded Reed-Solomon\n  codes",
            "updated": "2023-11-29T17:46:17Z",
            "published": "2023-11-29T17:46:17Z",
            "summary": "We show that the known list-decoding algorithms for univariate multiplicity\nand folded Reed-Solomon (FRS) codes can be made to run in nearly-linear time.\nThis yields, to our knowledge, the first known family of codes that can be\ndecoded in nearly linear time, even as they approach the list decoding\ncapacity. Univariate multiplicity codes and FRS codes are natural variants of\nReed-Solomon codes that were discovered and studied for their applications to\nlist-decoding. It is known that for every $\\epsilon >0$, and rate $R \\in\n(0,1)$, there exist explicit families of these codes that have rate $R$ and can\nbe list-decoded from a $(1-R-\\epsilon)$ fraction of errors with constant list\nsize in polynomial time (Guruswami & Wang (IEEE Trans. Inform. Theory, 2013)\nand Kopparty, Ron-Zewi, Saraf & Wootters (SIAM J. Comput. 2023)). In this work,\nwe present randomized algorithms that perform the above tasks in nearly linear\ntime. Our algorithms have two main components. The first builds upon the\nlattice-based approach of Alekhnovich (IEEE Trans. Inf. Theory 2005), who\ndesigned a nearly linear time list-decoding algorithm for Reed-Solomon codes\napproaching the Johnson radius. As part of the second component, we design\nnearly-linear time algorithms for two natural algebraic problems. The first\nalgorithm solves linear differential equations of the form $Q\\left(x, f(x),\n\\frac{df}{dx}, \\dots,\\frac{d^m f}{dx^m}\\right) \\equiv 0$ where $Q$ has the form\n$Q(x,y_0,\\dots,y_m) = \\tilde{Q}(x) + \\sum_{i = 0}^m Q_i(x)\\cdot y_i$. The\nsecond solves functional equations of the form $Q\\left(x, f(x), f(\\gamma x),\n\\dots,f(\\gamma^m x)\\right) \\equiv 0$ where $\\gamma$ is a high-order field\nelement. These algorithms can be viewed as generalizations of classical\nalgorithms of Sieveking (Computing 1972) and Kung (Numer. Math. 1974) for\ncomputing the modular inverse of a power series, and might be of independent\ninterest.",
            "author": [
                "Rohan Goyal",
                "Prahladh Harsha",
                "Mrinal Kumar",
                "Ashutosh Shankar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17841v1",
                "http://arxiv.org/pdf/2311.17841v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.CC",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17812v2",
            "title": "DAP: Domain-aware Prompt Learning for Vision-and-Language Navigation",
            "updated": "2023-11-30T11:28:59Z",
            "published": "2023-11-29T17:03:37Z",
            "summary": "Following language instructions to navigate in unseen environments is a\nchallenging task for autonomous embodied agents. With strong representation\ncapabilities, pretrained vision-and-language models are widely used in VLN.\nHowever, most of them are trained on web-crawled general-purpose datasets,\nwhich incurs a considerable domain gap when used for VLN tasks. To address the\nproblem, we propose a novel and model-agnostic domain-aware prompt learning\n(DAP) framework. For equipping the pretrained models with specific object-level\nand scene-level cross-modal alignment in VLN tasks, DAP applies a low-cost\nprompt tuning paradigm to learn soft visual prompts for extracting in-domain\nimage semantics. Specifically, we first generate a set of in-domain image-text\npairs with the help of the CLIP model. Then we introduce soft visual prompts in\nthe input space of the visual encoder in a pretrained model. DAP injects\nin-domain visual knowledge into the visual encoder of the pretrained model in\nan efficient way. Experimental results on both R2R and REVERIE show the\nsuperiority of DAP compared to existing state-of-the-art methods.",
            "author": [
                "Ting Liu",
                "Yue Hu",
                "Wansen Wu",
                "Youkai Wang",
                "Kai Xu",
                "Quanjun Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17812v2",
                "http://arxiv.org/pdf/2311.17812v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17809v2",
            "title": "Zeta Functions for Spherical Tits Buildings of Finite General Linear\n  Groups",
            "updated": "2023-11-30T07:01:54Z",
            "published": "2023-11-29T16:57:51Z",
            "summary": "The zeta function of a graph captures essential connectivity properties and\nhas found diverse applications. We generalize this concept by defining edge\nzeta functions for spherical buildings associated with finite general linear\ngroups. To analyze these zeta functions, we undertake a journey through\ncombinatorics and representation theory. By introducing and applying insightful\ntools including digraphs X_0 and X_2, cyclic n-partite graphs,\npartite-transitive group actions, and Springer's theorem on Hecke algebras, we\nderive elegant formulas for the zeta functions of buildings of general linear\ngroups. The eigenvalues are revealed to be roots of unity times powers of q.\nThis demonstrates the power of combining algebraic, graph-theoretic and\nrepresentation-theoretic perspectives when exploring rich mathematical\nstructures like buildings.",
            "author": [
                "Jianhao Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17809v2",
                "http://arxiv.org/pdf/2311.17809v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17801v1",
            "title": "Towards Efficient Hyperdimensional Computing Using Photonics",
            "updated": "2023-11-29T16:51:21Z",
            "published": "2023-11-29T16:51:21Z",
            "summary": "Over the past few years, silicon photonics-based computing has emerged as a\npromising alternative to CMOS-based computing for Deep Neural Networks (DNN).\nUnfortunately, the non-linear operations and the high-precision requirements of\nDNNs make it extremely challenging to design efficient silicon photonics-based\nsystems for DNN inference and training. Hyperdimensional Computing (HDC) is an\nemerging, brain-inspired machine learning technique that enjoys several\nadvantages over existing DNNs, including being lightweight, requiring\nlow-precision operands, and being robust to noise introduced by the\nnonidealities in the hardware. For HDC, computing in-memory (CiM) approaches\nhave been widely used, as CiM reduces the data transfer cost if the operands\ncan fit into the memory. However, inefficient multi-bit operations, high write\nlatency, and low endurance make CiM ill-suited for HDC. On the other hand, the\nexisting electro-photonic DNN accelerators are inefficient for HDC because they\nare specifically optimized for matrix multiplication in DNNs and consume a lot\nof power with high-precision data converters.\n  In this paper, we argue that photonic computing and HDC complement each other\nbetter than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,\nthe first-ever electro-photonic accelerator for HDC training and inference,\nsupporting the basic, record-based, and graph encoding schemes. Evaluating with\npopular datasets, we show that our accelerator can achieve two to five orders\nof magnitude lower EDP than the state-of-the-art electro-photonic DNN\naccelerators for implementing HDC training and inference. PhotoHDC also\nachieves four orders of magnitude lower energy-delay product than CiM-based\naccelerators for both HDC training and inference.",
            "author": [
                "Farbin Fayza",
                "Cansu Demirkiran",
                "Hanning Chen",
                "Che-Kai Liu",
                "Avi Mohan",
                "Hamza Errahmouni",
                "Sanggeon Yun",
                "Mohsen Imani",
                "David Zhang",
                "Darius Bunandar",
                "Ajay Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17801v1",
                "http://arxiv.org/pdf/2311.17801v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET",
                "cs.AR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17797v1",
            "title": "Learning to Simulate: Generative Metamodeling via Quantile Regression",
            "updated": "2023-11-29T16:46:24Z",
            "published": "2023-11-29T16:46:24Z",
            "summary": "Stochastic simulation models, while effective in capturing the dynamics of\ncomplex systems, are often too slow to run for real-time decision-making.\nMetamodeling techniques are widely used to learn the relationship between a\nsummary statistic of the outputs (e.g., the mean or quantile) and the inputs of\nthe simulator, so that it can be used in real time. However, this methodology\nrequires the knowledge of an appropriate summary statistic in advance, making\nit inflexible for many practical situations. In this paper, we propose a new\nmetamodeling concept, called generative metamodeling, which aims to construct a\n\"fast simulator of the simulator\". This technique can generate random outputs\nsubstantially faster than the original simulation model, while retaining an\napproximately equal conditional distribution given the same inputs. Once\nconstructed, a generative metamodel can instantaneously generate a large amount\nof random outputs as soon as the inputs are specified, thereby facilitating the\nimmediate computation of any summary statistic for real-time decision-making.\nFurthermore, we propose a new algorithm -- quantile-regression-based generative\nmetamodeling (QRGMM) -- and study its convergence and rate of convergence.\nExtensive numerical experiments are conducted to investigate the empirical\nperformance of QRGMM, compare it with other state-of-the-art generative\nalgorithms, and demonstrate its usefulness in practical real-time\ndecision-making.",
            "author": [
                "L. Jeff Hong",
                "Yanxi Hou",
                "Qingkai Zhang",
                "Xiaowei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17797v1",
                "http://arxiv.org/pdf/2311.17797v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17793v1",
            "title": "Vines and MAT-labeled graphs",
            "updated": "2023-11-29T16:36:45Z",
            "published": "2023-11-29T16:36:45Z",
            "summary": "The present paper explores a connection between two concepts arising from\ndifferent fields of mathematics. The first concept, called vine, is a graphical\nmodel for dependent random variables. This concept first appeared in a work of\nJoe (1994), and the formal definition was given later by Cooke (1997). Vines\nhave nowadays become an active research area whose applications can be found in\nprobability theory and uncertainty analysis. The second concept, called\nMAT-freeness, is a combinatorial property in the theory of freeness of\nlogarithmic derivation module of hyperplane arrangements. This concept was\nfirst studied by Abe-Barakat-Cuntz-Hoge-Terao (2016), and soon afterwards\ninvestigated further by Cuntz-M\\\"ucksch (2020).\n  In the particular case of graphic arrangements, the last two authors (2023)\nrecently proved that the MAT-freeness is completely characterized by the\nexistence of certain edge-labeled graphs, called MAT-labeled graphs. In this\npaper, we first introduce a poset characterization of a vine, the so-called\nvineposet. Then we show that, interestingly, there exists an explicit\nequivalence between the categories of locally regular vineposets and\nMAT-labeled graphs. In particular, we obtain an equivalence between the\ncategories of regular vineposets and MAT-labeled complete graphs.\n  Several applications will be mentioned to illustrate the interaction between\nthe two concepts. Notably, we give an affirmative answer to a question of\nCuntz-M\\\"ucksch that MAT-freeness can be characterized by a generalization of\nthe root poset in the case of graphic arrangements.",
            "author": [
                "Hung Manh Tran",
                "Tan Nhat Tran",
                "Shuhei Tsujie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17793v1",
                "http://arxiv.org/pdf/2311.17793v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "Primary 06A07, Secondary 52C35"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17788v1",
            "title": "Design of a millifluidic device for the evaluation of blue-light\n  disinfection on Pseudomonas aeruginosa PA01 biofilm",
            "updated": "2023-11-29T16:34:32Z",
            "published": "2023-11-29T16:34:32Z",
            "summary": "Significance: For the first time to our knowledge, a millifluidic device has\nbeen designed to observe the effects of blue-light irradiation in Pseudomonas\naeruginosa PA01 bacterial biofilm. Approach: A bacterial biofilm is formed\nunder controlled flow of nutrients in microfabricated channels settled on a\nmicroscope stage. The setup is devised to apply defined irradiation and\nevaluate the photo-killing efficiency in situ using video-microscopy and\nfluorescent probes. Results: We investigated bacterial survival after\ndelivering a precise, controlled and spatially defined blue light dose upon\ngrowing biofilm. We demonstrate in this report that the combined use of\nconstitutive GFP and propidium iodide in a millifluidic setup enables to\nevidence light-induced mortality at the single cell and community level.\nConclusions: Real-time monitoring of a bacterial biofilm growing in a\nmillifluidic device after irradiation opens an avenue for a better\nunderstanding of the biofilm community response to blue-light, permitting\nresearchers to approach photokilling efficiency of biofilm as a dynamic\nprocess.",
            "author": [
                "Nidia Maldonado Carmona",
                "Murielle Balthazar",
                "Joanne L Fothergill",
                "Nelly Henry"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17788v1",
                "http://arxiv.org/pdf/2311.17788v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00060v1",
            "title": "Open data ecosystems: what models to co-create service innovations in\n  smart cities?",
            "updated": "2023-11-29T16:26:39Z",
            "published": "2023-11-29T16:26:39Z",
            "summary": "While smart cities are recently providing open data, how to organise the\ncollective creation of data, knowledge and related products and services\nproduced from this collective resource, still remains to be thought. This paper\naims at gathering the literature review on open data ecosystems to tackle the\nfollowing research question: what models can be imagined to stimulate the\ncollective co-creation of services between smart cities' stakeholders acting as\nproviders and users of open data? Such issue is currently at stake in many\nmunicipalities such as Lisbon which decided to position itself as a platform\n(O'Reilly, 2010) in the local digital ecosystem. With the implementation of its\nCity Operation Center (COI), Lisbon's municipality provides an Information\nInfrastructure (Bowker et al., 2009) to many different types of actors such as\ntelecom companies, municipalities, energy utilities or transport companies.\nThrough this infrastructure, Lisbon encourages such actors to gather, integrate\nand release heterogeneous datasets and tries to orchestrate synergies among\nthem so data-driven solution to urban problems can emerge (Carvalho and Vale,\n2018). The remaining question being: what models for the municipalities such as\nLisbon to lean on so as to drive this cutting-edge type of service innovation?",
            "author": [
                "Arthur Sarazin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00060v1",
                "http://arxiv.org/pdf/2312.00060v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17781v1",
            "title": "Propagate & Distill: Towards Effective Graph Learners Using\n  Propagation-Embracing MLPs",
            "updated": "2023-11-29T16:26:24Z",
            "published": "2023-11-29T16:26:24Z",
            "summary": "Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve\nsemisupervised node classification on graphs, by training a student MLP by\nknowledge distillation from a teacher graph neural network (GNN). While\nprevious studies have focused mostly on training the student MLP by matching\nthe output probability distributions between the teacher and student models\nduring distillation, it has not been systematically studied how to inject the\nstructural information in an explicit and interpretable manner. Inspired by\nGNNs that separate feature transformation $T$ and propagation $\\Pi$, we\nre-frame the distillation process as making the student MLP learn both $T$ and\n$\\Pi$. Although this can be achieved by applying the inverse propagation\n$\\Pi^{-1}$ before distillation from the teacher, it still comes with a high\ncomputational cost from large matrix multiplications during training. To solve\nthis problem, we propose Propagate & Distill (P&D), which propagates the output\nof the teacher before distillation, which can be interpreted as an approximate\nprocess of the inverse propagation. We demonstrate that P&D can readily improve\nthe performance of the student MLP.",
            "author": [
                "Yong-Min Shin",
                "Won-Yong Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17781v1",
                "http://arxiv.org/pdf/2311.17781v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "cs.NE",
                "cs.SI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17762v1",
            "title": "Exchange graphs and Ext-quivers of hearts of tube categories",
            "updated": "2023-11-29T16:04:01Z",
            "published": "2023-11-29T16:04:01Z",
            "summary": "In this paper we introduce the notion of pre-simple-minded collection\n(pre-SMC) of type $\\mathbb{A}$ in the bounded derived categories\n$\\mathcal{D}^{b} (${\\rm{\\textbf{T}}}$_p)$ of tube categories $\\textbf{T}_{p}$\nof rank $p$. This provides an effective approach to classify the hearts in\n$\\mathcal{D}^{b} (${\\rm{\\textbf{T}}}$_p)$. We then use this classification to\nprove the exchange graph of hearts in $\\mathcal{D}^{b} (${\\rm{\\textbf{T}}}$_p)$\nis connnected. Further, we classify the Ext-quivers of hearts in\n$\\mathcal{D}^{b} (${\\rm{\\textbf{T}}}$_p)$. As an application, we show that the\nspace of Bridgeland stability conditions on\n$\\mathcal{D}^{b}(${\\rm{\\textbf{T}}}$_p)$ is connected.",
            "author": [
                "Mingfa Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17762v1",
                "http://arxiv.org/pdf/2311.17762v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17755v1",
            "title": "Experimental and Theoretical Brownian Dynamics Analysis of Ion Transport\n  During Cellular Electroporation of E. coli Bacteria",
            "updated": "2023-11-29T15:57:32Z",
            "published": "2023-11-29T15:57:32Z",
            "summary": "Escherichia coli bacterium is a rod-shaped organism composed of a complex\ndouble membrane structure. Knowledge of electric field driven ion transport\nthrough both membranes and the evolution of their induced permeabilization has\nimportant applications in biomedical engineering, delivery of genes and\nantibacterial agents. However, few studies have been conducted on Gram-negative\nbacteria in this regard considering the contribution of all ion types. To\naddress this gap in knowledge, we have developed a deterministic and stochastic\nBrownian dynamics model to simulate in 3D space the motion of ions through\npores formed in the plasma membranes of E. coli cells during electroporation.\nThe diffusion coefficient, mobility, and translation time of Ca$^{2+}$,\nMg$^{2+}$, Na$^+$, K$^+$, and Cl$^-$ ions within the pore region are estimated\nfrom the numerical model. Calculations of pore's conductance have been\nvalidated with experiments conducted at Gustave Roussy. From the simulations,\nit was found that the main driving force of ionic uptake during the pulse is\nthe one due to the externally applied electric field. The results from this\nwork provide a better understanding of ion transport during electroporation,\naiding in the design of electrical pulses for maximizing ion throughput,\nprimarily for application in cancer treatment.",
            "author": [
                "Juan Gonz\u00e1lez-Cuevas",
                "Ricardo Arg\u00fcello",
                "Marcos Florentin",
                "Franck M. Andr\u00e9",
                "Lluis Mir"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s10439-023-03353-4",
                "http://arxiv.org/abs/2311.17755v1",
                "http://arxiv.org/pdf/2311.17755v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17751v1",
            "title": "On Sum Graphs over Some Magmas",
            "updated": "2023-11-29T15:54:41Z",
            "published": "2023-11-29T15:54:41Z",
            "summary": "We consider the notion of a sum graph and relaxed sum graph over a magma,\ngive several examples and results of sum graphs over some natural magmas. We\nclassify the cycles that are sum graphs for the magma of the subsets of a set\nwith the operation of union, determine the abelian groups that provide a sum\nlabelling of $C_4$, and show that $C_{4\\ell}$ is a sum graph over the abelian\ngroup $\\mathbb{Z}_f\\times\\mathbb{Z}_f$, where $f=f_{2\\ell}$ is the\ncorresponding Fibonacci number. For integral sum graphs, we give a linear upper\nbound for the radius of matchings, improving Harary's labelling for this family\nof graphs, and give the exact radius for the family of totally disconnected\ngraphs. Using the Z3 SMT-system, we found integer labellings for the 4D-cube,\ngiving a negative answering to a question of Melnikov and Pyatikin, having also\nobtained some results on mod sum graphs and relaxed sum graphs. Finally, we\nshow that the direct product operation is closed for strong integral sum\ngraphs.",
            "author": [
                "Ant\u00f3nio Machiavelo",
                "Rog\u00e9rio Reis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17751v1",
                "http://arxiv.org/pdf/2311.17751v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17742v1",
            "title": "Robust Localization and Tracking of UAVs in OTFS-based Networks",
            "updated": "2023-11-29T15:48:45Z",
            "published": "2023-11-29T15:48:45Z",
            "summary": "We consider the problem of accurately localizing N unmanned aerial vehicles\n(UAV) in 3D space where the UAVs are part of a swarm and communicate with each\nother through orthogonal time-frequency space (OTFS) modulated signals. Each\nreceiving UAV estimates the multipath wireless channel on each link formed by\nthe line-of-sight (LoS) transmission and by the single reflections from the\nremaining N-2 UAVs. The estimated power delay profiles are communicated to an\nedge server, which is in charge of computing the exact location and speed of\nthe UAVs. To obtain the UAVs locations and velocities, we propose an iterative\nalgorithm, named Turbo Iterative Positioning (TIP), which, using a\nbelief-propagation approach, effectively exploits the time difference of\narrival (TDoA) measurements between the LoS and the non-LoS paths. Enabling a\nfull cold start (no prior knowledge), our solution first maps each TDoA's\nprofile element to a specific ID of the reflecting UAV's. The Doppler shifts\nmeasured by the OTFS receivers associated with each path are also used to\nestimate the UAV's velocities. The localization of the N UAVs is then derived\nvia gradient descent optimization, with the aid of turbo-like iterations that\ncan progressively correct some of the residual errors in the initial ID mapping\noperation. Our numerical results, obtained also using real-world traces, show\nhow the multipath links are beneficial to achieving very accurate localization\nand speed of all UAVs, even with a limited delay-Doppler resolution. Robustness\nof our scheme is proven by its performance approaching the Cramer-Rao bound.",
            "author": [
                "Alessandro Nordio",
                "Carla Fabiana Chiasserini",
                "Emanuele Viterbo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17742v1",
                "http://arxiv.org/pdf/2311.17742v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17728v1",
            "title": "Know your audience",
            "updated": "2023-11-29T15:34:55Z",
            "published": "2023-11-29T15:34:55Z",
            "summary": "Distributed function computation is the problem, for a networked system of\n$n$ autonomous agents, to collectively compute the value $f(v_1, \\ldots, v_n)$\nof some input values, each initially private to one agent in the network. Here,\nwe study and organize results pertaining to distributed function computation in\nanonymous networks, both for the static and the dynamic case, under a\ncommunication model of directed and synchronous message exchanges, but with\nvarying assumptions in the degree of awareness or control that a single agent\nhas over its outneighbors.\n  Our main argument is three-fold. First, in the \"blind broadcast\" model, where\nin each round an agent merely casts out a unique message without any knowledge\nor control over its addressees, the computable functions are those that only\ndepend on the set of the input values, but not on their multiplicities or\nrelative frequencies in the input. Second, in contrast, when we assume either\nthat a) in each round, the agents know how many outneighbors they have; b) all\ncommunications links in the network are bidirectional; or c) the agents may\naddress each of their outneighbors individually, then the set of computable\nfunctions grows to contain all functions that depend on the relative\nfrequencies of each value in the input - such as the average - but not on their\nmultiplicities - thus, not the sum. Third, however, if one or several agents\nare distinguished as leaders, or if the cardinality of the network is known,\nthen under any of the above three assumptions it becomes possible to recover\nthe complete multiset of the input values, and thus compute any function of the\ndistributed input as long as it is invariant under permutation of its\narguments. In the case of dynamic networks, we also discuss the impact of\nmultiple connectivity assumptions.",
            "author": [
                "Bernadette Charron-Bost",
                "Patrick Lambein-Monette"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17728v1",
                "http://arxiv.org/pdf/2311.17728v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17969v1",
            "title": "Generation of a Compendium of Transcription Factor Cascades and\n  Identification of Potential Therapeutic Targets using Graph Machine Learning",
            "updated": "2023-11-29T15:31:58Z",
            "published": "2023-11-29T15:31:58Z",
            "summary": "Transcription factors (TFs) play a vital role in the regulation of gene\nexpression thereby making them critical to many cellular processes. In this\nstudy, we used graph machine learning methods to create a compendium of TF\ncascades using data extracted from the STRING database. A TF cascade is a\nsequence of TFs that regulate each other, forming a directed path in the TF\nnetwork. We constructed a knowledge graph of 81,488 unique TF cascades, with\nthe longest cascade consisting of 62 TFs. Our results highlight the complex and\nintricate nature of TF interactions, where multiple TFs work together to\nregulate gene expression. We also identified 10 TFs with the highest regulatory\ninfluence based on centrality measurements, providing valuable information for\nresearchers interested in studying specific TFs. Furthermore, our pathway\nenrichment analysis revealed significant enrichment of various pathways and\nfunctional categories, including those involved in cancer and other diseases,\nas well as those involved in development, differentiation, and cell signaling.\nThe enriched pathways identified in this study may have potential as targets\nfor therapeutic intervention in diseases associated with dysregulation of\ntranscription factors. We have released the dataset, knowledge graph, and\ngraphML methods for the TF cascades, and created a website to display the\nresults, which can be accessed by researchers interested in using this dataset.\nOur study provides a valuable resource for understanding the complex network of\ninteractions between TFs and their regulatory roles in cellular processes.",
            "author": [
                "Sonish Sivarajkumar",
                "Pratyush Tandale",
                "Ankit Bhardwaj",
                "Kipp W. Johnson",
                "Anoop Titus",
                "Benjamin S. Glicksberg",
                "Shameer Khader",
                "Kamlesh K. Yadav",
                "Lakshminarayanan Subramanian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17969v1",
                "http://arxiv.org/pdf/2311.17969v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17723v2",
            "title": "First results of evaporation residue cross-section measurements of\n  $^{32}$S+$^{208}$Pb system",
            "updated": "2023-11-30T04:12:56Z",
            "published": "2023-11-29T15:28:01Z",
            "summary": "The dynamics of heavy ion-induced reactions play a critical role in forming\nsuper heavy elements (SHE), and one clear signature of the SHE formation is the\nevaporation residue (ER). In our pursuit of SHE, we present the heaviest\nelement populated in India for ER cross-section measurements. These are the\nfirst-ever measurements of the Evaporation Residue (ER) cross-sections for the\nnuclear reactions between $^{32}$S and $^{208}$Pb. These measurements were\nconducted above the Coulomb barrier at four distinct beam energies in the\nlaboratory frame, ranging from 176 to 191 MeV at the pelletron Linac facility\nat the Inter-University Accelerator Centre (IUAC), New Delhi. The Hybrid Recoil\nMass Analyzer (HYRA) in a gas-filled mode was employed for these experiments.\nThe obtained range of ER cross-sections enriches our knowledge and helps\nadvance the field of heavy ion-induced reactions, especially in the context of\nsuper heavy element formation.",
            "author": [
                "R. Sariyal",
                "I. Mazumdar",
                "D. Mehta",
                "N. Madhavan",
                "S. Nath",
                "J. Gehlot",
                "Gonika",
                "S. M. Patel",
                "P. B. Chavan",
                "S. Panwar",
                "V. Ranga",
                "A. Parihari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17723v2",
                "http://arxiv.org/pdf/2311.17723v2"
            ],
            "primary_category": "nucl-ex",
            "category": [
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17705v1",
            "title": "Q-PAC: Automated Detection of Quantum Bug-Fix Patterns",
            "updated": "2023-11-29T15:09:32Z",
            "published": "2023-11-29T15:09:32Z",
            "summary": "Context: Bug-fix pattern detection has been investigated in the past in the\ncontext of classical software. However, while quantum software is developing\nrapidly, the literature still lacks automated methods and tools to identify,\nanalyze, and detect bug-fix patterns. To the best of our knowledge, our work\npreviously published in SEKE'23 was the first to leverage classical techniques\nto detect bug-fix patterns in quantum code.\n  Objective: To extend our previous effort, we present a research agenda\n(Q-Repair), including a series of testing and debugging methodologies, to\nimprove the quality of quantum software. The ultimate goal is to utilize\nmachine learning techniques to automatically predict fix patterns for existing\nquantum bugs.\n  Method: As part of the first stage of the agenda, we extend our initial study\nand propose a more comprehensive automated framework, called Q-PAC, for\ndetecting bug-fix patterns in IBM Qiskit quantum code. In the framework, we\ndevelop seven bug-fix pattern detectors using abstract syntax trees, syntactic\nfilters, and semantic checks.\n  Results: To demonstrate our method, we run Q-PAC on a variety of quantum\nbug-fix patterns using both real-world and handcrafted examples of bugs and\nfixes. The experimental results show that Q-PAC can effectively identify\nbug-fix patterns in IBM Qiskit.\n  Conclusion: We hope our initial study on quantum bug-fix detection can bring\nawareness of quantum software engineering to both researchers and\npractitioners. Thus, we also publish Q-PAC as an open-source software on\nGitHub. We would like to encourage other researchers to work on research\ndirections (such as Q-Repair) to improve the quality of the quantum\nprogramming.",
            "author": [
                "Pranav K. Nayak",
                "Krishn V. Kher",
                "M. Bharat Chandra",
                "M. V. Panduranga Rao",
                "Lei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17705v1",
                "http://arxiv.org/pdf/2311.17705v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17696v2",
            "title": "How to Build an AI Tutor that Can Adapt to Any Course and Provide\n  Accurate Answers Using Large Language Model and Retrieval-Augmented\n  Generation",
            "updated": "2023-11-30T06:28:22Z",
            "published": "2023-11-29T15:02:46Z",
            "summary": "Artificial intelligence is transforming education through data-driven,\npersonalized learning solutions. This paper introduces AI Tutor, an innovative\nweb application that provides personalized tutoring in any subject using\nstate-of-the-art Large Language Model (LLM). AI Tutor ingests course materials\nto construct an adaptive knowledge base tailored to the course. When students\npose questions, it retrieves the most relevant information and generates\ndetailed, conversational responses citing supporting evidence. The system is\npowered by advanced large language models and Retrieval-Augmented Generation\n(RAG) techniques for accurate, natural question answering. We present a\nfully-functional web interface and video demonstration that showcase AI Tutor's\nversatility across diverse subjects and its ability to produce pedagogically\ncogent responses. While an initial prototype, this work represents a pioneering\nstep toward AI-enabled tutoring systems that can democratize access to\nhigh-quality, customized educational support.",
            "author": [
                "Chenxi Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17696v2",
                "http://arxiv.org/pdf/2311.17696v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17687v1",
            "title": "There is no stationary $p$-cyclically monotone Poisson matching in 2D",
            "updated": "2023-11-29T14:49:38Z",
            "published": "2023-11-29T14:49:38Z",
            "summary": "We show that for $p>1$ there is no $p$-cyclically monotone stationary\nmatching of two independent Poisson processes in dimension $d=2$. The proof\ncombines the $p$-harmonic approximation result from \\cite[Theorem 1.1]{koch23}\nwith local asymptotics for the two-dimensional matching problem. Moreover, we\nprove a.s. local upper bounds of the correct order in the case $p>1$, which, to\nthe best of our knowledge, are not readily available in the current literature.",
            "author": [
                "Martin Huesmann",
                "Francesco Mattesini",
                "Felix Otto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17687v1",
                "http://arxiv.org/pdf/2311.17687v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17683v1",
            "title": "Melting curve of black phosphorus: evidence for a solid-liquid-liquid\n  triple point",
            "updated": "2023-11-29T14:44:40Z",
            "published": "2023-11-29T14:44:40Z",
            "summary": "Black phosphorus (bP) is a crystalline material that can be seen as ordered\nstackings of two-dimensional layers, which lead to outstanding anisotropic\nphysical properties. The knowledge of its pressure-temperature (P-T) phase\ndiagram, and in particular, the slope and location of its melting curve is\nfundamental for better understanding the synthesis and stability conditions of\nthis important material. Despite several experimental studies, important\nuncertainties remain in the determination of this melting curve. Here we report\naccurate melting points measurements, using in situ high-temperature and\nhigh-pressure high-resolution synchrotron x-ray diffraction. In particular, we\nhave employed an original and accurate pressure and temperature metrology based\non the unique anisotropic P-T response of bP, that we used as sensor for the\nsimultaneous determination of pressure and temperature up to 5 GPa and 1700 K.\nWe confirmed the existence of and located a solid-liquid-liquid triple point at\nthe intersection of the low- and high-pressure melting curves. Finally, we have\ncharacterized the irreversibility of the transformation in the low-pressure\nregime below 1 GPa, as the low-density liquid does not crystallize back to bP\nbut into red phosphorus on temperature quenching.",
            "author": [
                "Hermann Muhammad",
                "Mohamed Mezouar",
                "Gaston Garbarino",
                "Laura Henry",
                "Tomasz Por\u0119ba",
                "Matteo Ceppatelli",
                "Manuel Serrano-Ruiz",
                "Maurizio Peruzzini",
                "Fr\u00e9d\u00e9ric Datchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17683v1",
                "http://arxiv.org/pdf/2311.17683v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "physics.class-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17677v1",
            "title": "COVIDx CXR-4: An Expanded Multi-Institutional Open-Source Benchmark\n  Dataset for Chest X-ray Image-Based Computer-Aided COVID-19 Diagnostics",
            "updated": "2023-11-29T14:40:31Z",
            "published": "2023-11-29T14:40:31Z",
            "summary": "The global ramifications of the COVID-19 pandemic remain significant,\nexerting persistent pressure on nations even three years after its initial\noutbreak. Deep learning models have shown promise in improving COVID-19\ndiagnostics but require diverse and larger-scale datasets to improve\nperformance. In this paper, we introduce COVIDx CXR-4, an expanded\nmulti-institutional open-source benchmark dataset for chest X-ray image-based\ncomputer-aided COVID-19 diagnostics. COVIDx CXR-4 expands significantly on the\nprevious COVIDx CXR-3 dataset by increasing the total patient cohort size by\ngreater than 2.66 times, resulting in 84,818 images from 45,342 patients across\nmultiple institutions. We provide extensive analysis on the diversity of the\npatient demographic, imaging metadata, and disease distributions to highlight\npotential dataset biases. To the best of the authors' knowledge, COVIDx CXR-4\nis the largest and most diverse open-source COVID-19 CXR dataset and is made\npublicly available as part of an open initiative to advance research to aid\nclinicians against the COVID-19 disease.",
            "author": [
                "Yifan Wu",
                "Hayden Gunraj",
                "Chi-en Amy Tai",
                "Alexander Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17677v1",
                "http://arxiv.org/pdf/2311.17677v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17631v1",
            "title": "Q-learning Based Optimal False Data Injection Attack on Probabilistic\n  Boolean Control Networks",
            "updated": "2023-11-29T13:45:07Z",
            "published": "2023-11-29T13:45:07Z",
            "summary": "In this paper, we present a reinforcement learning (RL) method for solving\noptimal false data injection attack problems in probabilistic Boolean control\nnetworks (PBCNs) where the attacker lacks knowledge of the system model.\nSpecifically, we employ a Q-learning (QL) algorithm to address this problem. We\nthen propose an improved QL algorithm that not only enhances learning\nefficiency but also obtains optimal attack strategies for large-scale PBCNs\nthat the standard QL algorithm cannot handle. Finally, we verify the\neffectiveness of our proposed approach by considering two attacked PBCNs,\nincluding a 10-node network and a 28-node network.",
            "author": [
                "Xianlun Peng",
                "Yang Tang",
                "Fangfei Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17631v1",
                "http://arxiv.org/pdf/2311.17631v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.CR",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17621v1",
            "title": "The AutoSPADA Platform: User-Friendly Edge Computing for Distributed\n  Learning and Data Analytics in Connected Vehicles",
            "updated": "2023-11-29T13:30:26Z",
            "published": "2023-11-29T13:30:26Z",
            "summary": "Contemporary connected vehicles host numerous applications, such as\ndiagnostics and navigation, and new software is continuously being developed.\nHowever, the development process typically requires offline batch processing of\nlarge data volumes. In an edge computing approach, data analysts and developers\ncan instead process sensor data directly on computational resources inside\nvehicles. This enables rapid prototyping to shorten development cycles and\nreduce the time to create new business values or insights. This paper presents\nthe design, implementation, and operation of the AutoSPADA edge computing\nplatform for distributed data analytics. The platform's design follows\nscalability, reliability, resource efficiency, privacy, and security principles\npromoted through mature and industrially proven technologies. In AutoSPADA,\ncomputational tasks are general Python scripts, and we provide a library to,\nfor example, read signals from the vehicle and publish results to the cloud.\nHence, users only need Python knowledge to use the platform. Moreover, the\nplatform is designed to be extended to support additional programming\nlanguages.",
            "author": [
                "Adrian Nilsson",
                "Simon Smith",
                "Jonas Hagmar",
                "Magnus \u00d6nnheim",
                "Mats Jirstrand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17621v1",
                "http://arxiv.org/pdf/2311.17621v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17606v1",
            "title": "Large components in the subcritical Norros-Reittu model",
            "updated": "2023-11-29T13:01:43Z",
            "published": "2023-11-29T13:01:43Z",
            "summary": "The Norros-Reittu model is a random graph with $n$ vertices and i.i.d.\nweights assigned to them. The number of edges between any two vertices follows\nan independent Poisson distribution whose parameter is increasing in the\nweights of the two vertices. Choosing a suitable weight distribution leads to a\npower-law behaviour of the degree distribution as observed in many real-world\ncomplex networks. We study this model in the subcritical regime, i.e. in the\nabsence of a giant component. For each component, we count the vertices and\nshow convergence of the corresponding point process to a Poisson process. More\ngenerally, one can also count only specific vertices per component, like\nleaves. From this one can deduce asymptotic results on the size of the largest\ncomponent or the maximal number of leaves in a single component. The results\nalso apply to the Chung-Lu model and the generalised random graph.",
            "author": [
                "Matthias Lienau",
                "Matthias Schulte"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17606v1",
                "http://arxiv.org/pdf/2311.17606v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60F05, 60G70, 05C80"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17598v1",
            "title": "Improving embedding of graphs with missing data by soft manifolds",
            "updated": "2023-11-29T12:48:33Z",
            "published": "2023-11-29T12:48:33Z",
            "summary": "Embedding graphs in continous spaces is a key factor in designing and\ndeveloping algorithms for automatic information extraction to be applied in\ndiverse tasks (e.g., learning, inferring, predicting). The reliability of graph\nembeddings directly depends on how much the geometry of the continuous space\nmatches the graph structure. Manifolds are mathematical structure that can\nenable to incorporate in their topological spaces the graph characteristics,\nand in particular nodes distances. State-of-the-art of manifold-based graph\nembedding algorithms take advantage of the assumption that the projection on a\ntangential space of each point in the manifold (corresponding to a node in the\ngraph) would locally resemble a Euclidean space. Although this condition helps\nin achieving efficient analytical solutions to the embedding problem, it does\nnot represent an adequate set-up to work with modern real life graphs, that are\ncharacterized by weighted connections across nodes often computed over sparse\ndatasets with missing records. In this work, we introduce a new class of\nmanifold, named soft manifold, that can solve this situation. In particular,\nsoft manifolds are mathematical structures with spherical symmetry where the\ntangent spaces to each point are hypocycloids whose shape is defined according\nto the velocity of information propagation across the data points. Using soft\nmanifolds for graph embedding, we can provide continuous spaces to pursue any\ntask in data analysis over complex datasets. Experimental results on\nreconstruction tasks on synthetic and real datasets show how the proposed\napproach enable more accurate and reliable characterization of graphs in\ncontinuous spaces with respect to the state-of-the-art.",
            "author": [
                "Andrea Marinoni",
                "Pietro Lio'",
                "Alessandro Barp",
                "Christian Jutten",
                "Mark Girolami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17598v1",
                "http://arxiv.org/pdf/2311.17598v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17597v2",
            "title": "Continual Self-supervised Learning: Towards Universal Multi-modal\n  Medical Data Representation Learning",
            "updated": "2023-11-30T02:06:13Z",
            "published": "2023-11-29T12:47:42Z",
            "summary": "Self-supervised learning is an efficient pre-training method for medical\nimage analysis. However, current research is mostly confined to\nspecific-modality data pre-training, consuming considerable time and resources\nwithout achieving universality across different modalities. A straightforward\nsolution is combining all modality data for joint self-supervised pre-training,\nwhich poses practical challenges. Firstly, our experiments reveal conflicts in\nrepresentation learning as the number of modalities increases. Secondly,\nmulti-modal data collected in advance cannot cover all real-world scenarios. In\nthis paper, we reconsider versatile self-supervised learning from the\nperspective of continual learning and propose MedCoSS, a continuous\nself-supervised learning approach for multi-modal medical data. Unlike joint\nself-supervised learning, MedCoSS assigns different modality data to different\ntraining stages, forming a multi-stage pre-training process. To balance modal\nconflicts and prevent catastrophic forgetting, we propose a rehearsal-based\ncontinual learning method. We introduce the k-means sampling strategy to retain\ndata from previous modalities and rehearse it when learning new modalities.\nInstead of executing the pretext task on buffer data, a feature distillation\nstrategy and an intra-modal mixup strategy are applied to these data for\nknowledge retention. We conduct continuous self-supervised pre-training on a\nlarge-scale multi-modal unlabeled dataset, including clinical reports, X-rays,\nCT scans, MRI scans, and pathological images. Experimental results demonstrate\nMedCoSS's exceptional generalization ability across nine downstream datasets\nand its significant scalability in integrating new modality data. Code and\npre-trained weight are available at https://github.com/yeerwen/MedCoSS.",
            "author": [
                "Yiwen Ye",
                "Yutong Xie",
                "Jianpeng Zhang",
                "Ziyang Chen",
                "Qi Wu",
                "Yong Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17597v2",
                "http://arxiv.org/pdf/2311.17597v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17967v1",
            "title": "Discovering Galaxy Features via Dataset Distillation",
            "updated": "2023-11-29T12:39:31Z",
            "published": "2023-11-29T12:39:31Z",
            "summary": "In many applications, Neural Nets (NNs) have classification performance on\npar or even exceeding human capacity. Moreover, it is likely that NNs leverage\nunderlying features that might differ from those humans perceive to classify.\nCan we \"reverse-engineer\" pertinent features to enhance our scientific\nunderstanding? Here, we apply this idea to the notoriously difficult task of\ngalaxy classification: NNs have reached high performance for this task, but\nwhat does a neural net (NN) \"see\" when it classifies galaxies? Are there\nmorphological features that the human eye might overlook that could help with\nthe task and provide new insights? Can we visualize tracers of early evolution,\nor additionally incorporated spectral data? We present a novel way to summarize\nand visualize galaxy morphology through the lens of neural networks, leveraging\nDataset Distillation, a recent deep-learning methodology with the primary\nobjective to distill knowledge from a large dataset and condense it into a\ncompact synthetic dataset, such that a model trained on this synthetic dataset\nachieves performance comparable to a model trained on the full dataset. We\ncurate a class-balanced, medium-size high-confidence version of the Galaxy Zoo\n2 dataset, and proceed with dataset distillation from our accurate\nNN-classifier to create synthesized prototypical images of galaxy morphological\nfeatures, demonstrating its effectiveness. Of independent interest, we\nintroduce a self-adaptive version of the state-of-the-art Matching Trajectory\nalgorithm to automate the distillation process, and show enhanced performance\non computer vision benchmarks.",
            "author": [
                "Haowen Guan",
                "Xuan Zhao",
                "Zishi Wang",
                "Zhiyang Li",
                "Julia Kempe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17967v1",
                "http://arxiv.org/pdf/2311.17967v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "astro-ph.IM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17587v1",
            "title": "Deep Reinforcement Learning Graphs: Feedback Motion Planning via Neural\n  Lyapunov Verification",
            "updated": "2023-11-29T12:31:06Z",
            "published": "2023-11-29T12:31:06Z",
            "summary": "Recent advancements in model-free deep reinforcement learning have enabled\nefficient agent training. However, challenges arise when determining the region\nof attraction for these controllers, especially if the region does not fully\ncover the desired area. This paper addresses this issue by introducing a\nfeedback motion control algorithm that utilizes data-driven techniques and\nneural networks. The algorithm constructs a graph of connected\nreinforcement-learning based controllers, each with its own defined region of\nattraction. This incremental approach effectively covers a bounded region of\ninterest, creating a trajectory of interconnected nodes that guide the system\nfrom an initial state to the goal. Two approaches are presented for connecting\nnodes within the algorithm. The first is a tree-structured method, facilitating\n\"point-to-point\" control by constructing a tree connecting the initial state to\nthe goal state. The second is a graph-structured method, enabling\n\"space-to-space\" control by building a graph within a bounded region. This\napproach allows for control from arbitrary initial and goal states. The\nproposed method's performance is evaluated on a first-order dynamic system,\nconsidering scenarios both with and without obstacles. The results demonstrate\nthe effectiveness of the proposed algorithm in achieving the desired control\nobjectives.",
            "author": [
                "Armin Ghanbarzadeh",
                "Esmaeil Najafi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17587v1",
                "http://arxiv.org/pdf/2311.17587v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17582v1",
            "title": "LoCoMotif: Discovering time-warped motifs in time series",
            "updated": "2023-11-29T12:18:46Z",
            "published": "2023-11-29T12:18:46Z",
            "summary": "Time Series Motif Discovery (TSMD) refers to the task of identifying patterns\nthat occur multiple times (possibly with minor variations) in a time series.\nAll existing methods for TSMD have one or more of the following limitations:\nthey only look for the two most similar occurrences of a pattern; they only\nlook for patterns of a pre-specified, fixed length; they cannot handle\nvariability along the time axis; and they only handle univariate time series.\nIn this paper, we present a new method, LoCoMotif, that has none of these\nlimitations. The method is motivated by a concrete use case from physiotherapy.\nWe demonstrate the value of the proposed method on this use case. We also\nintroduce a new quantitative evaluation metric for motif discovery, and\nbenchmark data for comparing TSMD methods. LoCoMotif substantially outperforms\nthe existing methods, on top of being more broadly applicable.",
            "author": [
                "Daan Van Wesenbeeck",
                "Aras Yurtman",
                "Wannes Meert",
                "Hendrik Blockeel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17582v1",
                "http://arxiv.org/pdf/2311.17582v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17573v1",
            "title": "Spectral extremal results for Berge-$K_{3,t}$-free $r$-graphs",
            "updated": "2023-11-29T12:09:43Z",
            "published": "2023-11-29T12:09:43Z",
            "summary": "An $r$-uniform hypergraph ($r$-graph) is linear if any two edges intersect at\nmost one vertex. For a graph $F$, a hypergraph $H$ is Berge-$F$ if there is a\nbijection $\\phi:E(F)\\rightarrow E(H)$ such that $e\\subseteq \\phi(e)$ for each\n$e\\in E(F)$. For an $r$-graph $H$, let $\\mathcal{A}(H)$ be the adjacency tensor\nof $H$. The spectral radius of $H$ is the spectral radius of the tensor\n$\\mathcal{A}(H)$. In this paper, a kind of stability results for\nBerge-$K_{3,t}$ linear $r$-graphs are built. Based on these results, an upper\nbound for the spectral radius of connected Berge-$K_{3,t}$-free linear\n$r$-graphs is obtained. Meanwhile, the structure of the spectral extremal\nconnected Berge-$K_{3,t}$-free linear $r$-graphs is studied.",
            "author": [
                "Junpeng Zhou",
                "Xiying Yuan",
                "Wenhuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17573v1",
                "http://arxiv.org/pdf/2311.17573v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17567v1",
            "title": "Network characteristics of financial networks",
            "updated": "2023-11-29T12:01:17Z",
            "published": "2023-11-29T12:01:17Z",
            "summary": "We embrace a fresh perspective to auditing by analyzing a large set of\ncompanies as complex financial networks rather than static aggregates of\nbalance sheet data. Preliminary analyses show that network centrality measures\nwithin these networks could significantly enhance auditors' insights into\nfinancial structures. Utilizing data from over 300 diverse companies, we\nexamine the structure of financial statement networks through bipartite graph\nanalysis, exploring their scale-freeness by comparing degree distributions to\npower-law and exponential models. Our findings indicate heavy-tailed degree\ndistribution for financial account nodes, networks that grow with the same\ndiameter, and the presence of influential hubs. This study lays the groundwork\nfor future auditing methodologies where baseline network statistics could serve\nas indicators for anomaly detection, marking a substantial advancement in audit\nresearch and network science.",
            "author": [
                "M. Boersma",
                "S. Sourabh",
                "L. A. Hoogduin",
                "D. Kandhai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17567v1",
                "http://arxiv.org/pdf/2311.17567v1"
            ],
            "primary_category": "cs.CE",
            "category": [
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17964v1",
            "title": "Linear normalised hash function for clustering gene sequences and\n  identifying reference sequences from multiple sequence alignments",
            "updated": "2023-11-29T11:51:05Z",
            "published": "2023-11-29T11:51:05Z",
            "summary": "The aim of this study was to develop a method that would identify the cluster\ncentroids and the optimal number of clusters for a given sensitivity level and\ncould work equally well for the different sequence datasets. A novel method\nthat combines the linear mapping hash function and multiple sequence alignment\n(MSA) was developed. This method takes advantage of the already sorted by\nsimilarity sequences from the MSA output, and identifies the optimal number of\nclusters, clusters cut-offs, and clusters centroids that can represent\nreference gene vouchers for the different species. The linear mapping hash\nfunction can map an already ordered by similarity distance matrix to indices to\nreveal gaps in the values around which the optimal cut-offs of the different\nclusters can be identified. The method was evaluated using sets of closely\nrelated (16S rRNA gene sequences of Nocardia species) and highly variable (VP1\ngenomic region of Enterovirus 71) sequences and outperformed existing\nunsupervised machine learning clustering methods and dimensionality reduction\nmethods. This method does not require prior knowledge of the number of clusters\nor the distance between clusters, handles clusters of different sizes and\nshapes, and scales linearly with the dataset. The combination of MSA with the\nlinear mapping hash function is a computationally efficient way of gene\nsequence clustering and can be a valuable tool for the assessment of\nsimilarity, clustering of different microbial genomes, identifying reference\nsequences, and for the study of evolution of bacteria and viruses.",
            "author": [
                "Manal Helal",
                "Fanrong Kong",
                "Sharon C-A Chen",
                "Fei Zhou",
                "Dominic E Dwyer",
                "John Potter",
                "Vitali Sintchenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17964v1",
                "http://arxiv.org/pdf/2311.17964v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN",
                "cs.LG",
                "I.2.6"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17962v1",
            "title": "In search of the perfect fit: interpretation, flexible modelling, and\n  the existing generalisations of the normal distribution",
            "updated": "2023-11-29T11:28:35Z",
            "published": "2023-11-29T11:28:35Z",
            "summary": "Many generalised distributions exist for modelling data with vastly diverse\ncharacteristics. However, very few of these generalisations of the normal\ndistribution have shape parameters with clear roles that determine, for\ninstance, skewness and tail shape. In this chapter, we review existing skewing\nmechanisms and their properties in detail. Using the knowledge acquired, we add\na skewness parameter to the body-tail generalised normal distribution\n\\cite{BTGN}, that yields the \\ac{FIN} with parameters for location, scale,\nbody-shape, skewness, and tail weight. Basic statistical properties of the\n\\ac{FIN} are provided, such as the \\ac{PDF}, cumulative distribution function,\nmoments, and likelihood equations. Additionally, the \\ac{FIN} \\ac{PDF} is\nextended to a multivariate setting using a student t-copula, yielding the\n\\ac{MFIN}. The \\ac{MFIN} is applied to stock returns data, where it outperforms\nthe t-copula multivariate generalised hyperbolic, Azzalini skew-t, hyperbolic,\nand normal inverse Gaussian distributions.",
            "author": [
                "Andriette Bekker",
                "Matthias Wagener",
                "Muhammad Arashi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17962v1",
                "http://arxiv.org/pdf/2311.17962v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17541v2",
            "title": "TaskWeaver: A Code-First Agent Framework",
            "updated": "2023-12-01T07:42:56Z",
            "published": "2023-11-29T11:23:42Z",
            "summary": "Large Language Models (LLMs) have shown impressive abilities in natural\nlanguage understanding and generation, leading to their use in applications\nsuch as chatbots and virtual assistants. However, existing LLM frameworks face\nlimitations in handling domain-specific data analytics tasks with rich data\nstructures. Moreover, they struggle with flexibility to meet diverse user\nrequirements. To address these issues, TaskWeaver is proposed as a code-first\nframework for building LLM-powered autonomous agents. It converts user requests\ninto executable code and treats user-defined plugins as callable functions.\nTaskWeaver provides support for rich data structures, flexible plugin usage,\nand dynamic plugin selection, and leverages LLM coding capabilities for complex\nlogic. It also incorporates domain-specific knowledge through examples and\nensures the secure execution of generated code. TaskWeaver offers a powerful\nand flexible framework for creating intelligent conversational agents that can\nhandle complex tasks and adapt to domain-specific scenarios. The code is\nopen-sourced at https://github.com/microsoft/TaskWeaver/.",
            "author": [
                "Bo Qiao",
                "Liqun Li",
                "Xu Zhang",
                "Shilin He",
                "Yu Kang",
                "Chaoyun Zhang",
                "Fangkai Yang",
                "Hang Dong",
                "Jue Zhang",
                "Lu Wang",
                "Minghua Ma",
                "Pu Zhao",
                "Si Qin",
                "Xiaoting Qin",
                "Chao Du",
                "Yong Xu",
                "Qingwei Lin",
                "Saravan Rajmohan",
                "Dongmei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17541v2",
                "http://arxiv.org/pdf/2311.17541v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17526v1",
            "title": "Combinatorial quantum gravity and emergent 3D quantum behaviour",
            "updated": "2023-11-29T10:54:51Z",
            "published": "2023-11-29T10:54:51Z",
            "summary": "We review combinatorial quantum gravity, an approach which combines\nEinstein's idea of dynamical geometry with Wheeler's \"it from bit\" hypothesis\nin a model of dynamical graphs governed by the coarse Ollivier-Ricci curvature.\nThis drives a continuous phase transition from a random to a geometric phase,\ndue to a condensation of loops on the graph. In the 2D case, the geometric\nphase describes negative-curvature surfaces with two inversely related scales,\nan ultraviolet (UV) Planck length and an infrared (IR) radius of curvature.\nBelow the Planck scale the random bit character survives: chunks of random bits\nof the Planck size describe matter particles of excitation energy given by\ntheir excess curvature. Between the Planck length and the curvature radius, the\nsurface is smooth, with spectral and Hausdorff dimension 2; at scales larger\nthan the curvature radius, particles see the surface as an effective Lorentzian\nde Sitter surface, the spectral dimension becomes 3 and the effective slow\ndynamics of particles, as seen by co-moving observers, emerges as quantum\nmechanics in Euclidean 3D space. Since the 3D distances are inherited from the\nunderlying 2D de Sitter surface, we obtain curved trajectories around massive\nparticles also in 3D, representing the large-scale gravity interactions. We\nshall thus propose that this 2D model describes a generic holographic screen\nrelevant for real quantum gravity.",
            "author": [
                "Carlo A. Trugenberger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17526v1",
                "http://arxiv.org/pdf/2311.17526v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "gr-qc",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17521v1",
            "title": "Spinal Muscle Atrophy Disease Modelling as Bayesian Network",
            "updated": "2023-11-29T10:45:27Z",
            "published": "2023-11-29T10:45:27Z",
            "summary": "We investigate the molecular gene expressions studies and public databases\nfor disease modelling using Probabilistic Graphical Models and Bayesian\nInference. A case study on Spinal Muscle Atrophy Genome-Wide Association Study\nresults is modelled and analyzed. The genes up and down-regulated in two stages\nof the disease development are linked to prior knowledge published in the\npublic domain and co-expressions network is created and analyzed. The Molecular\nPathways triggered by these genes are identified. The Bayesian inference\nposteriors distributions are estimated using a variational analytical algorithm\nand a Markov chain Monte Carlo sampling algorithm. Assumptions, limitations and\npossible future work are concluded.",
            "author": [
                "Mohammed Ezzat Helal",
                "Manal Ezzat Helal",
                "Sherif Fadel Fahmy"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1742-6596/2128/1/012015",
                "http://arxiv.org/abs/2311.17521v1",
                "http://arxiv.org/pdf/2311.17521v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "cs.AI",
                "I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17502v1",
            "title": "Enhancing Answer Selection in Community Question Answering with\n  Pre-trained and Large Language Models",
            "updated": "2023-11-29T10:24:50Z",
            "published": "2023-11-29T10:24:50Z",
            "summary": "Community Question Answering (CQA) becomes increasingly prevalent in recent\nyears. However, there are a large number of answers, which is difficult for\nusers to select the relevant answers. Therefore, answer selection is a very\nsignificant subtask of CQA. In this paper, we first propose the Question-Answer\ncross attention networks (QAN) with pre-trained models for answer selection and\nutilize large language model (LLM) to perform answer selection with knowledge\naugmentation. Specifically, we apply the BERT model as the encoder layer to do\npre-training for question subjects, question bodies and answers, respectively,\nthen the cross attention mechanism selects the most relevant answer for\ndifferent questions. Experiments show that the QAN model achieves\nstate-of-the-art performance on two datasets, SemEval2015 and SemEval2017.\nMoreover, we use the LLM to generate external knowledge from questions and\ncorrect answers to achieve knowledge augmentation for the answer selection task\nby LLM, while optimizing the prompt of LLM in different aspects. The results\nshow that the introduction of external knowledge can improve the correct answer\nselection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM\ncan also select the correct answer on more questions by optimized prompt.",
            "author": [
                "Xinghang Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17502v1",
                "http://arxiv.org/pdf/2311.17502v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00056v1",
            "title": "Combinatorial decompositions for deformed or decorated classes of maps",
            "updated": "2023-11-29T10:19:04Z",
            "published": "2023-11-29T10:19:04Z",
            "summary": "The perturbative expansion of tensorial field theories in Feynman graphs can\nbe interpreted as weighted generating series of some piecewise linear\nvarieties. This simple fact establishes a link between two a priori distinct\nfields: the combinatorics of discrete manifolds on one hand and tensorial field\ntheories on the other hand. In this thesis, we study different aspects\nrevolving around this connection between combinatorics and field theory. First,\nwe consider constellations model, which generalize maps and their algebraic\nproperties. This makes them suited to probe the b-deformation, a deformation of\nthe algebra of symmetric functions. We will study the constraints satisfied by\nthe generating series of cubical b-deformed constellations. Second, we analyze\nthe double scaling limit of particular tensor models of order 3. For tensor of\norder greater than two, the nature of the 1/N-expansion is qualitatively\ndifferent from the matrix case of order 2. In particular, only the leading\norder graphs are fully characterized. Despite this fact, it is possible to\nidentify graphs of subleading orders contributing to the double scaling limit\nby implementing the scheme decomposition for Feynman graphs of these theories.\nAn analysis of the singularity of the schemes then allows us to give a complete\ncharacterization of the graphs contributing to the double scaling limit.\nFinally, we investigate a particular link between a tensor and a vector field\ntheory which both admit a melonic limit. Namely, we will show that we can\nobtain the vectorial Amit-Roginski model by considering perturbations around a\nclassical solution of the Boulatov model, a tensorial theory. We give\nsufficient conditions on the classical solution so that the effective action\nfor the perturbation around this solution takes the form of the Amit-Roginski\naction.",
            "author": [
                "Victor Nador"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00056v1",
                "http://arxiv.org/pdf/2312.00056v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17479v1",
            "title": "CrimeGNN: Harnessing the Power of Graph Neural Networks for Community\n  Detection in Criminal Networks",
            "updated": "2023-11-29T09:41:55Z",
            "published": "2023-11-29T09:41:55Z",
            "summary": "In this paper, we introduce CrimeGNN, a novel application of Graph Neural\nNetworks (GNNs) specifically designed to uncover hidden communities within\ncriminal networks. As criminal activities increasingly rely on complex network\nstructures, traditional methods of network analysis often fall short in\ndetecting the intricate and dynamic communities within these networks.\nLeveraging the power of GNNs, CrimeGNN provides an advanced and specialized\nsolution to this problem. The model ingests a graph structure of a criminal\nnetwork, where vertices represent individuals and edges represent relationships\nbetween them. CrimeGNN aims to identify a partition of the vertex set, such\nthat each subset represents a distinct community within the network, maximizing\nthe modularity function. Experimental results on several benchmark datasets\ndemonstrate the effectiveness of CrimeGNN, outperforming existing methods in\nterms of both accuracy and computational efficiency. The proposed framework\noffers significant potential for aiding law enforcement agencies in proactive\npolicing and crime prevention measures by providing a more in-depth\nunderstanding of the structure and operation of criminal networks.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17479v1",
                "http://arxiv.org/pdf/2311.17479v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17474v1",
            "title": "Large Language Models for Networking: Applications, Enabling Techniques,\n  and Challenges",
            "updated": "2023-11-29T09:31:13Z",
            "published": "2023-11-29T09:31:13Z",
            "summary": "The rapid evolution of network technologies and the growing complexity of\nnetwork tasks necessitate a paradigm shift in how networks are designed,\nconfigured, and managed. With a wealth of knowledge and expertise, large\nlanguage models (LLMs) are one of the most promising candidates. This paper\naims to pave the way for constructing domain-adapted LLMs for networking.\nFirstly, we present potential LLM applications for vertical network fields and\nshowcase the mapping from natural language to network language. Then, several\nenabling technologies are investigated, including parameter-efficient\nfinetuning and prompt engineering. The insight is that language understanding\nand tool usage are both required for network LLMs. Driven by the idea of\nembodied intelligence, we propose the ChatNet, a domain-adapted network LLM\nframework with access to various external network tools. ChatNet can reduce the\ntime required for burdensome network planning tasks significantly, leading to a\nsubstantial improvement in efficiency. Finally, key challenges and future\nresearch directions are highlighted.",
            "author": [
                "Yudong Huang",
                "Hongyang Du",
                "Xinyuan Zhang",
                "Dusit Niyato",
                "Jiawen Kang",
                "Zehui Xiong",
                "Shuo Wang",
                "Tao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17474v1",
                "http://arxiv.org/pdf/2311.17474v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17465v3",
            "title": "AgentAvatar: Disentangling Planning, Driving and Rendering for\n  Photorealistic Avatar Agents",
            "updated": "2023-12-04T16:49:18Z",
            "published": "2023-11-29T09:13:00Z",
            "summary": "In this study, our goal is to create interactive avatar agents that can\nautonomously plan and animate nuanced facial movements realistically, from both\nvisual and behavioral perspectives. Given high-level inputs about the\nenvironment and agent profile, our framework harnesses LLMs to produce a series\nof detailed text descriptions of the avatar agents' facial motions. These\ndescriptions are then processed by our task-agnostic driving engine into motion\ntoken sequences, which are subsequently converted into continuous motion\nembeddings that are further consumed by our standalone neural-based renderer to\ngenerate the final photorealistic avatar animations. These streamlined\nprocesses allow our framework to adapt to a variety of non-verbal avatar\ninteractions, both monadic and dyadic. Our extensive study, which includes\nexperiments on both newly compiled and existing datasets featuring two types of\nagents -- one capable of monadic interaction with the environment, and the\nother designed for dyadic conversation -- validates the effectiveness and\nversatility of our approach. To our knowledge, we advanced a leap step by\ncombining LLMs and neural rendering for generalized non-verbal prediction and\nphoto-realistic rendering of avatar agents.",
            "author": [
                "Duomin Wang",
                "Bin Dai",
                "Yu Deng",
                "Baoyuan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17465v3",
                "http://arxiv.org/pdf/2311.17465v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17454v1",
            "title": "Eden: An Ultra Fast, Provably Secure, and Fully Decentralized Blockchain\n  Interoperability Protocol",
            "updated": "2023-11-29T08:52:07Z",
            "published": "2023-11-29T08:52:07Z",
            "summary": "As the blockchain ecosystem continues to evolve and expand, the need for\nseamless interoperability between disparate blockchain networks has become\nincreasingly paramount. Interoperability not only enhances the functionality\nand reach of individual blockchains but also fosters a collaborative\nenvironment that can unlock new possibilities for decentralized applications.\nIn this paper, we present Eden, an elastic decentralized envoy network that\nleverage zero-knowledge MapReduce framework to facilitates ultra-fast and\nsecure cross-chain communication while maintaining complete decentralization.\nWe detail the Eden's design choices, its comprehensive security model, and the\ninnovative mechanisms it incorporates to ensure elasticity and resilience, even\nunder challenging network conditions.",
            "author": [
                "Ke Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17454v1",
                "http://arxiv.org/pdf/2311.17454v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17450v1",
            "title": "Continual Learning for Image Segmentation with Dynamic Query",
            "updated": "2023-11-29T08:46:46Z",
            "published": "2023-11-29T08:46:46Z",
            "summary": "Image segmentation based on continual learning exhibits a critical drop of\nperformance, mainly due to catastrophic forgetting and background shift, as\nthey are required to incorporate new classes continually. In this paper, we\npropose a simple, yet effective Continual Image Segmentation method with\nincremental Dynamic Query (CISDQ), which decouples the representation learning\nof both old and new knowledge with lightweight query embedding. CISDQ mainly\nincludes three contributions: 1) We define dynamic queries with adaptive\nbackground class to exploit past knowledge and learn future classes naturally.\n2) CISDQ proposes a class/instance-aware Query Guided Knowledge Distillation\nstrategy to overcome catastrophic forgetting by capturing the inter-class\ndiversity and intra-class identity. 3) Apart from semantic segmentation, CISDQ\nintroduce the continual learning for instance segmentation in which\ninstance-wise labeling and supervision are considered. Extensive experiments on\nthree datasets for two tasks (i.e., continual semantic and instance\nsegmentation are conducted to demonstrate that CISDQ achieves the\nstate-of-the-art performance, specifically, obtaining 4.4% and 2.9% mIoU\nimprovements for the ADE 100-10 (6 steps) setting and ADE 100-5 (11 steps)\nsetting.",
            "author": [
                "Weijia Wu",
                "Yuzhong Zhao",
                "Zhuang Li",
                "Lianlei Shan",
                "Hong Zhou",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17450v1",
                "http://arxiv.org/pdf/2311.17450v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17437v1",
            "title": "Robust network formation with biological applications",
            "updated": "2023-11-29T08:29:37Z",
            "published": "2023-11-29T08:29:37Z",
            "summary": "We provide new results on the structure of optimal transportation networks\nobtained as minimizers of an energy cost functional consisting of a kinetic\n(pumping) and material (metabolic) cost terms, constrained by a local mass\nconservation law. In particular, we prove that every tree (i.e., graph without\nloops) represents a local minimizer of the energy with concave metabolic cost.\nFor the linear metabolic cost, we prove that the set of minimizers contains a\nloop-free structure. Moreover, we enrich the energy functional such that it\naccounts also for robustness of the network, measured in terms of the Fiedler\nnumber of the graph with edge weights given by their conductivities. We examine\nfundamental properties of the modified functional, in particular, its convexity\nand differentiability. We provide analytical insights into the new model by\nconsidering two simple examples. Subsequently, we employ the projected\nsubgradient method to find global minimizers of the modified functional\nnumerically. We then present two numerical examples, illustrating how the\noptimal graph's structure and energy expenditure depend on the required\nrobustness of the network.",
            "author": [
                "Jan Haskovec",
                "Jan Vybiral"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17437v1",
                "http://arxiv.org/pdf/2311.17437v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17433v1",
            "title": "Cospectrality results for signed graphs with two eigenvalues unequal to\n  $\\pm 1$",
            "updated": "2023-11-29T08:24:52Z",
            "published": "2023-11-29T08:24:52Z",
            "summary": "Recently the collection $\\cal G$ of all signed graphs for which the adjacency\nmatrix has all but at most two eigenvalues equal to $\\pm 1$ has been\ndetermined. Here we investigate $\\cal G$ for cospectral pairs, and for signed\ngraphs determined by their spectrum (up to switching). If the order is at most\n20, the outcome is presented in a clear table. If the spectrum is symmetric we\nfind all signed graphs in $\\cal G$ determined by their spectrum, and we obtain\nall signed graphs cospectral with the bipartite double of the complete graph.\nIn addition we determine all signed graphs cospectral with the Friendship graph\n$F_\\ell$, and show that there is no connected signed graph cospectral but not\nswitching equivalent with $F_\\ell$.",
            "author": [
                "Willem H. Haemers",
                "Hatice Topcu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17433v1",
                "http://arxiv.org/pdf/2311.17433v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17432v1",
            "title": "Evolutionary accessibility of random and structured fitness landscapes",
            "updated": "2023-11-29T08:23:37Z",
            "published": "2023-11-29T08:23:37Z",
            "summary": "Biological evolution can be conceptualized as a search process in the space\nof gene sequences guided by the fitness landscape, a mapping that assigns a\nmeasure of reproductive value to each genotype. Here we discuss probabilistic\nmodels of fitness landscapes with a focus on their evolutionary accessibility,\nwhere a path in a fitness landscape is said to be accessible if the fitness\nvalues encountered along the path increase monotonically. For uncorrelated\n(random) landscapes with independent and identically distributed fitness\nvalues, the probability of existence of accessible paths between genotypes at a\ndistance linear in the sequence length $L$ becomes nonzero at a nontrivial\nthreshold value of the fitness difference between the initial and final\ngenotype, which can be explicitly computed for large classes of genotype\ngraphs. The behaviour in uncorrelated random landscapes is contrasted with\nlandscape models that display additional, biologically motivated structural\nfeatures. In particular, landscapes defined by a tradeoff between adaptation to\nenvironmental extremes have been found to display a combinatorially large\nnumber of accessible paths to all local fitness maxima. We show that this\nproperty is characteristic of a broad class of models that satisfy a certain\nglobal constraint, and provide further examples from this class.",
            "author": [
                "Joachim Krug",
                "Daniel Oros"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17432v1",
                "http://arxiv.org/pdf/2311.17432v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "cond-mat.dis-nn",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17431v4",
            "title": "Grounding Foundation Models through Federated Transfer Learning: A\n  General Framework",
            "updated": "2023-12-05T09:35:03Z",
            "published": "2023-11-29T08:21:42Z",
            "summary": "Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and\npowerful emergent abilities have achieved remarkable success in various natural\nlanguage processing and computer vision tasks. Grounding FMs by adapting them\nto domain-specific tasks or augmenting them with domain-specific knowledge\nenables us to exploit the full potential of FMs. However, grounding FMs faces\nseveral challenges, stemming primarily from constrained computing resources,\ndata privacy, model heterogeneity, and model ownership. Federated Transfer\nLearning (FTL), the combination of federated learning and transfer learning,\nprovides promising solutions to address these challenges. In recent years, the\nneed for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in\nboth academia and industry. Motivated by the strong growth in FTL-FM research\nand the potential impact of FTL-FM on industrial applications, we propose an\nFTL-FM framework that formulates problems of grounding FMs in the federated\nlearning setting, construct a detailed taxonomy based on the FTL-FM framework\nto categorize state-of-the-art FTL-FM works, and comprehensively overview\nFTL-FM works based on the proposed taxonomy. We also establish correspondences\nbetween FTL-FM and conventional phases of adapting FM so that FM practitioners\ncan align their research works with FTL-FM. In addition, we overview advanced\nefficiency-improving and privacy-preserving techniques because efficiency and\nprivacy are critical concerns in FTL-FM. Last, we discuss opportunities and\nfuture research directions of FTL-FM.",
            "author": [
                "Yan Kang",
                "Tao Fan",
                "Hanlin Gu",
                "Lixin Fan",
                "Qiang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17431v4",
                "http://arxiv.org/pdf/2311.17431v4"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17410v2",
            "title": "GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on\n  Dynamic Graphs",
            "updated": "2023-11-30T03:48:24Z",
            "published": "2023-11-29T07:30:32Z",
            "summary": "Graph Neural Networks (GNNs) play a crucial role in various fields. However,\nmost existing deep graph learning frameworks assume pre-stored static graphs\nand do not support training on graph streams. In contrast, many real-world\ngraphs are dynamic and contain time domain information. We introduce GNNFlow, a\ndistributed framework that enables efficient continuous temporal graph\nrepresentation learning on dynamic graphs on multi-GPU machines. GNNFlow\nintroduces an adaptive time-indexed block-based data structure that effectively\nbalances memory usage with graph update and sampling operation efficiency. It\nfeatures a hybrid GPU-CPU graph data placement for rapid GPU-based temporal\nneighborhood sampling and kernel optimizations for enhanced sampling processes.\nA dynamic GPU cache for node and edge features is developed to maximize cache\nhit rates through reuse and restoration strategies. GNNFlow supports\ndistributed training across multiple machines with static scheduling to ensure\nload balance. We implement GNNFlow based on DGL and PyTorch. Our experimental\nresults show that GNNFlow provides up to 21.1x faster continuous learning than\nexisting systems.",
            "author": [
                "Yuchen Zhong",
                "Guangming Sheng",
                "Tianzuo Qin",
                "Minjie Wang",
                "Quan Gan",
                "Chuan Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17410v2",
                "http://arxiv.org/pdf/2311.17410v2"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17408v1",
            "title": "Dynamic Dense Graph Convolutional Network for Skeleton-based Human\n  Motion Prediction",
            "updated": "2023-11-29T07:25:49Z",
            "published": "2023-11-29T07:25:49Z",
            "summary": "Graph Convolutional Networks (GCN) which typically follows a neural message\npassing framework to model dependencies among skeletal joints has achieved high\nsuccess in skeleton-based human motion prediction task. Nevertheless, how to\nconstruct a graph from a skeleton sequence and how to perform message passing\non the graph are still open problems, which severely affect the performance of\nGCN. To solve both problems, this paper presents a Dynamic Dense Graph\nConvolutional Network (DD-GCN), which constructs a dense graph and implements\nan integrated dynamic message passing. More specifically, we construct a dense\ngraph with 4D adjacency modeling as a comprehensive representation of motion\nsequence at different levels of abstraction. Based on the dense graph, we\npropose a dynamic message passing framework that learns dynamically from data\nto generate distinctive messages reflecting sample-specific relevance among\nnodes in the graph. Extensive experiments on benchmark Human 3.6M and CMU Mocap\ndatasets verify the effectiveness of our DD-GCN which obviously outperforms\nstate-of-the-art GCN-based methods, especially when using long-term and our\nproposed extremely long-term protocol.",
            "author": [
                "Xinshun Wang",
                "Wanying Zhang",
                "Can Wang",
                "Yuan Gao",
                "Mengyuan Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TIP.2023.3334954",
                "http://arxiv.org/abs/2311.17408v1",
                "http://arxiv.org/pdf/2311.17408v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17400v2",
            "title": "Improving the Robustness of Transformer-based Large Language Models with\n  Dynamic Attention",
            "updated": "2023-11-30T02:08:24Z",
            "published": "2023-11-29T07:09:13Z",
            "summary": "Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.",
            "author": [
                "Lujia Shen",
                "Yuwen Pu",
                "Shouling Ji",
                "Changjiang Li",
                "Xuhong Zhang",
                "Chunpeng Ge",
                "Ting Wang"
            ],
            "link": [
                "http://dx.doi.org/10.14722/ndss.2024.24115",
                "http://arxiv.org/abs/2311.17400v2",
                "http://arxiv.org/pdf/2311.17400v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17397v1",
            "title": "Geometric interpretation of First Betti numbers of smooth functions\n  orbits",
            "updated": "2023-11-29T06:54:39Z",
            "published": "2023-11-29T06:54:39Z",
            "summary": "Let $M$ be a 2-disk or a cylinder, and $f$ be a smooth function on $M$ with\nconstant values at $\\partial M$, devoid of critical points in $\\partial M$, and\nexhibiting a property wherein for every critical point $z$ of $f$ there is a\nlocal presentation of $f$ near $z$ that is a homogeneous polynomial without\nmultiple factors. We consider $V$ to be either the boundary $\\partial M$ (in\nthe case of a 2-disk) or one of its boundary components (in the case of a\ncylinder) and $\\mathcal{S}^{'}(f, V)$ to consist of diffeomorphisms preserving\n$f$, isotopic to the identity relative to $V$. We establish a correspondence:\nthe first Betti number of the $f$-orbit is shown to be equal to the number of\norbits resulting from the action of $\\mathcal{S}^{'}(f,V)$ on the internal\nedges of the Kronrod-Reeb graph associated with $f$.",
            "author": [
                "Iryna Kuznietsova",
                "Yuliia Soroka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17397v1",
                "http://arxiv.org/pdf/2311.17397v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17373v1",
            "title": "The Devil is in the Data: Learning Fair Graph Neural Networks via\n  Partial Knowledge Distillation",
            "updated": "2023-11-29T05:54:58Z",
            "published": "2023-11-29T05:54:58Z",
            "summary": "Graph neural networks (GNNs) are being increasingly used in many high-stakes\ntasks, and as a result, there is growing attention on their fairness recently.\nGNNs have been shown to be unfair as they tend to make discriminatory decisions\ntoward certain demographic groups, divided by sensitive attributes such as\ngender and race. While recent works have been devoted to improving their\nfairness performance, they often require accessible demographic information.\nThis greatly limits their applicability in real-world scenarios due to legal\nrestrictions. To address this problem, we present a demographic-agnostic method\nto learn fair GNNs via knowledge distillation, namely FairGKD. Our work is\nmotivated by the empirical observation that training GNNs on partial data\n(i.e., only node attributes or topology data) can improve their fairness,\nalbeit at the cost of utility. To make a balanced trade-off between fairness\nand utility performance, we employ a set of fairness experts (i.e., GNNs\ntrained on different partial data) to construct the synthetic teacher, which\ndistills fairer and informative knowledge to guide the learning of the GNN\nstudent. Experiments on several benchmark datasets demonstrate that FairGKD,\nwhich does not require access to demographic information, significantly\nimproves the fairness of GNNs by a large margin while maintaining their\nutility.",
            "author": [
                "Yuchang Zhu",
                "Jintang Li",
                "Liang Chen",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17373v1",
                "http://arxiv.org/pdf/2311.17373v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17365v1",
            "title": "Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human\n  Activity Reasoning",
            "updated": "2023-11-29T05:27:14Z",
            "published": "2023-11-29T05:27:14Z",
            "summary": "Human reasoning can be understood as a cooperation between the intuitive,\nassociative \"System-1\" and the deliberative, logical \"System-2\". For existing\nSystem-1-like methods in visual activity understanding, it is crucial to\nintegrate System-2 processing to improve explainability, generalization, and\ndata efficiency. One possible path of activity reasoning is building a symbolic\nsystem composed of symbols and rules, where one rule connects multiple symbols,\nimplying human knowledge and reasoning abilities. Previous methods have made\nprogress, but are defective with limited symbols from handcraft and limited\nrules from visual-based annotations, failing to cover the complex patterns of\nactivities and lacking compositional generalization. To overcome the defects,\nwe propose a new symbolic system with two ideal important properties:\nbroad-coverage symbols and rational rules. Collecting massive human knowledge\nvia manual annotations is expensive to instantiate this symbolic system.\nInstead, we leverage the recent advancement of LLMs (Large Language Models) as\nan approximation of the two ideal properties, i.e., Symbols from Large Language\nModels (Symbol-LLM). Then, given an image, visual contents from the images are\nextracted and checked as symbols and activity semantics are reasoned out based\non rules via fuzzy logic calculation. Our method shows superiority in extensive\nactivity understanding tasks. Code and data are available at\nhttps://mvig-rhos.com/symbol_llm.",
            "author": [
                "Xiaoqian Wu",
                "Yong-Lu Li",
                "Jianhua Sun",
                "Cewu Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17365v1",
                "http://arxiv.org/pdf/2311.17365v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17361v2",
            "title": "How does spatial structure affect psychological restoration? A method\n  based on Graph Neural Networks and Street View Imagery",
            "updated": "2023-11-30T02:18:36Z",
            "published": "2023-11-29T05:20:10Z",
            "summary": "The Attention Restoration Theory (ART) presents a theoretical framework with\nfour essential indicators (being away, extent, fascinating, and compatibility)\nfor comprehending urban and natural restoration quality. However, previous\nstudies relied on non-sequential data and non-spatial dependent methods, which\noverlooks the impact of spatial structure defined here as the positional\nrelationships between scene entities on restoration quality. The past methods\nalso make it challenging to measure restoration quality on an urban scale. In\nthis work, a spatial-dependent graph neural networks (GNNs) approach is\nproposed to reveal the relation between spatial structure and restoration\nquality on an urban scale. Specifically, we constructed two different types of\ngraphs at the street and city levels. The street-level graphs, using sequential\nstreet view images (SVIs) of road segments to capture position relationships\nbetween entities, were used to represent spatial structure. The city-level\ngraph, modeling the topological relationships of roads as non-Euclidean data\nstructures and embedding urban features (including Perception-features,\nSpatial-features, and Socioeconomic-features), was used to measure restoration\nquality. The results demonstrate that: 1) spatial-dependent GNNs model\noutperforms traditional methods (Acc = 0.735, F1 = 0.732); 2) spatial structure\nportrayed through sequential SVIs data significantly influences restoration\nquality; 3) spaces with the same restoration quality exhibited distinct spatial\nstructures patterns. This study clarifies the association between spatial\nstructure and restoration quality, providing a new perspective to improve urban\nwell-being in the future.",
            "author": [
                "Haoran Ma",
                "Yan Zhang",
                "Pengyuan Liu",
                "Fan Zhang",
                "Pengyu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17361v2",
                "http://arxiv.org/pdf/2311.17361v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17359v1",
            "title": "Classical vs Quantum Annealing and Manifold Reduction in Soft-Spin\n  Minimizers of Ising Hamiltonians",
            "updated": "2023-11-29T05:15:38Z",
            "published": "2023-11-29T05:15:38Z",
            "summary": "We investigate the minimization of the Ising Hamiltonians, comparing the\ndynamics of semi-classical soft-spin models with quantum annealing. We\nsystematically analyze how the energy landscape for the circulant couplings of\na Mobius graph evolves with increased annealing parameters. Our findings\nindicate that these semi-classical models face challenges due to a widening\ndimensionality landscape. To counteract this issue, we introduce the `manifold\nreduction' method, which restricts the soft-spin amplitudes to a defined phase\nspace region. Concurrently, quantum annealing demonstrates a natural capability\nto navigate the Ising Hamiltonian's energy landscape due to its operation\nwithin the comprehensive Hilbert space. Our study indicates that\nphysics-inspired or physics-enhanced optimizers will likely benefit from a\nblend of classical and quantum annealing techniques.",
            "author": [
                "James S. Cummins",
                "Hayder Salman",
                "Natalia G. Berloff"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17359v1",
                "http://arxiv.org/pdf/2311.17359v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.other",
                "physics.comp-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17355v1",
            "title": "Are Large Language Models Good Fact Checkers: A Preliminary Study",
            "updated": "2023-11-29T05:04:52Z",
            "published": "2023-11-29T05:04:52Z",
            "summary": "Recently, Large Language Models (LLMs) have drawn significant attention due\nto their outstanding reasoning capabilities and extensive knowledge repository,\npositioning them as superior in handling various natural language processing\ntasks compared to other language models. In this paper, we present a\npreliminary investigation into the potential of LLMs in fact-checking. This\nstudy aims to comprehensively evaluate various LLMs in tackling specific\nfact-checking subtasks, systematically evaluating their capabilities, and\nconducting a comparative analysis of their performance against pre-trained and\nstate-of-the-art low-parameter models. Experiments demonstrate that LLMs\nachieve competitive performance compared to other small models in most\nscenarios. However, they encounter challenges in effectively handling Chinese\nfact verification and the entirety of the fact-checking pipeline due to\nlanguage inconsistencies and hallucinations. These findings underscore the need\nfor further exploration and research to enhance the proficiency of LLMs as\nreliable fact-checkers, unveiling the potential capability of LLMs and the\npossible challenges in fact-checking tasks.",
            "author": [
                "Han Cao",
                "Lingwei Wei",
                "Mengyang Chen",
                "Wei Zhou",
                "Songlin Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17355v1",
                "http://arxiv.org/pdf/2311.17355v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03742v1",
            "title": "Clinical Risk Prediction Using Language Models: Benefits And\n  Considerations",
            "updated": "2023-11-29T04:32:19Z",
            "published": "2023-11-29T04:32:19Z",
            "summary": "The utilization of Electronic Health Records (EHRs) for clinical risk\nprediction is on the rise. However, strict privacy regulations limit access to\ncomprehensive health records, making it challenging to apply standard machine\nlearning algorithms in practical real-world scenarios. Previous research has\naddressed this data limitation by incorporating medical ontologies and\nemploying transfer learning methods. In this study, we investigate the\npotential of leveraging language models (LMs) as a means to incorporate\nsupplementary domain knowledge for improving the performance of various\nEHR-based risk prediction tasks. Unlike applying LMs to unstructured EHR data\nsuch as clinical notes, this study focuses on using textual descriptions within\nstructured EHR to make predictions exclusively based on that information. We\nextensively compare against previous approaches across various data types and\nsizes. We find that employing LMs to represent structured EHRs, such as\ndiagnostic histories, leads to improved or at least comparable performance in\ndiverse risk prediction tasks. Furthermore, LM-based approaches offer numerous\nadvantages, including few-shot learning, the capability to handle previously\nunseen medical concepts, and adaptability to various medical vocabularies.\nNevertheless, we underscore, through various experiments, the importance of\nbeing cautious when employing such models, as concerns regarding the\nreliability of LMs persist.",
            "author": [
                "Angeela Acharya",
                "Sulabh Shrestha",
                "Anyi Chen",
                "Joseph Conte",
                "Sanja Avramovic",
                "Siddhartha Sikdar",
                "Antonios Anastasopoulos",
                "Sanmay Das"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03742v1",
                "http://arxiv.org/pdf/2312.03742v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00055v1",
            "title": "LEAP: LLM-Generation of Egocentric Action Programs",
            "updated": "2023-11-29T04:25:52Z",
            "published": "2023-11-29T04:25:52Z",
            "summary": "We introduce LEAP (illustrated in Figure 1), a novel method for generating\nvideo-grounded action programs through use of a Large Language Model (LLM).\nThese action programs represent the motoric, perceptual, and structural aspects\nof action, and consist of sub-actions, pre- and post-conditions, and control\nflows. LEAP's action programs are centered on egocentric video and employ\nrecent developments in LLMs both as a source for program knowledge and as an\naggregator and assessor of multimodal video information. We apply LEAP over a\nmajority (87\\%) of the training set of the EPIC Kitchens dataset, and release\nthe resulting action programs as a publicly available dataset here\n(https://drive.google.com/drive/folders/1Cpkw_TI1IIxXdzor0pOXG3rWJWuKU5Ex?usp=drive_link).\nWe employ LEAP as a secondary source of supervision, using its action programs\nin a loss term applied to action recognition and anticipation networks. We\ndemonstrate sizable improvements in performance in both tasks due to training\nwith the LEAP dataset. Our method achieves 1st place on the EPIC Kitchens\nAction Recognition leaderboard as of November 17 among the networks restricted\nto RGB-input (see Supplementary Materials).",
            "author": [
                "Eadom Dessalene",
                "Michael Maynord",
                "Cornelia Ferm\u00fcller",
                "Yiannis Aloimonos"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00055v1",
                "http://arxiv.org/pdf/2312.00055v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17342v1",
            "title": "On the size and complexity of scrambles",
            "updated": "2023-11-29T03:42:30Z",
            "published": "2023-11-29T03:42:30Z",
            "summary": "The scramble number of a graph, a natural generalization of bramble number,\nis an invariant recently developed to study chip-firing games and graph\ngonality. We introduce the carton number of a graph, defined to be the minimum\nsize of a maximum order scramble, to study the computational complexity of\nscramble number. We show that there exist graphs with carton number exponential\nin the size of the graph, proving that scrambles are not valid NP certificates.\nWe characterize families of graphs whose scramble number and gonality can be\nconstant-factor approximated in polynomial time and show that the disjoint\nversion of scramble number is fixed parameter tractable. Lastly, we find that\nvertex congestion is an upper bound on screewidth and thus scramble number,\nleading to a new proof of the best known bound on the treewidth of line graphs\nand a bound on the scramble number of planar graphs with bounded degree.",
            "author": [
                "Seamus Connor",
                "Steven DiSilvio",
                "Sasha Kononova",
                "Ralph Morrison",
                "Krish Singal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17342v1",
                "http://arxiv.org/pdf/2311.17342v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C57, 05C85, 14T05"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17331v1",
            "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering",
            "updated": "2023-11-29T03:10:42Z",
            "published": "2023-11-29T03:10:42Z",
            "summary": "Recently, Vision Language Models (VLMs) have gained significant attention,\nexhibiting notable advancements across various tasks by leveraging extensive\nimage-text paired data. However, prevailing VLMs often treat Visual Question\nAnswering (VQA) as perception tasks, employing black-box models that overlook\nexplicit modeling of relationships between different questions within the same\nvisual scene. Moreover, the existing VQA methods that rely on Knowledge Bases\n(KBs) might frequently encounter biases from limited data and face challenges\nin relevant information indexing. Attempt to overcome these limitations, this\npaper introduces an explainable multi-agent collaboration framework by tapping\ninto knowledge embedded in Large Language Models (LLMs) trained on extensive\ncorpora. Inspired by human cognition, our framework uncovers latent information\nwithin the given question by employing three agents, i.e., Seeker, Responder,\nand Integrator, to perform a top-down reasoning process. The Seeker agent\ngenerates relevant issues related to the original question. The Responder\nagent, based on VLM, handles simple VQA tasks and provides candidate answers.\nThe Integrator agent combines information from the Seeker agent and the\nResponder agent to produce the final VQA answer. Through the above\ncollaboration mechanism, our framework explicitly constructs a multi-view\nknowledge base for a specific image scene, reasoning answers in a top-down\nprocessing manner. We extensively evaluate our method on diverse VQA datasets\nand VLMs, demonstrating its broad applicability and interpretability with\ncomprehensive experimental results.",
            "author": [
                "Zeqing Wang",
                "Wentao Wan",
                "Runmeng Chen",
                "Qiqing Lao",
                "Minjie Lang",
                "Keze Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17331v1",
                "http://arxiv.org/pdf/2311.17331v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17330v1",
            "title": "Biomedical knowledge graph-enhanced prompt generation for large language\n  models",
            "updated": "2023-11-29T03:07:00Z",
            "published": "2023-11-29T03:07:00Z",
            "summary": "Large Language Models (LLMs) have been driving progress in AI at an\nunprecedented rate, yet still face challenges in knowledge-intensive domains\nlike biomedicine. Solutions such as pre-training and domain-specific\nfine-tuning add substantial computational overhead, and the latter require\ndomain-expertise. External knowledge infusion is task-specific and requires\nmodel training. Here, we introduce a task-agnostic Knowledge Graph-based\nRetrieval Augmented Generation (KG-RAG) framework by leveraging the massive\nbiomedical KG SPOKE with LLMs such as Llama-2-13b, GPT-3.5-Turbo and GPT-4, to\ngenerate meaningful biomedical text rooted in established knowledge. KG-RAG\nconsistently enhanced the performance of LLMs across various prompt types,\nincluding one-hop and two-hop prompts, drug repurposing queries, biomedical\ntrue/false questions, and multiple-choice questions (MCQ). Notably, KG-RAG\nprovides a remarkable 71% boost in the performance of the Llama-2 model on the\nchallenging MCQ dataset, demonstrating the framework's capacity to empower\nopen-source models with fewer parameters for domain-specific questions.\nFurthermore, KG-RAG enhanced the performance of proprietary GPT models, such as\nGPT-3.5 which exhibited improvement over GPT-4 in context utilization on MCQ\ndata. Our approach was also able to address drug repurposing questions,\nreturning meaningful repurposing suggestions. In summary, the proposed\nframework combines explicit and implicit knowledge of KG and LLM, respectively,\nin an optimized fashion, thus enhancing the adaptability of general-purpose\nLLMs to tackle domain-specific questions in a unified framework.",
            "author": [
                "Karthik Soman",
                "Peter W Rose",
                "John H Morris",
                "Rabia E Akbas",
                "Brett Smith",
                "Braian Peetoom",
                "Catalina Villouta-Reyes",
                "Gabriel Cerono",
                "Yongmei Shi",
                "Angela Rizk-Jackson",
                "Sharat Israni",
                "Charlotte A Nelson",
                "Sui Huang",
                "Sergio E Baranzini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17330v1",
                "http://arxiv.org/pdf/2311.17330v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17327v1",
            "title": "Improving Self-supervised Molecular Representation Learning using\n  Persistent Homology",
            "updated": "2023-11-29T02:58:30Z",
            "published": "2023-11-29T02:58:30Z",
            "summary": "Self-supervised learning (SSL) has great potential for molecular\nrepresentation learning given the complexity of molecular graphs, the large\namounts of unlabelled data available, the considerable cost of obtaining labels\nexperimentally, and the hence often only small training datasets. The\nimportance of the topic is reflected in the variety of paradigms and\narchitectures that have been investigated recently. Yet the differences in\nperformance seem often minor and are barely understood to date. In this paper,\nwe study SSL based on persistent homology (PH), a mathematical tool for\nmodeling topological features of data that persist across multiple scales. It\nhas several unique features which particularly suit SSL, naturally offering:\ndifferent views of the data, stability in terms of distance preservation, and\nthe opportunity to flexibly incorporate domain knowledge. We (1) investigate an\nautoencoder, which shows the general representational power of PH, and (2)\npropose a contrastive loss that complements existing approaches. We rigorously\nevaluate our approach for molecular property prediction and demonstrate its\nparticular features in improving the embedding space: after SSL, the\nrepresentations are better and offer considerably more predictive power than\nthe baselines over different probing tasks; our loss increases baseline\nperformance, sometimes largely; and we often obtain substantial improvements\nover very small datasets, a common scenario in practice.",
            "author": [
                "Yuankai Luo",
                "Lei Shi",
                "Veronika Thost"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17327v1",
                "http://arxiv.org/pdf/2311.17327v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17324v1",
            "title": "Control of complex systems with generalized embedding and empirical\n  dynamic modeling",
            "updated": "2023-11-29T02:40:37Z",
            "published": "2023-11-29T02:40:37Z",
            "summary": "Feedback control is ubiquitous in complex systems. Effective control requires\nknowledge of the dynamics informing feedback compensation to guide the system\ntoward desired states. In many control applications this knowledge is expressed\nmathematically or through data-driven models, however, as complexity grows\nobtaining a satisfactory mathematical representation is increasingly difficult.\nFurther, many data-driven approaches consist of abstract internal\nrepresentations that may have no obvious connection to the underlying dynamics\nand control, or, require a-priori specification of functions to represent the\ndynamics. To remove these constraints we demonstrate that generalized state\nspace embedding and prediction can provide data-driven process model\nrepresentation for control of complex systems. Generalized embedding naturally\nencompasses multivariate dynamics enabling state space variable cross mapping\nfor direct assessment of multivariate contributions to the dynamics. Further,\nstate space kernel regression allows inspection of intervariable dependencies.\nTo illustrate this an agent based model is used to generate nonlinear dynamics\nwhich are then modeled by generalized state space embedding providing state\npredictions to a controller regulating the system dynamics. The method is\ngenerally applicable to any dynamic system representable in a state space.",
            "author": [
                "Joseph Park",
                "George Sugihara",
                "Gerald Pao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17324v1",
                "http://arxiv.org/pdf/2311.17324v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17307v1",
            "title": "RoKEPG: RoBERTa and Knowledge Enhancement for Prescription Generation of\n  Traditional Chinese Medicine",
            "updated": "2023-11-29T01:59:38Z",
            "published": "2023-11-29T01:59:38Z",
            "summary": "Traditional Chinese medicine (TCM) prescription is the most critical form of\nTCM treatment, and uncovering the complex nonlinear relationship between\nsymptoms and TCM is of great significance for clinical practice and assisting\nphysicians in diagnosis and treatment. Although there have been some studies on\nTCM prescription generation, these studies consider a single factor and\ndirectly model the symptom-prescription generation problem mainly based on\nsymptom descriptions, lacking guidance from TCM knowledge. To this end, we\npropose a RoBERTa and Knowledge Enhancement model for Prescription Generation\nof Traditional Chinese Medicine (RoKEPG). RoKEPG is firstly pre-trained by our\nconstructed TCM corpus, followed by fine-tuning the pre-trained model, and the\nmodel is guided to generate TCM prescriptions by introducing four classes of\nknowledge of TCM through the attention mask matrix. Experimental results on the\npublicly available TCM prescription dataset show that RoKEPG improves the F1\nmetric by about 2% over the baseline model with the best results.",
            "author": [
                "Hua Pu",
                "Jiacong Mi",
                "Shan Lu",
                "Jieyue He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17307v1",
                "http://arxiv.org/pdf/2311.17307v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17304v1",
            "title": "The Common Envelope Evolution Outcome. II. Short Orbital Period Hot\n  Subdwarf B Binaries Reveal a Clear Picture",
            "updated": "2023-11-29T01:31:12Z",
            "published": "2023-11-29T01:31:12Z",
            "summary": "The common envelope evolution (CEE) is vital in forming short orbital period\ncompact binaries. It covers many objects, such as double compact merging\nbinaries, type Ia supernovae progenitors, binary pulsars, and X-ray binaries.\nKnowledge about the common envelope (CE) eject efficiency still needs to be\nimproved, though progress has been made recently. Short orbital period hot\nsubdwarf B star plus white dwarf binaries are the most straightforward samples\nto constrain CEE physics. We apply the known orbital period-white dwarf\nrelation to constrain the sdB progenitor of seven sdB+WD binaries with a known\ninclination angle. The average value of the CE efficiency parameter is 0.32,\nwhich is consistent with previous studies. However, the CE efficiency might not\nbe a constant but is a function of the initial mass ratio based on\nwell-constrained sdB progenitor mass and evolutionary stage. Our results can be\nused as physical inputs for binary population synthesis simulations on related\nobjects. A similar method can also be applied to study other short orbital\nperiod WD binaries.",
            "author": [
                "Hongwei Ge",
                "Christopher A Tout",
                "Ronald F Webbink",
                "Xuefei Chen",
                "Arnab Sarkar",
                "Jiao Li",
                "Zhenwei Li",
                "Lifu Zhang",
                "Zhanwen Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17304v1",
                "http://arxiv.org/pdf/2311.17304v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17303v2",
            "title": "Enhancing the Performance of Neural Networks Through Causal Discovery\n  and Integration of Domain Knowledge",
            "updated": "2023-12-01T01:34:47Z",
            "published": "2023-11-29T01:25:00Z",
            "summary": "In this paper, we develop a generic methodology to encode hierarchical\ncausality structure among observed variables into a neural network in order to\nimprove its predictive performance. The proposed methodology, called\ncausality-informed neural network (CINN), leverages three coherent steps to\nsystematically map the structural causal knowledge into the layer-to-layer\ndesign of neural network while strictly preserving the orientation of every\ncausal relationship. In the first step, CINN discovers causal relationships\nfrom observational data via directed acyclic graph (DAG) learning, where causal\ndiscovery is recast as a continuous optimization problem to avoid the\ncombinatorial nature. In the second step, the discovered hierarchical causality\nstructure among observed variables is systematically encoded into neural\nnetwork through a dedicated architecture and customized loss function. By\ncategorizing variables in the causal DAG as root, intermediate, and leaf nodes,\nthe hierarchical causal DAG is translated into CINN with a one-to-one\ncorrespondence between nodes in the causal DAG and units in the CINN while\nmaintaining the relative order among these nodes. Regarding the loss function,\nboth intermediate and leaf nodes in the DAG graph are treated as target outputs\nduring CINN training so as to drive co-learning of causal relationships among\ndifferent types of nodes. As multiple loss components emerge in CINN, we\nleverage the projection of conflicting gradients to mitigate gradient\ninterference among the multiple learning tasks. Computational experiments\nacross a broad spectrum of UCI data sets demonstrate substantial advantages of\nCINN in predictive performance over other state-of-the-art methods. In\naddition, an ablation study underscores the value of integrating structural and\nquantitative causal knowledge in enhancing the neural network's predictive\nperformance incrementally.",
            "author": [
                "Xiaoge Zhang",
                "Xiao-Lin Wang",
                "Fenglei Fan",
                "Yiu-Ming Cheung",
                "Indranil Bose"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17303v2",
                "http://arxiv.org/pdf/2311.17303v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17943v1",
            "title": "LayerCollapse: Adaptive compression of neural networks",
            "updated": "2023-11-29T01:23:41Z",
            "published": "2023-11-29T01:23:41Z",
            "summary": "Handling the ever-increasing scale of contemporary deep learning and\ntransformer-based models poses a significant challenge. Although great strides\nhave been made in optimizing model compression techniques such as model\narchitecture search and knowledge distillation, the availability of data and\ncomputational resources remains a considerable hurdle for these optimizations.\nThis paper introduces LayerCollapse, a novel alternative adaptive model\ncompression methodology. LayerCollapse works by eliminating non-linearities\nwithin the network and collapsing two consecutive fully connected layers into a\nsingle linear transformation. This approach simultaneously reduces both the\nnumber of layers and the parameter count, thereby enhancing model efficiency.\nWe also introduce a compression aware regularizer, which compresses the model\nin alignment with the dataset quality and model expressiveness, consequently\nreducing overfitting across tasks. Our results demonstrate LayerCollapse's\neffective compression and regularization capabilities in multiple fine-grained\nclassification benchmarks, achieving up to 74% post training compression with\nminimal accuracy loss. We compare this method with knowledge distillation on\nthe same target network, showcasing a five-fold increase in computational\nefficiency and 8% improvement in overall accuracy on the ImageNet dataset.",
            "author": [
                "Soheil Zibakhsh Shabgahi",
                "Mohammad Soheil Shariff",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17943v1",
                "http://arxiv.org/pdf/2311.17943v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17302v1",
            "title": "Peer interaction facilitates co-construction of knowledge in quantum\n  mechanics",
            "updated": "2023-11-29T01:19:50Z",
            "published": "2023-11-29T01:19:50Z",
            "summary": "Collaborative learning with peers can lead to students learning from each\nother and solving physics problems correctly not only in situations in which\none student knows how to solve the problems but also when none of the students\ncan solve the problems alone. We define the rate of construction as the\npercentage of groups collaborating on problem-solving that solve the problem\ncorrectly out of all groups having at least one member who answered correctly\nand one incorrectly while solving the same problem individually first. We\ndefine the rate of co-construction on each problem as the percentage of\ncollaborating groups that answered it correctly if no student in the group\nindividually answered it correctly before the collaborative work. In this\nstudy, we investigated student learning measured by student performance on a\nvalidated quantum mechanics survey and rates of construction and\nco-construction of knowledge when students first worked individually after\nlecture-based instruction in relevant concepts and then worked with peers\nduring class without receiving any feedback from the course instructor. We find\nthat construction of knowledge consistently occurred at a high rate during peer\ncollaboration. However, rates of co-construction were more varied. High rates\nof co-construction were generally achieved when approximately half of the\nstudents knew the correct answers initially. We also conducted an analysis of\nsome of the survey questions that correlate with high rates of co-construction\nto gain some insight into what students converged on after peer interaction and\nwhat types of difficulties were reduced. Our findings can be valuable for\ninstructors who want to provide in-class and out-of-class opportunities for\npeer collaboration in their physics courses.",
            "author": [
                "Mary Jane Brundage",
                "Alysa Malespina",
                "Chandralekha Singh"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevPhysEducRes.19.020133",
                "http://arxiv.org/abs/2311.17302v1",
                "http://arxiv.org/pdf/2311.17302v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17293v1",
            "title": "Analyzing Query Optimizer Performance in the Presence and Absence of\n  Cardinality Estimates",
            "updated": "2023-11-29T00:39:38Z",
            "published": "2023-11-29T00:39:38Z",
            "summary": "Most query optimizers rely on cardinality estimates to determine optimal\nexecution plans. While traditional databases such as PostgreSQL, Oracle, and\nDb2 utilize many types of synopses -- including histograms, samples, and\nsketches -- recent main-memory databases like DuckDB and Heavy.AI often operate\nwith minimal or no estimates, yet their performance does not necessarily\nsuffer. To the best of our knowledge, no analytical comparison has been\nconducted between optimizers with and without cardinality estimates to\nunderstand their performance characteristics in different settings, such as\nindexed, non-indexed, and multi-threaded. In this paper, we present a\ncomparative analysis between optimizers that use cardinality estimates and\nthose that do not. We use the Join Order Benchmark (JOB) for our evaluation and\ntrue cardinalities as the baseline. Our investigation reveals that cardinality\nestimates have marginal impact in non-indexed settings. Meanwhile, when indexes\nare available, inaccurate estimates may lead to sub-optimal physical operators\n-- even with an optimal join order. Furthermore, the impact of cardinality\nestimates is less significant in highly-parallel main-memory databases.",
            "author": [
                "Asoke Datta",
                "Brian Tsan",
                "Yesdaulet Izenov",
                "Florin Rusu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17293v1",
                "http://arxiv.org/pdf/2311.17293v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17290v1",
            "title": "Ferroelectric domain nucleation and switching pathways in hafnium oxide",
            "updated": "2023-11-29T00:23:47Z",
            "published": "2023-11-29T00:23:47Z",
            "summary": "Nanoscale ferroelectrics that can be integrated into microelectronic\nfabrication processes are highly desirable for low-power computing and\nnon-volatile memory devices. However, scalable novel ferroelectric materials,\nsuch as hafnium oxide (HfO2), remain in a state of development, and a clear\nunderstanding of the effects of relevant compositional and processing\nparameters to control their ferroelectric properties and the actual\npolarization switching mechanisms are still under investigation. One key\nfundamental knowledge gap is the polarization switching pathway in\nferroelectric hafnia. To further our fundamental understanding of domain\nnucleation and switching, we have studied polarization switching pathways in\nHfO2-x thin films in real-time at the atomic scale using transmission electron\nmicroscopy. We employed differential phase contrast imaging that allows for the\nacquisition of both hafnium and oxygen atomic column signals and facilitates\nthe observation of relative movement of atomic columns between both\nsublattices. Our results demonstrate that the switching pathway involves a\ntransient tetragonal-like local structure, as oxygen ions shift in locations\nand remain within their parent hafnium polyhedra.",
            "author": [
                "Sebastian Calderon V",
                "Samantha T. Jaszewski",
                "Kyle P. Kelley",
                "Jon F. Ihlefeld",
                "Elizabeth Dickey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17290v1",
                "http://arxiv.org/pdf/2311.17290v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17284v1",
            "title": "Discrete-to-continuum limits of optimal transport with linear growth on\n  periodic graphs",
            "updated": "2023-11-28T23:55:59Z",
            "published": "2023-11-28T23:55:59Z",
            "summary": "We prove discrete-to-continuum convergence for dynamical optimal transport on\n$\\mathbb{Z}^d$-periodic graphs with energy density having linear growth at\ninfinity. This result provides an answer to a problem left open by Gladbach,\nKopfer, Maas, and Portinale (Calc Var Partial Differential Equations 62(5),\n2023), where the convergence behaviour of discrete boundary-value dynamical\ntransport problems is proved under the stronger assumption of superlinear\ngrowth. Our result extends the known literature to some important classes of\nexamples, such as scaling limits of 1-Wasserstein transport problems. Similarly\nto what happens in the quadratic case, the geometry of the graph plays a\ncrucial role in the structure of the limit cost function, as we discuss in the\nfinal part of this work, which includes some visual representations.",
            "author": [
                "Lorenzo Portinale",
                "Filippo Quattrocchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17284v1",
                "http://arxiv.org/pdf/2311.17284v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.NA",
                "math.AP",
                "math.NA",
                "49Q22, 49M25, 49J45, 65K10, 74Q10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17276v1",
            "title": "Machine Unlearning in Learned Databases: An Experimental Analysis",
            "updated": "2023-11-28T23:32:42Z",
            "published": "2023-11-28T23:32:42Z",
            "summary": "Machine learning models based on neural networks (NNs) are enjoying\never-increasing attention in the DB community. However, an important issue has\nbeen largely overlooked, namely the challenge of dealing with the highly\ndynamic nature of DBs, where data updates are fundamental, highly-frequent\noperations. Although some recent research has addressed the issues of\nmaintaining updated NN models in the presence of new data insertions, the\neffects of data deletions (a.k.a., \"machine unlearning\") remain a blind spot.\nWith this work, for the first time to our knowledge, we pose and answer the\nfollowing key questions: What is the effect of unlearning algorithms on\nNN-based DB models? How do these effects translate to effects on downstream DB\ntasks, such as selectivity estimation (SE), approximate query processing (AQP),\ndata generation (DG), and upstream tasks like data classification (DC)? What\nmetrics should we use to assess the impact and efficacy of unlearning\nalgorithms in learned DBs? Is the problem of machine unlearning in DBs\ndifferent from that of machine learning in DBs in the face of data insertions?\nIs the problem of machine unlearning for DBs different from unlearning in the\nML literature? what are the overhead and efficiency of unlearning algorithms?\nWhat is the sensitivity of unlearning on batching delete operations? If we have\na suitable unlearning algorithm, can we combine it with an algorithm handling\ndata insertions en route to solving the general adaptability/updatability\nrequirement in learned DBs in the face of both data inserts and deletes? We\nanswer these questions using a comprehensive set of experiments, various\nunlearning algorithms, a variety of downstream DB tasks, and an upstream task\n(DC), each with different NNs, and using a variety of metrics on a variety of\nreal datasets, making this also a first key step towards a benchmark for\nlearned DB unlearning.",
            "author": [
                "Meghdad Kurmanji",
                "Eleni Triantafillou",
                "Peter Triantafillou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17276v1",
                "http://arxiv.org/pdf/2311.17276v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17256v1",
            "title": "Pattern retrieval of traffic congestion using graph-based associations\n  of traffic domain-specific features",
            "updated": "2023-11-28T22:33:22Z",
            "published": "2023-11-28T22:33:22Z",
            "summary": "The fast-growing amount of traffic data brings many opportunities for\nrevealing more insightful information about traffic dynamics. However, it also\ndemands an effective database management system in which information retrieval\nis arguably an important feature. The ability to locate similar patterns in big\ndatasets potentially paves the way for further valuable analyses in traffic\nmanagement. This paper proposes a content-based retrieval system for\nspatiotemporal patterns of highway traffic congestion. There are two main\ncomponents in our framework, namely pattern representation and similarity\nmeasurement. To effectively interpret retrieval outcomes, the paper proposes a\ngraph-based approach (relation-graph) for the former component, in which\nfundamental traffic phenomena are encoded as nodes and their spatiotemporal\nrelationships as edges. In the latter component, the similarities between\ncongestion patterns are customizable with various aspects according to user\nexpectations. We evaluated the proposed framework by applying it to a dataset\nof hundreds of patterns with various complexities (temporally and spatially).\nThe example queries indicate the effectiveness of the proposed method, i.e. the\nobtained patterns present similar traffic phenomena as in the given examples.\nIn addition, the success of the proposed approach directly derives a new\nopportunity for semantic retrieval, in which expected patterns are described by\nadopting the relation-graph notion to associate fundamental traffic phenomena.",
            "author": [
                "Tin T. Nguyen",
                "Simeon C. Calvert",
                "Guopeng Li",
                "Hans van Lint"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17256v1",
                "http://arxiv.org/pdf/2311.17256v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17245v2",
            "title": "LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and\n  200+ FPS",
            "updated": "2023-12-04T07:30:36Z",
            "published": "2023-11-28T21:39:20Z",
            "summary": "Recent advancements in real-time neural rendering using point-based\ntechniques have paved the way for the widespread adoption of 3D\nrepresentations. However, foundational approaches like 3D Gaussian Splatting\ncome with a substantial storage overhead caused by growing the SfM points to\nmillions, often demanding gigabyte-level disk space for a single unbounded\nscene, posing significant scalability challenges and hindering the splatting\nefficiency.\n  To address this challenge, we introduce LightGaussian, a novel method\ndesigned to transform 3D Gaussians into a more efficient and compact format.\nDrawing inspiration from the concept of Network Pruning, LightGaussian\nidentifies Gaussians that are insignificant in contributing to the scene\nreconstruction and adopts a pruning and recovery process, effectively reducing\nredundancy in Gaussian counts while preserving visual effects. Additionally,\nLightGaussian employs distillation and pseudo-view augmentation to distill\nspherical harmonics to a lower degree, allowing knowledge transfer to more\ncompact representations while maintaining reflectance. Furthermore, we propose\na hybrid scheme, VecTree Quantization, to quantize all attributes, resulting in\nlower bitwidth representations with minimal accuracy losses.\n  In summary, LightGaussian achieves an averaged compression rate over 15x\nwhile boosting the FPS from 139 to 215, enabling an efficient representation of\ncomplex scenes on Mip-NeRF 360, Tank and Temple datasets.\n  Project website: https://lightgaussian.github.io/",
            "author": [
                "Zhiwen Fan",
                "Kevin Wang",
                "Kairun Wen",
                "Zehao Zhu",
                "Dejia Xu",
                "Zhangyang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17245v2",
                "http://arxiv.org/pdf/2311.17245v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17234v1",
            "title": "Promise Clique Homology on weighted graphs is $\\text{QMA}_1$-hard and\n  contained in $\\text{QMA}$",
            "updated": "2023-11-28T21:15:30Z",
            "published": "2023-11-28T21:15:30Z",
            "summary": "We study the complexity of a classic problem in computational topology, the\nhomology problem: given a description of some space $X$ and an integer $k$,\ndecide if $X$ contains a $k$-dimensional hole. The setting and statement of the\nhomology problem are completely classical, yet we find that the complexity is\ncharacterized by quantum complexity classes. Our result can be seen as an\naspect of a connection between homology and supersymmetric quantum mechanics\n[Wit82].\n  We consider clique complexes, motivated by the practical application of\ntopological data analysis (TDA). The clique complex of a graph is the\nsimplicial complex formed by declaring every $k+1$-clique in the graph to be a\n$k$-simplex. Our main result is that deciding whether the clique complex of a\nweighted graph has a hole or not, given a suitable promise, is\n$\\text{QMA}_1$-hard and contained in $\\text{QMA}$.\n  Our main innovation is a technique to lower bound the eigenvalues of the\ncombinatorial Laplacian operator. For this, we invoke a tool from algebraic\ntopology known as spectral sequences. In particular, we exploit a connection\nbetween spectral sequences and Hodge theory [For94]. Spectral sequences will\nplay a role analogous to perturbation theory for combinatorial Laplacians. In\naddition, we develop the simplicial surgery technique used in prior work\n[CK22].\n  Our result provides some suggestion that the quantum TDA algorithm [LGZ16]\ncannot be dequantized. More broadly, we hope that our results will open up new\npossibilities for quantum advantage in topological data analysis.",
            "author": [
                "Robbie King",
                "Tamara Kohler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17234v1",
                "http://arxiv.org/pdf/2311.17234v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17200v1",
            "title": "Greybox fuzzing time-intensive programs",
            "updated": "2023-11-28T20:10:38Z",
            "published": "2023-11-28T20:10:38Z",
            "summary": "We examine (directed) greybox fuzzing from a geometrical perspective, viewing\ndissimilarities on inputs and on control flow graphs (with dynamical\nstatistics) as primitive objects of interest. We prototype and evaluate\nGoExploreFuzz, a greybox fuzzer for time-intensive programs that incorporates\nthis perspective. The results indicate useful capabilities for greybox fuzzing\nthat have hitherto been underutilized, notably quantifying the diversity of\npaths and autonomously tuning the \"bandwidth\" of mutations.",
            "author": [
                "Steve Huntsman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17200v1",
                "http://arxiv.org/pdf/2311.17200v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "D.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17193v1",
            "title": "El tequila para consumo nacional como una ventana de oportunidades para\n  el peque\u00f1o productor agavero",
            "updated": "2023-11-28T19:54:11Z",
            "published": "2023-11-28T19:54:11Z",
            "summary": "The objective of this research was to determine the degree of knowledge that\nthe inhabitants of the Guadalajara Metropolitan Area (made up of the\nmunicipalities Guadalajara, Tlajomulco de Z\\'u\\~niga, Tlaquepaque, Zapopan and\nTonal\\'a) had regarding tequila and the brands produced in Los Altos of\nJalisco. For this, a survey consisting of five questions was designed, which\nwas applied in the central square, center or z\\'ocalo of each municipality. The\nresults show that the big brands, when acquired by international companies,\nfocused their attention on capturing the consumer in international markets,\nsince the prices of the same products that they export have been out of the\npocket of those who like That drink, so it could be considered that the big\nbrands, have left the national market a little behind, of course they did not\nabandon it completely, but it stopped being their main objective. Therefore, it\ncan be concluded that the national market is the window of opportunity to join\nthe small and still unknown producers to work together and in a grouped way,\nthey are able to standardize a series of products that being of the same\nquality and same packaging, they can cover the national market, and perhaps, in\nthe future, become a large company distributed throughout the territory and\nbegin the export process only with national capital.",
            "author": [
                "Guillermo Jos\u00e9 Navarro del Toro"
            ],
            "link": [
                "http://dx.doi.org/10.23913/ricea.v10i19.159",
                "http://arxiv.org/abs/2311.17193v1",
                "http://arxiv.org/pdf/2311.17193v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17190v1",
            "title": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play",
            "updated": "2023-11-28T19:34:40Z",
            "published": "2023-11-28T19:34:40Z",
            "summary": "Recent advances in Competitive Self-Play (CSP) have achieved, or even\nsurpassed, human level performance in complex game environments such as Dota 2\nand StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).\nOne core component of these methods relies on creating a pool of learning\nagents -- consisting of the Main Agent, past versions of this agent, and\nExploiter Agents -- where Exploiter Agents learn counter-strategies to the Main\nAgents. A key drawback of these approaches is the large computational cost and\nphysical time that is required to train the system, making them impractical to\ndeploy in highly iterative real-life settings such as video game productions.\nIn this paper, we propose the Minimax Exploiter, a game theoretic approach to\nexploiting Main Agents that leverages knowledge of its opponents, leading to\nsignificant increases in data efficiency. We validate our approach in a\ndiversity of settings, including simple turn based games, the arcade learning\nenvironment, and For Honor, a modern video game. The Minimax Exploiter\nconsistently outperforms strong baselines, demonstrating improved stability and\ndata efficiency, leading to a robust CSP-MARL method that is both flexible and\neasy to deploy.",
            "author": [
                "Daniel Bairamian",
                "Philippe Marcotte",
                "Joshua Romoff",
                "Gabriel Robert",
                "Derek Nowrouzezahrai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17190v1",
                "http://arxiv.org/pdf/2311.17190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17189v1",
            "title": "TorchAmi: Generalized CPU/GPU Implementation of Algorithmic Matsubara\n  Integration",
            "updated": "2023-11-28T19:33:51Z",
            "published": "2023-11-28T19:33:51Z",
            "summary": "We present torchami, an advanced implementation of algorithmic Matsubara\nintegration (AMI) that utilizes pytorch as a backend to provide easy\nparallelization and GPU support. AMI is a tool for analytically resolving the\nsequence of nested Matsubara integrals that arise in virtually all Feynman\nperturbative expansions. In this implementation we present a new AMI algorithm\nthat creates a more natural symbolic representation of the Feynman integrands.\nIn addition, we include peripheral tools that allow for import and labelling of\nsimple graph structures and conversion to torchami input. The code is written\nin c++ with python bindings provided.",
            "author": [
                "M. D. Burke",
                "J. P. F. LeBlanc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17189v1",
                "http://arxiv.org/pdf/2311.17189v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "cond-mat.other",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17187v2",
            "title": "Investing in the Quantum Future : State of Play and Way Forward for\n  Quantum Venture Capital",
            "updated": "2023-12-01T08:17:08Z",
            "published": "2023-11-28T19:31:51Z",
            "summary": "Building on decades of fundamental research, new applications of Quantum\nScience have started to emerge in the fields of computing, sensing and\nnetworks. In the current phase of deployment, in which quantum technology is\nnot yet in routine use but is still transitioning out of the laboratory,\nVenture Capital (VC) is critical. In association with public funding programs,\nVC supports startups born in academic institutions and has a role to play in\nstructuring the priorities of the ecosystem, guiding it towards applications\nwith the greatest impact on society. This paper illustrates this thesis with a\ncase-study: the experience of the first dedicated quantum fund, Quantonation I,\nchronicling its impacts on the production of scientific knowledge, job creation\nand funding of the industry. The paper introduces concepts to support the\nemergence of new startups and advocates for funding of scale-up quantum\ncompanies. The paper concludes with proposals to improve the impact of the\nindustry by taking steps to better involve society-at-large and with a call for\ncollaboration on projects focused on the applications with a large societal\nbenefit.",
            "author": [
                "Christophe Jurczak"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17187v2",
                "http://arxiv.org/pdf/2311.17187v2"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17938v1",
            "title": "Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP\n  Limitations",
            "updated": "2023-11-28T19:24:07Z",
            "published": "2023-11-28T19:24:07Z",
            "summary": "Active recognition, which allows intelligent agents to explore observations\nfor better recognition performance, serves as a prerequisite for various\nembodied AI tasks, such as grasping, navigation and room arrangements. Given\nthe evolving environment and the multitude of object classes, it is impractical\nto include all possible classes during the training stage. In this paper, we\naim at advancing active open-vocabulary recognition, empowering embodied agents\nto actively perceive and classify arbitrary objects. However, directly adopting\nrecent open-vocabulary classification models, like Contrastive Language Image\nPretraining (CLIP), poses its unique challenges. Specifically, we observe that\nCLIP's performance is heavily affected by the viewpoint and occlusions,\ncompromising its reliability in unconstrained embodied perception scenarios.\nFurther, the sequential nature of observations in agent-environment\ninteractions necessitates an effective method for integrating features that\nmaintains discriminative strength for open-vocabulary classification. To\naddress these issues, we introduce a novel agent for active open-vocabulary\nrecognition. The proposed method leverages inter-frame and inter-concept\nsimilarities to navigate agent movements and to fuse features, without relying\non class-specific knowledge. Compared to baseline CLIP model with 29.6%\naccuracy on ShapeNet dataset, the proposed agent could achieve 53.3% accuracy\nfor open-vocabulary recognition, without any fine-tuning to the equipped CLIP\nmodel. Additional experiments conducted with the Habitat simulator further\naffirm the efficacy of our method.",
            "author": [
                "Lei Fan",
                "Jianxiong Zhou",
                "Xiaoying Xing",
                "Ying Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17938v1",
                "http://arxiv.org/pdf/2311.17938v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17182v1",
            "title": "New recursive constructions of amoebas and their balancing number",
            "updated": "2023-11-28T19:22:50Z",
            "published": "2023-11-28T19:22:50Z",
            "summary": "The definition of amoeba graphs is based on iterative \\emph{feasible\nedge-replacements}, where, at each step, an edge from the graph is removed and\nplaced in an available spot in a way that the resulting graph is isomorphic to\nthe original graph. Broadly speaking, amoebas are graphs that, by means of a\nchain of feasible edge-replacements, can be transformed into any other copy of\nitself on a given vertex set (which is defined according to whether these are\nlocal or global amoebas). Global amoebas were born as examples of\n\\emph{balanceable} graphs, which are graphs that appear with half of their\nedges in each color in any $2$-edge coloring of a large enough complete graph\nwith a sufficient amount of edges in each color. The least amount of edges\nrequired in each color is called the \\emph{balancing number} of $G$. In a work\nby Caro et al., an infinite family of global amoeba trees with arbitrarily\nlarge maximum degree is presented, and the question if they were also local\namoebas is raised. In this paper, we provide a recursive construction to\ngenerate very diverse infinite families of local and global amoebas, by which\nnot only this question is answered positively, it also yields an efficient\nalgorithm that, given any copy of the graph on the same vertex set, provides a\nchain of feasible edge-replacements that one can perform in order to move the\ngraph into the aimed copy. All results are illustrated by applying them to\nthree different families of local amoebas, including the Fibonacci-type trees.\nConcerning the balancing number of a global amoeba $G$, we are able to express\nit in terms of the extremal number of a class of subgraphs of $G$. By means of\nthis, we give a general lower bound for the balancing number of a global amoeba\n$G$, and we provide linear (in terms of order) lower and upper bounds for the\nbalancing number of our three case studies.",
            "author": [
                "Laura Eslava",
                "Adriana Hansberg",
                "Tonatiuh Matos Wiederhold",
                "Denae Ventura"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17182v1",
                "http://arxiv.org/pdf/2311.17182v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05D99"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17170v1",
            "title": "Using the Conceptual Survey of Electricity and Magnetism to investigate\n  progression in student understanding from introductory to advanced levels",
            "updated": "2023-11-28T19:04:17Z",
            "published": "2023-11-28T19:04:17Z",
            "summary": "The Conceptual Survey of Electricity and Magnetism (CSEM) is a\nmultiple-choice survey that contains a variety of electricity and magnetism\nconcepts from Coulomb's law to Faraday's law at the level of introductory\nphysics used to help inform instructors of student mastery of those concepts.\nPrior studies suggest that many concepts on the survey are challenging for\nintroductory physics students and the average student scores after traditional\ninstruction are low. The research presented here investigates the progression\nin student understanding on the CSEM. We compare the performance of students in\nintroductory and advanced level physics courses to understand the evolution of\nstudent understanding of concepts covered in the CSEM after traditional\nlecture-based instruction. We find that on all CSEM questions on which less\nthan 50% of the introductory physics students answered a question correctly\nafter instruction, less than two thirds of the upper-level undergraduate\nstudents provided the correct response after traditional instruction. We also\nanalyzed the CSEM data from graduate students for benchmarking purposes. We\ndiscuss the CSEM questions that remain challenging and the common alternative\nconceptions among upper-level students. The findings presented here at least\npartly point to the fact that traditional instruction in upper-level courses\nwhich typically focuses primarily on quantitative problem solving and\nincentivizes use of algorithmic approaches is not effective for helping\nstudents develop a solid understanding of these concepts. However, it is\nimportant for helping students integrate conceptual and quantitative aspects of\nlearning in order to build a robust knowledge structure of basic concepts in\nelectricity and magnetism.",
            "author": [
                "Alexandru Maries",
                "Mary Jane Brundage",
                "Chandralekha Singh"
            ],
            "link": [
                "http://dx.doi.org/10.1103/PhysRevPhysEducRes.18.020114",
                "http://arxiv.org/abs/2311.17170v1",
                "http://arxiv.org/pdf/2311.17170v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17146v1",
            "title": "Calabi-Yau Four/Five/Six-folds as $\\mathbb{P}^n_\\textbf{w}$\n  Hypersurfaces: Machine Learning, Approximation, and Generation",
            "updated": "2023-11-28T19:00:00Z",
            "published": "2023-11-28T19:00:00Z",
            "summary": "Calabi-Yau four-folds may be constructed as hypersurfaces in weighted\nprojective spaces of complex dimension 5 defined via weight systems of 6\nweights. In this work, neural networks were implemented to learn the Calabi-Yau\nHodge numbers from the weight systems, where gradient saliency and symbolic\nregression then inspired a truncation of the Landau-Ginzburg model formula for\nthe Hodge numbers of any dimensional Calabi-Yau constructed in this way. The\napproximation always provides a tight lower bound, is shown to be dramatically\nquicker to compute (with compute times reduced by up to four orders of\nmagnitude), and gives remarkably accurate results for systems with large\nweights. Additionally, complementary datasets of weight systems satisfying the\nnecessary but insufficient conditions for transversality were constructed,\nincluding considerations of the IP, reflexivity, and intradivisibility\nproperties. Overall producing a classification of this weight system landscape,\nfurther confirmed with machine learning methods. Using the knowledge of this\nclassification, and the properties of the presented approximation, a novel\ndataset of transverse weight systems consisting of 7 weights was generated for\na sum of weights $\\leq 200$; producing a new database of Calabi-Yau five-folds,\nwith their respective topological properties computed. Further to this an\nequivalent database of candidate Calabi-Yau six-folds was generated with\napproximated Hodge numbers.",
            "author": [
                "Edward Hirst",
                "Tancredi Schettini Gherardini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17146v1",
                "http://arxiv.org/pdf/2311.17146v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math.AG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17058v1",
            "title": "Panoptic Video Scene Graph Generation",
            "updated": "2023-11-28T18:59:57Z",
            "published": "2023-11-28T18:59:57Z",
            "summary": "Towards building comprehensive real-world visual perception systems, we\npropose and study a new problem called panoptic scene graph generation (PVSG).\nPVSG relates to the existing video scene graph generation (VidSGG) problem,\nwhich focuses on temporal interactions between humans and objects grounded with\nbounding boxes in videos. However, the limitation of bounding boxes in\ndetecting non-rigid objects and backgrounds often causes VidSGG to miss key\ndetails crucial for comprehensive video understanding. In contrast, PVSG\nrequires nodes in scene graphs to be grounded by more precise, pixel-level\nsegmentation masks, which facilitate holistic scene understanding. To advance\nresearch in this new area, we contribute the PVSG dataset, which consists of\n400 videos (289 third-person + 111 egocentric videos) with a total of 150K\nframes labeled with panoptic segmentation masks as well as fine, temporal scene\ngraphs. We also provide a variety of baseline methods and share useful design\npractices for future work.",
            "author": [
                "Jingkang Yang",
                "Wenxuan Peng",
                "Xiangtai Li",
                "Zujin Guo",
                "Liangyu Chen",
                "Bo Li",
                "Zheng Ma",
                "Kaiyang Zhou",
                "Wayne Zhang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17058v1",
                "http://arxiv.org/pdf/2311.17058v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17049v1",
            "title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced\n  Training",
            "updated": "2023-11-28T18:55:42Z",
            "published": "2023-11-28T18:55:42Z",
            "summary": "Contrastive pretraining of image-text foundation models, such as CLIP,\ndemonstrated excellent zero-shot performance and improved robustness on a wide\nrange of downstream tasks. However, these models utilize large\ntransformer-based encoders with significant memory and latency overhead which\npose challenges for deployment on mobile devices. In this work, we introduce\nMobileCLIP -- a new family of efficient image-text models optimized for runtime\nperformance along with a novel and efficient training approach, namely\nmulti-modal reinforced training. The proposed training approach leverages\nknowledge transfer from an image captioning model and an ensemble of strong\nCLIP encoders to improve the accuracy of efficient models. Our approach avoids\ntrain-time compute overhead by storing the additional knowledge in a reinforced\ndataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for\nzero-shot classification and retrieval tasks on several datasets. Our\nMobileCLIP-S2 variant is 2.3$\\times$ faster while more accurate compared to\nprevious best CLIP model based on ViT-B/16. We further demonstrate the\neffectiveness of our multi-modal reinforced training by training a CLIP model\nbased on ViT-B/16 image backbone and achieving +2.9% average performance\nimprovement on 38 evaluation benchmarks compared to the previous best.\nMoreover, we show that the proposed approach achieves 10$\\times$-1000$\\times$\nimproved learning efficiency when compared with non-reinforced CLIP training.",
            "author": [
                "Pavan Kumar Anasosalu Vasu",
                "Hadi Pouransari",
                "Fartash Faghri",
                "Raviteja Vemulapalli",
                "Oncel Tuzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17049v1",
                "http://arxiv.org/pdf/2311.17049v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17040v1",
            "title": "Rumors with Changing Credibility",
            "updated": "2023-11-28T18:52:38Z",
            "published": "2023-11-28T18:52:38Z",
            "summary": "Randomized rumor spreading processes diffuse information on an undirected\ngraph and have been widely studied. In this work, we present a generic\nframework for analyzing a broad class of such processes on regular graphs. Our\nanalysis is protocol-agnostic, as it only requires the expected proportion of\nnewly informed vertices in each round to be bounded, and a natural negative\ncorrelation property.\n  This framework allows us to analyze various protocols, including PUSH, PULL,\nand PUSH-PULL, thereby extending prior research. Unlike previous work, our\nframework accommodates message failures at any time $t\\geq 0$ with a\nprobability of $1-q(t)$, where the credibility $q(t)$ is any function of time.\nThis enables us to model real-world scenarios in which the transmissibility of\nrumors may fluctuate, as seen in the spread of ``fake news'' and viruses.\nAdditionally, our framework is sufficiently broad to cover dynamic graphs.",
            "author": [
                "Charlotte Out",
                "Nicol\u00e1s Rivera",
                "Thomas Sauerwald",
                "John Sylvester"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17040v1",
                "http://arxiv.org/pdf/2311.17040v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.DC",
                "math.CO",
                "math.PR",
                "05C85, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17134v2",
            "title": "GlycoNMR: Dataset and benchmarks for NMR chemical shift prediction of\n  carbohydrates with graph neural networks",
            "updated": "2023-11-30T02:06:29Z",
            "published": "2023-11-28T18:51:19Z",
            "summary": "Molecular representation learning (MRL) is a powerful tool for bridging the\ngap between machine learning and chemical sciences, as it converts molecules\ninto numerical representations while preserving their chemical features. These\nencoded representations serve as a foundation for various downstream\nbiochemical studies, including property prediction and drug design. MRL has had\ngreat success with proteins and general biomolecule datasets. Yet, in the\ngrowing sub-field of glycoscience (the study of carbohydrates, where longer\ncarbohydrates are also called glycans), MRL methods have been barely explored.\nThis under-exploration can be primarily attributed to the limited availability\nof comprehensive and well-curated carbohydrate-specific datasets and a lack of\nMachine learning (ML) pipelines specifically tailored to meet the unique\nproblems presented by carbohydrate data. Since interpreting and annotating\ncarbohydrate-specific data is generally more complicated than protein data,\ndomain experts are usually required to get involved. The existing MRL methods,\npredominately optimized for proteins and small biomolecules, also cannot be\ndirectly used in carbohydrate applications without special modifications. To\naddress this challenge, accelerate progress in glycoscience, and enrich the\ndata resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains\ntwo laboriously curated datasets with 2,609 carbohydrate structures and 211,543\nannotated nuclear magnetic resonance (NMR) chemical shifts for precise\natomic-level prediction. We tailored carbohydrate-specific features and adapted\nexisting MRL models to tackle this problem effectively. For illustration, we\nbenchmark four modified MRL models on our new datasets.",
            "author": [
                "Zizhang Chen",
                "Ryan Paul Badman",
                "Lachele Foley",
                "Robert Woods",
                "Pengyu Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17134v2",
                "http://arxiv.org/pdf/2311.17134v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17037v1",
            "title": "Concurrent Stochastic Lossy Channel Games",
            "updated": "2023-11-28T18:49:14Z",
            "published": "2023-11-28T18:49:14Z",
            "summary": "Concurrent stochastic games are an important formalism for the rational\nverification of probabilistic multi-agent systems, which involves verifying\nwhether a temporal logic property is satisfied in some or all game-theoretic\nequilibria of such systems. In this work, we study the rational verification of\nprobabilistic multi-agent systems where agents can cooperate by communicating\nover unbounded lossy channels. To model such systems, we present concurrent\nstochastic lossy channel games (CSLCG) and employ an equilibrium concept from\ncooperative game theory known as the core, which is the most fundamental and\nwidely studied cooperative equilibrium concept. Our main contribution is\ntwofold. First, we show that the rational verification problem is undecidable\nfor systems whose agents have almost-sure LTL objectives. Second, we provide a\ndecidable fragment of such a class of objectives that subsumes almost-sure\nreachability and safety. Our techniques involve reductions to solving\ninfinite-state zero-sum games with conjunctions of qualitative objectives. To\nthe best of our knowledge, our result represents the first decidability result\non the rational verification of stochastic multi-agent systems on infinite\narenas.",
            "author": [
                "Daniel Stan",
                "Muhammad Najib",
                "Anthony Widjaja Lin",
                "Parosh Aziz Abdulla"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17037v1",
                "http://arxiv.org/pdf/2311.17037v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17036v1",
            "title": "The Aizenbud-Lapid binary operation for symmetrizable Cartan types",
            "updated": "2023-11-28T18:47:52Z",
            "published": "2023-11-28T18:47:52Z",
            "summary": "Aizenbud and Lapid recently introduced a binary operation on the crystal\ngraph $B(-\\infty)$ associated to a symmetric Cartan matrix. We extend their\nconstruction to symmetrizable Cartan matrices and strengthen a cancellation\nproperty of the binary operation.",
            "author": [
                "Markus Kleinau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17036v1",
                "http://arxiv.org/pdf/2311.17036v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17035v1",
            "title": "Scalable Extraction of Training Data from (Production) Language Models",
            "updated": "2023-11-28T18:47:03Z",
            "published": "2023-11-28T18:47:03Z",
            "summary": "This paper studies extractable memorization: training data that an adversary\ncan efficiently extract by querying a machine learning model without prior\nknowledge of the training dataset. We show an adversary can extract gigabytes\nof training data from open-source language models like Pythia or GPT-Neo,\nsemi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing\ntechniques from the literature suffice to attack unaligned models; in order to\nattack the aligned ChatGPT, we develop a new divergence attack that causes the\nmodel to diverge from its chatbot-style generations and emit training data at a\nrate 150x higher than when behaving properly. Our methods show practical\nattacks can recover far more data than previously thought, and reveal that\ncurrent alignment techniques do not eliminate memorization.",
            "author": [
                "Milad Nasr",
                "Nicholas Carlini",
                "Jonathan Hayase",
                "Matthew Jagielski",
                "A. Feder Cooper",
                "Daphne Ippolito",
                "Christopher A. Choquette-Choo",
                "Eric Wallace",
                "Florian Tram\u00e8r",
                "Katherine Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17035v1",
                "http://arxiv.org/pdf/2311.17035v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17021v1",
            "title": "Optimal Categorical Instrumental Variables",
            "updated": "2023-11-28T18:20:05Z",
            "published": "2023-11-28T18:20:05Z",
            "summary": "This paper discusses estimation with a categorical instrumental variable in\nsettings with potentially few observations per category. The proposed\ncategorical instrumental variable estimator (CIV) leverages a regularization\nassumption that implies existence of a latent categorical variable with fixed\nfinite support achieving the same first stage fit as the observed instrument.\nIn asymptotic regimes that allow the number of observations per category to\ngrow at arbitrary small polynomial rate with the sample size, I show that when\nthe cardinality of the support of the optimal instrument is known, CIV is\nroot-n asymptotically normal, achieves the same asymptotic variance as the\noracle IV estimator that presumes knowledge of the optimal instrument, and is\nsemiparametrically efficient under homoskedasticity. Under-specifying the\nnumber of support points reduces efficiency but maintains asymptotic normality.",
            "author": [
                "Thomas Wiemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17021v1",
                "http://arxiv.org/pdf/2311.17021v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17010v1",
            "title": "Node Connectivity Augmentation of Highly Connected Graphs",
            "updated": "2023-11-28T18:04:17Z",
            "published": "2023-11-28T18:04:17Z",
            "summary": "Node-connectivity augmentation is a fundamental network design problem. We\nare given a $k$-node connected graph $G$ together with an additional set of\nlinks, and the goal is to add a cheap subset of links to $G$ to make it\n$(k+1)$-node connected.\n  In this work, we characterize completely the computational complexity status\nof the problem, by showing hardness for all values of $k$ which were not\naddressed previously in the literature.\n  We then focus on $k$-node connectivity augmentation for $k=n-4$, which\ncorresponds to the highest value of $k$ for which the problem is NP-hard. We\nimprove over the previously best known approximation bounds for this problem,\nby developing a $\\frac{3}{2}$-approximation algorithm for the weighted setting,\nand a $\\frac{4}{3}$-approximation algorithm for the unweighted setting.",
            "author": [
                "Waldo Galvez",
                "Dylan Hyatt-Denesik",
                "Afrouz Jabal Ameli",
                "Laura Sanita"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17010v1",
                "http://arxiv.org/pdf/2311.17010v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17007v1",
            "title": "Computational Hypergraph Discovery, a Gaussian Process framework for\n  connecting the dots",
            "updated": "2023-11-28T18:02:06Z",
            "published": "2023-11-28T18:02:06Z",
            "summary": "Most scientific challenges can be framed into one of the following three\nlevels of complexity of function approximation. Type 1: Approximate an unknown\nfunction given input/output data. Type 2: Consider a collection of variables\nand functions, some of which are unknown, indexed by the nodes and hyperedges\nof a hypergraph (a generalized graph where edges can connect more than two\nvertices). Given partial observations of the variables of the hypergraph\n(satisfying the functional dependencies imposed by its structure), approximate\nall the unobserved variables and unknown functions. Type 3: Expanding on Type\n2, if the hypergraph structure itself is unknown, use partial observations of\nthe variables of the hypergraph to discover its structure and approximate its\nunknown functions. While most Computational Science and Engineering and\nScientific Machine Learning challenges can be framed as Type 1 and Type 2\nproblems, many scientific problems can only be categorized as Type 3. Despite\ntheir prevalence, these Type 3 challenges have been largely overlooked due to\ntheir inherent complexity. Although Gaussian Process (GP) methods are sometimes\nperceived as well-founded but old technology limited to Type 1 curve fitting,\ntheir scope has recently been expanded to Type 2 problems. In this paper, we\nintroduce an interpretable GP framework for Type 3 problems, targeting the\ndata-driven discovery and completion of computational hypergraphs. Our approach\nis based on a kernel generalization of Row Echelon Form reduction from linear\nsystems to nonlinear ones and variance-based analysis. Here, variables are\nlinked via GPs and those contributing to the highest data variance unveil the\nhypergraph's structure. We illustrate the scope and efficiency of the proposed\napproach with applications to (algebraic) equation discovery, network discovery\n(gene pathways, chemical, and mechanical) and raw data analysis.",
            "author": [
                "Th\u00e9o Bourdais",
                "Pau Batlle",
                "Xianjin Yang",
                "Ricardo Baptista",
                "Nicolas Rouquette",
                "Houman Owhadi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17007v1",
                "http://arxiv.org/pdf/2311.17007v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.NA",
                "cs.SI",
                "math.NA",
                "stat.ML",
                "62A09, 62H22, 65S05, 90C35, 94C15, 46E22, 62J02, 15A83, 62D20, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17001v1",
            "title": "New Approximation Bounds for Small-Set Vertex Expansion",
            "updated": "2023-11-28T17:57:24Z",
            "published": "2023-11-28T17:57:24Z",
            "summary": "The vertex expansion of the graph is a fundamental graph parameter. Given a\ngraph $G=(V,E)$ and a parameter $\\delta \\in (0,1/2]$, its $\\delta$-Small-Set\nVertex Expansion (SSVE) is defined as \\[ \\min_{S : |S| = \\delta |V|}\n\\frac{|{\\partial^V(S)}|}{ \\min \\{ |S|, |S^c| \\} } \\] where $\\partial^V(S)$ is\nthe vertex boundary of a set $S$. The SSVE~problem, in addition to being of\nindependent interest as a natural graph partitioning problem, is also of\ninterest due to its connections to the Strong Unique Games problem. We give a\nrandomized algorithm running in time $n^{{\\sf poly}(1/\\delta)}$, which outputs\na set $S$ of size $\\Theta(\\delta n)$, having vertex expansion at most \\[\n\\max\\left(O(\\sqrt{\\phi^* \\log d \\log (1/\\delta)}) ,\n\\tilde{O}(d\\log^2(1/\\delta)) \\cdot \\phi^* \\right), \\] where $d$ is the largest\nvertex degree of the graph, and $\\phi^*$ is the optimal $\\delta$-SSVE. The\nprevious best-known guarantees for this were the bi-criteria bounds of\n$\\tilde{O}(1/\\delta)\\sqrt{\\phi^* \\log d}$ and $\\tilde{O}(1/\\delta)\\phi^*\n\\sqrt{\\log n}$ due to Louis-Makarychev [TOC'16].\n  Our algorithm uses the basic SDP relaxation of the problem augmented with\n${\\rm poly}(1/\\delta)$ rounds of the Lasserre/SoS hierarchy. Our rounding\nalgorithm is a combination of the rounding algorithms of Raghavendra-Tan\n[SODA'12] and Austrin-Benabbas-Georgiou [SODA'13]. A key component of our\nanalysis is novel Gaussian rounding lemma for hyperedges which might be of\nindependent interest.",
            "author": [
                "Suprovat Ghoshal",
                "Anand Louis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17001v1",
                "http://arxiv.org/pdf/2311.17001v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16996v1",
            "title": "Goal-conditioned Offline Planning from Curious Exploration",
            "updated": "2023-11-28T17:48:18Z",
            "published": "2023-11-28T17:48:18Z",
            "summary": "Curiosity has established itself as a powerful exploration strategy in deep\nreinforcement learning. Notably, leveraging expected future novelty as\nintrinsic motivation has been shown to efficiently generate exploratory\ntrajectories, as well as a robust dynamics model. We consider the challenge of\nextracting goal-conditioned behavior from the products of such unsupervised\nexploration techniques, without any additional environment interaction. We find\nthat conventional goal-conditioned reinforcement learning approaches for\nextracting a value function and policy fall short in this difficult offline\nsetting. By analyzing the geometry of optimal goal-conditioned value functions,\nwe relate this issue to a specific class of estimation artifacts in learned\nvalues. In order to mitigate their occurrence, we propose to combine\nmodel-based planning over learned value landscapes with a graph-based value\naggregation scheme. We show how this combination can correct both local and\nglobal artifacts, obtaining significant improvements in zero-shot goal-reaching\nperformance across diverse simulated environments.",
            "author": [
                "Marco Bagatella",
                "Georg Martius"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16996v1",
                "http://arxiv.org/pdf/2311.16996v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16987v1",
            "title": "A Non-Archimedean Approach to Stratifications",
            "updated": "2023-11-28T17:39:33Z",
            "published": "2023-11-28T17:39:33Z",
            "summary": "These are notes from a mini-course about the main results of\narXiv:2206.03438: I explain how, using suitable valued fields, one obtains a\nnatural notion of canonical stratifications (of e.g. algebraic subsets of\n$\\mathbb{R}^n$). I also explain how the same techniques yield more invariants\nof singularities, and I present an application to Poincar\\'e series. While some\nrudimentary knowledge of model theory is useful, the notes should also be\naccessible without such knowledge. In particular, they contain an introduction\nto the non-standard analysis needed for this approach.",
            "author": [
                "Immanuel Halupczok"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16987v1",
                "http://arxiv.org/pdf/2311.16987v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "14B05, 32S60, 12J25, 03C98, 03H05, 14B20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16981v1",
            "title": "Temporal networks with node-specific memory: unbiased inference of\n  transition probabilities, relaxation times and structural breaks",
            "updated": "2023-11-28T17:32:43Z",
            "published": "2023-11-28T17:32:43Z",
            "summary": "One of the main challenges in the study of time-varying networks is the\ninterplay of memory effects with structural heterogeneity. In particular,\ndifferent nodes and dyads can have very different statistical properties in\nterms of both link formation and link persistence, leading to a superposition\nof typical timescales, sub-optimal parametrizations and substantial estimation\nbiases. Here we develop an unbiased maximum-entropy framework to study\nempirical network trajectories by controlling for the observed structural\nheterogeneity and local link persistence simultaneously. An exact mapping to a\nheterogeneous version of the one-dimensional Ising model leads to an analytic\nsolution that rigorously disentangles the hidden variables that jointly\ndetermine both static and temporal properties. Additionally, model selection\nvia likelihood maximization identifies the most parsimonious structural level\n(either global, node-specific or dyadic) accounting for memory effects. As we\nillustrate on a real-world social network, this method enables an improved\nestimation of dyadic transition probabilities, relaxation times and structural\nbreaks between dynamical regimes. In the resulting picture, the graph follows a\ngeneralized configuration model with given degrees and given time-persisting\ndegrees, undergoing transitions between empirically identifiable stationary\nregimes.",
            "author": [
                "Giulio Virginio Clemente",
                "Claudio J. Tessone",
                "Diego Garlaschelli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16981v1",
                "http://arxiv.org/pdf/2311.16981v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16972v1",
            "title": "The invariant $\u03bb_2$ from graph configurations",
            "updated": "2023-11-28T17:19:02Z",
            "published": "2023-11-28T17:19:02Z",
            "summary": "For an oriented rational homology sphere $M$, the logarithm of the\nKontsevich-Kuperberg-Thurston invariant $z_{KKT}(M)$ counts embeddings of\nconnected trivalent graphs in $M$, using integrals on configuration spaces of\npoints in $M$. It is a universal finite type invariant of oriented rational\nhomology spheres. The quantity $\\exp(z_{KKT})$ is often called the perturbative\nexpansion of the Chern-Simons theory. In this article, we give an independent\noriginal definition of the degree two part of $z_{KKT}$ appropriate for\nconcrete computations. This article can also serve as an introduction to the\ndefinition of $z_{KKT}$.",
            "author": [
                "Yohan Mandin-Hubl\u00e9"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16972v1",
                "http://arxiv.org/pdf/2311.16972v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57K16 57K31 55R80 81Q30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16962v1",
            "title": "Direct Imaging Methods for Inverse Obstacle Scattering",
            "updated": "2023-11-28T17:07:36Z",
            "published": "2023-11-28T17:07:36Z",
            "summary": "Direct imaging methods recover the presence, position, and shape of the\nunknown obstacles in time-harmonic inverse scattering without a priori\nknowledge of either the physical properties or the number of disconnected\ncomponents of the scatterer, i.e., on the boundary condition. However, most of\nthese methods require multi-static data and only obtain partial information\nabout the obstacle. These qualitative methods are based on constructing\nindicator functions defined on the domain of interest, which help determine\nwhether a spatial point or point source lies inside or outside the scatterer.\nThis paper explains the main themes of each of these methods, with emphasis on\nhighlighting the advantages and limitations of each scheme. Additionally, we\nwill classify each method and describe how some of these methods are closely\nrelated to each other.",
            "author": [
                "General Ozochiawaeze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16962v1",
                "http://arxiv.org/pdf/2311.16962v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16957v1",
            "title": "Space-Efficient Data Structures for Polyominoes and Bar Graphs",
            "updated": "2023-11-28T17:04:27Z",
            "published": "2023-11-28T17:04:27Z",
            "summary": "We provide a compact data structure for representing polyominoes that\nsupports neighborhood and visibility queries. Neighborhood queries concern\nreporting adjacent cells to a given cell, and visibility queries determine\nwhether a straight line can be drawn within the polyomino that connects two\nspecified cells. For an arbitrary small $\\epsilon >0$, our data structure can\nencode a polyomino with $n$ cells in $(3+\\epsilon)n + o(n)$ bits while\nsupporting all queries in constant time. The space complexity can be improved\nto $3n+o(n)$, while supporting neighborhood queries in $\\mathcal{O}(1)$ and\nvisibility queries in $\\mathcal{O}(t(n))$ for any arbitrary $t(n) \\in\n\\omega(1)$. Previous attempts at enumerating polyominoes have indicated that at\nleast $2.00091n - o(n)$ bits are required to differentiate between distinct\npolyominoes, which shows our data structure is compact.\n  In addition, we introduce a succinct data structure tailored for bar graphs,\na specific subclass of polyominoes resembling histograms. We demonstrate that a\nbar graph comprising $n$ cells can be encoded using only $n + o(n)$ bits,\nenabling constant-time query processing. Meanwhile, $n-1$ bits are necessary to\nrepresent any bar graph, proving our data structure is succinct.",
            "author": [
                "Magnus Berg",
                "Shahin Kamali",
                "Katherine Ling",
                "Cooper Sigrist"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16957v1",
                "http://arxiv.org/pdf/2311.16957v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "E.1"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16953v2",
            "title": "Local certification of geometric graph classes",
            "updated": "2023-11-29T16:58:31Z",
            "published": "2023-11-28T16:59:42Z",
            "summary": "The goal of local certification is to locally convince the vertices of a\ngraph $G$ that $G$ satisfies a given property. A prover assigns short\ncertificates to the vertices of the graph, then the vertices are allowed to\ncheck their certificates and the certificates of their neighbors, and based\nonly on this local view, they must decide whether $G$ satisfies the given\nproperty. If the graph indeed satisfies the property, all vertices must accept\nthe instance, and otherwise at least one vertex must reject the instance (for\nany possible assignment of certificates). The goal is to minimize the size of\nthe certificates.\n  In this paper we study the local certification of geometric and topological\ngraph classes. While it is known that in $n$-vertex graphs, planarity can be\ncertified locally with certificates of size $O(\\log n)$, we show that several\nclosely related graph classes require certificates of size $\\Omega(n)$. This\nincludes penny graphs, unit-distance graphs, (induced) subgraphs of the square\ngrid, 1-planar graphs, and unit-square graphs. These bounds are tight up to a\nconstant factor and give the first known examples of hereditary (and even\nmonotone) graph classes for which the certificates must have linear size. For\nunit-disk graphs we obtain a lower bound of $\\Omega(n^{1-\\delta})$ for any\n$\\delta>0$ on the size of the certificates, and an upper bound of $O(n \\log\nn)$. The lower bounds are obtained by proving rigidity properties of the\nconsidered graphs, which might be of independent interest.",
            "author": [
                "Oscar Defrain",
                "Louis Esperet",
                "Aur\u00e9lie Lagoutte",
                "Pat Morin",
                "Jean-Florent Raymond"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16953v2",
                "http://arxiv.org/pdf/2311.16953v2"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.CG",
                "cs.DC",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16942v1",
            "title": "Advancing state estimation for lithium-ion batteries with hysteresis:\n  systematic extended Kalman filter tuning",
            "updated": "2023-11-28T16:46:40Z",
            "published": "2023-11-28T16:46:40Z",
            "summary": "Knowledge of remaining battery charge is fundamental to electric vehicle\ndeployment. Accurate measurements of state-of-charge (SOC) cannot be directly\nobtained, and estimation methods must be used instead. This requires both a\ngood model of a battery and a well-designed state estimator. Here, hysteretic\nreduced-order battery models and adaptive extended Kalman filter estimators are\nshown to be highly effective, accurate SOC estimators. A battery model\nparameterisation framework is proposed, which enhances standardised methods to\ncapture hysteresis effects. The hysteretic model is parameterised for three\nindependent NMC811 lithium-ion cells and is shown to reduce voltage RMS error\nby 50% across 18-hour automotive drive-cycles. Parameterised models are used\nalongside an extended Kalman filter, which demonstrates the value of adaptive\nfilter parameterisation schemes. When used alongside an extended Kalman filter,\nadaptive covariance matrices yield highly accurate SOC estimates, reducing SOC\nestimation error by 85%, compared to the industry standard battery model.",
            "author": [
                "Jasper Knox",
                "Mark Blyth",
                "Alastair Hales"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16942v1",
                "http://arxiv.org/pdf/2311.16942v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16941v1",
            "title": "Debiasing Multimodal Models via Causal Information Minimization",
            "updated": "2023-11-28T16:46:14Z",
            "published": "2023-11-28T16:46:14Z",
            "summary": "Most existing debiasing methods for multimodal models, including causal\nintervention and inference methods, utilize approximate heuristics to represent\nthe biases, such as shallow features from early stages of training or unimodal\nfeatures for multimodal tasks like VQA, etc., which may not be accurate. In\nthis paper, we study bias arising from confounders in a causal graph for\nmultimodal data and examine a novel approach that leverages causally-motivated\ninformation minimization to learn the confounder representations. Robust\npredictive features contain diverse information that helps a model generalize\nto out-of-distribution data. Hence, minimizing the information content of\nfeatures obtained from a pretrained biased model helps learn the simplest\npredictive features that capture the underlying data distribution. We treat\nthese features as confounder representations and use them via methods motivated\nby causal theory to remove bias from models. We find that the learned\nconfounder representations indeed capture dataset biases, and the proposed\ndebiasing methods improve out-of-distribution (OOD) performance on multiple\nmultimodal datasets without sacrificing in-distribution performance.\nAdditionally, we introduce a novel metric to quantify the sufficiency of\nspurious features in models' predictions that further demonstrates the\neffectiveness of our proposed methods. Our code is available at:\nhttps://github.com/Vaidehi99/CausalInfoMin",
            "author": [
                "Vaidehi Patil",
                "Adyasha Maharana",
                "Mohit Bansal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16941v1",
                "http://arxiv.org/pdf/2311.16941v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16939v1",
            "title": "Decomposition numbers for unipotent blocks with small\n  $\\mathfrak{sl}_2$-weight in finite classical groups",
            "updated": "2023-11-28T16:42:42Z",
            "published": "2023-11-28T16:42:42Z",
            "summary": "We show that parabolic Kazhdan-Lusztig polynomials of type $A$ compute the\ndecomposition numbers in certain Harish-Chandra series of unipotent characters\nof finite groups of Lie types $B$, $C$ and $D$ over a field of non-defining\ncharacteristic $\\ell$. Here, $\\ell$ is a ``unitary prime\" -- the case that\nremains open in general. The bipartitions labeling the characters in these\nseries are small with respect to $d$, the order of $q$ mod $\\ell$, although\nthey occur in blocks of arbitrarily high defect. Our main technical tool is the\ncategorical action of an affine Lie algebra on the category of unipotent\nrepresentations, which identifies the branching graph for Harish-Chandra\ninduction with the $\\widehat{\\mathfrak{sl}}_d$-crystal on a sum of level $2$\nFock spaces. Further key combinatorics has been adapted from Brundan and\nStroppel's work on Khovanov arc algebras to obtain the closed formula for the\ndecomposition numbers in a $d$-small Harish-Chandra series.",
            "author": [
                "Olivier Dudas",
                "Emily Norton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16939v1",
                "http://arxiv.org/pdf/2311.16939v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16926v3",
            "title": "LLaFS: When Large-Language Models Meet Few-Shot Segmentation",
            "updated": "2023-12-05T10:04:37Z",
            "published": "2023-11-28T16:31:27Z",
            "summary": "This paper proposes LLaFS, the first attempt to leverage large language\nmodels (LLMs) in few-shot segmentation. In contrast to the conventional\nfew-shot segmentation methods that only rely on the limited and biased\ninformation from the annotated support images, LLaFS leverages the vast prior\nknowledge gained by LLM as an effective supplement and directly uses the LLM to\nsegment images in a few-shot manner. To enable the text-based LLM to handle\nimage-related tasks, we carefully design an input instruction that allows the\nLLM to produce segmentation results represented as polygons, and propose a\nregion-attribute table to simulate the human visual mechanism and provide\nmulti-modal guidance. We also synthesize pseudo samples and use curriculum\nlearning for pretraining to augment data and achieve better optimization. LLaFS\nachieves state-of-the-art results on multiple datasets, showing the potential\nof using LLMs for few-shot computer vision tasks. Code will be available at\nhttps://github.com/lanyunzhu99/LLaFS.",
            "author": [
                "Lanyun Zhu",
                "Tianrun Chen",
                "Deyi Ji",
                "Jieping Ye",
                "Jun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16926v3",
                "http://arxiv.org/pdf/2311.16926v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16916v1",
            "title": "Stein Variational Belief Propagation for Multi-Robot Coordination",
            "updated": "2023-11-28T16:19:30Z",
            "published": "2023-11-28T16:19:30Z",
            "summary": "Decentralized coordination for multi-robot systems involves planning in\nchallenging, high-dimensional spaces. The planning problem is particularly\nchallenging in the presence of obstacles and different sources of uncertainty\nsuch as inaccurate dynamic models and sensor noise. In this paper, we introduce\nStein Variational Belief Propagation (SVBP), a novel algorithm for performing\ninference over nonparametric marginal distributions of nodes in a graph. We\napply SVBP to multi-robot coordination by modelling a robot swarm as a\ngraphical model and performing inference for each robot. We demonstrate our\nalgorithm on a simulated multi-robot perception task, and on a multi-robot\nplanning task within a Model-Predictive Control (MPC) framework, on both\nsimulated and real-world mobile robots. Our experiments show that SVBP\nrepresents multi-modal distributions better than sampling-based or Gaussian\nbaselines, resulting in improved performance on perception and planning tasks.\nFurthermore, we show that SVBP's ability to represent diverse trajectories for\ndecentralized multi-robot planning makes it less prone to deadlock scenarios\nthan leading baselines.",
            "author": [
                "Jana Pavlasek",
                "Joshua Jing Zhi Mah",
                "Ruihan Xu",
                "Odest Chadwicke Jenkins",
                "Fabio Ramos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16916v1",
                "http://arxiv.org/pdf/2311.16916v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16912v1",
            "title": "Continuous optimization methods for the graph isomorphism problem",
            "updated": "2023-11-28T16:15:30Z",
            "published": "2023-11-28T16:15:30Z",
            "summary": "The graph isomorphism problem looks deceptively simple, but although\npolynomial-time algorithms exist for certain types of graphs such as planar\ngraphs and graphs with bounded degree or eigenvalue multiplicity, its\ncomplexity class is still unknown. Information about potential isomorphisms\nbetween two graphs is contained in the eigenvalues and eigenvectors of their\nadjacency matrices. However, symmetries of graphs often lead to repeated\neigenvalues so that associated eigenvectors are determined only up to basis\nrotations, which complicates graph isomorphism testing. We consider orthogonal\nand doubly stochastic relaxations of the graph isomorphism problem, analyze the\ngeometric properties of the resulting solution spaces, and show that their\ncomplexity increases significantly if repeated eigenvalues exist. By\nrestricting the search space to suitable subspaces, we derive an efficient\nFrank-Wolfe based continuous optimization approach for detecting isomorphisms.\nWe illustrate the efficacy of the algorithm with the aid of various highly\nsymmetric graphs.",
            "author": [
                "Stefan Klus",
                "Patrick Gel\u00df"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16912v1",
                "http://arxiv.org/pdf/2311.16912v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16908v1",
            "title": "A Novel Method to Eliminate the Symmetry Dependence of Fiber Coils for\n  Shupe Mitigation",
            "updated": "2023-11-28T16:11:31Z",
            "published": "2023-11-28T16:11:31Z",
            "summary": "It is a well-known fact that interferometric fiber optic gyroscopes (IFOGs)\nare easily distorted by thermal effects and distortion results in the\ndegradation of the performance of these sensors. Changing the fiber coil\ngeometry, increasing the winding symmetry, adding fiber buffer layers around\nthe fiber coil, using different modulation methods for multifunctional\nintegrated optic chips (MIOCs), and using special types of fibers, such as\nphotonic crystal fibers (PCFs), are some alternative solutions for preventing\nthis degradation. This paper, theoretically and experimentally, investigates\nnot only how different types of fiber coil winding methods behave under\ndifferent rates of temperature change but also presents a novel method, to the\nbest of our knowledge, to eliminate the Shupe effect, without violating the\nsimplest IFOG scheme. This method rules out the importance of the winding\nsymmetry epochally and the need of any extra treatment for the fiber coil to\nincrease the thermal performance of the system. Regardless of the symmetry of\nthe fiber coil winding, the rate error due to the Shupe effect can be reduced\nto about $\\pm$$0.05^\\circ/$h for any rate of temperature change with this new\nmethod according to the experimental results.",
            "author": [
                "Tugba Andac Senol",
                "Onder Akcaalan",
                "Aylin Yertutanol",
                "Ekmel Ozbay"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16908v1",
                "http://arxiv.org/pdf/2311.16908v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16905v1",
            "title": "Analyzing the Influence of Language Model-Generated Responses in\n  Mitigating Hate Speech on Social Media Directed at Ukrainian Refugees in\n  Poland",
            "updated": "2023-11-28T16:08:42Z",
            "published": "2023-11-28T16:08:42Z",
            "summary": "In the context of escalating hate speech and polarization on social media,\nthis study investigates the potential of employing responses generated by Large\nLanguage Models (LLM), complemented with pertinent verified knowledge links, to\ncounteract such trends. Through extensive A/B testing involving the posting of\n753 automatically generated responses, the goal was to minimize the propagation\nof hate speech directed at Ukrainian refugees in Poland.\n  The results indicate that deploying LLM-generated responses as replies to\nharmful tweets effectively diminishes user engagement, as measured by\nlikes/impressions. When we respond to an original tweet, i.e., which is not a\nreply, we reduce the engagement of users by over 20\\% without increasing the\nnumber of impressions. On the other hand, our responses increase the ratio of\nthe number of replies to a harmful tweet to impressions, especially if the\nharmful tweet is not original. Additionally, the study examines how generated\nresponses influence the overall sentiment of tweets in the discussion,\nrevealing that our intervention does not significantly alter the mean\nsentiment.\n  This paper suggests the implementation of an automatic moderation system to\ncombat hate speech on social media and provides an in-depth analysis of the A/B\nexperiment, covering methodology, data collection, and statistical outcomes.\nEthical considerations and challenges are also discussed, offering guidance for\nthe development of discourse moderation systems leveraging the capabilities of\ngenerative AI.",
            "author": [
                "Jakub Podolak",
                "Szymon \u0141ukasik",
                "Pawe\u0142 Balawender",
                "Jan Ossowski",
                "Katarzyna B\u0105kowicz",
                "Piotr Sankowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16905v1",
                "http://arxiv.org/pdf/2311.16905v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03739v1",
            "title": "Syntax-Informed Interactive Model for Comprehensive Aspect-Based\n  Sentiment Analysis",
            "updated": "2023-11-28T16:03:22Z",
            "published": "2023-11-28T16:03:22Z",
            "summary": "Aspect-based sentiment analysis (ABSA), a nuanced task in text analysis,\nseeks to discern sentiment orientation linked to specific aspect terms in text.\nTraditional approaches often overlook or inadequately model the explicit\nsyntactic structures of sentences, crucial for effective aspect term\nidentification and sentiment determination. Addressing this gap, we introduce\nan innovative model: Syntactic Dependency Enhanced Multi-Task Interaction\nArchitecture (SDEMTIA) for comprehensive ABSA. Our approach innovatively\nexploits syntactic knowledge (dependency relations and types) using a\nspecialized Syntactic Dependency Embedded Interactive Network (SDEIN). We also\nincorporate a novel and efficient message-passing mechanism within a multi-task\nlearning framework to bolster learning efficacy. Our extensive experiments on\nbenchmark datasets showcase our model's superiority, significantly surpassing\nexisting methods. Additionally, incorporating BERT as an auxiliary feature\nextractor further enhances our model's performance.",
            "author": [
                "Ullman Galen",
                "Frey Lee",
                "Woods Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03739v1",
                "http://arxiv.org/pdf/2312.03739v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16899v1",
            "title": "On the saturation spectrum of the unions of disjoint cycles",
            "updated": "2023-11-28T15:51:04Z",
            "published": "2023-11-28T15:51:04Z",
            "summary": "Let $G$ be a graph and $\\mathcal{H}$ be a family of graphs. We say $G$ is\n$\\mathcal{H}$-saturated if $G$ does not contain a copy of $H$ with\n$H\\in\\mathcal{H}$, but the addition of any edge $e\\notin E(G)$ creates at least\none copy of some $H\\in\\mathcal{H}$ within $G+e$. The saturation number of\n$\\mathcal{H}$ is the minimum size of an $\\mathcal{H}$-saturated graph on $n$\nvertices, and the saturation spectrum of $\\mathcal{H}$ is the set of all\npossible sizes of an $\\mathcal{H}$-saturated graph on $n$ vertices. Let\n$k\\mathcal{C}_{\\ge 3}$ be the family of the unions of $k$ vertex-disjoint\ncycles. In this note, we completely determine the saturation number and the\nsaturation spectrum of $k\\mathcal{C}_{\\ge 3}$ for $k=2$ and give some results\nfor $k\\ge 3$.",
            "author": [
                "Yue Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16899v1",
                "http://arxiv.org/pdf/2311.16899v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16880v1",
            "title": "Using a Grassmann graph to recover the underlying projective geometry",
            "updated": "2023-11-28T15:31:02Z",
            "published": "2023-11-28T15:31:02Z",
            "summary": "Let $n,k$ denote integers with $n>2k\\geq 6$. Let $\\mathbb{F}_q$ denote a\nfinite field with $q$ elements, and let $V$ denote a vector space over\n$\\mathbb{F}_q$ that has dimension $n$. The projective geometry $P_q(n)$ is the\npartially ordered set consisting of the subspaces of $V$; the partial order is\ngiven by inclusion. For the Grassman graph $J_q(n,k)$ the vertex set consists\nof the $k$-dimensional subspaces of $V$. Two vertices of $J_q(n,k)$ are\nadjacent whenever their intersection has dimension $k-1$. The graph $J_q(n,k)$\nis known to be distance-regular. Let $\\partial$ denote the path-length distance\nfunction of $J_q(n,k)$. Pick two vertices $x,y$ in $J_q(n,k)$ such that\n$1<\\partial(x,y)<k$. The set $P_q(n)$ contains the elements $x,y,x\\cap y,x+y$.\nIn our main result, we describe $x\\cap y$ and $x+y$ using only the graph\nstructure of $J_q(n,k)$. To achieve this result, we make heavy use of the\nEuclidean representation of $J_q(n,k)$ that corresponds to the second largest\neigenvalue of the adjacency matrix.",
            "author": [
                "Ian Seong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16880v1",
                "http://arxiv.org/pdf/2311.16880v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05E30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03738v1",
            "title": "Syntactic Fusion: Enhancing Aspect-Level Sentiment Analysis Through\n  Multi-Tree Graph Integration",
            "updated": "2023-11-28T15:28:22Z",
            "published": "2023-11-28T15:28:22Z",
            "summary": "Recent progress in aspect-level sentiment classification has been propelled\nby the incorporation of graph neural networks (GNNs) leveraging syntactic\nstructures, particularly dependency trees. Nevertheless, the performance of\nthese models is often hampered by the innate inaccuracies of parsing\nalgorithms. To mitigate this challenge, we introduce SynthFusion, an innovative\ngraph ensemble method that amalgamates predictions from multiple parsers. This\nstrategy blends diverse dependency relations prior to the application of GNNs,\nenhancing robustness against parsing errors while avoiding extra computational\nburdens. SynthFusion circumvents the pitfalls of overparameterization and\ndiminishes the risk of overfitting, prevalent in models with stacked GNN\nlayers, by optimizing graph connectivity. Our empirical evaluations on the\nSemEval14 and Twitter14 datasets affirm that SynthFusion not only outshines\nmodels reliant on single dependency trees but also eclipses alternative\nensemble techniques, achieving this without an escalation in model complexity.",
            "author": [
                "Jane Sunny",
                "Tom Padraig",
                "Roggie Terry",
                "Woods Ali"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03738v1",
                "http://arxiv.org/pdf/2312.03738v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16876v1",
            "title": "Digital Twin-Enhanced Deep Reinforcement Learning for Resource\n  Management in Networks Slicing",
            "updated": "2023-11-28T15:25:14Z",
            "published": "2023-11-28T15:25:14Z",
            "summary": "Network slicing-based communication systems can dynamically and efficiently\nallocate resources for diversified services. However, due to the limitation of\nthe network interface on channel access and the complexity of the resource\nallocation, it is challenging to achieve an acceptable solution in the\npractical system without precise prior knowledge of the dynamics probability\nmodel of the service requests. Existing work attempts to solve this problem\nusing deep reinforcement learning (DRL), however, such methods usually require\na lot of interaction with the real environment in order to achieve good\nresults. In this paper, a framework consisting of a digital twin and\nreinforcement learning agents is present to handle the issue. Specifically, we\npropose to use the historical data and the neural networks to build a digital\ntwin model to simulate the state variation law of the real environment. Then,\nwe use the data generated by the network slicing environment to calibrate the\ndigital twin so that it is in sync with the real environment. Finally, DRL for\nslice optimization optimizes its own performance in this virtual\npre-verification environment. We conducted an exhaustive verification of the\nproposed digital twin framework to confirm its scalability. Specifically, we\npropose to use loss landscapes to visualize the generalization of DRL\nsolutions. We explore a distillation-based optimization scheme for lightweight\nslicing strategies. In addition, we also extend the framework to offline\nreinforcement learning, where solutions can be used to obtain intelligent\ndecisions based solely on historical data. Numerical simulation experiments\nshow that the proposed digital twin can significantly improve the performance\nof the slice optimization strategy.",
            "author": [
                "Zhengming Zhang",
                "Yongming Huang",
                "Cheng Zhang",
                "Qingbi Zheng",
                "Luxi Yang",
                "Xiaohu You"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16876v1",
                "http://arxiv.org/pdf/2311.16876v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16867v2",
            "title": "The Falcon Series of Open Language Models",
            "updated": "2023-11-29T19:45:10Z",
            "published": "2023-11-28T15:12:47Z",
            "summary": "We introduce the Falcon series: 7B, 40B, and 180B parameters causal\ndecoder-only models trained on a diverse high-quality corpora predominantly\nassembled from web data. The largest model, Falcon-180B, has been trained on\nover 3.5 trillion tokens of text--the largest openly documented pretraining\nrun. Falcon-180B significantly outperforms models such as PaLM or Chinchilla,\nand improves upon concurrently developed models such as LLaMA 2 or\nInflection-1. It nears the performance of PaLM-2-Large at a reduced pretraining\nand inference cost, making it, to our knowledge, one of the three best language\nmodels in the world along with GPT-4 and PaLM-2-Large. We report detailed\nevaluations, as well as a deep dive into the methods and custom tooling\nemployed to pretrain Falcon. Notably, we report on our custom distributed\ntraining codebase, allowing us to efficiently pretrain these models on up to\n4,096 A100s on cloud AWS infrastructure with limited interconnect. We release a\n600B tokens extract of our web dataset, as well as the Falcon-7/40/180B models\nunder a permissive license to foster open-science and accelerate the\ndevelopment of an open ecosystem of large language models.",
            "author": [
                "Ebtesam Almazrouei",
                "Hamza Alobeidli",
                "Abdulaziz Alshamsi",
                "Alessandro Cappelli",
                "Ruxandra Cojocaru",
                "M\u00e9rouane Debbah",
                "\u00c9tienne Goffinet",
                "Daniel Hesslow",
                "Julien Launay",
                "Quentin Malartic",
                "Daniele Mazzotta",
                "Badreddine Noune",
                "Baptiste Pannier",
                "Guilherme Penedo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16867v2",
                "http://arxiv.org/pdf/2311.16867v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16856v1",
            "title": "Attentional Graph Neural Networks for Robust Massive Network\n  Localization",
            "updated": "2023-11-28T15:05:13Z",
            "published": "2023-11-28T15:05:13Z",
            "summary": "Graph neural networks (GNNs) have gained significant popularity for\nclassification tasks in machine learning, yet their applications to regression\nproblems remain limited. Concurrently, attention mechanisms have emerged as\npowerful tools in sequential learning tasks. In this paper, we employ GNNs and\nattention mechanisms to address a classical but challenging nonlinear\nregression problem: network localization. We propose a novel GNN-based network\nlocalization method that achieves exceptional stability and accuracy in the\npresence of severe non-line-of-sight (NLOS) propagations, while eliminating the\nneed for laborious offline calibration or NLOS identification. Extensive\nexperimental results validate the effectiveness and high accuracy of our\nGNN-based localization model, particularly in challenging NLOS scenarios.\nHowever, the proposed GNN-based model exhibits limited flexibility, and its\naccuracy is highly sensitive to a specific hyperparameter that determines the\ngraph structure. To address the limitations and extend the applicability of the\nGNN-based model to real scenarios, we introduce two attentional graph neural\nnetworks (AGNNs) that offer enhanced flexibility and the ability to\nautomatically learn the optimal hyperparameter for each node. Experimental\nresults confirm that the AGNN models are able to enhance localization accuracy,\nproviding a promising solution for real-world applications. We also provide\nsome analyses of the improved performance achieved by the AGNN models from the\nperspectives of dynamic attention and signal denoising characteristics.",
            "author": [
                "Wenzhong Yan",
                "Juntao Wang",
                "Feng Yin",
                "Abdelhak M. Zoubir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16856v1",
                "http://arxiv.org/pdf/2311.16856v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16843v1",
            "title": "Self-training solutions for the ICCV 2023 GeoNet Challenge",
            "updated": "2023-11-28T14:57:14Z",
            "published": "2023-11-28T14:57:14Z",
            "summary": "GeoNet is a recently proposed domain adaptation benchmark consisting of three\nchallenges (i.e., GeoUniDA, GeoImNet, and GeoPlaces). Each challenge contains\nimages collected from the USA and Asia where there are huge geographical gaps.\nOur solution adopts a two-stage source-free domain adaptation framework with a\nSwin Transformer backbone to achieve knowledge transfer from the USA (source)\ndomain to Asia (target) domain. In the first stage, we train a source model\nusing labeled source data with a re-sampling strategy and two types of\ncross-entropy loss. In the second stage, we generate pseudo labels for\nunlabeled target data to fine-tune the model. Our method achieves an H-score of\n74.56% and ultimately ranks 1st in the GeoUniDA challenge. In GeoImNet and\nGeoPlaces challenges, our solution also reaches a top-3 accuracy of 64.46% and\n51.23%, respectively.",
            "author": [
                "Lijun Sheng",
                "Zhengbo Wang",
                "Jian Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16843v1",
                "http://arxiv.org/pdf/2311.16843v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17124v1",
            "title": "A knowledge-driven AutoML architecture",
            "updated": "2023-11-28T14:31:38Z",
            "published": "2023-11-28T14:31:38Z",
            "summary": "This paper proposes a knowledge-driven AutoML architecture for pipeline and\ndeep feature synthesis. The main goal is to render the AutoML process\nexplainable and to leverage domain knowledge in the synthesis of pipelines and\nfeatures. The architecture explores several novel ideas: first, the\nconstruction of pipelines and deep features is approached in an unified way.\nNext, synthesis is driven by a shared knowledge system, interactively queried\nas to what pipeline operations to use or features to compute. Lastly, the\nsynthesis processes takes decisions at runtime using partial solutions and\nresults of their application on data. Two experiments are conducted to\ndemonstrate the functionality of a na\\\"{\\i}ve implementation of the proposed\narchitecture and to discuss its advantages, trade-offs as well as future\npotential for AutoML.",
            "author": [
                "Corneliu Cofaru",
                "Johan Loeckx"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17124v1",
                "http://arxiv.org/pdf/2311.17124v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16816v1",
            "title": "Packing even directed circuits quarter-integrally",
            "updated": "2023-11-28T14:27:23Z",
            "published": "2023-11-28T14:27:23Z",
            "summary": "We prove the existence of a computable function\n$f\\colon\\mathbb{N}\\to\\mathbb{N}$ such that for every integer $k$ and every\ndigraph $D$ either contains a collection $\\mathcal{C}$ of directed cycles of\neven length such that no vertex of $D$ belongs to more than four cycles in\n$\\mathcal{C}$, or there exists a set $S\\subseteq V(D)$ of size at most $f(k)$\nsuch that $D-S$ has no directed cycle of even length. Moreover, we provide an\nalgorithm that finds one of the two outcomes of this statement in time\n$g(k)n^{\\mathcal{O}(1)}$ for some computable function $g\\colon\n\\mathbb{N}\\to\\mathbb{N}$.\n  Our result unites two deep fields of research from the algorithmic theory for\ndigraphs: The study of the Erd\\H{o}s-P\\'osa property of digraphs and the study\nof the Even Dicycle Problem. The latter is the decision problem which asks if a\ngiven digraph contains an even dicycle and can be traced back to a question of\nP\\'olya from 1913. It remained open until a polynomial time algorithm was\nfinally found by Robertson, Seymour, and Thomas (Ann. of Math. (2) 1999) and,\nindependently, McCuaig (Electron. J. Combin. 2004; announced jointly at STOC\n1997). The Even Dicycle Problem is equivalent to the recognition problem of\nPfaffian bipartite graphs and has applications even beyond discrete mathematics\nand theoretical computer science. On the other hand, Younger's Conjecture\n(1973), states that dicycles have the Erd\\H{o}s-P\\'osa property. The conjecture\nwas proven more than two decades later by Reed, Robertson, Seymour, and Thomas\n(Combinatorica 1996) and opened the path for structural digraph theory as well\nas the algorithmic study of the directed feedback vertex set problem. Our\napproach builds upon the techniques used to resolve both problems and combines\nthem into a powerful structural theorem that yields further algorithmic\napplications for other prominent problems.",
            "author": [
                "Maximilian Gorsky",
                "Ken-ichi Kawarabayashi",
                "Stephan Kreutzer",
                "Sebastian Wiederrecht"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16816v1",
                "http://arxiv.org/pdf/2311.16816v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C20, 05C38, 05C83, 05C85, 68R10",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16789v1",
            "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems",
            "updated": "2023-11-28T13:51:32Z",
            "published": "2023-11-28T13:51:32Z",
            "summary": "Dialogue systems, including task-oriented_dialogue_system (TOD) and\nopen-domain_dialogue_system (ODD), have undergone significant transformations,\nwith language_models (LM) playing a central role. This survey delves into the\nhistorical trajectory of dialogue systems, elucidating their intricate\nrelationship with advancements in language models by categorizing this\nevolution into four distinct stages, each marked by pivotal LM breakthroughs:\n1) Early_Stage: characterized by statistical LMs, resulting in rule-based or\nmachine-learning-driven dialogue_systems; 2) Independent development of TOD and\nODD based on neural_language_models (NLM; e.g., LSTM and GRU), since NLMs lack\nintrinsic knowledge in their parameters; 3) fusion between different types of\ndialogue systems with the advert of pre-trained_language_models (PLMs),\nstarting from the fusion between four_sub-tasks_within_TOD, and then\nTOD_with_ODD; and 4) current LLM-based_dialogue_system, wherein LLMs can be\nused to conduct TOD and ODD seamlessly. Thus, our survey provides a\nchronological perspective aligned with LM breakthroughs, offering a\ncomprehensive review of state-of-the-art research outcomes. What's more, we\nfocus on emerging topics and discuss open challenges, providing valuable\ninsights into future directions for LLM-based_dialogue_systems. Through this\nexploration, we pave the way for a deeper_comprehension of the evolution,\nguiding future developments in LM-based dialogue_systems.",
            "author": [
                "Hongru Wang",
                "Lingzhi Wang",
                "Yiming Du",
                "Liang Chen",
                "Jingyan Zhou",
                "Yufei Wang",
                "Kam-Fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16789v1",
                "http://arxiv.org/pdf/2311.16789v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17122v1",
            "title": "Large Model Based Referring Camouflaged Object Detection",
            "updated": "2023-11-28T13:45:09Z",
            "published": "2023-11-28T13:45:09Z",
            "summary": "Referring camouflaged object detection (Ref-COD) is a recently-proposed\nproblem aiming to segment out specified camouflaged objects matched with a\ntextual or visual reference. This task involves two major challenges: the COD\ndomain-specific perception and multimodal reference-image alignment. Our\nmotivation is to make full use of the semantic intelligence and intrinsic\nknowledge of recent Multimodal Large Language Models (MLLMs) to decompose this\ncomplex task in a human-like way. As language is highly condensed and\ninductive, linguistic expression is the main media of human knowledge learning,\nand the transmission of knowledge information follows a multi-level progression\nfrom simplicity to complexity. In this paper, we propose a large-model-based\nMulti-Level Knowledge-Guided multimodal method for Ref-COD termed MLKG, where\nmulti-level knowledge descriptions from MLLM are organized to guide the large\nvision model of segmentation to perceive the camouflage-targets and\ncamouflage-scene progressively and meanwhile deeply align the textual\nreferences with camouflaged photos. To our knowledge, our contributions mainly\ninclude: (1) This is the first time that the MLLM knowledge is studied for\nRef-COD and COD. (2) We, for the first time, propose decomposing Ref-COD into\ntwo main perspectives of perceiving the target and scene by integrating MLLM\nknowledge, and contribute a multi-level knowledge-guided method. (3) Our method\nachieves the state-of-the-art on the Ref-COD benchmark outperforming numerous\nstrong competitors. Moreover, thanks to the injected rich knowledge, it\ndemonstrates zero-shot generalization ability on uni-modal COD datasets. We\nwill release our code soon.",
            "author": [
                "Shupeng Cheng",
                "Ge-Peng Ji",
                "Pengda Qin",
                "Deng-Ping Fan",
                "Bowen Zhou",
                "Peng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17122v1",
                "http://arxiv.org/pdf/2311.17122v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16769v1",
            "title": "Equilibrium in the Computing Continuum through Active Inference",
            "updated": "2023-11-28T13:19:54Z",
            "published": "2023-11-28T13:19:54Z",
            "summary": "Computing Continuum (CC) systems are challenged to ensure the intricate\nrequirements of each computational tier. Given the system's scale, the Service\nLevel Objectives (SLOs) which are expressed as these requirements, must be\nbroken down into smaller parts that can be decentralized. We present our\nframework for collaborative edge intelligence enabling individual edge devices\nto (1) develop a causal understanding of how to enforce their SLOs, and (2)\ntransfer knowledge to speed up the onboarding of heterogeneous devices. Through\ncollaboration, they (3) increase the scope of SLO fulfillment. We implemented\nthe framework and evaluated a use case in which a CC system is responsible for\nensuring Quality of Service (QoS) and Quality of Experience (QoE) during video\nstreaming. Our results showed that edge devices required only ten training\nrounds to ensure four SLOs; furthermore, the underlying causal structures were\nalso rationally explainable. The addition of new types of devices can be done a\nposteriori, the framework allowed them to reuse existing models, even though\nthe device type had been unknown. Finally, rebalancing the load within a device\ncluster allowed individual edge devices to recover their SLO compliance after a\nnetwork failure from 22% to 89%.",
            "author": [
                "Boris Sedlak",
                "Victor Casamayor Pujol",
                "Praveen Kumar Donta",
                "Schahram Dustdar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16769v1",
                "http://arxiv.org/pdf/2311.16769v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AI",
                "cs.LG",
                "cs.PF",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16764v1",
            "title": "Radiology-Aware Model-Based Evaluation Metric for Report Generation",
            "updated": "2023-11-28T13:08:26Z",
            "published": "2023-11-28T13:08:26Z",
            "summary": "We propose a new automated evaluation metric for machine-generated radiology\nreports using the successful COMET architecture adapted for the radiology\ndomain. We train and publish four medically-oriented model checkpoints,\nincluding one trained on RadGraph, a radiology knowledge graph. Our results\nshow that our metric correlates moderately to high with established metrics\nsuch as BERTscore, BLEU, and CheXbert scores. Furthermore, we demonstrate that\none of our checkpoints exhibits a high correlation with human judgment, as\nassessed using the publicly available annotations of six board-certified\nradiologists, using a set of 200 reports. We also performed our own analysis\ngathering annotations with two radiologists on a collection of 100 reports. The\nresults indicate the potential effectiveness of our method as a\nradiology-specific evaluation metric. The code, data, and model checkpoints to\nreproduce our findings will be publicly available.",
            "author": [
                "Amos Calamida",
                "Farhad Nooralahzadeh",
                "Morteza Rohanian",
                "Koji Fujimoto",
                "Mizuho Nishio",
                "Michael Krauthammer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16764v1",
                "http://arxiv.org/pdf/2311.16764v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17115v1",
            "title": "Time- and Communication-Efficient Overlay Network Construction via\n  Gossip",
            "updated": "2023-11-28T12:09:08Z",
            "published": "2023-11-28T12:09:08Z",
            "summary": "We focus on the well-studied problem of distributed overlay network\nconstruction. We consider a synchronous gossip-based communication model where\nin each round a node can send a message of small size to another node whose\nidentifier it knows. The network is assumed to be reconfigurable, i.e., a node\ncan add new connections (edges) to other nodes whose identifier it knows or\ndrop existing connections. Each node initially has only knowledge of its own\nidentifier and the identifiers of its neighbors. The overlay construction\nproblem is, given an arbitrary (connected) graph, to reconfigure it to obtain a\nbounded-degree expander graph as efficiently as possible. The overlay\nconstruction problem is relevant to building real-world peer-to-peer network\ntopologies that have desirable properties such as low diameter, high\nconductance, robustness to adversarial deletions, etc.\n  Our main result is that we show that starting from any arbitrary (connected)\ngraph $G$ on $n$ nodes and $m$ edges, we can construct an overlay network that\nis a constant-degree expander in polylog $n$ rounds using only $\\tilde{O}(n)$\nmessages. Our time and message bounds are both essentially optimal (up to\npolylogarithmic factors). Our distributed overlay construction protocol is very\nlightweight as it uses gossip (each node communicates with only one neighbor in\neach round) and also scalable as it uses only $\\tilde{O}(n)$ messages, which is\nsublinear in $m$ (even when $m$ is moderately dense). To the best of our\nknowledge, this is the first result that achieves overlay network construction\nin polylog $n$ rounds and $o(m)$ messages. Our protocol uses graph sketches in\na novel way to construct an expander overlay that is both time and\ncommunication efficient. A consequence of our overlay construction protocol is\nthat distributed computation can be performed very efficiently in this model.",
            "author": [
                "Fabien Dufoulon",
                "Michael Moorman",
                "William K. Moses Jr.",
                "Gopal Pandurangan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17115v1",
                "http://arxiv.org/pdf/2311.17115v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16721v1",
            "title": "Analytic solution of Markovian epidemics without re-infections on\n  heterogeneous networks",
            "updated": "2023-11-28T12:05:31Z",
            "published": "2023-11-28T12:05:31Z",
            "summary": "Most epidemic processes on networks can be modelled by a compartmental model,\nthat specifies the spread of a disease in a population. The corresponding\ncompartmental graph describes how the viral state of the nodes (individuals)\nchanges from one compartment to another. If the compartmental graph does not\ncontain directed cycles (e.g. the famous SIR model satisfies this property),\nthen we provide an analytic, closed-form solution of the continuous-time\nMarkovian compartmental model on heterogeneous networks. The eigenvalues of the\nMarkovian process are related to cut sets in the contact graph between nodes\nwith different viral states. We illustrate our finding by analytically solving\nthe continuous-time Markovian SI and SIR processes on heterogeneous networks.\nWe show that analytic extensions to e.g. non-Markovian dynamics, temporal\nnetworks, simplicial contagion and more advanced compartmental models are\npossible. Our exact and explicit formula contains sums over all paths between\ntwo states in the SIR Markov graph, which prevents the computation of the exact\nsolution for arbitrary large graphs.",
            "author": [
                "Massimo A. Achterberg",
                "Piet Van Mieghem"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16721v1",
                "http://arxiv.org/pdf/2311.16721v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16716v1",
            "title": "Graph Pre-training and Prompt Learning for Recommendation",
            "updated": "2023-11-28T12:00:06Z",
            "published": "2023-11-28T12:00:06Z",
            "summary": "GNN-based recommenders have excelled in modeling intricate user-item\ninteractions through multi-hop message passing. However, existing methods often\noverlook the dynamic nature of evolving user-item interactions, which impedes\nthe adaption to changing user preferences and distribution shifts in newly\narriving data. Thus, their scalability and performances in real-world dynamic\nenvironments are limited. In this study, we propose GraphPL, a framework that\nincorporates parameter-efficient and dynamic graph pre-training with prompt\nlearning. This novel combination empowers GNNs to effectively capture both\nlong-term user preferences and short-term behavior dynamics, enabling the\ndelivery of accurate and timely recommendations. Our GraphPL framework\naddresses the challenge of evolving user preferences by seamlessly integrating\na temporal prompt mechanism and a graph-structural prompt learning mechanism\ninto the pre-trained GNN model. The temporal prompt mechanism encodes time\ninformation on user-item interaction, allowing the model to naturally capture\ntemporal context, while the graph-structural prompt learning mechanism enables\nthe transfer of pre-trained knowledge to adapt to behavior dynamics without the\nneed for continuous incremental training. We further bring in a dynamic\nevaluation setting for recommendation to mimic real-world dynamic scenarios and\nbridge the offline-online gap to a better level. Our extensive experiments\nincluding a large-scale industrial deployment showcases the lightweight plug-in\nscalability of our GraphPL when integrated with various state-of-the-art\nrecommenders, emphasizing the advantages of GraphPL in terms of effectiveness,\nrobustness and efficiency.",
            "author": [
                "Yuhao Yang",
                "Lianghao Xia",
                "Da Luo",
                "Kangyi Lin",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16716v1",
                "http://arxiv.org/pdf/2311.16716v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16714v1",
            "title": "Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld",
            "updated": "2023-11-28T11:53:56Z",
            "published": "2023-11-28T11:53:56Z",
            "summary": "While large language models (LLMs) excel in a simulated world of texts, they\nstruggle to interact with the more realistic world without perceptions of other\nmodalities such as visual or audio signals. Although vision-language models\n(VLMs) integrate LLM modules (1) aligned with static image features, and (2)\nmay possess prior knowledge of world dynamics (as demonstrated in the text\nworld), they have not been trained in an embodied visual world and thus cannot\nalign with its dynamics. On the other hand, training an embodied agent in a\nnoisy visual world without expert guidance is often challenging and\ninefficient. In this paper, we train a VLM agent living in a visual world using\nan LLM agent excelling in a parallel text world (but inapplicable to the visual\nworld). Specifically, we distill LLM's reflection outcomes (improved actions by\nanalyzing mistakes) in a text world's tasks to finetune the VLM on the same\ntasks of the visual world, resulting in an Embodied Multi-Modal Agent (EMMA)\nquickly adapting to the visual world dynamics. Such cross-modality imitation\nlearning between the two parallel worlds enables EMMA to generalize to a broad\nscope of new tasks without any further guidance from the LLM expert. Extensive\nevaluations on the ALFWorld benchmark highlight EMMA's superior performance to\nSOTA VLM-based agents across diverse tasks, e.g., 20%-70% improvement in the\nsuccess rate.",
            "author": [
                "Yijun Yang",
                "Tianyi Zhou",
                "Kanxue Li",
                "Dapeng Tao",
                "Lusong Li",
                "Li Shen",
                "Xiaodong He",
                "Jing Jiang",
                "Yuhui Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16714v1",
                "http://arxiv.org/pdf/2311.16714v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16713v1",
            "title": "\u03c0-\u03c0 Interaction-facilitated formation of interwoven trimeric\n  cage-catenanes with topological chirality",
            "updated": "2023-11-28T11:53:25Z",
            "published": "2023-11-28T11:53:25Z",
            "summary": "Catenanes as interlocked molecules with a nonplanar graph have gained\nincreasing attention for their unique features such as topological chirality.\nTo date, the majority of research in this field has been focusing on catenanes\ncomprising monocyclic rings. Due to the lack of rational synthetic strategy,\ncatenanes of cage-like monomers are hardly accessible. Here we report on the\nconstruction of an interwoven trimeric catenane that is composed of achiral\norganic cages, which exhibits topological chirality. Our rational design begins\nwith a pure mathematical analysis, revealing that the formation probability of\nthe interwoven trimeric catenane surpasses that of its chain-like analogue by\n20%; while driven by efficient template effect provided by strong {\\pi}-{\\pi}\nstacking of aromatic panels, the interwoven structure emerges as the dominant\nspecies, almost ruling out the formation of the chain-like isomer. Its\ntopological chirality is unambiguously unravelled by chiral-HPLC, CD\nspectroscopy and X-ray diffraction. Our probability analysis-aided rational\ndesign strategy would pave a new venue for the efficient synthesis of\ntopologically sophisticated structures in one pot.",
            "author": [
                "Lihua Chen",
                "Zhenghong Chen",
                "Weihao Wang",
                "Chenhao Chen",
                "Kuboi Yoshiaki",
                "Chi Zhang",
                "Chenfei Li",
                "Shaodong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16713v1",
                "http://arxiv.org/pdf/2311.16713v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16709v1",
            "title": "Superfluid Fraction and Leggett Bound in a Density Modulated Strongly\n  Interacting Fermi Gas at Zero Temperature",
            "updated": "2023-11-28T11:37:31Z",
            "published": "2023-11-28T11:37:31Z",
            "summary": "We calculate the superfluid fraction of an interacting Fermi gas, in the\npresence of a one-dimensional periodic potential of strength $V_0$ and\nwave-vector $q$. Special focus is given to the unitary Fermi gas, characterized\nby the divergent behavior of the s-wave scattering length. Comparison with the\nLeggett's upper bound $(\\langle n_{1D}\\rangle <1/n_{1D}>)^{-1}$, with $n_{1D}$\nthe 1D column density, explicitly shows that, differently from the case of a\ndilute interacting Bose gas, the bound significantly overestimates the value of\nthe superfluid fraction, except in the phonon regime of small $q$. Sum rule\narguments show that the combined knowledge of the Leggett bound and of the\nactual value of the superfluid fraction allows for the determination of\ncurvature effects providing the deviation of the dispersion of the\nAnderson-Bogoliubov mode from the linear phonon dependence. The comparison with\nthe predictions of the weakly interacting BCS Fermi gas points out the crucial\nrole of two-body interactions. The implications of our predictions on the\nanisotropic behavior of the sound velocity are also discussed.",
            "author": [
                "Giuliano Orso",
                "Sandro Stringari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16709v1",
                "http://arxiv.org/pdf/2311.16709v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16700v1",
            "title": "Rethinking Intermediate Layers design in Knowledge Distillation for\n  Kidney and Liver Tumor Segmentation",
            "updated": "2023-11-28T11:22:08Z",
            "published": "2023-11-28T11:22:08Z",
            "summary": "Knowledge distillation(KD) has demonstrated remarkable success across various\ndomains, but its application to medical imaging tasks, such as kidney and liver\ntumor segmentation, has encountered challenges. Many existing KD methods are\nnot specifically tailored for these tasks. Moreover, prevalent KD methods often\nlack a careful consideration of what and from where to distill knowledge from\nthe teacher to the student. This oversight may lead to issues like the\naccumulation of training bias within shallower student layers, potentially\ncompromising the effectiveness of KD. To address these challenges, we propose\nHierarchical Layer-selective Feedback Distillation (HLFD). HLFD strategically\ndistills knowledge from a combination of middle layers to earlier layers and\ntransfers final layer knowledge to intermediate layers at both the feature and\npixel levels. This design allows the model to learn higher-quality\nrepresentations from earlier layers, resulting in a robust and compact student\nmodel. Extensive quantitative evaluations reveal that HLFD outperforms existing\nmethods by a significant margin. For example, in the kidney segmentation task,\nHLFD surpasses the student model (without KD) by over 10pp, significantly\nimproving its focus on tumor-specific features. From a qualitative standpoint,\nthe student model trained using HLFD excels at suppressing irrelevant\ninformation and can focus sharply on tumor-specific details, which opens a new\npathway for more efficient and accurate diagnostic tools.",
            "author": [
                "Vandan Gorade",
                "Sparsh Mittal",
                "Debesh Jha",
                "Ulas Bagci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16700v1",
                "http://arxiv.org/pdf/2311.16700v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG",
                "q-bio.TO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03734v1",
            "title": "Conditional Prompt Tuning for Multimodal Fusion",
            "updated": "2023-11-28T11:05:20Z",
            "published": "2023-11-28T11:05:20Z",
            "summary": "We show that the representation of one modality can effectively guide the\nprompting of another modality for parameter-efficient multimodal fusion.\nSpecifically, we first encode one modality and use its representation as a\nprior to conditionally prompt all frozen layers of the other modality. This is\nachieved by disentangling the vanilla prompt vectors into three types of\nspecialized prompts that adaptively capture global-level and instance-level\nfeatures. To better produce the instance-wise prompt, we introduce the mixture\nof prompt experts (MoPE) to dynamically route each instance to the most\nsuitable prompt experts for encoding. We further study a regularization term to\navoid degenerated prompt expert routing. Thanks to our design, our method can\neffectively transfer the pretrained knowledge in unimodal encoders for\ndownstream multimodal tasks. Compared with vanilla prompting, we show that our\nMoPE-based conditional prompting is more expressive, thereby scales better with\ntraining data and the total number of prompts. We also demonstrate that our\nprompt tuning is architecture-agnostic, thereby offering high modularity.\nExtensive experiments over three multimodal datasets demonstrate\nstate-of-the-art results, matching or surpassing the performance achieved\nthrough fine-tuning, while only necessitating 0.7% of the trainable parameters.\nCode will be released: https://github.com/songrise/ConditionalPrompt.",
            "author": [
                "Ruixiang Jiang",
                "Lingbo Liu",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03734v1",
                "http://arxiv.org/pdf/2312.03734v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16689v1",
            "title": "Parameterised Geant4 simulation for total body PET research",
            "updated": "2023-11-28T11:01:27Z",
            "published": "2023-11-28T11:01:27Z",
            "summary": "Total-body positron emission tomography (PET) imaging has the potential to\ntransform medical care of a number of diseases and augment our knowledge of\nsystems biology. Various detector designs and geometries are currently under\ndevelopment for total-body PET imaging of humans. This variety, in particular\nthe variation in axial field-of-view (aFOV), motivates a need to compare the\nperformance of these devices in a consistent simulated environment.\n  We present an open-source Geant4 simulation package that allows variation of\nrelevant parameters such as the detector aFOV and the tracer radioisotope from\nthe command line. Two simplified detector geometries based on the Siemens\nBiograph Vision Quadra and United Imaging uEXPLORER models are supported with\nvariable granularity. The intrinsic radioactivity of the detector crystals is\nfully simulated. The simulation can be viewed with the built-in GUI, and the\nresults are saved in a plain text format for easy analysis. Example Python\nanalysis code is provided with the simulation, demonstrating calculation of the\nnoise equivalent count rate (NECR) figure of merit using an approximation to\nthe NEMA NU 2-2012 standard method.\n  A good agreement between the simulated count rate performance and\nexperimental data is observed for both geometries. The differences in results\nare attributed to simplifications in the simulation code, namely not accounting\nfor the light-collection efficiency or readout dead-time. We demonstrate the\nimportance of assessing the scanner performance using appropriate phantom\nlength which significantly affects the obtained results. A dependence between\nthe detector aFOV and the length of the source, with peak NECR plateauing as\nthe detector extends beyond the region of interest is also presented.",
            "author": [
                "Benjamin M. Wynne",
                "Hanna Borecka-Bielska",
                "Matthew Needham",
                "Adriana A. S. Tavares"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16689v1",
                "http://arxiv.org/pdf/2311.16689v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16683v1",
            "title": "Hyper-Relational Knowledge Graph Neural Network for Next POI",
            "updated": "2023-11-28T10:55:00Z",
            "published": "2023-11-28T10:55:00Z",
            "summary": "With the advancement of mobile technology, Point of Interest (POI)\nrecommendation systems in Location-based Social Networks (LBSN) have brought\nnumerous benefits to both users and companies. Many existing works employ\nKnowledge Graph (KG) to alleviate the data sparsity issue in LBSN. These\napproaches primarily focus on modeling the pair-wise relations in LBSN to\nenrich the semantics and thereby relieve the data sparsity issue. However,\nexisting approaches seldom consider the hyper-relations in LBSN, such as the\nmobility relation (a 3-ary relation: user-POI-time). This makes the model hard\nto exploit the semantics accurately. In addition, prior works overlook the rich\nstructural information inherent in KG, which consists of higher-order relations\nand can further alleviate the impact of data sparsity.To this end, we propose a\nHyper-Relational Knowledge Graph Neural Network (HKGNN) model. In HKGNN, a\nHyper-Relational Knowledge Graph (HKG) that models the LBSN data is constructed\nto maintain and exploit the rich semantics of hyper-relations. Then we proposed\na Hypergraph Neural Network to utilize the structural information of HKG in a\ncohesive way. In addition, a self-attention network is used to leverage\nsequential information and make personalized recommendations. Furthermore, side\ninformation, essential in reducing data sparsity by providing background\nknowledge of POIs, is not fully utilized in current methods. In light of this,\nwe extended the current dataset with available side information to further\nlessen the impact of data sparsity. Results of experiments on four real-world\nLBSN datasets demonstrate the effectiveness of our approach compared to\nexisting state-of-the-art methods.",
            "author": [
                "Jixiao Zhang",
                "Yongkang Li",
                "Ruotong Zou",
                "Jingyuan Zhang",
                "Zipei Fan",
                "Xuan Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16683v1",
                "http://arxiv.org/pdf/2311.16683v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16680v2",
            "title": "ROSO: Improving Robotic Policy Inference via Synthetic Observations",
            "updated": "2023-11-29T05:16:40Z",
            "published": "2023-11-28T10:52:35Z",
            "summary": "In this paper, we propose the use of generative artificial intelligence (AI)\nto improve zero-shot performance of a pre-trained policy by altering\nobservations during inference. Modern robotic systems, powered by advanced\nneural networks, have demonstrated remarkable capabilities on pre-trained\ntasks. However, generalizing and adapting to new objects and environments is\nchallenging, and fine-tuning visuomotor policies is time-consuming. To overcome\nthese issues we propose Robotic Policy Inference via Synthetic Observations\n(ROSO). ROSO uses stable diffusion to pre-process a robot's observation of\nnovel objects during inference time to fit within its distribution of\nobservations of the pre-trained policies. This novel paradigm allows us to\ntransfer learned knowledge from known tasks to previously unseen scenarios,\nenhancing the robot's adaptability without requiring lengthy fine-tuning. Our\nexperiments show that incorporating generative AI into robotic inference\nsignificantly improves successful outcomes, finishing up to 57% of tasks\notherwise unsuccessful with the pre-trained policy.",
            "author": [
                "Yusuke Miyashita",
                "Dimitris Gahtidis",
                "Colin La",
                "Jeremy Rabinowicz",
                "Jurgen Leitner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16680v2",
                "http://arxiv.org/pdf/2311.16680v2"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17109v1",
            "title": "Neural Texture Puppeteer: A Framework for Neural Geometry and Texture\n  Rendering of Articulated Shapes, Enabling Re-Identification at Interactive\n  Speed",
            "updated": "2023-11-28T10:51:05Z",
            "published": "2023-11-28T10:51:05Z",
            "summary": "In this paper, we present a neural rendering pipeline for textured\narticulated shapes that we call Neural Texture Puppeteer. Our method separates\ngeometry and texture encoding. The geometry pipeline learns to capture spatial\nrelationships on the surface of the articulated shape from ground truth data\nthat provides this geometric information. A texture auto-encoder makes use of\nthis information to encode textured images into a global latent code. This\nglobal texture embedding can be efficiently trained separately from the\ngeometry, and used in a downstream task to identify individuals. The neural\ntexture rendering and the identification of individuals run at interactive\nspeeds. To the best of our knowledge, we are the first to offer a promising\nalternative to CNN- or transformer-based approaches for re-identification of\narticulated individuals based on neural rendering. Realistic looking novel view\nand pose synthesis for different synthetic cow textures further demonstrate the\nquality of our method. Restricted by the availability of ground truth data for\nthe articulated shape's geometry, the quality for real-world data synthesis is\nreduced. We further demonstrate the flexibility of our model for real-world\ndata by applying a synthetic to real-world texture domain shift where we\nreconstruct the texture from a real-world 2D RGB image. Thus, our method can be\napplied to endangered species where data is limited. Our novel synthetic\ntexture dataset NePuMoo is publicly available to inspire further development in\nthe field of neural rendering-based re-identification.",
            "author": [
                "Urs Waldmann",
                "Ole Johannsen",
                "Bastian Goldluecke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17109v1",
                "http://arxiv.org/pdf/2311.17109v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16670v1",
            "title": "PyTorch Geometric High Order: A Unified Library for High Order Graph\n  Neural Network",
            "updated": "2023-11-28T10:34:48Z",
            "published": "2023-11-28T10:34:48Z",
            "summary": "We introduce PyTorch Geometric High Order (PyGHO), a library for High Order\nGraph Neural Networks (HOGNNs) that extends PyTorch Geometric (PyG). Unlike\nordinary Message Passing Neural Networks (MPNNs) that exchange messages between\nnodes, HOGNNs, encompassing subgraph GNNs and k-WL GNNs, encode node tuples, a\nmethod previously lacking a standardized framework and often requiring complex\ncoding. PyGHO's main objective is to provide an unified and user-friendly\ninterface for various HOGNNs. It accomplishes this through streamlined data\nstructures for node tuples, comprehensive data processing utilities, and a\nflexible suite of operators for high-order GNN methodologies. In this work, we\npresent a detailed in-depth of PyGHO and compare HOGNNs implemented with PyGHO\nwith their official implementation on real-world tasks. PyGHO achieves up to\n$50\\%$ acceleration and reduces the code needed for implementation by an order\nof magnitude. Our library is available at\n\\url{https://github.com/GraphPKU/PygHO}.",
            "author": [
                "Xiyuan Wang",
                "Muhan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16670v1",
                "http://arxiv.org/pdf/2311.16670v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16666v1",
            "title": "MultiModal-Learning for Predicting Molecular Properties: A Framework\n  Based on Image and Graph Structures",
            "updated": "2023-11-28T10:28:35Z",
            "published": "2023-11-28T10:28:35Z",
            "summary": "The quest for accurate prediction of drug molecule properties poses a\nfundamental challenge in the realm of Artificial Intelligence Drug Discovery\n(AIDD). An effective representation of drug molecules emerges as a pivotal\ncomponent in this pursuit. Contemporary leading-edge research predominantly\nresorts to self-supervised learning (SSL) techniques to extract meaningful\nstructural representations from large-scale, unlabeled molecular data,\nsubsequently fine-tuning these representations for an array of downstream\ntasks. However, an inherent shortcoming of these studies lies in their singular\nreliance on one modality of molecular information, such as molecule image or\nSMILES representations, thus neglecting the potential complementarity of\nvarious molecular modalities. In response to this limitation, we propose MolIG,\na novel MultiModaL molecular pre-training framework for predicting molecular\nproperties based on Image and Graph structures. MolIG model innovatively\nleverages the coherence and correlation between molecule graph and molecule\nimage to execute self-supervised tasks, effectively amalgamating the strengths\nof both molecular representation forms. This holistic approach allows for the\ncapture of pivotal molecular structural characteristics and high-level semantic\ninformation. Upon completion of pre-training, Graph Neural Network (GNN)\nEncoder is used for the prediction of downstream tasks. In comparison to\nadvanced baseline models, MolIG exhibits enhanced performance in downstream\ntasks pertaining to molecular property prediction within benchmark groups such\nas MoleculeNet Benchmark Group and ADMET Benchmark Group.",
            "author": [
                "Zhuoyuan Wang",
                "Jiacong Mi",
                "Shan Lu",
                "Jieyue He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16666v1",
                "http://arxiv.org/pdf/2311.16666v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "physics.chem-ph",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16665v1",
            "title": "Recognizing trees from incomplete decks",
            "updated": "2023-11-28T10:26:27Z",
            "published": "2023-11-28T10:26:27Z",
            "summary": "For a given graph, the unlabeled subgraphs $G-v$ are called the cards of $G$\nand the deck of $G$ is the multiset $\\{G-v: v \\in V(G)\\}$. Wendy Myrvold [Ars\nCombinatoria, 1989] showed that a non-connected graph and a connected graph\nboth on $n$ vertices have at most $\\lfloor \\frac{n}{2} \\rfloor +1$ cards in\ncommon and she found (infinite) families of trees and non-connected forests for\nwhich this upper bound is tight. Bowler, Brown, and Fenner [Journal of Graph\nTheory, 2010] conjectured that this bound is tight for $n \\geq 44$. In this\narticle, we prove this conjecture for sufficiently large $n$. The main result\nis that a tree $T$ and a unicyclic graph $G$ on $n$ vertices have at most\n$\\lfloor \\frac{n}{2} \\rfloor+1$ common cards. Combined with Myrvold's work this\nshows that it can be determined whether a graph on $n$ vertices is a tree from\nany $\\lfloor \\frac{n}{2}\\rfloor+1$ of its cards.\n  Based on this theorem, it follows that any forest and non-forest also have at\nmost $\\lfloor \\frac{n}{2} \\rfloor +1$ common cards. Moreover, we have\nclassified all except finitely many pairs for which this bound is strict.\nFurthermore, the main ideas of the proof for trees are used to show that the\ngirth of a graph on $n$ vertices can be determined based on any $\\frac{2n}{3}\n+1$ of its cards. Lastly, we show that any $\\frac{5n}{6} +2$ cards determine\nwhether a graph is bipartite.",
            "author": [
                "Gabri\u00eblle Zwaneveld"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16665v1",
                "http://arxiv.org/pdf/2311.16665v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16654v1",
            "title": "Elucidating Discrepancy in Explanations of Predictive Models Developed\n  using EMR",
            "updated": "2023-11-28T10:13:31Z",
            "published": "2023-11-28T10:13:31Z",
            "summary": "The lack of transparency and explainability hinders the clinical adoption of\nMachine learning (ML) algorithms. While explainable artificial intelligence\n(XAI) methods have been proposed, little research has focused on the agreement\nbetween these methods and expert clinical knowledge. This study applies current\nstate-of-the-art explainability methods to clinical decision support algorithms\ndeveloped for Electronic Medical Records (EMR) data to analyse the concordance\nbetween these factors and discusses causes for identified discrepancies from a\nclinical and technical perspective. Important factors for achieving trustworthy\nXAI solutions for clinical decision support are also discussed.",
            "author": [
                "Aida Brankovic",
                "Wenjie Huang",
                "David Cook",
                "Sankalp Khanna",
                "Konstanty Bialkowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16654v1",
                "http://arxiv.org/pdf/2311.16654v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16644v1",
            "title": "Finnish 5th and 6th graders' misconceptions about Artificial\n  Intelligence",
            "updated": "2023-11-28T09:49:11Z",
            "published": "2023-11-28T09:49:11Z",
            "summary": "Research on children's initial conceptions of AI is in an emerging state,\nwhich, from a constructivist viewpoint, challenges the development of\npedagogically sound AI-literacy curricula, methods, and materials. To\ncontribute to resolving this need in the present paper, qualitative survey data\nfrom 195 children were analyzed abductively to answer the following three\nresearch questions: What kind of misconceptions do Finnish 5th and 6th graders'\nhave about the essence AI?; 2) How do these misconceptions relate to common\nmisconception types?; and 3) How profound are these misconceptions? As a\nresult, three misconception categories were identified: 1) Non-technological\nAI, in which AI was conceptualized as peoples' cognitive processes (factual\nmisconception); 2) Anthropomorphic AI, in which AI was conceptualized as a\nhuman-like entity (vernacular, non-scientific, and conceptual misconception);\nand 3) AI as a machine with a pre-installed intelligence or knowledge (factual\nmisconception). Majority of the children evaluated their AI-knowledge low,\nwhich implies that the misconceptions are more superficial than profound. The\nfindings suggest that context-specific linguistic features can contribute to\nstudents' AI misconceptions. Implications for future research and AI literacy\neducation are discussed.",
            "author": [
                "Pekka Mertala",
                "Janne Fagerlund"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16644v1",
                "http://arxiv.org/pdf/2311.16644v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16632v1",
            "title": "Opening the Black Box: Towards inherently interpretable energy data\n  imputation models using building physics insight",
            "updated": "2023-11-28T09:34:44Z",
            "published": "2023-11-28T09:34:44Z",
            "summary": "Missing data are frequently observed by practitioners and researchers in the\nbuilding energy modeling community. In this regard, advanced data-driven\nsolutions, such as Deep Learning methods, are typically required to reflect the\nnon-linear behavior of these anomalies. As an ongoing research question related\nto Deep Learning, a model's applicability to limited data settings can be\nexplored by introducing prior knowledge in the network. This same strategy can\nalso lead to more interpretable predictions, hence facilitating the field\napplication of the approach. For that purpose, the aim of this paper is to\npropose the use of Physics-informed Denoising Autoencoders (PI-DAE) for missing\ndata imputation in commercial buildings. In particular, the presented method\nenforces physics-inspired soft constraints to the loss function of a Denoising\nAutoencoder (DAE). In order to quantify the benefits of the physical component,\nan ablation study between different DAE configurations is conducted. First,\nthree univariate DAEs are optimized separately on indoor air temperature,\nheating, and cooling data. Then, two multivariate DAEs are derived from the\nprevious configurations. Eventually, a building thermal balance equation is\ncoupled to the last multivariate configuration to obtain PI-DAE. Additionally,\ntwo commonly used benchmarks are employed to support the findings. It is shown\nhow introducing physical knowledge in a multivariate Denoising Autoencoder can\nenhance the inherent model interpretability through the optimized physics-based\ncoefficients. While no significant improvement is observed in terms of\nreconstruction error with the proposed PI-DAE, its enhanced robustness to\nvarying rates of missing data and the valuable insights derived from the\nphysics-based coefficients create opportunities for wider applications within\nbuilding systems and the built environment.",
            "author": [
                "Antonio Liguori",
                "Matias Quintana",
                "Chun Fu",
                "Clayton Miller",
                "J\u00e9r\u00f4me Frisch",
                "Christoph van Treeck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16632v1",
                "http://arxiv.org/pdf/2311.16632v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17104v1",
            "title": "Single-Cell Clustering via Dual-Graph Alignment",
            "updated": "2023-11-28T09:14:55Z",
            "published": "2023-11-28T09:14:55Z",
            "summary": "In recent years, the field of single-cell RNA sequencing has seen a surge in\nthe development of clustering methods. These methods enable the identification\nof cell subpopulations, thereby facilitating the understanding of tumor\nmicroenvironments. Despite their utility, most existing clustering algorithms\nprimarily focus on the attribute information provided by the cell matrix or the\nnetwork structure between cells, often neglecting the network between genes.\nThis oversight could lead to loss of information and clustering results that\nlack clinical significance. To address this limitation, we develop an advanced\nsingle-cell clustering model incorporating dual-graph alignment, which\nintegrates gene network information into the clustering process based on\nself-supervised and unsupervised optimization. Specifically, we designed a\ngraph-based autoencoder enhanced by an attention mechanism to effectively\ncapture relationships between cells. Moreover, we performed the node2vec method\non Protein-Protein Interaction (PPI) networks to derive the gene network\nstructure and maintained this structure throughout the clustering process. Our\nproposed method has been demonstrated to be effective through experimental\nresults, showcasing its ability to optimize clustering outcomes while\npreserving the original associations between cells and genes. This research\ncontributes to obtaining accurate cell subpopulations and generates clustering\nresults that more closely resemble real-world biological scenarios. It provides\nbetter insights into the characteristics and distribution of diseased cells,\nultimately building a foundation for early disease diagnosis and treatment.",
            "author": [
                "Dayu Hu",
                "Ke Liang",
                "Xinwang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17104v1",
                "http://arxiv.org/pdf/2311.17104v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.MN"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16605v2",
            "title": "LasTGL: An Industrial Framework for Large-Scale Temporal Graph Learning",
            "updated": "2023-11-30T09:19:39Z",
            "published": "2023-11-28T08:45:37Z",
            "summary": "Over the past few years, graph neural networks (GNNs) have become powerful\nand practical tools for learning on (static) graph-structure data. However,\nmany real-world applications, such as social networks and e-commerce, involve\ntemporal graphs where nodes and edges are dynamically evolving. Temporal graph\nneural networks (TGNNs) have progressively emerged as an extension of GNNs to\naddress time-evolving graphs and have gradually become a trending research\ntopic in both academics and industry. Advancing research and application in\nsuch an emerging field necessitates the development of new tools to compose\nTGNN models and unify their different schemes for dealing with temporal graphs.\nIn this work, we introduce LasTGL, an industrial framework that integrates\nunified and extensible implementations of common temporal graph learning\nalgorithms for various advanced tasks. The purpose of LasTGL is to provide the\nessential building blocks for solving temporal graph learning tasks, focusing\non the guiding principles of user-friendliness and quick prototyping on which\nPyTorch is based. In particular, LasTGL provides comprehensive temporal graph\ndatasets, TGNN models and utilities along with well-documented tutorials,\nmaking it suitable for both absolute beginners and expert deep learning\npractitioners alike.",
            "author": [
                "Jintang Li",
                "Jiawang Dan",
                "Ruofan Wu",
                "Jing Zhou",
                "Sheng Tian",
                "Yunfei Liu",
                "Baokun Wang",
                "Changhua Meng",
                "Weiqiang Wang",
                "Yuchang Zhu",
                "Liang Chen",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16605v2",
                "http://arxiv.org/pdf/2311.16605v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16604v1",
            "title": "LC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker\n  Verification Models",
            "updated": "2023-11-28T08:44:04Z",
            "published": "2023-11-28T08:44:04Z",
            "summary": "The performance of speaker verification (SV) models may drop dramatically in\nnoisy environments. A speech enhancement (SE) module can be used as a front-end\nstrategy. However, existing SE methods may fail to bring performance\nimprovements to downstream SV systems due to artifacts in the predicted signals\nof SE models. To compensate for artifacts, we propose a generic denoising\nframework named LC4SV, which can serve as a pre-processor for various unknown\ndownstream SV models. In LC4SV, we employ a learning-based interpolation agent\nto automatically generate the appropriate coefficients between the enhanced\nsignal and its noisy input to improve SV performance in noisy environments. Our\nexperimental results demonstrate that LC4SV consistently improves the\nperformance of various unseen SV systems. To the best of our knowledge, this\nwork is the first attempt to develop a learning-based interpolation scheme\naiming at improving SV performance in noisy environments.",
            "author": [
                "Chi-Chang Lee",
                "Hong-Wei Chen",
                "Chu-Song Chen",
                "Hsin-Min Wang",
                "Tsung-Te Liu",
                "Yu Tsao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16604v1",
                "http://arxiv.org/pdf/2311.16604v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16603v1",
            "title": "l2Match: Optimization Techniques on Subgraph Matching Algorithm using\n  Label Pair, Neighboring Label Index, and Jump-Redo method",
            "updated": "2023-11-28T08:44:01Z",
            "published": "2023-11-28T08:44:01Z",
            "summary": "Graph database is designed to store bidirectional relationships between\nobjects and facilitate the traversal process to extract a subgraph. However,\nthe subgraph matching process is an NP-Complete problem. Existing solutions to\nthis problem usually employ a filter-and-verification framework and a\ndivide-and-conquer method. The filter-and-verification framework minimizes the\nnumber of inputs to the verification stage by filtering and pruning invalid\ncandidates as much as possible. Meanwhile, subgraph matching is performed on\nthe substructure decomposed from the larger graph to yield partial embedding.\nSubsequently, the recursive traversal or set intersection technique combines\nthe partial embedding into a complete subgraph. In this paper, we first present\na comprehensive literature review of the state-of-the-art solutions. l2Match, a\nsubgraph isomorphism algorithm for small queries utilizing a Label-Pair Index\nand filtering method, is then proposed and presented as a proof of concept.\nEmpirical experimentation shows that l2Match outperforms related\nstate-of-the-art solutions, and the proposed methods optimize the existing\nalgorithms.",
            "author": [
                "C. Q. Cheng",
                "K. S. Wong",
                "L. K. Soon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16603v1",
                "http://arxiv.org/pdf/2311.16603v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.IR",
                "05C60 (Primary), 05C30 (Secondary), 68R10",
                "G.4.1; H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16602v1",
            "title": "GSP-KalmanNet: Tracking Graph Signals via Neural-Aided Kalman Filtering",
            "updated": "2023-11-28T08:43:10Z",
            "published": "2023-11-28T08:43:10Z",
            "summary": "Dynamic systems of graph signals are encountered in various applications,\nincluding social networks, power grids, and transportation. While such systems\ncan often be described as state space (SS) models, tracking graph signals via\nconventional tools based on the Kalman filter (KF) and its variants is\ntypically challenging. This is due to the nonlinearity, high dimensionality,\nirregularity of the domain, and complex modeling associated with real-world\ndynamic systems of graph signals. In this work, we study the tracking of graph\nsignals using a hybrid model-based/data-driven approach. We develop the\nGSP-KalmanNet, which tracks the hidden graphical states from the graphical\nmeasurements by jointly leveraging graph signal processing (GSP) tools and deep\nlearning (DL) techniques. The derivations of the GSP-KalmanNet are based on\nextending the KF to exploit the inherent graph structure via graph frequency\ndomain filtering, which considerably simplifies the computational complexity\nentailed in processing high-dimensional signals and increases the robustness to\nsmall topology changes. Then, we use data to learn the Kalman gain following\nthe recently proposed KalmanNet framework, which copes with partial and\napproximated modeling, without forcing a specific model over the noise\nstatistics. Our empirical results demonstrate that the proposed GSP-KalmanNet\nachieves enhanced accuracy and run time performance as well as improved\nrobustness to model misspecifications compared with both model-based and\ndata-driven benchmarks.",
            "author": [
                "Itay Buchnik",
                "Guy Sagi",
                "Nimrod Leinwand",
                "Yuval Loya",
                "Nir Shlezinger",
                "Tirza Routtenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16602v1",
                "http://arxiv.org/pdf/2311.16602v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.02992v1",
            "title": "Advancing Web Accessibility -- A guide to transitioning Design Systems\n  from WCAG 2.0 to WCAG 2.1",
            "updated": "2023-11-28T08:33:32Z",
            "published": "2023-11-28T08:33:32Z",
            "summary": "This research focuses on the critical process of upgrading a Design System\nfrom Web Content Accessibility Guidelines (WCAG) 2.0 to WCAG 2.1, which is an\nessential step in enhancing web accessibility. It emphasizes the importance of\nstaying up to date on increasing accessibility requirements, as well as the\ncritical function of Design Systems in supporting inclusion in digital\nenvironments. The article lays out a complete strategy for meeting WCAG 2.1\ncompliance. Assessment, strategic planning, implementation, and testing are all\npart of this strategy. The need for collaboration and user involvement is\nemphasized as critical strategies and best practices for a successful migration\njourney. In addition, the article digs into migration barriers and discusses\nsignificant lessons acquired, offering a realistic view of the intricacies of\nthis transforming road. Finally, it is a practical guide and a necessary\nresource for organizations committed to accessible and user-centered design.\nThe document provides them with the knowledge and resources they need to\nnavigate the changing world of web accessibility properly.",
            "author": [
                "Hardik Shah"
            ],
            "link": [
                "http://dx.doi.org/10.5121/csit.2023.132218",
                "http://arxiv.org/abs/2312.02992v1",
                "http://arxiv.org/pdf/2312.02992v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16595v1",
            "title": "D4AM: A General Denoising Framework for Downstream Acoustic Models",
            "updated": "2023-11-28T08:27:27Z",
            "published": "2023-11-28T08:27:27Z",
            "summary": "The performance of acoustic models degrades notably in noisy environments.\nSpeech enhancement (SE) can be used as a front-end strategy to aid automatic\nspeech recognition (ASR) systems. However, existing training objectives of SE\nmethods are not fully effective at integrating speech-text and noisy-clean\npaired data for training toward unseen ASR systems. In this study, we propose a\ngeneral denoising framework, D4AM, for various downstream acoustic models. Our\nframework fine-tunes the SE model with the backward gradient according to a\nspecific acoustic model and the corresponding classification objective. In\naddition, our method aims to consider the regression objective as an auxiliary\nloss to make the SE model generalize to other unseen acoustic models. To\njointly train an SE unit with regression and classification objectives, D4AM\nuses an adjustment scheme to directly estimate suitable weighting coefficients\nrather than undergoing a grid search process with additional training costs.\nThe adjustment scheme consists of two parts: gradient calibration and\nregression objective weighting. The experimental results show that D4AM can\nconsistently and effectively provide improvements to various unseen acoustic\nmodels and outperforms other combination setups. Specifically, when evaluated\non the Google ASR API with real noisy data completely unseen during SE\ntraining, D4AM achieves a relative WER reduction of 24.65% compared with the\ndirect feeding of noisy input. To our knowledge, this is the first work that\ndeploys an effective combination scheme of regression (denoising) and\nclassification (ASR) objectives to derive a general pre-processor applicable to\nvarious unseen ASR systems. Our code is available at\nhttps://github.com/ChangLee0903/D4AM.",
            "author": [
                "Chi-Chang Lee",
                "Yu Tsao",
                "Hsin-Min Wang",
                "Chu-Song Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16595v1",
                "http://arxiv.org/pdf/2311.16595v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16584v1",
            "title": "FedAL: Black-Box Federated Knowledge Distillation Enabled by Adversarial\n  Learning",
            "updated": "2023-11-28T08:01:43Z",
            "published": "2023-11-28T08:01:43Z",
            "summary": "Knowledge distillation (KD) can enable collaborative learning among\ndistributed clients that have different model architectures and do not share\ntheir local data and model parameters with others. Each client updates its\nlocal model using the average model output/feature of all client models as the\ntarget, known as federated KD. However, existing federated KD methods often do\nnot perform well when clients' local models are trained with heterogeneous\nlocal datasets. In this paper, we propose Federated knowledge distillation\nenabled by Adversarial Learning (FedAL) to address the data heterogeneity among\nclients. First, to alleviate the local model output divergence across clients\ncaused by data heterogeneity, the server acts as a discriminator to guide\nclients' local model training to achieve consensus model outputs among clients\nthrough a min-max game between clients and the discriminator. Moreover,\ncatastrophic forgetting may happen during the clients' local training and\nglobal knowledge transfer due to clients' heterogeneous local data. Towards\nthis challenge, we design the less-forgetting regularization for both local\ntraining and global knowledge transfer to guarantee clients' ability to\ntransfer/learn knowledge to/from others. Experimental results show that FedAL\nand its variants achieve higher accuracy than other federated KD baselines.",
            "author": [
                "Pengchao Han",
                "Xingyan Shi",
                "Jianwei Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16584v1",
                "http://arxiv.org/pdf/2311.16584v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17097v1",
            "title": "Anonymous Jamming Detection in 5G with Bayesian Network Model Based\n  Inference Analysis",
            "updated": "2023-11-28T07:23:15Z",
            "published": "2023-11-28T07:23:15Z",
            "summary": "Jamming and intrusion detection are critical in 5G research, aiming to\nmaintain reliability, prevent user experience degradation, and avoid\ninfrastructure failure. This paper introduces an anonymous jamming detection\nmodel for 5G based on signal parameters from the protocol stacks. The system\nuses supervised and unsupervised learning for real-time, high-accuracy\ndetection of jamming, including unknown types. Supervised models reach an AUC\nof 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the\nneed for data annotation limits the supervised approach. To address this, an\nunsupervised auto-encoder-based anomaly detection is presented with an AUC of\n0.987. The approach is resistant to adversarial training samples. For\ntransparency and domain knowledge injection, a Bayesian network-based causation\nanalysis is introduced.",
            "author": [
                "Ying Wang",
                "Shashank Jere",
                "Soumya Banerjee",
                "Lingjia Liu",
                "Sachin Shetty",
                "Shehadi Dayekh"
            ],
            "link": [
                "http://dx.doi.org/10.1109/HPSR54439.2022.9831286",
                "http://arxiv.org/abs/2311.17097v1",
                "http://arxiv.org/pdf/2311.17097v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16565v2",
            "title": "DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D\n  Face Diffuser",
            "updated": "2023-12-02T16:48:09Z",
            "published": "2023-11-28T07:13:20Z",
            "summary": "Speech-driven 3D facial animation has been an attractive task in both\nacademia and industry. Traditional methods mostly focus on learning a\ndeterministic mapping from speech to animation. Recent approaches start to\nconsider the non-deterministic fact of speech-driven 3D face animation and\nemploy the diffusion model for the task. However, personalizing facial\nanimation and accelerating animation generation are still two major limitations\nof existing diffusion-based methods. To address the above limitations, we\npropose DiffusionTalker, a diffusion-based method that utilizes contrastive\nlearning to personalize 3D facial animation and knowledge distillation to\naccelerate 3D animation generation. Specifically, to enable personalization, we\nintroduce a learnable talking identity to aggregate knowledge in audio\nsequences. The proposed identity embeddings extract customized facial cues\nacross different people in a contrastive learning manner. During inference,\nusers can obtain personalized facial animation based on input audio, reflecting\na specific talking style. With a trained diffusion model with hundreds of\nsteps, we distill it into a lightweight model with 8 steps for acceleration.\nExtensive experiments are conducted to demonstrate that our method outperforms\nstate-of-the-art methods. The code will be released.",
            "author": [
                "Peng Chen",
                "Xiaobao Wei",
                "Ming Lu",
                "Yitong Zhu",
                "Naiming Yao",
                "Xingyu Xiao",
                "Hui Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16565v2",
                "http://arxiv.org/pdf/2311.16565v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16559v1",
            "title": "Graph Partitioning with Fujitsu Digital Annealer",
            "updated": "2023-11-28T06:59:01Z",
            "published": "2023-11-28T06:59:01Z",
            "summary": "Graph partitioning, or community detection, is the cornerstone of many\nfields, such as logistics, transportation and smart power grids. Efficient\ncomputation and efficacious evaluation of communities are both essential,\nespecially in commercial and industrial settings. However, the solution space\nof graph partitioning increases drastically with the number of vertices and\nsubgroups. With an eye to solving large scale graph partitioning and other\noptimization problems within a short period of time, the Digital Annealer (DA),\na specialized CMOS hardware also featuring improved algorithms, has been\ndevised by Fujitsu Ltd. This study gauges Fujitsu DA's performance and running\ntimes. The modularity was implemented as both the objective function and metric\nfor the solutions. The graph partitioning problems were formatted into\nQuadratic Unconstrained Binary Optimization (QUBO) structures so that they\ncould be adequately imported into the DA. The DA yielded the highest modularity\namong other studies when partitioning Karate Club, Les Miserables, American\nFootball, and Dolphin. Moreover, the DA was able to partition the Case\n1354pegase power grid network into 45 subgroups, calling for 60,930 binary\nvariables, whilst delivering optimal modularity results within a solving time\nof roughly 80 seconds. Our results suggest that the Fujitsu DA can be applied\nfor rapid and efficient optimization for graph partitioning.",
            "author": [
                "Yu-Ting Kao",
                "Hsiu-Chuan Hsu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16559v1",
                "http://arxiv.org/pdf/2311.16559v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17096v1",
            "title": "Robust Transductive Few-shot Learning via Joint Message Passing and\n  Prototype-based Soft-label Propagation",
            "updated": "2023-11-28T06:44:27Z",
            "published": "2023-11-28T06:44:27Z",
            "summary": "Few-shot learning (FSL) aims to develop a learning model with the ability to\ngeneralize to new classes using a few support samples. For transductive FSL\ntasks, prototype learning and label propagation methods are commonly employed.\nPrototype methods generally first learn the representative prototypes from the\nsupport set and then determine the labels of queries based on the metric\nbetween query samples and prototypes. Label propagation methods try to\npropagate the labels of support samples on the constructed graph encoding the\nrelationships between both support and query samples. This paper aims to\nintegrate these two principles together and develop an efficient and robust\ntransductive FSL approach, termed Prototype-based Soft-label Propagation\n(PSLP). Specifically, we first estimate the soft-label presentation for each\nquery sample by leveraging prototypes. Then, we conduct soft-label propagation\non our learned query-support graph. Both steps are conducted progressively to\nboost their respective performance. Moreover, to learn effective prototypes for\nsoft-label estimation as well as the desirable query-support graph for\nsoft-label propagation, we design a new joint message passing scheme to learn\nsample presentation and relational graph jointly. Our PSLP method is\nparameter-free and can be implemented very efficiently. On four popular\ndatasets, our method achieves competitive results on both balanced and\nimbalanced settings compared to the state-of-the-art methods. The code will be\nreleased upon acceptance.",
            "author": [
                "Jiahui Wang",
                "Qin Xu",
                "Bo Jiang",
                "Bin Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17096v1",
                "http://arxiv.org/pdf/2311.17096v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16548v1",
            "title": "Graph Theoretic Analysis of Three-Terminal Quantum Dot Thermocouples:\n  Onsager Relations and Spin-Thermoelectric Effects",
            "updated": "2023-11-28T06:35:02Z",
            "published": "2023-11-28T06:35:02Z",
            "summary": "We introduce a simplified model for a three-terminal quantum thermocouple\nconsisting of two strongly-coupled quantum dots. To elucidate spin-dependent\nSeebeck and Peltier effects, we employ a microscopic Hamiltonian and map the\nLindblad master equation onto a quantum transition network, capturing the key\nworking principles for both reciprocal effects. Our analysis reveals quantum\nthermodynamic networks encompassing both Coulomb interaction and spin-flipping\nprocesses, lead to the emergence of spin-thermolectric effects. Using algebraic\ngraph theory, we recover the phenomenological law of irreversible\nthermodynamics from the stochastic version of the entropy production rate\nexpressed in terms of cycle flux and cycle forces. Remarkably, Onsager\nreciprocity and Kelvin relation for transport coefficients find their premises\nin the properties of cycle flux trajectories within the quantum transition\nnetwork. This underscores the universal generality of thermodynamic principles\nacross classical and quantum realms, despite their fundamentally different\nbasis from classical laws of irreversible thermodynamics relying on local\nequilibrium assumptions.",
            "author": [
                "Nikhil Gupt",
                "Shuvadip Ghosh",
                "Arnab Ghosh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16548v1",
                "http://arxiv.org/pdf/2311.16548v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.stat-mech",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16546v1",
            "title": "Phase transitions for the $XY$ model in non-uniformly elliptic and\n  Poisson-Voronoi environments",
            "updated": "2023-11-28T06:30:59Z",
            "published": "2023-11-28T06:30:59Z",
            "summary": "The goal of this paper is to analyze how the celebrated phase transitions of\nthe $XY$ model are affected by the presence of a non-elliptic quenched\ndisorder.\n  In dimension $d=2$, we prove that if one considers an $XY$ model on the\ninfinite cluster of a supercritical percolation configuration, the\nBerezinskii-Kosterlitz-Thouless (BKT) phase transition still occurs despite the\npresence of quenched disorder. The proof works for all $p>p_c$ (site or edge).\nWe also show that the $XY$ model defined on a planar Poisson-Voronoi graph also\nundergoes a BKT phase transition.\n  When $d\\geq 3$, we show in a similar fashion that the continuous symmetry\nbreaking of the $XY$ model at low enough temperature is not affected by the\npresence of quenched disorder such as supercritical percolation (in\n$\\mathbb{Z}^d$) or Poisson-Voronoi (in $\\mathbb{R}^d$).\n  Adapting either Fr\\\"{o}hlich-Spencer's proof of existence of a BKT phase\ntransition or the more recent proofs of Lammers, van Engelenburg-Lis and\nAizenman-Harel-Peled-Shapiro to such non-uniformly elliptic disorders appears\nto be non-trivial. Instead, our proofs rely on a relatively little known\ncorrelation inequality called Wells' inequality.",
            "author": [
                "Paul Dario",
                "Christophe Garban"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16546v1",
                "http://arxiv.org/pdf/2311.16546v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16544v1",
            "title": "Multi-Irreducible Spectral Synchronization for Robust Rotation Averaging",
            "updated": "2023-11-28T06:25:26Z",
            "published": "2023-11-28T06:25:26Z",
            "summary": "Rotation averaging (RA) is a fundamental problem in robotics and computer\nvision. In RA, the goal is to estimate a set of $N$ unknown orientations\n$R_{1}, ..., R_{N} \\in SO(3)$, given noisy measurements $R_{ij} \\sim R^{-1}_{i}\nR_{j}$ of a subset of their pairwise relative rotations. This problem is both\nnonconvex and NP-hard, and thus difficult to solve in the general case. We\napply harmonic analysis on compact groups to derive a (convex) spectral\nrelaxation constructed from truncated Fourier decompositions of the individual\nsummands appearing in the RA objective; we then recover an estimate of the RA\nsolution by computing a few extremal eigenpairs of this relaxation, and\n(approximately) solving a consensus problem. Our approach affords several\nnotable advantages versus prior RA methods: it can be used in conjunction with\n\\emph{any} smooth loss function (including, but not limited to, robust\nM-estimators), does not require any initialization, and is implemented using\nonly simple (and highly scalable) linear-algebraic computations and\nparallelizable optimizations over band-limited functions of individual\nrotational states. Moreover, under the (physically well-motivated) assumption\nof multiplicative Langevin measurement noise, we derive explicit performance\nguarantees for our spectral estimator (in the form of probabilistic tail bounds\non the estimation error) that are parameterized in terms of graph-theoretic\nquantities of the underlying measurement network. By concretely linking\nestimator performance with properties of the underlying measurement graph, our\nresults also indicate how to devise measurement networks that are\n\\emph{guaranteed} to achieve accurate estimation, enabling such downstream\ntasks as sensor placement, network compression, and active sensing.",
            "author": [
                "Owen Howell",
                "Haoen Huang",
                "David Rosen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16544v1",
                "http://arxiv.org/pdf/2311.16544v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "math.GR",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16507v1",
            "title": "Exploring Straighter Trajectories of Flow Matching with Diffusion\n  Guidance",
            "updated": "2023-11-28T06:19:30Z",
            "published": "2023-11-28T06:19:30Z",
            "summary": "Flow matching as a paradigm of generative model achieves notable success\nacross various domains. However, existing methods use either multi-round\ntraining or knowledge within minibatches, posing challenges in finding a\nfavorable coupling strategy for straight trajectories. To address this issue,\nwe propose a novel approach, Straighter trajectories of Flow Matching\n(StraightFM). It straightens trajectories with the coupling strategy guided by\ndiffusion model from entire distribution level. First, we propose a coupling\nstrategy to straighten trajectories, creating couplings between image and noise\nsamples under diffusion model guidance. Second, StraightFM also integrates real\ndata to enhance training, employing a neural network to parameterize another\ncoupling process from images to noise samples. StraightFM is jointly optimized\nwith couplings from above two mutually complementary directions, resulting in\nstraighter trajectories and enabling both one-step and few-step generation.\nExtensive experiments demonstrate that StraightFM yields high quality samples\nwith fewer step. StraightFM generates visually appealing images with a lower\nFID among diffusion and traditional flow matching methods within 5 sampling\nsteps when trained on pixel space. In the latent space (i.e., Latent\nDiffusion), StraightFM achieves a lower KID value compared to existing methods\non the CelebA-HQ 256 dataset in fewer than 10 sampling steps.",
            "author": [
                "Siyu Xing",
                "Jie Cao",
                "Huaibo Huang",
                "Xiao-Yu Zhang",
                "Ran He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16507v1",
                "http://arxiv.org/pdf/2311.16507v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16542v1",
            "title": "Agents meet OKR: An Object and Key Results Driven Agent System with\n  Hierarchical Self-Collaboration and Self-Evaluation",
            "updated": "2023-11-28T06:16:30Z",
            "published": "2023-11-28T06:16:30Z",
            "summary": "In this study, we introduce the concept of OKR-Agent designed to enhance the\ncapabilities of Large Language Models (LLMs) in task-solving. Our approach\nutilizes both self-collaboration and self-correction mechanism, facilitated by\nhierarchical agents, to address the inherent complexities in task-solving. Our\nkey observations are two-fold: first, effective task-solving demands in-depth\ndomain knowledge and intricate reasoning, for which deploying specialized\nagents for individual sub-tasks can markedly enhance LLM performance. Second,\ntask-solving intrinsically adheres to a hierarchical execution structure,\ncomprising both high-level strategic planning and detailed task execution.\nTowards this end, our OKR-Agent paradigm aligns closely with this hierarchical\nstructure, promising enhanced efficacy and adaptability across a range of\nscenarios. Specifically, our framework includes two novel modules: hierarchical\nObjects and Key Results generation and multi-level evaluation, each\ncontributing to more efficient and robust task-solving. In practical,\nhierarchical OKR generation decomposes Objects into multiple sub-Objects and\nassigns new agents based on key results and agent responsibilities. These\nagents subsequently elaborate on their designated tasks and may further\ndecompose them as necessary. Such generation operates recursively and\nhierarchically, culminating in a comprehensive set of detailed solutions. The\nmulti-level evaluation module of OKR-Agent refines solution by leveraging\nfeedback from all associated agents, optimizing each step of the process. This\nensures solution is accurate, practical, and effectively address intricate task\nrequirements, enhancing the overall reliability and quality of the outcome.\nExperimental results also show our method outperforms the previous methods on\nseveral tasks. Code and demo are available at https://okr-agent.github.io/",
            "author": [
                "Yi Zheng",
                "Chongyang Ma",
                "Kanle Shi",
                "Haibin Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16542v1",
                "http://arxiv.org/pdf/2311.16542v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16534v1",
            "title": "Graph Prompt Learning: A Comprehensive Survey and Beyond",
            "updated": "2023-11-28T05:36:59Z",
            "published": "2023-11-28T05:36:59Z",
            "summary": "Artificial General Intelligence (AGI) has revolutionized numerous fields, yet\nits integration with graph data, a cornerstone in our interconnected world,\nremains nascent. This paper presents a pioneering survey on the emerging domain\nof graph prompts in AGI, addressing key challenges and opportunities in\nharnessing graph data for AGI applications. Despite substantial advancements in\nAGI across natural language processing and computer vision, the application to\ngraph data is relatively underexplored. This survey critically evaluates the\ncurrent landscape of AGI in handling graph data, highlighting the distinct\nchallenges in cross-modality, cross-domain, and cross-task applications\nspecific to graphs. Our work is the first to propose a unified framework for\nunderstanding graph prompt learning, offering clarity on prompt tokens, token\nstructures, and insertion patterns in the graph domain. We delve into the\nintrinsic properties of graph prompts, exploring their flexibility,\nexpressiveness, and interplay with existing graph models. A comprehensive\ntaxonomy categorizes over 100 works in this field, aligning them with\npre-training tasks across node-level, edge-level, and graph-level objectives.\nAdditionally, we present, ProG, a Python library, and an accompanying website,\nto support and advance research in graph prompting. The survey culminates in a\ndiscussion of current challenges and future directions, offering a roadmap for\nresearch in graph prompting within AGI. Through this comprehensive analysis, we\naim to catalyze further exploration and practical applications of AGI in graph\ndata, underlining its potential to reshape AGI fields and beyond. ProG and the\nwebsite can be accessed by\n\\url{https://github.com/WxxShirley/Awesome-Graph-Prompt}, and\n\\url{https://github.com/sheldonresearch/ProG}, respectively.",
            "author": [
                "Xiangguo Sun",
                "Jiawen Zhang",
                "Xixi Wu",
                "Hong Cheng",
                "Yun Xiong",
                "Jia Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16534v1",
                "http://arxiv.org/pdf/2311.16534v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16524v1",
            "title": "3D Teeth Reconstruction from Panoramic Radiographs using Neural Implicit\n  Functions",
            "updated": "2023-11-28T05:06:22Z",
            "published": "2023-11-28T05:06:22Z",
            "summary": "Panoramic radiography is a widely used imaging modality in dental practice\nand research. However, it only provides flattened 2D images, which limits the\ndetailed assessment of dental structures. In this paper, we propose Occudent, a\nframework for 3D teeth reconstruction from panoramic radiographs using neural\nimplicit functions, which, to the best of our knowledge, is the first work to\ndo so. For a given point in 3D space, the implicit function estimates whether\nthe point is occupied by a tooth, and thus implicitly determines the boundaries\nof 3D tooth shapes. Firstly, Occudent applies multi-label segmentation to the\ninput panoramic radiograph. Next, tooth shape embeddings as well as tooth class\nembeddings are generated from the segmentation outputs, which are fed to the\nreconstruction network. A novel module called Conditional eXcitation (CX) is\nproposed in order to effectively incorporate the combined shape and class\nembeddings into the implicit function. The performance of Occudent is evaluated\nusing both quantitative and qualitative measures. Importantly, Occudent is\ntrained and validated with actual panoramic radiographs as input, distinct from\nrecent works which used synthesized images. Experiments demonstrate the\nsuperiority of Occudent over state-of-the-art methods.",
            "author": [
                "Sihwa Park",
                "Seongjun Kim",
                "In-Seok Song",
                "Seung Jun Baek"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43999-5_36",
                "http://arxiv.org/abs/2311.16524v1",
                "http://arxiv.org/pdf/2311.16524v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16522v1",
            "title": "Evaluation of dynamic characteristics of power grid based on GNN and\n  application on knowledge graph",
            "updated": "2023-11-28T05:00:27Z",
            "published": "2023-11-28T05:00:27Z",
            "summary": "A novel method for detecting faults in power grids using a graph neural\nnetwork (GNN) has been developed, aimed at enhancing intelligent fault\ndiagnosis in network operation and maintenance. This GNN-based approach\nidentifies faulty nodes within the power grid through a specialized electrical\nfeature extraction model coupled with a knowledge graph. Incorporating temporal\ndata, the method leverages the status of nodes from preceding and subsequent\ntime periods to aid in current fault detection. To validate the effectiveness\nof this GNN in extracting node features, a correlation analysis of the output\nfeatures from each node within the neural network layer was conducted. The\nresults from experiments show that this method can accurately locate fault\nnodes in simulated scenarios with a remarkable 99.53% accuracy. Additionally,\nthe graph neural network's feature modeling allows for a qualitative\nexamination of how faults spread across nodes, providing valuable insights for\nanalyzing fault nodes.",
            "author": [
                "Hao Pei",
                "Si Lin",
                "Chuanfu Li",
                "Che Wang",
                "Haoming Chen",
                "Sizhe Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16522v1",
                "http://arxiv.org/pdf/2311.16522v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16464v1",
            "title": "Bridging the Gap: A Unified Video Comprehension Framework for Moment\n  Retrieval and Highlight Detection",
            "updated": "2023-11-28T03:55:23Z",
            "published": "2023-11-28T03:55:23Z",
            "summary": "Video Moment Retrieval (MR) and Highlight Detection (HD) have attracted\nsignificant attention due to the growing demand for video analysis. Recent\napproaches treat MR and HD as similar video grounding problems and address them\ntogether with transformer-based architecture. However, we observe that the\nemphasis of MR and HD differs, with one necessitating the perception of local\nrelationships and the other prioritizing the understanding of global contexts.\nConsequently, the lack of task-specific design will inevitably lead to\nlimitations in associating the intrinsic specialty of two tasks. To tackle the\nissue, we propose a Unified Video COMprehension framework (UVCOM) to bridge the\ngap and jointly solve MR and HD effectively. By performing progressive\nintegration on intra and inter-modality across multi-granularity, UVCOM\nachieves the comprehensive understanding in processing a video. Moreover, we\npresent multi-aspect contrastive learning to consolidate the local relation\nmodeling and global knowledge accumulation via well aligned multi-modal space.\nExtensive experiments on QVHighlights, Charades-STA, TACoS , YouTube Highlights\nand TVSum datasets demonstrate the effectiveness and rationality of UVCOM which\noutperforms the state-of-the-art methods by a remarkable margin.",
            "author": [
                "Yicheng Xiao",
                "Zhuoyan Luo",
                "Yong Liu",
                "Yue Ma",
                "Hengwei Bian",
                "Yatai Ji",
                "Yujiu Yang",
                "Xiu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16464v1",
                "http://arxiv.org/pdf/2311.16464v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16462v1",
            "title": "Viewport Prediction for Volumetric Video Streaming by Exploring Video\n  Saliency and Trajectory Information",
            "updated": "2023-11-28T03:45:29Z",
            "published": "2023-11-28T03:45:29Z",
            "summary": "Volumetric video, also known as hologram video, is a novel medium that\nportrays natural content in Virtual Reality (VR), Augmented Reality (AR), and\nMixed Reality (MR). It is expected to be the next-gen video technology and a\nprevalent use case for 5G and beyond wireless communication. Considering that\neach user typically only watches a section of the volumetric video, known as\nthe viewport, it is essential to have precise viewport prediction for optimal\nperformance. However, research on this topic is still in its infancy. In the\nend, this paper presents and proposes a novel approach, named Saliency and\nTrajectory Viewport Prediction (STVP), which aims to improve the precision of\nviewport prediction in volumetric video streaming. The STVP extensively\nutilizes video saliency information and viewport trajectory. To our knowledge,\nthis is the first comprehensive study of viewport prediction in volumetric\nvideo streaming. In particular, we introduce a novel sampling method, Uniform\nRandom Sampling (URS), to reduce computational complexity while still\npreserving video features in an efficient manner. Then we present a saliency\ndetection technique that incorporates both spatial and temporal information for\ndetecting static, dynamic geometric, and color salient regions. Finally, we\nintelligently fuse saliency and trajectory information to achieve more accurate\nviewport prediction. We conduct extensive simulations to evaluate the\neffectiveness of our proposed viewport prediction methods using\nstate-of-the-art volumetric video sequences. The experimental results show the\nsuperiority of the proposed method over existing schemes. The dataset and\nsource code will be publicly accessible after acceptance.",
            "author": [
                "Jie Li",
                "Zhixin Li",
                "Zhi Liu",
                "Pengyuan Zhou",
                "Richang Hong",
                "Qiyue Li",
                "Han Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16462v1",
                "http://arxiv.org/pdf/2311.16462v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00812v1",
            "title": "Empowering Autonomous Driving with Large Language Models: A Safety\n  Perspective",
            "updated": "2023-11-28T03:13:09Z",
            "published": "2023-11-28T03:13:09Z",
            "summary": "Autonomous Driving (AD) faces crucial hurdles for commercial launch, notably\nin the form of diminished public trust and safety concerns from long-tail\nunforeseen driving scenarios. This predicament is due to the limitation of deep\nneural networks in AD software, which struggle with interpretability and\nexhibit poor generalization capabilities in out-of-distribution and uncertain\nscenarios. To this end, this paper advocates for the integration of Large\nLanguage Models (LLMs) into the AD system, leveraging their robust common-sense\nknowledge, reasoning abilities, and human-interaction capabilities. The\nproposed approach deploys the LLM as an intelligent decision-maker in planning,\nincorporating safety verifiers for contextual safety learning to enhance\noverall AD performance and safety. We present results from two case studies\nthat affirm the efficacy of our approach. We further discuss the potential\nintegration of LLM for other AD software components including perception,\nprediction, and simulation. Despite the observed challenges in the case\nstudies, the integration of LLMs is promising and beneficial for reinforcing\nboth safety and performance in AD.",
            "author": [
                "Yixuan Wang",
                "Ruochen Jiao",
                "Chengtian Lang",
                "Sinong Simon Zhan",
                "Chao Huang",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Qi Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00812v1",
                "http://arxiv.org/pdf/2312.00812v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16450v2",
            "title": "Typhoon Intensity Prediction with Vision Transformer",
            "updated": "2023-12-04T07:59:05Z",
            "published": "2023-11-28T03:11:33Z",
            "summary": "Predicting typhoon intensity accurately across space and time is crucial for\nissuing timely disaster warnings and facilitating emergency response. This has\nvast potential for minimizing life losses and property damages as well as\nreducing economic and environmental impacts. Leveraging satellite imagery for\nscenario analysis is effective but also introduces additional challenges due to\nthe complex relations among clouds and the highly dynamic context. Existing\ndeep learning methods in this domain rely on convolutional neural networks\n(CNNs), which suffer from limited per-layer receptive fields. This limitation\nhinders their ability to capture long-range dependencies and global contextual\nknowledge during inference. In response, we introduce a novel approach, namely\n\"Typhoon Intensity Transformer\" (Tint), which leverages self-attention\nmechanisms with global receptive fields per layer. Tint adopts a\nsequence-to-sequence feature representation learning perspective. It begins by\ncutting a given satellite image into a sequence of patches and recursively\nemploys self-attention operations to extract both local and global contextual\nrelations between all patch pairs simultaneously, thereby enhancing per-patch\nfeature representation learning. Extensive experiments on a publicly available\ntyphoon benchmark validate the efficacy of Tint in comparison with both\nstate-of-the-art deep learning and conventional meteorological methods. Our\ncode is available at https://github.com/chen-huanxin/Tint.",
            "author": [
                "Huanxin Chen",
                "Pengshuai Yin",
                "Huichou Huang",
                "Qingyao Wu",
                "Ruirui Liu",
                "Xiatian Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16450v2",
                "http://arxiv.org/pdf/2311.16450v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16444v2",
            "title": "Exo2EgoDVC: Dense Video Captioning of Egocentric Procedural Activities\n  Using Web Instructional Videos",
            "updated": "2023-11-29T06:01:34Z",
            "published": "2023-11-28T02:51:13Z",
            "summary": "We propose a novel benchmark for cross-view knowledge transfer of dense video\ncaptioning, adapting models from web instructional videos with exocentric views\nto an egocentric view. While dense video captioning (predicting time segments\nand their captions) is primarily studied with exocentric videos (e.g.,\nYouCook2), benchmarks with egocentric videos are restricted due to data\nscarcity. To overcome the limited video availability, transferring knowledge\nfrom abundant exocentric web videos is demanded as a practical approach.\nHowever, learning the correspondence between exocentric and egocentric views is\ndifficult due to their dynamic view changes. The web videos contain mixed views\nfocusing on either human body actions or close-up hand-object interactions,\nwhile the egocentric view is constantly shifting as the camera wearer moves.\nThis necessitates the in-depth study of cross-view transfer under complex view\nchanges. In this work, we first create a real-life egocentric dataset (EgoYC2)\nwhose captions are shared with YouCook2, enabling transfer learning between\nthese datasets assuming their ground-truth is accessible. To bridge the view\ngaps, we propose a view-invariant learning method using adversarial training in\nboth the pre-training and fine-tuning stages. While the pre-training is\ndesigned to learn invariant features against the mixed views in the web videos,\nthe view-invariant fine-tuning further mitigates the view gaps between both\ndatasets. We validate our proposed method by studying how effectively it\novercomes the view change problem and efficiently transfers the knowledge to\nthe egocentric domain. Our benchmark pushes the study of the cross-view\ntransfer into a new task domain of dense video captioning and will envision\nmethodologies to describe egocentric videos in natural language.",
            "author": [
                "Takehiko Ohkawa",
                "Takuma Yagi",
                "Taichi Nishimura",
                "Ryosuke Furuta",
                "Atsushi Hashimoto",
                "Yoshitaka Ushiku",
                "Yoichi Sato"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16444v2",
                "http://arxiv.org/pdf/2311.16444v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.03731v1",
            "title": "MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs",
            "updated": "2023-11-28T02:36:53Z",
            "published": "2023-11-28T02:36:53Z",
            "summary": "Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.",
            "author": [
                "Xingtong Yu",
                "Chang Zhou",
                "Yuan Fang",
                "Xinming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03731v1",
                "http://arxiv.org/pdf/2312.03731v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17086v1",
            "title": "PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation\n  in non-English Text-to-Image Generation",
            "updated": "2023-11-28T02:31:52Z",
            "published": "2023-11-28T02:31:52Z",
            "summary": "Text-to-image diffusion models are well-known for their ability to generate\nrealistic images based on textual prompts. However, the existing works have\npredominantly focused on English, lacking support for non-English text-to-image\nmodels. The most commonly used translation methods cannot solve the generation\nproblem related to language culture, while training from scratch on a specific\nlanguage dataset is prohibitively expensive. In this paper, we are inspired to\npropose a simple plug-and-play language transfer method based on knowledge\ndistillation. All we need to do is train a lightweight MLP-like\nparameter-efficient adapter (PEA) with only 6M parameters under teacher\nknowledge distillation along with a small parallel data corpus. We are\nsurprised to find that freezing the parameters of UNet can still achieve\nremarkable performance on the language-specific prompt evaluation set,\ndemonstrating that PEA can stimulate the potential generation ability of the\noriginal UNet. Additionally, it closely approaches the performance of the\nEnglish text-to-image model on a general prompt evaluation set. Furthermore,\nour adapter can be used as a plugin to achieve significant results in\ndownstream tasks in cross-lingual text-to-image generation. Code will be\navailable at: https://github.com/OPPO-Mente-Lab/PEA-Diffusion",
            "author": [
                "Jian Ma",
                "Chen Chen",
                "Qingsong Xie",
                "Haonan Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17086v1",
                "http://arxiv.org/pdf/2311.17086v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17084v1",
            "title": "DepthSSC: Depth-Spatial Alignment and Dynamic Voxel Resolution for\n  Monocular 3D Semantic Scene Completion",
            "updated": "2023-11-28T01:47:51Z",
            "published": "2023-11-28T01:47:51Z",
            "summary": "The task of 3D semantic scene completion with monocular cameras is gaining\nincreasing attention in the field of autonomous driving. Its objective is to\npredict the occupancy status of each voxel in the 3D scene from partial image\ninputs. Despite the existence of numerous methods, many of them overlook the\nissue of accurate alignment between spatial and depth information. To address\nthis, we propose DepthSSC, an advanced method for semantic scene completion\nsolely based on monocular cameras. DepthSSC combines the ST-GF (Spatial\nTransformation Graph Fusion) module with geometric-aware voxelization, enabling\ndynamic adjustment of voxel resolution and considering the geometric complexity\nof 3D space to ensure precise alignment between spatial and depth information.\nThis approach successfully mitigates spatial misalignment and distortion issues\nobserved in prior methods. Through evaluation on the SemanticKITTI dataset,\nDepthSSC not only demonstrates its effectiveness in capturing intricate 3D\nstructural details but also achieves state-of-the-art performance. We believe\nDepthSSC provides a fresh perspective on monocular camera-based 3D semantic\nscene completion research and anticipate it will inspire further related\nstudies.",
            "author": [
                "Jiawei Yao",
                "Jusheng Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17084v1",
                "http://arxiv.org/pdf/2311.17084v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16403v1",
            "title": "Towards the classification of finite-dimensional diagonally graded\n  commutative algebras",
            "updated": "2023-11-28T01:15:07Z",
            "published": "2023-11-28T01:15:07Z",
            "summary": "Any finite-dimensional commutative (associative) graded algebra with all\nnonzero homogeneous subspaces one-dimensional is defined by a symmetric\ncoefficient matrix. This algebraic structure gives a basic kind of $A$-graded\nalgebras originally studied by Arnold. In this paper, we call them diagonally\ngraded commutative algebras (DGCAs) and verify that the isomorphism classes of\nDGCAs of dimension $\\leq 7$ over an arbitrary field are in bijection with the\nequivalence classes consisting of coefficient matrices with the same\ndistribution of nonzero entries, while dramatically there may be infinitely\nmany isomorphism classes of dimension $n$ corresponding to one equivalence\nclass of coefficient matrices when $n\\geq 8$.\n  Furthermore, we adopt the Skjelbred-Sund method of central extensions to\nstudy the isomorphism classes of DGCAs, and associate any DGCA with a\nundirected simple graph to explicitly describe its corresponding second\n(graded) commutative cohomology group as an affine variety.",
            "author": [
                "Yunnan Li",
                "Shi Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16403v1",
                "http://arxiv.org/pdf/2311.16403v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "math.AG",
                "math.CO",
                "13A02, 13E10, 14L30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00811v1",
            "title": "Seizure detection from Electroencephalogram signals via Wavelets and\n  Graph Theory metrics",
            "updated": "2023-11-28T01:07:14Z",
            "published": "2023-11-28T01:07:14Z",
            "summary": "Epilepsy is one of the most prevalent neurological conditions, where an\nepileptic seizure is a transient occurrence due to abnormal, excessive and\nsynchronous activity in the brain. Electroencephalogram signals emanating from\nthe brain may be captured, analysed and then play a significant role in\ndetection and prediction of epileptic seizures. In this work we enhance upon a\nprevious approach that relied on the differing properties of the wavelet\ntransform. Here we apply the Maximum Overlap Discrete Wavelet Transform to both\nreduce signal \\textit{noise} and use signal variance exhibited at differing\ninherent frequency levels to develop various metrics of connection between the\nelectrodes placed upon the scalp. %The properties of both the noise reduced\nsignal and the interconnected electrodes differ significantly during the\ndifferent brain states.\n  Using short duration epochs, to approximate close to real time monitoring,\ntogether with simple statistical parameters derived from the reconstructed\nnoise reduced signals we initiate seizure detection. To further improve\nperformance we utilise graph theoretic indicators from derived electrode\nconnectivity. From there we build the attribute space. We utilise open-source\nsoftware and publicly available data to highlight the superior\nRecall/Sensitivity performance of our approach, when compared to existing\npublished methods.",
            "author": [
                "Paul Grant",
                "Md Zahidul Islam"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00811v1",
                "http://arxiv.org/pdf/2312.00811v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16390v1",
            "title": "Relative Fractional Packing Number and Its Properties",
            "updated": "2023-11-28T00:23:43Z",
            "published": "2023-11-28T00:23:43Z",
            "summary": "The concept of the \\textit{relative fractional packing number} between two\ngraphs $G$ and $H$, initially introduced in arXiv:2307.06155 [math.CO], serves\nas an upper bound for the ratio of the zero-error Shannon capacity of these\ngraphs. Defined as: \\begin{equation*} \\sup\\limits_{W} \\frac{\\alpha(G \\boxtimes\nW)}{\\alpha(H \\boxtimes W)} \\end{equation*} where the supremum is computed over\nall arbitrary graphs and $\\boxtimes$ denotes the strong product of graphs.\n  This article delves into various critical theorems regarding the computation\nof this number. Specifically, we address its NP-hardness and the complexity of\napproximating it. Furthermore, we develop a conjecture for necessary and\nsufficient conditions for this number to be less than one. We also validate\nthis conjecture for specific graph families. Additionally, we present\nmiscellaneous concepts and introduce a generalized version of the independence\nnumber that gives insights that could significantly contribute to the study of\nthe relative fractional packing number.",
            "author": [
                "Mehrshad Taziki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16390v1",
                "http://arxiv.org/pdf/2311.16390v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16382v1",
            "title": "On indication, strict monotonicity, and efficiency of projections in a\n  general class of path-based data envelopment models",
            "updated": "2023-11-28T00:06:20Z",
            "published": "2023-11-28T00:06:20Z",
            "summary": "Data envelopment analysis (DEA) theory formulates a number of desirable\nproperties that DEA models should satisfy. Among these, indication, strict\nmonotonicity, and strong efficiency of projections tend to be grouped together\nin the sense that, in individual models, typically, either all three are\nsatisfied or all three fail at the same time. Specifically, in slacks-based\ngraph models, the three properties are always met; in path-based models, such\nas radial models, directional distance function models, and the hyperbolic\nfunction model, the three properties, with some minor exceptions, typically all\nfail.\n  Motivated by this observation, the article examines relationships among\nindication, strict monotonicity, and strong efficiency of projections in the\nclass of path-based models over variable returns-to-scale technology sets.\nUnder mild assumptions, it is shown that the property of strict monotonicity\nand strong efficiency of projections are equivalent, and that both properties\nimply indication. This paper also characterises a narrow class of technology\nsets and path directions for which the three properties hold in path-based\nmodels.",
            "author": [
                "Margar\u00e9ta Halick\u00e1",
                "M\u00e1ria Trnovsk\u00e1",
                "Ale\u0161 \u010cern\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16382v1",
                "http://arxiv.org/pdf/2311.16382v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16378v1",
            "title": "Bayesian Formulations for Graph Spectral Denoising",
            "updated": "2023-11-27T23:53:19Z",
            "published": "2023-11-27T23:53:19Z",
            "summary": "We consider noisy signals which are defined on the vertices of a graph and\npresent smoothing algorithms for the cases of Gaussian, dropout, and uniformly\ndistributed noise. The signals are assumed to follow a prior distribution\ndefined in the frequency domain which favors signals which are smooth across\nthe edges of the graph. By pairing this prior distribution with our three\nmodels of noise generation, we propose \\textit{Maximum A Posteriori} (M.A.P.)\nestimates of the true signal in the presence of noisy data and provide\nalgorithms for computing the M.A.P. Finally, we demonstrate the algorithms'\nability to effectively restore white noise on image data, and from severe\ndropout in toy \\& EHR data.",
            "author": [
                "Sam Leone",
                "Xingzhi Sun",
                "Michael Perlmutter",
                "Smita Krishnaswamy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16378v1",
                "http://arxiv.org/pdf/2311.16378v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16370v1",
            "title": "Climate, Crops, and Postharvest Conflict",
            "updated": "2023-11-27T23:29:29Z",
            "published": "2023-11-27T23:29:29Z",
            "summary": "I present new evidence of the effects of climate shocks on political violence\nand social unrest. Using granular conflict and weather data covering the entire\ncontinent of Africa from 1997 to 2023, I find that exposure to El Ni\\~no events\nduring the crop-growing season decreases political violence targeted at\ncivilians during the early postharvest season. A moderate-strength El Ni\\~no\nevent results in a three percent reduction in political violence with civilian\ntargeting in croplands compared with the benchmark levels of this conflict\nevaluated at average cropland size and average growing-season exposure of local\nweather to El Ni\\~no shocks. Because this effect manifests itself only in cells\nwith crop agriculture and only during the postharvest season supports the idea\nthat agriculture is the key channel and rapacity is the key motive connecting\nclimatic shocks and political violence. Reassuringly, the magnitude of the\nestimated effect increases substantially, in one instance more than doubles,\nwhen I use subsets of data that are better suited for unveiling the proposed\nmechanism. This study advances knowledge of the relationship between climate\nand conflict. And because El Ni\\~no events can be predicted several months in\nadvance, these findings can contribute to creating a platform for early warning\nof political violence, specifically in predominantly agrarian societies in\nAfrica.",
            "author": [
                "David Ubilava"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16370v1",
                "http://arxiv.org/pdf/2311.16370v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17932v2",
            "title": "Generating Molecular Conformer Fields",
            "updated": "2023-12-05T05:48:16Z",
            "published": "2023-11-27T22:53:41Z",
            "summary": "In this paper we tackle the problem of generating conformers of a molecule in\n3D space given its molecular graph. We parameterize these conformers as\ncontinuous functions that map elements from the molecular graph to points in 3D\nspace. We then formulate the problem of learning to generate conformers as\nlearning a distribution over these functions using a diffusion generative\nmodel, called Molecular Conformer Fields (MCF). Our approach is simple and\nscalable, and achieves state-of-the-art performance on challenging molecular\nconformer generation benchmarks while making no assumptions about the explicit\nstructure of molecules (e.g. modeling torsional angles). MCF represents an\nadvance in extending diffusion models to handle complex scientific problems in\na conceptually simple, scalable and effective manner.",
            "author": [
                "Yuyang Wang",
                "Ahmed A. Elhag",
                "Navdeep Jaitly",
                "Joshua M. Susskind",
                "Miguel Angel Bautista"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17932v2",
                "http://arxiv.org/pdf/2311.17932v2"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16360v1",
            "title": "Capturing Values at the Boundaries",
            "updated": "2023-11-27T22:51:20Z",
            "published": "2023-11-27T22:51:20Z",
            "summary": "Novelty is not a sufficient condition for innovation. For new ideas and\nproducts to succeed, they must be integrated into the collective understanding\nand existing infrastructure, illustrating how the past determines the future.\nHere, we develop a comprehensive framework to understand how the structure of\naccumulated past successes curves the adjacent possible trajectory of future\ninnovations. We observe that certain technological building blocks, upon\nfrequent combination, coalesce into noticeable clusters manifested as\nwell-defined domains within the exploration landscape. These clusters compress\nthe space around them, thus bending the trajectory of exploration towards them\nas if exerting a gravitational pull on new ideas and actions. Our methodology\nquantifies this effect, mapping out the curvatures within the adjacent possible\nspace of actions and identifying significant curvatures that define the\nboundaries of consensus domains. These domains, serving as knowledge\nrepertoire, guide inventors towards proven solutions and past successes,\nexplaining why the most commercially successful inventions often emerge at the\nfringes of established domains. Through a case study of Edison's patents, we\ndemonstrate his well-known design strategy of leveraging institutionalized\ndomains, manifested as high curvature in this space. In contrast, Tesla's\ninventions are predominantly located in low-curvature areas. Our further\nanalysis reveals that innovations in areas of high curvature are indeed more\nlikely to capture market values, supporting our observations. Our framework\nprovides insights into how new ideas interact with and evolve alongside\nestablished structures in institutional frameworks and collective\nunderstanding, illustrating the complex dialogue between innovation and\nconvention.",
            "author": [
                "Seolmin Yang",
                "Hyejin Youn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16360v1",
                "http://arxiv.org/pdf/2311.16360v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17077v2",
            "title": "Game-Theoretic Analysis of Adversarial Decision Making in a Complex\n  Sociophysical System",
            "updated": "2023-12-04T23:22:22Z",
            "published": "2023-11-27T22:24:23Z",
            "summary": "We apply Game Theory to a mathematical representation of two competing teams\nof agents connected within a complex network, where the ability of each side to\nmanoeuvre their resource and degrade that of the other depends on their ability\nto internally synchronise decision-making while out-pacing the other. Such a\nrepresentation of an adversarial socio-physical system has application in a\nrange of business, sporting, and military contexts. Specifically, we unite here\ntwo physics-based models, that of Kuramoto to represent decision-making cycles,\nand an adaptation of a multi-species Lotka-Volterra system for the resource\ncompetition. For complex networks we employ variations of the\nBarab\\'asi-Alberts scale-free graph, varying how resources are initially\ndistributed between graph hub and periphery. We adapt as equilibrium solution\nNash Dominant Game Pruning as a means of efficiently exploring the dynamical\ndecision tree. Across various scenarios we find Nash solutions where the side\ninitially concentrating resources in the periphery can sustain competition to\nachieve victory except when asymmetries exist between the two. When structural\nadvantage is limited we find that agility in how the victor stays ahead of\ndecision-state of the other becomes critical.",
            "author": [
                "Andrew C. Cullen",
                "Tansu Alpcan",
                "Alexander C. Kalloniatis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17077v2",
                "http://arxiv.org/pdf/2311.17077v2"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "math-ph",
                "math.MP",
                "nlin.AO",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17076v1",
            "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
            "updated": "2023-11-27T22:23:27Z",
            "published": "2023-11-27T22:23:27Z",
            "summary": "The combination of strong visual backbones and Large Language Model (LLM)\nreasoning has led to Large Multimodal Models (LMMs) becoming the current\nstandard for a wide range of vision and language (VL) tasks. However, recent\nresearch has shown that even the most advanced LMMs still struggle to capture\naspects of compositional visual reasoning, such as attributes and relationships\nbetween objects. One solution is to utilize scene graphs (SGs)--a formalization\nof objects and their relations and attributes that has been extensively used as\na bridge between the visual and textual domains. Yet, scene graph data requires\nscene graph annotations, which are expensive to collect and thus not easily\nscalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic\nforgetting of the pretraining objective. To overcome this, inspired by\nchain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a\nnovel zero-shot Chain-of-Thought prompting method that utilizes SG\nrepresentations in order to extract compositional knowledge from an LMM.\nSpecifically, we first generate an SG using the LMM, and then use that SG in\nthe prompt to produce a response. Through extensive experiments, we find that\nthe proposed CCoT approach not only improves LMM performance on several vision\nand language VL compositional benchmarks but also improves the performance of\nseveral popular LMMs on general multimodal benchmarks, without the need for\nfine-tuning or annotated ground-truth SGs.",
            "author": [
                "Chancharik Mitra",
                "Brandon Huang",
                "Trevor Darrell",
                "Roei Herzig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17076v1",
                "http://arxiv.org/pdf/2311.17076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16334v2",
            "title": "Robust Basket Recommendation via Noise-tolerated Graph Contrastive\n  Learning",
            "updated": "2023-12-01T02:07:06Z",
            "published": "2023-11-27T21:38:10Z",
            "summary": "The growth of e-commerce has seen a surge in popularity of platforms like\nAmazon, eBay, and Taobao. This has given rise to a unique shopping behavior\ninvolving baskets - sets of items purchased together. As a less studied\ninteraction mode in the community, the question of how should shopping basket\ncomplement personalized recommendation systems remains under-explored. While\nprevious attempts focused on jointly modeling user purchases and baskets, the\ndistinct semantic nature of these elements can introduce noise when directly\nintegrated. This noise negatively impacts the model's performance, further\nexacerbated by significant noise (e.g., a user is misled to click an item or\nrecognizes it as uninteresting after consuming it) within both user and basket\nbehaviors. In order to cope with the above difficulties, we propose a novel\nBasket recommendation framework via Noise-tolerated Contrastive Learning, named\nBNCL, to handle the noise existing in the cross-behavior integration and\nwithin-behavior modeling. First, we represent the basket-item interactions as\nthe hypergraph to model the complex basket behavior, where all items appearing\nin the same basket are treated as a single hyperedge. Second, cross-behavior\ncontrastive learning is designed to suppress the noise during the fusion of\ndiverse behaviors. Next, to further inhibit the within-behavior noise of the\nuser and basket interactions, we propose to exploit invariant properties of the\nrecommenders w.r.t augmentations through within-behavior contrastive learning.\nA novel consistency-aware augmentation approach is further designed to better\nidentify noisy interactions with the consideration of the above two types of\ninteractions. Our framework BNCL offers a generic training paradigm that is\napplicable to different backbones. Extensive experiments on three shopping\ntransaction datasets verify the effectiveness of our proposed method.",
            "author": [
                "Xinrui He",
                "Tianxin Wei",
                "Jingrui He"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3583780.3615039",
                "http://arxiv.org/abs/2311.16334v2",
                "http://arxiv.org/pdf/2311.16334v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16326v1",
            "title": "Atomic Cluster Expansion for semilocal interactions beyond equivariant\n  message passing",
            "updated": "2023-11-27T21:19:55Z",
            "published": "2023-11-27T21:19:55Z",
            "summary": "We extend the basis functions of the Atomic Cluster Expansion to graphs. This\nnaturally leads to a representation that enables us to describe semilocal\ninteractions in physiscally and chemically transparent form. Simplifications of\nthe graph Atomic Cluster Expansion recover the currently most accurate\nmessage-passing representations of atomic interactions. We demonstrate the\naccuracy and efficiency of our expansion for a number of small molecules,\nclusters and a general-purpose model for carbon.",
            "author": [
                "Anton Bochkarev",
                "Yury Lysogorskiy",
                "Ralf Drautz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16326v1",
                "http://arxiv.org/pdf/2311.16326v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16307v1",
            "title": "Orientable total domination in graphs",
            "updated": "2023-11-27T20:47:23Z",
            "published": "2023-11-27T20:47:23Z",
            "summary": "Given a directed graph $D$, a set $S \\subseteq V(D)$ is a total dominating\nset of $D$ if each vertex in $D$ has an in-neighbor in $S$. The total\ndomination number of $D$, denoted $\\gamma_t(D)$, is the minimum cardinality\namong all total dominating sets of $D$. Given an undirected graph $G$, we study\nthe maximum and minimum total domination numbers among all orientations of $G$.\nThat is, we study the upper (or lower) orientable domination number of $G$,\n$\\rm{DOM}_t(G)$ (or $\\rm{dom}_t(G)$), which is the largest (or smallest) total\ndomination number over all orientations of $G$. We characterize those graphs\nwith $\\rm{DOM}_t(G) =\\rm{dom}_t(G)$ when the girth is at least $7$ as well as\nthose graphs with $\\rm{dom}_t(G) = |V(G)|-1$. We also consider how these\nparameters are effected by removing a vertex from $G$, give exact values of\n$\\rm{DOM}_t(K_{m,n})$ and $\\rm{dom}_t(K_{m,n})$ and bound these parameters when\n$G$ is a grid graph.",
            "author": [
                "Sarah E. Anderson",
                "Tanja Dravec",
                "Daniel Johnston",
                "Kirsti Kuenzel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16307v1",
                "http://arxiv.org/pdf/2311.16307v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C20, 05C69, 05C76"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16286v1",
            "title": "A statistical approach to latent dynamic modeling with differential\n  equations",
            "updated": "2023-11-27T20:02:55Z",
            "published": "2023-11-27T20:02:55Z",
            "summary": "Ordinary differential equations (ODEs) can provide mechanistic models of\ntemporally local changes of processes, where parameters are often informed by\nexternal knowledge. While ODEs are popular in systems modeling, they are less\nestablished for statistical modeling of longitudinal cohort data, e.g., in a\nclinical setting. Yet, modeling of local changes could also be attractive for\nassessing the trajectory of an individual in a cohort in the immediate future\ngiven its current status, where ODE parameters could be informed by further\ncharacteristics of the individual. However, several hurdles so far limit such\nuse of ODEs, as compared to regression-based function fitting approaches. The\npotentially higher level of noise in cohort data might be detrimental to ODEs,\nas the shape of the ODE solution heavily depends on the initial value. In\naddition, larger numbers of variables multiply such problems and might be\ndifficult to handle for ODEs. To address this, we propose to use each\nobservation in the course of time as the initial value to obtain multiple local\nODE solutions and build a combined estimator of the underlying dynamics. Neural\nnetworks are used for obtaining a low-dimensional latent space for dynamic\nmodeling from a potentially large number of variables, and for obtaining\npatient-specific ODE parameters from baseline variables. Simultaneous\nidentification of dynamic models and of a latent space is enabled by recently\ndeveloped differentiable programming techniques. We illustrate the proposed\napproach in an application with spinal muscular atrophy patients and a\ncorresponding simulation study. In particular, modeling of local changes in\nhealth status at any point in time is contrasted to the interpretation of\nfunctions obtained from a global regression. This more generally highlights how\ndifferent application settings might demand different modeling strategies.",
            "author": [
                "Maren Hackenberg",
                "Astrid Pechmann",
                "Clemens Kreutz",
                "Janbernd Kirschner",
                "Harald Binder"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16286v1",
                "http://arxiv.org/pdf/2311.16286v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16278v1",
            "title": "VehicleGAN: Pair-flexible Pose Guided Image Synthesis for Vehicle\n  Re-identification",
            "updated": "2023-11-27T19:34:04Z",
            "published": "2023-11-27T19:34:04Z",
            "summary": "Vehicle Re-identification (Re-ID) has been broadly studied in the last\ndecade; however, the different camera view angle leading to confused\ndiscrimination in the feature subspace for the vehicles of various poses, is\nstill challenging for the Vehicle Re-ID models in the real world. To promote\nthe Vehicle Re-ID models, this paper proposes to synthesize a large number of\nvehicle images in the target pose, whose idea is to project the vehicles of\ndiverse poses into the unified target pose so as to enhance feature\ndiscrimination. Considering that the paired data of the same vehicles in\ndifferent traffic surveillance cameras might be not available in the real\nworld, we propose the first Pair-flexible Pose Guided Image Synthesis method\nfor Vehicle Re-ID, named as VehicleGAN in this paper, which works for both\nsupervised and unsupervised settings without the knowledge of geometric 3D\nmodels. Because of the feature distribution difference between real and\nsynthetic data, simply training a traditional metric learning based Re-ID model\nwith data-level fusion (i.e., data augmentation) is not satisfactory, therefore\nwe propose a new Joint Metric Learning (JML) via effective feature-level fusion\nfrom both real and synthetic data. Intensive experimental results on the public\nVeRi-776 and VehicleID datasets prove the accuracy and effectiveness of our\nproposed VehicleGAN and JML.",
            "author": [
                "Baolu Li",
                "Ping Liu",
                "Lan Fu",
                "Jinlong Li",
                "Jianwu Fang",
                "Zhigang Xu",
                "Hongkai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16278v1",
                "http://arxiv.org/pdf/2311.16278v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16277v1",
            "title": "A Graph Neural Network-Based QUBO-Formulated Hamiltonian-Inspired Loss\n  Function for Combinatorial Optimization using Reinforcement Learning",
            "updated": "2023-11-27T19:33:14Z",
            "published": "2023-11-27T19:33:14Z",
            "summary": "Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to\nmodel various NP-hard Combinatorial Optimization problems (CO) in the form of\nbinary variables. Ising Hamiltonian is used to model the energy function of a\nsystem. QUBO to Ising Hamiltonian is regarded as a technique to solve various\ncanonical optimization problems through quantum optimization algorithms.\nRecently, PI-GNN, a generic framework, has been proposed to address CO problems\nover graphs based on Graph Neural Network (GNN) architecture. They introduced a\ngeneric QUBO-formulated Hamiltonian-inspired loss function that was directly\noptimized using GNN. PI-GNN is highly scalable but there lies a noticeable\ndecrease in the number of satisfied constraints when compared to\nproblem-specific algorithms and becomes more pronounced with increased graph\ndensities. Here, We identify a behavioral pattern related to it and devise\nstrategies to improve its performance. Another group of literature uses\nReinforcement learning (RL) to solve the aforementioned NP-hard problems using\nproblem-specific reward functions. In this work, we also focus on creating a\nbridge between the RL-based solutions and the QUBO-formulated Hamiltonian. We\nformulate and empirically evaluate the compatibility of the QUBO-formulated\nHamiltonian as the generic reward function in the RL-based paradigm in the form\nof rewards. Furthermore, we also introduce a novel Monty Carlo Tree\nSearch-based strategy with GNN where we apply a guided search through manual\nperturbation of node labels during training. We empirically evaluated our\nmethods and observed up to 44% improvement in the number of constraint\nviolations compared to the PI-GNN.",
            "author": [
                "Redwan Ahmed Rizvee",
                "Raheeb Hasan",
                "Md. Mosaddek Khan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16277v1",
                "http://arxiv.org/pdf/2311.16277v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16267v1",
            "title": "Applications of Large Language Models in Data Processing: Innovative\n  Approaches to Segmenting and Renewing Information",
            "updated": "2023-11-27T19:17:39Z",
            "published": "2023-11-27T19:17:39Z",
            "summary": "Our paper investigates effective methods for code generation in\n\"specific-domain\" applications, including the use of Large Language Models\n(LLMs) for data segmentation and renewal, as well as stimulating deeper\nthinking in LLMs through prompt adjustments. Using a real company product as an\nexample, we provide user manuals, API documentation, and other data. The ideas\ndiscussed in this paper help segment and then convert this data into semantic\nvectors to better reflect their true positioning. Subsequently, user\nrequirements are transformed into vectors to retrieve the most relevant\ncontent, achieving about 70% accuracy in simple to medium-complexity tasks\nthrough various prompt techniques. This paper is the first to enhance\nspecific-domain code generation effectiveness from this perspective.\nAdditionally, we experiment with generating more scripts from a limited number\nusing llama2-based fine-tuning to test its effectiveness in professional domain\ncode generation. This is a challenging and promising field, and once achieved,\nit will not only lead to breakthroughs in LLM development across multiple\nindustries but also enable LLMs to understand and learn any new knowledge\neffectively.",
            "author": [
                "Yu-Chen Lin",
                "Akhilesh Kumar",
                "Wen-Liang Zhang",
                "Norman Chang",
                "Muhammad Zakir",
                "Rucha Apte",
                "Chao Wang",
                "Jyh-Shing Roger Jang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16267v1",
                "http://arxiv.org/pdf/2311.16267v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16241v1",
            "title": "SemiVL: Semi-Supervised Semantic Segmentation with Vision-Language\n  Guidance",
            "updated": "2023-11-27T19:00:06Z",
            "published": "2023-11-27T19:00:06Z",
            "summary": "In semi-supervised semantic segmentation, a model is trained with a limited\nnumber of labeled images along with a large corpus of unlabeled images to\nreduce the high annotation effort. While previous methods are able to learn\ngood segmentation boundaries, they are prone to confuse classes with similar\nvisual appearance due to the limited supervision. On the other hand,\nvision-language models (VLMs) are able to learn diverse semantic knowledge from\nimage-caption datasets but produce noisy segmentation due to the image-level\ntraining. In SemiVL, we propose to integrate rich priors from VLM pre-training\ninto semi-supervised semantic segmentation to learn better semantic decision\nboundaries. To adapt the VLM from global to local reasoning, we introduce a\nspatial fine-tuning strategy for label-efficient learning. Further, we design a\nlanguage-guided decoder to jointly reason over vision and language. Finally, we\npropose to handle inherent ambiguities in class labels by providing the model\nwith language guidance in the form of class definitions. We evaluate SemiVL on\n4 semantic segmentation datasets, where it significantly outperforms previous\nsemi-supervised methods. For instance, SemiVL improves the state-of-the-art by\n+13.5 mIoU on COCO with 232 annotated images and by +6.1 mIoU on Pascal VOC\nwith 92 labels. Project page: https://github.com/google-research/semivl",
            "author": [
                "Lukas Hoyer",
                "David Joseph Tan",
                "Muhammad Ferjad Naeem",
                "Luc Van Gool",
                "Federico Tombari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16241v1",
                "http://arxiv.org/pdf/2311.16241v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16223v1",
            "title": "Mapping quantum circuits to shallow-depth measurement patterns based on\n  graph states",
            "updated": "2023-11-27T19:00:00Z",
            "published": "2023-11-27T19:00:00Z",
            "summary": "The paradigm of measurement-based quantum computing (MBQC) starts from a\nhighly entangled resource state on which unitary operations are executed\nthrough adaptive measurements and corrections ensuring determinism. This is set\nin contrast to the more common quantum circuit model, in which unitary\noperations are directly implemented through quantum gates prior to final\nmeasurements. In this work, we incorporate concepts from MBQC into the circuit\nmodel to create a hybrid simulation technique, permitting us to split any\nquantum circuit into a classically efficiently simulatable Clifford-part and a\nsecond part consisting of a stabilizer state and local (adaptive) measurement\ninstructions, a so-called standard form, which is executed on a quantum\ncomputer. We further process the stabilizer state with the graph state\nformalism, thus enabling a significant decrease in circuit depth for certain\napplications. We show that groups of fully commuting operators can be\nimplemented using fully-parallel, i.e., non-adaptive, measurements within our\nprotocol. In addition, we discuss how such circuits can be implemented in\nconstant quantum depths by employing quantum teleportation. Finally, we\ndemonstrate the utility of our technique on two examples of high practical\nrelevance: the Quantum Approximate Optimization Algorithm (QAOA) and the\nVariational Quantum Eigensolver (VQE).",
            "author": [
                "Thierry Nicolas Kaldenbach",
                "Matthias Heller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16223v1",
                "http://arxiv.org/pdf/2311.16223v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16103v2",
            "title": "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating\n  Video-based Large Language Models",
            "updated": "2023-11-28T18:16:29Z",
            "published": "2023-11-27T18:59:58Z",
            "summary": "Video-based large language models (Video-LLMs) have been recently introduced,\ntargeting both fundamental improvements in perception and comprehension, and a\ndiverse range of user inquiries. In pursuit of the ultimate goal of achieving\nartificial general intelligence, a truly intelligent Video-LLM model should not\nonly see and understand the surroundings, but also possess human-level\ncommonsense, and make well-informed decisions for the users. To guide the\ndevelopment of such a model, the establishment of a robust and comprehensive\nevaluation system becomes crucial. To this end, this paper proposes\n\\textit{Video-Bench}, a new comprehensive benchmark along with a toolkit\nspecifically designed for evaluating Video-LLMs. The benchmark comprises 10\nmeticulously crafted tasks, evaluating the capabilities of Video-LLMs across\nthree distinct levels: Video-exclusive Understanding, Prior Knowledge-based\nQuestion-Answering, and Comprehension and Decision-making. In addition, we\nintroduce an automatic toolkit tailored to process model outputs for various\ntasks, facilitating the calculation of metrics and generating convenient final\nscores. We evaluate 8 representative Video-LLMs using \\textit{Video-Bench}. The\nfindings reveal that current Video-LLMs still fall considerably short of\nachieving human-like comprehension and analysis of real-world videos, offering\nvaluable insights for future research directions. The benchmark and toolkit are\navailable at: \\url{https://github.com/PKU-YuanGroup/Video-Bench}.",
            "author": [
                "Munan Ning",
                "Bin Zhu",
                "Yujia Xie",
                "Bin Lin",
                "Jiaxi Cui",
                "Lu Yuan",
                "Dongdong Chen",
                "Li Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16103v2",
                "http://arxiv.org/pdf/2311.16103v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16095v1",
            "title": "KPZ-type equation from growth driven by a non-Markovian diffusion",
            "updated": "2023-11-27T18:59:04Z",
            "published": "2023-11-27T18:59:04Z",
            "summary": "We study a stochastic geometric flow that describes a growing submanifold\n$\\mathbb{M}(t)\\subseteq\\mathbb{R}^{\\mathrm{d}+1}$. It is an SPDE that comes\nfrom a continuum version of origin-excited random walk or once-reinforced\nrandom walk. It is given by simultaneously smoothing and inflating the boundary\nof $\\\\mathbb{M}(t)$ in a neighborhood of the boundary trace of a reflecting\nBrownian motion. We show that the large-scale fluctuations of an associated\nheight function are given by a regularized Kardar-Parisi-Zhang (KPZ)-type\nequation on a manifold in $\\mathbb{R}^{\\mathrm{d}+1}$, modulated by a\nDirichlet-to-Neumann operator. This is shown in any dimension\n$\\mathrm{d}\\geq1$. We also prove that in dimension $\\mathrm{d}+1=2$, the\nregularization in this KPZ-type SPDE can be removed after renormalization.\nThus, in dimension $\\mathrm{d}+1=2$, fluctuations of the geometric flow have a\ndouble-scaling limit given by a singular KPZ-type equation. To our knowledge,\nthis is the first instance of KPZ-type behavior in stochastic Laplacian growth\nmodels, which was asked about (for somewhat different models) in Parisi-Zhang\n'84 and Ramirez-Sidoravicius '04.",
            "author": [
                "Amir Dembo",
                "Kevin Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16095v1",
                "http://arxiv.org/pdf/2311.16095v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "82C24, 60H15, 58J65, 35R60"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16091v1",
            "title": "Interactive Autonomous Navigation with Internal State Inference and\n  Interactivity Estimation",
            "updated": "2023-11-27T18:57:42Z",
            "published": "2023-11-27T18:57:42Z",
            "summary": "Deep reinforcement learning (DRL) provides a promising way for intelligent\nagents (e.g., autonomous vehicles) to learn to navigate complex scenarios.\nHowever, DRL with neural networks as function approximators is typically\nconsidered a black box with little explainability and often suffers from\nsuboptimal performance, especially for autonomous navigation in highly\ninteractive multi-agent environments. To address these issues, we propose three\nauxiliary tasks with spatio-temporal relational reasoning and integrate them\ninto the standard DRL framework, which improves the decision making performance\nand provides explainable intermediate indicators. We propose to explicitly\ninfer the internal states (i.e., traits and intentions) of surrounding agents\n(e.g., human drivers) as well as to predict their future trajectories in the\nsituations with and without the ego agent through counterfactual reasoning.\nThese auxiliary tasks provide additional supervision signals to infer the\nbehavior patterns of other interactive agents. Multiple variants of framework\nintegration strategies are compared. We also employ a spatio-temporal graph\nneural network to encode relations between dynamic entities, which enhances\nboth internal state inference and decision making of the ego agent. Moreover,\nwe propose an interactivity estimation mechanism based on the difference\nbetween predicted trajectories in these two situations, which indicates the\ndegree of influence of the ego agent on other agents. To validate the proposed\nmethod, we design an intersection driving simulator based on the Intelligent\nIntersection Driver Model (IIDM) that simulates vehicles and pedestrians. Our\napproach achieves robust and state-of-the-art performance in terms of standard\nevaluation metrics and provides explainable intermediate indicators (i.e.,\ninternal states, and interactivity scores) for decision making.",
            "author": [
                "Jiachen Li",
                "David Isele",
                "Kanghoon Lee",
                "Jinkyoo Park",
                "Kikuo Fujimura",
                "Mykel J. Kochenderfer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16091v1",
                "http://arxiv.org/pdf/2311.16091v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.CV",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16088v1",
            "title": "Long-range first-passage percolation on the complete graph",
            "updated": "2023-11-27T18:56:23Z",
            "published": "2023-11-27T18:56:23Z",
            "summary": "We study a geometric version of first-passage percolation on the complete\ngraph, known as long-range first-passage percolation. Here, the vertices of the\ncomplete graph $\\mathcal K_n$ are embedded in the $d$-dimensional torus\n$\\mathbb T_n^d$, and each edge $e$ is assigned an independent transmission time\n$T_e=\\|e\\|_{\\mathbb T_n^d}^\\alpha E_e$, where $E_e$ is a rate-one exponential\nrandom variable associated with the edge $e$, $\\|\\cdot\\|_{\\mathbb T_n^d}$\ndenotes the torus-norm, and $\\alpha\\geq0$ is a parameter. We are interested in\nthe case $\\alpha\\in[0,d)$, which corresponds to the instantaneous percolation\nregime for long-range first-passage percolation on $\\mathbb Z^d$ studied by\nChatterjee and Dey, and which extends first-passage percolation on the complete\ngraph (the $\\alpha=0$ case) studied by Janson. We consider the typical\ndistance, flooding time, and diameter of the model. Our results show a\n$1,2,3$-type result, akin to first-passage percolation on the complete graph as\nshown by Janson. The results also provide a quantitative perspective to the\nqualitative results observed by Chatterjee and Dey on $\\mathbb Z^d$.",
            "author": [
                "Remco van der Hofstad",
                "Bas Lodewijks"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16088v1",
                "http://arxiv.org/pdf/2311.16088v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16087v1",
            "title": "DUnE: Dataset for Unified Editing",
            "updated": "2023-11-27T18:56:14Z",
            "published": "2023-11-27T18:56:14Z",
            "summary": "Even the most advanced language models remain susceptible to errors\nnecessitating to modify these models without initiating a comprehensive\nretraining process. Model editing refers to the modification of a model's\nknowledge or representations in a manner that produces the desired outcomes.\nPrior research primarily centered around editing factual data e.g. \"Messi plays\nfor Inter Miami\" confining the definition of an edit to a knowledge triplet\ni.e. (subject, object, relation). However, as the applications of language\nmodels expand, so do the diverse ways in which we wish to edit and refine their\noutputs. In this study, we broaden the scope of the editing problem to include\nan array of editing cases such as debiasing and rectifying reasoning errors and\ndefine an edit as any natural language expression that solicits a change in the\nmodel's outputs. We are introducing DUnE-an editing benchmark where edits are\nnatural language sentences and propose that DUnE presents a challenging yet\nrelevant task. To substantiate this claim, we conduct an extensive series of\nexperiments testing various editing approaches to address DUnE, demonstrating\ntheir respective strengths and weaknesses. We show that retrieval-augmented\nlanguage modeling can outperform specialized editing techniques and neither set\nof approaches has fully solved the generalized editing problem covered by our\nbenchmark.",
            "author": [
                "Afra Feyza Aky\u00fcrek",
                "Eric Pan",
                "Garry Kuwanto",
                "Derry Wijaya"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16087v1",
                "http://arxiv.org/pdf/2311.16087v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16081v1",
            "title": "ViT-Lens-2: Gateway to Omni-modal Intelligence",
            "updated": "2023-11-27T18:52:09Z",
            "published": "2023-11-27T18:52:09Z",
            "summary": "Aiming to advance AI agents, large foundation models significantly improve\nreasoning and instruction execution, yet the current focus on vision and\nlanguage neglects the potential of perceiving diverse modalities in open-world\nenvironments. However, the success of data-driven vision and language models is\ncostly or even infeasible to be reproduced for rare modalities. In this paper,\nwe present ViT-Lens-2 that facilitates efficient omni-modal representation\nlearning by perceiving novel modalities with a pretrained ViT and aligning them\nto a pre-defined space. Specifically, the modality-specific lens is tuned to\nproject any-modal signals to an intermediate embedding space, which are then\nprocessed by a strong ViT with pre-trained visual knowledge. The encoded\nrepresentations are optimized toward aligning with the modal-independent space,\npre-defined by off-the-shelf foundation models. ViT-Lens-2 provides a unified\nsolution for representation learning of increasing modalities with two\nappealing advantages: (i) Unlocking the great potential of pretrained ViTs to\nnovel modalities effectively with efficient data regime; (ii) Enabling emergent\ndownstream capabilities through modality alignment and shared ViT parameters.\nWe tailor ViT-Lens-2 to learn representations for 3D point cloud, depth, audio,\ntactile and EEG, and set new state-of-the-art results across various\nunderstanding tasks, such as zero-shot classification. By seamlessly\nintegrating ViT-Lens-2 into Multimodal Foundation Models, we enable\nAny-modality to Text and Image Generation in a zero-shot manner. Code and\nmodels are available at https://github.com/TencentARC/ViT-Lens.",
            "author": [
                "Weixian Lei",
                "Yixiao Ge",
                "Kun Yi",
                "Jianfeng Zhang",
                "Difei Gao",
                "Dylan Sun",
                "Yuying Ge",
                "Ying Shan",
                "Mike Zheng Shou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16081v1",
                "http://arxiv.org/pdf/2311.16081v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16079v1",
            "title": "MEDITRON-70B: Scaling Medical Pretraining for Large Language Models",
            "updated": "2023-11-27T18:49:43Z",
            "published": "2023-11-27T18:49:43Z",
            "summary": "Large language models (LLMs) can potentially democratize access to medical\nknowledge. While many efforts have been made to harness and improve LLMs'\nmedical knowledge and reasoning capacities, the resulting models are either\nclosed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters),\nwhich restricts their abilities. In this work, we improve access to large-scale\nmedical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B\nparameters adapted to the medical domain. MEDITRON builds on Llama-2 (through\nour adaptation of Nvidia's Megatron-LM distributed trainer), and extends\npretraining on a comprehensively curated medical corpus, including selected\nPubMed articles, abstracts, and internationally-recognized medical guidelines.\nEvaluations using four major medical benchmarks show significant performance\ngains over several state-of-the-art baselines before and after task-specific\nfinetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the\nbest public baseline in its parameter class and 3% over the strongest baseline\nwe finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B\noutperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of\nMed-PaLM-2. We release our code for curating the medical pretraining corpus and\nthe MEDITRON model weights to drive open-source development of more capable\nmedical LLMs.",
            "author": [
                "Zeming Chen",
                "Alejandro Hern\u00e1ndez Cano",
                "Angelika Romanou",
                "Antoine Bonnet",
                "Kyle Matoba",
                "Francesco Salvi",
                "Matteo Pagliardini",
                "Simin Fan",
                "Andreas K\u00f6pf",
                "Amirkeivan Mohtashami",
                "Alexandre Sallinen",
                "Alireza Sakhaeirad",
                "Vinitra Swamy",
                "Igor Krawczuk",
                "Deniz Bayazit",
                "Axel Marmet",
                "Syrielle Montariol",
                "Mary-Anne Hartley",
                "Martin Jaggi",
                "Antoine Bosselut"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16079v1",
                "http://arxiv.org/pdf/2311.16079v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16075v1",
            "title": "BioLORD-2023: Semantic Textual Representations Fusing LLM and Clinical\n  Knowledge Graph Insights",
            "updated": "2023-11-27T18:46:17Z",
            "published": "2023-11-27T18:46:17Z",
            "summary": "In this study, we investigate the potential of Large Language Models to\ncomplement biomedical knowledge graphs in the training of semantic models for\nthe biomedical and clinical domains. Drawing on the wealth of the UMLS\nknowledge graph and harnessing cutting-edge Large Language Models, we propose a\nnew state-of-the-art approach for obtaining high-fidelity representations of\nbiomedical concepts and sentences, consisting of three steps: an improved\ncontrastive learning phase, a novel self-distillation phase, and a weight\naveraging phase. Through rigorous evaluations via the extensive BioLORD testing\nsuite and diverse downstream tasks, we demonstrate consistent and substantial\nperformance improvements over the previous state of the art (e.g. +2pts on\nMedSTS, +2.5pts on MedNLI-S, +6.1pts on EHR-Rel-B). Besides our new\nstate-of-the-art biomedical model for English, we also distill and release a\nmultilingual model compatible with 50+ languages and finetuned on 7 European\nlanguages. Many clinical pipelines can benefit from our latest models. Our new\nmultilingual model enables a range of languages to benefit from our\nadvancements in biomedical semantic representation learning, opening a new\navenue for bioinformatics researchers around the world. As a result, we hope to\nsee BioLORD-2023 becoming a precious tool for future biomedical applications.",
            "author": [
                "Fran\u00e7ois Remy",
                "Kris Demuynck",
                "Thomas Demeester"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16075v1",
                "http://arxiv.org/pdf/2311.16075v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16214v1",
            "title": "DGR: Tackling Drifted and Correlated Noise in Quantum Error Correction\n  via Decoding Graph Re-weighting",
            "updated": "2023-11-27T18:26:16Z",
            "published": "2023-11-27T18:26:16Z",
            "summary": "Quantum hardware suffers from high error rates and noise, which makes\ndirectly running applications on them ineffective. Quantum Error Correction\n(QEC) is a critical technique towards fault tolerance which encodes the quantum\ninformation distributively in multiple data qubits and uses syndrome qubits to\ncheck parity. Minimum-Weight-Perfect-Matching (MWPM) is a popular QEC decoder\nthat takes the syndromes as input and finds the matchings between syndromes\nthat infer the errors. However, there are two paramount challenges for MWPM\ndecoders. First, as noise in real quantum systems can drift over time, there is\na potential misalignment with the decoding graph's initial weights, leading to\na severe performance degradation in the logical error rates. Second, while the\nMWPM decoder addresses independent errors, it falls short when encountering\ncorrelated errors typical on real hardware, such as those in the 2Q\ndepolarizing channel.\n  We propose DGR, an efficient decoding graph edge re-weighting strategy with\nno quantum overhead. It leverages the insight that the statistics of matchings\nacross decoding iterations offer rich information about errors on real quantum\nhardware. By counting the occurrences of edges and edge pairs in decoded\nmatchings, we can statistically estimate the up-to-date probabilities of each\nedge and the correlations between them. The reweighting process includes two\nvital steps: alignment re-weighting and correlation re-weighting. The former\nupdates the MWPM weights based on statistics to align with actual noise, and\nthe latter adjusts the weight considering edge correlations.\n  Extensive evaluations on surface code and honeycomb code under various\nsettings show that DGR reduces the logical error rate by 3.6x on average-case\nnoise mismatch with exceeding 5000x improvement under worst-case mismatch.",
            "author": [
                "Hanrui Wang",
                "Pengyu Liu",
                "Yilian Liu",
                "Jiaqi Gu",
                "Jonathan Baker",
                "Frederic T. Chong",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16214v1",
                "http://arxiv.org/pdf/2311.16214v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AR",
                "cs.ET",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16050v1",
            "title": "An analysis of localization transitions using non-parametric\n  unsupervised learning",
            "updated": "2023-11-27T18:13:50Z",
            "published": "2023-11-27T18:13:50Z",
            "summary": "Localization transitions induced by disorder in quantum systems have been\nsubject of intense discussion in the past decades. In particular, whether or\nnot a localized phase is stable to the presence of interactions in the\nthermodynamic limit, is still an open question which is difficult to tackle\nboth with numerical and analytical approaches. Here, we provide an alternative\nviewpoint by analyzing the classical encoding configurations of the disordered\nquantum system state and showing that its critical properties can be seen also\nas a geometric transition in data space. We showcase our approach on the\nAnderson model on regular random graphs, estimating the transition point in\nagreement with results in the literature. We provide a simple and coherent\nexplanation of our findings, discussing the applicability of the method in\nreal-world scenarios with a modest number of measurements.",
            "author": [
                "Carlo Vanoni",
                "Vittorio Vitale"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16050v1",
                "http://arxiv.org/pdf/2311.16050v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16024v1",
            "title": "MadRadar: A Black-Box Physical Layer Attack Framework on mmWave\n  Automotive FMCW Radars",
            "updated": "2023-11-27T17:38:14Z",
            "published": "2023-11-27T17:38:14Z",
            "summary": "Frequency modulated continuous wave (FMCW) millimeter-wave (mmWave) radars\nplay a critical role in many of the advanced driver assistance systems (ADAS)\nfeatured on today's vehicles. While previous works have demonstrated (only)\nsuccessful false-positive spoofing attacks against these sensors, all but one\nassumed that an attacker had the runtime knowledge of the victim radar's\nconfiguration. In this work, we introduce MadRadar, a general black-box radar\nattack framework for automotive mmWave FMCW radars capable of estimating the\nvictim radar's configuration in real-time, and then executing an attack based\non the estimates. We evaluate the impact of such attacks maliciously\nmanipulating a victim radar's point cloud, and show the novel ability to\neffectively `add' (i.e., false positive attacks), `remove' (i.e., false\nnegative attacks), or `move' (i.e., translation attacks) object detections from\na victim vehicle's scene. Finally, we experimentally demonstrate the\nfeasibility of our attacks on real-world case studies performed using a\nreal-time physical prototype on a software-defined radio platform.",
            "author": [
                "David Hunt",
                "Kristen Angell",
                "Zhenzhou Qi",
                "Tingjun Chen",
                "Miroslav Pajic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16024v1",
                "http://arxiv.org/pdf/2311.16024v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16025v1",
            "title": "Change Point Detection for Random Objects using Distance Profiles",
            "updated": "2023-11-27T17:38:14Z",
            "published": "2023-11-27T17:38:14Z",
            "summary": "We introduce a new powerful scan statistic and an associated test for\ndetecting the presence and pinpointing the location of a change point within\nthe distribution of a data sequence where the data elements take values in a\ngeneral separable metric space $(\\Omega, d)$. These change points mark abrupt\nshifts in the distribution of the data sequence. Our method hinges on distance\nprofiles, where the distance profile of an element $\\omega \\in \\Omega$ is the\ndistribution of distances from $\\omega$ as dictated by the data. Our approach\nis fully non-parametric and universally applicable to diverse data types,\nincluding distributional and network data, as long as distances between the\ndata objects are available. From a practicable point of view, it is nearly\ntuning parameter-free, except for the specification of cut-off intervals near\nthe endpoints where change points are assumed not to occur. Our theoretical\nresults include a precise characterization of the asymptotic distribution of\nthe test statistic under the null hypothesis of no change points and rigorous\nguarantees on the consistency of the test in the presence of change points\nunder contiguous alternatives, as well as for the consistency of the estimated\nchange point location. Through comprehensive simulation studies encompassing\nmultivariate data, bivariate distributional data and sequences of graph\nLaplacians, we demonstrate the effectiveness of our approach in both change\npoint detection power and estimating the location of the change point. We apply\nour method to real datasets, including U.S. electricity generation compositions\nand Bluetooth proximity networks, underscoring its practical relevance.",
            "author": [
                "Paromita Dubey",
                "Minxing Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16025v1",
                "http://arxiv.org/pdf/2311.16025v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16502v1",
            "title": "MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning\n  Benchmark for Expert AGI",
            "updated": "2023-11-27T17:33:21Z",
            "published": "2023-11-27T17:33:21Z",
            "summary": "We introduce MMMU: a new benchmark designed to evaluate multimodal models on\nmassive multi-discipline tasks demanding college-level subject knowledge and\ndeliberate reasoning. MMMU includes 11.5K meticulously collected multimodal\nquestions from college exams, quizzes, and textbooks, covering six core\ndisciplines: Art & Design, Business, Science, Health & Medicine, Humanities &\nSocial Science, and Tech & Engineering. These questions span 30 subjects and\n183 subfields, comprising 30 highly heterogeneous image types, such as charts,\ndiagrams, maps, tables, music sheets, and chemical structures. Unlike existing\nbenchmarks, MMMU focuses on advanced perception and reasoning with\ndomain-specific knowledge, challenging models to perform tasks akin to those\nfaced by experts. Our evaluation of 14 open-source LMMs and the proprietary\nGPT-4V(ision) highlights the substantial challenges posed by MMMU. Even the\nadvanced GPT-4V only achieves a 56% accuracy, indicating significant room for\nimprovement. We believe MMMU will stimulate the community to build\nnext-generation multimodal foundation models towards expert artificial general\nintelligence.",
            "author": [
                "Xiang Yue",
                "Yuansheng Ni",
                "Kai Zhang",
                "Tianyu Zheng",
                "Ruoqi Liu",
                "Ge Zhang",
                "Samuel Stevens",
                "Dongfu Jiang",
                "Weiming Ren",
                "Yuxuan Sun",
                "Cong Wei",
                "Botao Yu",
                "Ruibin Yuan",
                "Renliang Sun",
                "Ming Yin",
                "Boyuan Zheng",
                "Zhenzhu Yang",
                "Yibo Liu",
                "Wenhao Huang",
                "Huan Sun",
                "Yu Su",
                "Wenhu Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16502v1",
                "http://arxiv.org/pdf/2311.16502v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16492v1",
            "title": "VLPrompt: Vision-Language Prompting for Panoptic Scene Graph Generation",
            "updated": "2023-11-27T17:05:25Z",
            "published": "2023-11-27T17:05:25Z",
            "summary": "Panoptic Scene Graph Generation (PSG) aims at achieving a comprehensive image\nunderstanding by simultaneously segmenting objects and predicting relations\namong objects. However, the long-tail problem among relations leads to\nunsatisfactory results in real-world applications. Prior methods predominantly\nrely on vision information or utilize limited language information, such as\nobject or relation names, thereby overlooking the utility of language\ninformation. Leveraging the recent progress in Large Language Models (LLMs), we\npropose to use language information to assist relation prediction, particularly\nfor rare relations. To this end, we propose the Vision-Language Prompting\n(VLPrompt) model, which acquires vision information from images and language\ninformation from LLMs. Then, through a prompter network based on attention\nmechanism, it achieves precise relation prediction. Our extensive experiments\nshow that VLPrompt significantly outperforms previous state-of-the-art methods\non the PSG dataset, proving the effectiveness of incorporating language\ninformation and alleviating the long-tail problem of relations.",
            "author": [
                "Zijian Zhou",
                "Miaojing Shi",
                "Holger Caesar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16492v1",
                "http://arxiv.org/pdf/2311.16492v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15979v1",
            "title": "Soil Organic Carbon Estimation from Climate-related Features with Graph\n  Neural Network",
            "updated": "2023-11-27T16:25:12Z",
            "published": "2023-11-27T16:25:12Z",
            "summary": "Soil organic carbon (SOC) plays a pivotal role in the global carbon cycle,\nimpacting climate dynamics and necessitating accurate estimation for\nsustainable land and agricultural management. While traditional methods of SOC\nestimation face resolution and accuracy challenges, recent technological\nsolutions harness remote sensing, machine learning, and high-resolution\nsatellite mapping. Graph Neural Networks (GNNs), especially when integrated\nwith positional encoders, can capture complex relationships between soil and\nclimate. Using the LUCAS database, this study compared four GNN operators in\nthe positional encoder framework. Results revealed that the PESAGE and\nPETransformer models outperformed others in SOC estimation, indicating their\npotential in capturing the complex relationship between SOC and climate\nfeatures. Our findings confirm the feasibility of applications of GNN\narchitectures in SOC prediction, establishing a framework for future\nexplorations of this topic with more advanced GNN models.",
            "author": [
                "Weiying Zhao",
                "Natalia Efremova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15979v1",
                "http://arxiv.org/pdf/2311.15979v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15965v1",
            "title": "FALCON: Fairness Learning via Contrastive Attention Approach to\n  Continual Semantic Scene Understanding in Open World",
            "updated": "2023-11-27T16:07:39Z",
            "published": "2023-11-27T16:07:39Z",
            "summary": "Continual Learning in semantic scene segmentation aims to continually learn\nnew unseen classes in dynamic environments while maintaining previously learned\nknowledge. Prior studies focused on modeling the catastrophic forgetting and\nbackground shift challenges in continual learning. However, fairness, another\nmajor challenge that causes unfair predictions leading to low performance among\nmajor and minor classes, still needs to be well addressed. In addition, prior\nmethods have yet to model the unknown classes well, thus resulting in producing\nnon-discriminative features among unknown classes. This paper presents a novel\nFairness Learning via Contrastive Attention Approach to continual learning in\nsemantic scene understanding. In particular, we first introduce a new Fairness\nContrastive Clustering loss to address the problems of catastrophic forgetting\nand fairness. Then, we propose an attention-based visual grammar approach to\neffectively model the background shift problem and unknown classes, producing\nbetter feature representations for different unknown classes. Through our\nexperiments, our proposed approach achieves State-of-the-Art (SOTA) performance\non different continual learning settings of three standard benchmarks, i.e.,\nADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual\nsemantic segmentation model.",
            "author": [
                "Thanh-Dat Truong",
                "Utsav Prabhu",
                "Bhiksha Raj",
                "Jackson Cothren",
                "Khoa Luu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15965v1",
                "http://arxiv.org/pdf/2311.15965v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15953v1",
            "title": "Fairness in Graph-Theoretical Optimization Problems",
            "updated": "2023-11-27T15:58:23Z",
            "published": "2023-11-27T15:58:23Z",
            "summary": "There is arbitrariness in optimum solutions of graph-theoretic problems that\ncan give rise to unfairness. Incorporating fairness in such problems, however,\ncan be done in multiple ways. For instance, fairness can be defined on an\nindividual level, for individual vertices or edges of a given graph, or on a\ngroup level. In this work, we analyze in detail two individual-fairness\nmeasures that are based on finding a probability distribution over the set of\nsolutions. One measure guarantees uniform fairness, i.e., entities have equal\nchance of being part of the solution when sampling from this probability\ndistribution. The other measure maximizes the minimum probability for every\nentity of being selected in a solution. In particular, we reveal that computing\nthese individual-fairness measures is in fact equivalent to computing the\nfractional covering number and the fractional partitioning number of a\nhypergraph. In addition, we show that for a general class of problems that we\nclassify as independence systems, these two measures coincide. We also analyze\ngroup fairness and how this can be combined with the individual-fairness\nmeasures. Finally, we establish the computational complexity of determining\ngroup-fair solutions for matching.",
            "author": [
                "Christopher Hojny",
                "Frits Spieksma",
                "Sten Wessel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15953v1",
                "http://arxiv.org/pdf/2311.15953v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15950v1",
            "title": "Auto-CsiNet: Scenario-customized Automatic Neural Network Architecture\n  Generation for Massive MIMO CSI Feedback",
            "updated": "2023-11-27T15:56:58Z",
            "published": "2023-11-27T15:56:58Z",
            "summary": "Deep learning has revolutionized the design of the channel state information\n(CSI) feedback module in wireless communications. However, designing the\noptimal neural network (NN) architecture for CSI feedback can be a laborious\nand time-consuming process. Manual design can be prohibitively expensive for\ncustomizing NNs to different scenarios. This paper proposes using neural\narchitecture search (NAS) to automate the generation of scenario-customized CSI\nfeedback NN architectures, thereby maximizing the potential of deep learning in\nexclusive environments. By employing automated machine learning and\ngradient-descent-based NAS, an efficient and cost-effective architecture design\nprocess is achieved. The proposed approach leverages implicit scene knowledge,\nintegrating it into the scenario customization process in a data-driven manner,\nand fully exploits the potential of deep learning for each specific scenario.\nTo address the issue of excessive search, early stopping and elastic selection\nmechanisms are employed, enhancing the efficiency of the proposed scheme. The\nexperimental results demonstrate that the automatically generated architecture,\nknown as Auto-CsiNet, outperforms manually-designed models in both\nreconstruction performance (achieving approximately a 14% improvement) and\ncomplexity (reducing it by approximately 50%). Furthermore, the paper analyzes\nthe impact of the scenario on the NN architecture and its capacity.",
            "author": [
                "Xiangyi Li",
                "Jiajia Guo",
                "Chao-Kai Wen",
                "Shi Jin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15950v1",
                "http://arxiv.org/pdf/2311.15950v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15945v1",
            "title": "Over-Squashing in Riemannian Graph Neural Networks",
            "updated": "2023-11-27T15:51:07Z",
            "published": "2023-11-27T15:51:07Z",
            "summary": "Most graph neural networks (GNNs) are prone to the phenomenon of\nover-squashing in which node features become insensitive to information from\ndistant nodes in the graph. Recent works have shown that the topology of the\ngraph has the greatest impact on over-squashing, suggesting graph rewiring\napproaches as a suitable solution. In this work, we explore whether\nover-squashing can be mitigated through the embedding space of the GNN. In\nparticular, we consider the generalization of Hyperbolic GNNs (HGNNs) to\nRiemannian manifolds of variable curvature in which the geometry of the\nembedding space is faithful to the graph's topology. We derive bounds on the\nsensitivity of the node features in these Riemannian GNNs as the number of\nlayers increases, which yield promising theoretical and empirical results for\nalleviating over-squashing in graphs with negative curvature.",
            "author": [
                "Julia Balla"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15945v1",
                "http://arxiv.org/pdf/2311.15945v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15931v1",
            "title": "Low-Degree Hardness of Detection for Correlated Erd\u0151s-R\u00e9nyi Graphs",
            "updated": "2023-11-27T15:38:53Z",
            "published": "2023-11-27T15:38:53Z",
            "summary": "Given two Erd\\H{o}s-R\\'enyi graphs with $n$ vertices whose edges are\ncorrelated through a latent vertex correspondence, we study complexity lower\nbounds for the associated correlation detection problem for the class of\nlow-degree polynomial algorithms. We provide evidence that any\ndegree-$O(\\rho^{-1})$ polynomial algorithm fails for detection, where $\\rho$ is\nthe edge correlation. Furthermore, in the sparse regime where the edge density\n$q=n^{-1+o(1)}$, we provide evidence that any degree-$d$ polynomial algorithm\nfails for detection, as long as $\\log d=o\\big( \\frac{\\log n}{\\log nq} \\wedge\n\\sqrt{\\log n} \\big)$ and the correlation $\\rho<\\sqrt{\\alpha}$ where\n$\\alpha\\approx 0.338$ is the Otter's constant. Our result suggests that several\nstate-of-the-art algorithms on correlation detection and exact matching\nrecovery may be essentially the best possible.",
            "author": [
                "Jian Ding",
                "Hang Du",
                "Zhangsong Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15931v1",
                "http://arxiv.org/pdf/2311.15931v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.PR",
                "math.ST",
                "stat.TH",
                "68Q87, 62M20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16207v1",
            "title": "The Graph Convolutional Network with Multi-representation Alignment for\n  Drug Synergy Prediction",
            "updated": "2023-11-27T15:34:14Z",
            "published": "2023-11-27T15:34:14Z",
            "summary": "Drug combination refers to the use of two or more drugs to treat a specific\ndisease at the same time. It is currently the mainstream way to treat complex\ndiseases. Compared with single drugs, drug combinations have better efficacy\nand can better inhibit toxicity and drug resistance. The computational model\nbased on deep learning concatenates the representation of multiple drugs and\nthe corresponding cell line feature as input, and the output is whether the\ndrug combination can have an inhibitory effect on the cell line. However, this\nstrategy of concatenating multiple representations has the following defects:\nthe alignment of drug representation and cell line representation is ignored,\nresulting in the synergistic relationship not being reflected positionally in\nthe embedding space. Moreover, the alignment measurement function in deep\nlearning cannot be suitable for drug synergy prediction tasks due to\ndifferences in input types. Therefore, in this work, we propose a graph\nconvolutional network with multi-representation alignment (GCNMRA) for\npredicting drug synergy. In the GCNMRA model, we designed a\nmulti-representation alignment function suitable for the drug synergy\nprediction task so that the positional relationship between drug\nrepresentations and cell line representation is reflected in the embedding\nspace. In addition, the vector modulus of drug representations and cell line\nrepresentation is considered to improve the accuracy of calculation results and\naccelerate model convergence. Finally, many relevant experiments were run on\nmultiple drug synergy datasets to verify the effectiveness of the above\ninnovative elements and the excellence of the GCNMRA model.",
            "author": [
                "Xinxing Yang",
                "Genke Yang",
                "Jian Chu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16207v1",
                "http://arxiv.org/pdf/2311.16207v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15917v1",
            "title": "When Graph Convolution Meets Double Attention: Online Privacy Disclosure\n  Detection with Multi-Label Text Classification",
            "updated": "2023-11-27T15:25:17Z",
            "published": "2023-11-27T15:25:17Z",
            "summary": "With the rise of Web 2.0 platforms such as online social media, people's\nprivate information, such as their location, occupation and even family\ninformation, is often inadvertently disclosed through online discussions.\nTherefore, it is important to detect such unwanted privacy disclosures to help\nalert people affected and the online platform. In this paper, privacy\ndisclosure detection is modeled as a multi-label text classification (MLTC)\nproblem, and a new privacy disclosure detection model is proposed to construct\nan MLTC classifier for detecting online privacy disclosures. This classifier\ntakes an online post as the input and outputs multiple labels, each reflecting\na possible privacy disclosure. The proposed presentation method combines three\ndifferent sources of information, the input text itself, the label-to-text\ncorrelation and the label-to-label correlation. A double-attention mechanism is\nused to combine the first two sources of information, and a graph convolutional\nnetwork (GCN) is employed to extract the third source of information that is\nthen used to help fuse features extracted from the first two sources of\ninformation. Our extensive experimental results, obtained on a public dataset\nof privacy-disclosing posts on Twitter, demonstrated that our proposed privacy\ndisclosure detection method significantly and consistently outperformed other\nstate-of-the-art methods in terms of all key performance indicators.",
            "author": [
                "Zhanbo Liang",
                "Jie Guo",
                "Weidong Qiu",
                "Zheng Huang",
                "Shujun Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15917v1",
                "http://arxiv.org/pdf/2311.15917v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15912v1",
            "title": "LIFT OFF: LoRaWAN Installation and Fiducial Tracking Operations for the\n  Flightline of the Future",
            "updated": "2023-11-27T15:22:17Z",
            "published": "2023-11-27T15:22:17Z",
            "summary": "Real-time situational awareness for the location of assets is critical to\nensure missions are completed efficiently and requirements are satisfied. In\nmany commercial settings, the application of global positioning system (GPS)\nsensors is appropriate to achieve timely knowledge of the position of people\nand equipment. However, GPS sensors are not appropriate for all situations due\nto flight clearance and operations security concerns. LIFT OFF: LoRaWAN\nInstallation and Fiducial Tracking Operations for the Flightline of the Future\nproposes a hybrid framework solution to achieve real-time situational awareness\nfor people, support equipment, and aircraft positions regardless of the\nenvironment. This framework included a machine-vision component, which involved\nsetting up cameras to detect AprilTag decals that were installed on the sides\nof aircraft. The framework included a geolocation sensor component, which\ninvolved installing GPS sensors on support equipment and helmets. The framework\nalso included creating a long-range wide area network (LoRaWAN) to transfer\ndata and developing a user interface to display the data. The framework was\ntested at Naval Air Station Oceana Flightline, the United States Naval Test\nPilot School, and at Naval Air Warfare Center Aircraft Division Lakehurst. LIFT\nOFF successfully provided a real-time updating map of all tracked assets using\nGPS sensors for people and support equipment and with visual fiducials for\naircraft. The trajectories of the assets were recorded for logistical analysis\nand playback. Future follow-on work is anticipated to apply the technology to\nother environments including carriers and amphibious assault ships in addition\nto the flightline.",
            "author": [
                "Ari Goodman",
                "Ryan O'Shea"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15912v1",
                "http://arxiv.org/pdf/2311.15912v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15910v1",
            "title": "Automorphisms of Leavitt path algebras: Zhang twist and irreducible\n  representations",
            "updated": "2023-11-27T15:21:41Z",
            "published": "2023-11-27T15:21:41Z",
            "summary": "In this article, we construct (graded) automorphisms fixing all vertices of\nLeavitt path algebras of arbitrary graphs in terms of general linear groups\nover corners of these algebras. As an application, we study Zhang twist of\nLeavitt path algebras and describe new classes of irreducible representations\nof Leavitt path algebras of the rose graphs $R_n$ with $n$ petals.",
            "author": [
                "Tran Giang Nam",
                "Ashish K. Srivastava",
                "Nguyen Thi Vien"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15910v1",
                "http://arxiv.org/pdf/2311.15910v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA",
                "16D60, 16D70, 16S88"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15906v1",
            "title": "MetaDefa: Meta-learning based on Domain Enhancement and Feature\n  Alignment for Single Domain Generalization",
            "updated": "2023-11-27T15:13:02Z",
            "published": "2023-11-27T15:13:02Z",
            "summary": "The single domain generalization(SDG) based on meta-learning has emerged as\nan effective technique for solving the domain-shift problem. However, the\ninadequate match of data distribution between source and augmented domains and\ndifficult separation of domain-invariant features from domain-related features\nmake SDG model hard to achieve great generalization. Therefore, a novel\nmeta-learning method based on domain enhancement and feature alignment\n(MetaDefa) is proposed to improve the model generalization performance. First,\nthe background substitution and visual corruptions techniques are used to\ngenerate diverse and effective augmented domains. Then, the multi-channel\nfeature alignment module based on class activation maps and class agnostic\nactivation maps is designed to effectively extract adequate transferability\nknowledge. In this module, domain-invariant features can be fully explored by\nfocusing on similar target regions between source and augmented domains feature\nspace and suppressing the feature representation of non-similar target regions.\nExtensive experiments on two publicly available datasets show that MetaDefa has\nsignificant generalization performance advantages in unknown multiple target\ndomains.",
            "author": [
                "Can Sun",
                "Hao Zheng",
                "Zhigang Hu",
                "Liu Yang",
                "Meiguang Zheng",
                "Bo Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15906v1",
                "http://arxiv.org/pdf/2311.15906v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15901v3",
            "title": "A novel cryogenic VUV spectrofluorometer for the characterization of\n  wavelength shifters",
            "updated": "2023-11-30T15:28:35Z",
            "published": "2023-11-27T15:03:54Z",
            "summary": "We present a novel cryogenic VUV spectrofluorometer designed for the\ncharacterization of wavelength shifters (WLS) crucial for experiments based on\nliquid argon (LAr) scintillation light detection. Wavelength shifters like\n1,1,4,4-tetraphenyl-1,3-butadiene (TPB) or polyethylene naphthalate (PEN) are\nused in these experiments to shift the VUV scintillation light to the visible\nregion. Precise knowledge of the optical properties of the WLS at liquid\nargon's temperature (87 K) and LAr scintillation wavelength (128 nm) is\nnecessary to model and understand the detector response. The cryogenic VUV\nspectrofluorometer was commissioned to measure the emission spectra and\nrelative wavelength shifting efficiency (WLSE) of samples between 300 K and 87\nK for VUV (128 nm) and UV (310 nm) excitation. New mitigation techniques for\nsurface effects on cold WLS were established. As part of this work, the\nTPB-based wavelength shifting reflector (WLSR) featured in the neutrinoless\ndouble-beta decay experiment LEGEND-200 was characterized. The wavelength\nshifting efficiency was observed to increase by (54 +/- 4)% from room\ntemperature (RT) to 87 K. PEN installed in LEGEND-200 was also characterized\nand a first measurement of the relative wavelength shifting efficiency and\nemission spectrum at RT and 87 K is presented. Surface effects from cooling\nwere corrected by normalizing the measurement at VUV excitation with the\nrespective ones at UV excitation. The WLSE of amorphous PEN was found to be\nenhanced by (49 +/- 1)% at 87 K compared to RT.",
            "author": [
                "A. Leonhardt",
                "M. Goldbrunner",
                "B. Hackett",
                "S. Sch\u00f6nert"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15901v3",
                "http://arxiv.org/pdf/2311.15901v3"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15900v1",
            "title": "Asymptotic smoothness, concentration properties in Banach spaces and\n  applications",
            "updated": "2023-11-27T15:03:08Z",
            "published": "2023-11-27T15:03:08Z",
            "summary": "We prove an optimal result of stability under $\\ell_p$-sums of some\nconcentration properties for Lipschitz maps defined on Hamming graphs into\nBanach spaces. As an application, we give examples of spaces with Szlenk index\narbitrarily high that admit nevertheless a concentration property. In\nparticular, we get the very first examples of Banach spaces with concentration\nbut without asymptotic smoothness property.",
            "author": [
                "Audrey Fovelle"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15900v1",
                "http://arxiv.org/pdf/2311.15900v1"
            ],
            "primary_category": "math.FA",
            "category": [
                "math.FA",
                "math.MG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15899v1",
            "title": "Exact Methods for the Longest Induced Cycle Problem",
            "updated": "2023-11-27T15:02:37Z",
            "published": "2023-11-27T15:02:37Z",
            "summary": "The longest induced (or chordless) cycle problem is a graph problem\nclassified as NP-complete and involves the task of determining the largest\npossible subset of vertices within a graph in such a way that the induced\nsubgraph forms a cycle. Within this paper, we present three integer linear\nprograms specifically formulated to yield optimal solutions for this problem.\nThe branch-and-cut algorithm has been used for two models. To demonstrate the\ncomputational efficiency of these methods, we utilize them on a range of\nreal-world graphs as well as random graphs. Additionally, we conduct a\ncomparative analysis against approaches previously proposed in the literature.",
            "author": [
                "Ahmad T. Anaqreh",
                "Bogl\u00e1rka G. -T\u00f3th",
                "Tam\u00e1s Vink\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15899v1",
                "http://arxiv.org/pdf/2311.15899v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15879v1",
            "title": "EVCap: Retrieval-Augmented Image Captioning with External Visual-Name\n  Memory for Open-World Comprehension",
            "updated": "2023-11-27T14:51:37Z",
            "published": "2023-11-27T14:51:37Z",
            "summary": "Large language models (LLMs)-based image captioning has the capability of\ndescribing objects not explicitly observed in training data; yet novel objects\noccur frequently, necessitating the requirement of sustaining up-to-date object\nknowledge for open-world comprehension. Instead of relying on large amounts of\ndata and scaling up network parameters, we introduce a highly effective\nretrieval-augmented image captioning method that prompts LLMs with object names\nretrieved from External Visual--name memory (EVCap). We build ever-changing\nobject knowledge memory using objects' visuals and names, enabling us to (i)\nupdate the memory at a minimal cost and (ii) effortlessly augment LLMs with\nretrieved object names utilizing a lightweight and fast-to-train model. Our\nmodel, which was trained only on the COCO dataset, can be adapted to out-domain\ndata without additional fine-tuning or retraining. Our comprehensive\nexperiments conducted on various benchmarks and synthetic commonsense-violating\ndata demonstrate that EVCap, comprising solely 3.97M trainable parameters,\nexhibits superior performance compared to other methods of equivalent model\nsize scale. Notably, it achieves competitive performance against specialist\nSOTAs with an enormous number of parameters. Our code is available at\nhttps://jiaxuan-li.github.io/EVCap.",
            "author": [
                "Jiaxuan Li",
                "Duc Minh Vo",
                "Akihiro Sugimoto",
                "Hideki Nakayama"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15879v1",
                "http://arxiv.org/pdf/2311.15879v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15878v1",
            "title": "Individualized Treatment Allocations with Distributional Welfare",
            "updated": "2023-11-27T14:51:30Z",
            "published": "2023-11-27T14:51:30Z",
            "summary": "In this paper, we explore optimal treatment allocation policies that target\ndistributional welfare. Most literature on treatment choice has considered\nutilitarian welfare based on the conditional average treatment effect (ATE).\nWhile average welfare is intuitive, it may yield undesirable allocations\nespecially when individuals are heterogeneous (e.g., with outliers) - the very\nreason individualized treatments were introduced in the first place. This\nobservation motivates us to propose an optimal policy that allocates the\ntreatment based on the conditional \\emph{quantile of individual treatment\neffects} (QoTE). Depending on the choice of the quantile probability, this\ncriterion can accommodate a policymaker who is either prudent or negligent. The\nchallenge of identifying the QoTE lies in its requirement for knowledge of the\njoint distribution of the counterfactual outcomes, which is generally hard to\nrecover even with experimental data. Therefore, we introduce minimax optimal\npolicies that are robust to model uncertainty. We then propose a range of\nidentifying assumptions under which we can point or partially identify the\nQoTE. We establish the asymptotic bound on the regret of implementing the\nproposed policies. We consider both stochastic and deterministic rules. In\nsimulations and two empirical applications, we compare optimal decisions based\non the QoTE with decisions based on other criteria.",
            "author": [
                "Yifan Cui",
                "Sukjin Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15878v1",
                "http://arxiv.org/pdf/2311.15878v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "econ.EM",
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15858v1",
            "title": "Multi-Agent Reinforcement Learning for Power Control in Wireless\n  Networks via Adaptive Graphs",
            "updated": "2023-11-27T14:25:40Z",
            "published": "2023-11-27T14:25:40Z",
            "summary": "The ever-increasing demand for high-quality and heterogeneous wireless\ncommunication services has driven extensive research on dynamic optimization\nstrategies in wireless networks. Among several possible approaches, multi-agent\ndeep reinforcement learning (MADRL) has emerged as a promising method to\naddress a wide range of complex optimization problems like power control.\nHowever, the seamless application of MADRL to a variety of network optimization\nproblems faces several challenges related to convergence. In this paper, we\npresent the use of graphs as communication-inducing structures among\ndistributed agents as an effective means to mitigate these challenges.\nSpecifically, we harness graph neural networks (GNNs) as neural architectures\nfor policy parameterization to introduce a relational inductive bias in the\ncollective decision-making process. Most importantly, we focus on modeling the\ndynamic interactions among sets of neighboring agents through the introduction\nof innovative methods for defining a graph-induced framework for integrated\ncommunication and learning. Finally, the superior generalization capabilities\nof the proposed methodology to larger networks and to networks with different\nuser categories is verified through simulations.",
            "author": [
                "Lorenzo Mario Amorosa",
                "Marco Skocaj",
                "Roberto Verdone",
                "Deniz G\u00fcnd\u00fcz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15858v1",
                "http://arxiv.org/pdf/2311.15858v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15846v1",
            "title": "Learning with Noisy Low-Cost MOS for Image Quality Assessment via\n  Dual-Bias Calibration",
            "updated": "2023-11-27T14:11:54Z",
            "published": "2023-11-27T14:11:54Z",
            "summary": "Learning based image quality assessment (IQA) models have obtained impressive\nperformance with the help of reliable subjective quality labels, where mean\nopinion score (MOS) is the most popular choice. However, in view of the\nsubjective bias of individual annotators, the labor-abundant MOS (LA-MOS)\ntypically requires a large collection of opinion scores from multiple\nannotators for each image, which significantly increases the learning cost. In\nthis paper, we aim to learn robust IQA models from low-cost MOS (LC-MOS), which\nonly requires very few opinion scores or even a single opinion score for each\nimage. More specifically, we consider the LC-MOS as the noisy observation of\nLA-MOS and enforce the IQA model learned from LC-MOS to approach the unbiased\nestimation of LA-MOS. In this way, we represent the subjective bias between\nLC-MOS and LA-MOS, and the model bias between IQA predictions learned from\nLC-MOS and LA-MOS (i.e., dual-bias) as two latent variables with unknown\nparameters. By means of the expectation-maximization based alternating\noptimization, we can jointly estimate the parameters of the dual-bias, which\nsuppresses the misleading of LC-MOS via a gated dual-bias calibration (GDBC)\nmodule. To the best of our knowledge, this is the first exploration of robust\nIQA model learning from noisy low-cost labels. Theoretical analysis and\nextensive experiments on four popular IQA datasets show that the proposed\nmethod is robust toward different bias rates and annotation numbers and\nsignificantly outperforms the other learning based IQA models when only LC-MOS\nis available. Furthermore, we also achieve comparable performance with respect\nto the other models learned with LA-MOS.",
            "author": [
                "Lei Wang",
                "Qingbo Wu",
                "Desen Yuan",
                "King Ngi Ngan",
                "Hongliang Li",
                "Fanman Meng",
                "Linfeng Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15846v1",
                "http://arxiv.org/pdf/2311.15846v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15845v3",
            "title": "On Learning the Optimal Regularization Parameter in Inverse Problems",
            "updated": "2023-12-01T15:14:52Z",
            "published": "2023-11-27T14:10:28Z",
            "summary": "Selecting the best regularization parameter in inverse problems is a\nclassical and yet challenging problem. Recently, data-driven approaches have\nbecome popular to tackle this challenge. These approaches are appealing since\nthey do require less a priori knowledge, but their theoretical analysis is\nlimited. In this paper, we propose and study a statistical machine learning\napproach, based on empirical risk minimization. Our main contribution is a\ntheoretical analysis, showing that, provided with enough data, this approach\ncan reach sharp rates while being essentially adaptive to the noise and\nsmoothness of the problem. Numerical simulations corroborate and illustrate the\ntheoretical findings. Our results are a step towards grounding theoretically\ndata-driven approaches to inverse problems.",
            "author": [
                "Jonathan Chirinos Rodriguez",
                "Ernesto De Vito",
                "Cesare Molinari",
                "Lorenzo Rosasco",
                "Silvia Villa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15845v3",
                "http://arxiv.org/pdf/2311.15845v3"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.OC",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15821v1",
            "title": "The minimum degree of minimal $k$-factor-critical claw-free graphs*",
            "updated": "2023-11-27T13:46:52Z",
            "published": "2023-11-27T13:46:52Z",
            "summary": "A graph $G$ of order $n$ is said to be $k$-factor-critical for integers\n$1\\leq k< n$, if the removal of any $k$ vertices results in a graph with a\nperfect matching. A $k$-factor-critical graph is minimal if for every edge, the\ndeletion of it results in a graph that is not $k$-factor-critical. In 1998, O.\nFavaron and M. Shi conjectured that every minimal $k$-factor-critical graph has\nminimum degree $k+1$. In this paper, we confirm the conjecture for minimal\n$k$-factor-critical claw-free graphs. Moreover, we show that every minimal\n$k$-factor-critical claw-free graph $G$ has at least $\\frac{k-1}{2k}|V(G)|$\nvertices of degree $k+1$ in the case of $(k+1)$-connected, yielding further\nevidence for S. Norine and R. Thomas' conjecture on the minimum degree of\nminimal bricks when $k=2$.",
            "author": [
                "Jing Guo",
                "Qiuli Li",
                "Fuliang Lu",
                "Heping Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15821v1",
                "http://arxiv.org/pdf/2311.15821v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15814v1",
            "title": "Towards complete characterization of topological insulators and\n  superconductors: A systematic construction of topological invariants based on\n  Atiyah-Hirzebruch spectral sequence",
            "updated": "2023-11-27T13:40:01Z",
            "published": "2023-11-27T13:40:01Z",
            "summary": "The past decade has witnessed significant progress in topological materials\ninvestigation. Symmetry-indicator theory and topological quantum chemistry\nprovide an efficient scheme to diagnose topological phases from only partial\ninformation of wave functions without full knowledge of topological invariants,\nwhich has resulted in a recent comprehensive materials search. However, not all\ntopological phases can be captured by this framework, and topological\ninvariants are needed for a more refined diagnosis of topological phases. In\nthis study, we present a systematic framework to construct topological\ninvariants for a large part of symmetry classes, which should be contrasted\nwith the existing invariants discovered through one-by-one approaches. Our\nmethod is based on the recently developed Atiyah-Hirzebruch spectral sequence\nin momentum space. As a demonstration, we construct topological invariants for\ntime-reversal symmetric spinful superconductors with conventional pairing\nsymmetries of all space groups, for which symmetry indicators are silent. We\nalso validate that the obtained quantities work as topological invariants by\ncomputing them for randomly generated symmetric Hamiltonians. Remarkably, the\nconstructed topological invariants completely characterize $K$-groups in 159\nspace groups. Our topological invariants for normal conducting phases are\ndefined under some gauge conditions. To facilitate efficient numerical\nsimulations, we discuss how to derive gauge-independent topological invariants\nfrom the gauge-fixed topological invariants through some examples. Combined\nwith first-principles calculations, our results will help us discover\ntopological materials that could be used in next-generation devices and pave\nthe way for a more comprehensive topological materials database.",
            "author": [
                "Seishiro Ono",
                "Ken Shiozaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15814v1",
                "http://arxiv.org/pdf/2311.15814v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.mtrl-sci",
                "cond-mat.supr-con"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16500v1",
            "title": "LLMGA: Multimodal Large Language Model based Generation Assistant",
            "updated": "2023-11-27T13:37:26Z",
            "published": "2023-11-27T13:37:26Z",
            "summary": "In this paper, we introduce a Multimodal Large Language Model-based\nGeneration Assistant (LLMGA), leveraging the vast reservoir of knowledge and\nproficiency in reasoning, comprehension, and response inherent in Large\nLanguage Models (LLMs) to assist users in image generation and editing.\nDiverging from existing approaches where Multimodal Large Language Models\n(MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our\nLLMGA provides a detailed language generation prompt for precise control over\nSD. This not only augments LLM context understanding but also reduces noise in\ngeneration prompts, yields images with more intricate and precise content, and\nelevates the interpretability of the network. To this end, we curate a\ncomprehensive dataset comprising prompt refinement, similar image generation,\ninpainting $\\&$ outpainting, and visual question answering. Moreover, we\npropose a two-stage training scheme. In the first stage, we train the MLLM to\ngrasp the properties of image generation and editing, enabling it to generate\ndetailed prompts. In the second stage, we optimize SD to align with the MLLM's\ngeneration prompts. Additionally, we propose a reference-based restoration\nnetwork to alleviate texture, brightness, and contrast disparities between\ngenerated and preserved regions during image editing. Extensive results show\nthat LLMGA has promising generative capabilities and can enable wider\napplications in an interactive manner.",
            "author": [
                "Bin Xia",
                "Shiyin Wang",
                "Yingfan Tao",
                "Yitong Wang",
                "Jiaya Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16500v1",
                "http://arxiv.org/pdf/2311.16500v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15810v1",
            "title": "Tascade: Hardware Support for Atomic-free, Asynchronous and Efficient\n  Reduction Trees",
            "updated": "2023-11-27T13:32:33Z",
            "published": "2023-11-27T13:32:33Z",
            "summary": "As system parallelism at chip- and server-level increases, challenges that\narose with network-level systems a decade ago, are now being encountered with\nthese massively parallel systems that have become an important workhorse for\nMachine Learning workloads as well as Graph and Sparse workloads. To tackle the\ncommunication bottlenecks, recent works have introduced task-based\nparallelization schemes to accelerate graph search and sparse data-structure\ntraversal, where some solutions scale up to thousands of processing units (PUs)\non a single chip. However, existing communication schemes do not scale to\nlarger than thousands of processing tiles. To address these challenges we\npropose Tascade, a system that offers hardware-supported, efficient and\nbalanced reduction trees to reduce communication overheads in task-based\nparallelization schemes and scales up to a million PUs. Tascade achieves this\nby implementing an execution model utilizing proxy regions and cascading\nupdates, along with a supporting hardware design that enables the execution of\nthe reduction tree at the chip level. The Tascade approach reduces overall\ncommunication and improves load balancing. We evaluate six applications and\nfour datasets to provide a detailed analysis of Tascade's performance, power,\nand traffic-reduction gains over prior work. Our parallelization of\nBreadth-First-Search with RMAT-26 across a million PUs, the largest of the\nliterature, reaches 5305 GTEPS.",
            "author": [
                "Marcelo Orenes-Vera",
                "Esin Tureci",
                "David Wentzlaff",
                "Margaret Martonosi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15810v1",
                "http://arxiv.org/pdf/2311.15810v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15809v1",
            "title": "From deepfake to deep useful: risks and opportunities through a\n  systematic literature review",
            "updated": "2023-11-27T13:31:40Z",
            "published": "2023-11-27T13:31:40Z",
            "summary": "Deepfake videos are defined as a resulting media from the synthesis of\ndifferent persons images and videos, mostly faces, replacing a real one. The\neasy spread of such videos leads to elevated misinformation and represents a\nthreat to society and democracy today. The present study aims to collect and\nanalyze the relevant literature through a systematic procedure. We present 27\narticles from scientific databases revealing threats to society, democracies,\nthe political life but present as well advantages of this technology in\nentertainment, gaming, education, and public life. The research indicates high\nscientific interest in deepfake detection algorithms as well as the ethical\naspect of such technology. This article covers the scientific gap since, to the\nbest of our knowledge, this is the first systematic literature review in the\nfield. A discussion has already started among academics and practitioners\nconcerning the spread of fake news. The next step of fake news considers the\nuse of artificial intelligence and machine learning algorithms that create\nhyper-realistic videos, called deepfake. Deepfake technology has continuously\nattracted the attention of scholars over the last 3 years more and more. The\nimportance of conducting research in this field derives from the necessity to\nunderstand the theory. The first contextual approach is related to the\nepistemological points of view of the concept. The second one is related to the\nphenomenological disadvantages of the field. Despite that, the authors will try\nto focus not only on the disadvantages of the field but also on the positive\naspects of the technology.",
            "author": [
                "Nikolaos Misirlis",
                "Harris Bin Munawar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15809v1",
                "http://arxiv.org/pdf/2311.15809v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15790v1",
            "title": "A Social-aware Gaussian Pre-trained Model for Effective Cold-start\n  Recommendation",
            "updated": "2023-11-27T13:04:33Z",
            "published": "2023-11-27T13:04:33Z",
            "summary": "The use of pre-training is an emerging technique to enhance a neural model's\nperformance, which has been shown to be effective for many neural language\nmodels such as BERT. This technique has also been used to enhance the\nperformance of recommender systems. In such recommender systems, pre-training\nmodels are used to learn a better initialisation for both users and items.\nHowever, recent existing pre-trained recommender systems tend to only\nincorporate the user interaction data at the pre-training stage, making it\ndifficult to deliver good recommendations, especially when the interaction data\nis sparse. To alleviate this common data sparsity issue, we propose to\npre-train the recommendation model not only with the interaction data but also\nwith other available information such as the social relations among users,\nthereby providing the recommender system with a better initialisation compared\nwith solely relying on the user interaction data. We propose a novel\nrecommendation model, the Social-aware Gaussian Pre-trained model (SGP), which\nencodes the user social relations and interaction data at the pre-training\nstage in a Graph Neural Network (GNN). Afterwards, in the subsequent\nfine-tuning stage, our SGP model adopts a Gaussian Mixture Model (GMM) to\nfactorise these pre-trained embeddings for further training, thereby benefiting\nthe cold-start users from these pre-built social relations. Our extensive\nexperiments on three public datasets show that, in comparison to 16 competitive\nbaselines, our SGP model significantly outperforms the best baseline by upto\n7.7% in terms of NDCG@10. In addition, we show that SGP permits to effectively\nalleviate the cold-start problem, especially when users newly register to the\nsystem through their friends' suggestions.",
            "author": [
                "Siwei Liu",
                "Xi Wang",
                "Craig Macdonald",
                "Iadh Ounis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15790v1",
                "http://arxiv.org/pdf/2311.15790v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "68P20",
                "H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15786v2",
            "title": "YUAN 2.0: A Large Language Model with Localized Filtering-based\n  Attention",
            "updated": "2023-12-04T10:20:57Z",
            "published": "2023-11-27T13:01:59Z",
            "summary": "In this work, we develop and release Yuan 2.0, a series of large language\nmodels with parameters ranging from 2.1 billion to 102.6 billion. The Localized\nFiltering-based Attention (LFA) is introduced to incorporate prior knowledge of\nlocal dependencies of natural language into Attention. A data filtering and\ngenerating system is presented to build pre-training and fine-tuning dataset in\nhigh quality. A distributed training method with non-uniform pipeline parallel,\ndata parallel, and optimizer parallel is proposed, which greatly reduces the\nbandwidth requirements of intra-node communication, and achieves good\nperformance in large-scale distributed training. Yuan 2.0 models display\nimpressive ability in code generation, math problem-solving, and chatting\ncompared with existing models. The latest version of YUAN 2.0, including model\nweights and source code, is accessible at Github.",
            "author": [
                "Shaohua Wu",
                "Xudong Zhao",
                "Shenling Wang",
                "Jiangang Luo",
                "Lingjun Li",
                "Xi Chen",
                "Bing Zhao",
                "Wei Wang",
                "Tong Yu",
                "Rongguo Zhang",
                "Jiahua Zhang",
                "Chao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15786v2",
                "http://arxiv.org/pdf/2311.15786v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16510v1",
            "title": "Source-Free Domain Adaptation with Frozen Multimodal Foundation Model",
            "updated": "2023-11-27T12:58:02Z",
            "published": "2023-11-27T12:58:02Z",
            "summary": "Source-Free Domain Adaptation (SFDA) aims to adapt a source model for a\ntarget domain, with only access to unlabeled target training data and the\nsource model pre-trained on a supervised source domain. Relying on pseudo\nlabeling and/or auxiliary supervision, conventional methods are inevitably\nerror-prone. To mitigate this limitation, in this work we for the first time\nexplore the potentials of off-the-shelf vision-language (ViL) multimodal models\n(e.g.,CLIP) with rich whilst heterogeneous knowledge. We find that directly\napplying the ViL model to the target domain in a zero-shot fashion is\nunsatisfactory, as it is not specialized for this particular task but largely\ngeneric. To make it task specific, we propose a novel Distilling multimodal\nFoundation model(DIFO)approach. Specifically, DIFO alternates between two steps\nduring adaptation: (i) Customizing the ViL model by maximizing the mutual\ninformation with the target model in a prompt learning manner, (ii) Distilling\nthe knowledge of this customized ViL model to the target model. For more\nfine-grained and reliable distillation, we further introduce two effective\nregularization terms, namely most-likely category encouragement and predictive\nconsistency. Extensive experiments show that DIFO significantly outperforms the\nstate-of-the-art alternatives. Our source code will be released.",
            "author": [
                "Song Tang",
                "Wenxin Su",
                "Mao Ye",
                "Xiatian Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16510v1",
                "http://arxiv.org/pdf/2311.16510v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15781v1",
            "title": "Increasing Coverage and Precision of Textual Information in Multilingual\n  Knowledge Graphs",
            "updated": "2023-11-27T12:54:47Z",
            "published": "2023-11-27T12:54:47Z",
            "summary": "Recent work in Natural Language Processing and Computer Vision has been using\ntextual information -- e.g., entity names and descriptions -- available in\nknowledge graphs to ground neural models to high-quality structured data.\nHowever, when it comes to non-English languages, the quantity and quality of\ntextual information are comparatively scarce. To address this issue, we\nintroduce the novel task of automatic Knowledge Graph Enhancement (KGE) and\nperform a thorough investigation on bridging the gap in both the quantity and\nquality of textual information between English and non-English languages. More\nspecifically, we: i) bring to light the problem of increasing multilingual\ncoverage and precision of entity names and descriptions in Wikidata; ii)\ndemonstrate that state-of-the-art methods, namely, Machine Translation (MT),\nWeb Search (WS), and Large Language Models (LLMs), struggle with this task;\niii) present M-NTA, a novel unsupervised approach that combines MT, WS, and\nLLMs to generate high-quality textual information; and, iv) study the impact of\nincreasing multilingual coverage and precision of non-English textual\ninformation in Entity Linking, Knowledge Graph Completion, and Question\nAnswering. As part of our effort towards better multilingual knowledge graphs,\nwe also introduce WikiKGE-10, the first human-curated benchmark to evaluate KGE\napproaches in 10 languages across 7 language families.",
            "author": [
                "Simone Conia",
                "Min Li",
                "Daniel Lee",
                "Umar Farooq Minhas",
                "Ihab Ilyas",
                "Yunyao Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15781v1",
                "http://arxiv.org/pdf/2311.15781v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15772v1",
            "title": "Attend Who is Weak: Enhancing Graph Condensation via Cross-Free\n  Adversarial Training",
            "updated": "2023-11-27T12:44:42Z",
            "published": "2023-11-27T12:44:42Z",
            "summary": "In this paper, we study the \\textit{graph condensation} problem by\ncompressing the large, complex graph into a concise, synthetic representation\nthat preserves the most essential and discriminative information of structure\nand features. We seminally propose the concept of Shock Absorber (a type of\nperturbation) that enhances the robustness and stability of the original graphs\nagainst changes in an adversarial training fashion. Concretely, (I) we forcibly\nmatch the gradients between pre-selected graph neural networks (GNNs) trained\non a synthetic, simplified graph and the original training graph at regularly\nspaced intervals. (II) Before each update synthetic graph point, a Shock\nAbsorber serves as a gradient attacker to maximize the distance between the\nsynthetic dataset and the original graph by selectively perturbing the parts\nthat are underrepresented or insufficiently informative. We iteratively repeat\nthe above two processes (I and II) in an adversarial training fashion to\nmaintain the highly-informative context without losing correlation with the\noriginal dataset. More importantly, our shock absorber and the synthesized\ngraph parallelly share the backward process in a free training manner. Compared\nto the original adversarial training, it introduces almost no additional time\noverhead.\n  We validate our framework across 8 datasets (3 graph and 5 node\nclassification datasets) and achieve prominent results: for example, on Cora,\nCiteseer and Ogbn-Arxiv, we can gain nearly 1.13% to 5.03% improvements compare\nwith SOTA models. Moreover, our algorithm adds only about 0.2% to 2.2%\nadditional time overhead over Flicker, Citeseer and Ogbn-Arxiv. Compared to the\ngeneral adversarial training, our approach improves time efficiency by nearly\n4-fold.",
            "author": [
                "Xinglin Li",
                "Kun Wang",
                "Hanhui Deng",
                "Yuxuan Liang",
                "Di Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15772v1",
                "http://arxiv.org/pdf/2311.15772v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15766v1",
            "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges",
            "updated": "2023-11-27T12:37:51Z",
            "published": "2023-11-27T12:37:51Z",
            "summary": "In recent years, large language models (LLMs) have spurred a new research\nparadigm in natural language processing. Despite their excellent capability in\nknowledge-based question answering and reasoning, their potential to retain\nfaulty or even harmful knowledge poses risks of malicious application. The\nchallenge of mitigating this issue and transforming these models into purer\nassistants is crucial for their widespread applicability. Unfortunately,\nRetraining LLMs repeatedly to eliminate undesirable knowledge is impractical\ndue to their immense parameters. Knowledge unlearning, derived from analogous\nstudies on machine unlearning, presents a promising avenue to address this\nconcern and is notably advantageous in the context of LLMs. It allows for the\nremoval of harmful knowledge in an efficient manner, without affecting\nunrelated knowledge in the model. To this end, we provide a survey of knowledge\nunlearning in the era of LLMs. Firstly, we formally define the knowledge\nunlearning problem and distinguish it from related works. Subsequently, we\ncategorize existing knowledge unlearning methods into three classes: those\nbased on parameter optimization, parameter merging, and in-context learning,\nand introduce details of these unlearning methods. We further present\nevaluation datasets used in existing methods, and finally conclude this survey\nby presenting the ongoing challenges and future directions.",
            "author": [
                "Nianwen Si",
                "Hao Zhang",
                "Heyu Chang",
                "Wenlin Zhang",
                "Dan Qu",
                "Weiqiang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15766v1",
                "http://arxiv.org/pdf/2311.15766v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15762v1",
            "title": "Universal fidelity-dissipation relations in quantum gates",
            "updated": "2023-11-27T12:31:52Z",
            "published": "2023-11-27T12:31:52Z",
            "summary": "Accurate quantum computing relies on the precision of quantum gates. However,\nquantum gates in practice are generally affected by dissipative environments,\nwhich can significantly reduce their fidelity. In this Letter, we elucidate\nuniversal relations between the average fidelity of generic quantum gates and\nthe dissipation that occurs during the computing processes. Considering\nscenarios in which a quantum gate is subject to Markovian environments, we\nrigorously derive fidelity-dissipation relations that universally hold for\narbitrary operational times. Intriguingly, when the quantum gate undergoes\nthermal relaxation, the result can be used as a valuable tool for estimating\ndissipation through experimentally measurable fidelity, without requiring\ndetailed knowledge of the dissipative structure. For the case of arbitrary\nenvironments, we uncover a tradeoff relation between the average fidelity and\nenergy dissipation, implying that these quantities cannot be large\nsimultaneously. Our results unveil the computational limitations imposed by\nthermodynamics, shedding light on the profound connection between\nthermodynamics and quantum computing.",
            "author": [
                "Tan Van Vu",
                "Tomotaka Kuwahara",
                "Keiji Saito"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15762v1",
                "http://arxiv.org/pdf/2311.15762v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15759v1",
            "title": "Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage\n  and Sharing in LLMs",
            "updated": "2023-11-27T12:29:20Z",
            "published": "2023-11-27T12:29:20Z",
            "summary": "Recent advancements in multimodal large language models (MLLMs) have achieved\nsignificant multimodal generation capabilities, akin to GPT-4. These models\npredominantly map visual information into language representation space,\nleveraging the vast knowledge and powerful text generation abilities of LLMs to\nproduce multimodal instruction-following responses. We could term this method\nas LLMs for Vision because of its employing LLMs for visual-language\nunderstanding, yet observe that these MLLMs neglect the potential of harnessing\nvisual knowledge to enhance overall capabilities of LLMs, which could be\nregraded as Vision Enhancing LLMs. In this paper, we propose an approach called\nMKS2, aimed at enhancing LLMs through empowering Multimodal Knowledge Storage\nand Sharing in LLMs. Specifically, we introduce the Modular Visual Memory, a\ncomponent integrated into the internal blocks of LLMs, designed to store\nopen-world visual information efficiently. Additionally, we present a soft\nMixtures-of-Multimodal Experts architecture in LLMs to invoke multimodal\nknowledge collaboration during generation. Our comprehensive experiments\ndemonstrate that MKS2 substantially augments the reasoning capabilities of LLMs\nin contexts necessitating physical or commonsense knowledge. It also delivers\ncompetitive results on multimodal benchmarks.",
            "author": [
                "Yunxin Li",
                "Baotian Hu",
                "Wei Wang",
                "Xiaochun Cao",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15759v1",
                "http://arxiv.org/pdf/2311.15759v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15756v1",
            "title": "Learning Multi-Frequency Partial Correlation Graphs",
            "updated": "2023-11-27T12:22:44Z",
            "published": "2023-11-27T12:22:44Z",
            "summary": "Despite the large research effort devoted to learning dependencies between\ntime series, the state of the art still faces a major limitation: existing\nmethods learn partial correlations but fail to discriminate across distinct\nfrequency bands. Motivated by many applications in which this differentiation\nis pivotal, we overcome this limitation by learning a block-sparse,\nfrequency-dependent, partial correlation graph, in which layers correspond to\ndifferent frequency bands, and partial correlations can occur over just a few\nlayers. To this aim, we formulate and solve two nonconvex learning problems:\nthe first has a closed-form solution and is suitable when there is prior\nknowledge about the number of partial correlations; the second hinges on an\niterative solution based on successive convex approximation, and is effective\nfor the general case where no prior knowledge is available. Numerical results\non synthetic data show that the proposed methods outperform the current state\nof the art. Finally, the analysis of financial time series confirms that\npartial correlations exist only within a few frequency bands, underscoring how\nour methods enable the gaining of valuable insights that would be undetected\nwithout discriminating along the frequency domain.",
            "author": [
                "Gabriele D'Acunto",
                "Paolo Di Lorenzo",
                "Francesco Bonchi",
                "Stefania Sardellitti",
                "Sergio Barbarossa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15756v1",
                "http://arxiv.org/pdf/2311.15756v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15732v1",
            "title": "GPT4Vis: What Can GPT-4 Do for Zero-shot Visual Recognition?",
            "updated": "2023-11-27T11:29:10Z",
            "published": "2023-11-27T11:29:10Z",
            "summary": "This paper does not present a novel method. Instead, it delves into an\nessential, yet must-know baseline in light of the latest advancements in\nGenerative Artificial Intelligence (GenAI): the utilization of GPT-4 for visual\nunderstanding. Our study centers on the evaluation of GPT-4's linguistic and\nvisual capabilities in zero-shot visual recognition tasks. Specifically, we\nexplore the potential of its generated rich textual descriptions across various\ncategories to enhance recognition performance without any training.\nAdditionally, we evaluate its visual proficiency in directly recognizing\ndiverse visual content. To achieve this, we conduct an extensive series of\nexperiments, systematically quantifying the performance of GPT-4 across three\nmodalities: images, videos, and point clouds. This comprehensive evaluation\nencompasses a total of 16 widely recognized benchmark datasets, providing top-1\nand top-5 accuracy metrics. Our study reveals that leveraging GPT-4's advanced\nlinguistic knowledge to generate rich descriptions markedly improves zero-shot\nrecognition. In terms of visual proficiency, GPT-4V's average performance\nacross 16 datasets sits roughly between the capabilities of OpenAI-CLIP's ViT-L\nand EVA-CLIP's ViT-E. We hope that this research will contribute valuable data\npoints and experience for future studies. We release our code at\nhttps://github.com/whwu95/GPT4Vis.",
            "author": [
                "Wenhao Wu",
                "Huanjin Yao",
                "Mengxi Zhang",
                "Yuxin Song",
                "Wanli Ouyang",
                "Jingdong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15732v1",
                "http://arxiv.org/pdf/2311.15732v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15688v1",
            "title": "A Knowledge Graph Approach for Exploratory Search in Research\n  Institutions",
            "updated": "2023-11-27T10:27:26Z",
            "published": "2023-11-27T10:27:26Z",
            "summary": "Over the past decades, research institutions have grown increasingly and\nconsequently also their research output. This poses a significant challenge for\nresearchers seeking to understand the research landscape of an institution. The\nprocess of exploring the research landscape of institutions has a vague\ninformation need, no precise goal, and is open-ended. Current applications are\nnot designed to fulfill the requirements for exploratory search in research\ninstitutions. In this paper, we analyze exploratory search in research\ninstitutions and propose a knowledge graph-based approach to enhance this\nprocess.",
            "author": [
                "Tim Schopf",
                "Nektrios Machner",
                "Florian Matthes"
            ],
            "link": [
                "http://dx.doi.org/10.5220/0012223800003598",
                "http://arxiv.org/abs/2311.15688v1",
                "http://arxiv.org/pdf/2311.15688v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16479v1",
            "title": "Mitigating Hallucination in Visual Language Models with Visual\n  Supervision",
            "updated": "2023-11-27T09:30:02Z",
            "published": "2023-11-27T09:30:02Z",
            "summary": "Large vision-language models (LVLMs) suffer from hallucination a lot,\ngenerating responses that apparently contradict to the image content\noccasionally. The key problem lies in its weak ability to comprehend detailed\ncontent in a multi-modal context, which can be mainly attributed to two factors\nin training data and loss function. The vision instruction dataset primarily\nfocuses on global description, and the auto-regressive loss function favors\ntext modeling rather than image understanding. In this paper, we bring more\ndetailed vision annotations and more discriminative vision models to facilitate\nthe training of LVLMs, so that they can generate more precise responses without\nencounter hallucination. On one hand, we generate image-text pairs with\ndetailed relationship annotations in panoptic scene graph dataset (PSG). These\nconversations pay more attention on detailed facts in the image, encouraging\nthe model to answer questions based on multi-modal contexts. On the other hand,\nwe integrate SAM and mask prediction loss as auxiliary supervision, forcing the\nLVLMs to have the capacity to identify context-related objects, so that they\ncan generate more accurate responses, mitigating hallucination. Moreover, to\nprovide a deeper evaluation on the hallucination in LVLMs, we propose a new\nbenchmark, RAH-Bench. It divides vision hallucination into three different\ntypes that contradicts the image with wrong categories, attributes or\nrelations, and introduces False Positive Rate as detailed sub-metric for each\ntype. In this benchmark, our approach demonstrates an +8.4% enhancement\ncompared to original LLaVA and achieves widespread performance improvements\nacross other models.",
            "author": [
                "Zhiyang Chen",
                "Yousong Zhu",
                "Yufei Zhan",
                "Zhaowen Li",
                "Chaoyang Zhao",
                "Jinqiao Wang",
                "Ming Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16479v1",
                "http://arxiv.org/pdf/2311.16479v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15643v1",
            "title": "A Survey on Monocular Re-Localization: From the Perspective of Scene Map\n  Representation",
            "updated": "2023-11-27T09:13:52Z",
            "published": "2023-11-27T09:13:52Z",
            "summary": "Monocular Re-Localization (MRL) is a critical component in numerous\nautonomous applications, which estimates 6 degree-of-freedom poses with regards\nto the scene map based on a single monocular image. In recent decades,\nsignificant progress has been made in the development of MRL techniques.\nNumerous landmark algorithms have accomplished extraordinary success in terms\nof localization accuracy and robustness against visual interference. In MRL\nresearch, scene maps are represented in various forms, and they determine how\nMRL methods work and even how MRL methods perform. However, to the best of our\nknowledge, existing surveys do not provide systematic reviews of MRL from the\nrespective of map. This survey fills the gap by comprehensively reviewing MRL\nmethods employing monocular cameras as main sensors, promoting further\nresearch. 1) We commence by delving into the problem definition of MRL and\nexploring current challenges, while also comparing ours with with previous\npublished surveys. 2) MRL methods are then categorized into five classes\naccording to the representation forms of utilized map, i.e., geo-tagged frames,\nvisual landmarks, point clouds, and vectorized semantic map, and we review the\nmilestone MRL works of each category. 3) To quantitatively and fairly compare\nMRL methods with various map, we also review some public datasets and provide\nthe performances of some typical MRL methods. The strengths and weakness of\ndifferent types of MRL methods are analyzed. 4) We finally introduce some\ntopics of interest in this field and give personal opinions. This survey can\nserve as a valuable referenced materials for newcomers and researchers\ninterested in MRL, and a continuously updated summary of this survey, including\nreviewed papers and datasets, is publicly available to the community at:\nhttps://github.com/jinyummiao/map-in-mono-reloc.",
            "author": [
                "Jinyu Miao",
                "Kun Jiang",
                "Tuopu Wen",
                "Yunlong Wang",
                "Peijing Jia",
                "Xuhe Zhao",
                "Zhongyang Xiao",
                "Jin Huang",
                "Zhihua Zhong",
                "Diange Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15643v1",
                "http://arxiv.org/pdf/2311.15643v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15642v1",
            "title": "InfoPattern: Unveiling Information Propagation Patterns in Social Media",
            "updated": "2023-11-27T09:12:35Z",
            "published": "2023-11-27T09:12:35Z",
            "summary": "Social media play a significant role in shaping public opinion and\ninfluencing ideological communities through information propagation. Our demo\nInfoPattern centers on the interplay between language and human ideology. The\ndemo (Code: https://github.com/blender-nlp/InfoPattern ) is capable of: (1) red\nteaming to simulate adversary responses from opposite ideology communities; (2)\nstance detection to identify the underlying political sentiments in each\nmessage; (3) information propagation graph discovery to reveal the evolution of\nclaims across various communities over time. (Live Demo:\nhttps://incas.csl.illinois.edu/blender/About )",
            "author": [
                "Chi Han",
                "Jialiang Xu",
                "Manling Li",
                "Hanning Zhang",
                "Tarek Abdelzaher",
                "Heng Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15642v1",
                "http://arxiv.org/pdf/2311.15642v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15639v1",
            "title": "Almost Logarithmic Approximation for Cutwidth and Pathwidth",
            "updated": "2023-11-27T09:09:53Z",
            "published": "2023-11-27T09:09:53Z",
            "summary": "We study several graph layout problems with a min max objective. Here, given\na graph we wish to find a linear ordering of the vertices that minimizes some\nworst case objective over the natural cuts in the ordering; which separate an\ninitial segment of the vertices from the rest. A prototypical problem here is\ncutwidth, where we want to minimize the maximum number of edges crossing a cut.\nThe only known algorithm here is by [Leighton-Rao J.ACM 99] based on\nrecursively partitioning the graph using balanced cuts. This achieves an\n$O(\\log^{3/2}{n})$ approximation using the $ O(\\log^{1/2}{n})$ approximation of\n[Arora-Rao-Vazirani J.ACM 09] for balanced cuts.\n  We depart from the above approach and give an improved $ O(\\log^{1+o(1)}{n})$\napproximation for cutwidth. Our approach also gives a similarly improved $\nO(\\log^{1+o(1)}{n})$ approximation for finding the pathwidth of a graph.\nPreviously, the best known approximation for pathwidth was $O(\\log^{3/2}{n})$.",
            "author": [
                "Nikhil Bansal",
                "Dor Katzelnick",
                "Roy Schwartz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15639v1",
                "http://arxiv.org/pdf/2311.15639v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16203v2",
            "title": "ChatTraffic: Text-to-Traffic Generation via Diffusion Model",
            "updated": "2023-11-29T01:53:46Z",
            "published": "2023-11-27T08:52:10Z",
            "summary": "Traffic prediction is one of the most significant foundations in Intelligent\nTransportation Systems (ITS). Traditional traffic prediction methods rely only\non historical traffic data to predict traffic trends and face two main\nchallenges. 1) insensitivity to unusual events. 2) poor performance in\nlong-term prediction. In this work, we explore how generative models combined\nwith text describing the traffic system can be applied for traffic generation\nand name the task Text-to-Traffic Generation (TTG). The key challenge of the\nTTG task is how to associate text with the spatial structure of the road\nnetwork and traffic data for generating traffic situations. To this end, we\npropose ChatTraffic, the first diffusion model for text-to-traffic generation.\nTo guarantee the consistency between synthetic and real data, we augment a\ndiffusion model with the Graph Convolutional Network (GCN) to extract spatial\ncorrelations of traffic data. In addition, we construct a large dataset\ncontaining text-traffic pairs for the TTG task. We benchmarked our model\nqualitatively and quantitatively on the released dataset. The experimental\nresults indicate that ChatTraffic can generate realistic traffic situations\nfrom the text. Our code and dataset are available at\nhttps://github.com/ChyaZhang/ChatTraffic.",
            "author": [
                "Chengyang Zhang",
                "Yong Zhang",
                "Qitan Shao",
                "Bo Li",
                "Yisheng Lv",
                "Xinglin Piao",
                "Baocai Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16203v2",
                "http://arxiv.org/pdf/2311.16203v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15626v1",
            "title": "The WebCrow French Crossword Solver",
            "updated": "2023-11-27T08:45:31Z",
            "published": "2023-11-27T08:45:31Z",
            "summary": "Crossword puzzles are one of the most popular word games, played in different\nlanguages all across the world, where riddle style can vary significantly from\none country to another. Automated crossword resolution is challenging, and\ntypical solvers rely on large databases of previously solved crosswords. In\nthis work, we extend WebCrow 2.0, an automatic crossword solver, to French,\nmaking it the first program for crossword solving in the French language. To\ncope with the lack of a large repository of clue-answer crossword data, WebCrow\n2.0 exploits multiple modules, called experts, that retrieve candidate answers\nfrom heterogeneous resources, such as the web, knowledge graphs, and linguistic\nrules. We compared WebCrow's performance against humans in two different\nchallenges. Despite the limited amount of past crosswords, French WebCrow was\ncompetitive, actually outperforming humans in terms of speed and accuracy, thus\nproving its capabilities to generalize to new languages.",
            "author": [
                "Giovanni Angelini",
                "Marco Ernandes",
                "Tommaso laquinta",
                "Caroline Stehl\u00e9",
                "Fanny Sim\u00f5es",
                "Kamyar Zeinalipour",
                "Andrea Zugarini",
                "Marco Gori"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15626v1",
                "http://arxiv.org/pdf/2311.15626v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15623v1",
            "title": "Injecting linguistic knowledge into BERT for Dialogue State Tracking",
            "updated": "2023-11-27T08:38:42Z",
            "published": "2023-11-27T08:38:42Z",
            "summary": "Dialogue State Tracking (DST) models often employ intricate neural network\narchitectures, necessitating substantial training data, and their inference\nprocesses lack transparency. This paper proposes a method that extracts\nlinguistic knowledge via an unsupervised framework and subsequently utilizes\nthis knowledge to augment BERT's performance and interpretability in DST tasks.\nThe knowledge extraction procedure is computationally economical and does not\nnecessitate annotations or additional training data. The injection of the\nextracted knowledge necessitates the addition of only simple neural modules. We\nemploy the Convex Polytopic Model (CPM) as a feature extraction tool for DST\ntasks and illustrate that the acquired features correlate with the syntactic\nand semantic patterns in the dialogues. This correlation facilitates a\ncomprehensive understanding of the linguistic features influencing the DST\nmodel's decision-making process. We benchmark this framework on various DST\ntasks and observe a notable improvement in accuracy.",
            "author": [
                "Xiaohan Feng",
                "Xixin Wu",
                "Helen Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15623v1",
                "http://arxiv.org/pdf/2311.15623v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15614v1",
            "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language\n  Models",
            "updated": "2023-11-27T08:23:08Z",
            "published": "2023-11-27T08:23:08Z",
            "summary": "Collecting high-quality labeled data for model training is notoriously\ntime-consuming and labor-intensive for various NLP tasks. While copious\nsolutions, such as active learning for small language models (SLMs) and\nprevalent in-context learning in the era of large language models (LLMs), have\nbeen proposed and alleviate the labeling burden to some extent, their\nperformances are still subject to human intervention. It is still underexplored\nhow to reduce the annotation cost in the LLMs era. To bridge this, we\nrevolutionize traditional active learning and propose an innovative\ncollaborative learning framework FreeAL to interactively distill and filter the\ntask-specific knowledge from LLMs. During collaborative training, an LLM serves\nas an active annotator inculcating its coarse-grained knowledge, while a\ndownstream SLM is incurred as a student to filter out high-quality in-context\nsamples to feedback LLM for the subsequent label refinery. Extensive\nexperiments on eight benchmark datasets demonstrate that FreeAL largely\nenhances the zero-shot performances for both SLM and LLM without any human\nsupervision. The code is available at https://github.com/Justherozen/FreeAL .",
            "author": [
                "Ruixuan Xiao",
                "Yiwen Dong",
                "Junbo Zhao",
                "Runze Wu",
                "Minmin Lin",
                "Gang Chen",
                "Haobo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15614v1",
                "http://arxiv.org/pdf/2311.15614v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15610v1",
            "title": "Bayesian Approach to Linear Bayesian Networks",
            "updated": "2023-11-27T08:10:53Z",
            "published": "2023-11-27T08:10:53Z",
            "summary": "This study proposes the first Bayesian approach for learning high-dimensional\nlinear Bayesian networks. The proposed approach iteratively estimates each\nelement of the topological ordering from backward and its parent using the\ninverse of a partial covariance matrix. The proposed method successfully\nrecovers the underlying structure when Bayesian regularization for the inverse\ncovariance matrix with unequal shrinkage is applied. Specifically, it shows\nthat the number of samples $n = \\Omega( d_M^2 \\log p)$ and $n = \\Omega(d_M^2\np^{2/m})$ are sufficient for the proposed algorithm to learn linear Bayesian\nnetworks with sub-Gaussian and 4m-th bounded-moment error distributions,\nrespectively, where $p$ is the number of nodes and $d_M$ is the maximum degree\nof the moralized graph. The theoretical findings are supported by extensive\nsimulation studies including real data analysis. Furthermore the proposed\nmethod is demonstrated to outperform state-of-the-art frequentist approaches,\nsuch as the BHLSM, LISTEN, and TD algorithms in synthetic data.",
            "author": [
                "Seyong Hwang",
                "Kyoungjae Lee",
                "Sunmin Oh",
                "Gunwoong Park"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15610v1",
                "http://arxiv.org/pdf/2311.15610v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.ME",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15603v1",
            "title": "QuickDrop: Efficient Federated Unlearning by Integrated Dataset\n  Distillation",
            "updated": "2023-11-27T07:53:44Z",
            "published": "2023-11-27T07:53:44Z",
            "summary": "Federated Unlearning (FU) aims to delete specific training data from an ML\nmodel trained using Federated Learning (FL). We introduce QuickDrop, an\nefficient and original FU method that utilizes dataset distillation (DD) to\naccelerate unlearning and drastically reduces computational overhead compared\nto existing approaches. In QuickDrop, each client uses DD to generate a compact\ndataset representative of the original training dataset, called a distilled\ndataset, and uses this compact dataset during unlearning. To unlearn specific\nknowledge from the global model, QuickDrop has clients execute Stochastic\nGradient Ascent with samples from the distilled datasets, thus significantly\nreducing computational overhead compared to conventional FU methods. We further\nincrease the efficiency of QuickDrop by ingeniously integrating DD into the FL\ntraining process. By reusing the gradient updates produced during FL training\nfor DD, the overhead of creating distilled datasets becomes close to\nnegligible. Evaluations on three standard datasets show that, with comparable\naccuracy guarantees, QuickDrop reduces the duration of unlearning by 463.8x\ncompared to model retraining from scratch and 65.1x compared to existing FU\napproaches. We also demonstrate the scalability of QuickDrop with 100 clients\nand show its effectiveness while handling multiple unlearning operations.",
            "author": [
                "Akash Dhasade",
                "Yaohong Ding",
                "Song Guo",
                "Anne-marie Kermarrec",
                "Martijn De Vos",
                "Leijie Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15603v1",
                "http://arxiv.org/pdf/2311.15603v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15600v1",
            "title": "Non-intrusive, transferable model for coupled turbulent channel-porous\n  media flow based upon neural networks",
            "updated": "2023-11-27T07:49:25Z",
            "published": "2023-11-27T07:49:25Z",
            "summary": "Turbulent flow over permeable interface is omnipresent featuring complex flow\ntopology. In this work, a data driven, end to end machine learning model has\nbeen developed to model the turbulent flow in porous media. For the same, we\nhave derived a non linear reduced order model with a deep convolution\nautoencoder network. This model can reduce highly resolved spatial dimensions,\nwhich is a prerequisite for direct numerical simulation. A downstream recurrent\nneural network has been trained to capture the temporal trend of reduced modes,\nthus it is able to provide future evolution of modes. We further evaluate the\ntrained model s capability on a newer dataset with a different porosity. In\nsuch cases, fine tuning could reduce the efforts (up to two order of magnitude)\nto train a model with limited dataset and knowledge and still show a good\nagreement on the mean velocity profile. Leveraging the current model, we find\nthat even quick fine tuning achieving an impressive order of magnitude\nreduction in training time by approximately still results in effective flow\npredictions. This promising discovery encourages the fast development of a\nsubstantial amount of data-driven models tailored for various types of porous\nmedia. The diminished training time substantially lowers the computational cost\nwhen dealing with changing porous topologies, making it feasible to\nsystematically explore interface engineering with different types of porous\nmedia. Overall, the data driven model shows a good agreement, especially for\nthe porous media which can aid the DNS and reduce the burden to resolve this\ncomplex domain during the simulations. The fine tuning is able to reduce the\ntraining cost significantly and maintain an acceptable accuracy when a new flow\ncondition comes into play.",
            "author": [
                "Xu Chu",
                "Sandeep Pandey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15600v1",
                "http://arxiv.org/pdf/2311.15600v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15579v1",
            "title": "Two-particle Hadamard walk on dynamically percolated line",
            "updated": "2023-11-27T07:11:59Z",
            "published": "2023-11-27T07:11:59Z",
            "summary": "Asymptotic dynamics of a Hadamard walk of two non-interacting quantum\nparticles on a dynamically percolated finite line or a circle is investigated.\nWe construct a basis of the attractor space of the corresponding random-unitary\ndynamics and prove the completeness of our solution. In comparison to the\none-particle case, the structure of the attractor space is much more complex,\nresulting in intriguing asymptotic dynamics. General results are illustrated on\ntwo examples. First, for circles of length not divisible by 4 the boundary\nconditions reduces the number of attractors considerably, allowing for fully\nanalytic solution. Second, we investigate line of length 4 and determine the\nasymptotic cycle of reduced coin states and position distributions, focusing on\nthe correlations between the two particles. Our results show that a random\nunitary evolution, which is a combination of quantum dynamics and a classical\nstochasticity, leads to correlations between initially uncorrelated particles.\nThis is not possible for purely unitary evolution of non-interacting quantum\nparticles. The shared dynamically percolated graph can thus be considered as a\nweak form of interaction.",
            "author": [
                "M. Paryzkova",
                "M. Stefanak",
                "J. Novotny",
                "B. Kollar",
                "T. Kiss"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15579v1",
                "http://arxiv.org/pdf/2311.15579v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15570v1",
            "title": "UFDA: Universal Federated Domain Adaptation with Practical Assumptions",
            "updated": "2023-11-27T06:38:07Z",
            "published": "2023-11-27T06:38:07Z",
            "summary": "Conventional Federated Domain Adaptation (FDA) approaches usually demand an\nabundance of assumptions, such as label set consistency, which makes them\nsignificantly less feasible for real-world situations and introduces security\nhazards. In this work, we propose a more practical scenario named Universal\nFederated Domain Adaptation (UFDA). It only requires the black-box model and\nthe label set information of each source domain, while the label sets of\ndifferent source domains could be inconsistent and the target-domain label set\nis totally blind. This relaxes the assumptions made by FDA, which are often\nchallenging to meet in real-world cases and diminish model security. To address\nthe UFDA scenario, we propose a corresponding framework called Hot-Learning\nwith Contrastive Label Disambiguation (HCLD), which tackles UFDA's domain\nshifts and category gaps problem by using one-hot outputs from the black-box\nmodels of various source domains. Moreover, to better distinguish the shared\nand unknown classes, we further present a cluster-level strategy named\nMutual-Voting Decision (MVD) to extract robust consensus knowledge across peer\nclasses from both source and target domains. The extensive experiments on three\nbenchmarks demonstrate that our HCLD achieves comparable performance for our\nUFDA scenario with much fewer assumptions, compared to the previous\nmethodologies with many additional assumptions.",
            "author": [
                "Xinhui Liu",
                "Zhenghao Chen",
                "Luping Zhou",
                "Dong Xu",
                "Wei Xi",
                "Gairui Bai",
                "Yihan Zhao",
                "Jizhong Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15570v1",
                "http://arxiv.org/pdf/2311.15570v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15569v1",
            "title": "Improving Adaptability and Generalizability of Efficient Transfer\n  Learning for Vision-Language Models",
            "updated": "2023-11-27T06:37:05Z",
            "published": "2023-11-27T06:37:05Z",
            "summary": "Vision-Language Models (VLMs) like CLIP have demonstrated remarkable\napplicability across a variety of downstream tasks, including zero-shot image\nclassification. Recently, the use of prompts or adapters for efficient transfer\nlearning has gained significant attention for effectively adapting to\ndownstream tasks. However, the roles of vision and text prompts, as well as\nadapters in terms of generalization and transfer difficulty, have been\noverlooked, limiting performance on unseen tasks. In this paper, we empirically\nanalyze how VLMs behave when using vision and text prompts, adapters, and a\ncombination of these components, marking a novel exploration by our study. Our\nobservations find that utilizing vision prompts for class separability and text\nadapters for task adaptation is crucial for adaptability and generalizability.\nMoreover, to improve generalization across every domain, we propose an adaptive\nensemble method that effectively combines the general knowledge of VLMs with\ntask-specific knowledge according to transfer difficulty. Upon experimenting\nwith extensive benchmarks, our method consistently outperforms all baselines,\nparticularly on unseen tasks, demonstrating the effectiveness of our proposed\napproach.",
            "author": [
                "Yongjin Yang",
                "Jongwoo Ko",
                "Se-Young Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15569v1",
                "http://arxiv.org/pdf/2311.15569v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15566v1",
            "title": "SpotServe: Serving Generative Large Language Models on Preemptible\n  Instances",
            "updated": "2023-11-27T06:31:17Z",
            "published": "2023-11-27T06:31:17Z",
            "summary": "The high computational and memory requirements of generative large language\nmodels (LLMs) make it challenging to serve them cheaply. This paper aims to\nreduce the monetary cost for serving LLMs by leveraging preemptible GPU\ninstances on modern clouds, which offer accesses to spare GPUs at a much\ncheaper price than regular instances but may be preempted by the cloud at any\ntime. Serving LLMs on preemptible instances requires addressing challenges\ninduced by frequent instance preemptions and the necessity of migrating\ninstances to handle these preemptions.\n  This paper presents SpotServe, the first distributed LLM serving system on\npreemptible instances. Several key techniques in SpotServe realize fast and\nreliable serving of generative LLMs on cheap preemptible instances. First,\nSpotServe dynamically adapts the LLM parallelization configuration for dynamic\ninstance availability and fluctuating workload, while balancing the trade-off\namong the overall throughput, inference latency and monetary costs. Second, to\nminimize the cost of migrating instances for dynamic reparallelization, the\ntask of migrating instances is formulated as a bipartite graph matching\nproblem, which uses the Kuhn-Munkres algorithm to identify an optimal migration\nplan that minimizes communications. Finally, to take advantage of the grace\nperiod offered by modern clouds, we introduce stateful inference recovery, a\nnew inference mechanism that commits inference progress at a much finer\ngranularity and allows SpotServe to cheaply resume inference upon preemption.\nWe evaluate on real spot instance preemption traces and various popular LLMs\nand show that SpotServe can reduce the P99 tail latency by 2.4 - 9.1x compared\nwith the best existing LLM serving systems. We also show that SpotServe can\nleverage the price advantage of preemptive instances, saving 54% monetary cost\ncompared with only using on-demand instances.",
            "author": [
                "Xupeng Miao",
                "Chunan Shi",
                "Jiangfei Duan",
                "Xiaoli Xi",
                "Dahua Lin",
                "Bin Cui",
                "Zhihao Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15566v1",
                "http://arxiv.org/pdf/2311.15566v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15545v1",
            "title": "Out-of-Distribution Generalized Dynamic Graph Neural Network for Human\n  Albumin Prediction",
            "updated": "2023-11-27T05:21:08Z",
            "published": "2023-11-27T05:21:08Z",
            "summary": "Human albumin is essential for indicating the body's overall health.\nAccurately predicting plasma albumin levels and determining appropriate doses\nare urgent clinical challenges, particularly in critically ill patients, to\nmaintain optimal blood levels. However, human albumin prediction is non-trivial\nthat has to leverage the dynamics of biochemical markers as well as the\nexperience of treating patients. Moreover, the problem of distribution shift is\noften encountered in real clinical data, which may lead to a decline in the\nmodel prediction performance and reduce the reliability of the model's\napplication. In this paper, we propose a framework named Out-of-Distribution\nGeneralized Dynamic Graph Neural Network for Human Albumin Prediction\n(DyG-HAP), which is able to provide accurate albumin predictions for Intensity\nCare Unit (ICU) patients during hospitalization. We first model human albumin\nprediction as a dynamic graph regression problem to model the dynamics and\npatient relationship. Then, we propose a disentangled dynamic graph attention\nmechanism to capture and disentangle the patterns whose relationship to labels\nunder distribution shifts is invariant and variant respectively. Last, we\npropose an invariant dynamic graph regression method to encourage the model to\nrely on invariant patterns to make predictions. Moreover, we propose a dataset\nnamed Albumin level testing and nutritional dosing data for Intensive Care\n(ANIC) for evaluation. Extensive experiments demonstrate the superiority of our\nmethod compared to several baseline methods in human albumin prediction.",
            "author": [
                "Zeyang Zhang",
                "Xingwang Li",
                "Fei Teng",
                "Ning Lin",
                "Xueling Zhu",
                "Xin Wang",
                "Wenwu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15545v1",
                "http://arxiv.org/pdf/2311.15545v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15520v1",
            "title": "An algorithmic approach in constructing infinitely many even size graphs\n  with local antimagic chromatic number 3",
            "updated": "2023-11-27T03:46:28Z",
            "published": "2023-11-27T03:46:28Z",
            "summary": "An edge labeling of a connected graph $G = (V, E)$ is said to be local\nantimagic if it is a bijection $f:E \\to\\{1,\\ldots ,|E|\\}$ such that for any\npair of adjacent vertices $x$ and $y$, $f^+(x)\\not= f^+(y)$, where the induced\nvertex label $f^+(x)= \\sum f(e)$, with $e$ ranging over all the edges incident\nto $x$. The local antimagic chromatic number of $G$, denoted by $\\chi_{la}(G)$,\nis the minimum number of distinct induced vertex labels over all local\nantimagic labelings of $G$. In this paper, we first introduce an algorithmic\napproach to construct a family of infinitely many even size non-regular\ntripartite graphs with $t\\ge 1$ component(s) in which every component, called a\n{\\it Luv} graph, is of odd order $p\\ge 9$ and size $q=n(p+1)$ for $n\\ge 2$. We\nshow that every graph in this family has local antimagic chromatic number 3. We\nthen allowed the $m$-th component to have order $p_m\\ge 9$ and size\n$n_m(p_m+1)$ for $n_m\\ge 2, 1\\le m\\le t$. We also proved that every such graph\nwith all components having same order and size also has local antimagic\nchromatic number 3. Lastly, we constructed another family of infinitely many\ngraphs such that different components may have different order and size all of\nwhich having local antimagic chromatic number 3. Consequently, many other\nfamilies of (possibly disconnected) graphs with local antimagic chromatic\nnumber 3 are also constructed.",
            "author": [
                "Gee-Choon Lau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15520v1",
                "http://arxiv.org/pdf/2311.15520v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C78, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15512v1",
            "title": "Sparse Pedestrian Character Learning for Trajectory Prediction",
            "updated": "2023-11-27T03:15:48Z",
            "published": "2023-11-27T03:15:48Z",
            "summary": "Pedestrian trajectory prediction in a first-person view has recently\nattracted much attention due to its importance in autonomous driving. Recent\nwork utilizes pedestrian character information, \\textit{i.e.}, action and\nappearance, to improve the learned trajectory embedding and achieves\nstate-of-the-art performance. However, it neglects the invalid and negative\npedestrian character information, which is harmful to trajectory representation\nand thus leads to performance degradation. To address this issue, we present a\ntwo-stream sparse-character-based network~(TSNet) for pedestrian trajectory\nprediction. Specifically, TSNet learns the negative-removed characters in the\nsparse character representation stream to improve the trajectory embedding\nobtained in the trajectory representation stream. Moreover, to model the\nnegative-removed characters, we propose a novel sparse character graph,\nincluding the sparse category and sparse temporal character graphs, to learn\nthe different effects of various characters in category and temporal\ndimensions, respectively. Extensive experiments on two first-person view\ndatasets, PIE and JAAD, show that our method outperforms existing\nstate-of-the-art methods. In addition, ablation studies demonstrate different\neffects of various characters and prove that TSNet outperforms approaches\nwithout eliminating negative characters.",
            "author": [
                "Yonghao Dong",
                "Le Wang",
                "Sanpin Zhou",
                "Gang Hua",
                "Changyin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15512v1",
                "http://arxiv.org/pdf/2311.15512v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15501v1",
            "title": "Extremal results for $\\mathcal{K}^-_{r + 1}$-free signed graphs",
            "updated": "2023-11-27T02:56:02Z",
            "published": "2023-11-27T02:56:02Z",
            "summary": "This paper gives tight upper bounds on the number of edges and the index for\n$\\mathcal{K}^-_{r + 1}$-free unbalanced signed graphs, where $\\mathcal{K}^-_{r\n+ 1}$ is the set of $r+1$-vertices unbalanced signed complete graphs.\n  \\indent We first prove that if $\\Gamma$ is an $n$-vertices $\\mathcal{K}^-_{r\n+ 1}$-free unbalanced signed graph, then the number of edges of $\\Gamma$ is\n$$e(\\Gamma) \\leq \\frac{n(n-1)}{2} - (n - r ).$$\n  \\indent Let $\\Gamma_{1,r-2}$ be a signed graph obtained by adding one\nnegative edge and $r - 2$ positive edges between a vertex and an all positive\nsigned complete graph $K_{n - 1}$.\n  Secondly, we show that if $\\Gamma$ is an $n$-vertices $\\mathcal{K}^-_{r +\n1}$-free unbalanced signed graph, then the index of $\\Gamma$ is\n  $$\\lambda_{1}(\\Gamma) \\leq \\lambda_{1}(\\Gamma_{1,r-2}), $$ with equality\nholding if and only if $\\Gamma$ is switching equivalent to $\\Gamma_{1,r-2}$.\n  \\indent It is shown that these results are significant in extremal graph\ntheory.\n  Because they can be regarded as extensions of Tur{\\'a}n's Theorem [Math. Fiz.\nLapok 48 (1941) 436--452] and spectral Tur{\\'a}n problem [Linear Algebra Appl.\n428 (2008) 1492--1498] on signed graphs, respectively.\n  Furthermore, the second result partly resolves a recent open problem raised\nby Wang [arXiv preprint arXiv:2309.15434 (2023)].",
            "author": [
                "Zhuang Xiong",
                "Yaoping Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15501v1",
                "http://arxiv.org/pdf/2311.15501v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15493v1",
            "title": "UFIN: Universal Feature Interaction Network for Multi-Domain\n  Click-Through Rate Prediction",
            "updated": "2023-11-27T02:30:39Z",
            "published": "2023-11-27T02:30:39Z",
            "summary": "Click-Through Rate (CTR) prediction, which aims to estimate the probability\nof a user clicking on an item, is a key task in online advertising. Numerous\nexisting CTR models concentrate on modeling the feature interactions within a\nsolitary domain, thereby rendering them inadequate for fulfilling the\nrequisites of multi-domain recommendations in real industrial scenarios. Some\nrecent approaches propose intricate architectures to enhance knowledge sharing\nand augment model training across multiple domains. However, these approaches\nencounter difficulties when being transferred to new recommendation domains,\nowing to their reliance on the modeling of ID features (e.g., item id). To\naddress the above issue, we propose the Universal Feature Interaction Network\n(UFIN) approach for CTR prediction. UFIN exploits textual data to learn\nuniversal feature interactions that can be effectively transferred across\ndiverse domains. For learning universal feature representations, we regard the\ntext and feature as two different modalities and propose an encoder-decoder\nnetwork founded on a Large Language Model (LLM) to enforce the transfer of data\nfrom the text modality to the feature modality. Building upon the above\nfoundation, we further develop a mixtureof-experts (MoE) enhanced adaptive\nfeature interaction model to learn transferable collaborative patterns across\nmultiple domains. Furthermore, we propose a multi-domain knowledge distillation\nframework to enhance feature interaction learning. Based on the above methods,\nUFIN can effectively bridge the semantic gap to learn common knowledge across\nvarious domains, surpassing the constraints of ID-based models. Extensive\nexperiments conducted on eight datasets show the effectiveness of UFIN, in both\nmultidomain and cross-platform settings. Our code is available at\nhttps://github.com/RUCAIBox/UFIN.",
            "author": [
                "Zhen Tian",
                "Changwang Zhang",
                "Wayne Xin Zhao",
                "Xin Zhao",
                "Ji-Rong Wen",
                "Zhao Cao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15493v1",
                "http://arxiv.org/pdf/2311.15493v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16485v1",
            "title": "Class-Adaptive Sampling Policy for Efficient Continual Learning",
            "updated": "2023-11-27T02:17:14Z",
            "published": "2023-11-27T02:17:14Z",
            "summary": "Continual learning (CL) aims to acquire new knowledge while preserving\ninformation from previous experiences without forgetting. Though buffer-based\nmethods (i.e., retaining samples from previous tasks) have achieved acceptable\nperformance, determining how to allocate the buffer remains a critical\nchallenge. Most recent research focuses on refining these methods but often\nfails to sufficiently consider the varying influence of samples on the learning\nprocess, and frequently overlooks the complexity of the classes/concepts being\nlearned. Generally, these methods do not directly take into account the\ncontribution of individual classes. However, our investigation indicates that\nmore challenging classes necessitate preserving a larger number of samples\ncompared to less challenging ones. To address this issue, we propose a novel\nmethod and policy named 'Class-Adaptive Sampling Policy' (CASP), which\ndynamically allocates storage space within the buffer. By utilizing concepts of\nclass contribution and difficulty, CASP adaptively manages buffer space,\nallowing certain classes to occupy a larger portion of the buffer while\nreducing storage for others. This approach significantly improves the\nefficiency of knowledge retention and utilization. CASP provides a versatile\nsolution to boost the performance and efficiency of CL. It meets the demand for\ndynamic buffer allocation, accommodating the varying contributions of different\nclasses and their learning complexities over time.",
            "author": [
                "Hossein Rezaei",
                "Mohammad Sabokrou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16485v1",
                "http://arxiv.org/pdf/2311.16485v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15490v1",
            "title": "Optimizing and Fine-tuning Large Language Model for Urban Renewal",
            "updated": "2023-11-27T02:17:11Z",
            "published": "2023-11-27T02:17:11Z",
            "summary": "This study aims to innovatively explore adaptive applications of large\nlanguage models (LLM) in urban renewal. It also aims to improve its performance\nand text generation quality for knowledge question-answering (QA) tasks. Based\non the ChatGLM, we automatically generate QA datasets using urban renewal\nscientific literature corpora in a self-instruct manner and then conduct joint\nfine-tuning training on the model using the Prefix and LoRA fine-tuning methods\nto create an LLM for urban renewal. By guiding the LLM to automatically\ngenerate QA data based on prompt words and given text, it is possible to\nquickly obtain datasets in the urban renewal field and provide data support for\nthe fine-tuning training of LLMs. The experimental results show that the joint\nfine-tuning training method proposed in this study can significantly improve\nthe performance of LLM on the QA tasks. Compared with LoRA fine-tuning, the\nmethod improves the Bleu and Rouge metrics on the test by about 5%; compared\nwith the model before fine-tuning, the method improves the Bleu and Rouge\nmetrics by about 15%-20%. This study demonstrates the effectiveness and\nsuperiority of the joint fine-tuning method using Prefix and LoRA for ChatGLM\nin the urban renewal knowledge QA tasks. It provides a new approach for\nfine-tuning LLMs on urban renewal-related tasks.",
            "author": [
                "Xi Wang",
                "Xianyao Ling",
                "Tom Zhang",
                "Xuecao Li",
                "Shaolan Wang",
                "Zhixing Li",
                "Liang Zhang",
                "Peng Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15490v1",
                "http://arxiv.org/pdf/2311.15490v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15478v1",
            "title": "AerialBooth: Mutual Information Guidance for Text Controlled Aerial View\n  Synthesis from a Single Image",
            "updated": "2023-11-27T01:41:25Z",
            "published": "2023-11-27T01:41:25Z",
            "summary": "We present a novel method, AerialBooth, for synthesizing the aerial view from\na single input image using its text description. We leverage the pretrained\ntext-to-2D image stable diffusion model as prior knowledge of the 3D world. The\nmodel is finetuned in two steps to optimize for the text embedding and the UNet\nthat reconstruct the input image and its inverse perspective mapping\nrespectively. The inverse perspective mapping creates variance within the\ntext-image space of the diffusion model, while providing weak guidance for\naerial view synthesis. At inference, we steer the contents of the generated\nimage towards the input image using novel mutual information guidance that\nmaximizes the information content between the probability distributions of the\ntwo images. We evaluate our approach on a wide spectrum of real and synthetic\ndata, including natural scenes, indoor scenes, human action, etc. Through\nextensive experiments and ablation studies, we demonstrate the effectiveness of\nAerialBooth and also its generalizability to other text-controlled views. We\nalso show that AerialBooth achieves the best viewpoint-fidelity trade-off\nthough quantitative evaluation on 7 metrics analyzing viewpoint and fidelity\nw.r.t. input image. Code and data is available at\nhttps://github.com/divyakraman/AerialBooth2023.",
            "author": [
                "Divya Kothandaraman",
                "Tianyi Zhou",
                "Ming Lin",
                "Dinesh Manocha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15478v1",
                "http://arxiv.org/pdf/2311.15478v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15475v1",
            "title": "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers",
            "updated": "2023-11-27T01:20:11Z",
            "published": "2023-11-27T01:20:11Z",
            "summary": "We introduce MeshGPT, a new approach for generating triangle meshes that\nreflects the compactness typical of artist-created meshes, in contrast to dense\ntriangle meshes extracted by iso-surfacing methods from neural fields. Inspired\nby recent advances in powerful large language models, we adopt a sequence-based\napproach to autoregressively generate triangle meshes as sequences of\ntriangles. We first learn a vocabulary of latent quantized embeddings, using\ngraph convolutions, which inform these embeddings of the local mesh geometry\nand topology. These embeddings are sequenced and decoded into triangles by a\ndecoder, ensuring that they can effectively reconstruct the mesh. A transformer\nis then trained on this learned vocabulary to predict the index of the next\nembedding given previous embeddings. Once trained, our model can be\nautoregressively sampled to generate new triangle meshes, directly generating\ncompact meshes with sharp edges, more closely imitating the efficient\ntriangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable\nimprovement over state of the art mesh generation methods, with a 9% increase\nin shape coverage and a 30-point enhancement in FID scores across various\ncategories.",
            "author": [
                "Yawar Siddiqui",
                "Antonio Alliegro",
                "Alexey Artemov",
                "Tatiana Tommasi",
                "Daniele Sirigatti",
                "Vladislav Rosov",
                "Angela Dai",
                "Matthias Nie\u00dfner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15475v1",
                "http://arxiv.org/pdf/2311.15475v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15472v1",
            "title": "Unexpected Field Evaporation Sequence in $\u03b3$-TiAl",
            "updated": "2023-11-27T01:07:23Z",
            "published": "2023-11-27T01:07:23Z",
            "summary": "In atom probe tomography (APT), atoms from the surface of a needle shape\nspecimen are evaporated under a high electric field and analyzed via time of\nflight mass spectrometry and position sensitive detection. 3D reconstruction of\nthe atom positions follows a simple projection law, which can sometimes lead to\nartifacts due to deviation from an assumed ideal evaporation sequence. Here, we\nrevisit the evaporation behavior of [001]-oriented $\\gamma$-TiAl using a\nfull-dynamics simulation approach empowered by molecular dynamics. Without any\nknowledge of charge states or assumptions about evaporation fields, we\nsuccessfully reproduced the lack of distinct Al and Ti layers observed in\nreconstructions of experimental data which is traditionally attributed to the\nretention of Al on the evaporating surface. We further showed that a step-wise\nbond breaking process of Ti in contrast to the simultaneous bond breaking of Al\nexplains the seemingly counterintuitive preferential evaporation of the\nstrongly bonded Ti atoms.",
            "author": [
                "Jiayuwen Qi",
                "Fei Xue",
                "Emmanuelle Marquis",
                "Wolfgang Windl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15472v1",
                "http://arxiv.org/pdf/2311.15472v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15463v1",
            "title": "Where to Begin? From Random to Foundation Model Instructed\n  Initialization in Federated Learning for Medical Image Segmentation",
            "updated": "2023-11-27T00:29:10Z",
            "published": "2023-11-27T00:29:10Z",
            "summary": "In medical image analysis, Federated Learning (FL) stands out as a key\ntechnology that enables privacy-preserved, decentralized data processing,\ncrucial for handling sensitive medical data. Currently, most FL models employ\nrandom initialization, which has been proven effective in various instances.\nHowever, given the unique challenges posed by non-IID (independently and\nidentically distributed) data in FL, we propose a novel perspective: exploring\nthe impact of using the foundation model with enormous pre-trained knowledge,\nsuch as the Segment Anything Model (SAM), as an instructive teacher for FL\nmodel initialization in medical image segmentation task. This work for the\nfirst time attempts to utilize the foundation model as an instructive teacher\nfor initialization in FL, assessing its impact on the performance of FL models,\nespecially in non-IID data scenarios. Our empirical evaluation on chest x-ray\nlung segmentation showcases that FL with foundation model instructed\ninitialization not only achieves faster convergence but also improves\nperformance in complex data contexts. These findings offer a new perspective\nfor model initialization in FL.",
            "author": [
                "Ming Li",
                "Guang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15463v1",
                "http://arxiv.org/pdf/2311.15463v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15460v1",
            "title": "Privacy-Preserving Data Sharing in Agriculture: Enforcing Policy Rules\n  for Secure and Confidential Data Synthesis",
            "updated": "2023-11-27T00:12:47Z",
            "published": "2023-11-27T00:12:47Z",
            "summary": "Big Data empowers the farming community with the information needed to\noptimize resource usage, increase productivity, and enhance the sustainability\nof agricultural practices. The use of Big Data in farming requires the\ncollection and analysis of data from various sources such as sensors,\nsatellites, and farmer surveys. While Big Data can provide the farming\ncommunity with valuable insights and improve efficiency, there is significant\nconcern regarding the security of this data as well as the privacy of the\nparticipants. Privacy regulations, such as the EU GDPR, the EU Code of Conduct\non agricultural data sharing by contractual agreement, and the proposed EU AI\nlaw, have been created to address the issue of data privacy and provide\nspecific guidelines on when and how data can be shared between organizations.\nTo make confidential agricultural data widely available for Big Data analysis\nwithout violating the privacy of the data subjects, we consider\nprivacy-preserving methods of data sharing in agriculture. Deep learning-based\nsynthetic data generation has been proposed for privacy-preserving data\nsharing. However, there is a lack of compliance with documented data privacy\npolicies in such privacy-preserving efforts. In this study, we propose a novel\nframework for enforcing privacy policy rules in privacy-preserving data\ngeneration algorithms. We explore several available agricultural codes of\nconduct, extract knowledge related to the privacy constraints in data, and use\nthe extracted knowledge to define privacy bounds in a privacy-preserving\ngenerative model. We use our framework to generate synthetic agricultural data\nand present experimental results that demonstrate the utility of the synthetic\ndataset in downstream tasks. We also show that our framework can evade\npotential threats and secure data based on applicable regulatory policy rules.",
            "author": [
                "Anantaa Kotal",
                "Lavanya Elluri",
                "Deepti Gupta",
                "Varun Mandalapu",
                "Anupam Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15460v1",
                "http://arxiv.org/pdf/2311.15460v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2312.00045v1",
            "title": "AI-driven E-Liability Knowledge Graphs: A Comprehensive Framework for\n  Supply Chain Carbon Accounting and Emissions Liability Management",
            "updated": "2023-11-26T23:09:36Z",
            "published": "2023-11-26T23:09:36Z",
            "summary": "While carbon accounting plays a fundamental role in our fight against climate\nchange, it is not without its challenges. We begin the paper with a critique of\nthe conventional carbon accounting practices, after which we proceed to\nintroduce the E-liability carbon accounting methodology and Emissions Liability\nManagement (ELM) originally proposed by Kaplan and Ramanna, highlighting their\nstrengths. Recognizing the immense value of this novel approach for real-world\ncarbon accounting improvement, we introduce a novel data-driven integrative\nframework that leverages AI and computation - the E-Liability Knowledge Graph\nframework - to achieve real-world implementation of the E-liability carbon\naccounting methodology. In addition to providing a path-to-implementation, our\nproposed framework brings clarity to the complex environmental interactions\nwithin supply chains, thus enabling better informed and more responsible\ndecision-making. We analyze the implementation aspects of this framework and\nconclude with a discourse on the role of this AI-aided knowledge graph in\nensuring the transparency and decarbonization of global supply chains.",
            "author": [
                "Olamide Oladeji",
                "Seyed Shahabeddin Mousavi",
                "Marc Roston"
            ],
            "link": [
                "http://arxiv.org/abs/2312.00045v1",
                "http://arxiv.org/pdf/2312.00045v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15448v1",
            "title": "GGNNs : Generalizing GNNs using Residual Connections and Weighted\n  Message Passing",
            "updated": "2023-11-26T22:22:38Z",
            "published": "2023-11-26T22:22:38Z",
            "summary": "Many real-world phenomena can be modeled as a graph, making them extremely\nvaluable due to their ubiquitous presence. GNNs excel at capturing those\nrelationships and patterns within these graphs, enabling effective learning and\nprediction tasks. GNNs are constructed using Multi-Layer Perceptrons (MLPs) and\nincorporate additional layers for message passing to facilitate the flow of\nfeatures among nodes. It is commonly believed that the generalizing power of\nGNNs is attributed to the message-passing mechanism between layers, where nodes\nexchange information with their neighbors, enabling them to effectively capture\nand propagate information across the nodes of a graph. Our technique builds on\nthese results, modifying the message-passing mechanism further: one by weighing\nthe messages before accumulating at each node and another by adding Residual\nconnections. These two mechanisms show significant improvements in learning and\nfaster convergence",
            "author": [
                "Abhinav Raghuvanshi",
                "Kushal Sokke Malleshappa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15448v1",
                "http://arxiv.org/pdf/2311.15448v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15443v1",
            "title": "DCRA: A Distributed Chiplet-based Reconfigurable Architecture for\n  Irregular Applications",
            "updated": "2023-11-26T22:02:04Z",
            "published": "2023-11-26T22:02:04Z",
            "summary": "In recent years, the growing demand to process large graphs and sparse\ndatasets has led to increased research efforts to develop hardware- and\nsoftware-based architectural solutions to accelerate them. While some of these\napproaches achieve scalable parallelization with up to thousands of cores,\nadaptation of these proposals by the industry remained slow. To help solve this\ndissonance, we identified a set of questions and considerations that current\nresearch has not considered deeply. Starting from a tile-based architecture, we\nput forward a Distributed Chiplet-based Reconfigurable Architecture (DCRA) for\nirregular applications that carefully consider fabrication constraints that\nmade prior work either hard or costly to implement or too rigid to be applied.\nWe identify and study pre-silicon, package-time and compile-time configurations\nthat help optimize DCRA for different deployments and target metrics. To enable\nthat, we propose a practical path for manufacturing chip packages by composing\nvariable numbers of DCRA and memory dies, with a software-configurable Torus\nnetwork to connect them. We evaluate six applications and four datasets, with\nseveral configurations and memory technologies, to provide a detailed analysis\nof the performance, power, and cost of DCRA as a compute node for scale-out\nsparse data processing. Finally, we present our findings and discuss how DCRA,\ntogether with our framework for design exploration, can help guide architects\nto build scalable and cost-efficient systems for irregular applications.",
            "author": [
                "Marcelo Orenes-Vera",
                "Esin Tureci",
                "Margaret Martonosi",
                "David Wentzlaff"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15443v1",
                "http://arxiv.org/pdf/2311.15443v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15434v1",
            "title": "Structural Discovery with Partial Ordering Information for\n  Time-Dependent Data with Convergence Guarantees",
            "updated": "2023-11-26T21:26:38Z",
            "published": "2023-11-26T21:26:38Z",
            "summary": "Structural discovery amongst a set of variables is of interest in both static\nand dynamic settings. In the presence of lead-lag dependencies in the data, the\ndynamics of the system can be represented through a structural equation model\n(SEM) that simultaneously captures the contemporaneous and temporal\nrelationships amongst the variables, with the former encoded through a directed\nacyclic graph (DAG) for model identification. In many real applications, a\npartial ordering amongst the nodes of the DAG is available, which makes it\neither beneficial or imperative to incorporate it as a constraint in the\nproblem formulation. This paper develops an algorithm that can seamlessly\nincorporate a priori partial ordering information for solving a linear SEM\n(also known as Structural Vector Autoregression) under a high-dimensional\nsetting. The proposed algorithm is provably convergent to a stationary point,\nand exhibits competitive performance on both synthetic and real data sets.",
            "author": [
                "Jiahe Lin",
                "Huitian Lei",
                "George Michailidis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15434v1",
                "http://arxiv.org/pdf/2311.15434v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15421v1",
            "title": "Wired Perspectives: Multi-View Wire Art Embraces Generative AI",
            "updated": "2023-11-26T21:09:00Z",
            "published": "2023-11-26T21:09:00Z",
            "summary": "Creating multi-view wire art (MVWA), a static 3D sculpture with diverse\ninterpretations from different viewpoints, is a complex task even for skilled\nartists. In response, we present DreamWire, an AI system enabling everyone to\ncraft MVWA easily. Users express their vision through text prompts or\nscribbles, freeing them from intricate 3D wire organisation. Our approach\nsynergises 3D B\\'ezier curves, Prim's algorithm, and knowledge distillation\nfrom diffusion models or their variants (e.g., ControlNet). This blend enables\nthe system to represent 3D wire art, ensuring spatial continuity and overcoming\ndata scarcity. Extensive evaluation and analysis are conducted to shed insight\non the inner workings of the proposed system, including the trade-off between\nconnectivity and visual aesthetics.",
            "author": [
                "Zhiyu Qu",
                "Lan Yang",
                "Honggang Zhang",
                "Tao Xiang",
                "Kaiyue Pang",
                "Yi-Zhe Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15421v1",
                "http://arxiv.org/pdf/2311.15421v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15408v1",
            "title": "Techniques for learning sparse Pauli-Lindblad noise models",
            "updated": "2023-11-26T20:17:20Z",
            "published": "2023-11-26T20:17:20Z",
            "summary": "Error-mitigation techniques such as probabilistic error cancellation and\nzero-noise extrapolation benefit from accurate noise models. The sparse\nPauli-Lindblad noise model is one of the most successful models for those\napplications. In existing implementations, the model decomposes into a series\nof simple Pauli channels with one- and two-local terms that follow the qubit\ntopology. While the model has been shown to accurately capture the noise in\ncontemporary superconducting quantum processors for error mitigation, it is\nimportant to consider higher-weight terms and effects beyond nearest-neighbor\ninteractions. For such extended models to remain practical, however, we need to\nensure that they can be learned efficiently. In this work we present new\ntechniques that accomplish exactly this. We introduce twirling based on Pauli\nrotations, which enables us to automatically generate single-qubit learning\ncorrection sequences and reduce the number of unique fidelities that need to be\nlearned. In addition, we propose a basis-selection strategy that leverages\ngraph coloring and uniform covering arrays to minimize the number of learning\nbases. Taken together, these techniques ensure that the learning of the\nextended noise models remains efficient, despite their increased complexity.",
            "author": [
                "Ewout van den Berg",
                "Pawel Wocjan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15408v1",
                "http://arxiv.org/pdf/2311.15408v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15384v1",
            "title": "Robust and Automatic Data Clustering: Dirichlet Process meets\n  Median-of-Means",
            "updated": "2023-11-26T19:01:15Z",
            "published": "2023-11-26T19:01:15Z",
            "summary": "Clustering stands as one of the most prominent challenges within the realm of\nunsupervised machine learning. Among the array of centroid-based clustering\nalgorithms, the classic $k$-means algorithm, rooted in Lloyd's heuristic, takes\ncenter stage as one of the extensively employed techniques in the literature.\nNonetheless, both $k$-means and its variants grapple with noteworthy\nlimitations. These encompass a heavy reliance on initial cluster centroids,\nsusceptibility to converging into local minima of the objective function, and\nsensitivity to outliers and noise in the data. When confronted with data\ncontaining noisy or outlier-laden observations, the Median-of-Means (MoM)\nestimator emerges as a stabilizing force for any centroid-based clustering\nframework. On a different note, a prevalent constraint among existing\nclustering methodologies resides in the prerequisite knowledge of the number of\nclusters prior to analysis. Utilizing model-based methodologies, such as\nBayesian nonparametric models, offers the advantage of infinite mixture models,\nthereby circumventing the need for such requirements. Motivated by these facts,\nin this article, we present an efficient and automatic clustering technique by\nintegrating the principles of model-based and centroid-based methodologies that\nmitigates the effect of noise on the quality of clustering while ensuring that\nthe number of clusters need not be specified in advance. Statistical guarantees\non the upper bound of clustering error, and rigorous assessment through\nsimulated and real datasets suggest the advantages of our proposed method over\nexisting state-of-the-art clustering algorithms.",
            "author": [
                "Supratik Basu",
                "Jyotishka Ray Choudhury",
                "Debolina Paul",
                "Swagatam Das"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15384v1",
                "http://arxiv.org/pdf/2311.15384v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15382v1",
            "title": "Evaluating Multi-Global Server Architecture for Federated Learning",
            "updated": "2023-11-26T18:55:46Z",
            "published": "2023-11-26T18:55:46Z",
            "summary": "Federated learning (FL) with a single global server framework is currently a\npopular approach for training machine learning models on decentralized\nenvironment, such as mobile devices and edge devices. However, the centralized\nserver architecture poses a risk as any challenge on the central/global server\nwould result in the failure of the entire system. To minimize this risk, we\npropose a novel federated learning framework that leverages the deployment of\nmultiple global servers. We posit that implementing multiple global servers in\nfederated learning can enhance efficiency by capitalizing on local\ncollaborations and aggregating knowledge, and the error tolerance in regard to\ncommunication failure in the single server framework would be handled. We\ntherefore propose a novel framework that leverages the deployment of multiple\nglobal servers. We conducted a series of experiments using a dataset containing\nthe event history of electric vehicle (EV) charging at numerous stations. We\ndeployed a federated learning setup with multiple global servers and client\nservers, where each client-server strategically represented a different region\nand a global server was responsible for aggregating local updates from those\ndevices. Our preliminary results of the global models demonstrate that the\ndifference in performance attributed to multiple servers is less than 1%. While\nthe hypothesis of enhanced model efficiency was not as expected, the rule for\nhandling communication challenges added to the algorithm could resolve the\nerror tolerance issue. Future research can focus on identifying specific uses\nfor the deployment of multiple global servers.",
            "author": [
                "Asfia Kawnine",
                "Hung Cao",
                "Atah Nuh Mih",
                "Monica Wachowicz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15382v1",
                "http://arxiv.org/pdf/2311.15382v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15381v1",
            "title": "On universality of regular realizability problems",
            "updated": "2023-11-26T18:53:33Z",
            "published": "2023-11-26T18:53:33Z",
            "summary": "We prove universality of the regular realizability problems for several\nclasses of filters. The filters are descriptions of finite relations on the set\nof non-negative integers in the format proposed by P. Wolf and H. Fernau. The\nuniversality has proven up to reductions using NP-oracles. It corresponds to\nthe results of P. Wolf and H. Fernau about decidability of regular\nrealizability problems for many graph-theoretic properties.",
            "author": [
                "Alexander Rubtsov",
                "Michael Vyalyi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15381v1",
                "http://arxiv.org/pdf/2311.15381v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "68Q45",
                "F.4.3"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15361v1",
            "title": "Ultra-Range Gesture Recognition using an RGB Camera in Human-Robot\n  Interaction",
            "updated": "2023-11-26T17:27:26Z",
            "published": "2023-11-26T17:27:26Z",
            "summary": "Hand gestures play a significant role in human interactions where non-verbal\nintentions, thoughts and commands are conveyed. In Human-Robot Interaction\n(HRI), hand gestures offer a similar and efficient medium for conveying clear\nand rapid directives to a robotic agent. However, state-of-the-art vision-based\nmethods for gesture recognition have been shown to be effective only up to a\nuser-camera distance of seven meters. Such a short distance range limits\npractical HRI with, for example, service robots, search and rescue robots and\ndrones. In this work, we address the Ultra-Range Gesture Recognition (URGR)\nproblem by aiming for a recognition distance of up to 25 meters and in the\ncontext of HRI. We propose a novel deep-learning framework for URGR using\nsolely a simple RGB camera. First, a novel super-resolution model termed HQ-Net\nis used to enhance the low-resolution image of the user. Then, we propose a\nnovel URGR classifier termed Graph Vision Transformer (GViT) which takes the\nenhanced image as input. GViT combines the benefits of a Graph Convolutional\nNetwork (GCN) and a modified Vision Transformer (ViT). Evaluation of the\nproposed framework over diverse test data yields a high recognition rate of\n98.1%. The framework has also exhibited superior performance compared to human\nrecognition in ultra-range distances. With the framework, we analyze and\ndemonstrate the performance of an autonomous quadruped robot directed by human\ngestures in complex ultra-range indoor and outdoor environments.",
            "author": [
                "Eran Bamani",
                "Eden Nissinman",
                "Inbar Meir",
                "Lisa Koenigsberg",
                "Avishai Sintov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15361v1",
                "http://arxiv.org/pdf/2311.15361v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15351v1",
            "title": "Multi-Feeder Restoration using Multi-Microgrid Formation and Management",
            "updated": "2023-11-26T16:59:25Z",
            "published": "2023-11-26T16:59:25Z",
            "summary": "This papers highlights the benefit of coordinating resources on mulitple\nactive distribution feeders during severe long duration outages through\nmulti-microgrid formation. A graph-theory based multi-microgrid formation\nalgorithm is developed which is agnostic of the underlying energy management\nscheme of the microgrids and solved in a rolling horizon fashion. The algorithm\nis then enhanced to handle multiple feeders where formation of long laterals\nneeds to be avoided due to potential voltage control issues in distribution\nsystems. The algorithm is evaluated on a synthetic two feeder system derived\nfrom interconnecting two IEEE 123 node system. The results indicate increased\nservice to loads in the system and better utilization of renewable resources.",
            "author": [
                "Valliappan Muthukaruppan",
                "Rongxing Hu",
                "Ashwin Shirsat",
                "Mesut Baran",
                "Ning Lu",
                "Wenyuan Tang",
                "David Lubkeman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15351v1",
                "http://arxiv.org/pdf/2311.15351v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15346v1",
            "title": "A Primal-Dual Extension of the Goemans--Williamson Algorithm for the\n  Weighted Fractional Cut-Covering Problem",
            "updated": "2023-11-26T16:23:35Z",
            "published": "2023-11-26T16:23:35Z",
            "summary": "We study a weighted generalization of the fractional cut-covering problem,\nwhich we relate to the maximum cut problem via antiblocker and gauge duality.\nThis relationship allows us to introduce a semidefinite programming (SDP)\nrelaxation whose solutions may be rounded into fractional cut covers by\nsampling via the random hyperplane technique. We then provide a\n$1/\\alpha_{\\scriptscriptstyle \\mathrm{GW}}$-approximation algorithm for the\nweighted fractional cut-covering problem, where $\\alpha_{\\scriptscriptstyle\n\\mathrm{GW}} \\approx 0.878$ is the approximation factor of the celebrated\nGoemans--Williamson algorithm for the maximum cut problem. Nearly optimal\nsolutions of the SDPs in our duality framework allow one to consider instances\nof the maximum cut and the fractional cut-covering problems as primal-dual\npairs, where cuts and fractional cut covers simultaneously certify each other's\napproximation quality. We exploit this relationship to introduce new\ncombinatorial certificates for both problems, as well as a randomized\npolynomial-time algorithm for producing such certificates. In~particular,\nwe~show how the Goemans--Williamson algorithm implicitly approximates a\nweighted instance of the fractional cut-covering problem, and how our new\nalgorithm explicitly approximates a weighted instance of the maximum cut\nproblem. We conclude by discussing the role played by geometric representations\nof graphs in our results, and by proving our algorithms and analyses to be\noptimal in several aspects.",
            "author": [
                "Nathan Benedetto Proen\u00e7a",
                "Marcel K. de Carli Silva",
                "Cristiane M. Sato",
                "Levent Tun\u00e7el"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15346v1",
                "http://arxiv.org/pdf/2311.15346v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DM",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15334v1",
            "title": "The number of independent sets in bipartite graphs and benzenoids",
            "updated": "2023-11-26T15:39:26Z",
            "published": "2023-11-26T15:39:26Z",
            "summary": "Given a graph $G$, we study the number of independent sets in $G$, denoted\n$i(G)$. This parameter is known as both the Merrifield-Simmons index of a graph\nas well as the Fibonacci number of a graph. In this paper, we give general\nbounds for $i(G)$ when $G$ is bipartite and we give its exact value when $G$ is\na balanced caterpillar. We improve upon a known upper bound for $i(T)$ when $T$\nis a tree, and study a conjecture that all but finitely many positive integers\nrepresent $i(T)$ for some tree $T$. We also give exact values for $i(G)$ when\n$G$ is a particular type of benzenoid.",
            "author": [
                "Michael Han",
                "Sycamore Herlihy",
                "Kirsti Kuenzel",
                "Daniel Martin",
                "Rachel Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15334v1",
                "http://arxiv.org/pdf/2311.15334v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C05, 05C69"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15321v1",
            "title": "Maxima of the index: forbidden unbalanced cycles",
            "updated": "2023-11-26T14:46:38Z",
            "published": "2023-11-26T14:46:38Z",
            "summary": "This paper aims to address the problem: what is the maximum index among all\n$\\mathcal{C}^-_r$-free unbalanced signed graphs, where $\\mathcal{C}^-_r$ is the\nset of unbalanced cycle of length $r$.\n  Let $\\Gamma_1 = C_3^- \\bullet K_{n-2}$ be a signed graph obtained by\nidentifying a vertex of $K_{n-2}$ with a vertex of $C_3^-$ whose two incident\nedges in $C_3^-$ are all positive, where $C_3^-$ is an unbalanced triangle with\none negative edge.\n  It is shown that if $\\Gamma$ is an unbalanced signed graph of order $n$, $r$\nis an integer in $\\{4, \\cdots, \\lfloor \\frac{n}{3}\\rfloor + 1 \\}$, and\n  $$\\lambda_{1}(\\Gamma) \\geq \\lambda_{1}(\\Gamma_1), $$ then $\\Gamma$ contains\nan unbalanced cycle of length $r$, unless $\\Gamma \\sim \\Gamma_1$.\n  \\indent It is shown that the result are significant in spectral extremal\ngraph problems.\n  Because they can be regarded as a extension of the spectral Tur{\\'a}n problem\nfor cycles [Linear Algebra Appl. 428 (2008) 1492--1498] in the context of\nsigned graphs.\n  Furthermore, our result partly resolved a recent open problem raised by Lin\nand Wang [arXiv preprint arXiv:2309.04101 (2023)].",
            "author": [
                "Zhuang Xiong",
                "Yaoping Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15321v1",
                "http://arxiv.org/pdf/2311.15321v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15317v1",
            "title": "Generalized Graph Prompt: Toward a Unification of Pre-Training and\n  Downstream Tasks on Graphs",
            "updated": "2023-11-26T14:35:28Z",
            "published": "2023-11-26T14:35:28Z",
            "summary": "Graph neural networks have emerged as a powerful tool for graph\nrepresentation learning, but their performance heavily relies on abundant\ntask-specific supervision. To reduce labeling requirement, the \"pre-train,\nprompt\" paradigms have become increasingly common. However, existing study of\nprompting on graphs is limited, lacking a universal treatment to appeal to\ndifferent downstream tasks. In this paper, we propose GraphPrompt, a novel\npre-training and prompting framework on graphs. GraphPrompt not only unifies\npre-training and downstream tasks into a common task template but also employs\na learnable prompt to assist a downstream task in locating the most relevant\nknowledge from the pre-trained model in a task-specific manner. To further\nenhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with\ntwo major enhancements. First, we generalize several popular graph pre-training\ntasks beyond simple link prediction to broaden the compatibility with our task\ntemplate. Second, we propose a more generalized prompt design that incorporates\na series of prompt vectors within every layer of the pre-trained graph encoder,\nin order to capitalize on the hierarchical information across different layers\nbeyond just the readout layer. Finally, we conduct extensive experiments on\nfive public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.",
            "author": [
                "Xingtong Yu",
                "Zhenghao Liu",
                "Yuan Fang",
                "Zemin Liu",
                "Sihong Chen",
                "Xinming Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15317v1",
                "http://arxiv.org/pdf/2311.15317v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15316v1",
            "title": "Enhancing Empathetic and Emotion Support Dialogue Generation with\n  Prophetic Commonsense Inference",
            "updated": "2023-11-26T14:35:23Z",
            "published": "2023-11-26T14:35:23Z",
            "summary": "The interest in Empathetic and Emotional Support conversations among the\npublic has significantly increased. To offer more sensitive and understanding\nresponses, leveraging commonsense knowledge has become a common strategy to\nbetter understand psychological aspects and causality. However, such\ncommonsense inferences can be out of context and unable to predict upcoming\ndialogue themes, resulting in responses that lack coherence and empathy. To\nremedy this issue, we present Prophetic Commonsense Inference, an innovative\nparadigm for inferring commonsense knowledge. By harnessing the capabilities of\nLarge Language Models in understanding dialogue and making commonsense\ndeductions, we train tunable models to bridge the gap between past and\npotential future dialogues. Extensive experiments conducted on\nEmpatheticDialogues and Emotion Support Conversation show that equipping\ndialogue agents with our proposed prophetic commonsense inference significantly\nenhances the quality of their responses.",
            "author": [
                "Lanrui Wang",
                "Jiangnan Li",
                "Chenxu Yang",
                "Zheng Lin",
                "Weiping Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15316v1",
                "http://arxiv.org/pdf/2311.15316v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15313v1",
            "title": "Low-Complexity Joint Beamforming for RIS-Assisted MU-MISO Systems Based\n  on Model-Driven Deep Learning",
            "updated": "2023-11-26T14:24:26Z",
            "published": "2023-11-26T14:24:26Z",
            "summary": "Reconfigurable intelligent surfaces (RIS) can improve signal propagation\nenvironments by adjusting the phase of the incident signal. However, optimizing\nthe phase shifts jointly with the beamforming vector at the access point is\nchallenging due to the non-convex objective function and constraints. In this\nstudy, we propose an algorithm based on weighted minimum mean square error\noptimization and power iteration to maximize the weighted sum rate (WSR) of a\nRIS-assisted downlink multi-user multiple-input single-output system. To\nfurther improve performance, a model-driven deep learning (DL) approach is\ndesigned, where trainable variables and graph neural networks are introduced to\naccelerate the convergence of the proposed algorithm. We also extend the\nproposed method to include beamforming with imperfect channel state information\nand derive a two-timescale stochastic optimization algorithm. Simulation\nresults show that the proposed algorithm outperforms state-of-the-art\nalgorithms in terms of complexity and WSR. Specifically, the model-driven DL\napproach has a runtime that is approximately 3% of the state-of-the-art\nalgorithm to achieve the same performance. Additionally, the proposed algorithm\nwith 2-bit phase shifters outperforms the compared algorithm with continuous\nphase shift.",
            "author": [
                "Weijie Jin",
                "Jing Zhang",
                "Chao-Kai Wen",
                "Shi Jin",
                "Xiao Li",
                "Shuangfeng Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15313v1",
                "http://arxiv.org/pdf/2311.15313v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15310v1",
            "title": "Secure and Verifiable Data Collaboration with Low-Cost Zero-Knowledge\n  Proofs",
            "updated": "2023-11-26T14:19:46Z",
            "published": "2023-11-26T14:19:46Z",
            "summary": "Organizations are increasingly recognizing the value of data collaboration\nfor data analytics purposes. Yet, stringent data protection laws prohibit the\ndirect exchange of raw data. To facilitate data collaboration, federated\nLearning (FL) emerges as a viable solution, which enables multiple clients to\ncollaboratively train a machine learning (ML) model under the supervision of a\ncentral server while ensuring the confidentiality of their raw data. However,\nexisting studies have unveiled two main risks: (i) the potential for the server\nto infer sensitive information from the client's uploaded updates (i.e., model\ngradients), compromising client input privacy, and (ii) the risk of malicious\nclients uploading malformed updates to poison the FL model, compromising input\nintegrity. Recent works utilize secure aggregation with zero-knowledge proofs\n(ZKP) to guarantee input privacy and integrity in FL. Nevertheless, they suffer\nfrom extremely low efficiency and, thus, are impractical for real deployment.\nIn this paper, we propose a novel and highly efficient solution RiseFL for\nsecure and verifiable data collaboration, ensuring input privacy and integrity\nsimultaneously.Firstly, we devise a probabilistic integrity check method that\nsignificantly reduces the cost of ZKP generation and verification. Secondly, we\ndesign a hybrid commitment scheme to satisfy Byzantine robustness with improved\nperformance. Thirdly, we theoretically prove the security guarantee of the\nproposed solution. Extensive experiments on synthetic and real-world datasets\nsuggest that our solution is effective and is highly efficient in both client\ncomputation and communication. For instance, RiseFL is up to 28x, 53x and 164x\nfaster than three state-of-the-art baselines ACORN, RoFL and EIFFeL for the\nclient computation.",
            "author": [
                "Yizheng Zhu",
                "Yuncheng Wu",
                "Zhaojing Luo",
                "Beng Chin Ooi",
                "Xiaokui Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15310v1",
                "http://arxiv.org/pdf/2311.15310v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.DB",
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16193v1",
            "title": "Students' interest in knowledge acquisition in Artificial Intelligence",
            "updated": "2023-11-26T14:13:53Z",
            "published": "2023-11-26T14:13:53Z",
            "summary": "Some students' expectations and points of view related to the Artificial\nIntelligence course are explored and analyzed in this study. We anonymous\ncollected answers from 58 undergraduate students out of 200 enrolled in the\nComputer Science specialization. The answers were analysed and interpreted\nusing thematic analysis to find out their interests and attractive and\nunattractive aspects related to the Artificial Intelligence study topic. We\nconcluded that students are interested in Artificial Intelligence due to its\ntrendiness, applicability, their passion and interest in the subject, the\npotential for future growth, and high salaries. However, the students'\nexpectations were mainly related to achieving medium knowledge in the\nArtificial Intelligence field, and men seem to be more interested in acquiring\nhigh-level skills than women. The most common part that wasn't enjoyed by the\nstudents was the mathematical aspect used in Artificial Intelligence. Some of\nthem (a small group) were also aware of the Artificial Intelligence potential\nwhich could be used in an unethical manner for negative purposes. Our study\nalso provides a short comparison to the Databases course, in which students\nwere not that passionate or interested in achieving medium knowledge, their\ninterest was related to DB usage and basic information.",
            "author": [
                "Manuela-Andreea Petrescu",
                "Emilia-Loredana Pop",
                "Tudor-Dan Mihoc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16193v1",
                "http://arxiv.org/pdf/2311.16193v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.HC",
                "cs.SE",
                "I.2.6; K.3.2; D.m"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15303v1",
            "title": "Concept Distillation: Leveraging Human-Centered Explanations for Model\n  Improvement",
            "updated": "2023-11-26T14:00:14Z",
            "published": "2023-11-26T14:00:14Z",
            "summary": "Humans use abstract concepts for understanding instead of hard features.\nRecent interpretability research has focused on human-centered concept\nexplanations of neural networks. Concept Activation Vectors (CAVs) estimate a\nmodel's sensitivity and possible biases to a given concept. In this paper, we\nextend CAVs from post-hoc analysis to ante-hoc training in order to reduce\nmodel bias through fine-tuning using an additional Concept Loss. Concepts were\ndefined on the final layer of the network in the past. We generalize it to\nintermediate layers using class prototypes. This facilitates class learning in\nthe last convolution layer, which is known to be most informative. We also\nintroduce Concept Distillation to create richer concepts using a pre-trained\nknowledgeable model as the teacher. Our method can sensitize or desensitize a\nmodel towards concepts. We show applications of concept-sensitive training to\ndebias several classification problems. We also use concepts to induce prior\nknowledge into IID, a reconstruction problem. Concept-sensitive training can\nimprove model interpretability, reduce biases, and induce prior knowledge.\nPlease visit https://avani17101.github.io/Concept-Distilllation/ for code and\nmore details.",
            "author": [
                "Avani Gupta",
                "Saurabh Saini",
                "P J Narayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15303v1",
                "http://arxiv.org/pdf/2311.15303v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15289v1",
            "title": "Counting cliques without generalized theta graphs",
            "updated": "2023-11-26T13:09:04Z",
            "published": "2023-11-26T13:09:04Z",
            "summary": "The \\textit{generalized Tur\\'an number} $\\mathrm{ex}(n, T, F)$ is the maximum\npossible number of copies of $T$ in an $F$-free graph on $n$ vertices for any\ntwo graphs $T$ and $F$. For the book graph $B_t$, there is a close connection\nbetween $\\ex(n,K_3,B_t)$ and the Ruzsa-Szemer\\'edi triangle removal lemma.\nMotivated by this, in this paper, we study the generalized Tur\\'an problem for\ngeneralized theta graphs, a natural extension of book graphs. Our main result\nprovides a complete characterization of the magnitude of $\\ex(n,K_3,H)$ when\n$H$ is a generalized theta graph, indicating when it is quadratic, when it is\nnearly quadratic, and when it is subquadratic. Furthermore, as an application,\nwe obtain the exact value of $\\ex(n, K_r, kF)$, where $F$ is an edge-critical\ngeneralized theta graph, and $3\\le r\\le k+1$, extending several recent results.",
            "author": [
                "Jun Gao",
                "Zhuo Wu",
                "Yisai Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15289v1",
                "http://arxiv.org/pdf/2311.15289v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15284v1",
            "title": "A Frequency-Domain Version of Willems' Fundamental Lemma",
            "updated": "2023-11-26T12:56:59Z",
            "published": "2023-11-26T12:56:59Z",
            "summary": "Willems' fundamental lemma has recently received an impressive amount of\nattention in the (data-driven) control community. In this paper, we formulate a\nfrequency-domain equivalent of this lemma. In doing so, we bridge the gap\nbetween recent developments in data-driven analysis and control and the\nextensive knowledge on non-parametric frequency-domain identification that has\naccumulated, particularly in industry, through decades of working with\nclassical (frequency-domain) control and identification techniques. Our\nformulation also allows for the combination of multiple data sets in the sense\nthat, in the data, multiple input directions may be excited at the same\nfrequency. We also illustrate the usefulness of our results by demonstrating\nhow they can be applied to perform frequency-domain-data-driven simulation.",
            "author": [
                "T. J. Meijer",
                "S. A. N. Nouwens",
                "V. S. Dolk",
                "W. P. M. H. Heemels"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15284v1",
                "http://arxiv.org/pdf/2311.15284v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15276v1",
            "title": "Efficient Rehearsal Free Zero Forgetting Continual Learning using\n  Adaptive Weight Modulation",
            "updated": "2023-11-26T12:36:05Z",
            "published": "2023-11-26T12:36:05Z",
            "summary": "Artificial neural networks encounter a notable challenge known as continual\nlearning, which involves acquiring knowledge of multiple tasks over an extended\nperiod. This challenge arises due to the tendency of previously learned weights\nto be adjusted to suit the objectives of new tasks, resulting in a phenomenon\ncalled catastrophic forgetting. Most approaches to this problem seek a balance\nbetween maximizing performance on the new tasks and minimizing the forgetting\nof previous tasks. In contrast, our approach attempts to maximize the\nperformance of the new task, while ensuring zero forgetting. This is\naccomplished by creating a task-specific modulation parameters for each task.\nOnly these would be learnable parameters during learning of consecutive tasks.\nThrough comprehensive experimental evaluations, our model demonstrates superior\nperformance in acquiring and retaining novel tasks that pose difficulties for\nother multi-task models. This emphasizes the efficacy of our approach in\npreventing catastrophic forgetting while accommodating the acquisition of new\ntasks",
            "author": [
                "Yonatan Sverdlov",
                "Shimon Ullman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15276v1",
                "http://arxiv.org/pdf/2311.15276v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15268v1",
            "title": "Unlearning via Sparse Representations",
            "updated": "2023-11-26T11:12:30Z",
            "published": "2023-11-26T11:12:30Z",
            "summary": "Machine \\emph{unlearning}, which involves erasing knowledge about a\n\\emph{forget set} from a trained model, can prove to be costly and infeasible\nby existing techniques. We propose a nearly compute-free zero-shot unlearning\ntechnique based on a discrete representational bottleneck. We show that the\nproposed technique efficiently unlearns the forget set and incurs negligible\ndamage to the model's performance on the rest of the data set. We evaluate the\nproposed technique on the problem of \\textit{class unlearning} using three\ndatasets: CIFAR-10, CIFAR-100, and LACUNA-100. We compare the proposed\ntechnique to SCRUB, a state-of-the-art approach which uses knowledge\ndistillation for unlearning. Across all three datasets, the proposed technique\nperforms as well as, if not better than SCRUB while incurring almost no\ncomputational cost.",
            "author": [
                "Vedant Shah",
                "Frederik Tr\u00e4uble",
                "Ashish Malik",
                "Hugo Larochelle",
                "Michael Mozer",
                "Sanjeev Arora",
                "Yoshua Bengio",
                "Anirudh Goyal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15268v1",
                "http://arxiv.org/pdf/2311.15268v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15262v1",
            "title": "Revealing Cortical Layers In Histological Brain Images With\n  Self-Supervised Graph Convolutional Networks Applied To Cell-Graphs",
            "updated": "2023-11-26T10:33:36Z",
            "published": "2023-11-26T10:33:36Z",
            "summary": "Identifying cerebral cortex layers is crucial for comparative studies of the\ncytoarchitecture aiming at providing insights into the relations between brain\nstructure and function across species. The absence of extensive annotated\ndatasets typically limits the adoption of machine learning approaches, leading\nto the manual delineation of cortical layers by neuroanatomists. We introduce a\nself-supervised approach to detect layers in 2D Nissl-stained histological\nslices of the cerebral cortex. It starts with the segmentation of individual\ncells and the creation of an attributed cell-graph. A self-supervised graph\nconvolutional network generates cell embeddings that encode morphological and\nstructural traits of the cellular environment and are exploited by a community\ndetection algorithm for the final layering. Our method, the first\nself-supervised of its kind with no spatial transcriptomics data involved,\nholds the potential to accelerate cytoarchitecture analyses, sidestepping\nannotation needs and advancing cross-species investigation.",
            "author": [
                "Valentina Vadori",
                "Antonella Peruffo",
                "Jean-Marie Gra\u00efc",
                "Giulia Vadori",
                "Livio Finos",
                "Enrico Grisan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15262v1",
                "http://arxiv.org/pdf/2311.15262v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15255v1",
            "title": "Well-covered Unitary Cayley Graphs of Matrix Rings over Finite Fields\n  and Applications",
            "updated": "2023-11-26T09:58:08Z",
            "published": "2023-11-26T09:58:08Z",
            "summary": "Suppose that $F$ is a finite field and $R=M_n(F)$ is the ring of $n$-square\nmatrices over $F$. Here we characterize when the Cayley graph of the additive\ngroup of $R$ with respect to the set of invertible elements of $R$, called the\nunitary Cayley graph of $R$, is well-covered. Then we apply this to\ncharacterize all finite rings with identity whose unitary Cayley graph is\nwell-covered or Cohen-Macaulay.",
            "author": [
                "Shahin Rahimi",
                "Ashkan Nikseresht"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15255v1",
                "http://arxiv.org/pdf/2311.15255v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AC",
                "math.RA",
                "05C25, 05C69, 15B33, 13H10"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15251v1",
            "title": "Should I use metaverse or not? An investigation of university students\n  behavioral intention to use MetaEducation technology",
            "updated": "2023-11-26T09:45:34Z",
            "published": "2023-11-26T09:45:34Z",
            "summary": "Metaverse, a burgeoning technological trend that combines virtual and\naugmented reality, provides users with a fully digital environment where they\ncan assume a virtual identity through a digital avatar and interact with others\nas they were in the real world. Its applications span diverse domains such as\neconomy (with its entry into the cryptocurrency field), finance, social life,\nworking environment, healthcare, real estate, and education. During the\nCOVID-19 and post-COVID-19 era, universities have rapidly adopted e-learning\ntechnologies to provide students with online access to learning content and\nplatforms, rendering previous considerations on integrating such technologies\nor preparing institutional infrastructures virtually obsolete. In light of this\ncontext, the present study proposes a framework for analyzing university\nstudents' acceptance and intention to use metaverse technologies in education,\ndrawing upon the Technology Acceptance Model (TAM). The study aims to\ninvestigate the relationship between students' intention to use metaverse\ntechnologies in education, hereafter referred to as MetaEducation, and selected\nTAM constructs, including Attitude, Perceived Usefulness, Perceived Ease of\nUse, Self-efficacy of metaverse technologies in education, and Subjective Norm.\nNotably, Self-efficacy and Subjective Norm have a positive influence on\nAttitude and Perceived Usefulness, whereas Perceived Ease of Use does not\nexhibit a strong correlation with Attitude or Perceived Usefulness. The authors\npostulate that the weak associations between the study's constructs may be\nattributed to limited knowledge regarding MetaEducation and its potential\nbenefits. Further investigation and analysis of the study's proposed model are\nwarranted to comprehensively understand the complex dynamics involved in the\nacceptance and utilization of MetaEducation technologies in the realm of higher\neducation",
            "author": [
                "Nikolaos Misirlis",
                "Yiannis Nikolaidis",
                "Anna Sabidussi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15251v1",
                "http://arxiv.org/pdf/2311.15251v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15249v1",
            "title": "Algorithm Evolution Using Large Language Model",
            "updated": "2023-11-26T09:38:44Z",
            "published": "2023-11-26T09:38:44Z",
            "summary": "Optimization can be found in many real-life applications. Designing an\neffective algorithm for a specific optimization problem typically requires a\ntedious amount of effort from human experts with domain knowledge and algorithm\ndesign skills. In this paper, we propose a novel approach called Algorithm\nEvolution using Large Language Model (AEL). It utilizes a large language model\n(LLM) to automatically generate optimization algorithms via an evolutionary\nframework. AEL does algorithm-level evolution without model training. Human\neffort and requirements for domain knowledge can be significantly reduced. We\ntake constructive methods for the salesman traveling problem as a test example,\nwe show that the constructive algorithm obtained by AEL outperforms simple\nhand-crafted and LLM-generated heuristics. Compared with other domain deep\nlearning model-based algorithms, these methods exhibit excellent scalability\nacross different problem sizes. AEL is also very different from previous\nattempts that utilize LLMs as search operators in algorithms.",
            "author": [
                "Fei Liu",
                "Xialiang Tong",
                "Mingxuan Yuan",
                "Qingfu Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15249v1",
                "http://arxiv.org/pdf/2311.15249v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15246v1",
            "title": "Thermodynamic Response and Neutral Excitations in Integer and Fractional\n  Quantum Anomalous Hall States Emerging from Correlated Flat Bands",
            "updated": "2023-11-26T09:28:49Z",
            "published": "2023-11-26T09:28:49Z",
            "summary": "Integer and fractional Chern insulators have been extensively explored in\ncorrelated flat band models. Recently, the prediction and experimental\nobservation of fractional quantum anomalous Hall (FQAH) states with spontaneous\ntime-reversal-symmetry breaking have garnered attention. While the\nthermodynamics of integer quantum anomalous Hall (IQAH) states have been\nsystematically studied based on models of twisted bilayer graphene, our\ntheoretical knowledge on finite-temperature thermodynamic properties of FQAH\nstates has been severely limited. In this study, we delve into the\nthermodynamic response and collective excitations of both IQAH and FQAH states\nwithin the paradigmatic correlated flat band model on the checkerboard lattice.\nOur key findings include: i) In both $\\nu = 1$ IQAH and $\\nu = 1/3$ FQAH\nstates, it is the charge-neutral collective excitations that determine the\nonset temperature of these topological states, a value significantly smaller\nthan the charge gap, aligning with recent experimental observations; ii) The\nlowest collective excitations manifest as the zero-momentum excitons in the\nIQAH state, whereas in the FQAH state, they take the form of magneto-rotons\nwith finite momentum; iii) The excitons and rotons exhibit distinct\nexperimental signatures, which we propose to detect in future experiments.",
            "author": [
                "Hongyu Lu",
                "Bin-Bin Chen",
                "Han-Qing Wu",
                "Kai Sun",
                "Zi Yang Meng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15246v1",
                "http://arxiv.org/pdf/2311.15246v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15231v1",
            "title": "Double Reverse Regularization Network Based on Self-Knowledge\n  Distillation for SAR Object Classification",
            "updated": "2023-11-26T08:09:43Z",
            "published": "2023-11-26T08:09:43Z",
            "summary": "In current synthetic aperture radar (SAR) object classification, one of the\nmajor challenges is the severe overfitting issue due to the limited dataset\n(few-shot) and noisy data. Considering the advantages of knowledge distillation\nas a learned label smoothing regularization, this paper proposes a novel Double\nReverse Regularization Network based on Self-Knowledge Distillation\n(DRRNet-SKD). Specifically, through exploring the effect of distillation weight\non the process of distillation, we are inspired to adopt the double reverse\nthought to implement an effective regularization network by combining offline\nand online distillation in a complementary way. Then, the Adaptive Weight\nAssignment (AWA) module is designed to adaptively assign two reverse-changing\nweights based on the network performance, allowing the student network to\nbetter benefit from both teachers. The experimental results on OpenSARShip and\nFUSAR-Ship demonstrate that DRRNet-SKD exhibits remarkable performance\nimprovement on classical CNNs, outperforming state-of-the-art self-knowledge\ndistillation methods.",
            "author": [
                "Bo Xu",
                "Hao Zheng",
                "Zhigang Hu",
                "Liu Yang",
                "Meiguang Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15231v1",
                "http://arxiv.org/pdf/2311.15231v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15226v1",
            "title": "Ring Bose-Einstein condensate in a cavity: Chirality Detection and\n  Rotation Sensing",
            "updated": "2023-11-26T07:44:24Z",
            "published": "2023-11-26T07:44:24Z",
            "summary": "Recently, a method has been proposed to detect the rotation of a ring\nBose-Einstein condensate, in situ, in real-time and with minimal destruction,\nusing a cavity driven with optical fields carrying orbital angular momentum.\nThis method is sensitive to the magnitude of the condensate winding number but\nnot its sign. In the present work, we consider simulations of the rotation of\nthe angular lattice formed by the optical fields and show that the resulting\ncavity transmission spectra are sensitive to the sign of the condensate winding\nnumber. We demonstrate the minimally destructive technique on persistent\ncurrent rotational eigenstates, counter-rotating superpositions, and a soliton\nsingly or in collision with a second soliton. Conversely, we also investigate\nthe sensitivity of the ring condensate, given knowledge of its winding number,\nto the rotation of the optical lattice. This characterizes the effectiveness of\nthe optomechanical configuration as a laboratory rotation sensor. Our results\nare important to studies of rotating ring condensates used in atomtronics,\nsuperfluid hydrodynamics, simulation of topological defects and cosmological\ntheories, interferometry using matter-wave solitons, and optomechanical\nsensing.",
            "author": [
                "Nalinikanta Pradhan",
                "Pardeep Kumar",
                "Rina Kanamoto",
                "Tarak Nath Dey",
                "M. Bhattacharya",
                "Pankaj Kumar Mishra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15226v1",
                "http://arxiv.org/pdf/2311.15226v1"
            ],
            "primary_category": "cond-mat.quant-gas",
            "category": [
                "cond-mat.quant-gas"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15223v1",
            "title": "Sufficient conditions for matching extendability in terms of the\n  Zeroth-order General Randi\u0107 Index",
            "updated": "2023-11-26T07:29:42Z",
            "published": "2023-11-26T07:29:42Z",
            "summary": "Topological indices are important bridge between graph theory and chemical\napplications. In this paper, we will provide some sufficient conditions for a\ngraph to be $k$-extendable graphs, $n$-factor-critical graphs and\n$(n,k,d)$-graphs in terms of a significant type of Topological index: the\nZeroth-order General Randi\\'c Index ($\\alpha\\ge-1$ and $\\alpha\\neq 0$).",
            "author": [
                "Shuai Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15223v1",
                "http://arxiv.org/pdf/2311.15223v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15216v1",
            "title": "Solve Large-scale Unit Commitment Problems by Physics-informed Graph\n  Learning",
            "updated": "2023-11-26T07:17:45Z",
            "published": "2023-11-26T07:17:45Z",
            "summary": "Unit commitment (UC) problems are typically formulated as mixed-integer\nprograms (MIP) and solved by the branch-and-bound (B&B) scheme. The recent\nadvances in graph neural networks (GNN) enable it to enhance the B&B algorithm\nin modern MIP solvers by learning to dive and branch. Existing GNN models that\ntackle MIP problems are mostly constructed from mathematical formulation, which\nis computationally expensive when dealing with large-scale UC problems. In this\npaper, we propose a physics-informed hierarchical graph convolutional network\n(PI-GCN) for neural diving that leverages the underlying features of various\ncomponents of power systems to find high-quality variable assignments.\nFurthermore, we adopt the MIP model-based graph convolutional network (MB-GCN)\nfor neural branching to select the optimal variables for branching at each node\nof the B&B tree. Finally, we integrate neural diving and neural branching into\na modern MIP solver to establish a novel neural MIP solver designed for\nlarge-scale UC problems. Numeral studies show that PI-GCN has better\nperformance and scalability than the baseline MB-GCN on neural diving.\nMoreover, the neural MIP solver yields the lowest operational cost and\noutperforms a modern MIP solver for all testing days after combining it with\nour proposed neural diving model and the baseline neural branching model.",
            "author": [
                "Jingtao Qin",
                "Nanpeng Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15216v1",
                "http://arxiv.org/pdf/2311.15216v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15213v1",
            "title": "Leveraging Anatomical Constraints with Uncertainty for Pneumothorax\n  Segmentation",
            "updated": "2023-11-26T07:03:17Z",
            "published": "2023-11-26T07:03:17Z",
            "summary": "Pneumothorax is a medical emergency caused by abnormal accumulation of air in\nthe pleural space - the potential space between the lungs and chest wall. On 2D\nchest radiographs, pneumothorax occurs within the thoracic cavity and outside\nof the mediastinum and we refer to this area as \"lung+ space\". While deep\nlearning (DL) has increasingly been utilized to segment pneumothorax lesions in\nchest radiographs, many existing DL models employ an end-to-end approach. These\nmodels directly map chest radiographs to clinician-annotated lesion areas,\noften neglecting the vital domain knowledge that pneumothorax is inherently\nlocation-sensitive.\n  We propose a novel approach that incorporates the lung+ space as a constraint\nduring DL model training for pneumothorax segmentation on 2D chest radiographs.\nTo circumvent the need for additional annotations and to prevent potential\nlabel leakage on the target task, our method utilizes external datasets and an\nauxiliary task of lung segmentation. This approach generates a specific\nconstraint of lung+ space for each chest radiograph. Furthermore, we have\nincorporated a discriminator to eliminate unreliable constraints caused by the\ndomain shift between the auxiliary and target datasets.\n  Our results demonstrated significant improvements, with average performance\ngains of 4.6%, 3.6%, and 3.3% regarding Intersection over Union (IoU), Dice\nSimilarity Coefficient (DSC), and Hausdorff Distance (HD). Our research\nunderscores the significance of incorporating medical domain knowledge about\nthe location-specific nature of pneumothorax to enhance DL-based lesion\nsegmentation.",
            "author": [
                "Han Yuan",
                "Chuan Hong",
                "Nguyen Tuan Anh Tran",
                "Xinxing Xu",
                "Nan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15213v1",
                "http://arxiv.org/pdf/2311.15213v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15211v1",
            "title": "Probabilistic Transformer: A Probabilistic Dependency Model for\n  Contextual Word Representation",
            "updated": "2023-11-26T06:56:02Z",
            "published": "2023-11-26T06:56:02Z",
            "summary": "Syntactic structures used to play a vital role in natural language processing\n(NLP), but since the deep learning revolution, NLP has been gradually dominated\nby neural models that do not consider syntactic structures in their design. One\nvastly successful class of neural models is transformers. When used as an\nencoder, a transformer produces contextual representation of words in the input\nsentence. In this work, we propose a new model of contextual word\nrepresentation, not from a neural perspective, but from a purely syntactic and\nprobabilistic perspective. Specifically, we design a conditional random field\nthat models discrete latent representations of all words in a sentence as well\nas dependency arcs between them; and we use mean field variational inference\nfor approximate inference. Strikingly, we find that the computation graph of\nour model resembles transformers, with correspondences between dependencies and\nself-attention and between distributions over latent representations and\ncontextual embeddings of words. Experiments show that our model performs\ncompetitively to transformers on small to medium sized datasets. We hope that\nour work could help bridge the gap between traditional syntactic and\nprobabilistic approaches and cutting-edge neural approaches to NLP, and inspire\nmore linguistically-principled neural approaches in the future.",
            "author": [
                "Haoyi Wu",
                "Kewei Tu"
            ],
            "link": [
                "http://dx.doi.org/10.18653/v1/2023.findings-acl.482",
                "http://arxiv.org/abs/2311.15211v1",
                "http://arxiv.org/pdf/2311.15211v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15209v2",
            "title": "See and Think: Embodied Agent in Virtual Environment",
            "updated": "2023-12-03T00:58:47Z",
            "published": "2023-11-26T06:38:16Z",
            "summary": "Large language models (LLMs) have achieved impressive progress on several\nopen-world tasks. Recently, using LLMs to build embodied agents has been a\nhotspot. In this paper, we propose STEVE, a comprehensive and visionary\nembodied agent in the Minecraft virtual environment. STEVE consists of three\nkey components: vision perception, language instruction, and code action.\nVision perception involves the interpretation of visual information in the\nenvironment, which is then integrated into the LLMs component with agent state\nand task instruction. Language instruction is responsible for iterative\nreasoning and decomposing complex tasks into manageable guidelines. Code action\ngenerates executable skill actions based on retrieval in skill database,\nenabling the agent to interact effectively within the Minecraft environment. We\nalso collect STEVE-21K dataset, which includes 600$+$ vision-environment pairs,\n20K knowledge question-answering pairs, and 200$+$ skill-code pairs. We conduct\ncontinuous block search, knowledge question and answering, and tech tree\nmastery to evaluate the performance. Extensive experiments show that STEVE\nachieves at most $1.5 \\times$ faster unlocking key tech trees and $2.5 \\times$\nquicker in block search tasks compared to previous state-of-the-art methods.",
            "author": [
                "Zhonghan Zhao",
                "Wenhao Chai",
                "Xuan Wang",
                "Li Boyi",
                "Shengyu Hao",
                "Shidong Cao",
                "Tian Ye",
                "Jenq-Neng Hwang",
                "Gaoang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15209v2",
                "http://arxiv.org/pdf/2311.15209v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15207v1",
            "title": "Efficient interpolation of molecular properties across chemical compound\n  space with low-dimensional descriptors",
            "updated": "2023-11-26T06:22:13Z",
            "published": "2023-11-26T06:22:13Z",
            "summary": "We demonstrate accurate data-starved models of molecular properties for\ninterpolation in chemical compound spaces with low-dimensional descriptors.\n  Our starting point is based on three-dimensional, universal, physical\ndescriptors derived from the properties of the distributions of the eigenvalues\nof Coulomb matrices. To account for the shape and composition of molecules, we\ncombine these descriptors with six-dimensional features informed by the\nGershgorin circle theorem. We use the nine-dimensional descriptors thus\nobtained for Gaussian process regression based on kernels with variable\nfunctional form, leading to extremely efficient, low-dimensional interpolation\nmodels. The resulting models trained with 100 molecules are able to predict the\nproduct of entropy and temperature ($S \\times T$) and zero point vibrational\nenergy (ZPVE) with the absolute error under 1 kcal mol$^{-1}$ for $> 78$ \\% and\nunder 1.3 kcal mol$^{-1}$ for $> 92$ \\% of molecules in the test data. The test\ndata comprises 20,000 molecules with complexity varying from three atoms to 29\natoms and the ranges of $S \\times T$ and ZPVE covering 36 kcal mol$^{-1}$ and\n161 kcal mol$^{-1}$, respectively. We also illustrate that the descriptors\nbased on the Gershgorin circle theorem yield more accurate models of molecular\nentropy than those based on graph neural networks that explicitly account for\nthe atomic connectivity of molecules.",
            "author": [
                "Yun-Wen Mao",
                "Roman V. Krems"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15207v1",
                "http://arxiv.org/pdf/2311.15207v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15204v1",
            "title": "OpenDigger: Data Mining and Information Service System for Open\n  Collaboration Digital Ecosystem",
            "updated": "2023-11-26T05:56:40Z",
            "published": "2023-11-26T05:56:40Z",
            "summary": "The widespread development and adoption of open-source software have built an\necosystem for open development and collaboration. In this ecosystem,\nindividuals and organizations collaborate to create high-quality software that\ncan be used by everyone. Social collaboration platforms like GitHub have\nfurther facilitated large-scale, distributed, and fine-grained code\ncollaboration and technical interactions. Countless developers contribute code,\nreview code, report bugs, and propose new features on these platforms every\nday, generating a massive amount of valuable behavioral data from the open\ncollaboration process. This paper presents the design and implementation of\nOpenDigger, a comprehensive data mining and information service system for open\ncollaboration in the digital ecosystem. The goal is to build a data\ninfrastructure for the open-source domain and promote the continuous\ndevelopment of the open-source ecosystem. The metrics and analysis models in\nthe OpenDigger system can mine various knowledge from the macro to micro levels\nin the open-source digital ecosystem. Through a unified information service\ninterface, OpenDigger provides various open-source information services to\ndifferent user groups, including governments, enterprises, foundations, and\nindividuals. As a novel information service system in the open-source\necosystem, this paper demonstrates the effectiveness of the metrics and models\nin OpenDigger through several real-world scenarios, including products, tools,\napplications, and courses. It showcases the significant and diverse practical\napplications of the metrics and models in both algorithmic and business\naspects.",
            "author": [
                "Xiaoya Xia",
                "Shengyu Zhao",
                "Fanyu Han",
                "Fenglin Bi",
                "Wei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15204v1",
                "http://arxiv.org/pdf/2311.15204v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15202v2",
            "title": "Dual-stream contrastive predictive network with joint handcrafted\n  feature view for SAR ship classification",
            "updated": "2023-11-30T10:45:11Z",
            "published": "2023-11-26T05:47:01Z",
            "summary": "Most existing synthetic aperture radar (SAR) ship classification technologies\nheavily rely on correctly labeled data, ignoring the discriminative features of\nunlabeled SAR ship images. Even though researchers try to enrich CNN-based\nfeatures by introducing traditional handcrafted features, existing methods\neasily cause information redundancy and fail to capture the interaction between\nthem. To address these issues, we propose a novel dual-stream contrastive\npredictive network (DCPNet), which consists of two asymmetric task designs and\nthe false negative sample elimination module. The first task is to construct\npositive sample pairs, guiding the core encoder to learn more general\nrepresentations. The second task is to encourage adaptive capture of the\ncorrespondence between deep features and handcrated features, achieving\nknowledge transfer within the model, and effectively improving the redundancy\ncaused by the feature fusion. To increase the separability between clusters, we\nalso design a cluster-level tasks. The experimental results on OpenSARShip and\nFUSAR-Ship datasets demonstrate the improvement in classification accuracy of\nsupervised models and confirm the capability of learning effective\nrepresentations of DCPNet.",
            "author": [
                "Xianting Feng",
                "Hao zheng",
                "Zhigang Hu",
                "Liu Yang",
                "Meiguang Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15202v2",
                "http://arxiv.org/pdf/2311.15202v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15172v1",
            "title": "Toward a density Corr\u00e1di--Hajnal theorem for degenerate hypergraphs",
            "updated": "2023-11-26T03:07:34Z",
            "published": "2023-11-26T03:07:34Z",
            "summary": "Given an $r$-graph $F$ with $r \\ge 2$, let $\\mathrm{ex}(n, (t+1) F)$ denote\nthe maximum number of edges in an $n$-vertex $r$-graph with at most $t$\npairwise vertex-disjoint copies of $F$. Extending several old results and\ncomplementing prior work [J. Hou, H. Li, X. Liu, L.-T. Yuan, and Y. Zhang. A\nstep towards a general density Corr\\'{a}di--Hajnal theorem. arXiv:2302.09849,\n2023.] on nondegenerate hypergraphs, we initiate a systematic study on\n$\\mathrm{ex}(n, (t+1) F)$ for degenerate hypergraphs $F$. For a broad class of\ndegenerate hypergraphs $F$, we present near-optimal upper bounds for\n$\\mathrm{ex}(n, (t+1) F)$ when $n$ is sufficiently large and $t$ lies in\nintervals $\\left[0, \\frac{\\varepsilon \\cdot \\mathrm{ex}(n,F)}{n^{r-1}}\\right]$,\n$\\left[\\frac{\\mathrm{ex}(n,F)}{\\varepsilon n^{r-1}}, \\varepsilon n \\right]$,\nand $\\left[ (1-\\varepsilon)\\frac{n}{v(F)}, \\frac{n}{v(F)} \\right]$, where\n$\\varepsilon > 0$ is a constant depending only on $F$. Our results reveal very\ndifferent structures for extremal constructions across the three intervals, and\nwe provide characterizations of extremal constructions within the first\ninterval. Additionally, for graphs, we offer a characterization of extremal\nconstructions within the second interval. Our proof for the first interval also\napplies to a special class of nondegenerate hypergraphs, including those with\nundetermined Tur\\'{a}n densities, partially improving a result in [J. Hou, H.\nLi, X. Liu, L.-T. Yuan, and Y. Zhang. A step towards a general density\nCorr\\'{a}di--Hajnal theorem. arXiv:2302.09849, 2023.]",
            "author": [
                "Jianfeng Hou",
                "Caiyun Hu",
                "Heng Li",
                "Xizhi Liu",
                "Caihong Yang",
                "Yixiao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15172v1",
                "http://arxiv.org/pdf/2311.15172v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16189v1",
            "title": "Many vertex-disjoint even cycles of fixed length in a graph",
            "updated": "2023-11-26T02:25:23Z",
            "published": "2023-11-26T02:25:23Z",
            "summary": "For every integer $k \\ge 3$, we determine the extremal structure of an\n$n$-vertex graph with at most $t$ vertex-disjoint copies of $C_{2k}$ when $n$\nis sufficiently large and $t$ lies in the interval\n$\\left[\\frac{\\mathrm{ex}(n,C_{2k})}{\\varepsilon n}, \\varepsilon n\\right]$,\nwhere $\\varepsilon>0$ is a constant depending only on $k$. The question for $k\n= 2$ and $t = o\\left(\\frac{\\mathrm{ex}(n,C_{2k})}{n}\\right)$ was explored in\nprior work~\\cite{HHLLYZ23a}, revealing different extremal structures in these\ncases. Our result can be viewed as an extension of the theorems by\nEgawa~\\cite{Ega96} and Verstra\\\"{e}te~\\cite{Ver03}, where the focus was on the\nexistence of many vertex-disjoint cycles of the same length without any length\nconstraints.",
            "author": [
                "Jianfeng Hou",
                "Caiyun Hu",
                "Heng Li",
                "Xizhi Liu",
                "Caihong Yang",
                "Yixiao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16189v1",
                "http://arxiv.org/pdf/2311.16189v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15162v1",
            "title": "Domain Knowledge Injection in Bayesian Search for New Materials",
            "updated": "2023-11-26T01:55:55Z",
            "published": "2023-11-26T01:55:55Z",
            "summary": "In this paper we propose DKIBO, a Bayesian optimization (BO) algorithm that\naccommodates domain knowledge to tune exploration in the search space. Bayesian\noptimization has recently emerged as a sample-efficient optimizer for many\nintractable scientific problems. While various existing BO frameworks allow the\ninput of prior beliefs to accelerate the search by narrowing down the space,\nincorporating such knowledge is not always straightforward and can often\nintroduce bias and lead to poor performance. Here we propose a simple approach\nto incorporate structural knowledge in the acquisition function by utilizing an\nadditional deterministic surrogate model to enrich the approximation power of\nthe Gaussian process. This is suitably chosen according to structural\ninformation of the problem at hand and acts a corrective term towards a\nbetter-informed sampling. We empirically demonstrate the practical utility of\nthe proposed method by successfully injecting domain knowledge in a materials\ndesign task. We further validate our method's performance on different\nexperimental settings and ablation analyses.",
            "author": [
                "Zikai Xie",
                "Xenophon Evangelopoulos",
                "Joseph Thacker",
                "Andrew Cooper"
            ],
            "link": [
                "http://dx.doi.org/10.3233/FAIA230587",
                "http://arxiv.org/abs/2311.15162v1",
                "http://arxiv.org/pdf/2311.15162v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "68W99",
                "I.2.8"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15161v1",
            "title": "Hessian Aware Low-Rank Weight Perturbation for Continual Learning",
            "updated": "2023-11-26T01:44:01Z",
            "published": "2023-11-26T01:44:01Z",
            "summary": "Continual learning aims to learn a series of tasks sequentially without\nforgetting the knowledge acquired from the previous ones. In this work, we\npropose the Hessian Aware Low-Rank Perturbation algorithm for continual\nlearning. By modeling the parameter transitions along the sequential tasks with\nthe weight matrix transformation, we propose to apply the low-rank\napproximation on the task-adaptive parameters in each layer of the neural\nnetworks. Specifically, we theoretically demonstrate the quantitative\nrelationship between the Hessian and the proposed low-rank approximation. The\napproximation ranks are then globally determined according to the marginal\nincrement of the empirical loss estimated by the layer-specific gradient and\nlow-rank approximation error. Furthermore, we control the model capacity by\npruning less important parameters to diminish the parameter growth. We conduct\nextensive experiments on various benchmarks, including a dataset with\nlarge-scale tasks, and compare our method against some recent state-of-the-art\nmethods to demonstrate the effectiveness and scalability of our proposed\nmethod. Empirical results show that our method performs better on different\nbenchmarks, especially in achieving task order robustness and handling the\nforgetting issue. A demo code can be found at https://github.com/lijiaqi/HALRP.",
            "author": [
                "Jiaqi Li",
                "Rui Wang",
                "Yuanhao Lai",
                "Changjian Shui",
                "Sabyasachi Sahoo",
                "Charles X. Ling",
                "Shichun Yang",
                "Boyu Wang",
                "Christian Gagn\u00e9",
                "Fan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15161v1",
                "http://arxiv.org/pdf/2311.15161v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15160v1",
            "title": "Causa prima: cosmology meets causal discovery for the first time",
            "updated": "2023-11-26T01:43:06Z",
            "published": "2023-11-26T01:43:06Z",
            "summary": "In astrophysics, experiments are impossible. We thus must rely exclusively on\nobservational data. Other observational sciences increasingly leverage causal\ninference methods, but this is not yet the case in astrophysics. Here we\nattempt causal discovery for the first time to address an important open\nproblem in astrophysics: the (co)evolution of supermassive black holes (SMBHs)\nand their host galaxies. We apply the Peter-Clark (PC) algorithm to a\ncomprehensive catalog of galaxy properties to obtain a completed partially\ndirected acyclic graph (CPDAG), representing a Markov equivalence class over\ndirected acyclic graphs (DAGs). Central density and velocity dispersion are\nfound to cause SMBH mass. We test the robustness of our analysis by random\nsub-sampling, recovering similar results. We also apply the Fast Causal\nInference (FCI) algorithm to our dataset to relax the hypothesis of causal\nsufficiency, admitting unobserved confounds. Hierarchical SMBH assembly may\nprovide a physical explanation for our findings.",
            "author": [
                "Mario Pasquato",
                "Zehao Jin",
                "Pablo Lemos",
                "Benjamin L. Davis",
                "Andrea V. Macci\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15160v1",
                "http://arxiv.org/pdf/2311.15160v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15153v1",
            "title": "Self-Supervised Learning for SAR ATR with a Knowledge-Guided Predictive\n  Architecture",
            "updated": "2023-11-26T01:05:55Z",
            "published": "2023-11-26T01:05:55Z",
            "summary": "Recently, the emergence of a large number of Synthetic Aperture Radar (SAR)\nsensors and target datasets has made it possible to unify downstream tasks with\nself-supervised learning techniques, which can pave the way for building the\nfoundation model in the SAR target recognition field. The major challenge of\nself-supervised learning for SAR target recognition lies in the generalizable\nrepresentation learning in low data quality and noise.To address the\naforementioned problem, we propose a knowledge-guided predictive architecture\nthat uses local masked patches to predict the multiscale SAR feature\nrepresentations of unseen context. The core of the proposed architecture lies\nin combining traditional SAR domain feature extraction with state-of-the-art\nscalable self-supervised learning for accurate generalized feature\nrepresentations. The proposed framework is validated on various downstream\ndatasets (MSTAR, FUSAR-Ship, SAR-ACD and SSDD), and can bring consistent\nperformance improvement for SAR target recognition. The experimental results\nstrongly demonstrate the unified performance improvement of the self-supervised\nlearning technique for SAR target recognition across diverse targets, scenes\nand sensors.",
            "author": [
                "Weijie Li",
                "Yang Wei",
                "Tianpeng Liu",
                "Yuenan Hou",
                "Yongxiang Liu",
                "Li Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15153v1",
                "http://arxiv.org/pdf/2311.15153v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15144v1",
            "title": "Some results on the Wiener index related to the \u0160olt\u00e9s problem\n  of graphs",
            "updated": "2023-11-26T00:04:10Z",
            "published": "2023-11-26T00:04:10Z",
            "summary": "The Wiener index, $W(G)$, of a connected graph $G$ is the sum of distances\nbetween its vertices. In 2021, Akhmejanova et al. posed the problem of finding\ngraphs $G$ with large $R_m(G)= |\\{v\\in V(G)\\,|\\,W(G)-W(G-v)=m \\in \\mathbb{Z}\n\\}|/ |V(G)|$. It is shown that there is a graph $G$ with $R_m(G) > 1/2$ for any\ninteger $m \\ge 0$. In particular, there is a regular graph of even degree with\nthis property for any odd $m \\ge 1$. The proposed approach allows to construct\nnew families of graphs $G$ with $R_0(G) \\rightarrow 1/2$ when the order of $G$\nincreases.",
            "author": [
                "Andrey A. Dobrynin",
                "Konstantin V. Vorob'ev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15144v1",
                "http://arxiv.org/pdf/2311.15144v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C09, 05C12, 05C92"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15135v1",
            "title": "Normal Rees algebras arising from vertex decomposable simplicial\n  complexes",
            "updated": "2023-11-25T23:03:53Z",
            "published": "2023-11-25T23:03:53Z",
            "summary": "We show that for a vertex decomposable simplicial complex $\\Delta$, the Rees\nalgebra of $I_{\\Delta^{\\vee}}$ is a normal Cohen-Macaulay domain. As\nconsequences, we show that any squarefree weakly polymatroidal ideal is normal\nand we obtain normal ideals among several interesting families of monomial\nideals such as cover ideals of graphs and edge ideals of hypergraphs. Moreover,\nbased on a construction on simplicial complexes given by Biermann and Van Tuyl\n[2], we present families of normal ideals attached to any squarefree monomial\nideal.",
            "author": [
                "Somayeh Moradi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15135v1",
                "http://arxiv.org/pdf/2311.15135v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "13F55, 13A30"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15131v1",
            "title": "Localizing Lying in Llama: Understanding Instructed Dishonesty on\n  True-False Questions Through Prompting, Probing, and Patching",
            "updated": "2023-11-25T22:41:23Z",
            "published": "2023-11-25T22:41:23Z",
            "summary": "Large language models (LLMs) demonstrate significant knowledge through their\noutputs, though it is often unclear whether false outputs are due to a lack of\nknowledge or dishonesty. In this paper, we investigate instructed dishonesty,\nwherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt\nengineering to find which prompts best induce lying behavior, and then use\nmechanistic interpretability approaches to localize where in the network this\nbehavior occurs. Using linear probing and activation patching, we localize five\nlayers that appear especially important for lying. We then find just 46\nattention heads within these layers that enable us to causally intervene such\nthat the lying model instead answers honestly. We show that these interventions\nwork robustly across many prompts and dataset splits. Overall, our work\ncontributes a greater understanding of dishonesty in LLMs so that we may hope\nto prevent it.",
            "author": [
                "James Campbell",
                "Richard Ren",
                "Phillip Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15131v1",
                "http://arxiv.org/pdf/2311.15131v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16188v1",
            "title": "Bell pair extraction using graph foliage techniques",
            "updated": "2023-11-25T22:33:29Z",
            "published": "2023-11-25T22:33:29Z",
            "summary": "Future quantum networks can facilitate communication of quantum information\nbetween various nodes. We are particularly interested in whether multiple pairs\ncan communicate simultaneously across a network. Quantum networks can be\nrepresented with graph states, and producing communication links amounts to\nperforming certain quantum operations on graph states. This problem can be\nformulated in a graph-theoretic sense with the (Bell) vertex-minor problem. We\ndiscuss the recently introduced foliage partition and provide a generalization.\nThis generalization leads us to a useful result for approaching the\nvertex-minor problem. We apply this result to identify the exact solution for\nthe Bell vertex-minor problem on line, tree, and ring graphs.",
            "author": [
                "Derek Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16188v1",
                "http://arxiv.org/pdf/2311.16188v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15128v1",
            "title": "Quickest Change Detection with Post-Change Density Estimation",
            "updated": "2023-11-25T22:29:05Z",
            "published": "2023-11-25T22:29:05Z",
            "summary": "The problem of quickest change detection in a sequence of independent\nobservations is considered. The pre-change distribution is assumed to be known,\nwhile the post-change distribution is unknown. Two tests based on post-change\ndensity estimation are developed for this problem, the window-limited\nnon-parametric generalized likelihood ratio (NGLR) CuSum test and the\nnon-parametric window-limited adaptive (NWLA) CuSum test. Both tests do not\nassume any knowledge of the post-change distribution, except that the\npost-change density satisfies certain smoothness conditions that allows for\nefficient non-parametric estimation. Also, they do not require any\npre-collected post-change training samples. Under certain convergence\nconditions on the density estimator, it is shown that both tests are\nfirst-order asymptotically optimal, as the false alarm rate goes to zero. The\nanalysis is validated through numerical results, where both tests are compared\nwith baseline tests that have distributional knowledge.",
            "author": [
                "Yuchen Liang",
                "Venugopal V. Veeravalli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15128v1",
                "http://arxiv.org/pdf/2311.15128v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "eess.SP",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17929v1",
            "title": "New Online Communities: Graph Deep Learning on Anonymous Voting Networks\n  to Identify Sybils in Polycentric Governance",
            "updated": "2023-11-25T22:26:58Z",
            "published": "2023-11-25T22:26:58Z",
            "summary": "This research examines the polycentric governance of digital assets in\nDecentralized Autonomous Organizations (DAOs). It offers a theoretical\nframework and addresses a critical challenge facing decentralized governance by\ndeveloping a method to identify sybils, or spurious identities. The method uses\ngraph deep learning techniques to identify sybil activity in a DAO governance\ndataset (snapshot.org). Specifically, a Graph Convolutional Neural Network\n(GCNN) learned voting behaviours and a fast k-means vector clustering algorithm\n(FAISS) used the high dimensional embeddings to identify similar nodes in a\ngraph. The results reveal that deep learning can effectively identify sybils,\nreducing the voting graph by 2-5%. This research underscores the importance of\nsybil resistance in DAOs and offers a novel perspective on decentralized\ngovernance, informing future policy, regulation, and governance practices.",
            "author": [
                "Quinn DuPont"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17929v1",
                "http://arxiv.org/pdf/2311.17929v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15123v1",
            "title": "FPQA-C: A Compilation Framework for Field Programmable Qubit Array",
            "updated": "2023-11-25T21:57:41Z",
            "published": "2023-11-25T21:57:41Z",
            "summary": "The neutral atom array has gained prominence in quantum computing for its\nscalability and operation fidelity. Previous works focus on \\textit{fixed} atom\narrays (FAA) that necessitate extensive SWAP operations for long-range\ninteractions. This work explores a novel architecture known as \\textit{field\nprogrammable qubit array (FPQA)}, which uniquely allows for coherent atom\nmovements during circuit execution and significantly \\textit{reduces the cost\nof long-range interactions}. However, the atom movements have multiple hardware\nconstraints, making movement scheduling very challenging.\n  In this work, we introduce FPQA-C, a compilation framework tailored for qubit\nmapping, atom movement, and gate scheduling of FPQA. It contains a qubit-array\nmapper to decide the coarse-grained mapping of qubit to arrays, leveraging MAX\nk-Cut on a constructed gate frequency graph to minimize SWAP overhead.\nSubsequently, a qubit-atom mapper determines the fine-grained mapping of qubits\nto specific atoms in the array, and considers load balance to prevent hardware\nconstraint violations. We further propose a high-parallelism router that\niteratively identifies parallelizable 2Q gates and decide the atom movements\nand gate executions, thus improving the parallelism. Besides, for\nfault-tolerant computing with FPQA, we provide comprehensive simulations\nevaluating logical error rates, execution times, physical qubit requirements,\ncode distances, and bandwidth.\n  We rigorously assess FPQA-C across 20+ diverse benchmarks, including generic\ncircuits (arbitrary, QASMBench, SupermarQ), Quantum Simulation, and QAOA\ncircuits. FPQA-C consistently outperforms the IBM Superconducting, FAA with\nlong-range gates, FAA with rectangular and triangular topologies, achieving 2Q\ngate reductions by factors of 5.3x, 3.2x, 3.4x, and 2.6x, and circuit depth\nreductions by factors of 3.6x, 3.2x, 3.1x, and 2.2x, respectively.",
            "author": [
                "Hanrui Wang",
                "Pengyu Liu",
                "Bochen Tan",
                "Yilian Liu",
                "Jiaqi Gu",
                "David Z. Pan",
                "Jason Cong",
                "Umut Acar",
                "Song Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15123v1",
                "http://arxiv.org/pdf/2311.15123v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AR",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15112v2",
            "title": "Everybody Needs a Little HELP: Explaining Graphs via Hierarchical\n  Concepts",
            "updated": "2023-12-02T10:44:33Z",
            "published": "2023-11-25T20:06:46Z",
            "summary": "Graph neural networks (GNNs) have led to major breakthroughs in a variety of\ndomains such as drug discovery, social network analysis, and travel time\nestimation. However, they lack interpretability which hinders human trust and\nthereby deployment to settings with high-stakes decisions. A line of\ninterpretable methods approach this by discovering a small set of relevant\nconcepts as subgraphs in the last GNN layer that together explain the\nprediction. This can yield oversimplified explanations, failing to explain the\ninteraction between GNN layers. To address this oversight, we provide HELP\n(Hierarchical Explainable Latent Pooling), a novel, inherently interpretable\ngraph pooling approach that reveals how concepts from different GNN layers\ncompose to new ones in later steps. HELP is more than 1-WL expressive and is\nthe first non-spectral, end-to-end-learnable, hierarchical graph pooling method\nthat can learn to pool a variable number of arbitrary connected components. We\nempirically demonstrate that it performs on-par with standard GCNs and popular\npooling methods in terms of accuracy while yielding explanations that are\naligned with expert knowledge in the domains of chemistry and social networks.\nIn addition to a qualitative analysis, we employ concept completeness scores as\nwell as concept conformity, a novel metric to measure the noise in discovered\nconcepts, quantitatively verifying that the discovered concepts are\nsignificantly easier to fully understand than those from previous work. Our\nwork represents a first step towards an understanding of graph neural networks\nthat goes beyond a set of concepts from the final layer and instead explains\nthe complex interplay of concepts on different levels.",
            "author": [
                "Jonas J\u00fcr\u00df",
                "Lucie Charlotte Magister",
                "Pietro Barbiero",
                "Pietro Li\u00f2",
                "Nikola Simidjievski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15112v2",
                "http://arxiv.org/pdf/2311.15112v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15106v1",
            "title": "Solving the Right Problem is Key for Translational NLP: A Case Study in\n  UMLS Vocabulary Insertion",
            "updated": "2023-11-25T19:35:53Z",
            "published": "2023-11-25T19:35:53Z",
            "summary": "As the immense opportunities enabled by large language models become more\napparent, NLP systems will be increasingly expected to excel in real-world\nsettings. However, in many instances, powerful models alone will not yield\ntranslational NLP solutions, especially if the formulated problem is not well\naligned with the real-world task. In this work, we study the case of UMLS\nvocabulary insertion, an important real-world task in which hundreds of\nthousands of new terms, referred to as atoms, are added to the UMLS, one of the\nmost comprehensive open-source biomedical knowledge bases. Previous work aimed\nto develop an automated NLP system to make this time-consuming, costly, and\nerror-prone task more efficient. Nevertheless, practical progress in this\ndirection has been difficult to achieve due to a problem formulation and\nevaluation gap between research output and the real-world task. In order to\naddress this gap, we introduce a new formulation for UMLS vocabulary insertion\nwhich mirrors the real-world task, datasets which faithfully represent it and\nseveral strong baselines we developed through re-purposing existing solutions.\nAdditionally, we propose an effective rule-enhanced biomedical language model\nwhich enables important new model behavior, outperforms all strong baselines\nand provides measurable qualitative improvements to editors who carry out the\nUVI task. We hope this case study provides insight into the considerable\nimportance of problem formulation for the success of translational NLP\nsolutions.",
            "author": [
                "Bernal Jimenez Gutierrez",
                "Yuqing Mao",
                "Vinh Nguyen",
                "Kin Wah Fung",
                "Yu Su",
                "Olivier Bodenreider"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15106v1",
                "http://arxiv.org/pdf/2311.15106v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15093v1",
            "title": "Optimizing a Model-Agnostic Measure of Graph Counterdeceptiveness via\n  Reattachment",
            "updated": "2023-11-25T18:16:44Z",
            "published": "2023-11-25T18:16:44Z",
            "summary": "Recognition of an adversary's objective is a core problem in physical\nsecurity and cyber defense. Prior work on target recognition focuses on\ndeveloping optimal inference strategies given the adversary's operating\nenvironment. However, the success of such strategies significantly depends on\nfeatures of the environment. We consider the problem of optimal\ncounterdeceptive environment design: construction of an environment which\npromotes early recognition of an adversary's objective, given operational\nconstraints. Interpreting counterdeception as a question of graph design with a\nbound on total edge length, we propose a measure of graph counterdeceptiveness\nand a novel heuristic algorithm for maximizing counterdeceptiveness based on\niterative reattachment of trees. We benchmark the performance of this algorithm\non synthetic networks as well as a graph inspired by a real-world high-security\nenvironment, verifying that the proposed algorithm is computationally feasible\nand yields meaningful network designs.",
            "author": [
                "Anakin Dey",
                "Sam Ruggerio",
                "Melkior Ornik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15093v1",
                "http://arxiv.org/pdf/2311.15093v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15089v1",
            "title": "Where2Start: Leveraging initial States for Robust and Sample-Efficient\n  Reinforcement Learning",
            "updated": "2023-11-25T18:00:26Z",
            "published": "2023-11-25T18:00:26Z",
            "summary": "The reinforcement learning algorithms that focus on how to compute the\ngradient and choose next actions, are effectively improved the performance of\nthe agents. However, these algorithms are environment-agnostic. This means that\nthe algorithms did not use the knowledge that has been captured by trajectory.\nThis poses that the algorithms should sample many trajectories to train the\nmodel. By considering the essence of environment and how much the agent learn\nfrom each scenario in that environment, the strategy of the learning procedure\ncan be changed. The strategy retrieves more informative trajectories, so the\nagent can learn with fewer trajectory sample. We propose Where2Start algorithm\nthat selects the initial state so that the agent has more instability in\nvicinity of that state. We show that this kind of selection decreases number of\ntrajectories that should be sampled that the agent reach to acceptable reward.\nOur experiments shows that Where2Start can improve sample efficiency up to 8\ntimes. Also Where2Start can combined with most of state-of-the-art algorithms\nand improve that robustness and sample efficiency significantly.",
            "author": [
                "Pouya Parsa",
                "Raoof Zare Moayedi",
                "Mohammad Bornosi",
                "Mohammad Mahdi Bejani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15089v1",
                "http://arxiv.org/pdf/2311.15089v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15082v3",
            "title": "Learning graph-Fourier spectra of textured surface images for defect\n  localization",
            "updated": "2023-12-02T03:33:56Z",
            "published": "2023-11-25T17:25:07Z",
            "summary": "In the realm of industrial manufacturing, product inspection remains a\nsignificant bottleneck, with only a small fraction of manufactured items\nundergoing inspection for surface defects. Advances in imaging systems and AI\ncan allow automated full inspection of manufactured surfaces. However, even the\nmost contemporary imaging and machine learning methods perform poorly for\ndetecting defects in images with highly textured backgrounds, that stem from\ndiverse manufacturing processes. This paper introduces an approach based on\ngraph Fourier analysis to automatically identify defective images, as well as\ncrucial graph Fourier coefficients that inform the defects in images amidst\nhighly textured backgrounds. The approach capitalizes on the ability of graph\nrepresentations to capture the complex dynamics inherent in high-dimensional\ndata, preserving crucial locality properties in a lower dimensional space. A\nconvolutional neural network model (1D-CNN) was trained with the coefficients\nof the graph Fourier transform of the images as the input to identify, with\nclassification accuracy of 99.4%, if the image contains a defect. An\nexplainable AI method using SHAP (SHapley Additive exPlanations) was used to\nfurther analyze the trained 1D-CNN model to discern important spectral\ncoefficients for each image. This approach sheds light on the crucial\ncontribution of low-frequency graph eigen waveforms to precisely localize\nsurface defects in images, thereby advancing the realization of zero-defect\nmanufacturing.",
            "author": [
                "Tapan Ganatma Nakkina",
                "Adithyaa Karthikeyan",
                "Yuhao Zhong",
                "Ceyhun Eksin",
                "Satish T. S. Bukkapatnam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15082v3",
                "http://arxiv.org/pdf/2311.15082v3"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15075v1",
            "title": "Mug-STAN: Adapting Image-Language Pretrained Models for General Video\n  Understanding",
            "updated": "2023-11-25T17:01:38Z",
            "published": "2023-11-25T17:01:38Z",
            "summary": "Large-scale image-language pretrained models, e.g., CLIP, have demonstrated\nremarkable proficiency in acquiring general multi-modal knowledge through\nweb-scale image-text data. Despite the impressive performance of image-language\nmodels on various image tasks, how to effectively expand them on general video\nunderstanding remains an area of ongoing exploration. In this paper, we\ninvestigate the image-to-video transferring from the perspective of the model\nand the data, unveiling two key obstacles impeding the adaptation of\nimage-language models: non-generalizable temporal modeling and partially\nmisaligned video-text data. To address these challenges, we propose\nSpatial-Temporal Auxiliary Network with Mutual-guided alignment module\n(Mug-STAN), a simple yet effective framework extending image-text model to\ndiverse video tasks and video-text data.Specifically, STAN adopts a branch\nstructure with decomposed spatial-temporal modules to enable generalizable\ntemporal modeling, while Mug suppresses misalignment by introducing token-wise\nfeature aggregation of either modality from the other. Extensive experimental\nresults verify Mug-STAN significantly improves adaptation of language-image\npretrained models such as CLIP and CoCa at both video-text post-pretraining and\nfinetuning stages. With our solution, state-of-the-art zero-shot and finetuning\nresults on various downstream datasets, including MSR-VTT, DiDeMo, LSMDC,\nKinetics-400, Something-Something-2, HMDB-51, UCF- 101, and AVA, are achieved.\nMoreover, by integrating pretrained Mug-STAN with the emerging multimodal\ndialogue model, we can realize zero-shot video chatting. Codes are available at\nhttps://github.com/farewellthree/STAN",
            "author": [
                "Ruyang Liu",
                "Jingjia Huang",
                "Wei Gao",
                "Thomas H. Li",
                "Ge Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15075v1",
                "http://arxiv.org/pdf/2311.15075v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15057v1",
            "title": "On Layered Area-Proportional Rectangle Contact Representations",
            "updated": "2023-11-25T15:45:09Z",
            "published": "2023-11-25T15:45:09Z",
            "summary": "A pair $\\langle G_0, G_1 \\rangle$ of graphs admits a mutual witness proximity\ndrawing $\\langle \\Gamma_0, \\Gamma_1 \\rangle$ when: (i) $\\Gamma_i$ represents\n$G_i$, and (ii) there is an edge $(u,v)$ in $\\Gamma_i$ if and only if there is\nno vertex $w$ in $\\Gamma_{1-i}$ that is ``too close'' to both $u$ and $v$\n($i=0,1$). In this paper, we consider infinitely many definitions of closeness\nby adopting the $\\beta$-proximity rule for any $\\beta \\in [1,\\infty]$ and study\npairs of isomorphic trees that admit a mutual witness $\\beta$-proximity\ndrawing. Specifically, we show that every two isomorphic trees admit a mutual\nwitness $\\beta$-proximity drawing for any $\\beta \\in [1,\\infty]$. The\nconstructive technique can be made ``robust'': For some tree pairs we can\nsuitably prune linearly many leaves from one of the two trees and still retain\ntheir mutual witness $\\beta$-proximity drawability. Notably, in the special\ncase of isomorphic caterpillars and $\\beta=1$, we construct linearly separable\nmutual witness Gabriel drawings.",
            "author": [
                "Carolina Haase",
                "Philipp Kindermann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15057v1",
                "http://arxiv.org/pdf/2311.15057v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15056v1",
            "title": "Accurate and interpretable drug-drug interaction prediction enabled by\n  knowledge subgraph learning",
            "updated": "2023-11-25T15:44:28Z",
            "published": "2023-11-25T15:44:28Z",
            "summary": "Background: Discovering potential drug-drug interactions (DDIs) is a\nlong-standing challenge in clinical treatments and drug developments. Recently,\ndeep learning techniques have been developed for DDI prediction. However, they\ngenerally require a huge number of samples, while known DDIs are rare.\n  Methods: In this work, we present KnowDDI, a graph neural network-based\nmethod that addresses the above challenge. KnowDDI enhances drug\nrepresentations by adaptively leveraging rich neighborhood information from\nlarge biomedical knowledge graphs. Then, it learns a knowledge subgraph for\neach drug-pair to interpret the predicted DDI, where each of the edges is\nassociated with a connection strength indicating the importance of a known DDI\nor resembling strength between a drug-pair whose connection is unknown. Thus,\nthe lack of DDIs is implicitly compensated by the enriched drug representations\nand propagated drug similarities.\n  Results: We evaluate KnowDDI on two benchmark DDI datasets. Results show that\nKnowDDI obtains the state-of-the-art prediction performance with better\ninterpretability. We also find that KnowDDI suffers less than existing works\ngiven a sparser knowledge graph. This indicates that the propagated drug\nsimilarities play a more important role in compensating for the lack of DDIs\nwhen the drug representations are less enriched.\n  Conclusions: KnowDDI nicely combines the efficiency of deep learning\ntechniques and the rich prior knowledge in biomedical knowledge graphs. As an\noriginal open-source tool, KnowDDI can help detect possible interactions in a\nbroad range of relevant interaction prediction tasks, such as protein-protein\ninteractions, drug-target interactions and disease-gene interactions,\neventually promoting the development of biomedicine and healthcare.",
            "author": [
                "Yaqing Wang",
                "Zaifei Yang",
                "Quanming Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15056v1",
                "http://arxiv.org/pdf/2311.15056v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15043v1",
            "title": "Plane Multigraphs with One-Bend and Circular-Arc Edges of a Fixed Angle",
            "updated": "2023-11-25T14:47:25Z",
            "published": "2023-11-25T14:47:25Z",
            "summary": "For an angle $\\alpha\\in (0,\\pi)$, we consider plane graphs and multigraphs in\nwhich the edges are either (i) one-bend polylines with an angle $\\alpha$\nbetween the two edge segments, or (ii) circular arcs of central angle\n$2(\\pi-\\alpha)$. We derive upper and lower bounds on the maximum density of\nsuch graphs in terms of $\\alpha$. As an application, we improve upon bounds for\nthe number of edges in $\\alpha AC_1^=$ graphs (i.e., graphs that can be drawn\nin the plane with one-bend edges such that any two crossing edges meet at angle\n$\\alpha$). This is the first improvement on the size of $\\alpha AC_1^=$ graphs\nin over a decade.",
            "author": [
                "Csaba D. T\u00f3th"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15043v1",
                "http://arxiv.org/pdf/2311.15043v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15037v1",
            "title": "Automatic Detection of Nuclear Spins at Arbitrary Magnetic Fields via\n  Signal-to-Image AI Model",
            "updated": "2023-11-25T14:18:38Z",
            "published": "2023-11-25T14:18:38Z",
            "summary": "Quantum sensors leverage matter's quantum properties to enable measurements\nwith unprecedented spatial and spectral resolution. Among these sensors, those\nutilizing nitrogen-vacancy (NV) centers in diamond offer the distinct advantage\nof operating at room temperature. Nevertheless, signals received from NV\ncenters are often complex, making interpretation challenging. This is\nespecially relevant in low magnetic field scenarios, where standard\napproximations for modeling the system fail. Additionally, NV signals feature a\nprominent noise component. In this work, we present a signal-to-image deep\nlearning model capable to automatically infer the number of nuclear spins\nsurrounding an NV sensor and the hyperfine couplings between the sensor and the\nnuclear spins. Our model can be trained to operate effectively across various\nmagnetic field scenarios, requires no prior knowledge of the involved nuclei,\nand is designed to handle noisy signals, leading to fast characterization of\nnuclear environments in real experimental conditions. With detailed numerical\nsimulations, we test the performance of our model in scenarios involving\nvarying numbers of nuclei, achieving an average error of less than $2\\\n\\rm{kHz}$ in the estimated hyperfine constants.",
            "author": [
                "B. Varona-Uriarte",
                "C. Munuera-Javaloy",
                "E. Terradillos",
                "A. Alvarez-Gila",
                "E. Garrote",
                "J. Casanova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15037v1",
                "http://arxiv.org/pdf/2311.15037v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15027v1",
            "title": "Double-Flow-based Steganography without Embedding for Image-to-Image\n  Hiding",
            "updated": "2023-11-25T13:44:37Z",
            "published": "2023-11-25T13:44:37Z",
            "summary": "As an emerging concept, steganography without embedding (SWE) hides a secret\nmessage without directly embedding it into a cover. Thus, SWE has the unique\nadvantage of being immune to typical steganalysis methods and can better\nprotect the secret message from being exposed. However, existing SWE methods\nare generally criticized for their poor payload capacity and low fidelity of\nrecovered secret messages. In this paper, we propose a novel\nsteganography-without-embedding technique, named DF-SWE, which addresses the\naforementioned drawbacks and produces diverse and natural stego images.\nSpecifically, DF-SWE employs a reversible circulation of double flow to build a\nreversible bijective transformation between the secret image and the generated\nstego image. Hence, it provides a way to directly generate stego images from\nsecret images without a cover image. Besides leveraging the invertible\nproperty, DF-SWE can invert a secret image from a generated stego image in a\nnearly lossless manner and increases the fidelity of extracted secret images.\nTo the best of our knowledge, DF-SWE is the first SWE method that can hide\nlarge images and multiple images into one image with the same size,\nsignificantly enhancing the payload capacity. According to the experimental\nresults, the payload capacity of DF-SWE achieves 24-72 BPP is 8000-16000 times\ncompared to its competitors while producing diverse images to minimize the\nexposure risk. Importantly, DF-SWE can be applied in the steganography of\nsecret images in various domains without requiring training data from the\ncorresponding domains. This domain-agnostic property suggests that DF-SWE can\n1) be applied to hiding private data and 2) be deployed in resource-limited\nsystems.",
            "author": [
                "Bingbing Song",
                "Derui Wang",
                "Tianwei Zhang",
                "Renyang Liu",
                "Yu Lin",
                "Wei Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15027v1",
                "http://arxiv.org/pdf/2311.15027v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15016v1",
            "title": "E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation",
            "updated": "2023-11-25T12:47:39Z",
            "published": "2023-11-25T12:47:39Z",
            "summary": "Achieving empathy is a crucial step toward humanized dialogue systems.\nCurrent approaches for empathetic dialogue generation mainly perceive an\nemotional label to generate an empathetic response conditioned on it, which\nsimply treat emotions independently, but ignore the intrinsic emotion\ncorrelation in dialogues, resulting in inaccurate emotion perception and\nunsuitable response generation. In this paper, we propose a novel emotion\ncorrelation enhanced empathetic dialogue generation framework, which\ncomprehensively realizes emotion correlation learning, utilization, and\nsupervising. Specifically, a multi-resolution emotion graph is devised to\ncapture context-based emotion interactions from different resolutions, further\nmodeling emotion correlation. Then we propose an emotion correlation enhanced\ndecoder, with a novel correlation-aware aggregation and soft/hard strategy,\nrespectively improving the emotion perception and response generation.\nExperimental results on the benchmark dataset demonstrate the superiority of\nour model in both empathetic perception and expression.",
            "author": [
                "Fengyi Fu",
                "Lei Zhang",
                "Quan Wang",
                "Zhendong Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15016v1",
                "http://arxiv.org/pdf/2311.15016v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.15011v1",
            "title": "VSCode: General Visual Salient and Camouflaged Object Detection with 2D\n  Prompt Learning",
            "updated": "2023-11-25T12:34:02Z",
            "published": "2023-11-25T12:34:02Z",
            "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are\nrelated yet distinct binary mapping tasks. These tasks involve multiple\nmodalities, sharing commonalities and unique cues. Existing research often\nemploys intricate task-specific specialist models, potentially leading to\nredundancy and suboptimal results. We introduce VSCode, a generalist model with\nnovel 2D prompt learning, to jointly address four SOD tasks and three COD\ntasks. We utilize VST as the foundation model and introduce 2D prompts within\nthe encoder-decoder architecture to learn domain and task-specific knowledge on\ntwo separate dimensions. A prompt discrimination loss helps disentangle\npeculiarities to benefit model optimization. VSCode outperforms\nstate-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot\ngeneralization to unseen tasks by combining 2D prompts, such as RGB-D COD.",
            "author": [
                "Ziyang Luo",
                "Nian Liu",
                "Wangbo Zhao",
                "Xuguang Yang",
                "Dingwen Zhang",
                "Deng-Ping Fan",
                "Fahad Khan",
                "Junwei Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.15011v1",
                "http://arxiv.org/pdf/2311.15011v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14995v1",
            "title": "Gohberg-Semencul Estimation of Toeplitz Structured Covariance Matrices\n  and Their Inverses",
            "updated": "2023-11-25T10:46:21Z",
            "published": "2023-11-25T10:46:21Z",
            "summary": "When only few data samples are accessible, utilizing structural prior\nknowledge is essential for estimating covariance matrices and their inverses.\nOne prominent example is knowing the covariance matrix to be Toeplitz\nstructured, which occurs when dealing with wide sense stationary (WSS)\nprocesses. This work introduces a novel class of positive definiteness ensuring\nlikelihood-based estimators for Toeplitz structured covariance matrices (CMs)\nand their inverses. In order to accomplish this, we derive positive\ndefiniteness enforcing constraint sets for the Gohberg-Semencul (GS)\nparameterization of inverse symmetric Toeplitz matrices. Motivated by the\nrelationship between the GS parameterization and autoregressive (AR) processes,\nwe propose hyperparameter tuning techniques, which enable our estimators to\ncombine advantages from state-of-the-art likelihood and non-parametric\nestimators. Moreover, we present a computationally cheap closed-form estimator,\nwhich is derived by maximizing an approximate likelihood. Due to the ensured\npositive definiteness, our estimators perform well for both the estimation of\nthe CM and the inverse covariance matrix (ICM). Extensive simulation results\nvalidate the proposed estimators' efficacy for several standard Toeplitz\nstructured CMs commonly employed in a wide range of applications.",
            "author": [
                "Benedikt B\u00f6ck",
                "Dominik Semmler",
                "Benedikt Fesl",
                "Michael Baur",
                "Wolfgang Utschick"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14995v1",
                "http://arxiv.org/pdf/2311.14995v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14994v1",
            "title": "Exploring Causal Learning through Graph Neural Networks: An In-depth\n  Review",
            "updated": "2023-11-25T10:46:06Z",
            "published": "2023-11-25T10:46:06Z",
            "summary": "In machine learning, exploring data correlations to predict outcomes is a\nfundamental task. Recognizing causal relationships embedded within data is\npivotal for a comprehensive understanding of system dynamics, the significance\nof which is paramount in data-driven decision-making processes. Beyond\ntraditional methods, there has been a surge in the use of graph neural networks\n(GNNs) for causal learning, given their capabilities as universal data\napproximators. Thus, a thorough review of the advancements in causal learning\nusing GNNs is both relevant and timely. To structure this review, we introduce\na novel taxonomy that encompasses various state-of-the-art GNN methods employed\nin studying causality. GNNs are further categorized based on their applications\nin the causality domain. We further provide an exhaustive compilation of\ndatasets integral to causal learning with GNNs to serve as a resource for\npractical study. This review also touches upon the application of causal\nlearning across diverse sectors. We conclude the review with insights into\npotential challenges and promising avenues for future exploration in this\nrapidly evolving field of machine learning.",
            "author": [
                "Simi Job",
                "Xiaohui Tao",
                "Taotao Cai",
                "Haoran Xie",
                "Lin Li",
                "Jianming Yong",
                "Qing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14994v1",
                "http://arxiv.org/pdf/2311.14994v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14979v1",
            "title": "Adaptive time delay based control of non-collocated oscillatory systems",
            "updated": "2023-11-25T09:45:14Z",
            "published": "2023-11-25T09:45:14Z",
            "summary": "Time delay based control, recently proposed for non-collocated fourth-order\nsystems, has several advantages over an observer-based state-feedback\ncancellation of the low-damped oscillations. In this paper, we discuss a\npractical infeasibility of such observer-based approach and bring forward the\napplication of the time delay based controller - simple in both the structure\nand design. A robust estimation of the output oscillation frequency is used and\nextended, in this work, by a bias cancellation that is required for tracking\nthe oscillatory load. This way, an adaptive tuning of the time delay based\ncontroller is realized which does not require knowledge of the mass and\nstiffness parameters. The results are demonstrated on the oscillatory\nexperimental setup with constraints in both the operation range and control\nvalue.",
            "author": [
                "Michael Ruderman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14979v1",
                "http://arxiv.org/pdf/2311.14979v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14977v1",
            "title": "Incorporating granularity bias as the margin into contrastive loss for\n  video captioning",
            "updated": "2023-11-25T09:38:24Z",
            "published": "2023-11-25T09:38:24Z",
            "summary": "Video captioning models easily suffer from long-tail distribution of phrases,\nwhich makes captioning models prone to generate vague sentences instead of\naccurate ones. However, existing debiasing strategies tend to export external\nknowledge to build dependency trees of words or refine frequency distribution\nby complex losses and extra input features, which lack interpretability and are\nhard to train. To mitigate the impact of granularity bias on the model, we\nintroduced a statistical-based bias extractor. This extractor quantifies the\ninformation content within sentences and videos, providing an estimate of the\nlikelihood that a video-sentence pair is affected by granularity bias.\nFurthermore, with the growing trend of integrating contrastive learning methods\ninto video captioning tasks, we use a bidirectional triplet loss to get more\nnegative samples in a batch. Subsequently, we incorporate the margin score into\nthe contrastive learning loss, establishing distinct training objectives for\nhead and tail sentences. This approach facilitates the model's training\neffectiveness on tail samples. Our simple yet effective loss, incorporating\nGranularity bias, is referred to as the Margin-Contrastive Loss (GMC Loss). The\nproposed model demonstrates state-of-the-art performance on MSRVTT with a CIDEr\nof 57.17, and MSVD, where CIDEr reaches up to 138.68.",
            "author": [
                "Jiayang Gu",
                "Fengming Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14977v1",
                "http://arxiv.org/pdf/2311.14977v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14975v1",
            "title": "Eliminating Domain Bias for Federated Learning in Representation Space",
            "updated": "2023-11-25T09:22:34Z",
            "published": "2023-11-25T09:22:34Z",
            "summary": "Recently, federated learning (FL) is popular for its privacy-preserving and\ncollaborative learning abilities. However, under statistically heterogeneous\nscenarios, we observe that biased data domains on clients cause a\nrepresentation bias phenomenon and further degenerate generic representations\nduring local training, i.e., the representation degeneration phenomenon. To\naddress these issues, we propose a general framework Domain Bias Eliminator\n(DBE) for FL. Our theoretical analysis reveals that DBE can promote\nbi-directional knowledge transfer between server and client, as it reduces the\ndomain discrepancy between server and client in representation space. Besides,\nextensive experiments on four datasets show that DBE can greatly improve\nexisting FL methods in both generalization and personalization abilities. The\nDBE-equipped FL method can outperform ten state-of-the-art personalized FL\nmethods by a large margin. Our code is public at\nhttps://github.com/TsingZ0/DBE.",
            "author": [
                "Jianqing Zhang",
                "Yang Hua",
                "Jian Cao",
                "Hao Wang",
                "Tao Song",
                "Zhengui Xue",
                "Ruhui Ma",
                "Haibing Guan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14975v1",
                "http://arxiv.org/pdf/2311.14975v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14968v2",
            "title": "Hide Your Model: A Parameter Transmission-free Federated Recommender\n  System",
            "updated": "2023-11-28T11:10:27Z",
            "published": "2023-11-25T08:59:45Z",
            "summary": "With the growing concerns regarding user data privacy, Federated Recommender\nSystem (FedRec) has garnered significant attention recently due to its\nprivacy-preserving capabilities. Existing FedRecs generally adhere to a\nlearning protocol in which a central server shares a global recommendation\nmodel with clients, and participants achieve collaborative learning by\nfrequently communicating the model's public parameters. Nevertheless, this\nlearning framework has two drawbacks that limit its practical usability: (1) It\nnecessitates a global-sharing recommendation model; however, in real-world\nscenarios, information related to the recommender model, including its\nalgorithm and parameters, constitutes the platforms' intellectual property.\nHence, service providers are unlikely to release such information actively. (2)\nThe communication costs of model parameter transmission are expensive since the\nmodel parameters are usually high-dimensional matrices. With the model size\nincreasing, the communication burden will be the bottleneck for such\ntraditional FedRecs.\n  Given the above limitations, this paper introduces a novel parameter\ntransmission-free federated recommendation framework that balances the\nprotection between users' data privacy and platforms' model privacy, namely\nPTF-FedRec. Specifically, participants in PTF-FedRec collaboratively exchange\nknowledge by sharing their predictions within a privacy-preserving mechanism.\nThrough this way, the central server can learn a recommender model without\ndisclosing its model parameters or accessing clients' raw data, preserving both\nthe server's model privacy and users' data privacy. Besides, since clients and\nthe central server only need to communicate prediction scores which are just a\nfew real numbers, the overhead is significantly reduced compared to traditional\nFedRecs.",
            "author": [
                "Wei Yuan",
                "Chaoqun Yang",
                "Liang Qu",
                "Quoc Viet Hung Nguyen",
                "Jianxin Li",
                "Hongzhi Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14968v2",
                "http://arxiv.org/pdf/2311.14968v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14960v1",
            "title": "Point Cloud Pre-training with Diffusion Models",
            "updated": "2023-11-25T08:10:05Z",
            "published": "2023-11-25T08:10:05Z",
            "summary": "Pre-training a model and then fine-tuning it on downstream tasks has\ndemonstrated significant success in the 2D image and NLP domains. However, due\nto the unordered and non-uniform density characteristics of point clouds, it is\nnon-trivial to explore the prior knowledge of point clouds and pre-train a\npoint cloud backbone. In this paper, we propose a novel pre-training method\ncalled Point cloud Diffusion pre-training (PointDif). We consider the point\ncloud pre-training task as a conditional point-to-point generation problem and\nintroduce a conditional point generator. This generator aggregates the features\nextracted by the backbone and employs them as the condition to guide the\npoint-to-point recovery from the noisy point cloud, thereby assisting the\nbackbone in capturing both local and global geometric priors as well as the\nglobal point density distribution of the object. We also present a recurrent\nuniform sampling optimization strategy, which enables the model to uniformly\nrecover from various noise levels and learn from balanced supervision. Our\nPointDif achieves substantial improvement across various real-world datasets\nfor diverse downstream tasks such as classification, segmentation and\ndetection. Specifically, PointDif attains 70.0% mIoU on S3DIS Area 5 for the\nsegmentation task and achieves an average improvement of 2.4% on ScanObjectNN\nfor the classification task compared to TAP. Furthermore, our pre-training\nframework can be flexibly applied to diverse point cloud backbones and bring\nconsiderable gains.",
            "author": [
                "Xiao Zheng",
                "Xiaoshui Huang",
                "Guofeng Mei",
                "Yuenan Hou",
                "Zhaoyang Lyu",
                "Bo Dai",
                "Wanli Ouyang",
                "Yongshun Gong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14960v1",
                "http://arxiv.org/pdf/2311.14960v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14934v1",
            "title": "Robust Graph Neural Networks via Unbiased Aggregation",
            "updated": "2023-11-25T05:34:36Z",
            "published": "2023-11-25T05:34:36Z",
            "summary": "The adversarial robustness of Graph Neural Networks (GNNs) has been\nquestioned due to the false sense of security uncovered by strong adaptive\nattacks despite the existence of numerous defenses. In this work, we delve into\nthe robustness analysis of representative robust GNNs and provide a unified\nrobust estimation point of view to understand their robustness and limitations.\nOur novel analysis of estimation bias motivates the design of a robust and\nunbiased graph signal estimator. We then develop an efficient Quasi-Newton\niterative reweighted least squares algorithm to solve the estimation problem,\nwhich unfolds as robust unbiased aggregation layers in GNNs with a theoretical\nconvergence guarantee. Our comprehensive experiments confirm the strong\nrobustness of our proposed model, and the ablation study provides a deep\nunderstanding of its advantages.",
            "author": [
                "Ruiqi Feng",
                "Zhichao Hou",
                "Tyler Derr",
                "Xiaorui Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14934v1",
                "http://arxiv.org/pdf/2311.14934v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14923v1",
            "title": "A method to compute the strength using bounds",
            "updated": "2023-11-25T03:58:48Z",
            "published": "2023-11-25T03:58:48Z",
            "summary": "A numbering $f$ of a graph $G$ of order $n$ is a labeling that assigns\ndistinct elements of the set $\\{1,2, \\ldots, n \\}$ to the vertices of $G$. The\nstrength $\\mathrm{str}\\left(G\\right) $ of $G$ is defined by $\\mathrm{str}\\left(\nG\\right) =\\min \\left\\{ \\mathrm{str}_{f}\\left( G\\right)\\left\\vert f\\text{ is a\nnumbering of }G\\right. \\right\\}$, where $\\mathrm{str}_{f}\\left( G\\right) =\\max\n\\left\\{ f\\left( u\\right) +f\\left( v\\right) \\left\\vert uv\\in E\\left( G\\right)\n\\right. \\right\\} $. A few lower and upper bounds for the strength are known\nand, although it is in general hard to compute the exact value for the\nstrength, a reasonable approach to this problem is to study for which graphs a\nlower bound and an upper bound for the strength coincide. In this paper, we\nstudy general conditions for graphs that allow us to determine which graphs\nhave the property that lower and upper bounds for the strength coincide and\nother graphs for which this approach is useless.",
            "author": [
                "Rikio Ichishima",
                "Francesc A. Muntaner-Batle",
                "Yukio Takahashi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14923v1",
                "http://arxiv.org/pdf/2311.14923v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C78 90C27"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14921v1",
            "title": "Graph Morphology of Non-Hermitian Bands",
            "updated": "2023-11-25T03:53:20Z",
            "published": "2023-11-25T03:53:20Z",
            "summary": "Non-Hermitian systems exhibit diverse graph patterns of energy spectra under\nopen boundary conditions. Here we present an algebraic framework to\ncomprehensively characterize the spectral geometry and graph topology of\nnon-Bloch bands. Using a locally defined potential function, we unravel the\nspectral-collapse mechanism from Bloch to non-Bloch bands, delicately placing\nthe spectral graph at the troughs of the potential landscape. The potential\nformalism deduces non-Bloch band theory and generates the density of states via\nPoisson equation. We further investigate the Euler-graph topology by\nclassifying spectral vertices based on their multiplicities and projections\nonto the generalized Brillouin zone. Through concrete models, we identify three\nelementary graph-topology transitions (UVY, PT-like, and self-crossing),\naccompanied by the emergence of singularities in the generalized Brillouin\nzone. Lastly, we unveil how to generally account for isolated edge states\noutside the spectral graph. Our work lays the cornerstone for exploring the\nversatile spectral geometry and graph topology of non-Hermitian non-Bloch\nbands.",
            "author": [
                "Yuncheng Xiong",
                "Haiping Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14921v1",
                "http://arxiv.org/pdf/2311.14921v1"
            ],
            "primary_category": "cond-mat.mes-hall",
            "category": [
                "cond-mat.mes-hall",
                "cond-mat.quant-gas",
                "math-ph",
                "math.MP",
                "physics.optics",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14917v1",
            "title": "Consolidate Viability and Information Theories for Task-Oriented\n  Communications: A Homeostasis Solution",
            "updated": "2023-11-25T03:32:18Z",
            "published": "2023-11-25T03:32:18Z",
            "summary": "The next generation of cellular networks, 6G, is expected to offer a range of\nexciting applications and services, including holographic communication,\nmachine-to-machine communication, and data sensing from millions of devices.\nThere is an incremental exhaustion of the spectral resources. It is crucial to\nefficiently manage these resources through value-driven approaches that\neliminate waste and continually enhance the communication process. These\nmanagement principles align with the Task-Oriented Communications (TOC)\nphilosophy. The aim is to allocate the minimum necessary communication resource\naccording to the receiver's objective and continuously improve the\ncommunication process. However, it is currently unclear how to build knowledge\nof the receiver's goal and operate accordingly for efficient-resource\nutilization. Our management approach combines viability theory and transfer\nentropy to ensure that the actor remains within a viable space as per their\ngoal and to gradually reduce the information exchange through knowledge\naccumulation. We discuss these theories in the context of TOC and examine their\napplication in the plant process control case. Finally, we provide insights\ninto future research directions from computational, performance, and protocol\nperspectives.",
            "author": [
                "Ozgur Ercetin",
                "Mohaned Chraiti",
                "Rustu Erciyes Karakaya"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14917v1",
                "http://arxiv.org/pdf/2311.14917v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.SY",
                "eess.SY",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14915v1",
            "title": "Equitable Coloring in 1-Planar Graphs",
            "updated": "2023-11-25T03:21:15Z",
            "published": "2023-11-25T03:21:15Z",
            "summary": "For every $r\\ge13$, we show every 1-planar graph $G$ with $\\Delta(G)\\le r$\nhas an equitable $r$-coloring.",
            "author": [
                "Daniel Cranston",
                "Reem Mahmoud"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14915v1",
                "http://arxiv.org/pdf/2311.14915v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14910v1",
            "title": "A latent linear model for nonlinear coupled oscillators on graphs",
            "updated": "2023-11-25T03:04:17Z",
            "published": "2023-11-25T03:04:17Z",
            "summary": "A system of coupled oscillators on an arbitrary graph is locally driven by\nthe tendency to mutual synchronization between nearby oscillators, but can and\noften exhibit nonlinear behavior on the whole graph. Understanding such\nnonlinear behavior has been a key challenge in predicting whether all\noscillators in such a system will eventually synchronize. In this paper, we\ndemonstrate that, surprisingly, such nonlinear behavior of coupled oscillators\ncan be effectively linearized in certain latent dynamic spaces. The key insight\nis that there is a small number of `latent dynamics filters', each with a\nspecific association with synchronizing and non-synchronizing dynamics on\nsubgraphs so that any observed dynamics on subgraphs can be approximated by a\nsuitable linear combination of such elementary dynamic patterns. Taking an\nensemble of subgraph-level predictions provides an interpretable predictor for\nwhether the system on the whole graph reaches global synchronization. We\npropose algorithms based on supervised matrix factorization to learn such\nlatent dynamics filters. We demonstrate that our method performs competitively\nin synchronization prediction tasks against baselines and black-box\nclassification algorithms, despite its simple and interpretable architecture.",
            "author": [
                "Agam Goyal",
                "Zhaoxing Wu",
                "Richard P. Yim",
                "Binhao Chen",
                "Zihong Xu",
                "Hanbaek Lyu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14910v1",
                "http://arxiv.org/pdf/2311.14910v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14909v1",
            "title": "Continual Referring Expression Comprehension via Dual Modular\n  Memorization",
            "updated": "2023-11-25T02:58:51Z",
            "published": "2023-11-25T02:58:51Z",
            "summary": "Referring Expression Comprehension (REC) aims to localize an image region of\na given object described by a natural-language expression. While promising\nperformance has been demonstrated, existing REC algorithms make a strong\nassumption that training data feeding into a model are given upfront, which\ndegrades its practicality for real-world scenarios. In this paper, we propose\nContinual Referring Expression Comprehension (CREC), a new setting for REC,\nwhere a model is learning on a stream of incoming tasks. In order to\ncontinuously improve the model on sequential tasks without forgetting prior\nlearned knowledge and without repeatedly re-training from a scratch, we propose\nan effective baseline method named Dual Modular Memorization (DMM), which\nalleviates the problem of catastrophic forgetting by two memorization modules:\nImplicit-Memory and Explicit-Memory. Specifically, the former module aims to\nconstrain drastic changes to important parameters learned on old tasks when\nlearning a new task; while the latter module maintains a buffer pool to\ndynamically select and store representative samples of each seen task for\nfuture rehearsal. We create three benchmarks for the new CREC setting, by\nrespectively re-splitting three widely-used REC datasets RefCOCO, RefCOCO+ and\nRefCOCOg into sequential tasks. Extensive experiments on the constructed\nbenchmarks demonstrate that our DMM method significantly outperforms other\nalternatives, based on two popular REC backbones. We make the source code and\nbenchmarks publicly available to foster future progress in this field:\nhttps://github.com/zackschen/DMM.",
            "author": [
                "Heng Tao Shen",
                "Cheng Chen",
                "Peng Wang",
                "Lianli Gao",
                "Meng Wang",
                "Jingkuan Song"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TIP.2022.3212317",
                "http://arxiv.org/abs/2311.14909v1",
                "http://arxiv.org/pdf/2311.14909v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14902v1",
            "title": "Parkinson Disease classification Using Contrastive Graph Cross-View\n  Learning with Multimodal Fusion of SPECT Images and Clinical Features",
            "updated": "2023-11-25T02:32:46Z",
            "published": "2023-11-25T02:32:46Z",
            "summary": "Parkinson's Disease (PD) is a neurodegenerative neurological disorder that\nimpacts movement and afflicts over 10 million people worldwide. Previous\nresearches have come up with deep learning models for predicting Parkinson's\ndisease primarily using medical images and didn't leverage the manifold\nstructure in the dataset. Our study introduces a multimodal approach with both\nimage and non-image features with a contrastive cross-view graph fusion for\nParkinson's disease classification. Specifically, we designed a multimodal\nco-attention module to integrate embeddings from two distinct graph views\nderived from low dimensional representation of images and clinical features,\nenabling the extraction of more stable and structured features from the\nmultiview data. Additionally, we have devised a simplified fusion method\nutilizing a contrastive loss for positive and negative pairs, to enhance the\nmodel's overall cross-view fusion learning capabilities. In our experiments,\nthe graph-view multimodal approach can achieve an accuracy rate of 91% and an\nAUC of 92.8% in five-fold cross-validation, and it also demonstrates superior\npredictive capabilities on non-image data as compared to methods that rely\nsolely on machine learning methods.",
            "author": [
                "Jun-En Ding",
                "Chien-Chin Hsu",
                "Feng Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14902v1",
                "http://arxiv.org/pdf/2311.14902v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.16180v1",
            "title": "Aiming to Minimize Alcohol-Impaired Road Fatalities: Utilizing\n  Fairness-Aware and Domain Knowledge-Infused Artificial Intelligence",
            "updated": "2023-11-25T02:05:39Z",
            "published": "2023-11-25T02:05:39Z",
            "summary": "Approximately 30% of all traffic fatalities in the United States are\nattributed to alcohol-impaired driving. This means that, despite stringent laws\nagainst this offense in every state, the frequency of drunk driving accidents\nis alarming, resulting in approximately one person being killed every 45\nminutes. The process of charging individuals with Driving Under the Influence\n(DUI) is intricate and can sometimes be subjective, involving multiple stages\nsuch as observing the vehicle in motion, interacting with the driver, and\nconducting Standardized Field Sobriety Tests (SFSTs). Biases have been observed\nthrough racial profiling, leading to some groups and geographical areas facing\nfewer DUI tests, resulting in many actual DUI incidents going undetected,\nultimately leading to a higher number of fatalities. To tackle this issue, our\nresearch introduces an Artificial Intelligence-based predictor that is both\nfairness-aware and incorporates domain knowledge to analyze DUI-related\nfatalities in different geographic locations. Through this model, we gain\nintriguing insights into the interplay between various demographic groups,\nincluding age, race, and income. By utilizing the provided information to\nallocate policing resources in a more equitable and efficient manner, there is\npotential to reduce DUI-related fatalities and have a significant impact on\nroad safety.",
            "author": [
                "Tejas Venkateswaran",
                "Sheikh Rabiul Islam",
                "Md Golam Moula Mehedi Hasan",
                "Mohiuddin Ahmed"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16180v1",
                "http://arxiv.org/pdf/2311.16180v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14898v1",
            "title": "HongTu: Scalable Full-Graph GNN Training on Multiple GPUs (via\n  communication-optimized CPU data offloading)",
            "updated": "2023-11-25T01:55:08Z",
            "published": "2023-11-25T01:55:08Z",
            "summary": "Full-graph training on graph neural networks (GNN) has emerged as a promising\ntraining method for its effectiveness. Full-graph training requires extensive\nmemory and computation resources. To accelerate this training process,\nresearchers have proposed employing multi-GPU processing. However the\nscalability of existing frameworks is limited as they necessitate maintaining\nthe training data for every layer in GPU memory. To efficiently train on large\ngraphs, we present HongTu, a scalable full-graph GNN training system running on\nGPU-accelerated platforms. HongTu stores vertex data in CPU memory and offloads\ntraining to GPUs. HongTu employs a memory-efficient full-graph training\nframework that reduces runtime memory consumption by using partition-based\ntraining and recomputation-caching-hybrid intermediate data management. To\naddress the issue of increased host-GPU communication caused by duplicated\nneighbor access among partitions, HongTu employs a deduplicated communication\nframework that converts the redundant host-GPU communication to efficient\ninter/intra-GPU data access. Further, HongTu uses a cost model-guided graph\nreorganization method to minimize communication overhead. Experimental results\non a 4XA100 GPU server show that HongTu effectively supports billion-scale\nfull-graph GNN training while reducing host-GPU data communication by 25%-71%.\nCompared to the full-graph GNN system DistGNN running on 16 CPU nodes, HongTu\nachieves speedups ranging from 7.8X to 20.2X. For small graphs where the\ntraining data fits into the GPUs, HongTu achieves performance comparable to\nexisting GPU-based GNN systems.",
            "author": [
                "Qiange Wang",
                "Yao Chen",
                "Weng-Fai Wong",
                "Bingsheng He"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3626733",
                "http://arxiv.org/abs/2311.14898v1",
                "http://arxiv.org/pdf/2311.14898v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14893v2",
            "title": "Persistent Dirac of Path and Hypergraph",
            "updated": "2023-12-03T15:50:45Z",
            "published": "2023-11-25T01:20:00Z",
            "summary": "This work introduces the development of path Dirac and hypergraph Dirac\noperators, along with an exploration of their persistence. These operators\nexcel in distinguishing between harmonic and non-harmonic spectra, offering\nvaluable insights into the subcomplexes within these structures. The paper\nshowcases the functionality of these operators through a series of examples in\nvarious contexts. An important facet of this research involves examining the\noperators' sensitivity to filtration, emphasizing their capacity to adapt to\ntopological changes. The paper also explores a significant application of\npersistent path Dirac and persistent hypergraph Dirac in the field of molecular\nscience, specifically in the analysis of molecular structures. The study\nintroduces strict preorders derived from molecular structures, which generate\ngraphs and digraphs with intricate path structures. The depth of information\nwithin these path complexes reflects the complexity of different preorder\nclasses influenced by molecular structures. This characteristic underscores the\neffectiveness of these tools in the realm of topological data analysis.",
            "author": [
                "Faisal Suwayyid",
                "Guo-Wei Wei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14893v2",
                "http://arxiv.org/pdf/2311.14893v2"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "62R40 (Primary), 55N31 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14888v1",
            "title": "A simple, static and stage mounted direct electron detector based\n  electron backscatter diffraction system",
            "updated": "2023-11-25T00:48:35Z",
            "published": "2023-11-25T00:48:35Z",
            "summary": "To engineer the next generation of advanced materials we must understand\ntheir microstructure, and this requires microstructural characterization. This\ncan be achieved through the collection of high contrast, data rich, and\ninsightful microstructural maps. Electron backscatter diffraction (EBSD) has\nemerged as a popular tool available within the scanning electron microscope\n(SEM), where maps are realized through the repeat capture and analysis of\nKikuchi diffraction patterns. Typical commercial EBSD systems require large and\nsophisticated detectors that are mounted on the side of the SEM vacuum chamber\nwhich can be limiting in terms of widespread access to the technique. In this\nwork, we present an alternative open-hardware solution based upon a compact\nEBSD system with a simple, static geometry that uses an off-the-shelf direct\nelectron detector co-mounted with a sample. This simple stage is easy to\nmanufacture and improves our knowledge of the diffraction geometry\nsignificantly. Microscope and detector control is achieved through software\napplication programming interface (API) integration. After pattern capture,\nanalysis of the diffraction patterns is performed using open-source analysis\nwithin AstroEBSD. To demonstrate the potential of this set up, we present two\nsimple EBSD experiments using line scan and mapping. We hope that the present\nsystem can inspire simpler EBSD system design for widespread access to the EBSD\ntechnique and promote the use of open-source software and hardware in the\nworkflow of EBSD experiments.",
            "author": [
                "Tianbi Zhang",
                "T. Ben Britton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14888v1",
                "http://arxiv.org/pdf/2311.14888v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14877v1",
            "title": "Triadic percolation induces dynamical topological patterns in\n  higher-order networks",
            "updated": "2023-11-25T00:00:30Z",
            "published": "2023-11-25T00:00:30Z",
            "summary": "Triadic interactions are higher-order interactions that occur when a set of\nnodes affects the interaction between two other nodes. Examples of triadic\ninteractions are present in the brain when glia modulate the synaptic signals\namong neuron pairs or when interneuron axon-axonic synapses enable presynaptic\ninhibition and facilitation, and in ecosystems when one or more species can\naffect the interaction among two other species. On random graphs, triadic\npercolation has been recently shown to turn percolation into a fully-fledged\ndynamical process in which the size of the giant component undergoes a route to\nchaos. However, in many real cases, triadic interactions are local and occur on\nspatially embedded networks. Here we show that triadic interactions in spatial\nnetworks induce a very complex spatio-temporal modulation of the giant\ncomponent which gives rise to triadic percolation patterns with significantly\ndifferent topology. We classify the observed patterns (stripes, octopus, and\nsmall clusters) with topological data analysis and we assess their information\ncontent (entropy and complexity). Moreover, we illustrate the multistability of\nthe dynamics of the triadic percolation patterns and we provide a comprehensive\nphase diagram of the model. These results open new perspectives in percolation\nas they demonstrate that in presence of spatial triadic interactions, the giant\ncomponent can acquire a time-varying topology. Hence, this work provides a\ntheoretical framework that can be applied to model realistic scenarios in which\nthe giant component is time-dependent as in neuroscience.",
            "author": [
                "Ana P. Mill\u00e1n",
                "Hanlin Sun",
                "Joaqu\u00ecn J. Torres",
                "Ginestra Bianconi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14877v1",
                "http://arxiv.org/pdf/2311.14877v1"
            ],
            "primary_category": "nlin.AO",
            "category": [
                "nlin.AO",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech",
                "nlin.CD",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14874v1",
            "title": "Advancing Fluid-Based Thermal Management Systems Design: Leveraging\n  Graph Neural Networks for Graph Regression and Efficient Enumeration\n  Reduction",
            "updated": "2023-11-24T23:51:53Z",
            "published": "2023-11-24T23:51:53Z",
            "summary": "In this research, we developed a graph-based framework to represent various\naspects of optimal thermal management system design, with the aim of rapidly\nand efficiently identifying optimal design candidates. Initially, the\ngraph-based framework is utilized to generate diverse thermal management system\narchitectures. The dynamics of these system architectures are modeled under\nvarious loading conditions, and an open-loop optimal controller is employed to\ndetermine each system's optimal performance. These modeled cases constitute the\ndataset, with the corresponding optimal performance values serving as the\nlabels for the data. In the subsequent step, a Graph Neural Network (GNN) model\nis trained on 30% of the labeled data to predict the systems' performance,\neffectively addressing a regression problem. Utilizing this trained model, we\nestimate the performance values for the remaining 70% of the data, which serves\nas the test set. In the third step, the predicted performance values are\nemployed to rank the test data, facilitating prioritized evaluation of the\ndesign scenarios. Specifically, a small subset of the test data with the\nhighest estimated ranks undergoes evaluation via the open-loop optimal control\nsolver. This targeted approach concentrates on evaluating higher-ranked designs\nidentified by the GNN, replacing the exhaustive search (enumeration-based) of\nall design cases. The results demonstrate a significant average reduction of\nover 92% in the number of system dynamic modeling and optimal control analyses\nrequired to identify optimal design scenarios.",
            "author": [
                "Saeid Bayat",
                "Nastaran Shahmansouri",
                "Satya RT Peddada",
                "Alex Tessier",
                "Adrian Butscher",
                "James T Allison"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14874v1",
                "http://arxiv.org/pdf/2311.14874v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.LG",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14868v1",
            "title": "A graph-theoretic remark on Stieltjes moment sequences",
            "updated": "2023-11-24T23:22:29Z",
            "published": "2023-11-24T23:22:29Z",
            "summary": "For any integer $k\\geq 1,$ define $\\mathcal{L}_k: \\mathbb{R}^\\mathbb{N}\\to\n\\mathbb{R}^\\mathbb{N}$ by $(a_n)_{n\\in\\mathbb{N}}\\mapsto\n(a'_n)_{n\\in\\mathbb{N}}$ where $a'_n=\\det(a_{n+i+j})_{i,j=0}^{k-1}$.\nPreviously, Zhu showed that $\\mathcal{L}_k$ preserves the Stieltjes moment (SM)\nproperty of sequences (Proc.\\ Am.\\ Math.\\ Soc., 2019). The proof used the\ncharacterization of SM sequences in terms of positive semidefinite Hankel\nmatrices. In this note, we give another proof by viewing SM sequences as\nweighted enumerations of closed walks on $\\mathbb{N}$. Our proof is essentially\na double-counting argument that views a $k$-tuple of non-crossing Dyck paths as\na single closed walk on some bipartite subgraph of $\\mathbb{N}^k.$",
            "author": [
                "Bryan Park"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14868v1",
                "http://arxiv.org/pdf/2311.14868v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "11B83, 15B05, 33B15, 05A20"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14865v1",
            "title": "Improving Cross-Domain Hate Speech Generalizability with Emotion\n  Knowledge",
            "updated": "2023-11-24T23:00:36Z",
            "published": "2023-11-24T23:00:36Z",
            "summary": "Reliable automatic hate speech (HS) detection systems must adapt to the\nin-flow of diverse new data to curtail hate speech. However, hate speech\ndetection systems commonly lack generalizability in identifying hate speech\ndissimilar to data used in training, impeding their robustness in real-world\ndeployments. In this work, we propose a hate speech generalization framework\nthat leverages emotion knowledge in a multitask architecture to improve the\ngeneralizability of hate speech detection in a cross-domain setting. We\ninvestigate emotion corpora with varying emotion categorical scopes to\ndetermine the best corpus scope for supplying emotion knowledge to foster\ngeneralized hate speech detection. We further assess the relationship between\nusing pretrained Transformers models adapted for hate speech and its effect on\nour emotion-enriched hate speech generalization model. We perform extensive\nexperiments on six publicly available datasets sourced from different online\ndomains and show that our emotion-enriched HS detection generalization method\ndemonstrates consistent generalization improvement in cross-domain evaluation,\nincreasing generalization performance up to 18.1% and average cross-domain\nperformance up to 8.5%, according to the F1 measure.",
            "author": [
                "Shi Yin Hong",
                "Susan Gauch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14865v1",
                "http://arxiv.org/pdf/2311.14865v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14864v1",
            "title": "Effective Structural Encodings via Local Curvature Profiles",
            "updated": "2023-11-24T22:42:37Z",
            "published": "2023-11-24T22:42:37Z",
            "summary": "Structural and Positional Encodings can significantly improve the performance\nof Graph Neural Networks in downstream tasks. Recent literature has begun to\nsystematically investigate differences in the structural properties that these\napproaches encode, as well as performance trade-offs between them. However, the\nquestion of which structural properties yield the most effective encoding\nremains open. In this paper, we investigate this question from a geometric\nperspective. We propose a novel structural encoding based on discrete Ricci\ncurvature (Local Curvature Profiles, short LCP) and show that it significantly\noutperforms existing encoding approaches. We further show that combining local\nstructural encodings, such as LCP, with global positional encodings improves\ndownstream performance, suggesting that they capture complementary geometric\ninformation. Finally, we compare different encoding types with\n(curvature-based) rewiring techniques. Rewiring has recently received a surge\nof interest due to its ability to improve the performance of Graph Neural\nNetworks by mitigating over-smoothing and over-squashing effects. Our results\nsuggest that utilizing curvature information for structural encodings delivers\nsignificantly larger performance increases than rewiring.",
            "author": [
                "Lukas Fesser",
                "Melanie Weber"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14864v1",
                "http://arxiv.org/pdf/2311.14864v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14848v1",
            "title": "Robotic Detection and Estimation of Single Scuba Diver Respiration Rate\n  from Underwater Video",
            "updated": "2023-11-24T21:42:57Z",
            "published": "2023-11-24T21:42:57Z",
            "summary": "Human respiration rate (HRR) is an important physiological metric for\ndiagnosing a variety of health conditions from stress levels to heart\nconditions. Estimation of HRR is well-studied in controlled terrestrial\nenvironments, yet robotic estimation of HRR as an indicator of diver stress in\nunderwater for underwater human robot interaction (UHRI) scenarios is to our\nknowledge unexplored. We introduce a novel system for robotic estimation of HRR\nfrom underwater visual data by utilizing bubbles from exhalation cycles in\nscuba diving to time respiration rate. We introduce a fuzzy labeling system\nthat utilizes audio information to label a diverse dataset of diver breathing\ndata on which we compare four different methods for characterizing the presence\nof bubbles in images. Ultimately we show that our method is effective at\nestimating HRR by comparing the respiration rate output with human analysts.",
            "author": [
                "Demetrious T. Kutzke",
                "Junaed Sattar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14848v1",
                "http://arxiv.org/pdf/2311.14848v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14845v1",
            "title": "Lightweight Public Key Encryption in Post-Quantum Computing Era",
            "updated": "2023-11-24T21:06:42Z",
            "published": "2023-11-24T21:06:42Z",
            "summary": "Confidentiality in our digital world is based on the security of\ncryptographic algorithms. These are usually executed transparently in the\nbackground, with people often relying on them without further knowledge. In the\ncourse of technological progress with quantum computers, the protective\nfunction of common encryption algorithms is threatened. This particularly\naffects public-key methods such as RSA and DH based on discrete logarithms and\nprime factorization. Our concept describes the transformation of a classical\nasymmetric encryption method to a modern complexity class. Thereby the approach\nof Cramer-Shoup is put on the new basis of elliptic curves. The system is\nprovable cryptographically strong, especially against adaptive\nchosen-ciphertext attacks. In addition, the new method features small key\nlengths, making it suitable for Internet-of-Things. It represents an\nintermediate step towards an encryption scheme based on isogeny elliptic\ncurves. This approach shows a way to a secure encryption scheme for the\npost-quantum computing era.",
            "author": [
                "Peter Hillmann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14845v1",
                "http://arxiv.org/pdf/2311.14845v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CC",
                "cs.DS",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14833v1",
            "title": "Surface Scattering Expansion of the Casimir-Polder Interaction for\n  Magneto-dielectric Bodies: Convergence Properties for Insulators, Conductors\n  and Semiconductors",
            "updated": "2023-11-24T20:10:00Z",
            "published": "2023-11-24T20:10:00Z",
            "summary": "Fluctuation induced forces are a hallmark of the interplay of fluctuations\nand geometry. We recently proved the existence of a multi-parametric family of\nexact representations of Casimir and Casimir-Polder interactions between bodies\nof arbitrary shape and material composition, admitting a multiple scattering\nexpansion (MSE) as a sequence of inter- and intra-body multiple wave\nscatterings [G. Bimonte, T. Emig, Phys. Rev. A 108, 052807 (2023)]. The\napproach requires no knowledge of the scattering amplitude (T-matrix) of the\nbodies. Here we investigate the convergence properties of the MSE for the\nCasimir-Polder interaction of a polarizable particle with a macroscopic body.\nWe consider representative materials from different classes, such as\ninsulators, conductors and semiconductors. Using a sphere and a cylinder as\nbenchmarks, we demonstrate that the MSE can be used to efficiently and\naccurately compute the Casimir-Polder interaction for bodies with smooth\nsurfaces.",
            "author": [
                "G. Bimonte",
                "T. Emig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14833v1",
                "http://arxiv.org/pdf/2311.14833v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14811v1",
            "title": "The Message Complexity of Distributed Graph Optimization",
            "updated": "2023-11-24T19:09:06Z",
            "published": "2023-11-24T19:09:06Z",
            "summary": "The message complexity of a distributed algorithm is the total number of\nmessages sent by all nodes over the course of the algorithm. This paper studies\nthe message complexity of distributed algorithms for fundamental graph\noptimization problems. We focus on four classical graph optimization problems:\nMaximum Matching (MaxM), Minimum Vertex Cover (MVC), Minimum Dominating Set\n(MDS), and Maximum Independent Set (MaxIS). In the sequential setting, these\nproblems are representative of a wide spectrum of hardness of approximation.\nWhile there has been some progress in understanding the round complexity of\ndistributed algorithms (for both exact and approximate versions) for these\nproblems, much less is known about their message complexity and its relation\nwith the quality of approximation. We almost fully quantify the message\ncomplexity of distributed graph optimization by showing the following\nresults...[see paper for full abstract]",
            "author": [
                "Fabien Dufoulon",
                "Shreyas Pai",
                "Gopal Pandurangan",
                "Sriram V. Pemmaraju",
                "Peter Robinson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14811v1",
                "http://arxiv.org/pdf/2311.14811v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14803v1",
            "title": "Black hole spectroscopy beyond Kerr: agnostic and theory-based tests\n  with next-generation interferometers",
            "updated": "2023-11-24T19:00:03Z",
            "published": "2023-11-24T19:00:03Z",
            "summary": "Black hole spectroscopy is a clean and powerful tool to test gravity in the\nstrong-field regime and to probe the nature of compact objects. Next-generation\nground-based detectors, such as the Einstein Telescope and Cosmic Explorer,\nwill observe thousands of binary black hole mergers with large signal-to-noise\nratios, allowing for accurate measurements of the remnant black hole\nquasinormal mode frequencies and damping times. In previous work we developed\nan observable-based parametrization of the quasinormal mode spectrum of\nspinning black holes beyond general relativity (ParSpec). In this paper we use\nthis parametrization to ask: can next-generation detectors detect or constrain\ndeviations from the Kerr spectrum by stacking multiple observations of binary\nmergers from astrophysically motivated populations? We focus on two families of\ntests: (i) agnostic (null) tests, and (ii) theory-based tests, which make use\nof quasinormal frequency calculations in specific modified theories of gravity.\nWe consider in particular two quadratic gravity theories\n(Einstein-scalar-Gauss-Bonnet and dynamical Chern-Simons gravity) and various\neffective field theory-based extensions of general relativity. We find that\nrobust inference of hypothetical corrections to general relativity requires\npushing the slow-rotation expansion to high orders. Even when high-order\nexpansions are available, ringdown observations alone may not be sufficient to\nmeasure deviations from the Kerr spectrum for theories with dimensionful\ncoupling constants. This is because the constraints are dominated by \"light\"\nblack hole remnants, and only few of them have sufficiently high\nsignal-to-noise ratio in the ringdown. Black hole spectroscopy with\nnext-generation detectors may be able to set tight constraints on theories with\ndimensionless coupling, as long as we assume prior knowledge of the mass and\nspin of the remnant black hole.",
            "author": [
                "Andrea Maselli",
                "Sophia Yi",
                "Lorenzo Pierini",
                "Vania Vellucci",
                "Luca Reali",
                "Leonardo Gualtieri",
                "Emanuele Berti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14803v1",
                "http://arxiv.org/pdf/2311.14803v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14656v2",
            "title": "Charting New Territories: Exploring the Geographic and Geospatial\n  Capabilities of Multimodal LLMs",
            "updated": "2023-11-30T18:56:42Z",
            "published": "2023-11-24T18:46:02Z",
            "summary": "Multimodal large language models (MLLMs) have shown remarkable capabilities\nacross a broad range of tasks but their knowledge and abilities in the\ngeographic and geospatial domains are yet to be explored, despite potential\nwide-ranging benefits to navigation, environmental research, urban development,\nand disaster response. We conduct a series of experiments exploring various\nvision capabilities of MLLMs within these domains, particularly focusing on the\nfrontier model GPT-4V, and benchmark its performance against open-source\ncounterparts. Our methodology involves challenging these models with a\nsmall-scale geographic benchmark consisting of a suite of visual tasks, testing\ntheir abilities across a spectrum of complexity. The analysis uncovers not only\nwhere such models excel, including instances where they outperform humans, but\nalso where they falter, providing a balanced view of their capabilities in the\ngeographic domain. To enable the comparison and evaluation of future models,\nour benchmark will be publicly released.",
            "author": [
                "Jonathan Roberts",
                "Timo L\u00fcddecke",
                "Rehan Sheikh",
                "Kai Han",
                "Samuel Albanie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14656v2",
                "http://arxiv.org/pdf/2311.14656v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14654v1",
            "title": "JetLOV: Enhancing Jet Tree Tagging through Neural Network Learning of\n  Optimal LundNet Variables",
            "updated": "2023-11-24T18:38:13Z",
            "published": "2023-11-24T18:38:13Z",
            "summary": "Machine learning has played a pivotal role in advancing physics, with deep\nlearning notably contributing to solving complex classification problems such\nas jet tagging in the field of jet physics. In this experiment, we aim to\nharness the full potential of neural networks while acknowledging that, at\ntimes, we may lose sight of the underlying physics governing these models.\nNevertheless, we demonstrate that we can achieve remarkable results obscuring\nphysics knowledge and relying completely on the model's outcome. We introduce\nJetLOV, a composite comprising two models: a straightforward multilayer\nperceptron (MLP) and the well-established LundNet. Our study reveals that we\ncan attain comparable jet tagging performance without relying on the\npre-computed LundNet variables. Instead, we allow the network to autonomously\nlearn an entirely new set of variables, devoid of a priori knowledge of the\nunderlying physics. These findings hold promise, particularly in addressing the\nissue of model dependence, which can be mitigated through generalization and\ntraining on diverse data sets.",
            "author": [
                "Mauricio A. Diaz",
                "Giorgio Cerro",
                "Jacan Chaplais",
                "Srinandan Dasmahapatra",
                "Stefano Moretti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14654v1",
                "http://arxiv.org/pdf/2311.14654v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14650v2",
            "title": "GVEL: Fast Graph Loading in Edgelist and Compressed Sparse Row (CSR)\n  formats",
            "updated": "2023-11-27T09:24:50Z",
            "published": "2023-11-24T18:32:34Z",
            "summary": "Efficient IO techniques are crucial in high-performance graph processing\nframeworks like Gunrock and Hornet, as fast graph loading is essential to\nminimize processing time and reduce system/cloud usage charges. This research\nstudy presents approaches for efficiently reading an Edgelist from a text file\nand converting it to a Compressed Sparse Row (CSR) representation. On a server\nwith dual 16-core Intel Xeon Gold 6226R processors and MegaRAID SAS-3 storage,\nour approach, which we term as GVEL, outperforms Hornet, Gunrock, and PIGO by\nsignificant margins in CSR reading, exhibiting an average speedup of 78x, 112x,\nand 1.8x, respectively. For Edgelist reading, GVEL is 2.6x faster than PIGO on\naverage, and achieves a Edgelist read rate of 1.9 billion edges/s. For every\ndoubling of threads, GVEL improves performance at an average rate of 1.9x and\n1.7x for reading Edgelist and reading CSR respectively.",
            "author": [
                "Subhajit Sahu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14650v2",
                "http://arxiv.org/pdf/2311.14650v2"
            ],
            "primary_category": "cs.PF",
            "category": [
                "cs.PF",
                "B.8.2"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14649v1",
            "title": "Learning in Deep Factor Graphs with Gaussian Belief Propagation",
            "updated": "2023-11-24T18:31:11Z",
            "published": "2023-11-24T18:31:11Z",
            "summary": "We propose an approach to do learning in Gaussian factor graphs. We treat all\nrelevant quantities (inputs, outputs, parameters, latents) as random variables\nin a graphical model, and view both training and prediction as inference\nproblems with different observed nodes. Our experiments show that these\nproblems can be efficiently solved with belief propagation (BP), whose updates\nare inherently local, presenting exciting opportunities for distributed and\nasynchronous training. Our approach can be scaled to deep networks and provides\na natural means to do continual learning: use the BP-estimated parameter\nmarginals of the current task as parameter priors for the next. On a video\ndenoising task we demonstrate the benefit of learnable parameters over a\nclassical factor graph approach and we show encouraging performance of deep\nfactor graphs for continual image classification on MNIST.",
            "author": [
                "Seth Nabarro",
                "Mark van der Wilk",
                "Andrew J Davison"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14649v1",
                "http://arxiv.org/pdf/2311.14649v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14645v1",
            "title": "A General Framework for User-Guided Bayesian Optimization",
            "updated": "2023-11-24T18:27:26Z",
            "published": "2023-11-24T18:27:26Z",
            "summary": "The optimization of expensive-to-evaluate black-box functions is prevalent in\nvarious scientific disciplines. Bayesian optimization is an automatic, general\nand sample-efficient method to solve these problems with minimal knowledge of\nthe underlying function dynamics. However, the ability of Bayesian optimization\nto incorporate prior knowledge or beliefs about the function at hand in order\nto accelerate the optimization is limited, which reduces its appeal for\nknowledgeable practitioners with tight budgets. To allow domain experts to\ncustomize the optimization routine, we propose ColaBO, the first\nBayesian-principled framework for incorporating prior beliefs beyond the\ntypical kernel structure, such as the likely location of the optimizer or the\noptimal value. The generality of ColaBO makes it applicable across different\nMonte Carlo acquisition functions and types of user beliefs. We empirically\ndemonstrate ColaBO's ability to substantially accelerate optimization when the\nprior information is accurate, and to retain approximately default performance\nwhen it is misleading.",
            "author": [
                "Carl Hvarfner",
                "Frank Hutter",
                "Luigi Nardi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14645v1",
                "http://arxiv.org/pdf/2311.14645v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14641v1",
            "title": "Neuromorphic Intermediate Representation: A Unified Instruction Set for\n  Interoperable Brain-Inspired Computing",
            "updated": "2023-11-24T18:15:59Z",
            "published": "2023-11-24T18:15:59Z",
            "summary": "Spiking neural networks and neuromorphic hardware platforms that emulate\nneural dynamics are slowly gaining momentum and entering main-stream usage.\nDespite a well-established mathematical foundation for neural dynamics, the\nimplementation details vary greatly across different platforms.\nCorrespondingly, there are a plethora of software and hardware implementations\nwith their own unique technology stacks. Consequently, neuromorphic systems\ntypically diverge from the expected computational model, which challenges the\nreproducibility and reliability across platforms. Additionally, most\nneuromorphic hardware is limited by its access via a single software frameworks\nwith a limited set of training procedures. Here, we establish a common\nreference-frame for computations in neuromorphic systems, dubbed the\nNeuromorphic Intermediate Representation (NIR). NIR defines a set of\ncomputational primitives as idealized continuous-time hybrid systems that can\nbe composed into graphs and mapped to and from various neuromorphic technology\nstacks. By abstracting away assumptions around discretization and hardware\nconstraints, NIR faithfully captures the fundamental computation, while\nsimultaneously exposing the exact differences between the evaluated\nimplementation and the idealized mathematical formalism. We reproduce three NIR\ngraphs across 7 neuromorphic simulators and 4 hardware platforms, demonstrating\nsupport for an unprecedented number of neuromorphic systems. With NIR, we\ndecouple the evolution of neuromorphic hardware and software, ultimately\nincreasing the interoperability between platforms and improving accessibility\nto neuromorphic technologies. We believe that NIR is an important step towards\nthe continued study of brain-inspired hardware and bottom-up approaches aimed\nat an improved understanding of the computational underpinnings of nervous\nsystems.",
            "author": [
                "Jens E. Pedersen",
                "Steven Abreu",
                "Matthias Jobst",
                "Gregor Lenz",
                "Vittorio Fra",
                "Felix C. Bauer",
                "Dylan R. Muir",
                "Peng Zhou",
                "Bernhard Vogginger",
                "Kade Heckel",
                "Gianvito Urgese",
                "Sadasivan Shankar",
                "Terrence C. Stewart",
                "Jason K. Eshraghian",
                "Sadique Sheik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14641v1",
                "http://arxiv.org/pdf/2311.14641v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14634v1",
            "title": "New Bounds on the Local and Global Edge-length Ratio of Planar Graphs",
            "updated": "2023-11-24T18:06:07Z",
            "published": "2023-11-24T18:06:07Z",
            "summary": "The \\emph{local edge-length ratio} of a planar straight-line drawing $\\Gamma$\nis the largest ratio between the lengths of any pair of edges of $\\Gamma$ that\nshare a common vertex. The \\emph{global edge-length ratio} of $\\Gamma$ is the\nlargest ratio between the lengths of any pair of edges of $\\Gamma$. The local\n(global) edge-length ratio of a planar graph is the infimum over all local\n(global) edge-length ratios of its planar straight-line drawings. We show that\nthere exist planar graphs with $n$ vertices whose local edge-length ratio is\n$\\Omega(\\sqrt{n})$. We then show a technique to establish upper bounds on the\nglobal (and hence local) edge-length ratio of planar graphs and~apply~it to\nHalin graphs and to other families of graphs having outerplanarity two.",
            "author": [
                "Emilio Di Giacomo",
                "Walter Didimo",
                "Giuseppe Liotta",
                "Henk Meijer",
                "Fabrizio Montecchiani",
                "Stephen Wismath"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14634v1",
                "http://arxiv.org/pdf/2311.14634v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14631v2",
            "title": "CatVersion: Concatenating Embeddings for Diffusion-Based Text-to-Image\n  Personalization",
            "updated": "2023-11-30T14:42:07Z",
            "published": "2023-11-24T17:55:10Z",
            "summary": "We propose CatVersion, an inversion-based method that learns the personalized\nconcept through a handful of examples. Subsequently, users can utilize text\nprompts to generate images that embody the personalized concept, thereby\nachieving text-to-image personalization. In contrast to existing approaches\nthat emphasize word embedding learning or parameter fine-tuning for the\ndiffusion model, which potentially causes concept dilution or overfitting, our\nmethod concatenates embeddings on the feature-dense space of the text encoder\nin the diffusion model to learn the gap between the personalized concept and\nits base class, aiming to maximize the preservation of prior knowledge in\ndiffusion models while restoring the personalized concepts. To this end, we\nfirst dissect the text encoder's integration in the image generation process to\nidentify the feature-dense space of the encoder. Afterward, we concatenate\nembeddings on the Keys and Values in this space to learn the gap between the\npersonalized concept and its base class. In this way, the concatenated\nembeddings ultimately manifest as a residual on the original attention output.\nTo more accurately and unbiasedly quantify the results of personalized image\ngeneration, we improve the CLIP image alignment score based on masks.\nQualitatively and quantitatively, CatVersion helps to restore personalization\nconcepts more faithfully and enables more robust editing.",
            "author": [
                "Ruoyu Zhao",
                "Mingrui Zhu",
                "Shiyin Dong",
                "Nannan Wang",
                "Xinbo Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14631v2",
                "http://arxiv.org/pdf/2311.14631v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14622v1",
            "title": "Resource-efficient shadow tomography using equatorial measurements",
            "updated": "2023-11-24T17:33:44Z",
            "published": "2023-11-24T17:33:44Z",
            "summary": "We propose a resource-efficient shadow-tomography scheme using\nequatorial-stabilizer measurements generated from subsets of Clifford\nunitaries. For $n$-qubit systems, equatorial-stabilizer-based shadow-tomography\nschemes can estimate $M$ observables (up to an additive error $\\varepsilon$)\nusing $\\mathcal{O}(\\log(M),\\mathrm{poly}(n),1/\\varepsilon^2)$ sampling copies\nfor a large class of observables, including those with traceless parts\npossessing polynomially-bounded Frobenius norms. For arbitrary quantum-state\nobservables, sampling complexity becomes $n$-independent --\n$\\mathcal{O}(\\log(M),1/\\varepsilon^2)$. Our scheme only requires an $n$-depth\ncontrolled-$Z$ (CZ) circuit [$\\mathcal{O}(n^2)$ CZ gates] and Pauli\nmeasurements per sampling copy, exhibiting a smaller maximal gate count\nrelative to previously-known randomized-Clifford-based proposals.\nImplementation-wise, the maximal circuit depth is reduced to\n$\\frac{n}{2}+\\mathcal{O}(\\log(n))$ with controlled-NOT (CNOT) gates.\nAlternatively, our scheme is realizable with $2n$-depth circuits comprising\n$O(n^2)$ nearest-neighboring CNOT gates, with possible further gate-count\nimprovements. We numerically confirm our theoretically-derived\nshadow-tomographic sampling complexities with random pure states and multiqubit\ngraph states. Finally, we numerically demonstrate that\nequatorial-stabilizer-based shadow tomography is more noise-tolerant than\nrandomized-Clifford-based schemes in terms of average gate fidelity and state\nverification for GHZ and W states.",
            "author": [
                "Guedong Park",
                "Yong Siah Teo",
                "Hyunseok Jeong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14622v1",
                "http://arxiv.org/pdf/2311.14622v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14608v1",
            "title": "Spectrometer to Explore Isotopologues of Lunar Volatiles on Luna-27\n  Lander",
            "updated": "2023-11-24T17:03:23Z",
            "published": "2023-11-24T17:03:23Z",
            "summary": "The study of volatiles and the search for water are the primary objectives of\nthe Luna-27 mission, which is planned to land on the south pole of the Moon in\n2028. Here we present the tunable Diode Laser Spectrometer (DLS-L) that will be\nonboard the lander. The DLS-L will perform isotopic analysis of volatiles that\nare pyrolytically evolved from regolith. This article dives into the design of\nthe spectrometer and the characterisation of isotopic signature retrieval. We\nlook forward to expanding our knowledge of Lunar geochemistry by measuring D/H,\n18O/17O/16O, 13C/12C ratios in situ, which could be the one-of-a-kind direct\nstudy of the lunar soil isotopy without sample contamination.",
            "author": [
                "Viacheslav Meshcherinov",
                "Iskander Gazizov",
                "Viktor Kazakov",
                "Maxim Spiridonov",
                "Yuri Lebedev",
                "Imant Vinogradov",
                "Mikhail Gerasimov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14608v1",
                "http://arxiv.org/pdf/2311.14608v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "astro-ph.EP",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14602v1",
            "title": "Urban Flood Drifters (UFDs): Onset of Movement",
            "updated": "2023-11-24T16:47:03Z",
            "published": "2023-11-24T16:47:03Z",
            "summary": "Despite their catastrophic implications in flood events, the mobilization and\ntransport of large, loose objects - termed Urban Flood Drifters (UFDs) - are\noften overlooked in flood management. This oversight stems from our limited\nunderstanding of how flowing water interacts with these heterogeneous objects.\nTo bridge this knowledge gap, we introduce a mechanistic stability model that\npredicts the onset of UFD mobilization across a diverse array of loose objects,\nfrom plastics to heavy vehicles. Built on an inventory of key physical\nproperties of UFDs, this model is also validated against existing mobilization\nstudies. Our model generates stability curves that delineate flow conditions\nleading to their mobilization. We further enhance the reliability of our model\nby incorporating a Monte Carlo-based probabilistic framework that accounts for\nuncertainties and interdependencies among the input parameters. These\nprobabilistic stability curves enable us to estimate the likelihood of\nmobilization for specific categories of UFDs under certain flow conditions.\nWhen integrated with flood maps or two-dimensional (2D) hydrodynamic models,\nour stability curves can guide urban planning efforts to predict and mitigate\nthe impacts of UFDs during extreme flood events.",
            "author": [
                "Daniel Valero",
                "Arnau Bay\u00f3n",
                "M\u00e1rio J. Franca"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14602v1",
                "http://arxiv.org/pdf/2311.14602v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14580v1",
            "title": "Large Language Models as Automated Aligners for benchmarking\n  Vision-Language Models",
            "updated": "2023-11-24T16:12:05Z",
            "published": "2023-11-24T16:12:05Z",
            "summary": "With the advancements in Large Language Models (LLMs), Vision-Language Models\n(VLMs) have reached a new level of sophistication, showing notable competence\nin executing intricate cognition and reasoning tasks. However, existing\nevaluation benchmarks, primarily relying on rigid, hand-crafted datasets to\nmeasure task-specific performance, face significant limitations in assessing\nthe alignment of these increasingly anthropomorphic models with human\nintelligence. In this work, we address the limitations via Auto-Bench, which\ndelves into exploring LLMs as proficient aligners, measuring the alignment\nbetween VLMs and human intelligence and value through automatic data curation\nand assessment. Specifically, for data curation, Auto-Bench utilizes LLMs\n(e.g., GPT-4) to automatically generate a vast set of question-answer-reasoning\ntriplets via prompting on visual symbolic representations (e.g., captions,\nobject locations, instance relationships, and etc.). The curated data closely\nmatches human intent, owing to the extensive world knowledge embedded in LLMs.\nThrough this pipeline, a total of 28.5K human-verified and 3,504K unfiltered\nquestion-answer-reasoning triplets have been curated, covering 4 primary\nabilities and 16 sub-abilities. We subsequently engage LLMs like GPT-3.5 to\nserve as judges, implementing the quantitative and qualitative automated\nassessments to facilitate a comprehensive evaluation of VLMs. Our validation\nresults reveal that LLMs are proficient in both evaluation data curation and\nmodel assessment, achieving an average agreement rate of 85%. We envision\nAuto-Bench as a flexible, scalable, and comprehensive benchmark for evaluating\nthe evolving sophisticated VLMs.",
            "author": [
                "Yuanfeng Ji",
                "Chongjian Ge",
                "Weikai Kong",
                "Enze Xie",
                "Zhengying Liu",
                "Zhengguo Li",
                "Ping Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14580v1",
                "http://arxiv.org/pdf/2311.14580v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14560v1",
            "title": "The attractive log gas: stability, uniqueness, and propagation of chaos",
            "updated": "2023-11-24T15:48:10Z",
            "published": "2023-11-24T15:48:10Z",
            "summary": "We consider overdamped Langevin dynamics for the attractive log gas on the\ntorus $\\mathbb{T}^\\mathsf{d}$, for $\\mathsf{d}\\geq 1$. In dimension\n$\\mathsf{d}=2$, this model coincides with a periodic version of the\nparabolic-elliptic Patlak-Keller-Segel model of chemotaxis. The attractive log\ngas (for our choice of units) is well-known to have a critical inverse\ntemperature $\\beta_{\\mathrm{c}}={2\\mathsf{d}}$ corresponding to when the free\nenergy is bounded from below. Moreover, it is well-known that the uniform\ndistribution is always a stationary state regardless of the temperature. We\nidentify another temperature threshold $\\beta_{\\mathrm{s}}$ sharply\ncorresponding to the nonlinear stability of the uniform distribution. We show\nthat for $\\beta>\\beta_{\\mathrm{s}}$, the uniform distribution does not minimize\nthe free energy and moreover is nonlinearly unstable, while for\n$\\beta<\\beta_{\\mathrm{s}}$, it is stable. We also show that there exists\n$\\beta_{\\mathrm{u}}$ for which uniqueness of equilibria holds for\n$\\beta<\\beta_{\\mathrm{u}}$.\n  We establish a uniform-in-time rate for entropic propagation of chaos for a\nrange of $\\beta<\\beta_{\\mathrm{s}}$. To our knowledge, this is the first such\nresult for singular attractive interactions and affirmatively answers a\nquestion of Bresch et al. arXiv:2011.08022. The proof of the convergence is\nthrough the modulated free energy method, relying on a modulated logarithmic\nHardy-Littlewood-Sobolev (mLHLS) inequality. Unlike Bresch et al., we show that\nsuch an inequality holds without truncation of the potential -- the avoidance\nof the truncation being essential to a uniform-in-time result -- for\nsufficiently small $\\beta$ and provide a counterexample to the mLHLS inequality\nwhen $\\beta>\\beta_{\\mathrm{s}}$. As a byproduct, we show that it is impossible\nto have a uniform-in-time rate of propagation of chaos if\n$\\beta>\\beta_{\\mathrm{s}}$.",
            "author": [
                "Antonin Chodron de Courcel",
                "Matthew Rosenzweig",
                "Sylvia Serfaty"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14560v1",
                "http://arxiv.org/pdf/2311.14560v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math-ph",
                "math.MP",
                "math.PR",
                "35Q70, 35Q82, 82C22, 82C40, 82C31, 35Q92, 94A17, 39B62"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14775v1",
            "title": "READS-V: Real-time Automated Detection of Epileptic Seizures from\n  Surveillance Videos via Skeleton-based Spatiotemporal ViG",
            "updated": "2023-11-24T15:07:29Z",
            "published": "2023-11-24T15:07:29Z",
            "summary": "An accurate and efficient epileptic seizure onset detection system can\nsignificantly benefit patients. Traditional diagnostic methods, primarily\nrelying on electroencephalograms (EEGs), often result in cumbersome and\nnon-portable solutions, making continuous patient monitoring challenging. The\nvideo-based seizure detection system is expected to free patients from the\nconstraints of scalp or implanted EEG devices and enable remote monitoring in\nresidential settings. Previous video-based methods neither enable all-day\nmonitoring nor provide short detection latency due to insufficient resources\nand ineffective patient action recognition techniques. Additionally,\nskeleton-based action recognition approaches remain limitations in identifying\nsubtle seizure-related actions. To address these challenges, we propose a novel\nskeleton-based spatiotemporal vision graph neural network (STViG) for\nefficient, accurate, and timely REal-time Automated Detection of epileptic\nSeizures from surveillance Videos (READS-V). Our experimental results indicate\nSTViG outperforms previous state-of-the-art action recognition models on our\ncollected patients' video data with higher accuracy (5.9% error) and lower\nFLOPs (0.4G). Furthermore, by integrating a decision-making rule that combines\noutput probabilities and an accumulative function, our READS-V system achieves\na 5.1 s EEG onset detection latency, a 13.1 s advance in clinical onset\ndetection, and zero false detection rate.",
            "author": [
                "Yankun Xu",
                "Jie Yang",
                "Wenjie Ming",
                "Shuang Wang",
                "Mohamad Sawan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14775v1",
                "http://arxiv.org/pdf/2311.14775v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14533v1",
            "title": "Comparing Feature Engineering and End-to-End Deep Learning for Autism\n  Spectrum Disorder Assessment based on Fullbody-Tracking",
            "updated": "2023-11-24T14:56:36Z",
            "published": "2023-11-24T14:56:36Z",
            "summary": "Autism Spectrum Disorder (ASD) is characterized by challenges in social\ncommunication and restricted patterns, with motor abnormalities gaining\ntraction for early detection. However, kinematic analysis in ASD is limited,\noften lacking robust validation and relying on hand-crafted features for single\ntasks, leading to inconsistencies across studies. Thus, end-to-end models have\nbecome promising methods to overcome the need for feature engineering. Our aim\nis to assess both approaches across various kinematic tasks to measure the\nefficacy of commonly used features in ASD assessment, while comparing them to\nend-to-end models. Specifically, we developed a virtual reality environment\nwith multiple motor tasks and trained models using both classification\napproaches. We prioritized a reliable validation framework with repeated\ncross-validation. Our comparative analysis revealed that hand-crafted features\noutperformed our deep learning approach in specific tasks, achieving a\nstate-of-the-art area under the curve (AUC) of 0.90$\\pm$0.06. Conversely,\nend-to-end models provided more consistent results with less variability across\nall VR tasks, demonstrating domain generalization and reliability, with a\nmaximum task AUC of 0.89$\\pm$0.06. These findings show that end-to-end models\nenable less variable and context-independent ASD assessments without requiring\ndomain knowledge or task specificity. However, they also recognize the\neffectiveness of hand-crafted features in specific task scenarios.",
            "author": [
                "Alberto Altozano",
                "Maria Eleonora Minissi",
                "Mariano Alca\u00f1iz",
                "Javier Mar\u00edn-Morales"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14533v1",
                "http://arxiv.org/pdf/2311.14533v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14530v1",
            "title": "Machine Translation for Ge'ez Language",
            "updated": "2023-11-24T14:55:23Z",
            "published": "2023-11-24T14:55:23Z",
            "summary": "Machine translation (MT) for low-resource languages such as Ge'ez, an ancient\nlanguage that is no longer spoken in daily life, faces challenges such as\nout-of-vocabulary words, domain mismatches, and lack of sufficient labeled\ntraining data. In this work, we explore various methods to improve Ge'ez MT,\nincluding transfer-learning from related languages, optimizing shared\nvocabulary and token segmentation approaches, finetuning large pre-trained\nmodels, and using large language models (LLMs) for few-shot translation with\nfuzzy matches. We develop a multilingual neural machine translation (MNMT)\nmodel based on languages relatedness, which brings an average performance\nimprovement of about 4 BLEU compared to standard bilingual models. We also\nattempt to finetune the NLLB-200 model, one of the most advanced translation\nmodels available today, but find that it performs poorly with only 4k training\nsamples for Ge'ez. Furthermore, we experiment with using GPT-3.5, a\nstate-of-the-art LLM, for few-shot translation with fuzzy matches, which\nleverages embedding similarity-based retrieval to find context examples from a\nparallel corpus. We observe that GPT-3.5 achieves a remarkable BLEU score of\n9.2 with no initial knowledge of Ge'ez, but still lower than the MNMT baseline\nof 15.2. Our work provides insights into the potential and limitations of\ndifferent approaches for low-resource and ancient language MT.",
            "author": [
                "Aman Kassahun Wassie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14530v1",
                "http://arxiv.org/pdf/2311.14530v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14516v1",
            "title": "Morphing Graph Drawings in the Presence of Point Obstacles",
            "updated": "2023-11-24T14:44:18Z",
            "published": "2023-11-24T14:44:18Z",
            "summary": "A crossing-free morph is a continuous deformation between two graph drawings\nthat preserves straight-line pairwise noncrossing edges. Motivated by\napplications in 3D morphing problems, we initiate the study of morphing graph\ndrawings in the plane in the presence of stationary point obstacles, which need\nto be avoided throughout the deformation. As our main result, we prove that it\nis NP-hard to decide whether such an obstacle-avoiding 2D morph between two\ngiven drawings of the same graph exists. This is in sharp contrast to the\nclassical case without obstacles, where there is an efficiently verifiable\n(necessary and sufficient) criterion for the existence of a morph.",
            "author": [
                "Oksana Firman",
                "Tim Hegemann",
                "Boris Klemz",
                "Felix Klesen",
                "Marie Diana Sieper",
                "Alexander Wolff",
                "Johannes Zink"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14516v1",
                "http://arxiv.org/pdf/2311.14516v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14510v1",
            "title": "The Westermo test system performance data set",
            "updated": "2023-11-24T14:38:49Z",
            "published": "2023-11-24T14:38:49Z",
            "summary": "There is a growing body of knowledge in the computer science, software\nengineering, software testing and software test automation disciplines.\nHowever, a challenge for researchers is to evaluate their research findings,\nideas and tools due to lack of realistic data. This paper presents the Westermo\ntest system performance data set. More than twenty performance metrics such as\nCPU and memory usage sampled twice per minute for a month on nineteen test\nsystems driving nightly testing of cyber-physical systems has been anonymized\nand released. The industrial motivation is to spur work on anomaly detection in\nseasonal data such that one may increase trust in nightly testing. One could\nask: If the test system is in an abnormal state - can we trust the test\nresults? How could one automate the detection of abnormal states? The data set\nhas previously been used by students and in hackathons. By releasing it we hope\nto simplify experiments on anomaly detection based on rules, thresholds,\nstatistics, machine learning or artificial intelligence, perhaps while\nincorporating seasonality. We also hope that the data set could lead to\nfindings in sustainable software engineering.",
            "author": [
                "Per Erik Strandberg",
                "Yosh Marklund"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14510v1",
                "http://arxiv.org/pdf/2311.14510v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14772v1",
            "title": "Trainwreck: A damaging adversarial attack on image classifiers",
            "updated": "2023-11-24T13:37:19Z",
            "published": "2023-11-24T13:37:19Z",
            "summary": "Adversarial attacks are an important security concern for computer vision\n(CV), as they enable malicious attackers to reliably manipulate CV models.\nExisting attacks aim to elicit an output desired by the attacker, but keep the\nmodel fully intact on clean data. With CV models becoming increasingly valuable\nassets in applied practice, a new attack vector is emerging: disrupting the\nmodels as a form of economic sabotage. This paper opens up the exploration of\ndamaging adversarial attacks (DAAs) that seek to damage the target model and\nmaximize the total cost incurred by the damage. As a pioneer DAA, this paper\nproposes Trainwreck, a train-time attack that poisons the training data of\nimage classifiers to degrade their performance. Trainwreck conflates the data\nof similar classes using stealthy ($\\epsilon \\leq 8/255$) class-pair universal\nperturbations computed using a surrogate model. Trainwreck is a black-box,\ntransferable attack: it requires no knowledge of the target model's\narchitecture, and a single poisoned dataset degrades the performance of any\nmodel trained on it. The experimental evaluation on CIFAR-10 and CIFAR-100\ndemonstrates that Trainwreck is indeed an effective attack across various model\narchitectures including EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16.\nThe strength of the attack can be customized by the poison rate parameter.\nFinally, data redundancy with file hashing and/or pixel difference are\nidentified as a reliable defense technique against Trainwreck or similar DAAs.\nThe code is available at https://github.com/JanZahalka/trainwreck.",
            "author": [
                "Jan Zah\u00e1lka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14772v1",
                "http://arxiv.org/pdf/2311.14772v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14469v1",
            "title": "Fault Detection in Telecom Networks using Bi-level Federated Graph\n  Neural Networks",
            "updated": "2023-11-24T13:23:54Z",
            "published": "2023-11-24T13:23:54Z",
            "summary": "5G and Beyond Networks become increasingly complex and heterogeneous, with\ndiversified and high requirements from a wide variety of emerging applications.\nThe complexity and diversity of Telecom networks place an increasing strain on\nmaintenance and operation efforts. Moreover, the strict security and privacy\nrequirements present a challenge for mobile operators to leverage network data.\nTo detect network faults, and mitigate future failures, prior work focused on\nleveraging traditional ML/DL methods to locate anomalies in networks. The\ncurrent approaches, although powerful, do not consider the intertwined nature\nof embedded and software-intensive Radio Access Network systems. In this paper,\nwe propose a Bi-level Federated Graph Neural Network anomaly detection and\ndiagnosis model that is able to detect anomalies in Telecom networks in a\nprivacy-preserving manner, while minimizing communication costs. Our method\nrevolves around conceptualizing Telecom data as a bi-level temporal Graph\nNeural Networks. The first graph captures the interactions between different\nRAN nodes that are exposed to different deployment scenarios in the network,\nwhile each individual Radio Access Network node is further elaborated into its\nsoftware (SW) execution graph. Additionally, we use Federated Learning to\naddress privacy and security limitations. Furthermore, we study the performance\nof anomaly detection model under three settings: (1) Centralized (2) Federated\nLearning and (3) Personalized Federated Learning using real-world data from an\noperational network. Our comprehensive experiments showed that Personalized\nFederated Temporal Graph Neural Networks method outperforms the most commonly\nused techniques for Anomaly Detection.",
            "author": [
                "R. Bourgerie",
                "T. Zanouda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14469v1",
                "http://arxiv.org/pdf/2311.14469v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14464v1",
            "title": "Finite Volume Features, Global Geometry Representations, and Residual\n  Training for Deep Learning-based CFD Simulation",
            "updated": "2023-11-24T13:19:06Z",
            "published": "2023-11-24T13:19:06Z",
            "summary": "Computational fluid dynamics (CFD) simulation is an irreplaceable modelling\nstep in many engineering designs, but it is often computationally expensive.\nSome graph neural network (GNN)-based CFD methods have been proposed. However,\nthe current methods inherit the weakness of traditional numerical simulators,\nas well as ignore the cell characteristics in the mesh used in the finite\nvolume method, a common method in practical CFD applications. Specifically, the\ninput nodes in these GNN methods have very limited information about any object\nimmersed in the simulation domain and its surrounding environment. Also, the\ncell characteristics of the mesh such as cell volume, face surface area, and\nface centroid are not included in the message-passing operations in the GNN\nmethods. To address these weaknesses, this work proposes two novel geometric\nrepresentations: Shortest Vector (SV) and Directional Integrated Distance\n(DID). Extracted from the mesh, the SV and DID provide global geometry\nperspective to each input node, thus removing the need to collect this\ninformation through message-passing. This work also introduces the use of\nFinite Volume Features (FVF) in the graph convolutions as node and edge\nattributes, enabling its message-passing operations to adjust to different\nnodes. Finally, this work is the first to demonstrate how residual training,\nwith the availability of low-resolution data, can be adopted to improve the\nflow field prediction accuracy. Experimental results on two datasets with five\ndifferent state-of-the-art GNN methods for CFD indicate that SV, DID, FVF and\nresidual training can effectively reduce the predictive error of current\nGNN-based methods by as much as 41%.",
            "author": [
                "Loh Sher En Jessica",
                "Naheed Anjum Arafat",
                "Wei Xian Lim",
                "Wai Lee Chan",
                "Adams Wai Kin Kong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14464v1",
                "http://arxiv.org/pdf/2311.14464v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CE",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14454v1",
            "title": "Can Robotic Experimenters help improve HRI Experiments? An Experimental\n  Study",
            "updated": "2023-11-24T13:09:00Z",
            "published": "2023-11-24T13:09:00Z",
            "summary": "To evaluate the design and skills of a robot or an algorithm for robotics,\nhuman-robot interaction user studies need to be performed. Classically, these\nstudies are conducted by human experimenters, requiring considerable effort,\nand introducing variability and potential human error. In this paper, we\ninvestigate the use of robots in support of HRI experiments. Robots can perform\nrepeated tasks accurately, thereby reducing human effort and improving validity\nthrough reduction of error and variability between participants. To assess the\npotential for robot led HRI experiments, we ran an HRI experiment with two\nparticipant groups, one led by a human experimenter and another led mostly by a\nrobot experimenter.We show that the replacement of several repetitive\nexperiment tasks through robots is not only possible but beneficial: Trials\nperformed by the robot experimenter had fewer errors and were more fluent.\nThere was no statistically significant difference in participants' perception\nw.r.t. cognitive load, comfortability, enjoyment, safety, trust and\nunderstandability between both groups. To the best of our knowledge, this is\nthe first comparison between robot-led and human-led HRI experiments. It\nsuggests that using robot experimenters can be beneficial and should be\nconsidered.",
            "author": [
                "Dan R. Suissa",
                "Shikhar Kumar",
                "Yael Edan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14454v1",
                "http://arxiv.org/pdf/2311.14454v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14451v1",
            "title": "Rigid partitions: from high connectivity to random graphs",
            "updated": "2023-11-24T13:01:01Z",
            "published": "2023-11-24T13:01:01Z",
            "summary": "A graph is called $d$-rigid if there exists a generic embedding of its vertex\nset into $\\mathbb{R}^d$ such that every continuous motion of the vertices that\npreserves the lengths of all edges actually preserves the distances between all\npairs of vertices. The rigidity of a graph is the maximal $d$ such that the\ngraph is $d$-rigid. We present new sufficient conditions for the $d$-rigidity\nof a graph in terms of the existence of \"rigid partitions\" -- partitions of the\ngraph that satisfy certain connectivity properties. This extends previous\nresults by Crapo, Lindemann, and Lew, Nevo, Peled and Raz.\n  As an application, we present new results on the rigidity of highly-connected\ngraphs, random graphs, random bipartite graphs, pseudorandom graphs, and dense\ngraphs. In particular, we prove that there exists $C>0$ such that every $C d^2\n{\\log^2}{n}$-connected $n$-vertex graph is $d$-rigid. This can be seen as a\nsignificant step towards the Lov\\'asz--Yemini conjecture, which states that for\nevery $d\\ge 1$ there exists $c_d\\ge 1$ such that every $c_d$-connected graph is\n$d$-rigid (this is known only in the case $d=2$, with $c_2=6$). Furthermore, we\nprove that random $C d\\log d$-regular graphs are typically $d$-rigid,\ndemonstrate the existence of a giant $d$-rigid component in sparse random\nbinomial graphs, and show that the rigidity of relatively sparse random\nbinomial bipartite graphs is roughly the same as that of the complete bipartite\ngraph, which we consider an interesting phenomenon. We also present an\nalternative short proof for a recent result by Lew, Nevo, Peled, and Raz, which\nasserts that the hitting time for $d$-rigidity in the random graph process\ntypically coincides with the hitting time for minimum degree $d$.",
            "author": [
                "Michael Krivelevich",
                "Alan Lew",
                "Peleg Michaeli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14451v1",
                "http://arxiv.org/pdf/2311.14451v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10, 52C25, 05C40, 05C80, 05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14427v1",
            "title": "Disentangling the Spectral Properties of the Hodge Laplacian: Not All\n  Small Eigenvalues Are Equal",
            "updated": "2023-11-24T12:00:50Z",
            "published": "2023-11-24T12:00:50Z",
            "summary": "The rich spectral information of the graph Laplacian has been instrumental in\ngraph theory, machine learning, and graph signal processing for applications\nsuch as graph classification, clustering, or eigenmode analysis. Recently, the\nHodge Laplacian has come into focus as a generalisation of the ordinary\nLaplacian for higher-order graph models such as simplicial and cellular\ncomplexes. Akin to the traditional analysis of graph Laplacians, many authors\nanalyse the smallest eigenvalues of the Hodge Laplacian, which are connected to\nimportant topological properties such as homology. However, small eigenvalues\nof the Hodge Laplacian can carry different information depending on whether\nthey are related to curl or gradient eigenmodes, and thus may not be\ncomparable. We therefore introduce the notion of persistent eigenvector\nsimilarity and provide a method to track individual harmonic, curl, and\ngradient eigenvectors/-values through the so-called persistence filtration,\nleveraging the full information contained in the Hodge-Laplacian spectrum\nacross all possible scales of a point cloud. Finally, we use our insights (a)\nto introduce a novel form of topological spectral clustering and (b) to\nclassify edges and higher-order simplices based on their relationship to the\nsmallest harmonic, curl, and gradient eigenvectors.",
            "author": [
                "Vincent P. Grande",
                "Michael T. Schaub"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14427v1",
                "http://arxiv.org/pdf/2311.14427v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14419v1",
            "title": "Narratives from GPT-derived Networks of News, and a link to Financial\n  Markets Dislocations",
            "updated": "2023-11-24T11:35:56Z",
            "published": "2023-11-24T11:35:56Z",
            "summary": "Starting from a corpus of economic articles from The Wall Street Journal, we\npresent a novel systematic way to analyse news content that evolves over time.\nWe leverage on state-of-the-art natural language processing techniques (i.e.\nGPT3.5) to extract the most important entities of each article available, and\naggregate co-occurrence of entities in a related graph at the weekly level.\nNetwork analysis techniques and fuzzy community detection are tested on the\nproposed set of graphs, and a framework is introduced that allows systematic\nbut interpretable detection of topics and narratives. In parallel, we propose\nto consider the sentiment around main entities of an article as a more accurate\nproxy for the overall sentiment of such piece of text, and describe a\ncase-study to motivate this choice. Finally, we design features that\ncharacterise the type and structure of news within each week, and map them to\nmoments of financial markets dislocations. The latter are identified as dates\nwith unusually high volatility across asset classes, and we find quantitative\nevidence that they relate to instances of high entropy in the high-dimensional\nspace of interconnected news. This result further motivates the pursued efforts\nto provide a novel framework for the systematic analysis of narratives within\nnews.",
            "author": [
                "Deborah Miori",
                "Constantin Petrov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14419v1",
                "http://arxiv.org/pdf/2311.14419v1"
            ],
            "primary_category": "q-fin.CP",
            "category": [
                "q-fin.CP",
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14411v1",
            "title": "Receding Horizon Optimization with PPUM: An Approach for Autonomous\n  Robot Path Planning in Uncertain Environments",
            "updated": "2023-11-24T11:09:29Z",
            "published": "2023-11-24T11:09:29Z",
            "summary": "The ability to understand spatial-temporal patterns for crowds of people is\ncrucial for achieving long-term autonomy of mobile robots deployed in human\nenvironments. However, traditional historical data-driven memory models are\ninadequate for handling anomalies, resulting in poor reasoning by robot in\nestimating the crowd spatial distribution. In this article, a Receding Horizon\nOptimization (RHO) formulation is proposed that incorporates a\nProbability-related Partially Updated Memory (PPUM) for robot path planning in\ncrowded environments with uncertainties. The PPUM acts as a memory layer that\ncombines real-time sensor observations with historical knowledge using a\nweighted evidence fusion theory to improve robot's adaptivity to the dynamic\nenvironments. RHO then utilizes the PPUM as a informed knowledge to generate a\npath that minimizes the likelihood of encountering dense crowds while reducing\nthe cost of local motion planning. The proposed approach provides an innovative\nsolution to the problem of robot's long-term safe interaction with human in\nuncertain crowded environments. In simulation, the results demonstrate the\nsuperior performance of our approach compared to benchmark methods in terms of\ncrowd distribution estimation accuracy, adaptability to anomalies and path\nplanning efficiency.",
            "author": [
                "Zijian Ge",
                "Jingjing Jiang",
                "Matthew Coombes",
                "Liang Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14411v1",
                "http://arxiv.org/pdf/2311.14411v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14404v1",
            "title": "BHGNN-RT: Network embedding for directed heterogeneous graphs",
            "updated": "2023-11-24T10:56:09Z",
            "published": "2023-11-24T10:56:09Z",
            "summary": "Networks are one of the most valuable data structures for modeling problems\nin the real world. However, the most recent node embedding strategies have\nfocused on undirected graphs, with limited attention to directed graphs,\nespecially directed heterogeneous graphs. In this study, we first investigated\nthe network properties of directed heterogeneous graphs. Based on network\nanalysis, we proposed an embedding method, a bidirectional heterogeneous graph\nneural network with random teleport (BHGNN-RT), for directed heterogeneous\ngraphs, that leverages bidirectional message-passing process and network\nheterogeneity. With the optimization of teleport proportion, BHGNN-RT is\nbeneficial to overcome the over-smoothing problem. Extensive experiments on\nvarious datasets were conducted to verify the efficacy and efficiency of\nBHGNN-RT. Furthermore, we investigated the effects of message components, model\nlayer, and teleport proportion on model performance. The performance comparison\nwith all other baselines illustrates that BHGNN-RT achieves state-of-the-art\nperformance, outperforming the benchmark methods in both node classification\nand unsupervised clustering tasks.",
            "author": [
                "Xiyang Sun",
                "Fumiyasu Komaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14404v1",
                "http://arxiv.org/pdf/2311.14404v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14373v1",
            "title": "From Local To Global Optimality in Concurrent Parity Games",
            "updated": "2023-11-24T09:42:24Z",
            "published": "2023-11-24T09:42:24Z",
            "summary": "We study two-player games on finite graphs. Turn-based games have many nice\nproperties, but concurrent games are harder to tame: e.g. turn-based stochastic\nparity games have positional optimal strategies, whereas even basic concurrent\nreachability games may fail to have optimal strategies. We study concurrent\nstochastic parity games, and identify a local structural condition that, when\nsatisfied at each state, guarantees existence of positional optimal strategies\nfor both players.",
            "author": [
                "Benjamin Bordais",
                "Patricia Bouyer",
                "St\u00e9phane Le Roux"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14373v1",
                "http://arxiv.org/pdf/2311.14373v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14367v1",
            "title": "Cultural data integration via random graphical modelling",
            "updated": "2023-11-24T09:23:13Z",
            "published": "2023-11-24T09:23:13Z",
            "summary": "Cultural values vary significantly around the world. Despite a large\nheterogeneity, similarities across national cultures are to be expected. This\npaper studies cross-country culture heterogeneity via the joint inference of\ncopula graphical models. To this end, a random graph generative model is\nintroduced, with a latent space that embeds cultural relatedness across\ncountries. Taking world-wide country-specific survey data as the primary source\nof information, the modelling framework allows to integrate external data, both\nat the level of cultural traits and of their interdependence. In this way, we\nare able to identify several dimensions of culture.",
            "author": [
                "Veronica Vinciotti",
                "Luca De Benedictis",
                "Ernst C. Wit"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14367v1",
                "http://arxiv.org/pdf/2311.14367v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14361v1",
            "title": "Deciphering and integrating invariants for neural operator learning with\n  various physical mechanisms",
            "updated": "2023-11-24T09:03:52Z",
            "published": "2023-11-24T09:03:52Z",
            "summary": "Neural operators have been explored as surrogate models for simulating\nphysical systems to overcome the limitations of traditional partial\ndifferential equation (PDE) solvers. However, most existing operator learning\nmethods assume that the data originate from a single physical mechanism,\nlimiting their applicability and performance in more realistic scenarios. To\nthis end, we propose Physical Invariant Attention Neural Operator (PIANO) to\ndecipher and integrate the physical invariants (PI) for operator learning from\nthe PDE series with various physical mechanisms. PIANO employs self-supervised\nlearning to extract physical knowledge and attention mechanisms to integrate\nthem into dynamic convolutional layers. Compared to existing techniques, PIANO\ncan reduce the relative error by 13.6\\%-82.2\\% on PDE forecasting tasks across\nvarying coefficients, forces, or boundary conditions. Additionally, varied\ndownstream tasks reveal that the PI embeddings deciphered by PIANO align well\nwith the underlying invariants in the PDE systems, verifying the physical\nsignificance of PIANO. The source code will be publicly available at:\nhttps://github.com/optray/PIANO.",
            "author": [
                "Rui Zhang",
                "Qi Meng",
                "Zhi-Ming Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14361v1",
                "http://arxiv.org/pdf/2311.14361v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "math.NA",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14352v1",
            "title": "The polynomial growth of the infinite long-range percolation cluster",
            "updated": "2023-11-24T08:53:07Z",
            "published": "2023-11-24T08:53:07Z",
            "summary": "We study independent long-range percolation on $\\mathbb{Z}^d$ where the\nnearest-neighbor edges are always open and the probability that two vertices\n$x,y$ with $\\|x-y\\|>1$ are connected by an edge is proportional to\n$\\frac{\\beta}{\\|x-y\\|^s}$, where $\\beta>0$ and $s> 0$ are parameters. We show\nthat the ball of radius $k$ centered at the origin in the graph metric grows\npolynomially if and only if $s\\geq 2d$. For the critical case $s=2d$, we show\nthat the volume growth exponent is inversely proportional to the distance\ngrowth exponent. Furthermore, we provide sharp upper and lower bounds on the\nprobability that the origin and $ne_1$ are connected by a path of length $k$ in\nthe critical case $s=2d$. We use these results to determine the Hausdorff\ndimension of the critical long-range percolation metric that was recently\nconstructed by Ding, Fan, and Huang [14].",
            "author": [
                "Johannes B\u00e4umler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14352v1",
                "http://arxiv.org/pdf/2311.14352v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "05C12, 60K35, 82B27, 82B43"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14348v1",
            "title": "Testing an instrument to measure the BPMS-KM Support Model",
            "updated": "2023-11-24T08:48:04Z",
            "published": "2023-11-24T08:48:04Z",
            "summary": "BPMS (Business Process Management System) represents a type of software that\nautomates the organizational processes looking for efficiency. Since the\nknowledge of organizations lies in their processes, it seems probable that a\nBPMS can be used to manage the knowledge applied in these processes. Through\nthe BPMS-KM Support Model, this study aims to determine the reliability and\nvalidity of a 65-item instrument to measure the utility and the use of a BPMS\nfor knowledge management (KM). A questionnaire was sent to 242 BPMS users and\nto determine its validity, a factorial analysis was conducted. The results\nshowed that the measuring instrument is trustworthy and valid. It represents\nimplications for research, since it provides an instrument validated for\nresearch on the success of a BPMS for KM. There would also be practical\nimplications, since managers can evaluate the use of BPMS, in addition to\nautomating processes to manage knowledge.",
            "author": [
                "Alicia Martin-Navarro",
                "Maria Paula Lechuga Sancho",
                "Jose Aurelio Medina-Garrido"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.eswa.2021.115005",
                "http://arxiv.org/abs/2311.14348v1",
                "http://arxiv.org/pdf/2311.14348v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14342v2",
            "title": "AI-based Attack Graph Generation",
            "updated": "2023-11-27T07:22:24Z",
            "published": "2023-11-24T08:35:16Z",
            "summary": "With the advancement of IoT technology, many electronic devices are\ninterconnected through networks, communicating with each other and performing\nspecific roles. However, as numerous devices join networks, the threat of\ncyberattacks also escalates. Preventing and detecting cyber threats are\ncrucial, and one method of preventing such threats involves using attack\ngraphs. Attack graphs are widely used to assess security threats within\nnetworks. However, a drawback emerges as the network scales, as generating\nattack graphs becomes time-consuming. To overcome this limitation, artificial\nintelligence models can be employed. By utilizing AI models, attack graphs can\nbe created within a short period, approximating optimal outcomes. AI models\ndesigned for attack graph generation consist of encoders and decoders, trained\nusing reinforcement learning algorithms. After training the AI models, we\nconfirmed the model's learning effectiveness by observing changes in loss and\nreward values. Additionally, we compared attack graphs generated by the AI\nmodel with those created through conventional methods.",
            "author": [
                "Sangbeom Park",
                "Jaesung Lee",
                "Jeong Do Yoo",
                "Min Geun Song",
                "Hyosun Lee",
                "Jaewoong Choi",
                "Chaeyeon Sagong",
                "Huy Kang Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14342v2",
                "http://arxiv.org/pdf/2311.14342v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14339v1",
            "title": "Towards Concept-based Interpretability of Skin Lesion Diagnosis using\n  Vision-Language Models",
            "updated": "2023-11-24T08:31:34Z",
            "published": "2023-11-24T08:31:34Z",
            "summary": "Concept-based models naturally lend themselves to the development of\ninherently interpretable skin lesion diagnosis, as medical experts make\ndecisions based on a set of visual patterns of the lesion. Nevertheless, the\ndevelopment of these models depends on the existence of concept-annotated\ndatasets, whose availability is scarce due to the specialized knowledge and\nexpertise required in the annotation process. In this work, we show that\nvision-language models can be used to alleviate the dependence on a large\nnumber of concept-annotated samples. In particular, we propose an embedding\nlearning strategy to adapt CLIP to the downstream task of skin lesion\nclassification using concept-based descriptions as textual embeddings. Our\nexperiments reveal that vision-language models not only attain better accuracy\nwhen using concepts as textual embeddings, but also require a smaller number of\nconcept-annotated samples to attain comparable performance to approaches\nspecifically devised for automatic concept generation.",
            "author": [
                "Cristiano Patr\u00edcio",
                "Lu\u00eds F. Teixeira",
                "Jo\u00e3o C. Neves"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14339v1",
                "http://arxiv.org/pdf/2311.14339v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14334v1",
            "title": "Maximizing Discrimination Capability of Knowledge Distillation with\n  Energy-based Score",
            "updated": "2023-11-24T08:16:10Z",
            "published": "2023-11-24T08:16:10Z",
            "summary": "To apply the latest computer vision techniques that require a large\ncomputational cost in real industrial applications, knowledge distillation\nmethods (KDs) are essential. Existing logit-based KDs apply the constant\ntemperature scaling to all samples in dataset, limiting the utilization of\nknowledge inherent in each sample individually. In our approach, we classify\nthe dataset into two categories (i.e., low energy and high energy samples)\nbased on their energy score. Through experiments, we have confirmed that low\nenergy samples exhibit high confidence scores, indicating certain predictions,\nwhile high energy samples yield low confidence scores, meaning uncertain\npredictions. To distill optimal knowledge by adjusting non-target class\npredictions, we apply a higher temperature to low energy samples to create\nsmoother distributions and a lower temperature to high energy samples to\nachieve sharper distributions. When compared to previous logit-based and\nfeature-based methods, our energy-based KD (Energy KD) achieves better\nperformance on various datasets. Especially, Energy KD shows significant\nimprovements on CIFAR-100-LT and ImageNet datasets, which contain many\nchallenging samples. Furthermore, we propose high energy-based data\naugmentation (HE-DA) for further improving the performance. We demonstrate that\nmeaningful performance improvement could be achieved by augmenting only 20-50%\nof dataset, suggesting that it can be employed on resource-limited devices. To\nthe best of our knowledge, this paper represents the first attempt to make use\nof energy scores in KD and DA, and we believe it will greatly contribute to\nfuture research.",
            "author": [
                "Seonghak Kim",
                "Gyeongdo Ham",
                "Suin Lee",
                "Donggon Jang",
                "Daeshik Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14334v1",
                "http://arxiv.org/pdf/2311.14334v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14333v2",
            "title": "Cycle Invariant Positional Encoding for Graph Representation Learning",
            "updated": "2023-11-30T08:39:27Z",
            "published": "2023-11-24T08:15:54Z",
            "summary": "Cycles are fundamental elements in graph-structured data and have\ndemonstrated their effectiveness in enhancing graph learning models. To encode\nsuch information into a graph learning framework, prior works often extract a\nsummary quantity, ranging from the number of cycles to the more sophisticated\npersistence diagram summaries. However, more detailed information, such as\nwhich edges are encoded in a cycle, has not yet been used in graph neural\nnetworks. In this paper, we make one step towards addressing this gap, and\npropose a structure encoding module, called CycleNet, that encodes cycle\ninformation via edge structure encoding in a permutation invariant manner. To\nefficiently encode the space of all cycles, we start with a cycle basis (i.e.,\na minimal set of cycles generating the cycle space) which we compute via the\nkernel of the 1-dimensional Hodge Laplacian of the input graph. To guarantee\nthe encoding is invariant w.r.t. the choice of cycle basis, we encode the cycle\ninformation via the orthogonal projector of the cycle basis, which is inspired\nby BasisNet proposed by Lim et al. We also develop a more efficient variant\nwhich however requires that the input graph has a unique shortest cycle basis.\nTo demonstrate the effectiveness of the proposed module, we provide some\ntheoretical understandings of its expressive power. Moreover, we show via a\nrange of experiments that networks enhanced by our CycleNet module perform\nbetter in various benchmarks compared to several existing SOTA models.",
            "author": [
                "Zuoyu Yan",
                "Tengfei Ma",
                "Liangcai Gao",
                "Zhi Tang",
                "Chao Chen",
                "Yusu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14333v2",
                "http://arxiv.org/pdf/2311.14333v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14332v1",
            "title": "GATGPT: A Pre-trained Large Language Model with Graph Attention Network\n  for Spatiotemporal Imputation",
            "updated": "2023-11-24T08:15:11Z",
            "published": "2023-11-24T08:15:11Z",
            "summary": "The analysis of spatiotemporal data is increasingly utilized across diverse\ndomains, including transportation, healthcare, and meteorology. In real-world\nsettings, such data often contain missing elements due to issues like sensor\nmalfunctions and data transmission errors. The objective of spatiotemporal\nimputation is to estimate these missing values by understanding the inherent\nspatial and temporal relationships in the observed multivariate time series.\nTraditionally, spatiotemporal imputation has relied on specific, intricate\narchitectures designed for this purpose, which suffer from limited\napplicability and high computational complexity. In contrast, our approach\nintegrates pre-trained large language models (LLMs) into spatiotemporal\nimputation, introducing a groundbreaking framework, GATGPT. This framework\nmerges a graph attention mechanism with LLMs. We maintain most of the LLM\nparameters unchanged to leverage existing knowledge for learning temporal\npatterns, while fine-tuning the upper layers tailored to various applications.\nThe graph attention component enhances the LLM's ability to understand spatial\nrelationships. Through tests on three distinct real-world datasets, our\ninnovative approach demonstrates comparable results to established deep\nlearning benchmarks.",
            "author": [
                "Yakun Chen",
                "Xianzhi Wang",
                "Guandong Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14332v1",
                "http://arxiv.org/pdf/2311.14332v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14324v1",
            "title": "Large Language Models as Topological Structure Enhancers for\n  Text-Attributed Graphs",
            "updated": "2023-11-24T07:53:48Z",
            "published": "2023-11-24T07:53:48Z",
            "summary": "The latest advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing (NLP). Inspired by the success of LLMs\nin NLP tasks, some recent work has begun investigating the potential of\napplying LLMs in graph learning tasks. However, most of the existing work\nfocuses on utilizing LLMs as powerful node feature augmenters, leaving\nemploying LLMs to enhance graph topological structures an understudied problem.\nIn this work, we explore how to leverage the information retrieval and text\ngeneration capabilities of LLMs to refine/enhance the topological structure of\ntext-attributed graphs (TAGs) under the node classification setting. First, we\npropose using LLMs to help remove unreliable edges and add reliable ones in the\nTAG. Specifically, we first let the LLM output the semantic similarity between\nnode attributes through delicate prompt designs, and then perform edge deletion\nand edge addition based on the similarity. Second, we propose using\npseudo-labels generated by the LLM to improve graph topology, that is, we\nintroduce the pseudo-label propagation as a regularization to guide the graph\nneural network (GNN) in learning proper edge weights. Finally, we incorporate\nthe two aforementioned LLM-based methods for graph topological refinement into\nthe process of GNN training, and perform extensive experiments on four\nreal-world datasets. The experimental results demonstrate the effectiveness of\nLLM-based graph topology refinement (achieving a 0.15%--2.47% performance gain\non public benchmarks).",
            "author": [
                "Shengyin Sun",
                "Yuxiang Ren",
                "Chen Ma",
                "Xuecang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14324v1",
                "http://arxiv.org/pdf/2311.14324v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14321v1",
            "title": "Hypercontractivity for Quantum Erasure Channels via Multipartite\n  Log-Sobolev Inequality",
            "updated": "2023-11-24T07:44:46Z",
            "published": "2023-11-24T07:44:46Z",
            "summary": "We prove an almost optimal hypercontractive inequality for quantum erasure\nchannels, generalizing the hypercontractivity for classical binary erasure\nchannels [NW16]. To our knowledge, this is the first hypercontractivity bound\nfor non-unital quantum channels. The traditional inductive arguments for\nclassical hypercontractivity cannot be generalized to the quantum setting due\nto the nature of non-commutativity of matrices. To overcome the difficulty, we\nestablish a multipartite quantum log-Sobolev inequality, which includes the\nclassical log-Sobolev inequality [DSC96] and the quantum log-Sobolev inequality\n[KT13] as one-partite cases. We establish a connection between our multipartite\nquantum log-Sobolev inequality and the hypercontractivity bound for quantum\nerasure channels via a refined quantum Gross' lemma [Gro75a], extending the\nanalogous connection [Kin14] between the quantum log-Sobolev inequality and the\nhypercontractivity for qubit unital channels. As an application, we prove an\nalmost tight bound (up to a constant factor) on the classical communication\ncomplexity of two-party common randomness generation assisted with erased-noisy\nEPR states, generalizing the tight bound on the same task assisted with\nerased-noisy random strings due to Guruswami and Radhakrishnan [GR16].",
            "author": [
                "Zongbo Bao",
                "Yangjing Dong",
                "Fengning Ou",
                "Penghui Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14321v1",
                "http://arxiv.org/pdf/2311.14321v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14307v1",
            "title": "Cosine Similarity Knowledge Distillation for Individual Class\n  Information Transfer",
            "updated": "2023-11-24T06:34:47Z",
            "published": "2023-11-24T06:34:47Z",
            "summary": "Previous logits-based Knowledge Distillation (KD) have utilized predictions\nabout multiple categories within each sample (i.e., class predictions) and have\nemployed Kullback-Leibler (KL) divergence to reduce the discrepancy between the\nstudent and teacher predictions. Despite the proliferation of KD techniques,\nthe student model continues to fall short of achieving a similar level as\nteachers. In response, we introduce a novel and effective KD method capable of\nachieving results on par with or superior to the teacher models performance. We\nutilize teacher and student predictions about multiple samples for each\ncategory (i.e., batch predictions) and apply cosine similarity, a commonly used\ntechnique in Natural Language Processing (NLP) for measuring the resemblance\nbetween text embeddings. This metric's inherent scale-invariance property,\nwhich relies solely on vector direction and not magnitude, allows the student\nto dynamically learn from the teacher's knowledge, rather than being bound by a\nfixed distribution of the teacher's knowledge. Furthermore, we propose a method\ncalled cosine similarity weighted temperature (CSWT) to improve the\nperformance. CSWT reduces the temperature scaling in KD when the cosine\nsimilarity between the student and teacher models is high, and conversely, it\nincreases the temperature scaling when the cosine similarity is low. This\nadjustment optimizes the transfer of information from the teacher to the\nstudent model. Extensive experimental results show that our proposed method\nserves as a viable alternative to existing methods. We anticipate that this\napproach will offer valuable insights for future research on model compression.",
            "author": [
                "Gyeongdo Ham",
                "Seonghak Kim",
                "Suin Lee",
                "Jae-Hyeok Lee",
                "Daeshik Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14307v1",
                "http://arxiv.org/pdf/2311.14307v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14304v1",
            "title": "AdaMedGraph: Adaboosting Graph Neural Networks for Personalized Medicine",
            "updated": "2023-11-24T06:27:25Z",
            "published": "2023-11-24T06:27:25Z",
            "summary": "Precision medicine tailored to individual patients has gained significant\nattention in recent times. Machine learning techniques are now employed to\nprocess personalized data from various sources, including images, genetics, and\nassessments. These techniques have demonstrated good outcomes in many clinical\nprediction tasks. Notably, the approach of constructing graphs by linking\nsimilar patients and then applying graph neural networks (GNNs) stands out,\nbecause related information from analogous patients are aggregated and\nconsidered for prediction. However, selecting the appropriate edge feature to\ndefine patient similarity and construct the graph is challenging, given that\neach patient is depicted by high-dimensional features from diverse sources.\nPrevious studies rely on human expertise to select the edge feature, which is\nneither scalable nor efficient in pinpointing crucial edge features for complex\ndiseases. In this paper, we propose a novel algorithm named \\ours, which can\nautomatically select important features to construct multiple patient\nsimilarity graphs, and train GNNs based on these graphs as weak learners in\nadaptive boosting. \\ours{} is evaluated on two real-world medical scenarios and\nshows superiors performance.",
            "author": [
                "Jie Lian",
                "Xufang Luo",
                "Caihua Shan",
                "Dongqi Han",
                "Varut Vardhanabhuti",
                "Dongsheng Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14304v1",
                "http://arxiv.org/pdf/2311.14304v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14303v1",
            "title": "RFI Detection with Spiking Neural Networks",
            "updated": "2023-11-24T06:27:08Z",
            "published": "2023-11-24T06:27:08Z",
            "summary": "Radio Frequency Interference (RFI) detection and mitigation is critical for\nenabling and maximising the scientific output of radio telescopes. The\nemergence of machine learning methods capable of handling large datasets has\nled to their application in radio astronomy, particularly in RFI detection.\nSpiking Neural Networks (SNNs), inspired by biological systems, are well-suited\nfor processing spatio-temporal data. This study introduces the first\napplication of SNNs to an astronomical data-processing task, specifically RFI\ndetection. We adapt the nearest-latent-neighbours (NLN) algorithm and\nauto-encoder architecture proposed by previous authors to SNN execution by\ndirect ANN2SNN conversion, enabling simplified downstream RFI detection by\nsampling the naturally varying latent space from the internal spiking neurons.\nWe evaluate performance with the simulated HERA telescope and hand-labelled\nLOFAR dataset that the original authors provided. We additionally evaluate\nperformance with a new MeerKAT-inspired simulation dataset. This dataset\nfocuses on satellite-based RFI, an increasingly important class of RFI and is,\ntherefore, an additional contribution. Our SNN approach remains competitive\nwith the original NLN algorithm and AOFlagger in AUROC, AUPRC and F1 scores for\nthe HERA dataset but exhibits difficulty in the LOFAR and MeerKAT datasets.\nHowever, our method maintains this performance while completely removing the\ncompute and memory-intense latent sampling step found in NLN. This work\ndemonstrates the viability of SNNs as a promising avenue for\nmachine-learning-based RFI detection in radio telescopes by establishing a\nminimal performance baseline on traditional and nascent satellite-based RFI\nsources and is the first work to our knowledge to apply SNNs in astronomy.",
            "author": [
                "Nicholas J. Pritchard",
                "Andreas Wicenec",
                "Mohammed Bennamoun",
                "Richard Dodson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14303v1",
                "http://arxiv.org/pdf/2311.14303v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14289v1",
            "title": "Four-set Hypergraphlets for Characterization of Directed Hypergraphs",
            "updated": "2023-11-24T05:38:54Z",
            "published": "2023-11-24T05:38:54Z",
            "summary": "A directed hypergraph, which consists of nodes and hyperarcs, is a\nhigher-order data structure that naturally models directional group\ninteractions (e.g., chemical reactions of molecules). Although there have been\nextensive studies on local structures of (directed) graphs in the real world,\nthose of directed hypergraphs remain unexplored. In this work, we focus on\nmeasurements, findings, and applications related to local structures of\ndirected hypergraphs, and they together contribute to a systematic\nunderstanding of various real-world systems interconnected by directed group\ninteractions. Our first contribution is to define 91 directed hypergraphlets\n(DHGs), which disjointly categorize directed connections and overlaps among\nfour node sets that compose two incident hyperarcs. Our second contribution is\nto develop exact and approximate algorithms for counting the occurrences of\neach DHG. Our last contribution is to characterize 11 real-world directed\nhypergraphs and individual hyperarcs in them using the occurrences of DHGs,\nwhich reveals clear domain-based local structural patterns. Our experiments\ndemonstrate that our DHG-based characterization gives up to 12% and 33% better\nperformances on hypergraph clustering and hyperarc prediction, respectively,\nthan baseline characterization methods. Moreover, we show that CODA-A, which is\nour proposed approximate algorithm, is up to 32X faster than its competitors\nwith similar characterization quality.",
            "author": [
                "Heechan Moon",
                "Hyunju Kim",
                "Sunwoo Kim",
                "Kijung Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14289v1",
                "http://arxiv.org/pdf/2311.14289v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14280v1",
            "title": "Latent Diffusion Prior Enhanced Deep Unfolding for Spectral Image\n  Reconstruction",
            "updated": "2023-11-24T04:55:20Z",
            "published": "2023-11-24T04:55:20Z",
            "summary": "Snapshot compressive spectral imaging reconstruction aims to reconstruct\nthree-dimensional spatial-spectral images from a single-shot two-dimensional\ncompressed measurement. Existing state-of-the-art methods are mostly based on\ndeep unfolding structures but have intrinsic performance bottlenecks: $i$) the\nill-posed problem of dealing with heavily degraded measurement, and $ii$) the\nregression loss-based reconstruction models being prone to recover images with\nfew details. In this paper, we introduce a generative model, namely the latent\ndiffusion model (LDM), to generate degradation-free prior to enhance the\nregression-based deep unfolding method. Furthermore, to overcome the large\ncomputational cost challenge in LDM, we propose a lightweight model to generate\nknowledge priors in deep unfolding denoiser, and integrate these priors to\nguide the reconstruction process for compensating high-quality spectral signal\ndetails. Numeric and visual comparisons on synthetic and real-world datasets\nillustrate the superiority of our proposed method in both reconstruction\nquality and computational efficiency. Code will be released.",
            "author": [
                "Zongliang Wu",
                "Ruiying Lu",
                "Ying Fu",
                "Xin Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14280v1",
                "http://arxiv.org/pdf/2311.14280v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14270v1",
            "title": "Efficient Open-world Reinforcement Learning via Knowledge Distillation\n  and Autonomous Rule Discovery",
            "updated": "2023-11-24T04:12:50Z",
            "published": "2023-11-24T04:12:50Z",
            "summary": "Deep reinforcement learning suffers from catastrophic forgetting and sample\ninefficiency making it less applicable to the ever-changing real world.\nHowever, the ability to use previously learned knowledge is essential for AI\nagents to quickly adapt to novelties. Often, certain spatial information\nobserved by the agent in the previous interactions can be leveraged to infer\ntask-specific rules. Inferred rules can then help the agent to avoid\npotentially dangerous situations in the previously unseen states and guide the\nlearning process increasing agent's novelty adaptation speed. In this work, we\npropose a general framework that is applicable to deep reinforcement learning\nagents. Our framework provides the agent with an autonomous way to discover the\ntask-specific rules in the novel environments and self-supervise it's learning.\nWe provide a rule-driven deep Q-learning agent (RDQ) as one possible\nimplementation of that framework. We show that RDQ successfully extracts\ntask-specific rules as it interacts with the world and uses them to drastically\nincrease its learning efficiency. In our experiments, we show that the RDQ\nagent is significantly more resilient to the novelties than the baseline\nagents, and is able to detect and adapt to novel situations faster.",
            "author": [
                "Ekaterina Nikonova",
                "Cheng Xue",
                "Jochen Renz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14270v1",
                "http://arxiv.org/pdf/2311.14270v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14262v1",
            "title": "ZeroPS: High-quality Cross-modal Knowledge Transfer for Zero-Shot 3D\n  Part Segmentation",
            "updated": "2023-11-24T03:19:17Z",
            "published": "2023-11-24T03:19:17Z",
            "summary": "Recently, many 2D pretrained foundational models have demonstrated impressive\nzero-shot prediction capabilities. In this work, we design a novel pipeline for\nzero-shot 3D part segmentation, called ZeroPS. It high-quality transfers\nknowledge from 2D pretrained foundational models to 3D point clouds. The main\nidea of our approach is to explore the natural relationship between multi-view\ncorrespondences and the prompt mechanism of foundational models and build\nbridges on it. Our pipeline consists of two components: 1) a self-extension\ncomponent that extends 2D groups from a single viewpoint to spatial\nglobal-level 3D groups; 2) a multi-modal labeling component that introduces a\ntwo-dimensional checking mechanism to vote each 2D predicted bounding box to\nthe best matching 3D part, and a Class Non-highest Vote Penalty function to\nrefine the Vote Matrix. Additionally, a merging algorithm is included to merge\npart-level 3D groups. Extensive evaluation of three zero-shot segmentation\ntasks on PartnetE datasets, achieving state-of-the-art results with significant\nimprovements (+19.6%, +5.2% and +4.9%, respectively) over existing methods. Our\nproposed approach does not need any training, fine-tuning or learnable\nparameters. It is hardly affected by domain shift. The code will be released.",
            "author": [
                "Yuheng Xue",
                "Nenglun Chen",
                "Jun Liu",
                "Wenyun Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14262v1",
                "http://arxiv.org/pdf/2311.14262v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14259v1",
            "title": "Characterization of transmission irregular starlike and double starlike\n  trees",
            "updated": "2023-11-24T03:06:17Z",
            "published": "2023-11-24T03:06:17Z",
            "summary": "The transmission of a vertex in a connected graph is the sum of its distances\nto all the other vertices. A graph is transmission irregular (TI) when all of\nits vertices have mutually distinct transmissions. In an earlier paper,\nAl-Yakoob and Stevanovi\\'c [Appl. Math. Comput. 380 (2020), 125257] gave the\nfull characterization of TI starlike trees with three branches. Here, we\nimprove these results by using a different approach to provide the complete\ncharacterization of all TI starlike trees. Moreover, we find the precise\nconditions under which a double starlike tree is TI. Finally, we implement the\naforementioned conditions in order to find several infinite families of TI\nstarlike trees and TI double starlike trees.",
            "author": [
                "Ivan Damnjanovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14259v1",
                "http://arxiv.org/pdf/2311.14259v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C12, 05C05, 05C75"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14258v1",
            "title": "Minimal graphs with eigenvalue multiplicity of $n-d$",
            "updated": "2023-11-24T03:04:27Z",
            "published": "2023-11-24T03:04:27Z",
            "summary": "For a connected graph $G$ with order $n$, let $e(G)$ be the number of its\ndistinct eigenvalues and $d$ be the diameter. We denote by $m_G(\\mu)$ the\neigenvalue multiplicity of $\\mu$ in $G$. It is well known that $e(G)\\geq d+1$,\nwhich shows $m_G(\\mu)\\leq n-d$ for any real number $\\mu$. A graph is called\n$minimal$ if $e(G)= d+1$. In 2013, Wang (\\cite{WD}, Linear Algebra Appl.)\ncharacterize all minimal graphs with $m_G(0)=n-d$. In 2023, Du et al.\n(\\cite{Du}, Linear Algebra Appl.) characterize all the trees for which there is\na real symmetric matrix with nullity $n-d$ and $n-d-1$. In this paper, by\napplying the star complement theory, we prove that if $G$ is not a path and\n$m_G(\\mu)= n-d$, then $\\mu \\in \\{0,-1\\}$. Furthermore, we completely\ncharacterize all minimal graphs with $m_G(-1)=n-d$.",
            "author": [
                "Yuanshuai Zhang",
                "Dein Wong",
                "Wenhao Zhen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14258v1",
                "http://arxiv.org/pdf/2311.14258v1"
            ],
            "primary_category": "math.SP",
            "category": [
                "math.SP",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.17067v1",
            "title": "Hypergraphs Demonstrate Anastomoses During Divergent Integration",
            "updated": "2023-11-24T02:55:42Z",
            "published": "2023-11-24T02:55:42Z",
            "summary": "Complex networks can be used to analyze structures and systems in the embryo.\nNot only can we characterize growth and the emergence of form, but also\ndifferentiation. The process of differentiation from precursor cell populations\nto distinct functional tissues is of particular interest. These phenomena can\nbe captured using a hypergraph consisting of nodes represented by cell type\ncategories and arranged as a directed cyclic graph (lineage hypergraph) and a\ncomplex network (spatial hypergraph). The lineage hypergraph models the\ndevelopmental process as an n-ary tree, which can model two or more descendent\ncategories per division event. A lineage tree based on the mosaic development\nof the nematode C. elegans (2-ary tree), is used to capture this process. Each\nround of divisions produces a new set of categories that allow for exchange of\ncells between types. An example from single-cell morphogenesis based on the\ncyanobacterial species Nostoc punctiforme (multiple discontinuous 2-ary tree)\nis also used to demonstrate the flexibility of this method. This model allows\nfor new structures to emerge (such as a connectome) while also demonstrating\nhow precursor categories are maintained for purposes such as dedifferentiation\nor other forms of cell fate plasticity. To understand this process of divergent\nintegration, we analyze the directed hypergraph and categorical models, in\naddition to considering the role of network fistulas (spaces that conjoin two\nfunctional modules) and spatial restriction.",
            "author": [
                "Bradly Alicea"
            ],
            "link": [
                "http://arxiv.org/abs/2311.17067v1",
                "http://arxiv.org/pdf/2311.17067v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14255v1",
            "title": "Out-of-Distribution Generalized Dynamic Graph Neural Network with\n  Disentangled Intervention and Invariance Promotion",
            "updated": "2023-11-24T02:42:42Z",
            "published": "2023-11-24T02:42:42Z",
            "summary": "Dynamic graph neural networks (DyGNNs) have demonstrated powerful predictive\nabilities by exploiting graph structural and temporal dynamics. However, the\nexisting DyGNNs fail to handle distribution shifts, which naturally exist in\ndynamic graphs, mainly because the patterns exploited by DyGNNs may be variant\nwith respect to labels under distribution shifts. In this paper, we propose\nDisentangled Intervention-based Dynamic graph Attention networks with\nInvariance Promotion (I-DIDA) to handle spatio-temporal distribution shifts in\ndynamic graphs by discovering and utilizing invariant patterns, i.e.,\nstructures and features whose predictive abilities are stable across\ndistribution shifts. Specifically, we first propose a disentangled\nspatio-temporal attention network to capture the variant and invariant\npatterns. By utilizing the disentangled patterns, we design a spatio-temporal\nintervention mechanism to create multiple interventional distributions and an\nenvironment inference module to infer the latent spatio-temporal environments,\nand minimize the variance of predictions among these intervened distributions\nand environments, so that our model can make predictions based on invariant\npatterns with stable predictive abilities under distribution shifts. Extensive\nexperiments demonstrate the superiority of our method over state-of-the-art\nbaselines under distribution shifts. Our work is the first study of\nspatio-temporal distribution shifts in dynamic graphs, to the best of our\nknowledge.",
            "author": [
                "Zeyang Zhang",
                "Xin Wang",
                "Ziwei Zhang",
                "Haoyang Li",
                "Wenwu Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14255v1",
                "http://arxiv.org/pdf/2311.14255v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14237v1",
            "title": "Pseudo-label Correction for Instance-dependent Noise Using\n  Teacher-student Framework",
            "updated": "2023-11-24T00:36:17Z",
            "published": "2023-11-24T00:36:17Z",
            "summary": "The high capacity of deep learning models to learn complex patterns poses a\nsignificant challenge when confronted with label noise. The inability to\ndifferentiate clean and noisy labels ultimately results in poor generalization.\nWe approach this problem by reassigning the label for each image using a new\nteacher-student based framework termed P-LC (pseudo-label correction).\nTraditional teacher-student networks are composed of teacher and student\nclassifiers for knowledge distillation. In our novel approach, we reconfigure\nthe teacher network into a triple encoder, leveraging the triplet loss to\nestablish a pseudo-label correction system. As the student generates pseudo\nlabels for a set of given images, the teacher learns to choose between the\ninitially assigned labels and the pseudo labels. Experiments on MNIST,\nFashion-MNIST, and SVHN demonstrate P-LC's superior performance over existing\nstate-of-the-art methods across all noise levels, most notably in high noise.\nIn addition, we introduce a noise level estimation to help assess model\nperformance and inform the need for additional data cleaning procedures.",
            "author": [
                "Eugene Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14237v1",
                "http://arxiv.org/pdf/2311.14237v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14236v1",
            "title": "Maximum Cardinality $f$-Matching in Time $O(n^{2/3}m)$",
            "updated": "2023-11-24T00:32:17Z",
            "published": "2023-11-24T00:32:17Z",
            "summary": "We present an algorithm that finds a maximum cardinality $f$-matching of a\nsimple graph in time $O(n^{2/3} m)$. Here $f:V\\to \\mathbb{N}$ is a given\nfunction, and an $f$-matching is a subgraph wherein each vertex $v\\in V$ has\ndegree $\\le f(v)$. This result generalizes a string of algorithms,\nconcentrating on simple bipartite graphs. The bipartite case is based on the\nnotion of level graph, introduced by Dinic for network flow. For general graphs\nthe ``level'' of a vertex is unclear: A given vertex can occur on many\ndifferent levels in augmenting trails. In fact there does not seem to be a\nunique level graph, our notion of level graph depends on the trails being\nanalyzed. Our analysis presents new properties of blossoms of shortest\naugmenting trails.\n  Our algorithm, unmodified, is also efficient on multigraphs, achieving time\n$O(\\min \\{\\sqrt {f(V)}, n\\}\\,m)$, for $f(V)=\\sum_vf(v)$.",
            "author": [
                "Harold Gabow"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14236v1",
                "http://arxiv.org/pdf/2311.14236v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14235v1",
            "title": "Traffic flow phase transition phenomena based on the kinetic approach",
            "updated": "2023-11-24T00:27:02Z",
            "published": "2023-11-24T00:27:02Z",
            "summary": "We develop a discrete Boltzmann-type model that uses dynamics in phase space\nto describe the behavior of traffic flows. Firstly, we model the traffic flow\nat mesoscopic scale using dynamics in phase space, which is considered as an\nevolution model in a graph, and we demonstrate the existence of phase\ntransition phenomena through theoretical analysis. Secondly, we assumed the\ndensity of vehicles in geometric space to be homogeneous and single-peaked,\nrespectively, and performed numerical simulations to obtain the results\nconsistent with experience. According to this model, we can perform effective\nsimulations on the effects played by multiple parameters on the traffic flow.",
            "author": [
                "Zhizhen Zhang",
                "Changhong Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14235v1",
                "http://arxiv.org/pdf/2311.14235v1"
            ],
            "primary_category": "nlin.CG",
            "category": [
                "nlin.CG",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14231v1",
            "title": "An extremely energetic cosmic ray observed by a surface detector array",
            "updated": "2023-11-24T00:00:00Z",
            "published": "2023-11-24T00:00:00Z",
            "summary": "Cosmic rays are energetic charged particles from extraterrestrial sources,\nwith the highest energy events thought to come from extragalactic sources.\nTheir arrival is infrequent, so detection requires instruments with large\ncollecting areas. In this work, we report the detection of an extremely\nenergetic particle recorded by the surface detector array of the Telescope\nArray experiment. We calculate the particle's energy as 244 +- 29 (stat.)\n+51,-76 (syst.) exa-electron volts (~40 joules). Its arrival direction points\nback to a void in the large-scale structure of the Universe. Possible\nexplanations include a large deflection by the foreground magnetic field, an\nunidentified source in the local extragalactic neighborhood or an incomplete\nknowledge of particle physics.",
            "author": [
                "Telescope Array Collaboration"
            ],
            "link": [
                "http://dx.doi.org/10.1126/science.abo5095",
                "http://arxiv.org/abs/2311.14231v1",
                "http://arxiv.org/pdf/2311.14231v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14201v1",
            "title": "On the convergence of adaptive approximations for stochastic\n  differential equations",
            "updated": "2023-11-23T21:00:03Z",
            "published": "2023-11-23T21:00:03Z",
            "summary": "In this paper, we study numerical approximations for stochastic differential\nequations (SDEs) that use adaptive step sizes. In particular, we consider a\ngeneral setting where decisions to reduce step sizes are allowed to depend on\nthe future trajectory of the underlying Brownian motion. Since these adaptive\nstep sizes may not be previsible, the standard mean squared error analysis\ncannot be directly applied to show that the numerical method converges to the\nsolution of the SDE. Building upon the pioneering work of Gaines and Lyons, we\nshall instead use rough path theory to establish convergence for a wide class\nof adaptive numerical methods on general Stratonovich SDEs (with sufficiently\nsmooth vector fields). To the author's knowledge, this is the first error\nanalysis applicable to standard solvers, such as the Milstein and Heun methods,\nwith non-previsible step sizes. In our analysis, we require the sequence of\nadaptive step sizes to be nested and the SDE solver to have unbiased \"L\\'evy\narea\" terms in its Taylor expansion. We conjecture that for adaptive SDE\nsolvers more generally, convergence is still possible provided the method does\nnot introduce \"L\\'evy area bias\". We present a simple example where the step\nsize control can skip over previously considered times, resulting in the\nnumerical method converging to an incorrect limit (i.e. not the Stratonovich\nSDE). Finally, we conclude with a numerical experiment demonstrating a newly\nintroduced adaptive scheme and showing the potential improvements in accuracy\nwhen step sizes are allowed to be non-previsible.",
            "author": [
                "James Foster"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14201v1",
                "http://arxiv.org/pdf/2311.14201v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14176v1",
            "title": "Concentration and local smoothness of the averaging process",
            "updated": "2023-11-23T19:49:31Z",
            "published": "2023-11-23T19:49:31Z",
            "summary": "We consider the averaging process on the discrete $d$-dimensional torus. On\nthis graph, the process is known to converge to equilibrium on diffusive\ntimescales, not exhibiting cutoff. In this work, we refine this picture in two\nways. Firstly, we prove a concentration phenomenon of the averaging process\naround its mean, occurring on a shorter timescale than the one of its\nrelaxation to equilibrium. Secondly, we establish sharp gradient estimates,\nwhich capture its fast local smoothness property. This is the first setting in\nwhich these two features of the averaging process -- concentration and local\nsmoothness -- can be quantified. These features carry useful information on a\nnumber of large scale properties of the averaging process. As an illustration\nof this fact, we determine the limit profile of its distance to equilibrium and\nderive a quantitative hydrodynamic limit for it. Finally, we discuss their\nimplications on cutoff for the binomial splitting process, the particle\nanalogue of the averaging process.",
            "author": [
                "Federico Sau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14176v1",
                "http://arxiv.org/pdf/2311.14176v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.FA",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14160v1",
            "title": "Efficient and Robust Jet Tagging at the LHC with Knowledge Distillation",
            "updated": "2023-11-23T19:00:02Z",
            "published": "2023-11-23T19:00:02Z",
            "summary": "The challenging environment of real-time data processing systems at the Large\nHadron Collider (LHC) strictly limits the computational complexity of\nalgorithms that can be deployed. For deep learning models, this implies that\nonly models with low computational complexity that have weak inductive bias are\nfeasible. To address this issue, we utilize knowledge distillation to leverage\nboth the performance of large models and the reduced computational complexity\nof small ones. In this paper, we present an implementation of knowledge\ndistillation, demonstrating an overall boost in the student models' performance\nfor the task of classifying jets at the LHC. Furthermore, by using a teacher\nmodel with a strong inductive bias of Lorentz symmetry, we show that we can\ninduce the same inductive bias in the student model which leads to better\nrobustness against arbitrary Lorentz boost.",
            "author": [
                "Ryan Liu",
                "Abhijith Gandrakota",
                "Jennifer Ngadiuba",
                "Maria Spiropulu",
                "Jean-Roch Vlimant"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14160v1",
                "http://arxiv.org/pdf/2311.14160v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14156v1",
            "title": "Variational Annealing on Graphs for Combinatorial Optimization",
            "updated": "2023-11-23T18:56:51Z",
            "published": "2023-11-23T18:56:51Z",
            "summary": "Several recent unsupervised learning methods use probabilistic approaches to\nsolve combinatorial optimization (CO) problems based on the assumption of\nstatistically independent solution variables. We demonstrate that this\nassumption imposes performance limitations in particular on difficult problem\ninstances. Our results corroborate that an autoregressive approach which\ncaptures statistical dependencies among solution variables yields superior\nperformance on many popular CO problems. We introduce subgraph tokenization in\nwhich the configuration of a set of solution variables is represented by a\nsingle token. This tokenization technique alleviates the drawback of the long\nsequential sampling procedure which is inherent to autoregressive methods\nwithout sacrificing expressivity. Importantly, we theoretically motivate an\nannealed entropy regularization and show empirically that it is essential for\nefficient and stable learning.",
            "author": [
                "Sebastian Sanokowski",
                "Wilhelm Berghammer",
                "Sepp Hochreiter",
                "Sebastian Lehner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14156v1",
                "http://arxiv.org/pdf/2311.14156v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DM",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14150v1",
            "title": "Logarithmic enumerative geometry for curves and sheaves",
            "updated": "2023-11-23T18:39:21Z",
            "published": "2023-11-23T18:39:21Z",
            "summary": "We propose a logarithmic enhancement of the Gromov-Witten/Donaldson-Thomas\ncorrespondence, with descendants, and study the behaviour of the correspondence\nunder simple normal crossings degenerations. The formulation of the logarithmic\ncorrespondence requires a matching of tangency conditions and relative\ninsertions. This is achieved via a version of the Nakajima basis for the\ncohomology of the Hilbert schemes of points on logarithmic surfaces.\n  We then establish a strong form of the degeneration formula in logarithmic DT\ntheory - the numerical DT invariants of the general fiber of a degeneration are\ndetermined by the numerical DT invariants attached to strata of the special\nfiber. The GW version of this result, which we also prove here, is a\nstrengthening of the currently known formulas. A key role is played by a\ncertain exotic class of insertions, introduced here, and can be thought of as\nnon-local incidence conditions coupled across multiple boundary strata of the\ntarget geometry.\n  Finally, we prove compatiblity of the new logarithmic GW/DT correspondence\nwith degenerations, and in particular, knowledge of the conjecture on the\nstrata of the special fiber of a degeneration implies it on the general fiber.\nSeveral examples are included to illustrate the nature and utility of the\nformula.",
            "author": [
                "Davesh Maulik",
                "Dhruv Ranganathan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14150v1",
                "http://arxiv.org/pdf/2311.14150v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14137v1",
            "title": "Privacy-Preserving Algorithmic Recourse",
            "updated": "2023-11-23T18:08:15Z",
            "published": "2023-11-23T18:08:15Z",
            "summary": "When individuals are subject to adverse outcomes from machine learning\nmodels, providing a recourse path to help achieve a positive outcome is\ndesirable. Recent work has shown that counterfactual explanations - which can\nbe used as a means of single-step recourse - are vulnerable to privacy issues,\nputting an individuals' privacy at risk. Providing a sequential multi-step path\nfor recourse can amplify this risk. Furthermore, simply adding noise to\nrecourse paths found from existing methods can impact the realism and\nactionability of the path for an end-user. In this work, we address privacy\nissues when generating realistic recourse paths based on instance-based\ncounterfactual explanations, and provide PrivRecourse: an end-to-end privacy\npreserving pipeline that can provide realistic recourse paths. PrivRecourse\nuses differentially private (DP) clustering to represent non-overlapping\nsubsets of the private dataset. These DP cluster centers are then used to\ngenerate recourse paths by forming a graph with cluster centers as the nodes,\nso that we can generate realistic - feasible and actionable - recourse paths.\nWe empirically evaluate our approach on finance datasets and compare it to\nsimply adding noise to data instances, and to using DP synthetic data, to\ngenerate the graph. We observe that PrivRecourse can provide paths that are\nprivate and realistic.",
            "author": [
                "Sikha Pentyala",
                "Shubham Sharma",
                "Sanjay Kariyappa",
                "Freddy Lecue",
                "Daniele Magazzeni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14137v1",
                "http://arxiv.org/pdf/2311.14137v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14123v1",
            "title": "Exponential Quantum Space Advantage for Approximating Maximum Directed\n  Cut in the Streaming Model",
            "updated": "2023-11-23T17:38:16Z",
            "published": "2023-11-23T17:38:16Z",
            "summary": "While the search for quantum advantage typically focuses on speedups in\nexecution time, quantum algorithms also offer the potential for advantage in\nspace complexity. Previous work has shown such advantages for data stream\nproblems, in which elements arrive and must be processed sequentially without\nrandom access, but these have been restricted to specially-constructed problems\n[Le Gall, SPAA `06] or polynomial advantage [Kallaugher, FOCS `21]. We show an\nexponential quantum space advantage for the maximum directed cut problem. This\nis the first known exponential quantum space advantage for any natural\nstreaming problem. This also constitutes the first unconditional exponential\nquantum resource advantage for approximating a discrete optimization problem in\nany setting.\n  Our quantum streaming algorithm $0.4844$-approximates the value of the\nlargest directed cut in a graph stream with $n$ vertices using polylog$(n)$\nspace, while previous work by Chou, Golovnev, and Velusamy [FOCS '20] implies\nthat obtaining an approximation ratio better than $4/9 \\approx 0.4444$ requires\n$\\Omega(\\sqrt{n})$ space for any classical streaming algorithm. Our result is\nbased on a recent $\\widetilde{\\text{O}}(\\sqrt{n})$ space classical streaming\napproach by Saxena, Singer, Sudan, and Velusamy [FOCS '23], with an additional\nimprovement in the approximation ratio due to recent work by Singer [APPROX\n'23].",
            "author": [
                "John Kallaugher",
                "Ojas Parekh",
                "Nadezhda Voronova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14123v1",
                "http://arxiv.org/pdf/2311.14123v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14077v1",
            "title": "RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation",
            "updated": "2023-11-23T16:08:52Z",
            "published": "2023-11-23T16:08:52Z",
            "summary": "Retrosynthesis poses a fundamental challenge in biopharmaceuticals, aiming to\naid chemists in finding appropriate reactant molecules and synthetic pathways\ngiven determined product molecules. With the reactant and product represented\nas 2D graphs, retrosynthesis constitutes a conditional graph-to-graph\ngenerative task. Inspired by the recent advancements in discrete diffusion\nmodels for graph generation, we introduce Retrosynthesis Diffusion (RetroDiff),\na novel diffusion-based method designed to address this problem. However,\nintegrating a diffusion-based graph-to-graph framework while retaining\nessential chemical reaction template information presents a notable challenge.\nOur key innovation is to develop a multi-stage diffusion process. In this\nmethod, we decompose the retrosynthesis procedure to first sample external\ngroups from the dummy distribution given products and then generate the\nexternal bonds to connect the products and generated groups. Interestingly,\nsuch a generation process is exactly the reverse of the widely adapted\nsemi-template retrosynthesis procedure, i.e. from reaction center\nidentification to synthon completion, which significantly reduces the error\naccumulation. Experimental results on the benchmark have demonstrated the\nsuperiority of our method over all other semi-template methods.",
            "author": [
                "Yiming Wang",
                "Yuxuan Song",
                "Minkai Xu",
                "Rui Wang",
                "Hao Zhou",
                "Weiying Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14077v1",
                "http://arxiv.org/pdf/2311.14077v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14758v1",
            "title": "Point2RBox: Combine Knowledge from Synthetic Visual Patterns for\n  End-to-end Oriented Object Detection with Single Point Supervision",
            "updated": "2023-11-23T15:57:41Z",
            "published": "2023-11-23T15:57:41Z",
            "summary": "With the rapidly increasing demand for oriented object detection (OOD),\nrecent research involving weakly-supervised detectors for learning rotated box\n(RBox) from the horizontal box (HBox) has attracted more and more attention. In\nthis paper, we explore a more challenging yet label-efficient setting, namely\nsingle point-supervised OOD, and present our approach called Point2RBox.\nSpecifically, we propose to leverage two principles: 1) Synthetic pattern\nknowledge combination: By sampling around each labelled point on the image, we\ntransfer the object feature to synthetic visual patterns with the known\nbounding box to provide the knowledge for box regression. 2) Transform\nself-supervision: With a transformed input image (e.g. scaled/rotated), the\noutput RBoxes are trained to follow the same transformation so that the network\ncan perceive the relative size/rotation between objects. The detector is\nfurther enhanced by a few devised techniques to cope with peripheral issues,\ne.g. the anchor/layer assignment as the size of the object is not available in\nour point supervision setting. To our best knowledge, Point2RBox is the first\nend-to-end solution for point-supervised OOD. In particular, our method uses a\nlightweight paradigm, yet it achieves a competitive performance among\npoint-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.",
            "author": [
                "Yu Yi",
                "Xue Yang",
                "Qingyun Li",
                "Feipeng Da",
                "Junchi Yan",
                "Jifeng Dai",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14758v1",
                "http://arxiv.org/pdf/2311.14758v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "http://www.w3.org/2005/Atom",
            "@type": "Entry",
            "@id": "urn:research:http://arxiv.org/abs/2311.14064v1",
            "title": "HGCLIP: Exploring Vision-Language Models with Graph Representations for\n  Hierarchical Understanding",
            "updated": "2023-11-23T15:42:42Z",
            "published": "2023-11-23T15:42:42Z",
            "summary": "Object categories are typically organized into a multi-granularity taxonomic\nhierarchy. When classifying categories at different hierarchy levels,\ntraditional uni-modal approaches focus primarily on image features, revealing\nlimitations in complex scenarios. Recent studies integrating Vision-Language\nModels (VLMs) with class hierarchies have shown promise, yet they fall short of\nfully exploiting the hierarchical relationships. These efforts are constrained\nby their inability to perform effectively across varied granularity of\ncategories. To tackle this issue, we propose a novel framework (HGCLIP) that\neffectively combines CLIP with a deeper exploitation of the Hierarchical class\nstructure via Graph representation learning. We explore constructing the class\nhierarchy into a graph, with its nodes representing the textual or image\nfeatures of each category. After passing through a graph encoder, the textual\nfeatures incorporate hierarchical structure information, while the image\nfeatures emphasize class-aware features derived from prototypes through the\nattention mechanism. Our approach demonstrates significant improvements on both\ngeneric and fine-grained visual recognition benchmarks. Our codes are fully\navailable at https://github.com/richard-peng-xia/HGCLIP.",
            "author": [
                "Peng Xia",
                "Xingtong Yu",
                "Ming Hu",
                "Lie Ju",
                "Zhiyong Wang",
                "Peibo Duan",
                "Zongyuan Ge"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14064v1",
                "http://arxiv.org/pdf/2311.14064v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    }
]