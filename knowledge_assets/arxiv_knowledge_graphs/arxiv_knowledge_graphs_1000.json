[
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14030v1",
            "title": "PrivateLoRA For Efficient Privacy Preserving LLM",
            "updated": "2023-11-23T14:36:30Z",
            "published": "2023-11-23T14:36:30Z",
            "summary": "End users face a choice between privacy and efficiency in current Large\nLanguage Model (LLM) service paradigms. In cloud-based paradigms, users are\nforced to compromise data locality for generation quality and processing speed.\nConversely, edge device paradigms maintain data locality but fail to deliver\nsatisfactory performance. In this work, we propose a novel LLM service paradigm\nthat distributes privacy-sensitive computation on edge devices and shared\ncomputation in the cloud. Only activations are transmitted between the central\ncloud and edge devices to ensure data locality. Our core innovation,\nPrivateLoRA, addresses the challenging communication overhead by exploiting the\nlow rank of residual activations, achieving over 95% communication reduction.\nConsequently, PrivateLoRA effectively maintains data locality and is extremely\nresource efficient. Under standard 5G networks, PrivateLoRA achieves throughput\nover 300% of device-only solutions for 7B models and over 80% of an A100 GPU\nfor 33B models. PrivateLoRA also provides tuning performance comparable to LoRA\nfor advanced personalization. Our approach democratizes access to\nstate-of-the-art generative AI for edge devices, paving the way for more\ntailored LLM experiences for the general public. To our knowledge, our proposed\nframework is the first efficient and privacy-preserving LLM solution in the\nliterature.",
            "author": [
                "Yiming Wang",
                "Yu Lin",
                "Xiaodong Zeng",
                "Guannan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14030v1",
                "http://arxiv.org/pdf/2311.14030v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14028v1",
            "title": "Continual Learning of Diffusion Models with Generative Distillation",
            "updated": "2023-11-23T14:33:03Z",
            "published": "2023-11-23T14:33:03Z",
            "summary": "Diffusion models are powerful generative models that achieve state-of-the-art\nperformance in tasks such as image synthesis. However, training them demands\nsubstantial amounts of data and computational resources. Continual learning\nwould allow for incrementally learning new tasks and accumulating knowledge,\nthus reusing already trained models would be possible. One potentially suitable\napproach is generative replay, where a copy of a generative model trained on\nprevious tasks produces synthetic data that are interleaved with data from the\ncurrent task. However, standard generative replay applied to diffusion models\nresults in a catastrophic loss in denoising capabilities. In this paper, we\npropose generative distillation, an approach that distils the entire reverse\nprocess of a diffusion model. We demonstrate that our approach significantly\nimproves the continual learning performance of generative replay with only a\nmoderate increase in the computational costs.",
            "author": [
                "Sergi Masip",
                "Pau Rodriguez",
                "Tinne Tuytelaars",
                "Gido M. van de Ven"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14028v1",
                "http://arxiv.org/pdf/2311.14028v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14004v1",
            "title": "Constraining self-interacting fermionic dark matter in admixed neutron\n  stars using multimessenger astronomy",
            "updated": "2023-11-23T13:39:17Z",
            "published": "2023-11-23T13:39:17Z",
            "summary": "We investigate the structure of admixed neutron stars with a regular hadronic\ncomponent and a fraction of fermionic self-interacting dark matter. Using two\nlimiting equations of state for the dense baryonic interior, constructed from\npiecewise generalised polytropes, and an asymmetric self-interacting fermionic\ndark component, we analyse different scenarios of admixed neutron stars\ndepending on the mass of dark fermions $m_\\chi$, interaction mediators\n$m_\\phi$, and self-interacting strengths $g$. We find that the contribution of\ndark matter to the masses and radii of neutron stars leads to tension with mass\nestimates of the pulsar J0453+1559, the least massive neutron star, and with\nthe constraints coming from the GW170817 event. We discuss the possibilities of\nconstraining dark matter model parameters $g$ and $y \\equiv m_\\chi/m_\\phi$,\nusing current existing knowledge on neutron star estimations of mass, radius,\nand tidal deformability, along with the accepted cosmological dark matter\nfreeze-out values and self-interaction cross-section to mass ratio,\n$\\sigma_\\mathrm{SI}/m_\\chi$, fitted to explain Bullet, Abell, and dwarf galaxy\ncluster dynamics. By assuming the most restrictive upper limit,\n$\\sigma_\\mathrm{SI}/m_\\chi < 0.1$ cm$^2$/g, along with dark matter freeze-out\nrange values, the allowed $g$-$y$ region is $0.01 \\lesssim g \\lesssim 0.1$,\nwith $0.5 \\lesssim y \\lesssim 200$. For the first time, the combination of\nupdated complementary restrictions is used to set constraints on\nself-interacting dark matter.",
            "author": [
                "Mauro Mariani",
                "Conrado Albertus",
                "M. del Rosario Alessandroni",
                "Milva G. Orsaria",
                "M. \u00c1ngeles P\u00e9rez-Garc\u00eda",
                "Ignacio F. Ranea-Sandoval"
            ],
            "link": [
                "http://dx.doi.org/10.1093/mnras/stad3658",
                "http://arxiv.org/abs/2311.14004v1",
                "http://arxiv.org/pdf/2311.14004v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13994v4",
            "title": "An Efficient Distributed Nash Equilibrium Seeking with Compressed and\n  Event-triggered Communication",
            "updated": "2023-11-30T12:02:57Z",
            "published": "2023-11-23T13:23:40Z",
            "summary": "Distributed Nash equilibrium (NE) seeking problems for networked games have\nbeen widely investigated in recent years. Despite the increasing attention,\ncommunication expenditure is becoming a major bottleneck for scaling up\ndistributed approaches within limited communication bandwidth between agents.\nTo reduce communication cost, an efficient distributed NE seeking (ETC-DNES)\nalgorithm is proposed to obtain an NE for games over directed graphs, where the\ncommunication efficiency is improved by event-triggered exchanges of compressed\ninformation among neighbors. ETC-DNES saves communication costs in both\ntransmitted bits and rounds of communication. Furthermore, our method only\nrequires the row-stochastic property of the adjacency matrix, unlike previous\napproaches that hinged on doubly-stochastic communication matrices. We provide\nconvergence guarantees for ETC-DNES on games with restricted strongly monotone\nmappings and testify its efficiency with no sacrifice on the accuracy. The\nalgorithm and analysis are extended to a compressed algorithm with stochastic\nevent-triggered mechanism (SETC-DNES). In SETC-DNES, we introduce a random\nvariable in the triggering condition to further enhance algorithm efficiency.\nWe demonstrate that SETC-DNES guarantees linear convergence to the NE while\nachieving even greater reductions in communication costs compared to ETC-DNES.\nFinally, numerical simulations illustrate the effectiveness of the proposed\nalgorithms.",
            "author": [
                "Xiaomeng Chen",
                "Wei Huo",
                "Yuchi Wu",
                "Subhrakanti Dey",
                "Ling Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13994v4",
                "http://arxiv.org/pdf/2311.13994v4"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13982v1",
            "title": "Probabilistic Tree-of-thought Reasoning for Answering\n  Knowledge-intensive Complex Questions",
            "updated": "2023-11-23T12:52:37Z",
            "published": "2023-11-23T12:52:37Z",
            "summary": "Large language models (LLMs) are capable of answering knowledge-intensive\ncomplex questions with chain-of-thought (CoT) reasoning. However, they tend to\ngenerate factually incorrect reasoning steps when the required knowledge is not\navailable or up-to-date in models' parameters. Recent works turn to retrieving\nexternal knowledge to augment CoT reasoning. Despite being promising, these\nchain-based methods suffer from: 1) Negative retrieval. Unnecessary or\nincorrect retrieval may mislead the reasoning; 2) Limited sight. Lacking the\nability to look backward or forward, a local error in one step will propagate\nalong the chain.\n  In this paper, we propose a novel approach: Probabilistic Tree-of-thought\nReasoning (ProbTree). First, LLMs translate a complex question into a query\ntree, in which each non-root node denotes a sub-question of its parent node.\nThen, probabilistic reasoning is conducted over the tree, by solving questions\nfrom leaf to root considering the confidence of both question decomposing and\nanswering. During reasoning, for leaf nodes, LLMs choose a more confident\nanswer from Closed-book QA that employs parametric knowledge and Open-book QA\nthat employs retrieved external knowledge, thus eliminating the negative\nretrieval problem. For non-leaf nodes, with the hierarchical structure, LLMs\nhave broader sights and are able to globally reason with the information from\nchild nodes, thus recovering from local errors. The experiments on three\nComplex QA datasets under the open-domain setting show that our approach\noutperforms SOTA methods significantly, demonstrating the effect of\nprobabilistic tree-of-thought reasoning.",
            "author": [
                "Shulin Cao",
                "Jiajie Zhang",
                "Jiaxin Shi",
                "Xin Lv",
                "Zijun Yao",
                "Qi Tian",
                "Juanzi Li",
                "Lei Hou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13982v1",
                "http://arxiv.org/pdf/2311.13982v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13980v1",
            "title": "On the Permutation-Representation Number of Bipartite Graphs using\n  Neighborhood Graphs",
            "updated": "2023-11-23T12:52:09Z",
            "published": "2023-11-23T12:52:09Z",
            "summary": "The problems of determining the permutation-representation number (prn) and\nthe representation number of bipartite graphs are open in the literature.\nMoreover, the decision problem corresponding to the determination of the prn of\na bipartite graph is NP-complete. However, these numbers were established for\ncertain subclasses of bipartite graphs, e.g., for crown graphs. Further, it was\nconjectured that the crown graphs have the highest representation number among\nthe bipartite graphs. In this work, first, we reconcile the relation between\nthe prn of a comparability graph and the dimension of its induced poset and\nreview the upper bounds on the prn of bipartite graphs. Then, we study the prn\nof bipartite graphs using the notion called neighborhood graphs. This approach\nsubstantiates the aforesaid conjecture and gives us theoretical evidence. In\nthis connection, we devise a polynomial-time procedure to construct a word that\nrepresents a given bipartite graph permutationally. Accordingly, we provide a\nbetter upper bound for the prn of bipartite graphs. Further, we construct a\nclass of bipartite graphs, viz., extended crown graphs, defined over posets and\ninvestigate its prn using the neighborhood graphs.",
            "author": [
                "Khyodeno Mozhui",
                "K. V. Krishna"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13980v1",
                "http://arxiv.org/pdf/2311.13980v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "math.CO",
                "68R10, 68R15, 05C90, 06A07"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13979v1",
            "title": "Location-Domination Type Problems Under the Mycielski Construction",
            "updated": "2023-11-23T12:49:05Z",
            "published": "2023-11-23T12:49:05Z",
            "summary": "We consider the following variants of the classical minimum dominating set\nproblem in graphs: locating-dominating set, locating total-dominating set and\nopen locating-dominating set. All these problems are known to be hard for\ngeneral graphs. A typical line of attack, therefore, is to either determine the\nminimum cardinalities of such sets in general or to establish bounds on these\nminimum cardinalities in special graph classes. In this paper, we study the\nminimum cardinalities of these variants of the dominating set under a graph\noperation defined by Mycielski in~\\cite{Mycielski1955} and is called the\nMycielski construction. We provide some general lower and upper bounds on the\nminimum sizes of the studied sets under the Mycielski construction. We apply\nthe Mycielski construction to stars, paths and cycles in particular, and\nprovide lower and upper bounds on the minimum cardinalities of such sets in\nthese graph classes. Our results either improve or attain the general known\nupper bounds.",
            "author": [
                "Silvia M. Bianchi",
                "Dipayan Chakraborty",
                "Yanina Lucarini",
                "Annegret K. Wagler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13979v1",
                "http://arxiv.org/pdf/2311.13979v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13967v2",
            "title": "Unconstrained learning of networked nonlinear systems via free\n  parametrization of stable interconnected operators",
            "updated": "2023-11-29T16:38:25Z",
            "published": "2023-11-23T12:31:28Z",
            "summary": "This paper characterizes a new parametrization of nonlinear networked\nincrementally $L_2$-bounded operators in discrete time. The distinctive novelty\nis that our parametrization is \\emph{free} -- that is, a sparse large-scale\noperator with bounded incremental $L_2$ gain is obtained for any choice of the\nreal values of our parameters. This property allows one to freely search over\noptimal parameters via unconstrained gradient descent, enabling direct\napplications in large-scale optimal control and system identification. Further,\nwe can embed prior knowledge about the interconnection topology and stability\nproperties of the system directly into the large-scale distributed operator we\ndesign. Our approach is extremely general in that it can seamlessly encapsulate\nand interconnect state-of-the-art Neural Network (NN) parametrizations of\nstable dynamical systems. To demonstrate the effectiveness of this approach, we\nprovide a simulation example showcasing the identification of a networked\nnonlinear system. The results underscore the superiority of our free\nparametrizations over standard NN-based identification methods where a prior\nover the system topology and local stability properties are not enforced.",
            "author": [
                "Leonardo Massai",
                "Danilo Saccani",
                "Luca Furieri",
                "Giancarlo Ferrari-Trecate"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13967v2",
                "http://arxiv.org/pdf/2311.13967v2"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13953v1",
            "title": "Learning Uniform Clusters on Hypersphere for Deep Graph-level Clustering",
            "updated": "2023-11-23T12:08:20Z",
            "published": "2023-11-23T12:08:20Z",
            "summary": "Graph clustering has been popularly studied in recent years. However, most\nexisting graph clustering methods focus on node-level clustering, i.e.,\ngrouping nodes in a single graph into clusters. In contrast, graph-level\nclustering, i.e., grouping multiple graphs into clusters, remains largely\nunexplored. Graph-level clustering is critical in a variety of real-world\napplications, such as, properties prediction of molecules and community\nanalysis in social networks. However, graph-level clustering is challenging due\nto the insufficient discriminability of graph-level representations, and the\ninsufficient discriminability makes deep clustering be more likely to obtain\ndegenerate solutions (cluster collapse). To address the issue, we propose a\nnovel deep graph-level clustering method called Uniform Deep Graph Clustering\n(UDGC). UDGC assigns instances evenly to different clusters and then scatters\nthose clusters on unit hypersphere, leading to a more uniform cluster-level\ndistribution and a slighter cluster collapse. Specifically, we first propose\nAugmentation-Consensus Optimal Transport (ACOT) for generating uniformly\ndistributed and reliable pseudo labels for partitioning clusters. Then we adopt\ncontrastive learning to scatter those clusters. Besides, we propose Center\nAlignment Optimal Transport (CAOT) for guiding the model to learn better\nparameters, which further promotes the cluster performance. Our empirical study\non eight well-known datasets demonstrates that UDGC significantly outperforms\nthe state-of-the-art models.",
            "author": [
                "Mengling Hu",
                "Chaochao Chen",
                "Weiming Liu",
                "Xinyi Zhang",
                "Xinting Liao",
                "Xiaolin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13953v1",
                "http://arxiv.org/pdf/2311.13953v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13945v1",
            "title": "Quantum network-entanglement measures",
            "updated": "2023-11-23T11:50:09Z",
            "published": "2023-11-23T11:50:09Z",
            "summary": "Quantum networks are of high interest nowadays and a quantum internet has\nbeen long envisioned. Network-entanglement adapts the notion of entanglement to\nthe network scenario and network-entangled states are considered to be a\nresource to overcome the limitations of a given network structure. In this work\nwe introduce measures of quantum network-entanglement that are well-defined\nwithin the general framework of quantum resource theories, which at the same\ntime have a clear operational interpretation characterizing the extra resources\nnecessary to prepare a targeted quantum state within a given network. In\nparticular, we define the network communication cost and the network round\ncomplexity, which turn out to be intimately related to graph-theoretic\nparameters. We also provide device-dependent and device-independent methods to\nestimate these measures.",
            "author": [
                "Zhen-Peng Xu",
                "Julio I. de Vicente",
                "Liang-Liang Sun",
                "Sixia Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13945v1",
                "http://arxiv.org/pdf/2311.13945v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13943v1",
            "title": "Further results on the permanental sums of bicyclic graphs",
            "updated": "2023-11-23T11:46:41Z",
            "published": "2023-11-23T11:46:41Z",
            "summary": "Let $G$ be a graph, and let $A(G)$ be the adjacency matrix of $G$. The\npermanental polynomial of $G$ is defined as $\\pi(G,x)=\\mathrm{per}(xI-A(G))$.\nThe permanental sum of $G$ can be defined as the sum of absolute value of\ncoefficients of $\\pi(G,x)$. Computing the permanental sum is $\\#$P-complete.\nAny a bicyclic graph can be generated from three types of induced subgraphs. In\nthis paper, we determine the upper bound of permanental sums of bicyclic graphs\ngenerated from each a type of induced subgraph. And we also determine the\nsecond maximal permanental sum of all bicyclic graphs.",
            "author": [
                "Tingzeng Wu",
                "Yinggang Bai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13943v1",
                "http://arxiv.org/pdf/2311.13943v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13937v1",
            "title": "Exploring Methods for Cross-lingual Text Style Transfer: The Case of\n  Text Detoxification",
            "updated": "2023-11-23T11:40:28Z",
            "published": "2023-11-23T11:40:28Z",
            "summary": "Text detoxification is the task of transferring the style of text from toxic\nto neutral. While here are approaches yielding promising results in monolingual\nsetup, e.g., (Dale et al., 2021; Hallinan et al., 2022), cross-lingual transfer\nfor this task remains a challenging open problem (Moskovskiy et al., 2022). In\nthis work, we present a large-scale study of strategies for cross-lingual text\ndetoxification -- given a parallel detoxification corpus for one language; the\ngoal is to transfer detoxification ability to another language for which we do\nnot have such a corpus. Moreover, we are the first to explore a new task where\ntext translation and detoxification are performed simultaneously, providing\nseveral strong baselines for this task. Finally, we introduce new automatic\ndetoxification evaluation metrics with higher correlations with human judgments\nthan previous benchmarks. We assess the most promising approaches also with\nmanual markup, determining the answer for the best strategy to transfer the\nknowledge of text detoxification between languages.",
            "author": [
                "Daryna Dementieva",
                "Daniil Moskovskiy",
                "David Dale",
                "Alexander Panchenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13937v1",
                "http://arxiv.org/pdf/2311.13937v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13934v1",
            "title": "Robustness-Reinforced Knowledge Distillation with Correlation Distance\n  and Network Pruning",
            "updated": "2023-11-23T11:34:48Z",
            "published": "2023-11-23T11:34:48Z",
            "summary": "The improvement in the performance of efficient and lightweight models (i.e.,\nthe student model) is achieved through knowledge distillation (KD), which\ninvolves transferring knowledge from more complex models (i.e., the teacher\nmodel). However, most existing KD techniques rely on Kullback-Leibler (KL)\ndivergence, which has certain limitations. First, if the teacher distribution\nhas high entropy, the KL divergence's mode-averaging nature hinders the\ntransfer of sufficient target information. Second, when the teacher\ndistribution has low entropy, the KL divergence tends to excessively focus on\nspecific modes, which fails to convey an abundant amount of valuable knowledge\nto the student. Consequently, when dealing with datasets that contain numerous\nconfounding or challenging samples, student models may struggle to acquire\nsufficient knowledge, resulting in subpar performance. Furthermore, in previous\nKD approaches, we observed that data augmentation, a technique aimed at\nenhancing a model's generalization, can have an adverse impact. Therefore, we\npropose a Robustness-Reinforced Knowledge Distillation (R2KD) that leverages\ncorrelation distance and network pruning. This approach enables KD to\neffectively incorporate data augmentation for performance improvement.\nExtensive experiments on various datasets, including CIFAR-100, FGVR,\nTinyImagenet, and ImageNet, demonstrate our method's superiority over current\nstate-of-the-art methods.",
            "author": [
                "Seonghak Kim",
                "Gyeongdo Ham",
                "Yucheol Cho",
                "Daeshik Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13934v1",
                "http://arxiv.org/pdf/2311.13934v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13930v1",
            "title": "Periodically Exchange Teacher-Student for Source-Free Object Detection",
            "updated": "2023-11-23T11:30:54Z",
            "published": "2023-11-23T11:30:54Z",
            "summary": "Source-free object detection (SFOD) aims to adapt the source detector to\nunlabeled target domain data in the absence of source domain data. Most SFOD\nmethods follow the same self-training paradigm using mean-teacher (MT)\nframework where the student model is guided by only one single teacher model.\nHowever, such paradigm can easily fall into a training instability problem that\nwhen the teacher model collapses uncontrollably due to the domain shift, the\nstudent model also suffers drastic performance degradation. To address this\nissue, we propose the Periodically Exchange Teacher-Student (PETS) method, a\nsimple yet novel approach that introduces a multiple-teacher framework\nconsisting of a static teacher, a dynamic teacher, and a student model. During\nthe training phase, we periodically exchange the weights between the static\nteacher and the student model. Then, we update the dynamic teacher using the\nmoving average of the student model that has already been exchanged by the\nstatic teacher. In this way, the dynamic teacher can integrate knowledge from\npast periods, effectively reducing error accumulation and enabling a more\nstable training process within the MT-based framework. Further, we develop a\nconsensus mechanism to merge the predictions of two teacher models to provide\nhigher-quality pseudo labels for student model. Extensive experiments on\nmultiple SFOD benchmarks show that the proposed method achieves\nstate-of-the-art performance compared with other related methods, demonstrating\nthe effectiveness and superiority of our method on SFOD task.",
            "author": [
                "Qipeng Liu",
                "Luojun Lin",
                "Zhifeng Shen",
                "Zhifeng Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13930v1",
                "http://arxiv.org/pdf/2311.13930v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14750v2",
            "title": "Attribute-Aware Representation Rectification for Generalized Zero-Shot\n  Learning",
            "updated": "2023-12-01T15:25:35Z",
            "published": "2023-11-23T11:30:32Z",
            "summary": "Generalized Zero-shot Learning (GZSL) has yielded remarkable performance by\ndesigning a series of unbiased visual-semantics mappings, wherein, the\nprecision relies heavily on the completeness of extracted visual features from\nboth seen and unseen classes. However, as a common practice in GZSL, the\npre-trained feature extractor may easily exhibit difficulty in capturing\ndomain-specific traits of the downstream tasks/datasets to provide fine-grained\ndiscriminative features, i.e., domain bias, which hinders the overall\nrecognition performance, especially for unseen classes. Recent studies\npartially address this issue by fine-tuning feature extractors, while may\ninevitably incur catastrophic forgetting and overfitting issues. In this paper,\nwe propose a simple yet effective Attribute-Aware Representation Rectification\nframework for GZSL, dubbed $\\mathbf{(AR)^{2}}$, to adaptively rectify the\nfeature extractor to learn novel features while keeping original valuable\nfeatures. Specifically, our method consists of two key components, i.e.,\nUnseen-Aware Distillation (UAD) and Attribute-Guided Learning (AGL). During\ntraining, UAD exploits the prior knowledge of attribute texts that are shared\nby both seen/unseen classes with attention mechanisms to detect and maintain\nunseen class-sensitive visual features in a targeted manner, and meanwhile, AGL\naims to steer the model to focus on valuable features and suppress them to fit\nnoisy elements in the seen classes by attribute-guided representation learning.\nExtensive experiments on various benchmark datasets demonstrate the\neffectiveness of our method.",
            "author": [
                "Zhijie Rao",
                "Jingcai Guo",
                "Xiaocheng Lu",
                "Qihua Zhou",
                "Jie Zhang",
                "Kang Wei",
                "Chenxin Li",
                "Song Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14750v2",
                "http://arxiv.org/pdf/2311.14750v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13921v1",
            "title": "Some Like It Small: Czech Semantic Embedding Models for Industry\n  Applications",
            "updated": "2023-11-23T11:14:13Z",
            "published": "2023-11-23T11:14:13Z",
            "summary": "This article focuses on the development and evaluation of Small-sized Czech\nsentence embedding models. Small models are important components for real-time\nindustry applications in resource-constrained environments. Given the limited\navailability of labeled Czech data, alternative approaches, including\npre-training, knowledge distillation, and unsupervised contrastive fine-tuning,\nare investigated. Comprehensive intrinsic and extrinsic analyses are conducted,\nshowcasing the competitive performance of our models compared to significantly\nlarger counterparts, with approximately 8 times smaller size and 5 times faster\nspeed than conventional Base-sized models. To promote cooperation and\nreproducibility, both the models and the evaluation pipeline are made publicly\naccessible. Ultimately, this article presents practical applications of the\ndeveloped sentence embedding models in Seznam.cz, the Czech search engine.\nThese models have effectively replaced previous counterparts, enhancing the\noverall search experience for instance, in organic search, featured snippets,\nand image search. This transition has yielded improved performance.",
            "author": [
                "Ji\u0159\u00ed Bedn\u00e1\u0159",
                "Jakub N\u00e1plava",
                "Petra Baran\u010d\u00edkov\u00e1",
                "Ond\u0159ej Lisick\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13921v1",
                "http://arxiv.org/pdf/2311.13921v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13903v1",
            "title": "Borsuk number for planar convex bodies",
            "updated": "2023-11-23T10:36:54Z",
            "published": "2023-11-23T10:36:54Z",
            "summary": "By using some simple tools from graph theory, we obtain a characterization of\nthe planar convex bodies with Borsuk number equal to two. This result allows to\ngive some examples of planar convex bodies with Borsuk number equal to three.\nMoreover, we also prove that the unique centrally symmetric planar convex\nbodies with Borsuk number equal to three are the Euclidean balls.",
            "author": [
                "Antonio Ca\u00f1ete",
                "Uwe Schnell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13903v1",
                "http://arxiv.org/pdf/2311.13903v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "52A10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13889v1",
            "title": "SIMBa: System Identification Methods leveraging Backpropagation",
            "updated": "2023-11-23T10:20:12Z",
            "published": "2023-11-23T10:20:12Z",
            "summary": "This manuscript details the SIMBa toolbox (System Identification Methods\nleveraging Backpropagation), which uses well-established Machine Learning tools\nfor discrete-time linear multi-step-ahead state-space System Identification\n(SI). Backed up by novel linear-matrix-inequality-based free parametrizations\nof Schur matrices to guarantee the stability of the identified model by design,\nSIMBa allows for seamless integration of prior system knowledge. In particular,\nit can simultaneously enforce desired system properties - such as sparsity\npatterns - and stability on the model, solving an open SI problem.\n  We extensively investigate SIMBa's behavior when identifying diverse systems\nwith various properties from both simulated and real-world data. Overall, we\nfind it consistently outperforms traditional stable subspace identification\nmethods, and sometimes significantly, even while enforcing desired model\nproperties. These results hint at the potential of SIMBa to pave the way for\ngeneric structured nonlinear SI. The toolbox is open-sourced on\nhttps://github.com/Cemempamoi/simba.",
            "author": [
                "Loris Di Natale",
                "Muhammad Zakwan",
                "Philipp Heer",
                "Giancarlo Ferrari Trecate",
                "Colin N. Jones"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13889v1",
                "http://arxiv.org/pdf/2311.13889v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14749v1",
            "title": "Compositional Zero-shot Learning via Progressive Language-based\n  Observations",
            "updated": "2023-11-23T10:14:23Z",
            "published": "2023-11-23T10:14:23Z",
            "summary": "Compositional zero-shot learning aims to recognize unseen state-object\ncompositions by leveraging known primitives (state and object) during training.\nHowever, effectively modeling interactions between primitives and generalizing\nknowledge to novel compositions remains a perennial challenge. There are two\nkey factors: object-conditioned and state-conditioned variance, i.e., the\nappearance of states (or objects) can vary significantly when combined with\ndifferent objects (or states). For instance, the state \"old\" can signify a\nvintage design for a \"car\" or an advanced age for a \"cat\". In this paper, we\nargue that these variances can be mitigated by predicting composition\ncategories based on pre-observed primitive. To this end, we propose Progressive\nLanguage-based Observations (PLO), which can dynamically determine a better\nobservation order of primitives. These observations comprise a series of\nconcepts or languages that allow the model to understand image content in a\nstep-by-step manner. Specifically, PLO adopts pre-trained vision-language\nmodels (VLMs) to empower the model with observation capabilities. We further\ndevise two variants: 1) PLO-VLM: a two-step method, where a pre-observing\nclassifier dynamically determines the observation order of two primitives. 2)\nPLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to\ncraft composition-specific prompts for step-by-step observing. Extensive\nablations on three challenging datasets demonstrate the superiority of PLO\ncompared with state-of-the-art methods, affirming its abilities in\ncompositional recognition.",
            "author": [
                "Lin Li",
                "Guikun Chen",
                "Jun Xiao",
                "Long Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14749v1",
                "http://arxiv.org/pdf/2311.14749v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13877v1",
            "title": "Locally Optimal Descent for Dynamic Stepsize Scheduling",
            "updated": "2023-11-23T09:57:35Z",
            "published": "2023-11-23T09:57:35Z",
            "summary": "We introduce a novel dynamic learning-rate scheduling scheme grounded in\ntheory with the goal of simplifying the manual and time-consuming tuning of\nschedules in practice. Our approach is based on estimating the locally-optimal\nstepsize, guaranteeing maximal descent in the direction of the stochastic\ngradient of the current step. We first establish theoretical convergence bounds\nfor our method within the context of smooth non-convex stochastic optimization,\nmatching state-of-the-art bounds while only assuming knowledge of the\nsmoothness parameter. We then present a practical implementation of our\nalgorithm and conduct systematic experiments across diverse datasets and\noptimization algorithms, comparing our scheme with existing state-of-the-art\nlearning-rate schedulers. Our findings indicate that our method needs minimal\ntuning when compared to existing approaches, removing the need for auxiliary\nmanual schedules and warm-up phases and achieving comparable performance with\ndrastically reduced parameter tuning.",
            "author": [
                "Gilad Yehudai",
                "Alon Cohen",
                "Amit Daniely",
                "Yoel Drori",
                "Tomer Koren",
                "Mariano Schain"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13877v1",
                "http://arxiv.org/pdf/2311.13877v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13874v1",
            "title": "Transitive inference as probabilistic preference learning",
            "updated": "2023-11-23T09:40:56Z",
            "published": "2023-11-23T09:40:56Z",
            "summary": "Transitive Inference (TI) is a cognitive task that assesses an organism's\nability to infer novel relations between items based on previously acquired\nknowledge. TI is known for exhibiting various behavioral and neural signatures,\nsuch as the Serial Position Effect (SPE), Symbolic Distance Effect (SDE), and\nthe brain's capacity to maintain and merge separate ranking models. We propose\na novel framework that casts TI as a probabilistic preference learning task,\nusing one-parameter Mallows models. We present a series of simulations that\nhighlight the effectiveness of our novel approach. We show that the Mallows\nranking model natively reproduces SDE and SPE. Furthermore, extending the model\nusing Bayesian selection showcases its capacity to generate and merge ranking\nhypotheses as pairs with connecting symbols are encountered. Finally, we employ\nneural networks to replicate Mallows models, demonstrating how this framework\naligns with observed prefrontal neural activity during TI. Our innovative\napproach sheds new light on the nature of TI, emphasizing the potential of\nprobabilistic preference learning for unraveling its underlying neural\nmechanisms.",
            "author": [
                "Francesco Mannella",
                "Giovanni Pezzulo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13874v1",
                "http://arxiv.org/pdf/2311.13874v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13864v1",
            "title": "Which Matters Most in Making Fund Investment Decisions? A\n  Multi-granularity Graph Disentangled Learning Framework",
            "updated": "2023-11-23T09:08:43Z",
            "published": "2023-11-23T09:08:43Z",
            "summary": "In this paper, we highlight that both conformity and risk preference matter\nin making fund investment decisions beyond personal interest and seek to\njointly characterize these aspects in a disentangled manner. Consequently, we\ndevelop a novel M ulti-granularity Graph Disentangled Learning framework named\nMGDL to effectively perform intelligent matching of fund investment products.\nBenefiting from the well-established fund graph and the attention module,\nmulti-granularity user representations are derived from historical behaviors to\nseparately express personal interest, conformity and risk preference in a\nfine-grained way. To attain stronger disentangled representations with specific\nsemantics, MGDL explicitly involve two self-supervised signals, i.e., fund type\nbased contrasts and fund popularity. Extensive experiments in offline and\nonline environments verify the effectiveness of MGDL.",
            "author": [
                "Chunjing Gan",
                "Binbin Hu",
                "Bo Huang",
                "Tianyu Zhao",
                "Yingru Lin",
                "Wenliang Zhong",
                "Zhiqiang Zhang",
                "Jun Zhou",
                "Chuan Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13864v1",
                "http://arxiv.org/pdf/2311.13864v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13859v1",
            "title": "A TETRA-based System for Remote-Health Monitoring of First Responders:\n  Peak AoI Assessment in Direct and Trunked Mode",
            "updated": "2023-11-23T08:58:48Z",
            "published": "2023-11-23T08:58:48Z",
            "summary": "In this paper, we study peak age of information (PAoI) performance of a novel\nIoT solution for remote health-monitoring of first responders over TErrestrial\nTrunked RAdio (TETRA) links. The solution features a set of sensors embedded in\na smart garment that periodically record and send physiological parameters of\nfirst responders to a remote agent. The received data is analyzed by the remote\nagent, which feeds back notifications and warnings to the first responders in\nthe form of electrotactile stimuli. The communication in the system is\nperformed over the TETRA Short Data Service (SDS), which is the default option\nfor the development of third-party applications and which has rather limited\ncapabilities. The choice of the PAoI as the parameter of interest is motivated\nby its suitable to measure data freshness in IoT applications with periodic\nmonitoring. We derive closed-form expressions of PAoI for different\npacket-management schemes allowed by the TETRA standard, and verify the\nanalytical results through extensive simulations under varying message\ngeneration rates. Our results provide important insights on the expected PAoI\nperformance, which can be used for the system design guidelines. To the best of\nour knowledge, this is the first work that analyzes AoI performance of TETRA\nnetworks.",
            "author": [
                "Hossam Farag",
                "Aleksandar Vujic",
                "Milos Kostic",
                "Goran Bijelic",
                "Cedomir Stefanovic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13859v1",
                "http://arxiv.org/pdf/2311.13859v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13852v1",
            "title": "A Cross Attention Approach to Diagnostic Explainability using Clinical\n  Practice Guidelines for Depression",
            "updated": "2023-11-23T08:42:18Z",
            "published": "2023-11-23T08:42:18Z",
            "summary": "The lack of explainability using relevant clinical knowledge hinders the\nadoption of Artificial Intelligence-powered analysis of unstructured clinical\ndialogue. A wealth of relevant, untapped Mental Health (MH) data is available\nin online communities, providing the opportunity to address the explainability\nproblem with substantial potential impact as a screening tool for both online\nand offline applications. We develop a method to enhance attention in popular\ntransformer models and generate clinician-understandable explanations for\nclassification by incorporating external clinical knowledge. Inspired by how\nclinicians rely on their expertise when interacting with patients, we leverage\nrelevant clinical knowledge to model patient inputs, providing meaningful\nexplanations for classification. This will save manual review time and engender\ntrust. We develop such a system in the context of MH using clinical practice\nguidelines (CPG) for diagnosing depression, a mental health disorder of global\nconcern. We propose an application-specific language model called ProcesS\nknowledge-infused cross ATtention (PSAT), which incorporates CPGs when\ncomputing attention. Through rigorous evaluation on three expert-curated\ndatasets related to depression, we demonstrate application-relevant\nexplainability of PSAT. PSAT also surpasses the performance of nine baseline\nmodels and can provide explanations where other baselines fall short. We\ntransform a CPG resource focused on depression, such as the Patient Health\nQuestionnaire (e.g. PHQ-9) and related questions, into a machine-readable\nontology using SNOMED-CT. With this resource, PSAT enhances the ability of\nmodels like GPT-3.5 to generate application-relevant explanations.",
            "author": [
                "Sumit Dalal",
                "Deepa Tilwani",
                "Manas Gaur",
                "Sarika Jain",
                "Valerie Shalin",
                "Amit Seth"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13852v1",
                "http://arxiv.org/pdf/2311.13852v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13843v1",
            "title": "Exact Combinatorial Optimization with Temporo-Attentional Graph Neural\n  Networks",
            "updated": "2023-11-23T08:07:15Z",
            "published": "2023-11-23T08:07:15Z",
            "summary": "Combinatorial optimization finds an optimal solution within a discrete set of\nvariables and constraints. The field has seen tremendous progress both in\nresearch and industry. With the success of deep learning in the past decade, a\nrecent trend in combinatorial optimization has been to improve state-of-the-art\ncombinatorial optimization solvers by replacing key heuristic components with\nmachine learning (ML) models. In this paper, we investigate two essential\naspects of machine learning algorithms for combinatorial optimization: temporal\ncharacteristics and attention. We argue that for the task of variable selection\nin the branch-and-bound (B&B) algorithm, incorporating the temporal information\nas well as the bipartite graph attention improves the solver's performance. We\nsupport our claims with intuitions and numerical results over several standard\ndatasets used in the literature and competitions. Code is available at:\nhttps://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=047c6cf2-8463-40d7-b92f-7b2ca998e935",
            "author": [
                "Mehdi Seyfi",
                "Amin Banitalebi-Dehkordi",
                "Zirui Zhou",
                "Yong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13843v1",
                "http://arxiv.org/pdf/2311.13843v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.MS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13841v1",
            "title": "Adversarial defense based on distribution transfer",
            "updated": "2023-11-23T08:01:18Z",
            "published": "2023-11-23T08:01:18Z",
            "summary": "The presence of adversarial examples poses a significant threat to deep\nlearning models and their applications. Existing defense methods provide\ncertain resilience against adversarial examples, but often suffer from\ndecreased accuracy and generalization performance, making it challenging to\nachieve a trade-off between robustness and generalization. To address this, our\npaper interprets the adversarial example problem from the perspective of sample\ndistribution and proposes a defense method based on distribution shift,\nleveraging the distribution transfer capability of a diffusion model for\nadversarial defense. The core idea is to exploit the discrepancy between normal\nand adversarial sample distributions to achieve adversarial defense using a\npretrained diffusion model. Specifically, an adversarial sample undergoes a\nforward diffusion process, moving away from the source distribution, followed\nby a reverse process guided by the protected model (victim model) output to map\nit back to the normal distribution. Experimental evaluations on CIFAR10 and\nImageNet30 datasets are conducted, comparing with adversarial training and\ninput preprocessing methods. For infinite-norm attacks with 8/255 perturbation,\naccuracy rates of 78.1% and 83.5% are achieved, respectively. For 2-norm\nattacks with 128/255 perturbation, accuracy rates are 74.3% and 82.5%.\nAdditional experiments considering perturbation amplitude, diffusion\niterations, and adaptive attacks also validate the effectiveness of the\nproposed method. Results demonstrate that even when the attacker has knowledge\nof the defense, the proposed distribution-based method effectively withstands\nadversarial examples. It fills the gaps of traditional approaches, restoring\nhigh-quality original samples and showcasing superior performance in model\nrobustness and generalization.",
            "author": [
                "Jiahao Chen",
                "Diqun Yan",
                "Li Dong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13841v1",
                "http://arxiv.org/pdf/2311.13841v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13838v1",
            "title": "Primal subgradient methods with predefined stepsizes",
            "updated": "2023-11-23T07:49:38Z",
            "published": "2023-11-23T07:49:38Z",
            "summary": "In this paper, we suggest a new framework for analyzing primal subgradient\nmethods for nonsmooth convex optimization problems. We show that the classical\nstep-size rules, based on normalization of subgradient, or on the knowledge of\noptimal value of the objective function, need corrections when they are applied\nto optimization problems with constraints. Their proper modifications allow a\nsignificant acceleration of these schemes when the objective function has\nfavorable properties (smoothness, strong convexity). We show how the new\nmethods can be used for solving optimization problems with functional\nconstraints with possibility to approximate the optimal Lagrange multipliers.\nOne of our primal-dual methods works also for unbounded feasible set.",
            "author": [
                "Yurii Nesterov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13838v1",
                "http://arxiv.org/pdf/2311.13838v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13817v1",
            "title": "Molecular Identification and Peak Assignment: Leveraging Multi-Level\n  Multimodal Alignment on NMR",
            "updated": "2023-11-23T05:52:28Z",
            "published": "2023-11-23T05:52:28Z",
            "summary": "Nuclear magnetic resonance (NMR) spectroscopy plays an essential role across\nvarious scientific disciplines, providing valuable insights into molecular\ndynamics and interactions. Despite the promise of AI-enhanced NMR prediction\nmodels, challenges persist in the interpretation of spectra for tasks such as\nmolecular retrieval, isomer recognition, and peak assignment. In response, this\npaper introduces Multi-Level Multimodal Alignment with Knowledge-Guided\nInstance-Wise Discrimination (K-M3AID) to establish meaningful correspondences\nbetween two heterogeneous modalities: molecular graphs (structures) and NMR\nspectra. In particular, K-M3AID employs a dual-coordinated contrastive learning\narchitecture, and incorporates a graph-level alignment module, a node-level\nalignment module, and a communication channel. Notably, the framework\nintroduces knowledge-guided instance-wise discrimination into contrastive\nlearning within the node-level alignment module, significantly enhancing\naccuracy in cross-modal alignment. Additionally, K-M3AID showcases its\ncapability of meta-learning by demonstrating that skills acquired during\nnode-level alignment positively impact graph-level alignment. Empirical\nvalidation underscores K-M3AID's effectiveness in addressing multiple zero-shot\ntasks, offering a promising solution to bridge the gap between structural\ninformation and spectral data in complex NMR scenarios.",
            "author": [
                "Hao Xu",
                "Zhengyang Zhou",
                "Pengyu Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13817v1",
                "http://arxiv.org/pdf/2311.13817v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.chem-ph",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13811v2",
            "title": "Education distillation:getting student models to learn in shcools",
            "updated": "2023-11-27T02:32:54Z",
            "published": "2023-11-23T05:20:18Z",
            "summary": "Knowledge distillation is one of the methods for model compression, and\nexisting knowledge distillation techniques focus on how to improve the\ndistillation algorithm so as to enhance the distillation efficiency. This paper\nintroduces dynamic incremental learning into knowledge distillation and\nproposes a distillation strategy for education distillation. Specifically, it\nis proposed to take fragmented student models divided from the complete student\nmodel as lower-grade models. As the grade level rises, fragmented student\nmodels deepen in conjunction with designed teaching reference layers, while\nlearning and distilling from more teacher models. By moving from lower to\nhigher grades, fragmented student models were gradually integrated into a\ncomplete target student model, and the performance of the student models\ngradually improved from lower to higher grades of the stage. Education\ndistillation strategies combined with distillation algorithms outperform the\nresults of single distillation algorithms on the public dataset\nCIFAR100,Caltech256, Food-101 dataset.",
            "author": [
                "Ling Feng",
                "Danyang Li",
                "Tianhao Wu",
                "Xuliang Duan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13811v2",
                "http://arxiv.org/pdf/2311.13811v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13810v1",
            "title": "Bridging Classical and Quantum Machine Learning: Knowledge Transfer From\n  Classical to Quantum Neural Networks Using Knowledge Distillation",
            "updated": "2023-11-23T05:06:43Z",
            "published": "2023-11-23T05:06:43Z",
            "summary": "Very recently, studies have shown that quantum neural networks surpass\nclassical neural networks in tasks like image classification when a similar\nnumber of learnable parameters are used. However, the development and\noptimization of quantum models are currently hindered by issues such as qubit\ninstability and limited qubit availability, leading to error-prone systems with\nweak performance. In contrast, classical models can exhibit high-performance\nowing to substantial resource availability. As a result, more studies have been\nfocusing on hybrid classical-quantum integration. A line of research\nparticularly focuses on transfer learning through classical-quantum integration\nor quantum-quantum approaches. Unlike previous studies, this paper introduces a\nnew method to transfer knowledge from classical to quantum neural networks\nusing knowledge distillation, effectively bridging the gap between classical\nmachine learning and emergent quantum computing techniques. We adapt classical\nconvolutional neural network (CNN) architectures like LeNet and AlexNet to\nserve as teacher networks, facilitating the training of student quantum models\nby sending supervisory signals during backpropagation through KL-divergence.\nThe approach yields significant performance improvements for the quantum models\nby solely depending on classical CNNs, with quantum models achieving an average\naccuracy improvement of 0.80% on the MNIST dataset and 5.40% on the more\ncomplex Fashion MNIST dataset. Applying this technique eliminates the\ncumbersome training of huge quantum models for transfer learning in\nresource-constrained settings and enables re-using existing pre-trained\nclassical models to improve performance.Thus, this study paves the way for\nfuture research in quantum machine learning (QML) by positioning knowledge\ndistillation as a core technique for advancing QML applications.",
            "author": [
                "Mohammad Junayed Hasan",
                "M. R. C. Mahdy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13810v1",
                "http://arxiv.org/pdf/2311.13810v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13803v1",
            "title": "Quantum Computing Dataset of Maximum Independent Set Problem on King's\n  Lattice of over Hundred Rydberg Atoms",
            "updated": "2023-11-23T04:28:30Z",
            "published": "2023-11-23T04:28:30Z",
            "summary": "Finding the maximum independent set (MIS) of a large-size graph is a\nnondeterministic polynomial-time (NP)-complete problem not efficiently solvable\nwith classical computations. Here, we present a set of quantum adiabatic\ncomputing data of Rydberg-atom experiments performed to solve the MIS problem\nof up to 141 atoms randomly arranged on the King's lattice. A total of 582,916\nevents of Rydberg-atom measurements are collected for experimental MIS\nsolutions of 733,853 different graphs. We provide the raw image data along with\nthe entire binary determinations of the measured many-body ground states and\nthe classified graph data, to offer bench-mark testing and advanced data-driven\nanalyses for validation of the performance and system improvements of the\nRydberg-atom approach.",
            "author": [
                "Kangheun Kim",
                "Minhyuk Kim",
                "Juyoung Park",
                "Andrew Byun",
                "Jaewook Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13803v1",
                "http://arxiv.org/pdf/2311.13803v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.atom-ph",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13798v1",
            "title": "Efficient $k$-Clique Listing: An Edge-Oriented Branching Strategy",
            "updated": "2023-11-23T04:01:51Z",
            "published": "2023-11-23T04:01:51Z",
            "summary": "$k$-clique listing is a vital graph mining operator with diverse applications\nin various networks. The state-of-the-art algorithms all adopt a\nbranch-and-bound (BB) framework with a vertex-oriented branching strategy\n(called VBBkC), which forms a sub-branch by expanding a partial $k$-clique with\na vertex. These algorithms have the time complexity of $O(k m\n(\\delta/2)^{k-2})$, where $m$ is the number of edges in the graph and $\\delta$\nis the degeneracy of the graph. In this paper, we propose a BB framework with a\nnew edge-oriented branching (called EBBkC), which forms a sub-branch by\nexpanding a partial $k$-clique with two vertices that connect each other (which\ncorrespond to an edge). We explore various edge orderings for EBBkC such that\nit achieves a time complexity of $O(\\delta m + k m (\\tau/2)^{k-2})$, where\n$\\tau$ is an integer related to the maximum truss number of the graph and we\nhave $\\tau < \\delta$. The time complexity of EBBkC is better than that of VBBkC\nalgorithms for $k>3$ since both $O(\\delta m)$ and $O(k m (\\tau/2)^{k-2})$ are\nbounded by $O(k m (\\delta/2)^{k-2})$. Furthermore, we develop specialized\nalgorithms for sub-branches on dense graphs so that we can early-terminate them\nand apply the specialized algorithms. We conduct extensive experiments on 19\nreal graphs, and the results show that our newly developed EBBkC-based\nalgorithms with the early termination technique consistently and largely\noutperform the state-of-the-art (VBBkC-based) algorithms.",
            "author": [
                "Kaixin Wang",
                "Kaiqiang Yu",
                "Cheng Long"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13798v1",
                "http://arxiv.org/pdf/2311.13798v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13792v2",
            "title": "Expanders Satisfy the Weak Meyniel Conjecture",
            "updated": "2023-11-27T17:38:56Z",
            "published": "2023-11-23T03:51:43Z",
            "summary": "We show that if $\\{G_n\\}_{n\\geq 1}$ is a sequence of graphs of order $n$ with\nbounded maximum degree and isoperimetric function $\\Phi(G_n,n^{1-\\alpha})$\nbounded away from $0$ as $n\\rightarrow \\infty$, then the cop number of $G_n$ is\nat most $O(n^{\\frac{1+\\alpha}{2}+o(1)})$. It is unclear if this bound is tight\ngiven our assumption that the maximum degree of our sequence is bounded. All\nthe same, recent work by Hosseini, Mohar, and Gonzalez Hermosillo de la Maza\nstrongly motivates considering the bounded degree case, as they show that\nproving that the cop number of graphs of bounded max degree is\n$O(n^{1-\\epsilon})$ would also prove that for all graphs the cop number is\n$O(n^{1-\\alpha})$ for some $\\alpha>0$. This would resolve a weak version of a\nnotable conjecture by Meyniel. Next, we prove that certain unintuitive graphs\nnecessarily exist and have large cop number if the weak version Meyniel's\nconjecture is false. We concluding by conjecturing that any such unintuitive\ngraphs must be expanders and would therefore not have large cop number by the\nupper bound we show. Thus, our conjecture would imply a weak version of\nMeyniel's conjecture.",
            "author": [
                "Alexander Clow"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13792v2",
                "http://arxiv.org/pdf/2311.13792v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C57, 05C48"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13789v1",
            "title": "Knowledge Distillation Based Semantic Communications For Multiple Users",
            "updated": "2023-11-23T03:28:14Z",
            "published": "2023-11-23T03:28:14Z",
            "summary": "Deep learning (DL) has shown great potential in revolutionizing the\ntraditional communications system. Many applications in communications have\nadopted DL techniques due to their powerful representation ability. However,\nthe learning-based methods can be dependent on the training dataset and perform\nworse on unseen interference due to limited model generalizability and\ncomplexity. In this paper, we consider the semantic communication (SemCom)\nsystem with multiple users, where there is a limited number of training samples\nand unexpected interference. To improve the model generalization ability and\nreduce the model size, we propose a knowledge distillation (KD) based system\nwhere Transformer based encoder-decoder is implemented as the semantic\nencoder-decoder and fully connected neural networks are implemented as the\nchannel encoder-decoder. Specifically, four types of knowledge transfer and\nmodel compression are analyzed. Important system and model parameters are\nconsidered, including the level of noise and interference, the number of\ninterfering users and the size of the encoder and decoder. Numerical results\ndemonstrate that KD significantly improves the robustness and the\ngeneralization ability when applied to unexpected interference, and it reduces\nthe performance loss when compressing the model size.",
            "author": [
                "Chenguang Liu",
                "Yuxin Zhou",
                "Yunfei Chen",
                "Shuang-Hua Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13789v1",
                "http://arxiv.org/pdf/2311.13789v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13781v1",
            "title": "Dynamic Compositional Graph Convolutional Network for Efficient\n  Composite Human Motion Prediction",
            "updated": "2023-11-23T02:49:46Z",
            "published": "2023-11-23T02:49:46Z",
            "summary": "With potential applications in fields including intelligent surveillance and\nhuman-robot interaction, the human motion prediction task has become a hot\nresearch topic and also has achieved high success, especially using the recent\nGraph Convolutional Network (GCN). Current human motion prediction task usually\nfocuses on predicting human motions for atomic actions. Observing that atomic\nactions can happen at the same time and thus formulating the composite actions,\nwe propose the composite human motion prediction task. To handle this task, we\nfirst present a Composite Action Generation (CAG) module to generate synthetic\ncomposite actions for training, thus avoiding the laborious work of collecting\ncomposite action samples. Moreover, we alleviate the effect of composite\nactions on demand for a more complicated model by presenting a Dynamic\nCompositional Graph Convolutional Network (DC-GCN). Extensive experiments on\nthe Human3.6M dataset and our newly collected CHAMP dataset consistently verify\nthe efficiency of our DC-GCN method, which achieves state-of-the-art motion\nprediction accuracies and meanwhile needs few extra computational costs than\ntraditional GCN-based human motion methods.",
            "author": [
                "Wanying Zhang",
                "Shen Zhao",
                "Fanyang Meng",
                "Songtao Wu",
                "Mengyuan Liu"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3581783.3612532",
                "http://arxiv.org/abs/2311.13781v1",
                "http://arxiv.org/pdf/2311.13781v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13766v1",
            "title": "A Unified Framework for Fair Spectral Clustering With Effective Graph\n  Learning",
            "updated": "2023-11-23T01:43:00Z",
            "published": "2023-11-23T01:43:00Z",
            "summary": "We consider the problem of spectral clustering under group fairness\nconstraints, where samples from each sensitive group are approximately\nproportionally represented in each cluster. Traditional fair spectral\nclustering (FSC) methods consist of two consecutive stages, i.e., performing\nfair spectral embedding on a given graph and conducting $k$means to obtain\ndiscrete cluster labels. However, in practice, the graph is usually unknown,\nand we need to construct the underlying graph from potentially noisy data, the\nquality of which inevitably affects subsequent fair clustering performance.\nFurthermore, performing FSC through separate steps breaks the connections among\nthese steps, leading to suboptimal results. To this end, we first theoretically\nanalyze the effect of the constructed graph on FSC. Motivated by the analysis,\nwe propose a novel graph construction method with a node-adaptive graph filter\nto learn graphs from noisy data. Then, all independent stages of conventional\nFSC are integrated into a single objective function, forming an end-to-end\nframework that inputs raw data and outputs discrete cluster labels. An\nalgorithm is developed to jointly and alternately update the variables in each\nstage. Finally, we conduct extensive experiments on synthetic, benchmark, and\nreal data, which show that our model is superior to state-of-the-art fair\nclustering methods.",
            "author": [
                "Xiang Zhang",
                "Qiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13766v1",
                "http://arxiv.org/pdf/2311.13766v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13743v2",
            "title": "FinMem: A Performance-Enhanced LLM Trading Agent with Layered Memory and\n  Character Design",
            "updated": "2023-12-03T16:18:55Z",
            "published": "2023-11-23T00:24:40Z",
            "summary": "Recent advancements in Large Language Models (LLMs) have exhibited notable\nefficacy in question-answering (QA) tasks across diverse domains. Their prowess\nin integrating extensive web knowledge has fueled interest in developing\nLLM-based autonomous agents. While LLMs are efficient in decoding human\ninstructions and deriving solutions by holistically processing historical\ninputs, transitioning to purpose-driven agents requires a supplementary\nrational architecture to process multi-source information, establish reasoning\nchains, and prioritize critical tasks. Addressing this, we introduce\n\\textsc{FinMem}, a novel LLM-based agent framework devised for financial\ndecision-making. It encompasses three core modules: Profiling, to customize the\nagent's characteristics; Memory, with layered message processing, to aid the\nagent in assimilating hierarchical financial data; and Decision-making, to\nconvert insights gained from memories into investment decisions. Notably,\n\\textsc{FinMem}'s memory module aligns closely with the cognitive structure of\nhuman traders, offering robust interpretability and real-time tuning. Its\nadjustable cognitive span allows for the retention of critical information\nbeyond human perceptual limits, thereby enhancing trading outcomes. This\nframework enables the agent to self-evolve its professional knowledge, react\nagilely to new investment cues, and continuously refine trading decisions in\nthe volatile financial environment. We first compare \\textsc{FinMem} with\nvarious algorithmic agents on a scalable real-world financial dataset,\nunderscoring its leading trading performance in stocks. We then fine-tuned the\nagent's perceptual span and character setting to achieve a significantly\nenhanced trading performance. Collectively, \\textsc{FinMem} presents a\ncutting-edge LLM agent framework for automated trading, boosting cumulative\ninvestment returns.",
            "author": [
                "Yangyang Yu",
                "Haohang Li",
                "Zhi Chen",
                "Yuechen Jiang",
                "Yang Li",
                "Denghui Zhang",
                "Rong Liu",
                "Jordan W. Suchow",
                "Khaldoun Khashanah"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13743v2",
                "http://arxiv.org/pdf/2311.13743v2"
            ],
            "primary_category": "q-fin.CP",
            "category": [
                "q-fin.CP",
                "cs.AI",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13741v1",
            "title": "Genus of Embedded Graphs in Orientable Closed Surfaces",
            "updated": "2023-11-23T00:17:59Z",
            "published": "2023-11-23T00:17:59Z",
            "summary": "We give an algorithm to calculate the minimal and maximal genus of the\norientable closed surface where a graph $G$ can be embedded. For this, we\nconstruct some special branched coverings of the 2-sphere. We apply this\nalgorithm to calculate the orientable genus and maximal genus of some Snarks\ngraphs.",
            "author": [
                "Lorena Armas-Sanabria",
                "V\u00edctor N\u00fa\u00f1ez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13741v1",
                "http://arxiv.org/pdf/2311.13741v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.CO",
                "05C10, 57M15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13734v1",
            "title": "Climate change adaptation stories: co-creating climate services with\n  reindeer herders in Finland",
            "updated": "2023-11-22T23:29:35Z",
            "published": "2023-11-22T23:29:35Z",
            "summary": "Reindeer husbandry in the Arctic region is strongly affected by the local\nclimate. Reindeer herders are used to coping with adverse weather, climate, and\ngrazing conditions through autonomous adaptation. However, todays rapidly\nchanging Arctic environment poses new challenges to the management of herding\nactivities. Finding means for combining traditional and scientific knowledge\nwithout depriving any of the systems of its fundamental strengths is hence\ndeemed necessary. We apply a transdisciplinary framework for knowledge\nco-production involving international researchers and reindeer herders from\ndifferent cooperatives in northern Finland. Through climate change adaptation\nstories, we co-explore how climate predictions can inform herders decision\nmaking during the herding season. Relevant decisions include the anticipation\nof summer harvest time, the inopportune periods of cold weather in spring, and\ninsect harassment in summer. The analysis of two different adaptation stories\nshows that seasonal predictions of temperature for May and June can\nsuccessfully advise about the likelihood of having an earlier than normal\nharvest. Likewise, sub-seasonal predictions of temperature during April and May\ncan be useful to anticipate the occurrence of backwinter episodes, which can\nsupport herders in deciding whether to feed reindeer in pens for longer,\navoiding putting the survival of calves at risk. This study, which would\nbenefit from co-evaluation in real world settings and consideration of\nadditional adaptation stories, sets the basis for a successful co-production of\nclimate services with Arctic reindeer herders. This research shows the\npotential to enhance the resilience of Polar regions, offering opportunities\nfor adaptation while supporting the sustainability and culture of traditional\npractices of Arctic communities.",
            "author": [
                "Marta Terrado",
                "Nuria P\u00e9rez-Zan\u00f3n",
                "Dragana Bojovic",
                "Nube Gonz\u00e1lez-Reviriego",
                "Gerrit Versteeg",
                "Sara Octenjak",
                "Albert Mart\u00ednez-Bot\u00ed",
                "Tanja Joona"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.scitotenv.2023.168520",
                "http://arxiv.org/abs/2311.13734v1",
                "http://arxiv.org/pdf/2311.13734v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13708v1",
            "title": "Dynamic Analysis Method for Hidden Dangers in Substation Based on\n  Knowledge Graph",
            "updated": "2023-11-22T21:59:46Z",
            "published": "2023-11-22T21:59:46Z",
            "summary": "To address the challenge of identifying and understanding hidden dangers in\nsubstations from unstructured text data, a novel dynamic analysis method is\nproposed. This approach begins by analyzing and extracting data from the\nunstructured text related to hidden dangers. It then leverages a flexible,\ndistributed data search engine built on Elastic-Search to handle this\ninformation. Following this, the hidden Markov model is employed to train the\ndata within the engine. The Viterbi algorithm is integrated to decipher the\nhidden state sequences, facilitating the segmentation and labeling of entities\nrelated to hidden dangers. The final step involves using the Neo4j graph\ndatabase to dynamically create a knowledge map that visualizes hidden dangers\nin the substation. This method's effectiveness is demonstrated through an\nexample analysis using data from a specific substation's hidden dangers.",
            "author": [
                "Weiwei Li",
                "Xing Liu",
                "Wei Wang",
                "Lu Chen",
                "Sizhe Li",
                "Hui Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13708v1",
                "http://arxiv.org/pdf/2311.13708v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13706v1",
            "title": "Multi-view Hybrid Graph Convolutional Network for Volume-to-mesh\n  Reconstruction in Cardiovascular MRI",
            "updated": "2023-11-22T21:51:29Z",
            "published": "2023-11-22T21:51:29Z",
            "summary": "Cardiovascular magnetic resonance imaging is emerging as a crucial tool to\nexamine cardiac morphology and function. Essential to this endeavour are\nanatomical 3D surface and volumetric meshes derived from CMR images, which\nfacilitate computational anatomy studies, biomarker discovery, and in-silico\nsimulations. However, conventional surface mesh generation methods, such as\nactive shape models and multi-atlas segmentation, are highly time-consuming and\nrequire complex processing pipelines to generate simulation-ready 3D meshes. In\nresponse, we introduce HybridVNet, a novel architecture for direct\nimage-to-mesh extraction seamlessly integrating standard convolutional neural\nnetworks with graph convolutions, which we prove can efficiently handle surface\nand volumetric meshes by encoding them as graph structures. To further enhance\naccuracy, we propose a multiview HybridVNet architecture which processes both\nlong axis and short axis CMR, showing that it can increase the performance of\ncardiac MR mesh generation. Our model combines traditional convolutional\nnetworks with variational graph generative models, deep supervision and\nmesh-specific regularisation. Experiments on a comprehensive dataset from the\nUK Biobank confirm the potential of HybridVNet to significantly advance cardiac\nimaging and computational cardiology by efficiently generating high-fidelity\nand simulation ready meshes from CMR images.",
            "author": [
                "Nicol\u00e1s Gaggion",
                "Benjamin A. Matheson",
                "Yan Xia",
                "Rodrigo Bonazzola",
                "Nishant Ravikumar",
                "Zeike A. Taylor",
                "Diego H. Milone",
                "Alejandro F. Frangi",
                "Enzo Ferrante"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13706v1",
                "http://arxiv.org/pdf/2311.13706v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13694v2",
            "title": "Limit Distribution Theory for Quantum Divergences",
            "updated": "2023-12-07T09:45:22Z",
            "published": "2023-11-22T21:06:41Z",
            "summary": "Estimation of quantum relative entropy and its R\\'{e}nyi generalizations is a\nfundamental statistical task in quantum information theory, physics, and\nbeyond. While several estimators of these divergences have been proposed in the\nliterature along with their computational complexities explored, a limit\ndistribution theory which characterizes the asymptotic fluctuations of the\nestimation error is still premature. As our main contribution, we characterize\nthese asymptotic distributions in terms of Fr\\'{e}chet derivatives of\nelementary operator-valued functions. We achieve this by leveraging an operator\nversion of Taylor's theorem and identifying the regularity conditions needed.\nAs an application of our results, we consider an estimator of quantum relative\nentropy based on Pauli tomography of quantum states and show that the resulting\nasymptotic distribution is a centered normal, with its variance characterized\nin terms of the Pauli operators and states. We utilize the knowledge of the\naforementioned limit distribution to obtain asymptotic performance guarantees\nfor a multi-hypothesis testing problem.",
            "author": [
                "Sreejith Sreekumar",
                "Mario Berta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13694v2",
                "http://arxiv.org/pdf/2311.13694v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.IT",
                "math.IT",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13680v1",
            "title": "Qudit Stabilizer Codes, CFTs, and Topological Surfaces",
            "updated": "2023-11-22T20:29:40Z",
            "published": "2023-11-22T20:29:40Z",
            "summary": "We study general maps from the space of rational CFTs with a fixed chiral\nalgebra and associated Chern-Simons (CS) theories to the space of qudit\nstabilizer codes with a fixed generalized Pauli group. We consider certain\nnatural constraints on such a map and show that the map can be described as a\ngraph homomorphism from an orbifold graph, which captures the orbifold\nstructure of CFTs, to a code graph, which captures the structure of self-dual\nstabilizer codes. By studying explicit examples, we show that this graph\nhomomorphism cannot always be a graph embedding. However, we construct a\nphysically motivated map from universal orbifold subgraphs of CFTs to operators\nin a generalized Pauli group. We show that this map results in a self-dual\nstabilizer code if and only if the surface operators in the bulk CS theories\ncorresponding to the CFTs in question are self-dual. For CFTs admitting a\nstabilizer code description, we show that the full abelianized generalized\nPauli group can be obtained from twisted sectors of certain 0-form symmetries\nof the CFT. Finally, we connect our construction with SymTFTs, and we argue\nthat many equivalences between codes that arise in our setup correspond to\nequivalence classes of bulk topological surfaces under fusion with invertible\nsurfaces.",
            "author": [
                "Matthew Buican",
                "Rajath Radhakrishnan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13680v1",
                "http://arxiv.org/pdf/2311.13680v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.str-el",
                "math-ph",
                "math.MP",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13675v1",
            "title": "Noisy kinetic-exchange opinion model with aging",
            "updated": "2023-11-22T20:08:10Z",
            "published": "2023-11-22T20:08:10Z",
            "summary": "We study the critical behavior of a noisy kinetic opinion model subject to\nresilience to change depending on aging, defined as the time spent on the\ncurrent opinion state. In this model, the opinion of each agent can take the\nthree discrete values, the extreme ones $\\pm 1$, and also the intermediate\nvalue 0, and it can evolve through kinetic exchange when interacting with\nanother agent, or independently, by stochastic choice (noise). The probability\nof change by pairwise interactions depends on the age that the agent has\nremained in the same state, according to a given kernel. We particularly\ndevelop the cases where the probability decays either algebraically or\nexponentially with the age, and we also consider the anti-aging scenario where\nthe probability increases with the age, meaning that it is more likely to\nchange mind the longer the persistence in the current state. For the opinion\ndynamics in a complete graph, we obtain analytical predictions for the critical\ncurves of the order parameters, in perfect agreement with agent-based\nsimulations. We observe that sufficiently weak aging (slow change of the kernel\nwith the age) favors partial consensus with respect to the aging-insensitive\nscenario, while for sufficiently strong aging, as well as for anti-aging, the\nopposite trend is observed.",
            "author": [
                "Allan R. Vieira",
                "Jaume Llabr\u00e9s",
                "Ra\u00fal Toral",
                "Celia Anteneodo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13675v1",
                "http://arxiv.org/pdf/2311.13675v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13673v1",
            "title": "On the Size Overhead of Pairwise Spanners",
            "updated": "2023-11-22T20:01:06Z",
            "published": "2023-11-22T20:01:06Z",
            "summary": "Given an undirected possibly weighted $n$-vertex graph $G=(V,E)$ and a set\n$\\mathcal{P}\\subseteq V^2$ of pairs, a subgraph $S=(V,E')$ is called a ${\\cal\nP}$-pairwise $\\alpha$-spanner of $G$, if for every pair $(u,v)\\in\\mathcal{P}$\nwe have $d_S(u,v)\\leq\\alpha\\cdot d_G(u,v)$. The parameter $\\alpha$ is called\nthe stretch of the spanner, and its size overhead is define as\n$\\frac{|E'|}{|{\\cal P}|}$.\n  A surprising connection was recently discussed between the additive stretch\nof $(1+\\epsilon,\\beta)$-spanners, to the hopbound of\n$(1+\\epsilon,\\beta)$-hopsets. A long sequence of works showed that if the\nspanner/hopset has size $\\approx n^{1+1/k}$ for some parameter $k\\ge 1$, then\n$\\beta\\approx\\left(\\frac1\\epsilon\\right)^{\\log k}$. In this paper we establish\na new connection to the size overhead of pairwise spanners. In particular, we\nshow that if $|{\\cal P}|\\approx n^{1+1/k}$, then a ${\\cal P}$-pairwise\n$(1+\\epsilon)$-spanner must have size at least $\\beta\\cdot |{\\cal P}|$ with\n$\\beta\\approx\\left(\\frac1\\epsilon\\right)^{\\log k}$ (a near matching upper bound\nwas recently shown in \\cite{ES23}).\n  We also extend the connection between pairwise spanners and hopsets to the\nlarge stretch regime, by showing nearly matching upper and lower bounds for\n${\\cal P}$-pairwise $\\alpha$-spanners. In particular, we show that if $|{\\cal\nP}|\\approx n^{1+1/k}$, then the size overhead is $\\beta\\approx\\frac k\\alpha$.\n  A source-wise spanner is a special type of pairwise spanner, for which ${\\cal\nP}=A\\times V$ for some $A\\subseteq V$. A prioritized spanner is given also a\nranking of the vertices $V=(v_1,\\dots,v_n)$, and is required to provide\nimproved stretch for pairs containing higher ranked vertices. By using a\nsequence of reductions, we improve on the state-of-the-art results for\nsource-wise and prioritized spanners.",
            "author": [
                "Ofer Neiman",
                "Idan Shabat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13673v1",
                "http://arxiv.org/pdf/2311.13673v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13662v1",
            "title": "Zarankiewicz's problem via $\u03b5$-t-nets",
            "updated": "2023-11-22T19:34:36Z",
            "published": "2023-11-22T19:34:36Z",
            "summary": "The classical Zarankiewicz's problem asks for the maximum number of edges in\na bipartite graph on $n$ vertices which does not contain the complete bipartite\ngraph $K_{t,t}$. In one of the cornerstones of extremal graph theory,\nK\\H{o}v\\'ari S\\'os and Tur\\'an proved an upper bound of $O(n^{2-\\frac{1}{t}})$.\nIn a celebrated result, Fox et al. obtained an improved bound of\n$O(n^{2-\\frac{1}{d}})$ for graphs of VC-dimension $d$ (where $d<t$). Basit,\nChernikov, Starchenko, Tao and Tran improved the bound for the case of\nsemilinear graphs. At SODA'23, Chan and Har-Peled further improved Basit et\nal.'s bounds and presented (quasi-)linear upper bounds for several classes of\ngeometrically-defined incidence graphs, including a bound of $O(n \\log \\log n)$\nfor the incidence graph of points and pseudo-discs in the plane.\n  In this paper we present a new approach to Zarankiewicz's problem, via\n$\\epsilon$-t-nets - a recently introduced generalization of the classical\nnotion of $\\epsilon$-nets. We show that the existence of `small'-sized\n$\\epsilon$-t-nets implies upper bounds for Zarankiewicz's problem. Using the\nnew approach, we obtain a sharp bound of $O(n)$ for the intersection graph of\ntwo families of pseudo-discs, thus both improving and generalizing the result\nof Chan and Har-Peled from incidence graphs to intersection graphs. We also\nobtain a short proof of the $O(n^{2-\\frac{1}{d}})$ bound of Fox et al., and\nshow improved bounds for several other classes of geometric intersection\ngraphs, including a sharp $O(n\\frac{\\log n}{\\log \\log n})$ bound for the\nintersection graph of two families of axis-parallel rectangles.",
            "author": [
                "Chaya Keller",
                "Shakhar Smorodinsky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13662v1",
                "http://arxiv.org/pdf/2311.13662v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.CG",
                "05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13657v1",
            "title": "Efficient Transformer Knowledge Distillation: A Performance Review",
            "updated": "2023-11-22T19:19:37Z",
            "published": "2023-11-22T19:19:37Z",
            "summary": "As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.",
            "author": [
                "Nathan Brown",
                "Ashton Williamson",
                "Tahj Anderson",
                "Logan Lawrence"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13657v1",
                "http://arxiv.org/pdf/2311.13657v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13648v1",
            "title": "Evaluating Pretrained models for Deployable Lifelong Learning",
            "updated": "2023-11-22T19:04:05Z",
            "published": "2023-11-22T19:04:05Z",
            "summary": "We create a novel benchmark for evaluating a Deployable Lifelong Learning\nsystem for Visual Reinforcement Learning (RL) that is pretrained on a curated\ndataset, and propose a novel Scalable Lifelong Learning system capable of\nretaining knowledge from the previously learnt RL tasks. Our benchmark measures\nthe efficacy of a deployable Lifelong Learning system that is evaluated on\nscalability, performance and resource utilization. Our proposed system, once\npretrained on the dataset, can be deployed to perform continual learning on\nunseen tasks. Our proposed method consists of a Few Shot Class Incremental\nLearning (FSCIL) based task-mapper and an encoder/backbone trained entirely\nusing the pretrain dataset. The policy parameters corresponding to the\nrecognized task are then loaded to perform the task. We show that this system\ncan be scaled to incorporate a large number of tasks due to the small memory\nfootprint and fewer computational resources. We perform experiments on our DeLL\n(Deployment for Lifelong Learning) benchmark on the Atari games to determine\nthe efficacy of the system.",
            "author": [
                "Kiran Lekkala",
                "Eshan Bhargava",
                "Laurent Itti"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13648v1",
                "http://arxiv.org/pdf/2311.13648v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13630v1",
            "title": "Functional Matching and Renormalization Group Equations at Two-Loop\n  Order",
            "updated": "2023-11-22T19:00:00Z",
            "published": "2023-11-22T19:00:00Z",
            "summary": "We present a systematic method for determining the two-loop effective\nLagrangian resulting from integrating out a set of heavy particles in an\nultraviolet scalar theory. We prove that the matching coefficients are entirely\ndetermined from the (double-)hard region of the loop integrals and present a\nmaster formula for matching, applicable to both diagrammatic and functional\napproaches. We further employ functional methods to determine compact\nexpressions for the effective Lagrangian that do not rely on any previous\nknowledge of its structure or symmetries. The same methods are also applicable\nto the computation of renormalization group equations. We demonstrate the\napplication of the functional approach by computing the two-loop matching\ncoefficients and renormalization group equations in a scalar toy model.",
            "author": [
                "Javier Fuentes-Mart\u00edn",
                "Ajdin Palavri\u0107",
                "Anders Eller Thomsen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13630v1",
                "http://arxiv.org/pdf/2311.13630v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13590v1",
            "title": "Triangle-free $2$-matchings",
            "updated": "2023-11-22T18:52:51Z",
            "published": "2023-11-22T18:52:51Z",
            "summary": "We consider the problem of finding a maximum size triangle-free $2$-matching\nin a graph $G$. A $2$-matching is any subset of the edges such that each vertex\nis incident to at most two edges from the subset. We present a fast\ncombinatorial algorithm for the problem. Our algorithm and its analysis are\ndramatically simpler than the very complicated result by Hartvigsen from 1984.\n  In the design of this algorithm we use several new concepts. It has been\nproven before that for any triangle-free $2$-matching $M$ which is not maximum\nthe graph contains an $M$-augmenting path, whose application to $M$ results in\na bigger triangle-free $2$-matching. It was not known how to efficiently find\nsuch a path. A new observation is that the search for an augmenting path $P$\ncan be restricted to so-called {\\em amenable} paths that go through any\ntriangle $t$ contained in $P \\cup M$ a limited number of times. To find an\naugmenting path that is amenable and hence whose application does not create\nany triangle we forbid some edges to be followed by certain others. This\noperation can be thought of as using gadgets, in which some pairs of edges get\ndisconnected. To be able to disconnect two edges we employ {\\em half-edges}. A\n{\\em half-edge} of edge $e$ is, informally speaking, a half of $e$ containing\nexactly one of its endpoints. This is another novel application of half-edges\nthat were already been used for TSP and other matching problems. Additionally,\ngadgets are not fixed during any augmentation phase, but are dynamically\nchanging according to the currently discovered state of reachability by\namenable paths.",
            "author": [
                "Katarzyna Paluch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13590v1",
                "http://arxiv.org/pdf/2311.13590v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13582v1",
            "title": "Some Upper Bounds on Ramsey Numbers Involving $C_4$",
            "updated": "2023-11-22T18:37:42Z",
            "published": "2023-11-22T18:37:42Z",
            "summary": "We obtain some new upper bounds on the Ramsey numbers of the form\n$R(\\underbrace{C_4,\\ldots,C_4}_m,G_1,\\ldots,G_n)$, where $m\\ge 1$ and\n$G_1,\\ldots,G_n$ are arbitrary graphs. We focus on the cases of $G_i$'s being\ncomplete, star $K_{1,k}$ or book graphs $B_k$, where $B_k=K_2+kK_1$. If $k\\ge\n2$, then our main upper bound theorem implies that $$R(C_4,B_k) \\le\nR(C_4,K_{1,k})+\\left\\lceil\\sqrt{R(C_4,K_{1,k})}\\right\\rceil+1.$$\n  Our techniques are used to obtain new upper bounds in several concrete cases,\nincluding: $R(C_4,K_{11})\\leq 43$, $R(C_4,K_{12})\\leq 51$, $R(C_4,K_3,K_4)\\leq\n29$, $R(C_4, K_4,K_4)\\leq 66$, $R(C_4,K_3,K_3,K_3)\\leq 57$,\n$R(C_4,C_4,K_3,K_4)\\leq 75$, and $R(C_4,C_4,K_4,K_4)\\leq 177$, and also\n$R(C_4,B_{17})\\leq 28$.",
            "author": [
                "Luis Boza",
                "Stanis\u0142aw Radziszowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13582v1",
                "http://arxiv.org/pdf/2311.13582v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C55"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13573v1",
            "title": "$h$-vectors of edge rings of odd-cycle compositions",
            "updated": "2023-11-22T18:27:48Z",
            "published": "2023-11-22T18:27:48Z",
            "summary": "Let $\\mathbb{K}[G]$ be the edge ring of a finite simple graph $G$.\nInvestigating properties of the $h$-vector of $\\mathbb{K}[G]$ is of great\ninterest in combinatorial commutative algebra. However, there are few families\nof graphs for which the $h$-vector has been explicitly determined. In this\npaper, we compute the $h$-vectors of a certain family of graphs that satisfy\nthe odd-cycle condition, generalizing a result of the second and third named\nauthors. As a corollary, we obtain a characterization of the graphs in this\nfamily whose edge rings are almost Gorenstein.",
            "author": [
                "Kieran Bhaskara",
                "Akihiro Higashitani",
                "Nayana Shibu Deepthi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13573v1",
                "http://arxiv.org/pdf/2311.13573v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO",
                "13D40 (Primary) 13P10, 13F55, 13F65, 05E40 Secondary"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13546v1",
            "title": "Enigma: Privacy-Preserving Execution of QAOA on Untrusted Quantum\n  Computers",
            "updated": "2023-11-22T17:40:23Z",
            "published": "2023-11-22T17:40:23Z",
            "summary": "Quantum computers can solve problems that are beyond the capabilities of\nconventional computers. As quantum computers are expensive and hard to\nmaintain, the typical model for performing quantum computation is to send the\ncircuit to a quantum cloud provider. This leads to privacy concerns for\ncommercial entities as an untrusted server can learn protected information from\nthe provided circuit. Current proposals for Secure Quantum Computing (SQC)\neither rely on emerging technologies (such as quantum networks) or incur\nprohibitive overheads (for Quantum Homomorphic Encryption). The goal of our\npaper is to enable low-cost privacy-preserving quantum computation that can be\nused with current systems.\n  We propose Enigma, a suite of privacy-preserving schemes specifically\ndesigned for the Quantum Approximate Optimization Algorithm (QAOA). Unlike\nprevious SQC techniques that obfuscate quantum circuits, Enigma transforms the\ninput problem of QAOA, such that the resulting circuit and the outcomes are\nunintelligible to the server. We introduce three variants of Enigma. Enigma-I\nprotects the coefficients of QAOA using random phase flipping and fudging of\nvalues. Enigma-II protects the nodes of the graph by introducing decoy qubits,\nwhich are indistinguishable from primary ones. Enigma-III protects the edge\ninformation of the graph by modifying the graph such that each node has an\nidentical number of connections. For all variants of Enigma, we demonstrate\nthat we can still obtain the solution for the original problem. We evaluate\nEnigma using IBM quantum devices and show that the privacy improvements of\nEnigma come at only a small reduction in fidelity (1%-13%).",
            "author": [
                "Ramin Ayanzadeh",
                "Ahmad Mousavi",
                "Narges Alavisamani",
                "Moinuddin Qureshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13546v1",
                "http://arxiv.org/pdf/2311.13546v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "cs.CR",
                "cs.DM",
                "cs.ET"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13533v1",
            "title": "Volumetric 3D Point Cloud Attribute Compression: Learned polynomial\n  bilateral filter for prediction",
            "updated": "2023-11-22T17:13:24Z",
            "published": "2023-11-22T17:13:24Z",
            "summary": "We extend a previous study on 3D point cloud attribute compression scheme\nthat uses a volumetric approach: given a target volumetric attribute function\n$f : \\mathbb{R}^3 \\mapsto \\mathbb{R}$, we quantize and encode parameters\n$\\theta$ that characterize $f$ at the encoder, for reconstruction\n$f_{\\hat{\\theta}}(\\mathbf(x))$ at known 3D points $\\mathbf(x)$ at the decoder.\nSpecifically, parameters $\\theta$ are quantized coefficients of B-spline basis\nvectors $\\mathbf{\\Phi}_l$ (for order $p \\geq 2$) that span the function space\n$\\mathcal{F}_l^{(p)}$ at a particular resolution $l$, which are coded from\ncoarse to fine resolutions for scalability. In this work, we focus on the\nprediction of finer-grained coefficients given coarser-grained ones by learning\nparameters of a polynomial bilateral filter (PBF) from data. PBF is a\npseudo-linear filter that is signal-dependent with a graph spectral\ninterpretation common in the graph signal processing (GSP) field. We\ndemonstrate PBF's predictive performance over a linear predictor inspired by\nMPEG standardization over a wide range of point cloud datasets.",
            "author": [
                "Tam Thuc Do",
                "Philip A. Chou",
                "Gene Cheung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13533v1",
                "http://arxiv.org/pdf/2311.13533v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13527v1",
            "title": "$\u03b2$ Pictoris b through the eyes of the upgraded CRIRES+",
            "updated": "2023-11-22T17:02:31Z",
            "published": "2023-11-22T17:02:31Z",
            "summary": "Context: High-resolution spectrographs fed by adaptive optics (AO) provide a\nunique opportunity to characterize directly imaged exoplanets. Observations\nwith such instruments allow us to probe the atmospheric composition, spin\nrotation, and radial velocity of the planet, thereby helping to reveal\ninformation on its formation and migration history. The recent upgrade of the\nCryogenic High-Resolution Infrared Echelle Spectrograph (CRIRES+) at the VLT\nmakes it a highly suitable instrument for characterizing directly imaged\nexoplanets.\n  Aims: In this work, we report on observations of $\\beta$ Pictoris b with\nCRIRES+ and use them to constrain the planets atmospheric properties and update\nthe estimation of its spin rotation.\n  Methods: The data were reduced using the open-source \\textit{pycrires}\npackage. We subsequently forward-modeled the stellar, planetary, and systematic\ncontribution to the data to detect molecules in the planet's atmosphere. We\nalso used atmospheric retrievals to provide new constraints on its atmosphere.\n  Results: We confidently detected water and carbon monoxide in the atmosphere\nof $\\beta$ Pictoris b and retrieved a slightly sub-solar carbon-to-oxygen\nratio, which is in agreement with previous results. The interpretation is\nhampered by our limited knowledge of the C/O ratio of the host star. We also\nobtained a much improved constraint on its spin rotation of $19.9 \\pm 1.0$\nkm/s, which gives a rotation period of $8.7 \\pm 0.8$ hours, assuming no\nobliquity. We find that there is a degeneracy between the metallicity and\nclouds, but this has minimal impact on the retrieved C/O, $v\\sin{i}$, and\nradial velocity. Our results show that CRIRES+ is performing well and stands as\na highly useful instrument for characterizing directly imaged planets.",
            "author": [
                "Rico Landman",
                "Tomas Stolker",
                "Ignas Snellen",
                "Jean Costes",
                "Sam de Regt",
                "Yapeng Zhang",
                "Siddharth Gandhi",
                "Paul Molli\u00e8re",
                "Aurora Kesseli",
                "Arthur Vigan",
                "Alejandro S\u00e1nchez-L\u00f3pez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13527v1",
                "http://arxiv.org/pdf/2311.13527v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13523v1",
            "title": "Outerplanar and Forest Storyplans",
            "updated": "2023-11-22T16:53:24Z",
            "published": "2023-11-22T16:53:24Z",
            "summary": "We study the problem of gradually representing a complex graph as a sequence\nof drawings of small subgraphs whose union is the complex graph. The sequence\nof drawings is called \\emph{storyplan}, and each drawing in the sequence is\ncalled a \\emph{frame}. In an outerplanar storyplan, every frame is outerplanar;\nin a forest storyplan, every frame is acyclic. We identify graph families that\nadmit such storyplans and families for which such storyplans do not always\nexist. In the affirmative case, we present efficient algorithms that produce\nstraight-line storyplans.",
            "author": [
                "Ji\u0159\u00ed Fiala",
                "Oksana Firman",
                "Giuseppe Liotta",
                "Alexander Wolff",
                "Johannes Zink"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13523v1",
                "http://arxiv.org/pdf/2311.13523v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13490v1",
            "title": "Benchmarking Toxic Molecule Classification using Graph Neural Networks\n  and Few Shot Learning",
            "updated": "2023-11-22T16:07:32Z",
            "published": "2023-11-22T16:07:32Z",
            "summary": "Traditional methods like Graph Convolutional Networks (GCNs) face challenges\nwith limited data and class imbalance, leading to suboptimal performance in\ngraph classification tasks during toxicity prediction of molecules as a whole.\nTo address these issues, we harness the power of Graph Isomorphic Networks,\nMulti Headed Attention and Free Large-scale Adversarial Augmentation separately\non Graphs for precisely capturing the structural data of molecules and their\ntoxicological properties. Additionally, we incorporate Few-Shot Learning to\nimprove the model's generalization with limited annotated samples. Extensive\nexperiments on a diverse toxicology dataset demonstrate that our method\nachieves an impressive state-of-art AUC-ROC value of 0.816, surpassing the\nbaseline GCN model by 11.4%. This highlights the significance of our proposed\nmethodology and Few Shot Learning in advancing Toxic Molecular Classification,\nwith the potential to enhance drug discovery and environmental risk assessment\nprocesses.",
            "author": [
                "Bhavya Mehta",
                "Kush Kothari",
                "Reshmika Nambiar",
                "Seema Shrawne"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13490v1",
                "http://arxiv.org/pdf/2311.13490v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13478v1",
            "title": "Solution discovery via reconfiguration for problems in P",
            "updated": "2023-11-22T15:48:19Z",
            "published": "2023-11-22T15:48:19Z",
            "summary": "In the recently introduced framework of solution discovery via\nreconfiguration [Fellows et al., ECAI 2023], we are given an initial\nconfiguration of $k$ tokens on a graph and the question is whether we can\ntransform this configuration into a feasible solution (for some problem) via a\nbounded number $b$ of small modification steps. In this work, we study solution\ndiscovery variants of polynomial-time solvable problems, namely Spanning Tree\nDiscovery, Shortest Path Discovery, Matching Discovery, and Vertex/Edge Cut\nDiscovery in the unrestricted token addition/removal model, the token jumping\nmodel, and the token sliding model. In the unrestricted token addition/removal\nmodel, we show that all four discovery variants remain in P. For the toking\njumping model we also prove containment in P, except for Vertex/Edge Cut\nDiscovery, for which we prove NP-completeness. Finally, in the token sliding\nmodel, almost all considered problems become NP-complete, the exception being\nSpanning Tree Discovery, which remains polynomial-time solvable. We then study\nthe parameterized complexity of the NP-complete problems and provide a full\nclassification of tractability with respect to the parameters solution size\n(number of tokens) $k$ and transformation budget (number of steps) $b$. Along\nthe way, we observe strong connections between the solution discovery variants\nof our base problems and their (weighted) rainbow variants as well as their\nred-blue variants with cardinality constraints.",
            "author": [
                "Mario Grobler",
                "Stephanie Maaz",
                "Nicole Megow",
                "Amer E. Mouawad",
                "Vijayaragunathan Ramamoorthi",
                "Daniel Schmand",
                "Sebastian Siebertz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13478v1",
                "http://arxiv.org/pdf/2311.13478v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13472v1",
            "title": "Complexity-Guided Curriculum Learning for Text Graphs",
            "updated": "2023-11-22T15:40:57Z",
            "published": "2023-11-22T15:40:57Z",
            "summary": "Curriculum learning provides a systematic approach to training. It refines\ntraining progressively, tailors training to task requirements, and improves\ngeneralization through exposure to diverse examples. We present a curriculum\nlearning approach that builds on existing knowledge about text and graph\ncomplexity formalisms for training with text graph data. The core part of our\napproach is a novel data scheduler, which employs \"spaced repetition\" and\ncomplexity formalisms to guide the training process. We demonstrate the\neffectiveness of the proposed approach on several text graph tasks and graph\nneural network architectures. The proposed model gains more and uses less data;\nconsistently prefers text over graph complexity indices throughout training,\nwhile the best curricula derived from text and graph complexity indices are\nequally effective; and it learns transferable curricula across GNN models and\ndatasets. In addition, we find that both node-level (local) and graph-level\n(global) graph complexity indices, as well as shallow and traditional text\ncomplexity indices play a crucial role in effective curriculum learning.",
            "author": [
                "Nidhi Vakil",
                "Hadi Amiri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13472v1",
                "http://arxiv.org/pdf/2311.13472v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13471v1",
            "title": "Comparative Analysis of Linear Regression, Gaussian Elimination, and LU\n  Decomposition for CT Real Estate Purchase Decisions",
            "updated": "2023-11-22T15:35:56Z",
            "published": "2023-11-22T15:35:56Z",
            "summary": "This paper presents a comprehensive evaluation of three distinct\ncomputational algorithms applied to the decision-making process of real estate\npurchases. Specifically, we analyze the efficacy of Linear Regression from\nScikit-learn library, Gaussian Elimination with partial pivoting, and LU\nDecomposition in predicting the advisability of buying a house in the State of\nConnecticut based on a set of financial and market-related parameters. The\nalgorithms' performances were compared using a dataset encompassing\ntown-specific details, yearly data, interest rates, and median sale ratios. Our\nresults demonstrate significant differences in predictive accuracy, with Linear\nRegression and LU Decomposition providing the most reliable recommendations and\nGaussian Elimination showing limitations in stability and performance. The\nstudy's findings emphasize the importance of algorithm selection in predictive\nanalytic and offer insights into the practical applications of computational\nmethods in real estate investment strategies. By evaluating model efficacy\nthrough metrics such as R-squared scores and Mean Squared Error, we provide a\nnuanced understanding of each method's strengths and weaknesses, contributing\nvaluable knowledge to the fields of real estate analysis and predictive\nmodeling.",
            "author": [
                "Xilin Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13471v1",
                "http://arxiv.org/pdf/2311.13471v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "cs.CE",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13470v1",
            "title": "Analysis of a multi-species Cahn-Hilliard-Keller-Segel tumor growth\n  model with chemotaxis and angiogenesis",
            "updated": "2023-11-22T15:35:19Z",
            "published": "2023-11-22T15:35:19Z",
            "summary": "We introduce a multi-species diffuse interface model for tumor growth,\ncharacterized by its incorporation of essential features related to chemotaxis,\nangiogenesis and proliferation mechanisms. We establish the weak well-posedness\nof the system within an appropriate variational framework, accommodating\nvarious choices for the nonlinear potentials. One of the primary novelties of\nthe work lies in the rigorous establishment of the existence of a weak solution\nthrough the introduction of delicate approximation schemes. To our knowledge,\nthis represents a novel advancement for both the intricate\nCahn-Hilliard-Keller-Segel system and the Keller-Segel subsystem with source\nterms. Moreover, when specific conditions are met, such as having more regular\ninitial data, a smallness condition on the chemotactic constant with respect to\nthe magnitude of initial conditions and potentially focusing solely on the\ntwo-dimensional case, we provide regularity results for the weak solutions.\nFinally, we derive a continuous dependence estimate, which, in turn, leads to\nthe uniqueness of the smoothed solution as a natural consequence.",
            "author": [
                "Abramo Agosti",
                "Andrea Signori"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13470v1",
                "http://arxiv.org/pdf/2311.13470v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13466v1",
            "title": "Accelerating Inference in Molecular Diffusion Models with Latent\n  Representations of Protein Structure",
            "updated": "2023-11-22T15:32:31Z",
            "published": "2023-11-22T15:32:31Z",
            "summary": "Diffusion generative models have emerged as a powerful framework for\naddressing problems in structural biology and structure-based drug design.\nThese models operate directly on 3D molecular structures. Due to the\nunfavorable scaling of graph neural networks (GNNs) with graph size as well as\nthe relatively slow inference speeds inherent to diffusion models, many\nexisting molecular diffusion models rely on coarse-grained representations of\nprotein structure to make training and inference feasible. However, such\ncoarse-grained representations discard essential information for modeling\nmolecular interactions and impair the quality of generated structures. In this\nwork, we present a novel GNN-based architecture for learning latent\nrepresentations of molecular structure. When trained end-to-end with a\ndiffusion model for de novo ligand design, our model achieves comparable\nperformance to one with an all-atom protein representation while exhibiting a\n3-fold reduction in inference time.",
            "author": [
                "Ian Dunn",
                "David Ryan Koes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13466v1",
                "http://arxiv.org/pdf/2311.13466v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13465v1",
            "title": "Continuous-time vertex-reinforced random walks on complete-like graphs",
            "updated": "2023-11-22T15:32:19Z",
            "published": "2023-11-22T15:32:19Z",
            "summary": "We introduce the continuous-time vertex-reinforced random walk (cVRRW) as a\ncontinuous-time version of the vertex-reinforced random walk (VRRW), which\nmight open a new perspective on the study of the VRRW.\n  It has been proved by Limic and Volkov that for the VRRW on a complete-like\ngraph $K_d \\cup \\partial K_d$, the asymptotic frequency of visits is uniform\nover the non-leaf vertices. We give short proofs of those results by\nestablishing a stochastic approximation result for the cVRRW on complete-like\ngraphs. We also prove that almost surely, the number of visits to each leaf up\nto time n divided by $n^{\\frac{1}{d-1}}$ converges to a non-zero limit. We\nsolve a conjecture by Limic and Volkov on the rate of convergence in the case\nof the complete graph.",
            "author": [
                "Shuo Qin",
                "Pierre Tarres"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13465v1",
                "http://arxiv.org/pdf/2311.13465v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13439v1",
            "title": "Pro-$\\mathcal{C}$ RAAGs",
            "updated": "2023-11-22T14:58:36Z",
            "published": "2023-11-22T14:58:36Z",
            "summary": "Let $\\mathcal{C}$ be a class of finite groups closed under taking subgroups,\nquotients, and extensions with abelian kernel. The right-angled Artin\npro-$\\mathcal{C}$ group $G_\\Gamma$ (pro-$\\mathcal{C}$ RAAG for short) is the\npro-$\\mathcal{C}$ completion of the right-angled Artin group $G(\\Gamma)$\nassociated with the finite simplicial graph $\\Gamma$.\n  In the first part, we describe structural properties of pro-$\\mathcal{C}$\nRAAGs. Among others, we describe the centraliser of an element and show that\npro-$\\mathcal{C}$ RAAGs satisfy the Tits' alternative, that standard subgroups\nare isolated, and that 2-generated pro-$p$ subgroups of pro-$\\mathcal{C}$ RAAGs\nare either free pro-$p$ or free abelian pro-$p$.\n  In the second part, we characterise splittings of pro-$\\mathcal{C}$ RAAGs in\nterms of the defining graph. More precisely, we prove that a pro-$\\mathcal{C}$\nRAAG $G_\\Gamma$ splits as a non-trivial direct product if and only if $\\Gamma$\nis a join and it splits over an abelian pro-$\\mathcal{C}$ group if and only if\na connected component of $\\Gamma$ is a complete graph or it has a complete\ndisconnecting subgraph. We then use this characterisation to describe an\nabelian JSJ decomposition of a pro-$\\mathcal{C}$ RAAG, in the sense of\nGuirardel and Levitt.",
            "author": [
                "Montserrat Casals-Ruiz",
                "Matteo Pintonello",
                "Pavel Zalesskii"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13439v1",
                "http://arxiv.org/pdf/2311.13439v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13414v1",
            "title": "From Images to Connections: Can DQN with GNNs learn the Strategic Game\n  of Hex?",
            "updated": "2023-11-22T14:20:15Z",
            "published": "2023-11-22T14:20:15Z",
            "summary": "The gameplay of strategic board games such as chess, Go and Hex is often\ncharacterized by combinatorial, relational structures -- capturing distinct\ninteractions and non-local patterns -- and not just images. Nonetheless, most\ncommon self-play reinforcement learning (RL) approaches simply approximate\npolicy and value functions using convolutional neural networks (CNN). A key\nfeature of CNNs is their relational inductive bias towards locality and\ntranslational invariance. In contrast, graph neural networks (GNN) can encode\nmore complicated and distinct relational structures. Hence, we investigate the\ncrucial question: Can GNNs, with their ability to encode complex connections,\nreplace CNNs in self-play reinforcement learning? To this end, we do a\ncomparison with Hex -- an abstract yet strategically rich board game -- serving\nas our experimental platform. Our findings reveal that GNNs excel at dealing\nwith long range dependency situations in game states and are less prone to\noverfitting, but also showing a reduced proficiency in discerning local\npatterns. This suggests a potential paradigm shift, signaling the use of\ngame-specific structures to reshape self-play reinforcement learning.",
            "author": [
                "Yannik Keller",
                "Jannis Bl\u00fcml",
                "Gopika Sudhakaran",
                "Kristian Kersting"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13414v1",
                "http://arxiv.org/pdf/2311.13414v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13404v2",
            "title": "Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions",
            "updated": "2023-11-27T02:33:36Z",
            "published": "2023-11-22T14:00:23Z",
            "summary": "We present a novel animatable 3D Gaussian model for rendering high-fidelity\nfree-view human motions in real time. Compared to existing NeRF-based methods,\nthe model owns better capability in synthesizing high-frequency details without\nthe jittering problem across video frames. The core of our model is a novel\naugmented 3D Gaussian representation, which attaches each Gaussian with a\nlearnable code. The learnable code serves as a pose-dependent appearance\nembedding for refining the erroneous appearance caused by geometric\ntransformation of Gaussians, based on which an appearance refinement model is\nlearned to produce residual Gaussian properties to match the appearance in\ntarget pose. To force the Gaussians to learn the foreground human only without\nbackground interference, we further design a novel alpha loss to explicitly\nconstrain the Gaussians within the human body. We also propose to jointly\noptimize the human joint parameters to improve the appearance accuracy. The\nanimatable 3D Gaussian model can be learned with shallow MLPs, so new human\nmotions can be synthesized in real time (66 fps on avarage). Experiments show\nthat our model has superior performance over NeRF-based methods.",
            "author": [
                "Keyang Ye",
                "Tianjia Shao",
                "Kun Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13404v2",
                "http://arxiv.org/pdf/2311.13404v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13373v3",
            "title": "Large Language Model is a Good Policy Teacher for Training Reinforcement\n  Learning Agents",
            "updated": "2023-11-29T08:39:37Z",
            "published": "2023-11-22T13:15:42Z",
            "summary": "Recent studies have shown that Large Language Models (LLMs) can be utilized\nfor solving complex sequential decision-making tasks by providing high-level\ninstructions. However, LLM-based agents face limitations in real-time dynamic\nenvironments due to their lack of specialization in solving specific target\nproblems. Moreover, the deployment of such LLM-based agents is both costly and\ntime-consuming in practical scenarios. In this paper, we introduce a novel\nframework that addresses these challenges by training a smaller scale\nspecialized student agent using instructions from an LLM-based teacher agent.\nBy leveraging guided actions provided by the teachers, the prior knowledge of\nthe LLM is distilled into the local student model. Consequently, the student\nagent can be trained with significantly less data. Furthermore, subsequent\ntraining with environment feedback empowers the student agents to surpass the\ncapabilities of their teachers. We conducted experiments on three challenging\nMiniGrid environments to evaluate the effectiveness of our framework. The\nresults demonstrate that our approach enhances sample efficiency and achieves\nsuperior performance compared to baseline methods. Our code is available at\nhttps://github.com/ZJLAB-AMMI/LLM4Teach.",
            "author": [
                "Zihao Zhou",
                "Bin Hu",
                "Pu Zhang",
                "Chenyang Zhao",
                "Bin Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13373v3",
                "http://arxiv.org/pdf/2311.13373v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13368v1",
            "title": "An Erd\u0151s-Stone-Simonovits type result for high-order spectra of\n  graphs",
            "updated": "2023-11-22T13:07:17Z",
            "published": "2023-11-22T13:07:17Z",
            "summary": "Erd\\H{o}s-Stone-Simonovits theorem (abbreviated as E-S-S theorem) is a\nwell-known result in extremal graph theory which states that the asymptotic\nmaximum number of edges in an $n$-vertex $H$-free graph. In 2009, Nikiforov\ngave a spectral version of E-S-S theorem. In this paper, we study the\nhigher-order spectral version of E-S-S theorem, the asymptotic maximum spectral\nradius of the clique tensor of $H$-free graphs with $n$ vertices is obtained.\nFurthermore, our conclusion implies a result of Alon and Shikhelman [J. Combin.\nTheory Ser. B 121 (2016)].",
            "author": [
                "Chunmeng Liu",
                "Changjiang Bu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13368v1",
                "http://arxiv.org/pdf/2311.13368v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13364v2",
            "title": "On the maximum $A_\u03b1$-spectral radius of unicyclic and bicyclic\n  graphs with fixed girth or fixed number of pendant vertices",
            "updated": "2023-11-30T00:01:50Z",
            "published": "2023-11-22T13:03:58Z",
            "summary": "For a connected graph $G$, let $A(G)$ be the adjacency matrix of $G$ and\n$D(G)$ be the diagonal matrix of the degrees of the vertices in $G$. The\n$A_{\\alpha}$-matrix of $G$ is defined as \\begin{align*} A_\\alpha (G) = \\alpha\nD(G) + (1-\\alpha) A(G) \\quad \\text{for any $\\alpha \\in [0,1]$}. \\end{align*}\n  The largest eigenvalue of $A_{\\alpha}(G)$ is called the $A_{\\alpha}$-spectral\nradius of $G$. In this article, we characterize the graphs with maximum\n$A_{\\alpha}$-spectral radius among the class of unicyclic and bicyclic graphs\nof order $n$ with fixed girth $g$. Also, we identify the unique graphs with\nmaximum $A_{\\alpha}$-spectral radius among the class of unicyclic and bicyclic\ngraphs of order $n$ with $k$ pendant vertices.",
            "author": [
                "Joyentanuj Das",
                "Iswar Mahato"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13364v2",
                "http://arxiv.org/pdf/2311.13364v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13354v1",
            "title": "\"Energy transfers in surface wave-averaged equations\"",
            "updated": "2023-11-22T12:45:50Z",
            "published": "2023-11-22T12:45:50Z",
            "summary": "Ocean surface gravity waves play an important role for the air-sea momentum\nfluxes and the upper ocean mixing, and knowledge of the sea state leads in\ngeneral circulation models to improved estimates of the ocean energy budget and\nallows to incorporate surface wave impacts, such as Langmuir turbulence.\nHowever, including the Stokes drift, in phase-averaged equations for the\nEulerian mean motion leads to an Eulerian energy budget which is physically\ndifficult to interpret. In this note, we show that a Lagrangian energy budget\nallows for a closed energy budget, in which all terms connecting the different\nenergy compartments correspond to well known energy transfer terms. We show\nthat the so-called Coriolis-Stokes force does not lead to an energy transfer\nbetween surface gravity waves and oceanic mean motions as previously suggested.\nIn an energy budget for the Lagrangian mean kinetic energy, the work done by\nthe Coriolis-Stokes force does not contribute, and should be used to estimate\nthe kinetic energy balance in the wave affected surface mixed layer. The\nLagrangian energy budget is used to discuss an energetically consistent\nframework, which can be used to couple a general circulation ocean model to a\nsurface wave model.",
            "author": [
                "Lars Czeschel",
                "Carsten Eden"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13354v1",
                "http://arxiv.org/pdf/2311.13354v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13349v1",
            "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
            "updated": "2023-11-22T12:34:51Z",
            "published": "2023-11-22T12:34:51Z",
            "summary": "Deep models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models, not capable to\nadapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks\n(REDS) to tackle model adaptation to variable resources. In contrast to the\nstate-of-the-art, REDS use structured sparsity constructively by exploiting\npermutation invariance of neurons, which allows for hardware-specific\noptimizations. Specifically, REDS achieve computational efficiency by (1)\nskipping sequential computational blocks identified by a novel iterative\nknapsack optimizer, and (2) leveraging simple math to re-arrange the order of\noperations in REDS computational graph to take advantage of the data cache.\nREDS support conventional deep networks frequently deployed on the edge and\nprovide computational benefits even for small and simple networks. We evaluate\nREDS on six benchmark architectures trained on the Google Speech Commands,\nFMNIST and CIFAR10 datasets, and test on four off-the-shelf mobile and embedded\nhardware platforms. We provide a theoretical result and empirical evidence for\nREDS outstanding performance in terms of submodels' test set accuracy, and\ndemonstrate an adaptation time in response to dynamic resource constraints of\nunder 40$\\mu$s, utilizing a 2-layer fully-connected network on Arduino Nano 33\nBLE Sense.",
            "author": [
                "Francesco Corti",
                "Balz Maag",
                "Joachim Schauer",
                "Ulrich Pferschy",
                "Olga Saukh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13349v1",
                "http://arxiv.org/pdf/2311.13349v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13348v1",
            "title": "MergeSFL: Split Federated Learning with Feature Merging and Batch Size\n  Regulation",
            "updated": "2023-11-22T12:25:02Z",
            "published": "2023-11-22T12:25:02Z",
            "summary": "Recently, federated learning (FL) has emerged as a popular technique for edge\nAI to mine valuable knowledge in edge computing (EC) systems. To mitigate the\ncomputing/communication burden on resource-constrained workers and protect\nmodel privacy, split federated learning (SFL) has been released by integrating\nboth data and model parallelism. Despite resource limitations, SFL still faces\ntwo other critical challenges in EC, i.e., statistical heterogeneity and system\nheterogeneity. To address these challenges, we propose a novel SFL framework,\ntermed MergeSFL, by incorporating feature merging and batch size regulation in\nSFL. Concretely, feature merging aims to merge the features from workers into a\nmixed feature sequence, which is approximately equivalent to the features\nderived from IID data and is employed to promote model accuracy. While batch\nsize regulation aims to assign diverse and suitable batch sizes for\nheterogeneous workers to improve training efficiency. Moreover, MergeSFL\nexplores to jointly optimize these two strategies upon their coupled\nrelationship to better enhance the performance of SFL. Extensive experiments\nare conducted on a physical platform with 80 NVIDIA Jetson edge devices, and\nthe experimental results show that MergeSFL can improve the final model\naccuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared\nto the baselines.",
            "author": [
                "Yunming Liao",
                "Yang Xu",
                "Hongli Xu",
                "Lun Wang",
                "Zhiwei Yao",
                "Chunming Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13348v1",
                "http://arxiv.org/pdf/2311.13348v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13342v1",
            "title": "Directed Evolution of Microorganisms for Engineered Living Materials",
            "updated": "2023-11-22T12:12:04Z",
            "published": "2023-11-22T12:12:04Z",
            "summary": "Microorganisms can create engineered materials with exquisite structures and\nliving functionalities. Although synthetic biology tools to genetically\nmanipulate microorganisms continue to expand, the bottom-up rational design of\nengineered living materials still relies on prior knowledge of\ngenotype-phenotype links for the function of interest. Here, we utilize a\nhigh-throughput directed evolution platform to enhance the fitness of whole\nmicroorganisms under selection pressure and identify novel genetic pathways to\nprogram the functionalities of engineered living materials. Using\nKomagataeibacter sucrofermentans as a model cellulose-producing microorganism,\nwe show that our droplet-based microfluidic platform enables the directed\nevolution of these bacteria towards a small number of cellulose overproducers\nfrom an initial pool of 40'000 random mutants. Sequencing of the evolved\nstrains reveals an unexpected link between the cellulose-forming ability of the\nbacteria and a gene encoding a protease complex responsible for protein\nturnover in the cell. The ability to enhance the fitness of microorganisms\ntowards specific phenotypes and to discover new genotype-phenotype links makes\nthis high-throughput directed evolution platform a promising tool for the\ndevelopment of the next generation of engineered living materials.",
            "author": [
                "Julie M. Laurent",
                "Ankit Jain",
                "Anton Kan",
                "Mathias Steinacher",
                "Nadia Enrriquez Casimiro",
                "Stavros Stavrakis",
                "Andrew J. deMello",
                "Andr\u00e9 R. Studart"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13342v1",
                "http://arxiv.org/pdf/2311.13342v1"
            ],
            "primary_category": "physics.bio-ph",
            "category": [
                "physics.bio-ph",
                "cond-mat.mtrl-sci",
                "cond-mat.soft",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13341v1",
            "title": "Learning principle and mathematical realization of the learning\n  mechanism in the brain",
            "updated": "2023-11-22T12:08:01Z",
            "published": "2023-11-22T12:08:01Z",
            "summary": "While deep learning has achieved remarkable success, there is no clear\nexplanation about why it works so well. In order to discuss this question\nquantitatively, we need a mathematical framework that explains what learning is\nin the first place. After several considerations, we succeeded in constructing\na mathematical framework that can provide a unified understanding of all types\nof learning, including deep learning and learning in the brain. We call it\nlearning principle, and it follows that all learning is equivalent to\nestimating the probability of input data. We not only derived this principle,\nbut also mentioned its application to actual machine learning models. For\nexample, we found that conventional supervised learning is equivalent to\nestimating conditional probabilities, and succeeded in making supervised\nlearning more effective and generalized. We also proposed a new method of\ndefining the values of estimated probability using differentiation, and showed\nthat unsupervised learning can be performed on arbitrary dataset without any\nprior knowledge. Namely, this method is a general-purpose machine learning in\nthe true sense. Moreover, we succeeded in describing the learning mechanism in\nthe brain by considering the time evolution of a fully or partially connected\nmodel and applying this new method. The learning principle provides solutions\nto many unsolved problems in deep learning and cognitive neuroscience.",
            "author": [
                "Taisuke Katayose"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13341v1",
                "http://arxiv.org/pdf/2311.13341v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "math.IT",
                "q-bio.NC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13335v1",
            "title": "Quantum learning and essential cognition under the traction of\n  meta-characteristics in an open world",
            "updated": "2023-11-22T11:55:41Z",
            "published": "2023-11-22T11:55:41Z",
            "summary": "Artificial intelligence has made significant progress in the Close World\nproblem, being able to accurately recognize old knowledge through training and\nclassification. However, AI faces significant challenges in the Open World\nproblem, as it involves a new and unknown exploration journey. AI is not\ninherently proactive in exploration, and its challenge lies in not knowing how\nto approach and adapt to the unknown world. How do humans acquire knowledge of\nthe unknown world. Humans identify new knowledge through intrinsic cognition.\nIn the process of recognizing new colors, the cognitive cues are different from\nknown color features and involve hue, saturation, brightness, and other\ncharacteristics. When AI encounters objects with different features in the new\nworld, it faces another challenge: where are the distinguishing features\nbetween influential features of new and old objects? AI often mistakes a new\nworld's brown bear for a known dog because it has not learned the differences\nin feature distributions between knowledge systems. This is because things in\nthe new and old worlds have different units and dimensions for their features.\nThis paper proposes an open-world model and elemental feature system that\nfocuses on fundamentally recognizing the distribution differences in objective\nfeatures between the new and old worlds. The quantum tunneling effect of\nlearning ability in the new and old worlds is realized through the tractive\nforce of meta-characteristic. The outstanding performance of the model system\nin learning new knowledge (using pedestrian re-identification datasets as an\nexample) demonstrates that AI has acquired the ability to recognize the new\nworld with an accuracy of $96.71\\%$ at most and has gained the capability to\nexplore new knowledge, similar to humans.",
            "author": [
                "Jin Wang",
                "Changlin Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13335v1",
                "http://arxiv.org/pdf/2311.13335v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13333v2",
            "title": "Trace-enabled Timing Model Synthesis for ROS2-based Autonomous\n  Applications",
            "updated": "2023-11-23T10:09:31Z",
            "published": "2023-11-22T11:54:42Z",
            "summary": "Autonomous applications are typically developed over Robot Operating System\n2.0 (ROS2) even in time-critical systems like automotive. Recent years have\nseen increased interest in developing model-based timing analysis and schedule\noptimization approaches for ROS2-based applications. To complement these\napproaches, we propose a tracing and measurement framework to obtain timing\nmodels of ROS2-based applications. It offers a tracer based on extended\nBerkeley Packet Filter (eBPF) that probes different functions in ROS2\nmiddleware and reads their arguments or return values to reason about the data\nflow in applications. It combines event traces from ROS2 and the operating\nsystem to generate a directed acyclic graph showing ROS2 callbacks, precedence\nrelations between them, and their timing attributes. While being compatible\nwith existing analyses, we also show how to model (i)~message synchronization,\ne.g., in sensor fusion, and (ii)~service requests from multiple clients, e.g.,\nin motion planning. Considering that, in real-world scenarios, the application\ncode might be confidential and formal models are unavailable, our framework\nstill enables the application of existing analysis and optimization techniques.",
            "author": [
                "Hazem Abaza",
                "Debayan Roy",
                "Shiqing Fan",
                "Selma Saidi",
                "Antonios Motakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13333v2",
                "http://arxiv.org/pdf/2311.13333v2"
            ],
            "primary_category": "cs.OS",
            "category": [
                "cs.OS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13328v1",
            "title": "MagGen: A graph aided deep generative model for inverse design of\n  stable, permanent magnets",
            "updated": "2023-11-22T11:50:17Z",
            "published": "2023-11-22T11:50:17Z",
            "summary": "A significant development towards inverse design of materials with\nwell-defined target properties is reported. A deep generative model based on\nvariational autoencoder (VAE), conditioned simultaneously by two target\nproperties, is developed to inverse design stable magnetic materials. Structure\nof the physics informed, property embedded latent space of the model is\nanalyzed using graph theory, based on the idea of similarity index. The graph\nidea is shown to be useful for generating new materials that are likely to\nsatisfy target properties. An impressive ~96% of the generated materials is\nfound to satisfy the target properties as per predictions from the target\nlearning branches. This is a huge improvement over approaches that do not\ncondition the VAE latent space by target properties, or do not consider\nconnectivity of the parent materials perturbing which the new materials are\ngenerated. In such models, the fraction of materials satisfying targets can be\nas low as ~5%. This impressive feat is achieved using a simple real-space only\nrepresentation called Invertible Real-space Crystallographic Representation\n(IRCR), that can be directly read from material cif files. Model predictions\nare finally validated by performing DFT calculations on a randomly chosen\nsubset of materials. Performance of the present model using IRCR is comparable\nor superior to that of the models reported earlier. This model for magnetic\nmaterial generation, MagGen, is applied to the problem of designing rare earth\nfree permanent magnets with promising results.",
            "author": [
                "Sourav Mal",
                "Gaurav Seal",
                "Prasenjit Sen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13328v1",
                "http://arxiv.org/pdf/2311.13328v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13323v2",
            "title": "Spectral condition for the existence of a chorded cycle",
            "updated": "2023-12-05T02:28:24Z",
            "published": "2023-11-22T11:35:15Z",
            "summary": "A chord of a cycle $C$ is an edge joining two non-consecutive vertices of\n$C$. A cycle $C$ in a graph $G$ is chorded if the vertex set of $C$ induces at\nleast one chord. In this paper, we prove that if $G$ is a graph with order\n$n\\geq 6$ and $\\rho(G)\\geq \\rho(K_{2,n-2})$, then $G$ contains a chorded cycle\nunless $G\\cong K_{2,n-2}$. This gives one answer to a question posed by Gould\n[Results and problems on chorded cycles: A survey, Graphs Combin. 38 (2022)\n189].",
            "author": [
                "Jiaxin Zheng",
                "Xueyi Huang",
                "Junjie Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13323v2",
                "http://arxiv.org/pdf/2311.13323v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13314v1",
            "title": "Mitigating Large Language Model Hallucinations via Autonomous Knowledge\n  Graph-based Retrofitting",
            "updated": "2023-11-22T11:08:38Z",
            "published": "2023-11-22T11:08:38Z",
            "summary": "Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.",
            "author": [
                "Xinyan Guan",
                "Yanjiang Liu",
                "Hongyu Lin",
                "Yaojie Lu",
                "Ben He",
                "Xianpei Han",
                "Le Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13314v1",
                "http://arxiv.org/pdf/2311.13314v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13297v1",
            "title": "Retargeting Visual Data with Deformation Fields",
            "updated": "2023-11-22T10:27:19Z",
            "published": "2023-11-22T10:27:19Z",
            "summary": "Seam carving is an image editing method that enable content-aware resizing,\nincluding operations like removing objects. However, the seam-finding strategy\nbased on dynamic programming or graph-cut limits its applications to broader\nvisual data formats and degrees of freedom for editing. Our observation is that\ndescribing the editing and retargeting of images more generally by a\ndisplacement field yields a generalisation of content-aware deformations. We\npropose to learn a deformation with a neural network that keeps the output\nplausible while trying to deform it only in places with low information\ncontent. This technique applies to different kinds of visual data, including\nimages, 3D scenes given as neural radiance fields, or even polygon meshes.\nExperiments conducted on different visual data show that our method achieves\nbetter content-aware retargeting compared to previous methods.",
            "author": [
                "Tim Elsner",
                "Julia Berger",
                "Tong Wu",
                "Victor Czech",
                "Lin Gao",
                "Leif Kobbelt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13297v1",
                "http://arxiv.org/pdf/2311.13297v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13287v2",
            "title": "Colloidal diffusiophoresis in crossed electrolyte gradients:\n  experimental demonstration of an 'action at a distance' effect predicted by\n  the Nernst-Planck equations",
            "updated": "2023-11-23T11:56:46Z",
            "published": "2023-11-22T10:12:54Z",
            "summary": "In an externally imposed electrolyte (salt) concentration gradient, charged\ncolloids drift at speeds of order one micrometre per second. This phenomenon is\nknown as diffusiophoresis. In systems with multiple salts and 'crossed' salt\ngradients, a nonlocal component of the electric field associated with a\ncirculating (solenoidal) ion current can arise. This is in addition to the\nconventional local component that depends only on the local salt gradients.\nHere we report experimental observations verifying the existence of this\nnonlocal contribution. To our knowledge this is the first observation of\nnonlocal diffusiophoresis. The current develops quasi-instantaneously on the\ntime scale of salt diffusion. Therefore, in systems with multiple salts and\ncrossed salt gradients, one can expect a nonlocal contribution to\ndiffusiophoresis which is dependent on the geometry of the system as a whole\nand appears as a kind of instantaneous 'action-at-a-distance' effect. The\ninterpretation is aided by a magnetostatic analogy. Our experiments are\nfacilitated by a judicious particle-dependent choice of salt (potassium\nacetate) for which the two local contributions to diffusiophoresis almost\ncancel, effectively eliminating conventional diffusiophoresis. This enables us\nto clearly identify the novel, nonlocal effect and may be useful in other\ncontexts, for example in sorting particle mixtures.",
            "author": [
                "Ian Williams",
                "Patrick B. Warren",
                "Richard P. Sear",
                "Joseph L. Keddie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13287v2",
                "http://arxiv.org/pdf/2311.13287v2"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13283v1",
            "title": "A new approach to b-coloring of regular graphs",
            "updated": "2023-11-22T10:06:32Z",
            "published": "2023-11-22T10:06:32Z",
            "summary": "Let $G$ be a graph and c a proper k-coloring of G, i.e. any two adjacent\nvertices u and v have different colors c(u) and c(v). A proper k-coloring is a\nb-coloring if there exists a vertex in every color class that contains all the\ncolors in its closed neighborhood. The maximum number of colors k admitting\nb-coloring of G is the b-chromatic number. We present two separate approaches\nto the conjecture posed by Blidia et. al that the b-chromatic number equals to\nd+1 for every d-regular graph of girth at least five except the Petersen graph.",
            "author": [
                "Magda Dettlaff",
                "Hanna Furma\u0144czyk",
                "Iztok Peterin",
                "Riana Roux",
                "Rados\u0142aw Ziemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13283v1",
                "http://arxiv.org/pdf/2311.13283v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13279v1",
            "title": "Comprehensive Evaluation of GNN Training Systems: A Data Management\n  Perspective",
            "updated": "2023-11-22T09:55:20Z",
            "published": "2023-11-22T09:55:20Z",
            "summary": "Many Graph Neural Network (GNN) training systems have emerged recently to\nsupport efficient GNN training. Since GNNs embody complex data dependencies\nbetween training samples, the training of GNNs should address distinct\nchallenges different from DNN training in data management, such as data\npartitioning, batch preparation for mini-batch training, and data transferring\nbetween CPUs and GPUs. These factors, which take up a large proportion of\ntraining time, make data management in GNN training more significant. This\npaper reviews GNN training from a data management perspective and provides a\ncomprehensive analysis and evaluation of the representative approaches. We\nconduct extensive experiments on various benchmark datasets and show many\ninteresting and valuable results. We also provide some practical tips learned\nfrom these experiments, which are helpful for designing GNN training systems in\nthe future.",
            "author": [
                "Hao Yuan",
                "Yajiong Liu",
                "Yanfeng Zhang",
                "Xin Ai",
                "Qiange Wang",
                "Chaoyi Chen",
                "Yu Gu",
                "Ge Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13279v1",
                "http://arxiv.org/pdf/2311.13279v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13258v1",
            "title": "ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided\n  Code-Vision Representation",
            "updated": "2023-11-22T09:23:34Z",
            "published": "2023-11-22T09:23:34Z",
            "summary": "State-of-the-art vision-language models (VLMs) still have limited performance\nin structural knowledge extraction, such as relations between objects. In this\nwork, we present ViStruct, a training framework to learn VLMs for effective\nvisual structural knowledge extraction. Two novel designs are incorporated.\nFirst, we propose to leverage the inherent structure of programming language to\ndepict visual structural information. This approach enables explicit and\nconsistent representation of visual structural information of multiple\ngranularities, such as concepts, relations, and events, in a well-organized\nstructured format. Second, we introduce curriculum-based learning for VLMs to\nprogressively comprehend visual structures, from fundamental visual concepts to\nintricate event structures. Our intuition is that lower-level knowledge may\ncontribute to complex visual structure understanding. Furthermore, we compile\nand release a collection of datasets tailored for visual structural knowledge\nextraction. We adopt a weakly-supervised approach to directly generate visual\nevent structures from captions for ViStruct training, capitalizing on abundant\nimage-caption pairs from the web. In experiments, we evaluate ViStruct on\nvisual structure prediction tasks, demonstrating its effectiveness in improving\nthe understanding of visual structures. The code is public at\n\\url{https://github.com/Yangyi-Chen/vi-struct}.",
            "author": [
                "Yangyi Chen",
                "Xingyao Wang",
                "Manling Li",
                "Derek Hoiem",
                "Heng Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13258v1",
                "http://arxiv.org/pdf/2311.13258v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14741v1",
            "title": "@ve: A Chatbot for Latin",
            "updated": "2023-11-22T09:06:11Z",
            "published": "2023-11-22T09:06:11Z",
            "summary": "Dead, extinct, and endangered languages have been preserved primarily through\naudio conservation and the collection and digitization of scripts and have been\npromoted through targeted language acquisition efforts. Another possibility\nwould be to build conversational agents that can master these languages. This\nwould provide an artificial, active conversational partner which has knowledge\nof the vocabulary and grammar, and one learns with it in a different way. The\nchatbot @ve, with which one can communicate in Latin, was developed in\n2022/2023 based on GPT-3.0. It was additionally equipped with a manually\ncreated knowledge base. After conceptual groundwork, this paper presents the\npreparation and implementation of the project. In addition, it summarizes the\ntest that a Latin expert conducted with the chatbot. A critical discussion\nelaborates advantages and disadvantages. @ve could be a new tool for teaching\nLatin in a memorable and entertaining way through dialogue. However, the\npresent implementation is still too prone to glitches for stand-alone use -\ni.e., without the accompaniment of a teacher. The use of GPT-4 could be a\nsolution as well as the extension of the knowledge base. In conclusion, it can\nbe argued that conversational agents are an innovative approach to promoting\nand preserving languages.",
            "author": [
                "Oliver Bendel",
                "Karim N'diaye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14741v1",
                "http://arxiv.org/pdf/2311.14741v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.RO",
                "I.2; K.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13244v1",
            "title": "Hard Label Black Box Node Injection Attack on Graph Neural Networks",
            "updated": "2023-11-22T09:02:04Z",
            "published": "2023-11-22T09:02:04Z",
            "summary": "While graph neural networks have achieved state-of-the-art performances in\nmany real-world tasks including graph classification and node classification,\nrecent works have demonstrated they are also extremely vulnerable to\nadversarial attacks. Most previous works have focused on attacking node\nclassification networks under impractical white-box scenarios. In this work, we\nwill propose a non-targeted Hard Label Black Box Node Injection Attack on Graph\nNeural Networks, which to the best of our knowledge, is the first of its kind.\nUnder this setting, more real world tasks can be studied because our attack\nassumes no prior knowledge about (1): the model architecture of the GNN we are\nattacking; (2): the model's gradients; (3): the output logits of the target GNN\nmodel. Our attack is based on an existing edge perturbation attack, from which\nwe restrict the optimization process to formulate a node injection attack. In\nthe work, we will evaluate the performance of the attack using three datasets,\nCOIL-DEL, IMDB-BINARY, and NCI1.",
            "author": [
                "Yu Zhou",
                "Zihao Dong",
                "Guofeng Zhang",
                "Jingchen Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13244v1",
                "http://arxiv.org/pdf/2311.13244v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14740v1",
            "title": "AutoKG: Efficient Automated Knowledge Graph Generation for Language\n  Models",
            "updated": "2023-11-22T08:58:25Z",
            "published": "2023-11-22T08:58:25Z",
            "summary": "Traditional methods of linking large language models (LLMs) to knowledge\nbases via the semantic similarity search often fall short of capturing complex\nrelational dynamics. To address these limitations, we introduce AutoKG, a\nlightweight and efficient approach for automated knowledge graph (KG)\nconstruction. For a given knowledge base consisting of text blocks, AutoKG\nfirst extracts keywords using a LLM and then evaluates the relationship weight\nbetween each pair of keywords using graph Laplace learning. We employ a hybrid\nsearch scheme combining vector similarity and graph-based associations to\nenrich LLM responses. Preliminary experiments demonstrate that AutoKG offers a\nmore comprehensive and interconnected knowledge retrieval mechanism compared to\nthe semantic similarity search, thereby enhancing the capabilities of LLMs in\ngenerating more insightful and relevant outputs.",
            "author": [
                "Bohan Chen",
                "Andrea L. Bertozzi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14740v1",
                "http://arxiv.org/pdf/2311.14740v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13237v1",
            "title": "Effect of Constraint Relaxation on the Minimum Vertex Cover Problem in\n  Random Graphs",
            "updated": "2023-11-22T08:53:11Z",
            "published": "2023-11-22T08:53:11Z",
            "summary": "A statistical-mechanical study of the effect of constraint relaxation on the\nminimum vertex cover problem in Erd\\H{o}s-R\\'enyi random graphs is presented.\nUsing a penalty-method formulation for constraint relaxation, typical\nproperties of solutions, including infeasible solutions that violate the\nconstraints, are analyzed by means of the replica method and cavity method. The\nproblem involves a competition between reducing the number of vertices to be\ncovered and satisfying the edge constraints. The analysis under the\nreplica-symmetric (RS) ansatz clarifies that the competition leads to\ndegeneracies in the vertex and edge states, which determine the quantitative\nproperties of the system, such as the cover and penalty ratios. A precise\nanalysis of these effects improves the accuracy of RS approximation for the\nminimum cover ratio in the replica symmetry breaking (RSB) region. Furthermore,\nthe analysis based on the RS cavity method indicates that the RS/RSB boundary\nof the ground states with respect to the mean degree of the graphs is expanded,\nand the critical temperature is lowered by constraint relaxation.",
            "author": [
                "Aki Dote",
                "Koji Hukushima"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13237v1",
                "http://arxiv.org/pdf/2311.13237v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "math.CO",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13234v1",
            "title": "TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry\n  Guided Transformer",
            "updated": "2023-11-22T08:45:01Z",
            "published": "2023-11-22T08:45:01Z",
            "summary": "Optical Intraoral Scanners (IOS) are widely used in digital dentistry to\nprovide detailed 3D information of dental crowns and the gingiva. Accurate 3D\ntooth segmentation in IOSs is critical for various dental applications, while\nprevious methods are error-prone at complicated boundaries and exhibit\nunsatisfactory results across patients. In this paper, we propose TSegFormer\nwhich captures both local and global dependencies among different teeth and the\ngingiva in the IOS point clouds with a multi-task 3D transformer architecture.\nMoreover, we design a geometry-guided loss based on a novel point curvature to\nrefine boundaries in an end-to-end manner, avoiding time-consuming\npost-processing to reach clinically applicable segmentation. In addition, we\ncreate a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of\nour knowledge. The experimental results demonstrate that our TSegFormer\nconsistently surpasses existing state-of-the-art baselines. The superiority of\nTSegFormer is corroborated by extensive analysis, visualizations and real-world\nclinical applicability tests. Our code is available at\nhttps://github.com/huiminxiong/TSegFormer.",
            "author": [
                "Huimin Xiong",
                "Kunle Li",
                "Kaiyuan Tan",
                "Yang Feng",
                "Joey Tianyi Zhou",
                "Jin Hao",
                "Haochao Ying",
                "Jian Wu",
                "Zuozhu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13234v1",
                "http://arxiv.org/pdf/2311.13234v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13230v1",
            "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
            "updated": "2023-11-22T08:39:17Z",
            "published": "2023-11-22T08:39:17Z",
            "summary": "Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.",
            "author": [
                "Tianhang Zhang",
                "Lin Qiu",
                "Qipeng Guo",
                "Cheng Deng",
                "Yue Zhang",
                "Zheng Zhang",
                "Chenghu Zhou",
                "Xinbing Wang",
                "Luoyi Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13230v1",
                "http://arxiv.org/pdf/2311.13230v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13621v1",
            "title": "Knowledge From the Dark Side: Entropy-Reweighted Knowledge Distillation\n  for Balanced Knowledge Transfer",
            "updated": "2023-11-22T08:34:33Z",
            "published": "2023-11-22T08:34:33Z",
            "summary": "Knowledge Distillation (KD) transfers knowledge from a larger \"teacher\" model\nto a compact \"student\" model, guiding the student with the \"dark knowledge\"\n$\\unicode{x2014}$ the implicit insights present in the teacher's soft\npredictions. Although existing KDs have shown the potential of transferring\nknowledge, the gap between the two parties still exists. With a series of\ninvestigations, we argue the gap is the result of the student's overconfidence\nin prediction, signaling an imbalanced focus on pronounced features while\noverlooking the subtle yet crucial dark knowledge. To overcome this, we\nintroduce the Entropy-Reweighted Knowledge Distillation (ER-KD), a novel\napproach that leverages the entropy in the teacher's predictions to reweight\nthe KD loss on a sample-wise basis. ER-KD precisely refocuses the student on\nchallenging instances rich in the teacher's nuanced insights while reducing the\nemphasis on simpler cases, enabling a more balanced knowledge transfer.\nConsequently, ER-KD not only demonstrates compatibility with various\nstate-of-the-art KD methods but also further enhances their performance at\nnegligible cost. This approach offers a streamlined and effective strategy to\nrefine the knowledge transfer process in KD, setting a new paradigm in the\nmeticulous handling of dark knowledge. Our code is available at\nhttps://github.com/cpsu00/ER-KD.",
            "author": [
                "Chi-Ping Su",
                "Ching-Hsun Tseng",
                "Shin-Jye Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13621v1",
                "http://arxiv.org/pdf/2311.13621v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13225v1",
            "title": "NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU\n  Heterogeneous Environments",
            "updated": "2023-11-22T08:26:42Z",
            "published": "2023-11-22T08:26:42Z",
            "summary": "Graph Neural Networks (GNNs) have demonstrated outstanding performance in\nvarious applications. Existing frameworks utilize CPU-GPU heterogeneous\nenvironments to train GNN models and integrate mini-batch and sampling\ntechniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous\nenvironments, we can divide sample-based GNN training into three steps: sample,\ngather, and train. Existing GNN systems use different task orchestrating\nmethods to employ each step on CPU or GPU. After extensive experiments and\nanalysis, we find that existing task orchestrating methods fail to fully\nutilize the heterogeneous resources, limited by inefficient CPU processing or\nGPU resource contention. In this paper, we propose NeutronOrch, a system for\nsample-based GNN training that incorporates a layer-based task orchestrating\nmethod and ensures balanced utilization of the CPU and GPU. NeutronOrch\ndecouples the training process by layer and pushes down the training task of\nthe bottom layer to the CPU. This significantly reduces the computational load\nand memory footprint of GPU training. To avoid inefficient CPU processing,\nNeutronOrch only offloads the training of frequently accessed vertices to the\nCPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,\nNeutronOrch provides a fine-grained pipeline design for the layer-based task\norchestrating method, fully overlapping different tasks on heterogeneous\nresources while strictly guaranteeing bounded staleness. The experimental\nresults show that compared with the state-of-the-art GNN systems, NeutronOrch\ncan achieve up to 4.61x performance speedup.",
            "author": [
                "Xin Ai",
                "Qiange Wang",
                "Chunyu Cao",
                "Yanfeng Zhang",
                "Chaoyi Chen",
                "Hao Yuan",
                "Yu Gu",
                "Ge Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13225v1",
                "http://arxiv.org/pdf/2311.13225v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.03716v1",
            "title": "Co-guiding for Multi-intent Spoken Language Understanding",
            "updated": "2023-11-22T08:06:22Z",
            "published": "2023-11-22T08:06:22Z",
            "summary": "Recent graph-based models for multi-intent SLU have obtained promising\nresults through modeling the guidance from the prediction of intents to the\ndecoding of slot filling. However, existing methods (1) only model the\nunidirectional guidance from intent to slot, while there are bidirectional\ninter-correlations between intent and slot; (2) adopt homogeneous graphs to\nmodel the interactions between the slot semantics nodes and intent label nodes,\nwhich limit the performance. In this paper, we propose a novel model termed\nCo-guiding Net, which implements a two-stage framework achieving the mutual\nguidances between the two tasks. In the first stage, the initial estimated\nlabels of both tasks are produced, and then they are leveraged in the second\nstage to model the mutual guidances. Specifically, we propose two heterogeneous\ngraph attention networks working on the proposed two heterogeneous semantics\nlabel graphs, which effectively represent the relations among the semantics\nnodes and label nodes. Besides, we further propose Co-guiding-SCL Net, which\nexploits the single-task and dual-task semantics contrastive relations. For the\nfirst stage, we propose single-task supervised contrastive learning, and for\nthe second stage, we propose co-guiding supervised contrastive learning, which\nconsiders the two tasks' mutual guidances in the contrastive learning\nprocedure. Experiment results on multi-intent SLU show that our model\noutperforms existing models by a large margin, obtaining a relative improvement\nof 21.3% over the previous best model on MixATIS dataset in overall accuracy.\nWe also evaluate our model on the zero-shot cross-lingual scenario and the\nresults show that our model can relatively improve the state-of-the-art model\nby 33.5% on average in terms of overall accuracy for the total 9 languages.",
            "author": [
                "Bowen Xing",
                "Ivor W. Tsang"
            ],
            "link": [
                "http://arxiv.org/abs/2312.03716v1",
                "http://arxiv.org/pdf/2312.03716v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13216v1",
            "title": "Asymptotically compatible energy and dissipation law of the nonuniform\n  L2-$1_\u03c3$ scheme for time fractional Allen-Cahn model",
            "updated": "2023-11-22T08:03:24Z",
            "published": "2023-11-22T08:03:24Z",
            "summary": "We build an asymptotically compatible energy of the variable-step\nL2-$1_{\\sigma}$ scheme for the time-fractional Allen-Cahn model with the\nCaputo's fractional derivative of order $\\alpha\\in(0,1)$, under a weak\nstep-ratio constraint $\\tau_k/\\tau_{k-1}\\geq r_{\\star}(\\alpha)$ for $k\\ge2$,\nwhere $\\tau_k$ is the $k$-th time-step size and\n$r_{\\star}(\\alpha)\\in(0.3865,0.4037)$ for $\\alpha\\in(0,1)$. It provides a\npositive answer to the open problem in [J. Comput. Phys., 414:109473], and, to\nthe best of our knowledge, it is the first second-order nonuniform\ntime-stepping scheme to preserve both the maximum bound principle and the\nenergy dissipation law of time-fractional Allen-Cahn model. The compatible\ndiscrete energy is constructed via a novel discrete gradient structure of the\nsecond-order L2-$1_{\\sigma}$ formula by a local-nonlocal splitting technique.\nIt splits the discrete fractional derivative into two parts: one is a local\nterm analogue to the trapezoid rule of the first derivative and the other is a\nnonlocal summation analogue to the L1 formula of Caputo derivative. Numerical\nexamples with an adaptive time-stepping strategy are provided to show the\neffectiveness of our scheme and the asymptotic properties of the associated\nmodified energy.",
            "author": [
                "Hong-lin Liao",
                "Xiaohan Zhu",
                "Hong Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13216v1",
                "http://arxiv.org/pdf/2311.13216v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA",
                "65M12, 65M06, 35Q99, 74A50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13213v1",
            "title": "Artificial Intelligence in the Service of Entrepreneurial Finance:\n  Knowledge Structure and the Foundational Algorithmic Paradigm",
            "updated": "2023-11-22T07:58:46Z",
            "published": "2023-11-22T07:58:46Z",
            "summary": "While the application of Artificial Intelligence in Finance has a long\ntradition, its potential in Entrepreneurship has been intensively explored only\nrecently. In this context, Entrepreneurial Finance is a particularly fertile\nground for future Artificial Intelligence proliferation. To support the latter,\nthe study provides a bibliometric review of Artificial Intelligence\napplications in (1) entrepreneurial finance literature, and (2) corporate\nfinance literature with implications for Entrepreneurship. Rigorous search and\nscreening procedures of the scientific database Web of Science Core Collection\nresulted in the identification of 1890 relevant journal articles subjected to\nanalysis. The bibliometric analysis gives a rich insight into the knowledge\nfield's conceptual, intellectual, and social structure, indicating nascent and\nunderdeveloped research directions. As far as we were able to identify, this is\nthe first study to map and bibliometrically analyze the academic field\nconcerning the relationship between Artificial Intelligence, Entrepreneurship,\nand Finance, and the first review that deals with Artificial Intelligence\nmethods in Entrepreneurship. According to the results, Artificial Neural\nNetwork, Deep Neural Network and Support Vector Machine are highly represented\nin almost all identified topic niches. At the same time, applying Topic\nModeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is\nquite rare. As an element of the research, and before final remarks, the\narticle deals as well with a discussion of certain gaps in the relationship\nbetween Computer Science and Economics. These gaps do represent problems in the\napplication of Artificial Intelligence in Economic Science. As a way to at\nleast in part remedy this situation, the foundational paradigm and the bespoke\ndemonstration of the Monte Carlo randomized algorithm are presented.",
            "author": [
                "Robert Kudeli\u0107",
                "Tamara \u0160maguc",
                "Sherry Robinson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13213v1",
                "http://arxiv.org/pdf/2311.13213v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13195v1",
            "title": "A Topological Embedding of the Binary Tree into the Square Lattice",
            "updated": "2023-11-22T06:52:02Z",
            "published": "2023-11-22T06:52:02Z",
            "summary": "We prove that for any finite tree $T$ with $n$ vertices and maximal degree\n$3$, there is a topological embedding of $T$ into the integer grid $Z^2$ which\nmaps vertices to vertices and whose image meets at most $\\frac{7}{3}n$\nvertices. This resolves question $7.7$ of arXiv:2112.05305, and furthermore\ngives the first example of a pair of graphs $X,Y$ such that there is no regular\nmap $X\\to Y$ but the coarse wiring profile of $X$ into $Y$ grows linearly.",
            "author": [
                "Samuel Kelly"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13195v1",
                "http://arxiv.org/pdf/2311.13195v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG",
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13616v1",
            "title": "Online Video Quality Enhancement with Spatial-Temporal Look-up Tables",
            "updated": "2023-11-22T06:49:44Z",
            "published": "2023-11-22T06:49:44Z",
            "summary": "Low latency rates are crucial for online video-based applications, such as\nvideo conferencing and cloud gaming, which make improving video quality in\nonline scenarios increasingly important. However, existing quality enhancement\nmethods are limited by slow inference speed and the requirement for temporal\ninformation contained in future frames, making it challenging to deploy them\ndirectly in online tasks. In this paper, we propose a novel method, STLVQE,\nspecifically designed to address the rarely studied online video quality\nenhancement (Online-VQE) problem. Our STLVQE designs a new VQE framework which\ncontains a Module-Agnostic Feature Extractor that greatly reduces the redundant\ncomputations and redesign the propagation, alignment, and enhancement module of\nthe network. A Spatial-Temporal Look-up Tables (STL) is proposed, which\nextracts spatial-temporal information in videos while saving substantial\ninference time. To the best of our knowledge, we are the first to exploit the\nLUT structure to extract temporal information in video tasks. Extensive\nexperiments on the MFQE 2.0 dataset demonstrate that our STLVQE achieves a\nsatisfactory performance-speed trade-off.",
            "author": [
                "Zefan Qu",
                "Xinyang Jiang",
                "Yifan Yang",
                "Dongsheng Li",
                "Cairong Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13616v1",
                "http://arxiv.org/pdf/2311.13616v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13188v1",
            "title": "Cracking the Code of Negative Transfer: A Cooperative Game Theoretic\n  Approach for Cross-Domain Sequential Recommendation",
            "updated": "2023-11-22T06:30:54Z",
            "published": "2023-11-22T06:30:54Z",
            "summary": "This paper investigates Cross-Domain Sequential Recommendation (CDSR), a\npromising method that uses information from multiple domains (more than three)\nto generate accurate and diverse recommendations, and takes into account the\nsequential nature of user interactions. The effectiveness of these systems\noften depends on the complex interplay among the multiple domains. In this\ndynamic landscape, the problem of negative transfer arises, where heterogeneous\nknowledge between dissimilar domains leads to performance degradation due to\ndifferences in user preferences across these domains. As a remedy, we propose a\nnew CDSR framework that addresses the problem of negative transfer by assessing\nthe extent of negative transfer from one domain to another and adaptively\nassigning low weight values to the corresponding prediction losses. To this\nend, the amount of negative transfer is estimated by measuring the marginal\ncontribution of each domain to model performance based on a cooperative game\ntheory. In addition, a hierarchical contrastive learning approach that\nincorporates information from the sequence of coarse-level categories into that\nof fine-level categories (e.g., item level) when implementing contrastive\nlearning was developed to mitigate negative transfer. Despite the potentially\nlow relevance between domains at the fine-level, there may be higher relevance\nat the category level due to its generalised and broader preferences. We show\nthat our model is superior to prior works in terms of model performance on two\nreal-world datasets across ten different domains.",
            "author": [
                "Chung Park",
                "Taesan Kim",
                "Taekyoon Choi",
                "Junui Hong",
                "Yelim Yu",
                "Mincheol Cho",
                "Kyunam Lee",
                "Sungil Ryu",
                "Hyungjun Yoon",
                "Minsung Choi",
                "Jaegul Choo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13188v1",
                "http://arxiv.org/pdf/2311.13188v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13187v2",
            "title": "NeISF: Neural Incident Stokes Field for Geometry and Material Estimation",
            "updated": "2023-11-29T08:22:11Z",
            "published": "2023-11-22T06:28:30Z",
            "summary": "Multi-view inverse rendering is the problem of estimating the scene\nparameters such as shapes, materials, or illuminations from a sequence of\nimages captured under different viewpoints. Many approaches, however, assume\nsingle light bounce and thus fail to recover challenging scenarios like\ninter-reflections. On the other hand, simply extending those methods to\nconsider multi-bounced light requires more assumptions to alleviate the\nambiguity. To address this problem, we propose Neural Incident Stokes Fields\n(NeISF), a multi-view inverse rendering framework that reduces ambiguities\nusing polarization cues. The primary motivation for using polarization cues is\nthat it is the accumulation of multi-bounced light, providing rich information\nabout geometry and material. Based on this knowledge, the proposed incident\nStokes field efficiently models the accumulated polarization effect with the\naid of an original physically-based differentiable polarimetric renderer.\nLastly, experimental results show that our method outperforms the existing\nworks in synthetic and real scenarios.",
            "author": [
                "Chenhao Li",
                "Taishi Ono",
                "Takeshi Uemori",
                "Hajime Mihara",
                "Alexander Gatto",
                "Hajime Nagahara",
                "Yusuke Moriuchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13187v2",
                "http://arxiv.org/pdf/2311.13187v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13177v1",
            "title": "Volumetric Reconstruction Resolves Off-Resonance Artifacts in Static and\n  Dynamic PROPELLER MRI",
            "updated": "2023-11-22T05:44:51Z",
            "published": "2023-11-22T05:44:51Z",
            "summary": "Off-resonance artifacts in magnetic resonance imaging (MRI) are visual\ndistortions that occur when the actual resonant frequencies of spins within the\nimaging volume differ from the expected frequencies used to encode spatial\ninformation. These discrepancies can be caused by a variety of factors,\nincluding magnetic field inhomogeneities, chemical shifts, or susceptibility\ndifferences within the tissues. Such artifacts can manifest as blurring,\nghosting, or misregistration of the reconstructed image, and they often\ncompromise its diagnostic quality. We propose to resolve these artifacts by\nlifting the 2D MRI reconstruction problem to 3D, introducing an additional\n\"spectral\" dimension to model this off-resonance. Our approach is inspired by\nrecent progress in modeling radiance fields, and is capable of reconstructing\nboth static and dynamic MR images as well as separating fat and water, which is\nof independent clinical interest. We demonstrate our approach in the context of\nPROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced\nReconstruction) MRI acquisitions, which are popular for their robustness to\nmotion artifacts. Our method operates in a few minutes on a single GPU, and to\nour knowledge is the first to correct for chemical shift in gradient echo\nPROPELLER MRI reconstruction without additional measurements or pretraining\ndata.",
            "author": [
                "Annesha Ghosh",
                "Gordon Wetzstein",
                "Mert Pilanci",
                "Sara Fridovich-Keil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13177v1",
                "http://arxiv.org/pdf/2311.13177v1"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13174v1",
            "title": "SecureCut: Federated Gradient Boosting Decision Trees with Efficient\n  Machine Unlearning",
            "updated": "2023-11-22T05:38:53Z",
            "published": "2023-11-22T05:38:53Z",
            "summary": "In response to legislation mandating companies to honor the \\textit{right to\nbe forgotten} by erasing user data, it has become imperative to enable data\nremoval in Vertical Federated Learning (VFL) where multiple parties provide\nprivate features for model training. In VFL, data removal, i.e.,\n\\textit{machine unlearning}, often requires removing specific features across\nall samples under privacy guarentee in federated learning. To address this\nchallenge, we propose \\methname, a novel Gradient Boosting Decision Tree (GBDT)\nframework that effectively enables both \\textit{instance unlearning} and\n\\textit{feature unlearning} without the need for retraining from scratch.\nLeveraging a robust GBDT structure, we enable effective data deletion while\nreducing degradation of model performance. Extensive experimental results on\npopular datasets demonstrate that our method achieves superior model utility\nand forgetfulness compared to \\textit{state-of-the-art} methods. To our best\nknowledge, this is the first work that investigates machine unlearning in VFL\nscenarios.",
            "author": [
                "Jian Zhang",
                "Bowen Li Jie Li",
                "Chentao Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13174v1",
                "http://arxiv.org/pdf/2311.13174v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13170v1",
            "title": "An iterative deep learning procedure for determining electron scattering\n  cross-sections from transport coefficients",
            "updated": "2023-11-22T05:28:02Z",
            "published": "2023-11-22T05:28:02Z",
            "summary": "We propose improvements to the Artificial Neural Network (ANN) method of\ndetermining electron scattering cross-sections from swarm data proposed by\ncoauthors. A limitation inherent to this problem, known as the inverse swarm\nproblem, is the non-unique nature of its solutions, particularly when there\nexists multiple cross-sections that each describe similar scattering processes.\nConsidering this, prior methods leveraged existing knowledge of a particular\ncross-section set to reduce the solution space of the problem. To reduce the\nneed for prior knowledge, we propose the following modifications to the ANN\nmethod. First, we propose a Multi-Branch ANN (MBANN) that assigns an\nindependent branch of hidden layers to each cross-section output. We show that\nin comparison with an equivalent conventional ANN, the MBANN architecture\nenables an efficient and physics informed feature map of each cross-section.\nAdditionally, we show that the MBANN solution can be improved upon by\nsuccessive networks that are each trained using perturbations of the previous\nregression. Crucially, the method requires much less input data and fewer\nrestrictive assumptions, and only assumes knowledge of energy loss thresholds\nand the number of cross-sections present.",
            "author": [
                "Dale L Muccignat",
                "Gregory G Boyle",
                "Nathan A Garland",
                "Peter W Stokes",
                "Ronald D White"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13170v1",
                "http://arxiv.org/pdf/2311.13170v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13153v1",
            "title": "Unique Factorization For Tensor Products of Parabolic Verma Modules",
            "updated": "2023-11-22T04:31:26Z",
            "published": "2023-11-22T04:31:26Z",
            "summary": "Let $\\mathfrak{g}$ be a symmetrizable Kac-Moody Lie algebra with Cartan\nsubalgebra $\\mathfrak{h}$. We prove a unique factorization property for tensor\nproducts of parabolic Verma modules. More generally, we prove unique\nfactorization for products of characters of parabolic Verma modules when\nrestricted to certain subalgebras of $\\mathfrak{h}$. These include fixed point\nsubalgebras of $\\mathfrak{h}$ under subgroups of diagram automorphisms of\n$\\mathfrak{g}$ and twisted graph automorphisms in the affine case.",
            "author": [
                "K. N. Raghavan",
                "V. Sathish Kumar",
                "R. Venkatesh",
                "Sankaran Viswanath"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13153v1",
                "http://arxiv.org/pdf/2311.13153v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "17B67, 17B10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13147v1",
            "title": "Optimal Transport with Cyclic Symmetry",
            "updated": "2023-11-22T04:18:23Z",
            "published": "2023-11-22T04:18:23Z",
            "summary": "We propose novel fast algorithms for optimal transport (OT) utilizing a\ncyclic symmetry structure of input data. Such OT with cyclic symmetry appears\nuniversally in various real-world examples: image processing, urban planning,\nand graph processing. Our main idea is to reduce OT to a small optimization\nproblem that has significantly fewer variables by utilizing cyclic symmetry and\nvarious optimization techniques. On the basis of this reduction, our algorithms\nsolve the small optimization problem instead of the original OT. As a result,\nour algorithms obtain the optimal solution and the objective function value of\nthe original OT faster than solving the original OT directly. In this paper,\nour focus is on two crucial OT formulations: the linear programming OT (LOT)\nand the strongly convex-regularized OT, which includes the well-known\nentropy-regularized OT (EROT). Experiments show the effectiveness of our\nalgorithms for LOT and EROT in synthetic/real-world data that has a\nstrict/approximate cyclic symmetry structure. Through theoretical and\nexperimental results, this paper successfully introduces the concept of\nsymmetry into the OT research field for the first time.",
            "author": [
                "Shoichiro Takeda",
                "Yasunori Akagi",
                "Naoki Marumo",
                "Kenta Niwa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13147v1",
                "http://arxiv.org/pdf/2311.13147v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16173v2",
            "title": "Conditions for Length Generalization in Learning Reasoning Skills",
            "updated": "2023-12-06T16:31:50Z",
            "published": "2023-11-22T03:36:18Z",
            "summary": "Reasoning is a fundamental capability of AI agents. Recently, large language\nmodels (LLMs) have shown remarkable abilities to perform reasoning tasks.\nHowever, numerous evaluations of the reasoning capabilities of LLMs have also\nshowed some limitations. An outstanding limitation is length generalization,\nmeaning that when trained on reasoning problems of smaller lengths or sizes,\nthe resulting models struggle with problems of larger sizes or lengths. This\npotentially indicates some theoretical limitations of generalization in\nlearning reasoning skills. These evaluations and their observations motivated\nus to perform a theoretical study of the length generalization problem. This\nwork focuses on reasoning tasks that can be formulated as Markov dynamic\nprocesses (MDPs) and/or directed acyclic graphs (DAGs). It identifies and\nproves conditions that decide whether the length generalization problem can be\nsolved or not for a reasoning task in a particular representation. Experiments\nare also conducted to verify the theoretical results.",
            "author": [
                "Changnan Xiao",
                "Bing Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16173v2",
                "http://arxiv.org/pdf/2311.16173v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13132v1",
            "title": "Orientable Burning Number of Graphs",
            "updated": "2023-11-22T03:35:16Z",
            "published": "2023-11-22T03:35:16Z",
            "summary": "In this paper, we introduce the problem of finding an orientation of a given\nundirected graph that maximizes the burning number of the resulting directed\ngraph. We show that the problem is polynomial-time solvable on\nK\\H{o}nig-Egerv\\'{a}ry graphs (and thus on bipartite graphs) and that an almost\noptimal solution can be computed in polynomial time for perfect graphs. On the\nother hand, we show that the problem is NP-hard in general and W[1]-hard\nparameterized by the target burning number. The hardness results are\ncomplemented by several fixed-parameter tractable results parameterized by\nstructural parameters. Our main result in this direction shows that the problem\nis fixed-parameter tractable parameterized by cluster vertex deletion number\nplus clique number (and thus also by vertex cover number).",
            "author": [
                "Julien Courtiel",
                "Paul Dorbec",
                "Tatsuya Gima",
                "Romain Lecoq",
                "Yota Otachi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13132v1",
                "http://arxiv.org/pdf/2311.13132v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13100v1",
            "title": "Automated Measurement of Pericoronary Adipose Tissue Attenuation and\n  Volume in CT Angiography",
            "updated": "2023-11-22T01:59:19Z",
            "published": "2023-11-22T01:59:19Z",
            "summary": "Pericoronary adipose tissue (PCAT) is the deposition of fat in the vicinity\nof the coronary arteries. It is an indicator of coronary inflammation and\nassociated with coronary artery disease. Non-invasive coronary CT angiography\n(CCTA) is presently used to obtain measures of the thickness, volume, and\nattenuation of fat deposition. However, prior works solely focus on measuring\nPCAT using semi-automated approaches at the right coronary artery (RCA) over\nthe left coronary artery (LCA). In this pilot work, we developed a fully\nautomated approach for the measurement of PCAT mean attenuation and volume in\nthe region around both coronary arteries. First, we used a large subset of\npatients from the public ImageCAS dataset (n = 735) to train a 3D full\nresolution nnUNet to segment LCA and RCA. Then, we automatically measured PCAT\nin the surrounding arterial regions. We evaluated our method on a held-out test\nset of patients (n = 183) from the same dataset. A mean Dice score of 83% and\nPCAT attenuation of -73.81 $\\pm$ 12.69 HU was calculated for the RCA, while a\nmean Dice score of 81% and PCAT attenuation of -77.51 $\\pm$ 7.94 HU was\ncomputed for the LCA. To the best of our knowledge, we are the first to develop\na fully automated method to measure PCAT attenuation and volume at both the RCA\nand LCA. Our work underscores how automated PCAT measurement holds promise as a\nbiomarker for identification of inflammation and cardiac disease.",
            "author": [
                "Andrew M. Nguyen",
                "Tejas Sudharshan Mathai",
                "Liangchen Liu",
                "Jianfei Liu",
                "Ronald M. Summers"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13100v1",
                "http://arxiv.org/pdf/2311.13100v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "62P10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13094v1",
            "title": "Newton-CG methods for nonconvex unconstrained optimization with H\u00f6lder\n  continuous Hessian",
            "updated": "2023-11-22T01:50:43Z",
            "published": "2023-11-22T01:50:43Z",
            "summary": "In this paper we consider a nonconvex unconstrained optimization problem\nminimizing a twice differentiable objective function with H\\\"older continuous\nHessian. Specifically, we first propose a Newton-conjugate gradient (Newton-CG)\nmethod for finding an approximate first-order stationary point (FOSP) of this\nproblem, assuming the associated the H\\\"older parameters are explicitly known.\nThen we develop a parameter-free Newton-CG method without requiring any prior\nknowledge of these parameters. To the best of our knowledge, this method is the\nfirst parameter-free second-order method achieving the best-known iteration and\noperation complexity for finding an approximate FOSP of this problem.\nFurthermore, we propose a Newton-CG method for finding an approximate\nsecond-order stationary point (SOSP) of the considered problem with high\nprobability and establish its iteration and operation complexity. Finally, we\npresent preliminary numerical results to demonstrate the superior practical\nperformance of our parameter-free Newton-CG method over a well-known\nregularized Newton method.",
            "author": [
                "Chuan He",
                "Zhaosong Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13094v1",
                "http://arxiv.org/pdf/2311.13094v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13080v1",
            "title": "High-Speed Voltage Control in Active Distribution Systems with Smart\n  Inverter Coordination and Deep Reinforcement Learning",
            "updated": "2023-11-22T00:58:16Z",
            "published": "2023-11-22T00:58:16Z",
            "summary": "The increasing penetration of renewable energy resources in distribution\nsystems necessitates high-speed monitoring and control of voltage for ensuring\nreliable system operation. However, existing voltage control algorithms often\nmake simplifying assumptions in their formulation, such as real-time\navailability of smart meter measurements (for monitoring), or real-time\nknowledge of every power injection information(for control).This paper\nleverages the recent advances made in highspeed state estimation for real-time\nunobservable distribution systems to formulate a deep reinforcement\nlearning-based control algorithm that utilizes the state estimates alone to\ncontrol the voltage of the entire system. The results obtained for a modified\n(renewable-rich) IEEE34-nodedistributionfeeder indicate that the proposed\napproach excels in monitoring and controlling voltage of active distribution\nsystems.",
            "author": [
                "Mohammad Golgol",
                "Anamitra Pal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13080v1",
                "http://arxiv.org/pdf/2311.13080v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13059v1",
            "title": "A note on estimating the dimension from a random geometric graph",
            "updated": "2023-11-21T23:46:44Z",
            "published": "2023-11-21T23:46:44Z",
            "summary": "Let $G_n$ be a random geometric graph with vertex set $[n]$ based on $n$\ni.i.d.\\ random vectors $X_1,\\ldots,X_n$ drawn from an unknown density $f$ on\n$\\R^d$. An edge $(i,j)$ is present when $\\|X_i -X_j\\| \\le r_n$, for a given\nthreshold $r_n$ possibly depending upon $n$, where $\\| \\cdot \\|$ denotes\nEuclidean distance. We study the problem of estimating the dimension $d$ of the\nunderlying space when we have access to the adjacency matrix of the graph but\ndo not know $r_n$ or the vectors $X_i$. The main result of the paper is that\nthere exists an estimator of $d$ that converges to $d$ in probability as $n \\to\n\\infty$ for all densities with $\\int f^5 < \\infty$ whenever $n^{3/2} r_n^d \\to\n\\infty$ and $r_n = o(1)$. The conditions allow very sparse graphs since when\n$n^{3/2} r_n^d \\to 0$, the graph contains isolated edges only, with high\nprobability. We also show that, without any condition on the density, a\nconsistent estimator of $d$ exists when $n r_n^d \\to \\infty$ and $r_n = o(1)$.",
            "author": [
                "Caelan Atamanchuk",
                "Luc Devroye",
                "Gabor Lugosi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13059v1",
                "http://arxiv.org/pdf/2311.13059v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13054v1",
            "title": "Tomographic Imaging of the Sagittarius Spiral Arm's Magnetic Field\n  Structure",
            "updated": "2023-11-21T23:31:40Z",
            "published": "2023-11-21T23:31:40Z",
            "summary": "The Galactic global magnetic field is thought to play a vital role in shaping\nGalactic structures such as spiral arms and giant molecular clouds. However,\nour knowledge of magnetic field structures in the Galactic plane at different\ndistances is limited, as measurements used to map the magnetic field are the\nintegrated effect along the line of sight. In this study, we present the\nfirst-ever tomographic imaging of magnetic field structures in a Galactic\nspiral arm. Using optical stellar polarimetry over a $17' \\times 10'$ field of\nview, we probe the Sagittarius spiral arm. Combining these data with stellar\ndistances from the $Gaia$ mission, we can isolate the contributions of five\nindividual clouds along the line of sight by analyzing the polarimetry data as\na function of distance. The observed clouds include a foreground cloud ($d <\n200$ pc) and four clouds in the Sagittarius arm at 1.23 kpc, 1.47 kpc, 1.63\nkpc, and 2.23 kpc. The column densities of these clouds range from 0.5 to $2.8\n\\times 10^{21}~\\mathrm{cm}^{-2}$. The magnetic fields associated with each\ncloud show smooth spatial distributions within their observed regions on scales\nsmaller than 10 pc and display distinct orientations. The position angles\nprojected on the plane-of-sky, measured from the Galactic north to east, for\nthe clouds in increasing order of distance are $135^\\circ$, $46^\\circ$,\n$58^\\circ$, $150^\\circ$, and $40^\\circ$, with uncertainties of a few degrees.\nNotably, these position angles deviate significantly from the direction\nparallel to the Galactic plane.",
            "author": [
                "Yasuo Doi",
                "Kengo Nakamura",
                "Koji S. Kawabata",
                "Masafumi Matsumura",
                "Hiroshi Akitaya",
                "Simon Coud\u00e9",
                "Claudia V. Rodrigues",
                "Jungmi Kwon",
                "Motohide Tamura",
                "Mehrnoosh Tahani",
                "Antonio Mario Magalh\u00e3es",
                "Reinaldo Santos-Lima",
                "Yenifer Angarita",
                "Jos\u00e9 Versteeg",
                "Marijke Haverkorn",
                "Tetsuo Hasegawa",
                "Sarah Sadavoy",
                "Doris Arzoumanian",
                "Pierre Bastien"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13054v1",
                "http://arxiv.org/pdf/2311.13054v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13051v1",
            "title": "Latent Lab: Large Language Models for Knowledge Exploration",
            "updated": "2023-11-21T23:23:16Z",
            "published": "2023-11-21T23:23:16Z",
            "summary": "This paper investigates the potential of AI models, particularly large\nlanguage models (LLMs), to support knowledge exploration and augment human\ncreativity during ideation. We present \"Latent Lab\" an interactive tool for\ndiscovering connections among MIT Media Lab research projects, emphasizing\n\"exploration\" over search. The work offers insights into collaborative AI\nsystems by addressing the challenges of organizing, searching, and synthesizing\ncontent. In a user study, the tool's success was evaluated based on its ability\nto introduce users to an unfamiliar knowledge base, ultimately setting the\ngroundwork for the ongoing advancement of human-AI knowledge exploration\nsystems.",
            "author": [
                "Kevin Dunnell",
                "Trudy Painter",
                "Andrew Stoddard",
                "Andy Lippman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13051v1",
                "http://arxiv.org/pdf/2311.13051v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13034v1",
            "title": "Soft random simplicial complexes",
            "updated": "2023-11-21T22:46:55Z",
            "published": "2023-11-21T22:46:55Z",
            "summary": "A soft random graph $G(n,r,p)$ can be obtained from the random geometric\ngraph $G(n,r)$ by keeping every edge in $G(n,r)$ with probability $p$. This\nrandom graph is a particular case of the soft random graph model introduced by\nPenrose, in which the probability between 2 vertices is a function that depends\non the distance between them. In this article, we define models for random\nsimplicial complexes built over the soft random graph $G(n,r,p)$, which also\npresent randomness in all other dimensions. Furthermore, we study the homology\nof those random simplicial complexes in different regimes of $n,r$, and $p$ by\ngiving asymptotic formulas for the expectation of the Betti numbers in the\nsparser regimes, and bounds in the denser regimes.",
            "author": [
                "Juli\u00e1n David Candela"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13034v1",
                "http://arxiv.org/pdf/2311.13034v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13022v1",
            "title": "Unsupervised Multimodal Surface Registration with Geometric Deep\n  Learning",
            "updated": "2023-11-21T22:05:00Z",
            "published": "2023-11-21T22:05:00Z",
            "summary": "This paper introduces GeoMorph, a novel geometric deep-learning framework\ndesigned for image registration of cortical surfaces. The registration process\nconsists of two main steps. First, independent feature extraction is performed\non each input surface using graph convolutions, generating low-dimensional\nfeature representations that capture important cortical surface\ncharacteristics. Subsequently, features are registered in a deep-discrete\nmanner to optimize the overlap of common structures across surfaces by learning\ndisplacements of a set of control points. To ensure smooth and biologically\nplausible deformations, we implement regularization through a deep conditional\nrandom field implemented with a recurrent neural network. Experimental results\ndemonstrate that GeoMorph surpasses existing deep-learning methods by achieving\nimproved alignment with smoother deformations. Furthermore, GeoMorph exhibits\ncompetitive performance compared to classical frameworks. Such versatility and\nrobustness suggest strong potential for various neuroscience applications.",
            "author": [
                "Mohamed A. Suliman",
                "Logan Z. J. Williams",
                "Abdulah Fawaz",
                "Emma C. Robinson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13022v1",
                "http://arxiv.org/pdf/2311.13022v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13015v1",
            "title": "Fast and Interpretable Mortality Risk Scores for Critical Care Patients",
            "updated": "2023-11-21T21:44:28Z",
            "published": "2023-11-21T21:44:28Z",
            "summary": "Prediction of mortality in intensive care unit (ICU) patients is an important\ntask in critical care medicine. Prior work in creating mortality risk models\nfalls into two major categories: domain-expert-created scoring systems, and\nblack box machine learning (ML) models. Both of these have disadvantages: black\nbox models are unacceptable for use in hospitals, whereas manual creation of\nmodels (including hand-tuning of logistic regression parameters) relies on\nhumans to perform high-dimensional constrained optimization, which leads to a\nloss in performance. In this work, we bridge the gap between accurate black box\nmodels and hand-tuned interpretable models. We build on modern interpretable ML\ntechniques to design accurate and interpretable mortality risk scores. We\nleverage the largest existing public ICU monitoring datasets, namely the MIMIC\nIII and eICU datasets. By evaluating risk across medical centers, we are able\nto study generalization across domains. In order to customize our risk score\nmodels, we develop a new algorithm, GroupFasterRisk, which has several\nimportant benefits: (1) it uses hard sparsity constraint, allowing users to\ndirectly control the number of features; (2) it incorporates group sparsity to\nallow more cohesive models; (3) it allows for monotonicity correction on models\nfor including domain knowledge; (4) it produces many equally-good models at\nonce, which allows domain experts to choose among them. GroupFasterRisk creates\nits risk scores within hours, even on the large datasets we study here.\nGroupFasterRisk's risk scores perform better than risk scores currently used in\nhospitals, and have similar prediction performance to black box ML models\n(despite being much sparser). Because GroupFasterRisk produces a variety of\nrisk scores and handles constraints, it allows design flexibility, which is the\nkey enabler of practical and trustworthy model creation.",
            "author": [
                "Chloe Qinyu Zhu",
                "Muhang Tian",
                "Lesia Semenova",
                "Jiachang Liu",
                "Jack Xu",
                "Joseph Scarpa",
                "Cynthia Rudin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13015v1",
                "http://arxiv.org/pdf/2311.13015v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13014v1",
            "title": "Neural Graph Control Barrier Functions Guided Distributed\n  Collision-avoidance Multi-agent Control",
            "updated": "2023-11-21T21:43:18Z",
            "published": "2023-11-21T21:43:18Z",
            "summary": "We consider the problem of designing distributed collision-avoidance\nmulti-agent control in large-scale environments with potentially moving\nobstacles, where a large number of agents are required to maintain safety using\nonly local information and reach their goals. This paper addresses the problem\nof collision avoidance, scalability, and generalizability by introducing graph\ncontrol barrier functions (GCBFs) for distributed control. The newly introduced\nGCBF is based on the well-established CBF theory for safety guarantees but\nutilizes a graph structure for scalable and generalizable decentralized\ncontrol. We use graph neural networks to learn both neural a GCBF certificate\nand distributed control. We also extend the framework from handling state-based\nmodels to directly taking point clouds from LiDAR for more practical robotics\nsettings. We demonstrated the efficacy of GCBF in a variety of numerical\nexperiments, where the number, density, and traveling distance of agents, as\nwell as the number of unseen and uncontrolled obstacles increase. Empirical\nresults show that GCBF outperforms leading methods such as MAPPO and\nmulti-agent distributed CBF (MDCBF). Trained with only 16 agents, GCBF can\nachieve up to 3 times improvement of success rate (agents reach goals and never\nencountered in any collisions) on <500 agents, and still maintain more than 50%\nsuccess rates for >1000 agents when other methods completely fail.",
            "author": [
                "Songyuan Zhang",
                "Kunal Garg",
                "Chuchu Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13014v1",
                "http://arxiv.org/pdf/2311.13014v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.13008v1",
            "title": "zkTax: A pragmatic way to support zero-knowledge tax disclosures",
            "updated": "2023-11-21T21:34:10Z",
            "published": "2023-11-21T21:34:10Z",
            "summary": "Tax returns contain key financial information of interest to third parties:\npublic officials are asked to share financial data for transparency, companies\nseek to assess the financial status of business partners, and individuals need\nto prove their income to landlords or to receive benefits. Tax returns also\ncontain sensitive data such that sharing them in their entirety undermines\nprivacy. We introduce a zero-knowledge tax disclosure system (zkTax) that\nallows individuals and organizations to make provable claims about select\ninformation in their tax returns without revealing additional information,\nwhich can be independently verified by third parties. The system consists of\nthree 3distinct services that can be distributed: a tax authority provides tax\ndocuments signed with a public key; a Redact & Prove Service enables users to\nproduce a redacted version of the tax documents with a zero-knowledge proof\nattesting the provenance of the redacted data; a Verify Service enables anyone\nto verify the proof. We implement a prototype with a user interface, compatible\nwith U.S. tax forms, and demonstrate how this design could be implemented with\nminimal changes to existing tax infrastructure. Our system is designed to be\nextensible to other contexts and jurisdictions. This work provides a practical\nexample of how distributed tools leveraging cryptography can enhance existing\ngovernment or financial infrastructures, providing immediate transparency\nalongside privacy without system overhauls.",
            "author": [
                "Alex Berke",
                "Tobin South",
                "Robert Mahari",
                "Kent Larson",
                "Alex Pentland"
            ],
            "link": [
                "http://arxiv.org/abs/2311.13008v1",
                "http://arxiv.org/pdf/2311.13008v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12994v1",
            "title": "Sum-of-Squares Lower Bounds for the Minimum Circuit Size Problem",
            "updated": "2023-11-21T21:04:08Z",
            "published": "2023-11-21T21:04:08Z",
            "summary": "We prove lower bounds for the Minimum Circuit Size Problem (MCSP) in the\nSum-of-Squares (SoS) proof system. Our main result is that for every Boolean\nfunction $f: \\{0,1\\}^n \\rightarrow \\{0,1\\}$, SoS requires degree\n$\\Omega(s^{1-\\epsilon})$ to prove that $f$ does not have circuits of size $s$\n(for any $s > \\mathrm{poly}(n)$). As a corollary we obtain that there are no\nlow degree SoS proofs of the statement NP $\\not \\subseteq $ P/poly.\n  We also show that for any $0 < \\alpha < 1$ there are Boolean functions with\ncircuit complexity larger than $2^{n^{\\alpha}}$ but SoS requires size\n$2^{2^{\\Omega(n^{\\alpha})}}$ to prove this. In addition we prove analogous\nresults on the minimum \\emph{monotone} circuit size for monotone Boolean slice\nfunctions.\n  Our approach is quite general. Namely, we show that if a proof system $Q$ has\nstrong enough constraint satisfaction problem lower bounds that only depend on\ngood expansion of the constraint-variable incidence graph and, furthermore, $Q$\nis expressive enough that variables can be substituted by local Boolean\nfunctions, then the MCSP problem is hard for $Q$.",
            "author": [
                "Per Austrin",
                "Kilian Risse"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12994v1",
                "http://arxiv.org/pdf/2311.12994v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12992v1",
            "title": "FollowMe: a Robust Person Following Framework Based on Re-Identification\n  and Gestures",
            "updated": "2023-11-21T20:59:27Z",
            "published": "2023-11-21T20:59:27Z",
            "summary": "Human-robot interaction (HRI) has become a crucial enabler in houses and\nindustries for facilitating operational flexibility. When it comes to mobile\ncollaborative robots, this flexibility can be further increased due to the\nautonomous mobility and navigation capacity of the robotic agents, expanding\ntheir workspace and consequently, the personalizable assistance they can\nprovide to the human operators. This however requires that the robot is capable\nof detecting and identifying the human counterpart in all stages of the\ncollaborative task, and in particular while following a human in crowded\nworkplaces. To respond to this need, we developed a unified perception and\nnavigation framework, which enables the robot to identify and follow a target\nperson using a combination of visual Re-Identification (Re-ID), hand gestures\ndetection, and collision-free navigation. The Re-ID module can autonomously\nlearn the features of a target person and use the acquired knowledge to\nvisually re-identify the target. The navigation stack is used to follow the\ntarget avoiding obstacles and other individuals in the environment. Experiments\nare conducted with few subjects in a laboratory setting where some unknown\ndynamic obstacles are introduced.",
            "author": [
                "Federico Rollo",
                "Andrea Zunino",
                "Gennaro Raiola",
                "Fabio Amadio",
                "Arash Ajoudani",
                "Nikolaos Tsagarakis"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ARSO56563.2023.10187536",
                "http://arxiv.org/abs/2311.12992v1",
                "http://arxiv.org/pdf/2311.12992v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12986v2",
            "title": "Unsupervised Graph Attention Autoencoder for Attributed Networks using\n  K-means Loss",
            "updated": "2023-11-24T22:24:50Z",
            "published": "2023-11-21T20:45:55Z",
            "summary": "Several natural phenomena and complex systems are often represented as\nnetworks. Discovering their community structure is a fundamental task for\nunderstanding these networks. Many algorithms have been proposed, but recently,\nGraph Neural Networks (GNN) have emerged as a compelling approach for enhancing\nthis task.In this paper, we introduce a simple, efficient, and\nclustering-oriented model based on unsupervised \\textbf{G}raph Attention\n\\textbf{A}uto\\textbf{E}ncoder for community detection in attributed networks\n(GAECO). The proposed model adeptly learns representations from both the\nnetwork's topology and attribute information, simultaneously addressing dual\nobjectives: reconstruction and community discovery. It places a particular\nemphasis on discovering compact communities by robustly minimizing clustering\nerrors. The model employs k-means as an objective function and utilizes a\nmulti-head Graph Attention Auto-Encoder for decoding the representations.\nExperiments conducted on three datasets of attributed networks show that our\nmethod surpasses state-of-the-art algorithms in terms of NMI and ARI.\nAdditionally, our approach scales effectively with the size of the network,\nmaking it suitable for large-scale applications. The implications of our\nfindings extend beyond biological network interpretation and social network\nanalysis, where knowledge of the fundamental community structure is essential.",
            "author": [
                "Abdelfateh Bekkaira",
                "Slimane Bellaouar",
                "Slimane Oulad-Naoui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12986v2",
                "http://arxiv.org/pdf/2311.12986v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "68T07",
                "I.2.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12978v1",
            "title": "Physics-Informed Priors with Application to Boundary Layer Velocity",
            "updated": "2023-11-21T20:25:50Z",
            "published": "2023-11-21T20:25:50Z",
            "summary": "One of the most popular recent areas of machine learning predicates the use\nof neural networks augmented by information about the underlying process in the\nform of Partial Differential Equations (PDEs). These physics-informed neural\nnetworks are obtained by penalizing the inference with a PDE, and have been\ncast as a minimization problem currently lacking a formal approach to quantify\nthe uncertainty. In this work, we propose a novel model-based framework which\nregards the PDE as a prior information of a deep Bayesian neural network. The\nprior is calibrated without data to resemble the PDE solution in the prior\nmean, while our degree in confidence on the PDE with respect to the data is\nexpressed in terms of the prior variance. The information embedded in the PDE\nis then propagated to the posterior yielding physics-informed forecasts with\nuncertainty quantification. We apply our approach to a simulated viscous fluid\nand to experimentally-obtained turbulent boundary layer velocity in a wind\ntunnel using an appropriately simplified Navier-Stokes equation. Our approach\nrequires very few observations to produce physically-consistent forecasts as\nopposed to non-physical forecasts stemming from non-informed priors, thereby\nallowing forecasting complex systems where some amount of data as well as some\ncontextual knowledge is available.",
            "author": [
                "Luca Menicali",
                "David H. Richter",
                "Stefano Castruccio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12978v1",
                "http://arxiv.org/pdf/2311.12978v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12976v1",
            "title": "Fast Deterministic Rendezvous in Labeled Lines",
            "updated": "2023-11-21T20:24:33Z",
            "published": "2023-11-21T20:24:33Z",
            "summary": "Two mobile agents, starting from different nodes of a network modeled as a\ngraph, and woken up at possibly different times, have to meet at the same node.\nThis problem is known as rendezvous. We consider deterministic distributed\nrendezvous in the infinite path. Each node has a distinct label which is a\npositive integer. The time of rendezvous is the number of rounds until meeting,\ncounted from the starting round of the earlier agent. We consider three\nscenarios. In the first scenario, each agent knows its position in the line,\ni.e., each of them knows its initial distance from the smallest-labeled node,\non which side of this node it is located, and the direction towards it. For\nthis scenario, we give a rendezvous algorithm working in time $O(D)$, where $D$\nis the initial distance between the agents. This complexity is clearly optimal.\nIn the second scenario, each agent initially knows only the label of its\nstarting node and the initial distance $D$ between the agents. In this\nscenario, we give a rendezvous algorithm working in time $O(D\\log^*\\ell)$,\nwhere $\\ell$ is the larger label of the starting nodes. We prove a matching\nlower bound $\\Omega(D\\log^*\\ell)$. Finally, in the most general scenario, where\neach agent initially knows only the label of its starting node, we give a\nrendezvous algorithm working in time $O(D^2(\\log^*\\ell)^3)$, which is at most\ncubic in the lower bound. All our results remain valid (with small changes) for\narbitrary finite paths and for cycles. Our algorithms are drastically better\nthan approaches that use graph exploration, whose running times depend on the\ngraph's size or diameter. Our main methodological tool, and the main novelty of\nthe paper, is a two way reduction: from fast colouring of the infinite labeled\npath using a constant number of colours in the LOCAL model to fast rendezvous\nin this path, and vice-versa.",
            "author": [
                "Avery Miller",
                "Andrzej Pelc"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12976v1",
                "http://arxiv.org/pdf/2311.12976v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12972v1",
            "title": "A Fermionic Grunsky operator",
            "updated": "2023-11-21T20:22:16Z",
            "published": "2023-11-21T20:22:16Z",
            "summary": "To a conformal map $f$ from the disk $\\mathbb{D}$ into the complex plane onto\na domain with rectifiable Ahlfors-regular boundary, we associate a new kind of\nGrunsky operator on the Hardy space of the unit disk. This is analogous to the\nclassical Grunsky operator, which itself can be viewed as an operator on\nBergman or Dirichlet space. We show that the pull-back of the Smirnov space of\nthe complement of $f(\\mathbb{D})$ by $f$ is the graph of the Grunsky operator.\nWe also characterize those domains with rectifiable Ahlfors-regular boundaries\nsuch that the Grunsky operator is Hilbert-Schmidt. In particular, we show that\nif the Grunsky operator is Hilbert-Schmidt, then $f(\\mathbb{D})$ is a\nWeil-Petersson quasidisk. The formulations of the results and proofs make\nessential use of a geometric treatment of Smirnov space as a space of\nhalf-order differentials.",
            "author": [
                "Peter Kristel",
                "Eric Schippers",
                "Wolfgang Staubach"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12972v1",
                "http://arxiv.org/pdf/2311.12972v1"
            ],
            "primary_category": "math.CV",
            "category": [
                "math.CV",
                "math-ph",
                "math.DG",
                "math.FA",
                "math.MP",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12961v4",
            "title": "Demystifying Digital Twin Buzzword: A Novel Generic Evaluation Model",
            "updated": "2023-12-07T07:56:00Z",
            "published": "2023-11-21T19:56:26Z",
            "summary": "Despite the growing popularity of digital twin (DT) developments, there is a\nlack of common understanding and definition for important concepts of DT. It is\nneeded to address this gap by building a shared understanding of DT before it\nbecomes an obstacle for future work. With this challenge in view, the objective\nof our study is to assess the existing DT from various domains on a common\nbasis and to unify the knowledge and understanding of DT developers and\nstakeholders before practice. To achieve this goal, we conducted a systematic\nliterature review and analyzed 25 selected papers to identify and discuss the\ncharacteristics of existing DT's. The review shows an inconsistency and\ncase-specific choices of dimensions in assessing DT. Therefore, this article\nproposes a four-dimensional evaluation framework to assess the maturity of\ndigital twins across different domains, focusing on the characteristics of\ndigital models. The four identified dimensions in this model are Capability,\nCooperability, Coverage, and Lifecycle. Additionally, a weight mechanism is\nimplemented inside the model to adapt the importance of each dimension for\ndifferent application requirements. Several case studies are devised to\nvalidate the proposed model in general, industrial and scientific cases.",
            "author": [
                "Zhengyu Liu",
                "Sina Namaki Araghi",
                "Arkopaul Sarkar",
                "Mohamed Hedi Karray"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12961v4",
                "http://arxiv.org/pdf/2311.12961v4"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12955v1",
            "title": "Don't forget private retrieval: distributed private similarity search\n  for large language models",
            "updated": "2023-11-21T19:41:46Z",
            "published": "2023-11-21T19:41:46Z",
            "summary": "While the flexible capabilities of large language models (LLMs) allow them to\nanswer a range of queries based on existing learned knowledge, information\nretrieval to augment generation is an important tool to allow LLMs to answer\nquestions on information not included in pre-training data. Such private\ninformation is increasingly being generated in a wide array of distributed\ncontexts by organizations and individuals. Performing such information\nretrieval using neural embeddings of queries and documents always leaked\ninformation about queries and database content unless both were stored locally.\nWe present Private Retrieval Augmented Generation (PRAG), an approach that uses\nmulti-party computation (MPC) to securely transmit queries to a distributed set\nof servers containing a privately constructed database to return top-k and\napproximate top-k documents. This is a first-of-its-kind approach to dense\ninformation retrieval that ensures no server observes a client's query or can\nsee the database content. The approach introduces a novel MPC friendly protocol\nfor inverted file approximate search (IVF) that allows for fast document search\nover distributed and private data in sublinear communication complexity. This\nwork presents new avenues through which data for use in LLMs can be accessed\nand used without needing to centralize or forgo privacy.",
            "author": [
                "Guy Zyskind",
                "Tobin South",
                "Alex Pentland"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12955v1",
                "http://arxiv.org/pdf/2311.12955v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12954v1",
            "title": "First Extragalactic Detection of Thermal Hydroxyl (OH) 18cm Emission in\n  M31 Reveals Abundant CO-faint Molecular Gas",
            "updated": "2023-11-21T19:38:31Z",
            "published": "2023-11-21T19:38:31Z",
            "summary": "The most abundant interstellar molecule, molecular Hydrogen (H$_{2}$), is\npractically invisible in cold molecular clouds. Astronomers typically use\ncarbon monoxide (CO) to trace the bulk distribution and mass of H$_{2}$ in our\ngalaxy and many others. CO observations alone fail to trace a massive component\nof molecular gas known as \"CO-dark\" gas. We present an ultra sensitive pilot\nsearch for the 18cm hydroxyl (OH) lines in the Andromeda Galaxy (M31) with the\n100m Robert C. Byrd Green Bank Telescope. We successfully detected the 1667 and\n1665 MHz OH in faint emission. The 1665/1667 MHz line ratio is consistent with\nthe characteristic 5:9 ratio associated with local thermodynamic equilibrium\n(LTE). To our knowledge, this is the first detection of non-maser 18cm OH\nemission in another galaxy. We compare our OH and HI observations with archival\nCO (1-0) observations. Our OH detection position overlaps with the previously\ndiscovered Arp Outer Arm in CO. Our best estimates show that the amount of\nH$_{2}$ traced by OH is 140% higher than the amount traced by CO in this\nsightline. We show that the amount of dark molecular gas implied by dust data\nsupports this conclusion. We conclude that the 18cm OH lines hold promise as a\nvaluable tool for mapping of the \"CO-dark\" and \"CO-faint\" molecular gas phase\nin nearby galaxies, especially with upcoming multi-beam, phased-array feed\nreceivers on radio telescopes which will allow for drastically improved mapping\nspeeds of faint signals.",
            "author": [
                "Michael P. Busch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12954v1",
                "http://arxiv.org/pdf/2311.12954v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12946v1",
            "title": "A Cosheaf Theory of Reciprocal Figures: Planar and Higher Genus Graphic\n  Statics",
            "updated": "2023-11-21T19:19:19Z",
            "published": "2023-11-21T19:19:19Z",
            "summary": "This paper introduces cellular sheaf theory to graphical methods and\nreciprocal constructions in structural engineering. The elementary mechanics\nand statics of trusses are derived from the linear algebra of sheaves and\ncosheaves. Further, the homological algebra of these mathematical constructions\ncleanly and concisely describes the formation of 2D reciprocal diagrams and 3D\npolyhedral lifts. Additional relationships between geometric quantities of\nthese dual diagrams are developed, including systems of impossible edge\nrotations. These constructions generalize to non-planar graphs. When a truss\nembedded in a torus or higher genus surface has a sufficient degree of axial\nself stress, we show non-trivial reciprocal figures and non-simply connected\npolyhedral lifts are guaranteed to exist.",
            "author": [
                "Zoe Cooperband",
                "Robert Ghrist",
                "Jakob Hansen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12946v1",
                "http://arxiv.org/pdf/2311.12946v1"
            ],
            "primary_category": "math.AT",
            "category": [
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12938v1",
            "title": "Linear divergence in finitely generated groups",
            "updated": "2023-11-21T19:02:44Z",
            "published": "2023-11-21T19:02:44Z",
            "summary": "In this paper, we show that wreath products of groups have linear divergence,\nand we generalise the argument to permutational wreath products. We also prove\nthat Houghton groups $\\mathcal{H}_m$ with $m\\geq 2$ and Baumslag-Solitar groups\nhave linear divergence. Finally, we show that wreath products of graphs and\nDiestel-Leader graphs have linear divergence.",
            "author": [
                "Letizia Issini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12938v1",
                "http://arxiv.org/pdf/2311.12938v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12931v1",
            "title": "Argyres-Douglas Theories, IR N-ality and Complete Graphs",
            "updated": "2023-11-21T19:00:05Z",
            "published": "2023-11-21T19:00:05Z",
            "summary": "We show that for a large subclass of Argyres-Douglas-type theories, the Higgs\nbranch admits multiple hyperkahler quotient realizations as Higgs branches of\nthree dimensional $\\mathcal{N}=4$ quiver gauge theories, which are related by a\nsequence of Seiberg-like IR dualities. We refer to this phenomenon as the\nHyperkahler Quotient N-ality of the four dimensional Higgs branch. The\nassociated set of 3d theories contains a special subset of maximal unitary\nquivers: quiver gauge theories for which the resolution/deformation parameters\nof the Higgs branch are manifest in the Lagrangian as Fayet-Iliopoulas\nparameters. Starting from the Type IIB description for a given SCFT, we present\nan explicit construction to determine the aforementioned set of 3d quivers,\nincluding the subset of maximal unitary quivers. As a byproduct, we find a\nsimple method for constructing the three dimensional mirror associated with the\nSCFT. We demonstrate the construction for the $(A_k, A_k)$ theories of Cecotti,\nNeitzke and Vafa, focusing on the cases $k=3$ and $k=4$. The associated maximal\nunitary quiver is unique up to field redefinitions and turns out to be an\nAbelian quiver gauge theory. The three dimensional mirror obtained in this\nfashion reproduces the well-known complete graph. In the appendices to the main\npaper, we study the quotient N-ality in the closely related family of $D^b_p\n(SU(N))$ SCFTs, for which both the maximal unitary quiver as well as the 3d\nmirror turn out to be non-Abelian gauge theories generically",
            "author": [
                "Anindya Dey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12931v1",
                "http://arxiv.org/pdf/2311.12931v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12929v1",
            "title": "Hierarchical Learning for Quantum ML: Novel Training Technique for\n  Large-Scale Variational Quantum Circuits",
            "updated": "2023-11-21T19:00:03Z",
            "published": "2023-11-21T19:00:03Z",
            "summary": "We present hierarchical learning, a novel variational architecture for\nefficient training of large-scale variational quantum circuits. We test and\nbenchmark our technique for distribution loading with quantum circuit born\nmachines (QCBMs). With QCBMs, probability distributions are loaded into the\nsquared amplitudes of computational basis vectors represented by bitstrings.\nOur key insight is to take advantage of the fact that the most significant\n(qu)bits have a greater effect on the final distribution and can be learned\nfirst. One can think of it as a generalization of layerwise learning, where\nsome parameters of the variational circuit are learned first to prevent the\nphenomena of barren plateaus. We briefly review adjoint methods for computing\nthe gradient, in particular for loss functions that are not expectation values\nof observables. We first compare the role of connectivity in the variational\nansatz for the task of loading a Gaussian distribution on nine qubits, finding\nthat 2D connectivity greatly outperforms qubits arranged on a line. Based on\nour observations, we then implement this strategy on large-scale numerical\nexperiments with GPUs, training a QCBM to reproduce a 3-dimensional\nmultivariate Gaussian distribution on 27 qubits up to $\\sim4\\%$ total variation\ndistance. Though barren plateau arguments do not strictly apply here due to the\nobjective function not being tied to an observable, this is to our knowledge\nthe first practical demonstration of variational learning on large numbers of\nqubits. We also demonstrate hierarchical learning as a resource-efficient way\nto load distributions for existing quantum hardware (IBM's 7 and 27 qubit\ndevices) in tandem with Fire Opal optimizations.",
            "author": [
                "Hrant Gharibyan",
                "Vincent Su",
                "Hayk Tepanyan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12929v1",
                "http://arxiv.org/pdf/2311.12929v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12793v2",
            "title": "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions",
            "updated": "2023-11-28T08:52:50Z",
            "published": "2023-11-21T18:58:11Z",
            "summary": "In the realm of large multi-modal models (LMMs), efficient modality alignment\nis crucial yet often constrained by the scarcity of high-quality image-text\ndata. To address this bottleneck, we introduce the ShareGPT4V dataset, a\npioneering large-scale resource featuring 1.2 million highly descriptive\ncaptions, which surpasses existing datasets in diversity and information\ncontent, covering world knowledge, object properties, spatial relationships,\nand aesthetic evaluations. Specifically, ShareGPT4V originates from a curated\n100K high-quality captions collected from advanced GPT4-Vision and has been\nexpanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V\nfirst demonstrates its effectiveness for the Supervised Fine-Tuning (SFT)\nphase, by substituting an equivalent quantity of detailed captions in existing\nSFT datasets with a subset of our high-quality captions, significantly\nenhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME\nand MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and\n2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training\nand SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple\narchitecture that has remarkable performance across a majority of the\nmulti-modal benchmarks. This project is available at\nhttps://ShareGPT4V.github.io to serve as a pivotal resource for advancing the\nLMMs community.",
            "author": [
                "Lin Chen",
                "Jinsong Li",
                "Xiaoyi Dong",
                "Pan Zhang",
                "Conghui He",
                "Jiaqi Wang",
                "Feng Zhao",
                "Dahua Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12793v2",
                "http://arxiv.org/pdf/2311.12793v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12784v1",
            "title": "Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian,\n  and Beyond $1+\u03b1$ Moments",
            "updated": "2023-11-21T18:50:38Z",
            "published": "2023-11-21T18:50:38Z",
            "summary": "There is growing interest in improving our algorithmic understanding of\nfundamental statistical problems such as mean estimation, driven by the goal of\nunderstanding the limits of what we can extract from valuable data. The state\nof the art results for mean estimation in $\\mathbb{R}$ are 1) the optimal\nsub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant for\nall distributions with finite but unknown variance, and 2) the analysis of the\nmedian-of-means algorithm by [BCL13] and a lower bound by [DLLO16],\ncharacterizing the big-O optimal errors for distributions for which only a\n$1+\\alpha$ moment exists for $\\alpha \\in (0,1)$. Both results, however, are\noptimal only in the worst case. We initiate the fine-grained study of the mean\nestimation problem: Can algorithms leverage useful features of the input\ndistribution to beat the sub-Gaussian rate, without explicit knowledge of such\nfeatures?\n  We resolve this question with an unexpectedly nuanced answer: \"Yes in limited\nregimes, but in general no\". For any distribution $p$ with a finite mean, we\nconstruct a distribution $q$ whose mean is well-separated from $p$'s, yet $p$\nand $q$ are not distinguishable with high probability, and $q$ further\npreserves $p$'s moments up to constants. The main consequence is that no\nreasonable estimator can asymptotically achieve better than the sub-Gaussian\nerror rate for any distribution, matching the worst-case result of [LV22]. More\ngenerally, we introduce a new definitional framework to analyze the\nfine-grained optimality of algorithms, which we call \"neighborhood optimality\",\ninterpolating between the unattainably strong \"instance optimality\" and the\ntrivially weak \"admissibility\" definitions. Applying the new framework, we show\nthat median-of-means is neighborhood optimal, up to constant factors. It is\nopen to find a neighborhood-optimal estimator without constant factor\nslackness.",
            "author": [
                "Trung Dang",
                "Jasper C. H. Lee",
                "Maoyuan Song",
                "Paul Valiant"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12784v1",
                "http://arxiv.org/pdf/2311.12784v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "cs.IT",
                "cs.LG",
                "math.IT",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12917v1",
            "title": "Orchard: building large cancer phylogenies using stochastic\n  combinatorial search",
            "updated": "2023-11-21T18:25:23Z",
            "published": "2023-11-21T18:25:23Z",
            "summary": "Phylogenies depicting the evolutionary history of genetically heterogeneous\nsubpopulations of cells from the same cancer i.e., cancer phylogenies, provide\nuseful insights about cancer development and inform treatment. Cancer\nphylogenies can be reconstructed using data obtained from bulk DNA sequencing\nof multiple tissue samples from the same cancer. We introduce Orchard, a fast\nalgorithm that reconstructs cancer phylogenies using point mutations detected\nin bulk DNA sequencing data. Orchard constructs cancer phylogenies\nprogressively, one point mutation at a time, ultimately sampling complete\nphylogenies from a posterior distribution implied by the bulk DNA data. Orchard\nreconstructs more plausible phylogenies than state-of-the-art cancer phylogeny\nreconstruction methods on 90 simulated cancers and 14 B-progenitor acute\nlymphoblastic leukemias (B-ALLs). These results demonstrate that Orchard\naccurately reconstructs cancer phylogenies with up to 300 mutations. We then\nintroduce a simple graph based clustering algorithm that uses a reconstructed\nphylogeny to infer unique groups of mutations i.e., mutation clusters, that\ncharacterize the genetic differences between cancer cell populations, and show\nthat this approach is competitive with state-of-the-art mutation clustering\nmethods.",
            "author": [
                "E. Kulman",
                "R. Kuang",
                "Q. Morris"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12917v1",
                "http://arxiv.org/pdf/2311.12917v1"
            ],
            "primary_category": "q-bio.PE",
            "category": [
                "q-bio.PE",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12761v2",
            "title": "Lecture hall graphs and the Askey scheme",
            "updated": "2023-11-22T14:07:26Z",
            "published": "2023-11-21T18:15:30Z",
            "summary": "We establish, for every family of orthogonal polynomials in the $q$-Askey\nscheme and the Askey scheme, a combinatorial model for mixed moments and\ncoefficients in terms of paths on the lecture hall graph. This generalizes the\nprevious results of Corteel and Kim for the little $q$-Jacobi polynomials. We\nbuild these combinatorial models by bootstrapping, beginning with polynomials\nat the bottom and working towards Askey-Wilson polynomials which sit at the top\nof the $q$-Askey scheme. As an application of the theory, we provide the first\ncombinatorial proof of the symmetries in the parameters of the Askey-Wilson\npolynomials.",
            "author": [
                "Sylvie Corteel",
                "Bhargavi Jonnadula",
                "Jonathan P. Keating",
                "Jang Soo Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12761v2",
                "http://arxiv.org/pdf/2311.12761v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math-ph",
                "math.CA",
                "math.MP",
                "05A15 (primary) 33D45, 05A10, 05A19, 05A30 (secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12750v1",
            "title": "Learning to Optimise Wind Farms with Graph Transformers",
            "updated": "2023-11-21T17:51:30Z",
            "published": "2023-11-21T17:51:30Z",
            "summary": "This work proposes a novel data-driven model capable of providing accurate\npredictions for the power generation of all wind turbines in wind farms of\narbitrary layout, yaw angle configurations and wind conditions. The proposed\nmodel functions by encoding a wind farm into a fully-connected graph and\nprocessing the graph representation through a graph transformer. The graph\ntransformer surrogate is shown to generalise well and is able to uncover latent\nstructural patterns within the graph representation of wind farms. It is\ndemonstrated how the resulting surrogate model can be used to optimise yaw\nangle configurations using genetic algorithms, achieving similar levels of\naccuracy to industrially-standard wind farm simulation tools while only taking\na fraction of the computational cost.",
            "author": [
                "Siyi Li",
                "Arnaud Robert",
                "A. Aldo Faisal",
                "Matthew D. Piggott"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12750v1",
                "http://arxiv.org/pdf/2311.12750v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2312.02988v1",
            "title": "Efficient Subgraph Isomorphism Finding in Large Graphs using\n  Eccentricity and Limiting Recursive Calls",
            "updated": "2023-11-21T17:49:27Z",
            "published": "2023-11-21T17:49:27Z",
            "summary": "The subgraph isomorphism finding problem is a well-studied problem in the\nfield of computer science and graph theory, and it aims to enumerate all\ninstances of a query graph in the respective data graph. In this paper, we\npropose an efficient method, SubISO, to find subgraph isomorphisms using an\nobjective function, which exploits some isomorphic invariants and eccentricity\nof the query graph's vertices. The proposed objective function is used to\ndetermine pivot vertex, which minimizes both number and size of the candidate\nregions in the data graph. SubISO also limits the maximum recursive calls of\nthe generic SubgraphSearch function to deal with straggler queries for which\nmost of the existing algorithms show exponential behaviour. The proposed\napproach is evaluated over three benchmark datasets. It is also compared with\nthree well known subgraph isomorphism finding algorithms in terms of execution\ntime, number of identified embeddings, and ability to deal with the straggler\nqueries, and it performs significantly better.",
            "author": [
                "Zubair Ali Ansari",
                "Muhammad Abulaish",
                "Irfan Rashid Thoker",
                "Jahiruddin"
            ],
            "link": [
                "http://arxiv.org/abs/2312.02988v1",
                "http://arxiv.org/pdf/2312.02988v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12741v1",
            "title": "Content Augmented Graph Neural Networks",
            "updated": "2023-11-21T17:30:57Z",
            "published": "2023-11-21T17:30:57Z",
            "summary": "In recent years, graph neural networks (GNNs) have become a popular tool for\nsolving various problems over graphs. In these models, the link structure of\nthe graph is typically exploited and nodes' embeddings are iteratively updated\nbased on adjacent nodes. Nodes' contents are used solely in the form of feature\nvectors, served as nodes' first-layer embeddings. However, the filters or\nconvolutions, applied during iterations/layers to these initial embeddings lead\nto their impact diminish and contribute insignificantly to the final\nembeddings. In order to address this issue, in this paper we propose augmenting\nnodes' embeddings by embeddings generating from their content, at higher GNN\nlayers. More precisely, we propose models wherein a structural embedding using\na GNN and a content embedding are computed for each node. These two are\ncombined using a combination layer to form the embedding of a node at a given\nlayer. We suggest methods such as using an auto-encoder or building a content\ngraph, to generate content embeddings. In the end, by conducting experiments\nover several real-world datasets, we demonstrate the high accuracy and\nperformance of our models.",
            "author": [
                "Fatemeh Gholamzadeh Nasrabadi",
                "AmirHossein Kashani",
                "Pegah Zahedi",
                "Mostafa Haghir Chehreghani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12741v1",
                "http://arxiv.org/pdf/2311.12741v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12912v2",
            "title": "Q-Seg: Quantum Annealing-based Unsupervised Image Segmentation",
            "updated": "2023-11-30T11:38:07Z",
            "published": "2023-11-21T17:27:20Z",
            "summary": "In this study, we present Q-Seg, a novel unsupervised image segmentation\nmethod based on quantum annealing, tailored for existing quantum hardware. We\nformulate the pixel-wise segmentation problem, which assimilates spectral and\nspatial information of the image, as a graph-cut optimization task. Our method\nefficiently leverages the interconnected qubit topology of the D-Wave Advantage\ndevice, offering superior scalability over existing quantum approaches and\noutperforming state-of-the-art classical methods. Our empirical evaluations on\nsynthetic datasets reveal that Q-Seg offers better runtime performance against\nthe classical optimizer Gurobi. Furthermore, we evaluate our method on\nsegmentation of Earth Observation images, an area of application where the\namount of labeled data is usually very limited. In this case, Q-Seg\ndemonstrates near-optimal results in flood mapping detection with respect to\nclassical supervised state-of-the-art machine learning methods. Also, Q-Seg\nprovides enhanced segmentation for forest coverage compared to existing\nannotated masks. Thus, Q-Seg emerges as a viable alternative for real-world\napplications using available quantum hardware, particularly in scenarios where\nthe lack of labeled data and computational runtime are critical.",
            "author": [
                "Supreeth Mysore Venkatesh",
                "Antonio Macaluso",
                "Marlon Nuske",
                "Matthias Klusch",
                "Andreas Dengel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12912v2",
                "http://arxiv.org/pdf/2311.12912v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12737v1",
            "title": "Exploring Graph Classification Techniques Under Low Data Constraints: A\n  Comprehensive Study",
            "updated": "2023-11-21T17:23:05Z",
            "published": "2023-11-21T17:23:05Z",
            "summary": "This survey paper presents a brief overview of recent research on graph data\naugmentation and few-shot learning. It covers various techniques for graph data\naugmentation, including node and edge perturbation, graph coarsening, and graph\ngeneration, as well as the latest developments in few-shot learning, such as\nmeta-learning and model-agnostic meta-learning. The paper explores these areas\nin depth and delves into further sub classifications. Rule based approaches and\nlearning based approaches are surveyed under graph augmentation techniques.\nFew-Shot Learning on graphs is also studied in terms of metric learning\ntechniques and optimization-based techniques. In all, this paper provides an\nextensive array of techniques that can be employed in solving graph processing\nproblems faced in low-data scenarios.",
            "author": [
                "Kush Kothari",
                "Bhavya Mehta",
                "Reshmika Nambiar",
                "Seema Shrawne"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ICCCNT56998.2023.10307388",
                "http://arxiv.org/abs/2311.12737v1",
                "http://arxiv.org/pdf/2311.12737v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12732v1",
            "title": "Tight Lieb-Robinson Bound for approximation ratio in Quantum Annealing",
            "updated": "2023-11-21T17:15:21Z",
            "published": "2023-11-21T17:15:21Z",
            "summary": "Quantum annealing (QA) holds promise for optimization problems in quantum\ncomputing, especially for combinatorial optimization. This analog framework\nattracts attention for its potential to address complex problems. Its\ngate-based homologous, QAOA with proven performance, has brought lots of\nattention to the NISQ era. Several numerical benchmarks try to classify these\ntwo metaheuristics however, classical computational power highly limits the\nperformance insights. In this work, we introduce a new parametrized version of\nQA enabling a precise 1-local analysis of the algorithm. We develop a tight\nLieb-Robinson bound for regular graphs, achieving the best-known numerical\nvalue to analyze QA locally. Studying MaxCut over cubic graph as a benchmark\noptimization problem, we show that a linear-schedule QA with a 1-local analysis\nachieves an approximation ratio over 0.7020, outperforming any known 1-local\nalgorithms.",
            "author": [
                "Arthur Braida",
                "Simon Martiel",
                "Ioan Todinca"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12732v1",
                "http://arxiv.org/pdf/2311.12732v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12910v1",
            "title": "The Hanna Neumann Conjecture for graphs of free groups with cyclic edge\n  groups",
            "updated": "2023-11-21T16:47:30Z",
            "published": "2023-11-21T16:47:30Z",
            "summary": "The Hanna Neumann Conjecture (HNC) for a free group $G$ predicts that\n$\\overline\\chi(U \\cap V) \\leqslant \\overline\\chi(U) \\overline\\chi(V)$ for all\nfinitely generated subgroups $U$ and $V$, where $\\overline\\chi(H) =\n\\min\\{-\\chi(H),0\\}$ denotes the reduced Euler characteristic of $H$. A\nstrengthened version of the HNC was proved independently by Friedman and\nMineyev in 2011. Recently, Antol\\'in and Jaikin-Zapirain introduced the\n$L^2$-Hall property and showed that if $G$ is a hyperbolic limit group that\nsatisfies this property, then $G$ satisfies the HNC. Antol\\'in--Jaikin-Zapirain\nestablished the $L^2$-Hall property for free and surface groups, which\nBrown--Kharlampovich extended to all limit groups. In this article, we prove\nthe $L^2$-Hall property for graphs of free groups with cyclic edge groups that\nare hyperbolic relative to virtually abelian subgroups and also give another\nproof of the $L^2$-Hall property for limit groups. As a corollary, we show that\nall these groups satisfy a strengthened version of the HNC.",
            "author": [
                "Sam P. Fisher",
                "Ismael Morales"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12910v1",
                "http://arxiv.org/pdf/2311.12910v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "20F65, 20J05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12700v1",
            "title": "A Data-Driven Integrated Framework for Fast-Charging Facility Planning\n  using Multi-Period Bi-Objective Optimization",
            "updated": "2023-11-21T16:11:54Z",
            "published": "2023-11-21T16:11:54Z",
            "summary": "With the electrification in freight transportation, the availability of\nfast-charging facilities becomes essential to facilitate en-route charging for\nfreight electric vehicles. Most studies focus on planning charging facilities\nbased on mathematical modeling and hypothetical scenarios. This study aims to\ndevelop a data-driven integrated framework for fast-charging facility planning.\nBy leveraging the highway traffic data, we extracted, analyzed, and compared\nspatial and temporal flow patterns of general traffic and freight traffic.\nFurthermore, graph theory-based network evaluation methods are employed to\nidentify traffic nodes within the highway network that play a significant role\nin accommodating charging infrastructure. A candidate selection method is\nproposed to obtain potential deployment locations for charging stations and\nto-go chargers. Based on this, we present a multi-period bi-objective\noptimization model to provide optimal solutions for the placement of charging\nfacilities, with the objectives of minimizing investment cost and maximizing\ndemand coverage. The case study on the Amsterdam highway network shows how\nexisting traffic data can be used to generate more realistic charging demand\nscenarios and how it can be integrated and evaluated within the optimization\nframework for facility planning. The study also shows that the proposed model\ncan leverage the potential of early investment in improving the charging demand\ncoverage.",
            "author": [
                "Mingjia He",
                "Panchamy Krishnakumari",
                "Ding Luo",
                "Jiaqi Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12700v1",
                "http://arxiv.org/pdf/2311.12700v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12693v2",
            "title": "Non-radial NLS equation with competing inhomogeneous nonlinearities:\n  Ground states, Blow-up and Scattering",
            "updated": "2023-11-23T08:45:41Z",
            "published": "2023-11-21T15:57:51Z",
            "summary": "We investigate a class of nonlinear Schr\\\"odinger equations with competing\ninhomogeneous nonlinearities in the non-radial inter-critical regime, $$ i\n\\partial_t u +\\Delta u =|x|^{-b_1} |u|^{p_1-2} u - |x|^{-b_2} |u|^{p_2-2}u\n\\quad \\mbox{in} \\,\\, \\mathbb{R} \\times \\mathbb{R}^N, $$ where $N \\geq 1$, $b_1,\nb_2>0$ and $p_1,p_2>2$.\n  First, we establish the existence/nonexistence, symmetry, decay, uniqueness,\nnon-degeneracy and instability of ground states. Then, we prove the scattering\nversus blowup below the ground state energy threshold. Our approach relies on\nTao's scattering criterion and Dodson-Murphy's Virial/Morawetz inequalities. We\nalso obtain an upper bound of the blow-up rate. The novelty here is that the\nequation does not enjoy any scaling invariance due to the presence of competing\nnonlinearties and the singular weights prevent the invariance by translation in\nthe space variable.\n  To the best of authors knowledge, this is the first time when inhomegenous\nNLS equation with a focusing leading order nonlinearity and a defocusing\nperturbation is investigated.",
            "author": [
                "Tianxiang Gou",
                "Mohamed Majdoub",
                "Tarek Saanouni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12693v2",
                "http://arxiv.org/pdf/2311.12693v2"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12682v1",
            "title": "Transferring to Real-World Layouts: A Depth-aware Framework for Scene\n  Adaptation",
            "updated": "2023-11-21T15:39:21Z",
            "published": "2023-11-21T15:39:21Z",
            "summary": "Scene segmentation via unsupervised domain adaptation (UDA) enables the\ntransfer of knowledge acquired from source synthetic data to real-world target\ndata, which largely reduces the need for manual pixel-level annotations in the\ntarget domain. To facilitate domain-invariant feature learning, existing\nmethods typically mix data from both the source domain and target domain by\nsimply copying and pasting the pixels. Such vanilla methods are usually\nsub-optimal since they do not take into account how well the mixed layouts\ncorrespond to real-world scenarios. Real-world scenarios are with an inherent\nlayout. We observe that semantic categories, such as sidewalks, buildings, and\nsky, display relatively consistent depth distributions, and could be clearly\ndistinguished in a depth map. Based on such observation, we propose a\ndepth-aware framework to explicitly leverage depth estimation to mix the\ncategories and facilitate the two complementary tasks, i.e., segmentation and\ndepth learning in an end-to-end manner. In particular, the framework contains a\nDepth-guided Contextual Filter (DCF) forndata augmentation and a cross-task\nencoder for contextual learning. DCF simulates the real-world layouts, while\nthe cross-task encoder further adaptively fuses the complementing features\nbetween two tasks. Besides, it is worth noting that several public datasets do\nnot provide depth annotation. Therefore, we leverage the off-the-shelf depth\nestimation network to generate the pseudo depth. Extensive experiments show\nthat our proposed methods, even with pseudo depth, achieve competitive\nperformance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes\nand 69.3 mIoU on Synthia to Cityscapes.",
            "author": [
                "Mu Chen",
                "Zhedong Zheng",
                "Yi Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12682v1",
                "http://arxiv.org/pdf/2311.12682v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12675v1",
            "title": "Occupation Number Representation of Graph",
            "updated": "2023-11-21T15:31:48Z",
            "published": "2023-11-21T15:31:48Z",
            "summary": "In this paper, we propose a new way to represent graphs in quantum space. In\nthat approach, we replace the rows of the adjacency matrix of the graph by\nstate vectors in the occupation number representation. Unlike the traditional\ndefinition of graph states, we actually let the occupation number of a\nsingle-particle state denote the number of edges between each two adjacent\nvertices. This allows us to avoid taking into account the interaction between\neach two particles. Based on the creation and annihilation operators, we\npropose the edge creation and annihilation operators. With these two operators,\nwe can implement the fundamental operation of adding and removing edges and\nvertices in a graph. Then all additional operations in the graph such as vertex\ncontractions can be defined. Our method can be used to represent both simple\nand multigraphs. Directed and undirected graphs are also compatible with our\napproach. The method of representation proposed in this paper enriches the\ntheory of graph representation in quantum space.",
            "author": [
                "Haoqian Pan",
                "Changhong Lu",
                "Ben Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12675v1",
                "http://arxiv.org/pdf/2311.12675v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12670v2",
            "title": "Towards a more inductive world for drug repurposing approaches",
            "updated": "2023-11-24T10:49:50Z",
            "published": "2023-11-21T15:28:44Z",
            "summary": "Drug-target interaction (DTI) prediction is a challenging, albeit essential\ntask in drug repurposing. Learning on graph models have drawn special attention\nas they can significantly reduce drug repurposing costs and time commitment.\nHowever, many current approaches require high-demanding additional information\nbesides DTIs that complicates their evaluation process and usability.\nAdditionally, structural differences in the learning architecture of current\nmodels hinder their fair benchmarking. In this work, we first perform an\nin-depth evaluation of current DTI datasets and prediction models through a\nrobust benchmarking process, and show that DTI prediction methods based on\ntransductive models lack generalization and lead to inflated performance when\nevaluated as previously done in the literature, hence not being suited for drug\nrepurposing approaches. We then propose a novel biologically-driven strategy\nfor negative edge subsampling and show through in vitro validation that newly\ndiscovered interactions are indeed true. We envision this work as the\nunderpinning for future fair benchmarking and robust model design. All\ngenerated resources and tools are publicly available as a python package.",
            "author": [
                "Jesus de la Fuente",
                "Guillermo Serrano",
                "Ux\u00eda Veleiro",
                "Mikel Casals",
                "Laura Vera",
                "Marija Pizurica",
                "Antonio Pineda-Lucena",
                "Idoia Ochoa",
                "Silve Vicent",
                "Olivier Gevaert",
                "Mikel Hernaez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12670v2",
                "http://arxiv.org/pdf/2311.12670v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12664v1",
            "title": "The DURel Annotation Tool: Human and Computational Measurement of\n  Semantic Proximity, Sense Clusters and Semantic Change",
            "updated": "2023-11-21T15:14:54Z",
            "published": "2023-11-21T15:14:54Z",
            "summary": "We present the DURel tool that implements the annotation of semantic\nproximity between uses of words into an online, open source interface. The tool\nsupports standardized human annotation as well as computational annotation,\nbuilding on recent advances with Word-in-Context models. Annotator judgments\nare clustered with automatic graph clustering techniques and visualized for\nanalysis. This allows to measure word senses with simple and intuitive\nmicro-task judgments between use pairs, requiring minimal preparation efforts.\nThe tool offers additional functionalities to compare the agreement between\nannotators to guarantee the inter-subjectivity of the obtained judgments and to\ncalculate summary statistics giving insights into sense frequency\ndistributions, semantic variation or changes of senses over time.",
            "author": [
                "Dominik Schlechtweg",
                "Shafqat Mumtaz Virk",
                "Pauline Sander",
                "Emma Sk\u00f6ldberg",
                "Lukas Theuer Linke",
                "Tuo Zhang",
                "Nina Tahmasebi",
                "Jonas Kuhn",
                "Sabine Schulte im Walde"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12664v1",
                "http://arxiv.org/pdf/2311.12664v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12660v1",
            "title": "Visually Guided Object Grasping",
            "updated": "2023-11-21T15:08:17Z",
            "published": "2023-11-21T15:08:17Z",
            "summary": "In this paper we present a visual servoing approach to the problem of object\ngrasping and more generally, to the problem of aligning an end-effector with an\nobject. First we extend the method proposed by Espiau et al. [1] to the case of\na camera which is not mounted onto the robot being controlled and we stress the\nimportance of the real-time estimation of the image Jacobian. Second, we show\nhow to represent a grasp or more generally, an alignment between two solids in\n3-D projective space using an uncalibrated stereo rig. Such a 3-D projective\nrepresentation is view-invariant in the sense that it can be easily mapped into\nan image set-point without any knowledge about the camera parameters. Third, we\nperform an analysis of the performances of the visual servoing algorithm and of\nthe grasping precision that can be expected from this type of approach.",
            "author": [
                "Radu Horaud",
                "Fadi Dornaika",
                "Bernard Espiau"
            ],
            "link": [
                "http://dx.doi.org/10.1109/70.704214",
                "http://arxiv.org/abs/2311.12660v1",
                "http://arxiv.org/pdf/2311.12660v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12657v1",
            "title": "Carbohydrate NMR chemical shift predictions using E(3) equivariant graph\n  neural networks",
            "updated": "2023-11-21T15:01:14Z",
            "published": "2023-11-21T15:01:14Z",
            "summary": "Carbohydrates, vital components of biological systems, are well-known for\ntheir structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays\na crucial role in understanding their intricate molecular arrangements and is\nessential in assessing and verifying the molecular structure of organic\nmolecules. An important part of this process is to predict the NMR chemical\nshift from the molecular structure. This work introduces a novel approach that\nleverages E(3) equivariant graph neural networks to predict carbohydrate NMR\nspectra. Notably, our model achieves a substantial reduction in mean absolute\nerror, up to threefold, compared to traditional models that rely solely on\ntwo-dimensional molecular structure. Even with limited data, the model excels,\nhighlighting its robustness and generalization capabilities. The implications\nare far-reaching and go beyond an advanced understanding of carbohydrate\nstructures and spectral interpretation. For example, it could accelerate\nresearch in pharmaceutical applications, biochemistry, and structural biology,\noffering a faster and more reliable analysis of molecular structures.\nFurthermore, our approach is a key step towards a new data-driven era in\nspectroscopy, potentially influencing spectroscopic techniques beyond NMR.",
            "author": [
                "Maria B\u00e5nkestad",
                "Keven M. Dorst",
                "G\u00f6ran Widmalm",
                "Jerk R\u00f6nnols"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12657v1",
                "http://arxiv.org/pdf/2311.12657v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12649v1",
            "title": "MathGloss: Building mathematical glossaries from text",
            "updated": "2023-11-21T14:49:00Z",
            "published": "2023-11-21T14:49:00Z",
            "summary": "MathGloss is a project to create a knowledge graph (KG) for undergraduate\nmathematics from text, automatically, using modern natural language processing\n(NLP) tools and resources already available on the web. MathGloss is a linked\ndatabase of undergraduate concepts in mathematics. So far, it combines five\nresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph\nhosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses\nat the University of Chicago, (iii) the syllabus of the French undergraduate\nmathematics curriculum which includes hyperlinks to the automated theorem\nprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by\nmathematicians, and (v) the nLab, a wiki for category theory also curated by\nmathematicians. MathGloss's goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their\nown preferences. Moreover, by organizing different resources for learning\nundergraduate mathematics alongside those for learning formal mathematics, we\nhope to make it easier for mathematicians and formal tools (theorem provers,\ncomputer algebra systems, etc) experts to \"understand\" each other and break\ndown some of the barriers to formal math.",
            "author": [
                "Lucy Horowitz",
                "Valeria de Paiva"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12649v1",
                "http://arxiv.org/pdf/2311.12649v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12646v1",
            "title": "Online landmark replacement for out-of-sample dimensionality reduction\n  methods",
            "updated": "2023-11-21T14:48:01Z",
            "published": "2023-11-21T14:48:01Z",
            "summary": "A strategy to assist visualization and analysis of large and complex data\nsets is dimensionality reduction, with which one maps each data point into a\nlow-dimensional manifold. However, various dimensionality reduction techniques\nare computationally infeasible for large data. Out-of-sample techniques aim to\nresolve this difficulty; they only apply the dimensionality reduction technique\non a small portion of data, referred to as landmarks, and determine the\nembedding coordinates of the other points using landmarks as references.\nOut-of-sample techniques have been applied to online settings, or when data\narrive as time series. However, existing online out-of-sample techniques use\neither all the previous data points as landmarks or the fixed set of landmarks\nand therefore are potentially not good at capturing the geometry of the entire\ndata set when the time series is non-stationary. To address this problem, we\npropose an online landmark replacement algorithm for out-of-sample techniques\nusing geometric graphs and the minimal dominating set on them. We\nmathematically analyze some properties of the proposed algorithm, particularly\nfocusing on the case of landmark multidimensional scaling as the out-of-sample\ntechnique, and test its performance on synthetic and empirical time series\ndata.",
            "author": [
                "Chanon Thongprayoon",
                "Naoki Masuda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12646v1",
                "http://arxiv.org/pdf/2311.12646v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12644v1",
            "title": "Careful Selection and Thoughtful Discarding: Graph Explicit Pooling\n  Utilizing Discarded Nodes",
            "updated": "2023-11-21T14:44:51Z",
            "published": "2023-11-21T14:44:51Z",
            "summary": "Graph pooling has been increasingly recognized as crucial for Graph Neural\nNetworks (GNNs) to facilitate hierarchical graph representation learning.\nExisting graph pooling methods commonly consist of two stages: selecting\ntop-ranked nodes and discarding the remaining to construct coarsened graph\nrepresentations. However, this paper highlights two key issues with these\nmethods: 1) The process of selecting nodes to discard frequently employs\nadditional Graph Convolutional Networks or Multilayer Perceptrons, lacking a\nthorough evaluation of each node's impact on the final graph representation and\nsubsequent prediction tasks. 2) Current graph pooling methods tend to directly\ndiscard the noise segment (dropped) of the graph without accounting for the\nlatent information contained within these elements. To address the first issue,\nwe introduce a novel Graph Explicit Pooling (GrePool) method, which selects\nnodes by explicitly leveraging the relationships between the nodes and final\nrepresentation vectors crucial for classification. The second issue is\naddressed using an extended version of GrePool (i.e., GrePool+), which applies\na uniform loss on the discarded nodes. This addition is designed to augment the\ntraining process and improve classification accuracy. Furthermore, we conduct\ncomprehensive experiments across 12 widely used datasets to validate our\nproposed method's effectiveness, including the Open Graph Benchmark datasets.\nOur experimental results uniformly demonstrate that GrePool outperforms 14\nbaseline methods for most datasets. Likewise, implementing GrePool+ enhances\nGrePool's performance without incurring additional computational costs.",
            "author": [
                "Chuang Liu",
                "Wenhang Yu",
                "Kuang Gao",
                "Xueqi Ma",
                "Yibing Zhan",
                "Jia Wu",
                "Bo Du",
                "Wenbin Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12644v1",
                "http://arxiv.org/pdf/2311.12644v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12641v1",
            "title": "Polyhedral Object Recognition by Indexing",
            "updated": "2023-11-21T14:41:21Z",
            "published": "2023-11-21T14:41:21Z",
            "summary": "In computer vision, the indexing problem is the problem of recognizing a few\nobjects in a large database of objects while avoiding the help of the classical\nimage-feature-to-object-feature matching paradigm. In this paper we address the\nproblem of recognizing 3-D polyhedral objects from 2-D images by indexing. Both\nthe objects to be recognized and the images are represented by weighted graphs.\nThe indexing problem is therefore the problem of determining whether a graph\nextracted from the image is present or absent in a database of model graphs. We\nintroduce a novel method for performing this graph indexing process which is\nbased both on polynomial characterization of binary and weighted graphs and on\nhashing. We describe in detail this polynomial characterization and then we\nshow how it can be used in the context of polyhedral object recognition. Next\nwe describe a practical recognition-by-indexing system that includes the\norganization of the database, the representation of polyhedral objects in terms\nof 2-D characteristic views, the representation of this views in terms of\nweighted graphs, and the associated image processing. Finally, some\nexperimental results allow the evaluation of the system performance.",
            "author": [
                "Radu Horaud",
                "Humberto Sossa"
            ],
            "link": [
                "http://dx.doi.org/10.1016/0031-3203(95)00048-8",
                "http://arxiv.org/abs/2311.12641v1",
                "http://arxiv.org/pdf/2311.12641v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12639v1",
            "title": "KNVQA: A Benchmark for evaluation knowledge-based VQA",
            "updated": "2023-11-21T14:39:18Z",
            "published": "2023-11-21T14:39:18Z",
            "summary": "Within the multimodal field, large vision-language models (LVLMs) have made\nsignificant progress due to their strong perception and reasoning capabilities\nin the visual and language systems. However, LVLMs are still plagued by the two\ncritical issues of object hallucination and factual accuracy, which limit the\npracticality of LVLMs in different scenarios. Furthermore, previous evaluation\nmethods focus more on the comprehension and reasoning of language content but\nlack a comprehensive evaluation of multimodal interactions, thereby resulting\nin potential limitations. To this end, we propose a novel KNVQA-Eval, which is\ndevoted to knowledge-based VQA task evaluation to reflect the factuality of\nmultimodal LVLMs. To ensure the robustness and scalability of the evaluation,\nwe develop a new KNVQA dataset by incorporating human judgment and perception,\naiming to evaluate the accuracy of standard answers relative to AI-generated\nanswers in knowledge-based VQA. This work not only comprehensively evaluates\nthe contextual information of LVLMs using reliable human annotations, but also\nfurther analyzes the fine-grained capabilities of current methods to reveal\npotential avenues for subsequent optimization of LVLMs-based estimators. Our\nproposed VQA-Eval and corresponding dataset KNVQA will facilitate the\ndevelopment of automatic evaluation tools with the advantages of low cost,\nprivacy protection, and reproducibility. Our code will be released upon\npublication.",
            "author": [
                "Sirui Cheng",
                "Siyu Zhang",
                "Jiayi Wu",
                "Muchen Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12639v1",
                "http://arxiv.org/pdf/2311.12639v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12630v2",
            "title": "Hierarchical Joint Graph Learning and Multivariate Time Series\n  Forecasting",
            "updated": "2023-11-30T13:44:21Z",
            "published": "2023-11-21T14:24:21Z",
            "summary": "Multivariate time series is prevalent in many scientific and industrial\ndomains. Modeling multivariate signals is challenging due to their long-range\ntemporal dependencies and intricate interactions--both direct and indirect. To\nconfront these complexities, we introduce a method of representing multivariate\nsignals as nodes in a graph with edges indicating interdependency between them.\nSpecifically, we leverage graph neural networks (GNN) and attention mechanisms\nto efficiently learn the underlying relationships within the time series data.\nMoreover, we suggest employing hierarchical signal decompositions running over\nthe graphs to capture multiple spatial dependencies. The effectiveness of our\nproposed model is evaluated across various real-world benchmark datasets\ndesigned for long-term forecasting tasks. The results consistently showcase the\nsuperiority of our model, achieving an average 23\\% reduction in mean squared\nerror (MSE) compared to existing models.",
            "author": [
                "Juhyeon Kim",
                "Hyungeun Lee",
                "Seungwon Yu",
                "Ung Hwang",
                "Wooyul Jung",
                "Miseon Park",
                "Kijung Yoon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12630v2",
                "http://arxiv.org/pdf/2311.12630v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12616v1",
            "title": "DeepTreeGAN: Fast Generation of High Dimensional Point Clouds",
            "updated": "2023-11-21T13:59:50Z",
            "published": "2023-11-21T13:59:50Z",
            "summary": "In High Energy Physics, detailed and time-consuming simulations are used for\nparticle interactions with detectors. To bypass these simulations with a\ngenerative model, the generation of large point clouds in a short time is\nrequired, while the complex dependencies between the particles must be\ncorrectly modelled. Particle showers are inherently tree-based processes, as\neach particle is produced by the decay or detector interaction of a particle of\nthe previous generation. In this work, we present a novel Graph Neural Network\nmodel (DeepTreeGAN) that is able to generate such point clouds in a tree-based\nmanner. We show that this model can reproduce complex distributions, and we\nevaluate its performance on the public JetNet dataset.",
            "author": [
                "Moritz Alfons Wilhelm Scham",
                "Dirk Kr\u00fccker",
                "Benno K\u00e4ch",
                "Kerstin Borras"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12616v1",
                "http://arxiv.org/pdf/2311.12616v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12603v2",
            "title": "Surgical Temporal Action-aware Network with Sequence Regularization for\n  Phase Recognition",
            "updated": "2023-11-22T02:15:51Z",
            "published": "2023-11-21T13:43:16Z",
            "summary": "To assist surgeons in the operating theatre, surgical phase recognition is\ncritical for developing computer-assisted surgical systems, which requires\ncomprehensive understanding of surgical videos. Although existing studies made\ngreat progress, there are still two significant limitations worthy of\nimprovement. First, due to the compromise of resource consumption, frame-wise\nvisual features are extracted by 2D networks and disregard spatial and temporal\nknowledge of surgical actions, which hinders subsequent inter-frame modeling\nfor phase prediction. Second, these works simply utilize ordinary\nclassification loss with one-hot phase labels to optimize the phase\npredictions, and cannot fully explore surgical videos under inadequate\nsupervision. To overcome these two limitations, we propose a Surgical Temporal\nAction-aware Network with sequence Regularization, named STAR-Net, to recognize\nsurgical phases more accurately from input videos. Specifically, we propose an\nefficient multi-scale surgical temporal action (MS-STA) module, which\nintegrates visual features with spatial and temporal knowledge of surgical\nactions at the cost of 2D networks. Moreover, we devise the dual-classifier\nsequence regularization (DSR) to facilitate the training of STAR-Net by the\nsequence guidance of an auxiliary classifier with a smaller capacity. Our\nSTAR-Net with MS-STA and DSR can exploit visual features of surgical actions\nwith effective regularization, thereby leading to the superior performance of\nsurgical phase recognition. Extensive experiments on a large-scale gastrectomy\nsurgery dataset and the public Cholec80 benchmark prove that our STAR-Net\nsignificantly outperforms state-of-the-arts of surgical phase recognition.",
            "author": [
                "Zhen Chen",
                "Yuhao Zhai",
                "Jun Zhang",
                "Jinqiao Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12603v2",
                "http://arxiv.org/pdf/2311.12603v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12589v2",
            "title": "Improving Source-Free Target Adaptation with Vision Transformers\n  Leveraging Domain Representation Images",
            "updated": "2023-12-02T11:01:26Z",
            "published": "2023-11-21T13:26:13Z",
            "summary": "Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer\nfrom a labeled source domain to an unlabeled target domain, navigating the\nobstacle of domain shift. While Convolutional Neural Networks (CNNs) are a\nstaple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for\ndomain generalization. This paper presents an innovative method to bolster ViT\nperformance in source-free target adaptation, beginning with an evaluation of\nhow key, query, and value elements affect ViT outcomes. Experiments indicate\nthat altering the key component has negligible effects on Transformer\nperformance. Leveraging this discovery, we introduce Domain Representation\nImages (DRIs), feeding embeddings through the key element. DRIs act as\ndomain-specific markers, effortlessly merging with the training regimen. To\nassess our method, we perform target adaptation tests on the Cross Instance DRI\nsource-only (SO) control. We measure the efficacy of target adaptation with and\nwithout DRIs, against existing benchmarks like SHOT-B* and adaptations via\nCDTrans. Findings demonstrate that excluding DRIs offers limited gains over\nSHOT-B*, while their inclusion in the key segment boosts average precision\npromoting superior domain generalization. This research underscores the vital\nrole of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent\nfor further domain adaptation explorations.",
            "author": [
                "Gauransh Sawhney",
                "Daksh Dave",
                "Adeel Ahmed",
                "Jiechao Gao",
                "Khalid Saleem"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12589v2",
                "http://arxiv.org/pdf/2311.12589v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12905v1",
            "title": "Revisiting the Domain Shift and Sample Uncertainty in Multi-source\n  Active Domain Transfer",
            "updated": "2023-11-21T13:12:21Z",
            "published": "2023-11-21T13:12:21Z",
            "summary": "Active Domain Adaptation (ADA) aims to maximally boost model adaptation in a\nnew target domain by actively selecting a limited number of target data to\nannotate.This setting neglects the more practical scenario where training data\nare collected from multiple sources. This motivates us to target a new and\nchallenging setting of knowledge transfer that extends ADA from a single source\ndomain to multiple source domains, termed Multi-source Active Domain Adaptation\n(MADA). Not surprisingly, we find that most traditional ADA methods cannot work\ndirectly in such a setting, mainly due to the excessive domain gap introduced\nby all the source domains and thus their uncertainty-aware sample selection can\neasily become miscalibrated under the multi-domain shifts. Considering this, we\npropose a Dynamic integrated uncertainty valuation framework(Detective) that\ncomprehensively consider the domain shift between multi-source domains and\ntarget domain to detect the informative target samples. Specifically, the\nleverages a dynamic Domain Adaptation(DA) model that learns how to adapt the\nmodel's parameters to fit the union of multi-source domains. This enables an\napproximate single-source domain modeling by the dynamic model. We then\ncomprehensively measure both domain uncertainty and predictive uncertainty in\nthe target domain to detect informative target samples using evidential deep\nlearning, thereby mitigating uncertainty miscalibration. Furthermore, we\nintroduce a contextual diversity-aware calculator to enhance the diversity of\nthe selected samples. Experiments demonstrate that our solution outperforms\nexisting methods by a considerable margin on three domain adaptation\nbenchmarks.",
            "author": [
                "Wenqiao Zhang",
                "Zheqi Lv",
                "Hao Zhou",
                "Jia-Wei Liu",
                "Juncheng Li",
                "Mengze Li",
                "Siliang Tang",
                "Yueting Zhuang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12905v1",
                "http://arxiv.org/pdf/2311.12905v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12576v1",
            "title": "Phi4tools: Compilation of Feynman diagrams for Landau-Ginzburg-Wilson\n  theories",
            "updated": "2023-11-21T12:41:36Z",
            "published": "2023-11-21T12:41:36Z",
            "summary": "Scalar field theories with quartic interactions are of central interest in\nthe study of second-order phase transitions. For three-dimensional theories,\nnumerous studies make use of the fixed-dimensional perturbative computation of\n[B. Nickel, D. Meiron, and G. Baker Jr, Compilation of 2-pt and 4-pt graphs for\ncontinuous spin model, University of Guelph report (1977)], unfortunately left\nunpublished. We independently verify the results of Nickel et al., and we\nextend the computation to the eighth order in the coupling constant. The\nresults of our calculations, together with the tools developed, are made\navailable in Phi4tools, a user-friendly package that allows displaying the\ninformation about the individual Feynman diagrams, including the numerical\nvalues for the diagrams for zero, two, and four-point functions. We also\nprovide the perturbative series up to order eight for the renormalization-group\nfunctions for the $O(N)$ and cubic anisotropic models.",
            "author": [
                "Giacomo Sberveglieri",
                "Gabriele Spada"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12576v1",
                "http://arxiv.org/pdf/2311.12576v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "cond-mat.stat-mech",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12553v3",
            "title": "HoVer-UNet: Accelerating HoVerNet with UNet-based multi-class nuclei\n  segmentation via knowledge distillation",
            "updated": "2023-12-04T10:41:15Z",
            "published": "2023-11-21T12:05:56Z",
            "summary": "We present HoVer-UNet, an approach to distill the knowledge of the\nmulti-branch HoVerNet framework for nuclei instance segmentation and\nclassification in histopathology. We propose a compact, streamlined single UNet\nnetwork with a Mix Vision Transformer backbone, and equip it with a custom loss\nfunction to optimally encode the distilled knowledge of HoVerNet, reducing\ncomputational requirements without compromising performances. We show that our\nmodel achieved results comparable to HoVerNet on the public PanNuke and Consep\ndatasets with a three-fold reduction in inference time. We make the code of our\nmodel publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.",
            "author": [
                "Cristian Tommasino",
                "Cristiano Russo",
                "Antonio Maria Rinaldi",
                "Francesco Ciompi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12553v3",
                "http://arxiv.org/pdf/2311.12553v3"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12551v1",
            "title": "Distilling particle knowledge for fast reconstruction at high-energy\n  physics experiments",
            "updated": "2023-11-21T12:02:14Z",
            "published": "2023-11-21T12:02:14Z",
            "summary": "Knowledge distillation is a form of model compression that allows to transfer\nknowledge between intelligent algorithms. Its main application is the\ncompactification of large deep neural networks to free up computational\nresources, in particular on edge devices. In this article, we consider\nproton-proton collisions at the High-Luminosity LHC (HL-LHC) and demonstrate a\nsuccessful knowledge transfer from an event-level graph neural network (GNN) to\na particle-level small deep neural network (DNN). Our algorithm, DistillNet, is\na DNN that is trained to learn about the provenance of particles, as provided\nby the soft labels that are the GNN outputs, to predict whether or not a\nparticle originates from the primary interaction vertex. The results indicate\nthat for this problem, which is one of the main challenges at the HL-LHC, there\nis minimal loss during the transfer of knowledge to the small student network,\nwhile improving significantly the computational resource needs compared to the\nteacher. This is demonstrated for the distilled student network on a CPU, as\nwell as for a quantized and pruned student network deployed on an FPGA. Our\nstudy proves that knowledge transfer between networks of different complexity\ncan be used for fast artificial intelligence (AI) in high-energy physics that\nimproves the expressiveness of observables over non-AI-based reconstruction\nalgorithms. Such an approach can become essential at the HL-LHC experiments,\ne.g., to comply with the resource budget of their trigger stages.",
            "author": [
                "Aritra Bal",
                "Tristan Brandes",
                "Fabio Iemmi",
                "Markus Klute",
                "Benedikt Maier",
                "Vinicius Mikuni",
                "Thea Aarrestad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12551v1",
                "http://arxiv.org/pdf/2311.12551v1"
            ],
            "primary_category": "hep-ex",
            "category": [
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12548v1",
            "title": "Multi-Session Budget Optimization for Forward Auction-based Federated\n  Learning",
            "updated": "2023-11-21T11:57:41Z",
            "published": "2023-11-21T11:57:41Z",
            "summary": "Auction-based Federated Learning (AFL) has emerged as an important research\nfield in recent years. The prevailing strategies for FL model users (MUs)\nassume that the entire team of the required data owners (DOs) for an FL task\nmust be assembled before training can commence. In practice, an MU can trigger\nthe FL training process multiple times. DOs can thus be gradually recruited\nover multiple FL model training sessions. Existing bidding strategies for AFL\nMUs are not designed to handle such scenarios. Therefore, the problem of\nmulti-session AFL remains open. To address this problem, we propose the\nMulti-session Budget Optimization Strategy for forward Auction-based Federated\nLearning (MultiBOS-AFL). Based on hierarchical reinforcement learning,\nMultiBOS-AFL jointly optimizes inter-session budget pacing and intra-session\nbidding for AFL MUs, with the objective of maximizing the total utility.\nExtensive experiments on six benchmark datasets show that it significantly\noutperforms seven state-of-the-art approaches. On average, MultiBOS-AFL\nachieves 12.28% higher utility, 14.52% more data acquired through auctions for\na given budget, and 1.23% higher test accuracy achieved by the resulting FL\nmodel compared to the best baseline. To the best of our knowledge, it is the\nfirst budget optimization decision support method with budget pacing capability\ndesigned for MUs in multi-session forward auction-based federated learning",
            "author": [
                "Xiaoli Tang",
                "Han Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12548v1",
                "http://arxiv.org/pdf/2311.12548v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12539v1",
            "title": "GMISeg: General Medical Image Segmentation without Re-Training",
            "updated": "2023-11-21T11:33:15Z",
            "published": "2023-11-21T11:33:15Z",
            "summary": "Although deep learning models have become the main method for medical image\nsegmentation, they often cannot be extended to unknown segmentation tasks\ninvolving new anatomical structures, image shapes, or labels. For new\nsegmentation tasks, researchers often have to retrain or fine-tune the model,\nwhich is time-consuming and poses a significant obstacle to clinical\nresearchers, who often lack the resources and professional knowledge to train\nneural networks. Therefore, we proposed a general method that can solve unknown\nmedical image segmentation tasks without requiring additional training. Given\nan example set of images and prompts for defining new segmentation tasks,\nGMISeg applies a novel low-rank fine-tuning strategy based on the proposed\napproach to the SAM (Segment Anything Model) image encoder, and works with the\nprompt encoder and mask decoder to fine-tune the labeled dataset without the\nneed for additional training. To achieve generalization of new tasks, we used\nmedical image datasets with different imaging modes for different parts. We\ntrained and generalized GMISeg on a different set of anatomical and imaging\nmodes using cardiac images on other site datasets. We have demonstrated that\nGMISeg outperforms the latest methods on unknown tasks and have conducted a\ncomprehensive analysis and summary of the important performance of the proposed\nmethod.",
            "author": [
                "Jing Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12539v1",
                "http://arxiv.org/pdf/2311.12539v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12528v1",
            "title": "Inverse Problems with Learned Forward Operators",
            "updated": "2023-11-21T11:15:14Z",
            "published": "2023-11-21T11:15:14Z",
            "summary": "Solving inverse problems requires knowledge of the forward operator, but\naccurate models can be computationally expensive and hence cheaper variants are\ndesired that do not compromise reconstruction quality. This chapter reviews\nreconstruction methods in inverse problems with learned forward operators that\nfollow two different paradigms. The first one is completely agnostic to the\nforward operator and learns its restriction to the subspace spanned by the\ntraining data. The framework of regularisation by projection is then used to\nfind a reconstruction. The second one uses a simplified model of the physics of\nthe measurement process and only relies on the training data to learn a model\ncorrection. We present the theory of these two approaches and compare them\nnumerically. A common theme emerges: both methods require, or at least benefit\nfrom, training data not only for the forward operator, but also for its\nadjoint.",
            "author": [
                "Simon Arridge",
                "Andreas Hauptmann",
                "Yury Korolev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12528v1",
                "http://arxiv.org/pdf/2311.12528v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.LG",
                "cs.NA",
                "65J22, 47A52, 35R30, 74J25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12495v1",
            "title": "Multi-Objective Reinforcement Learning based on Decomposition: A\n  taxonomy and framework",
            "updated": "2023-11-21T10:11:19Z",
            "published": "2023-11-21T10:11:19Z",
            "summary": "Multi-objective reinforcement learning (MORL) extends traditional RL by\nseeking policies making different compromises among conflicting objectives. The\nrecent surge of interest in MORL has led to diverse studies and solving\nmethods, often drawing from existing knowledge in multi-objective optimization\nbased on decomposition (MOO/D). Yet, a clear categorization based on both RL\nand MOO/D is lacking in the existing literature. Consequently, MORL researchers\nface difficulties when trying to classify contributions within a broader\ncontext due to the absence of a standardized taxonomy. To tackle such an issue,\nthis paper introduces Multi-Objective Reinforcement Learning based on\nDecomposition (MORL/D), a novel methodology bridging RL and MOO literature. A\ncomprehensive taxonomy for MORL/D is presented, providing a structured\nfoundation for categorizing existing and potential MORL works. The introduced\ntaxonomy is then used to scrutinize MORL research, enhancing clarity and\nconciseness through well-defined categorization. Moreover, a flexible framework\nderived from the taxonomy is introduced. This framework accommodates diverse\ninstantiations using tools from both RL and MOO/D. Implementation across\nvarious configurations demonstrates its versatility, assessed against benchmark\nproblems. Results indicate MORL/D instantiations achieve comparable performance\nwith significantly greater versatility than current state-of-the-art\napproaches. By presenting the taxonomy and framework, this paper offers a\ncomprehensive perspective and a unified vocabulary for MORL. This not only\nfacilitates the identification of algorithmic contributions but also lays the\ngroundwork for novel research avenues in MORL, contributing to the continued\nadvancement of this field.",
            "author": [
                "Florian Felten",
                "El-Ghazali Talbi",
                "Gr\u00e9goire Danoy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12495v1",
                "http://arxiv.org/pdf/2311.12495v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12472v1",
            "title": "Self-Supervised Deconfounding Against Spatio-Temporal Shifts: Theory and\n  Modeling",
            "updated": "2023-11-21T09:33:13Z",
            "published": "2023-11-21T09:33:13Z",
            "summary": "As an important application of spatio-temporal (ST) data, ST traffic\nforecasting plays a crucial role in improving urban travel efficiency and\npromoting sustainable development. In practice, the dynamics of traffic data\nfrequently undergo distributional shifts attributed to external factors such as\ntime evolution and spatial differences. This entails forecasting models to\nhandle the out-of-distribution (OOD) issue where test data is distributed\ndifferently from training data. In this work, we first formalize the problem by\nconstructing a causal graph of past traffic data, future traffic data, and\nexternal ST contexts. We reveal that the failure of prior arts in OOD traffic\ndata is due to ST contexts acting as a confounder, i.e., the common cause for\npast data and future ones. Then, we propose a theoretical solution named\nDisentangled Contextual Adjustment (DCA) from a causal lens. It differentiates\ninvariant causal correlations against variant spurious ones and deconfounds the\neffect of ST contexts. On top of that, we devise a Spatio-Temporal\nsElf-superVised dEconfounding (STEVE) framework. It first encodes traffic data\ninto two disentangled representations for associating invariant and variant ST\ncontexts. Then, we use representative ST contexts from three conceptually\ndifferent perspectives (i.e., temporal, spatial, and semantic) as\nself-supervised signals to inject context information into both\nrepresentations. In this way, we improve the generalization ability of the\nlearned context-oriented representations to OOD ST traffic forecasting.\nComprehensive experiments on four large-scale benchmark datasets demonstrate\nthat our STEVE consistently outperforms the state-of-the-art baselines across\nvarious ST OOD scenarios.",
            "author": [
                "Jiahao Ji",
                "Wentao Zhang",
                "Jingyuan Wang",
                "Yue He",
                "Chao Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12472v1",
                "http://arxiv.org/pdf/2311.12472v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12465v1",
            "title": "Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and\n  Embedding",
            "updated": "2023-11-21T09:22:02Z",
            "published": "2023-11-21T09:22:02Z",
            "summary": "One of the significant barriers to the training of statistical models on\nknowledge graphs is the difficulty that scientists have in finding the best\ninput data to address their prediction goal. In addition to this, a key\nchallenge is to determine how to manipulate these relational data, which are\noften in the form of particular triples (i.e., subject, predicate, object), to\nenable the learning process. Currently, many high-quality catalogs of knowledge\ngraphs, are available. However, their primary goal is the re-usability of these\nresources, and their interconnection, in the context of the Semantic Web. This\npaper describes the LiveSchema initiative, namely, a first version of a gateway\nthat has the main scope of leveraging the gold mine of data collected by many\nexisting catalogs collecting relational data like ontologies and knowledge\ngraphs. At the current state, LiveSchema contains - 1000 datasets from 4 main\nsources and offers some key facilities, which allow to: i) evolving LiveSchema,\nby aggregating other source catalogs and repositories as input sources; ii)\nquerying all the collected resources; iii) transforming each given dataset into\nformal concept analysis matrices that enable analysis and visualization\nservices; iv) generating models and tensors from each given dataset.",
            "author": [
                "Mattia Fumagalli",
                "Marco Boffo",
                "Daqian Shi",
                "Mayukh Bagchi",
                "Fausto Giunchiglia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12465v1",
                "http://arxiv.org/pdf/2311.12465v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12450v1",
            "title": "Can we hedge carbon risk? A network embedding approach",
            "updated": "2023-11-21T09:02:25Z",
            "published": "2023-11-21T09:02:25Z",
            "summary": "Sustainable investing refers to the integration of environmental and social\naspects in investors' decisions. We propose a novel methodology based on the\nTriangulated Maximally Filtered Graph and node2vec algorithms to construct an\nhedging portfolio for climate risk, represented by various risk factors, among\nwhich the CO2 and the ESG ones. The CO2 factor is strongly correlated\nconsistently over time with the Utility sector, which is the most carbon\nintensive in the S&P 500 index. Conversely, identifying a group of sectors\nlinked to the ESG factor proves challenging. As a consequence, while it is\npossible to obtain an efficient hedging portfolio strategy with our methodology\nfor the carbon factor, the same cannot be achieved for the ESG one. The ESG\nscores appears to be an indicator too broadly defined for market applications.",
            "author": [
                "Michele Azzone",
                "Maria Chiara Pocelli",
                "Davide Stocco"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12450v1",
                "http://arxiv.org/pdf/2311.12450v1"
            ],
            "primary_category": "q-fin.PM",
            "category": [
                "q-fin.PM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12443v1",
            "title": "Knowledge Base Enabled Semantic Communication: A Generative Perspective",
            "updated": "2023-11-21T08:54:49Z",
            "published": "2023-11-21T08:54:49Z",
            "summary": "Semantic communication is widely touted as a key technology for propelling\nthe sixth-generation (6G) wireless networks. However, providing effective\nsemantic representation is quite challenging in practice. To address this\nissue, this article takes a crack at exploiting semantic knowledge base (KB) to\nusher in a new era of generative semantic communication. Via semantic KB,\nsource messages can be characterized in low-dimensional subspaces without\ncompromising their desired meaning, thus significantly enhancing the\ncommunication efficiency. The fundamental principle of semantic KB is first\nintroduced, and a generative semantic communication architecture is developed\nby presenting three sub-KBs, namely source, task, and channel KBs. Then, the\ndetailed construction approaches for each sub-KB are described, followed by\ntheir utilization in terms of semantic coding and transmission. A case study is\nalso provided to showcase the superiority of generative semantic communication\nover conventional syntactic communication and classical semantic communication.\nIn a nutshell, this article establishes a scientific foundation for the\nexciting uncharted frontier of generative semantic communication.",
            "author": [
                "Jinke Ren",
                "Zezhong Zhang",
                "Jie Xu",
                "Guanying Chen",
                "Yaping Sun",
                "Ping Zhang",
                "Shuguang Cui"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12443v1",
                "http://arxiv.org/pdf/2311.12443v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.AI",
                "cs.NI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12440v1",
            "title": "A Random Walk Approach for Simulation-Based Continuous Dynamic Traffic\n  Assignment",
            "updated": "2023-11-21T08:54:05Z",
            "published": "2023-11-21T08:54:05Z",
            "summary": "This paper presents a new simulation-based approach to address the stochastic\nDynamic Traffic Assignment (DTA) problem, focusing on large congested networks\nand dynamic settings. The proposed methodology incorporates a random walk model\ninspired by the theoretical concept of the \\textit{equivalent impedance}\nmethod, specifically designed to overcome the limitations of traditional\nMultinomial Logit (MNL) models in handling overlapping routes and scaling\nissues. By iteratively contracting non-overlapping subnetworks into virtual\nlinks and computing equivalent virtual travel costs, the route choice\ndecision-making process is shifted to intersections, enabling a more accurate\nrepresentation of travelers' choices as traffic conditions evolve and allowing\nmore accurate performance under fine-grained temporal segmentation.\n  The approach leverages Directed Acyclic Graphs (DAGs) structure to\nefficiently find all routes between two nodes, thus obviating the need for\nroute enumeration, which is intractable in general networks. While with the\ncalculation approach of downstream node choice probabilities, all available\nroutes in the network can be selected with non-zero probability.\n  To evaluate the effectiveness of the proposed method, experiments are\nconducted on two synthetic networks under congested demand scenarios using\nSimulation of Urban MObility (SUMO), an open-source microscopic traffic\nsimulation software. The results demonstrate the method's robustness, faster\nconvergence, and realistic trip distribution compared to traditional route\nassignment methods, making it an ideal proposal for real-time or\nresource-intensive applications such as microscopic demand calibration.",
            "author": [
                "Kaveh Khoshkhah",
                "Mozhgan Pourmoradnasseri",
                "Sadok Ben Yahia",
                "Amnir Hadachi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12440v1",
                "http://arxiv.org/pdf/2311.12440v1"
            ],
            "primary_category": "cs.MA",
            "category": [
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12417v2",
            "title": "Eigenvalues and spanning trees with constrained degree",
            "updated": "2023-11-23T11:58:48Z",
            "published": "2023-11-21T08:09:59Z",
            "summary": "In this paper, we study some spanning trees with bounded degree and leaf\ndegree from eigenvalues. For any integer $k\\geq2$, a $k$-tree is a spanning\ntree in which every vertex has degree no more than $k$. Let $T$ be a spanning\ntree of a connected graph. The leaf degree of $T$ is the maximum number of\nend-vertices attached to $v$ in $T$ for any $v\\in V(T)$. By referring to the\ntechnique shown in [Eigenvalues and $[a,b]$-factors in regular graphs, J. Graph\nTheory. 100 (2022) 458-469], for a $r$-regular graph $G$, we provide an upper\nbound for the fourth largest eigenvalue of $G$ to guarantee the existence of a\n$k$-tree. Moreover, for a $t$-connected graph, we prove a tight sufficient\ncondition for the existence of a spanning tree with leaf degree at most $k$ in\nterms of spectral radius. This generalizes a result of Theorem 1.5 in [Spectral\nradius and spanning trees of graphs, Discrete Math. 346 (2023) 113400].\nFinally, for a general graph $G$, we present two sufficient conditions for the\nexistence of a spanning tree with leaf degree at most $k$ via the Laplacian\neigenvalues of $G$ and the spectral radius of the complement of $G$,\nrespectively.",
            "author": [
                "Chang Liu",
                "Hao Li",
                "Jianping Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12417v2",
                "http://arxiv.org/pdf/2311.12417v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 05C70"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12410v1",
            "title": "nach0: Multimodal Natural and Chemical Languages Foundation Model",
            "updated": "2023-11-21T07:56:30Z",
            "published": "2023-11-21T07:56:30Z",
            "summary": "Large Language Models (LLMs) have substantially driven scientific progress in\nvarious domains, and many papers have demonstrated their ability to tackle\ncomplex problems with creative solutions. Our paper introduces a new foundation\nmodel, nach0, capable of solving various chemical and biological tasks:\nbiomedical question answering, named entity recognition, molecular generation,\nmolecular synthesis, attributes prediction, and others. nach0 is a multi-domain\nand multi-task encoder-decoder LLM pre-trained on unlabeled text from\nscientific literature, patents, and molecule strings to incorporate a range of\nchemical and linguistic knowledge. We employed instruction tuning, where\nspecific task-related instructions are utilized to fine-tune nach0 for the\nfinal set of tasks. To train nach0 effectively, we leverage the NeMo framework,\nenabling efficient parallel optimization of both base and large model versions.\nExtensive experiments demonstrate that our model outperforms state-of-the-art\nbaselines on single-domain and cross-domain tasks. Furthermore, it can generate\nhigh-quality outputs in molecular and textual formats, showcasing its\neffectiveness in multi-domain setups.",
            "author": [
                "Micha Livne",
                "Zulfat Miftahutdinov",
                "Elena Tutubalina",
                "Maksim Kuznetsov",
                "Daniil Polykovskiy",
                "Annika Brundyn",
                "Aastha Jhunjhunwala",
                "Anthony Costa",
                "Alex Aliper",
                "Alex Zhavoronkov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12410v1",
                "http://arxiv.org/pdf/2311.12410v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12402v1",
            "title": "Examples of cubulable groups with fixed-point properties",
            "updated": "2023-11-21T07:31:34Z",
            "published": "2023-11-21T07:31:34Z",
            "summary": "For every $n \\geq 1$, let $(\\mathrm{FW}_n)$ denote the fixed-point property\nfor median graphs of cubical dimension $n$ (or equivalently, for CAT(0) cube\ncomplexes of dimension $n$). In this article, we construct groups satisfying\n$(\\mathrm{FW}_n)$ but not $(\\mathrm{FW}_{n+1})$. Our examples are virtually\nfree abelian and virtually graph products of finite groups. As an application\nof our constructions, we obtain, for every $n \\geq 1$, a group satisfying\n$(\\mathrm{FW}_n)$ but acting properly and cocompactly on a median graph of\ncubical dimension $n+1$. Several conjectures and open questions are included.",
            "author": [
                "Anthony Genevois"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12402v1",
                "http://arxiv.org/pdf/2311.12402v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.MG",
                "20F65, 20F67, 22D55"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12399v2",
            "title": "A Survey of Graph Meets Large Language Model: Progress and Future\n  Directions",
            "updated": "2023-11-28T12:32:05Z",
            "published": "2023-11-21T07:22:48Z",
            "summary": "Graph plays a significant role in representing and analyzing complex\nrelationships in real-world applications such as citation networks, social\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\nhave achieved tremendous success in various domains, have also been leveraged\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\nbased methods and yield state-of-the-art performance. In this survey, we first\npresent a comprehensive review and analysis of existing methods that integrate\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\nexisting methods into three categories based on the role (i.e., enhancer,\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\nwe systematically survey the representative methods along the three categories\nof the taxonomy. Finally, we discuss the remaining limitations of existing\nstudies and highlight promising avenues for future research. The relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.",
            "author": [
                "Yuhan Li",
                "Zhixun Li",
                "Peisong Wang",
                "Jia Li",
                "Xiangguo Sun",
                "Hong Cheng",
                "Jeffrey Xu Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12399v2",
                "http://arxiv.org/pdf/2311.12399v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12395v1",
            "title": "Problems of Non-equivalent Words in Technical Translation",
            "updated": "2023-11-21T07:11:39Z",
            "published": "2023-11-21T07:11:39Z",
            "summary": "Translating words which do not have equivalent in target language is not easy\nand finding proper equivalent of those words are very important to render\ncorrectly and understandably, the article defines some thoughts and ideas of\nscientists on the common problems of non-equivalent words from English to\nRussian language and includes English and Russian examples and ideas of certain\nscientist. The English language is worldwide spoken and there are 1.35 billion\nEnglish speakers and over 258 million Russian speakers according to the 2021s\nstatistics. Inevitably, these billions of speakers around the world have\nconnection and they may have deal in different criteria. In order to understand\none another they need to have a pure and fully-understood language. These pure\nlanguages understanding directly relates to translation knowledge where\nlinguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. Hence, this research paper includes different ways and rules of\nrendering non-equivalent words from source language to the target language.",
            "author": [
                "Mohammad Ibrahim Qani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12395v1",
                "http://arxiv.org/pdf/2311.12395v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12389v1",
            "title": "Linear-time online visibility graph transformation algorithm: for both\n  natural and horizontal visibility criteria",
            "updated": "2023-11-21T07:01:05Z",
            "published": "2023-11-21T07:01:05Z",
            "summary": "Visibility graph (VG) transformation is a technique used to convert a time\nseries into a graph based on specific visibility criteria. It has attracted\nincreasing interest in the fields of time series analysis, forecasting, and\nclassification. Optimizing the VG transformation algorithm to accelerate the\nprocess is a critical aspect of VG-related research, as it enhances the\napplicability of VG transformation in latency-sensitive areas and conserves\ncomputational resources. In the real world, many time series are presented in\nthe form of data streams. Despite the proposal of the concept of VG's online\nfunctionality, previous studies have not thoroughly explored the acceleration\nof VG transformation by leveraging the characteristics of data streams. In this\npaper, we propose that an efficient online VG algorithm should adhere to two\ncriteria and develop a linear-time method, termed the LOT framework, for both\nnatural and horizontal visibility graph transformations in data stream\nscenarios. Experiments are conducted on two datasets, comparing our approach\nwith five existing methods as baselines. The results demonstrate the validity\nand promising computational efficiency of our framework.",
            "author": [
                "Yusheng Huang",
                "Yong Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12389v1",
                "http://arxiv.org/pdf/2311.12389v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12386v2",
            "title": "Point, Segment and Count: A Generalized Framework for Object Counting",
            "updated": "2023-11-27T05:58:45Z",
            "published": "2023-11-21T06:55:21Z",
            "summary": "Class-agnostic object counting aims to count all objects in an image with\nrespect to example boxes or class names, \\emph{a.k.a} few-shot and zero-shot\ncounting. Current state-of-the-art methods highly rely on density maps to\npredict object counts, which lacks model interpretability. In this paper, we\npropose a generalized framework for both few-shot and zero-shot object counting\nbased on detection. Our framework combines the superior advantages of two\nfoundation models without compromising their zero-shot capability: (\\textbf{i})\nSAM to segment all possible objects as mask proposals, and (\\textbf{ii}) CLIP\nto classify proposals to obtain accurate object counts. However, this strategy\nmeets the obstacles of efficiency overhead and the small crowded objects that\ncannot be localized and distinguished. To address these issues, our framework,\ntermed PseCo, follows three steps: point, segment, and count. Specifically, we\nfirst propose a class-agnostic object localization to provide accurate but\nleast point prompts for SAM, which consequently not only reduces computation\ncosts but also avoids missing small objects. Furthermore, we propose a\ngeneralized object classification that leverages CLIP image/text embeddings as\nthe classifier, following a hierarchical knowledge distillation to obtain\ndiscriminative classifications among hierarchical mask proposals. Extensive\nexperimental results on FSC-147 dataset demonstrate that PseCo achieves\nstate-of-the-art performance in both few-shot/zero-shot object\ncounting/detection, with additional results on large-scale COCO and LVIS\ndatasets. The source code is available at\n\\url{https://github.com/Hzzone/PseCo}.",
            "author": [
                "Zhizhong Huang",
                "Mingliang Dai",
                "Yi Zhang",
                "Junping Zhang",
                "Hongming Shan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12386v2",
                "http://arxiv.org/pdf/2311.12386v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12889v1",
            "title": "Enhancing Scene Graph Generation with Hierarchical Relationships and\n  Commonsense Knowledge",
            "updated": "2023-11-21T06:03:20Z",
            "published": "2023-11-21T06:03:20Z",
            "summary": "This work presents an enhanced approach to generating scene graphs by\nincorporating a relationship hierarchy and commonsense knowledge. Specifically,\nwe propose a Bayesian classification head that exploits an informative\nhierarchical structure. It jointly predicts the super-category or type of\nrelationship between the two objects, along with the detailed relationship\nunder each super-category. We design a commonsense validation pipeline that\nuses a large language model to critique the results from the scene graph\nprediction system and then use that feedback to enhance the model performance.\nThe system requires no external large language model assistance at test time,\nmaking it more convenient for practical applications. Experiments on the Visual\nGenome and the OpenImage V6 datasets demonstrate that harnessing hierarchical\nrelationships enhances the model performance by a large margin. The proposed\nBayesian head can also be incorporated as a portable module in existing scene\ngraph generation algorithms to improve their results. In addition, the\ncommonsense validation enables the model to generate an extensive set of\nreasonable predictions beyond dataset annotations.",
            "author": [
                "Bowen Jiang",
                "Zhijun Zhuang",
                "Camillo Jose Taylor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12889v1",
                "http://arxiv.org/pdf/2311.12889v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12351v1",
            "title": "Advancing Transformer Architecture in Long-Context Large Language\n  Models: A Comprehensive Survey",
            "updated": "2023-11-21T04:59:17Z",
            "published": "2023-11-21T04:59:17Z",
            "summary": "With the bomb ignited by ChatGPT, Transformer-based Large Language Models\n(LLMs) have paved a revolutionary path toward Artificial General Intelligence\n(AGI) and have been applied in diverse areas as knowledge bases, human\ninterfaces, and dynamic agents. However, a prevailing limitation exists: many\ncurrent LLMs, constrained by resources, are primarily pre-trained on shorter\ntexts, rendering them less effective for longer-context prompts, commonly\nencountered in real-world settings. In this paper, we present a comprehensive\nsurvey focusing on the advancement of model architecture in Transformer-based\nLLMs to optimize long-context capabilities across all stages from pre-training\nto inference. We firstly delineate and analyze the problems of handling\nlong-context input and output with the current Transformer-based models. Then,\nwe mainly offer a holistic taxonomy to navigate the landscape of Transformer\nupgrades on architecture to solve these problems. Afterward, we provide the\ninvestigation on wildly used evaluation necessities tailored for long-context\nLLMs, including datasets, metrics, and baseline models, as well as some amazing\noptimization toolkits like libraries, systems, and compilers to augment LLMs'\nefficiency and efficacy across different stages. Finally, we further discuss\nthe predominant challenges and potential avenues for future research in this\ndomain. Additionally, we have established a repository where we curate relevant\nliterature with real-time updates at\nhttps://github.com/Strivin0311/long-llms-learning.",
            "author": [
                "Yunpeng Huang",
                "Jingwei Xu",
                "Zixu Jiang",
                "Junyu Lai",
                "Zenan Li",
                "Yuan Yao",
                "Taolue Chen",
                "Lijuan Yang",
                "Zhou Xin",
                "Xiaoxing Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12351v1",
                "http://arxiv.org/pdf/2311.12351v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12335v1",
            "title": "Toughness and distance spectral radius in graphs involving minimum\n  degree",
            "updated": "2023-11-21T03:57:39Z",
            "published": "2023-11-21T03:57:39Z",
            "summary": "The toughness $\\tau(G)=\\mathrm{min}\\{\\frac{|S|}{c(G-S)}: S~\\mbox{is a cut set\nof vertices in}~G\\}$ for $G\\ncong K_n.$ The concept of toughness initially\nproposed by Chv$\\mathrm{\\acute{a}}$tal in 1973, which serves as a simple way to\nmeasure how tightly various pieces of a graph hold together. A graph $G$ is\ncalled $t$-tough if $\\tau(G)\\geq t.$ It is very interesting to investigate the\nrelations between toughness and eigenvalues of graphs. Fan, Lin and Lu\n[European J. Combin. 110 (2023) 103701] provided sufficient conditions in terms\nof the spectral radius for a graph to be 1-tough with minimum degree $\\delta$\nand $t$-tough with $t\\geq 1$ being an integer, respectively. By using some\ntypical distance spectral techniques and structural analysis, we in this paper\npresent sufficient conditions based on the distance spectral radius to\nguarantee a graph to be 1-tough with minimum degree $\\delta.$ Moreover, we also\nprove sufficient conditions with respect to the distance spectral radius for a\ngraph to be $t$-tough, where $t$ or $\\frac{1}{t}$ is a positive integer.",
            "author": [
                "Jing Lou",
                "Ruifang Liu",
                "Jinlong Shu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12335v1",
                "http://arxiv.org/pdf/2311.12335v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12333v1",
            "title": "QuGeo: An End-to-end Quantum Learning Framework for Geoscience -- A Case\n  Study on Full-Waveform Inversion",
            "updated": "2023-11-21T03:49:09Z",
            "published": "2023-11-21T03:49:09Z",
            "summary": "The rapid advancement of quantum computing has generated considerable\nanticipation for its transformative potential. However, harnessing its full\npotential relies on identifying \"killer applications\". In this regard, QuGeo\nemerges as a groundbreaking quantum learning framework, poised to become a key\napplication in geoscience, particularly for Full-Waveform Inversion (FWI). This\nframework integrates variational quantum circuits with geoscience, representing\na novel fusion of quantum computing and geophysical analysis. This synergy\nunlocks quantum computing's potential within geoscience. It addresses the\ncritical need for physics-guided data scaling, ensuring high-performance\ngeoscientific analyses aligned with core physical principles. Furthermore,\nQuGeo's introduction of a quantum circuit custom-designed for FWI highlights\nthe critical importance of application-specific circuit design for quantum\ncomputing. In the OpenFWI's FlatVelA dataset experiments, the variational\nquantum circuit from QuGeo, with only 576 parameters, achieved significant\nimprovement in performance. It reached a Structural Similarity Image Metric\n(SSIM) score of 0.905 between the ground truth and the output velocity map.\nThis is a notable enhancement from the baseline design's SSIM score of 0.800,\nwhich was achieved without the incorporation of physics knowledge.",
            "author": [
                "Weiwen Jiang",
                "Youzuo Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12333v1",
                "http://arxiv.org/pdf/2311.12333v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12329v1",
            "title": "Graph Neural Ordinary Differential Equations-based method for\n  Collaborative Filtering",
            "updated": "2023-11-21T03:42:15Z",
            "published": "2023-11-21T03:42:15Z",
            "summary": "Graph Convolution Networks (GCNs) are widely considered state-of-the-art for\ncollaborative filtering. Although several GCN-based methods have been proposed\nand achieved state-of-the-art performance in various tasks, they can be\ncomputationally expensive and time-consuming to train if too many layers are\ncreated. However, since the linear GCN model can be interpreted as a\ndifferential equation, it is possible to transfer it to an ODE problem. This\ninspired us to address the computational limitations of GCN-based models by\ndesigning a simple and efficient NODE-based model that can skip some GCN layers\nto reach the final state, thus avoiding the need to create many layers. In this\nwork, we propose a Graph Neural Ordinary Differential Equation-based method for\nCollaborative Filtering (GODE-CF). This method estimates the final embedding by\nutilizing the information captured by one or two GCN layers. To validate our\napproach, we conducted experiments on multiple datasets. The results\ndemonstrate that our model outperforms competitive baselines, including\nGCN-based models and other state-of-the-art CF methods. Notably, our proposed\nGODE-CF model has several advantages over traditional GCN-based models. It is\nsimple, efficient, and has a fast training time, making it a practical choice\nfor real-world situations.",
            "author": [
                "Ke Xu",
                "Yuanjie Zhu",
                "Weizhi Zhang",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12329v1",
                "http://arxiv.org/pdf/2311.12329v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12327v1",
            "title": "ViLaM: A Vision-Language Model with Enhanced Visual Grounding and\n  Generalization Capability",
            "updated": "2023-11-21T03:40:09Z",
            "published": "2023-11-21T03:40:09Z",
            "summary": "Vision-language models have revolutionized human-computer interaction and\nshown significant progress in multi-modal tasks. However, applying these models\nto complex visual tasks like medical image analysis remains challenging. In\nthis study, we propose ViLaM, a unified Vision-Language transformer model that\nintegrates instruction tuning predicated on a large language model. This\napproach enables us to optimally utilize the knowledge and reasoning capacities\nof large pre-trained language models for an array of tasks encompassing both\nlanguage and vision. We employ frozen pre-trained encoders to encode and align\nboth image and text features, enabling ViLaM to handle a variety of visual\ntasks following textual instructions. Besides, we've designed cycle training\nfor referring expressions to address the need for high-quality, paired\nreferring expression datasets for training large models in terms of both\nquantity and quality. We evaluated ViLaM's exceptional performance on public\ngeneral datasets and further confirmed its generalizability on medical\ndatasets. Importantly, we've observed the model's impressive zero-shot learning\nability, indicating the potential future application of ViLaM in the medical\nfield.",
            "author": [
                "Xiaoyu Yang",
                "Lijian Xu",
                "Hongsheng Li",
                "Shaoting Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12327v1",
                "http://arxiv.org/pdf/2311.12327v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12315v1",
            "title": "AcademicGPT: Empowering Academic Research",
            "updated": "2023-11-21T03:17:14Z",
            "published": "2023-11-21T03:17:14Z",
            "summary": "Large Language Models (LLMs) have demonstrated exceptional capabilities\nacross various natural language processing tasks. Yet, many of these advanced\nLLMs are tailored for broad, general-purpose applications. In this technical\nreport, we introduce AcademicGPT, designed specifically to empower academic\nresearch. AcademicGPT is a continual training model derived from LLaMA2-70B.\nOur training corpus mainly consists of academic papers, thesis, content from\nsome academic domain, high-quality Chinese data and others. While it may not be\nextensive in data scale, AcademicGPT marks our initial venture into a\ndomain-specific GPT tailored for research area. We evaluate AcademicGPT on\nseveral established public benchmarks such as MMLU and CEval, as well as on\nsome specialized academic benchmarks like PubMedQA, SCIEval, and our\nnewly-created ComputerScienceQA, to demonstrate its ability from general\nknowledge ability, to Chinese ability, and to academic ability. Building upon\nAcademicGPT's foundation model, we also developed several applications catered\nto the academic area, including General Academic Question Answering,\nAI-assisted Paper Reading, Paper Review, and AI-assisted Title and Abstract\nGeneration.",
            "author": [
                "Shufa Wei",
                "Xiaolong Xu",
                "Xianbiao Qi",
                "Xi Yin",
                "Jun Xia",
                "Jingyi Ren",
                "Peijun Tang",
                "Yuxiang Zhong",
                "Yihao Chen",
                "Xiaoqin Ren",
                "Yuxin Liang",
                "Liankai Huang",
                "Kai Xie",
                "Weikang Gui",
                "Wei Tan",
                "Shuanglong Sun",
                "Yongquan Hu",
                "Qinxian Liu",
                "Nanjin Li",
                "Chihao Dai",
                "Lihua Wang",
                "Xiaohui Liu",
                "Lei Zhang",
                "Yutao Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12315v1",
                "http://arxiv.org/pdf/2311.12315v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12314v1",
            "title": "Demystifying Graph Sparsification Algorithms in Graph Properties\n  Preservation",
            "updated": "2023-11-21T03:14:07Z",
            "published": "2023-11-21T03:14:07Z",
            "summary": "Graph sparsification is a technique that approximates a given graph by a\nsparse graph with a subset of vertices and/or edges. The goal of an effective\nsparsification algorithm is to maintain specific graph properties relevant to\nthe downstream task while minimizing the graph's size. Graph algorithms often\nsuffer from long execution time due to the irregularity and the large\nreal-world graph size. Graph sparsification can be applied to greatly reduce\nthe run time of graph algorithms by substituting the full graph with a much\nsmaller sparsified graph, without significantly degrading the output quality.\nHowever, the interaction between numerous sparsifiers and graph properties is\nnot widely explored, and the potential of graph sparsification is not fully\nunderstood.\n  In this work, we cover 16 widely-used graph metrics, 12 representative graph\nsparsification algorithms, and 14 real-world input graphs spanning various\ncategories, exhibiting diverse characteristics, sizes, and densities. We\ndeveloped a framework to extensively assess the performance of these\nsparsification algorithms against graph metrics, and provide insights to the\nresults. Our study shows that there is no one sparsifier that performs the best\nin preserving all graph properties, e.g. sparsifiers that preserve\ndistance-related graph properties (eccentricity) struggle to perform well on\nGraph Neural Networks (GNN). This paper presents a comprehensive experimental\nstudy evaluating the performance of sparsification algorithms in preserving\nessential graph metrics. The insights inform future research in incorporating\nmatching graph sparsification to graph algorithms to maximize benefits while\nminimizing quality degradation. Furthermore, we provide a framework to\nfacilitate the future evaluation of evolving sparsification algorithms, graph\nmetrics, and ever-growing graph data.",
            "author": [
                "Yuhan Chen",
                "Haojie Ye",
                "Sanketh Vedula",
                "Alex Bronstein",
                "Ronald Dreslinski",
                "Trevor Mudge",
                "Nishil Talati"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12314v1",
                "http://arxiv.org/pdf/2311.12314v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12309v1",
            "title": "Power grid operational risk assessment using graph neural network\n  surrogates",
            "updated": "2023-11-21T03:02:30Z",
            "published": "2023-11-21T03:02:30Z",
            "summary": "We investigate the utility of graph neural networks (GNNs) as proxies of\npower grid operational decision-making algorithms (optimal power flow (OPF) and\nsecurity-constrained unit commitment (SCUC)) to enable rigorous quantification\nof the operational risk. To conduct principled risk analysis, numerous Monte\nCarlo (MC) samples are drawn from the (foretasted) probability distributions of\nspatio-temporally correlated stochastic grid variables. The corresponding OPF\nand SCUC solutions, which are needed to quantify the risk, are generated using\ntraditional OPF and SCUC solvers to generate data for training GNN model(s).\nThe GNN model performance is evaluated in terms of the accuracy of predicting\nquantities of interests (QoIs) derived from the decision variables in OPF and\nSCUC. Specifically, we focus on thermal power generation and load shedding at\nsystem and individual zone level. We also perform reliability and risk\nquantification based on GNN predictions and compare with that obtained from\nOPF/SCUC solutions. Our results demonstrate that GNNs are capable of providing\nfast and accurate prediction of QoIs and thus can be good surrogate models for\nOPF and SCUC. The excellent accuracy of GNN-based reliability and risk\nassessment further suggests that GNN surrogate has the potential to be applied\nin real-time and hours-ahead risk quantification.",
            "author": [
                "Yadong Zhang",
                "Pranav M Karve",
                "Sankaran Mahadevan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12309v1",
                "http://arxiv.org/pdf/2311.12309v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12307v1",
            "title": "Causality is all you need",
            "updated": "2023-11-21T02:53:40Z",
            "published": "2023-11-21T02:53:40Z",
            "summary": "In the fundamental statistics course, students are taught to remember the\nwell-known saying: \"Correlation is not Causation\". Till now, statistics (i.e.,\ncorrelation) have developed various successful frameworks, such as Transformer\nand Pre-training large-scale models, which have stacked multiple parallel\nself-attention blocks to imitate a wide range of tasks. However, in the\ncausation community, how to build an integrated causal framework still remains\nan untouched domain despite its excellent intervention capabilities. In this\npaper, we propose the Causal Graph Routing (CGR) framework, an integrated\ncausal scheme relying entirely on the intervention mechanisms to reveal the\ncause-effect forces hidden in data. Specifically, CGR is composed of a stack of\ncausal layers. Each layer includes a set of parallel deconfounding blocks from\ndifferent causal graphs. We combine these blocks via the concept of the\nproposed sufficient cause, which allows the model to dynamically select the\nsuitable deconfounding methods in each layer. CGR is implemented as the stacked\nnetworks, integrating no confounder, back-door adjustment, front-door\nadjustment, and probability of sufficient cause. We evaluate this framework on\ntwo classical tasks of CV and NLP. Experiments show CGR can surpass the current\nstate-of-the-art methods on both Visual Question Answer and Long Document\nClassification tasks. In particular, CGR has great potential in building the\n\"causal\" pre-training large-scale model that effectively generalizes to diverse\ntasks. It will improve the machines' comprehension of causal relationships\nwithin a broader semantic space.",
            "author": [
                "Ning Xu",
                "Yifei Gao",
                "Hongshuo Tian",
                "Yongdong Zhang",
                "An-An Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12307v1",
                "http://arxiv.org/pdf/2311.12307v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12302v1",
            "title": "Short rainbow cycles in edge-colored graphs",
            "updated": "2023-11-21T02:43:30Z",
            "published": "2023-11-21T02:43:30Z",
            "summary": "A famous conjecture of Caccetta and H\\\"{a}ggkvist (CHC) states that a\ndirected graph $D$ with $n$ vertices and minimum outdegree at least $r$ has a\ndirected cycle of length at most $\\lceil \\frac{n}{r}\\rceil$. In 2017, Aharoni\nproposed the following generalization: an edge-colored graph $G$ with $n$\nvertices, $n$ color classes of size at least $r$ has a rainbow cycle of length\nat most $\\lceil \\frac{n}{r}\\rceil$. Since CHC can be seen as the case of\nAharoni's Conjecture: color classes in the color partition are monochromatic\nstars centered at distinct vertices, one way to study Aharoni's Conjecture is\nto structure the color classes as each color class is either a star, a triangle\nor contains a matching of size 2. Guo improved the upper bound in Aharoni's\nConjecture to $O(\\log n)$ in some mixed cases when the color classes are not\nnecessarily stars. In this paper, we extend Guo's results. Our main result is\nas follows: Let $G$ an edge-colored graph on $n$ vertices and $n$ color\nclasses, if at least $\\alpha n$ color classes are either a matching of size 2\nor a triangle for $\\alpha >\\frac{1}{2}$, then $G$ contains a rainbow cycle of\nlength $O(\\log n)$. We also prove that the $\\log n$ bound is the right order of\nmagnitude.",
            "author": [
                "Xiaozheng Chen",
                "Shanshan Guo",
                "Fei Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12302v1",
                "http://arxiv.org/pdf/2311.12302v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12300v1",
            "title": "Challenges in Video-Based Infant Action Recognition: A Critical\n  Examination of the State of the Art",
            "updated": "2023-11-21T02:36:47Z",
            "published": "2023-11-21T02:36:47Z",
            "summary": "Automated human action recognition, a burgeoning field within computer\nvision, boasts diverse applications spanning surveillance, security,\nhuman-computer interaction, tele-health, and sports analysis. Precise action\nrecognition in infants serves a multitude of pivotal purposes, encompassing\nsafety monitoring, developmental milestone tracking, early intervention for\ndevelopmental delays, fostering parent-infant bonds, advancing computer-aided\ndiagnostics, and contributing to the scientific comprehension of child\ndevelopment. This paper delves into the intricacies of infant action\nrecognition, a domain that has remained relatively uncharted despite the\naccomplishments in adult action recognition. In this study, we introduce a\ngroundbreaking dataset called ``InfActPrimitive'', encompassing five\nsignificant infant milestone action categories, and we incorporate specialized\npreprocessing for infant data. We conducted an extensive comparative analysis\nemploying cutting-edge skeleton-based action recognition models using this\ndataset. Our findings reveal that, although the PoseC3D model achieves the\nhighest accuracy at approximately 71%, the remaining models struggle to\naccurately capture the dynamics of infant actions. This highlights a\nsubstantial knowledge gap between infant and adult action recognition domains\nand the urgent need for data-efficient pipeline models.",
            "author": [
                "Elaheh Hatamimajoumerd",
                "Pooria Daneshvar Kakhaki",
                "Xiaofei Huang",
                "Lingfei Luan",
                "Somaieh Amraee",
                "Sarah Ostadabbas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12300v1",
                "http://arxiv.org/pdf/2311.12300v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12290v1",
            "title": "A Supervised Contrastive Learning Pretrain-Finetune Approach for Time\n  Series",
            "updated": "2023-11-21T02:06:52Z",
            "published": "2023-11-21T02:06:52Z",
            "summary": "Foundation models have recently gained attention within the field of machine\nlearning thanks to its efficiency in broad data processing. While researchers\nhad attempted to extend this success to time series models, the main challenge\nis effectively extracting representations and transferring knowledge from\npretraining datasets to the target finetuning dataset. To tackle this issue, we\nintroduce a novel pretraining procedure that leverages supervised contrastive\nlearning to distinguish features within each pretraining dataset. This\npretraining phase enables a probabilistic similarity metric, which assesses the\nlikelihood of a univariate sample being closely related to one of the\npretraining datasets. Subsequently, using this similarity metric as a guide, we\npropose a fine-tuning procedure designed to enhance the accurate prediction of\nthe target data by aligning it more closely with the learned dynamics of the\npretraining datasets. Our experiments have shown promising results which\ndemonstrate the efficacy of our approach.",
            "author": [
                "Trang H. Tran",
                "Lam M. Nguyen",
                "Kyongmin Yeo",
                "Nam Nguyen",
                "Roman Vaculin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12290v1",
                "http://arxiv.org/pdf/2311.12290v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12289v1",
            "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for\n  Interdisciplinary Science",
            "updated": "2023-11-21T02:02:46Z",
            "published": "2023-11-21T02:02:46Z",
            "summary": "Large language models record impressive performance on many natural language\nprocessing tasks. However, their knowledge capacity is limited to the\npretraining corpus. Retrieval augmentation offers an effective solution by\nretrieving context from external knowledge sources to complement the language\nmodel. However, existing retrieval augmentation techniques ignore the\nstructural relationships between these documents. Furthermore, retrieval models\nare not explored much in scientific tasks, especially in regard to the\nfaithfulness of retrieved documents. In this paper, we propose a novel\nstructure-aware retrieval augmented language model that accommodates document\nstructure during retrieval augmentation. We create a heterogeneous document\ngraph capturing multiple types of relationships (e.g., citation, co-authorship,\netc.) that connect documents from more than 15 scientific disciplines (e.g.,\nPhysics, Medicine, Chemistry, etc.). We train a graph neural network on the\ncurated document graph to act as a structural encoder for the corresponding\npassages retrieved during the model pretraining. Particularly, along with text\nembeddings of the retrieved passages, we obtain structural embeddings of the\ndocuments (passages) and fuse them together before feeding them to the language\nmodel. We evaluate our model extensively on various scientific benchmarks that\ninclude science question-answering and scientific document classification\ntasks. Experimental results demonstrate that structure-aware retrieval improves\nretrieving more coherent, faithful and contextually relevant passages, while\nshowing a comparable performance in the overall accuracy.",
            "author": [
                "Sai Munikoti",
                "Anurag Acharya",
                "Sridevi Wagle",
                "Sameera Horawalavithana"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12289v1",
                "http://arxiv.org/pdf/2311.12289v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12281v2",
            "title": "GPUSCAN$^{++}$:Efficient Structural Graph Clustering on GPUs",
            "updated": "2023-11-30T01:14:27Z",
            "published": "2023-11-21T01:51:38Z",
            "summary": "Structural clustering is one of the most popular graph clustering methods,\nwhich has achieved great performance improvement by utilizing GPUs. Even\nthough, the state-of-the-art GPU-based structural clustering algorithm,\nGPUSCAN, still suffers from efficiency issues since lots of extra costs are\nintroduced for parallelization. Moreover, GPUSCAN assumes that the graph is\nresident in the GPU memory. However, the GPU memory capacity is limited\ncurrently while many real-world graphs are big and cannot fit in the GPU\nmemory, which makes GPUSCAN unable to handle large graphs. Motivated by this,\nwe present a new GPU-based structural clustering algorithm, GPUSCAN++, in this\npaper. To address the efficiency issue, we propose a new progressive clustering\nmethod tailored for GPUs that not only avoid high parallelization costs but\nalso fully exploits the computing resources of GPUs. To address the GPU memory\nlimitation issue, we propose a partition-based algorithm for structural\nclustering that can process large graphs with limited GPU memory. We conduct\nexperiments on real graphs, and the experimental results demonstrate that our\nalgorithm can achieve up to 168 times speedup compared with the\nstate-of-the-art GPU-based algorithm when the graph can be resident in the GPU\nmemory. Moreover, our algorithm is scalable to handle large graphs. As an\nexample, our algorithm can finish the structural clustering on a graph with 1.8\nbillion edges using less than 2 GB GPU memory.",
            "author": [
                "Long Yuan",
                "Zeyu Zhou",
                "Xuemin Lin",
                "Zi Chen",
                "Xiang Zhao",
                "Fan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12281v2",
                "http://arxiv.org/pdf/2311.12281v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12277v1",
            "title": "Dihedral groups of order $2pq$ or $2pqr$ are DCI",
            "updated": "2023-11-21T01:41:00Z",
            "published": "2023-11-21T01:41:00Z",
            "summary": "A group has the (D)CI ((Directed) Cayley Isomorphism) property, or more\ncommonly is a (D)CI group, if any two Cayley (di)graphs on the group are\nisomorphic via a group automorphism. That is, $G$ is a (D)CI group if whenever\n$\\rm{Cay}(G,S)\\cong \\rm{Cay}(G,T)$, there is some $\\delta \\in \\rm{Aut}(G)$ such\nthat $S^\\delta=T$. (For the CI property, we only require this to be true if $S$\nand $T$ are closed under inversion.)\n  Suppose $p,q,r$ are distinct odd primes. We show that $D_{2pqr}$ is a DCI\ngroup. We present this result in the more general context of dihedral groups of\nsquarefree order; some of our results apply to any such group, and may be\nuseful in future toward showing that all dihedral groups of squarefree order\nare DCI groups.",
            "author": [
                "Joy Morris"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12277v1",
                "http://arxiv.org/pdf/2311.12277v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12275v2",
            "title": "Enabling On-Device Large Language Model Personalization with\n  Self-Supervised Data Selection and Synthesis",
            "updated": "2023-12-02T17:35:28Z",
            "published": "2023-11-21T01:34:02Z",
            "summary": "After a large language model (LLM) is deployed on edge devices, it is\ndesirable for these devices to learn from user-generated conversation data to\ngenerate user-specific and personalized responses in real-time. However,\nuser-generated data usually contains sensitive and private information, and\nuploading such data to the cloud for annotation is not preferred if not\nprohibited. While it is possible to obtain annotation locally by directly\nasking users to provide preferred responses, such annotations have to be sparse\nto not affect user experience. In addition, the storage of edge devices is\nusually too limited to enable large-scale fine-tuning with full user-generated\ndata. It remains an open question how to enable on-device LLM personalization,\nconsidering sparse annotation and limited on-device storage. In this paper, we\npropose a novel framework to select and store the most representative data\nonline in a self-supervised way. Such data has a small memory footprint and\nallows infrequent requests of user annotations for further fine-tuning. To\nenhance fine-tuning quality, multiple semantically similar pairs of question\ntexts and expected responses are generated using the LLM. Our experiments show\nthat the proposed framework achieves the best user-specific content-generating\ncapability (accuracy) and fine-tuning speed (performance) compared with vanilla\nbaselines. To the best of our knowledge, this is the very first on-device LLM\npersonalization framework.",
            "author": [
                "Ruiyang Qin",
                "Jun Xia",
                "Zhenge Jia",
                "Meng Jiang",
                "Ahmed Abbasi",
                "Peipei Zhou",
                "Jingtong Hu",
                "Yiyu Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12275v2",
                "http://arxiv.org/pdf/2311.12275v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12268v1",
            "title": "Boosting Audio-visual Zero-shot Learning with Large Language Models",
            "updated": "2023-11-21T01:18:23Z",
            "published": "2023-11-21T01:18:23Z",
            "summary": "Audio-visual zero-shot learning aims to recognize unseen categories based on\npaired audio-visual sequences. Recent methods mainly focus on learning aligned\nand discriminative multi-modal features to boost generalization towards unseen\ncategories. However, these approaches ignore the obscure action concepts in\ncategory names and may inevitably introduce complex network structures with\ndifficult training objectives. In this paper, we propose a simple yet effective\nframework named Knowledge-aware Distribution Adaptation (KDA) to help the model\nbetter grasp the novel action contents with an external knowledge base.\nSpecifically, we first propose using large language models to generate rich\ndescriptions from category names, which leads to a better understanding of\nunseen categories. Additionally, we propose a distribution alignment loss as\nwell as a knowledge-aware adaptive margin loss to further improve the\ngeneralization ability towards unseen categories. Extensive experimental\nresults demonstrate that our proposed KDA can outperform state-of-the-art\nmethods on three popular audio-visual zero-shot learning datasets. Our code\nwill be avaliable at \\url{https://github.com/chenhaoxing/KDA}.",
            "author": [
                "Haoxing Chen",
                "Yaohui Li",
                "Yan Hong",
                "Zizheng Huang",
                "Zhuoer Xu",
                "Zhangxuan Gu",
                "Jun Lan",
                "Huijia Zhu",
                "Weiqiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12268v1",
                "http://arxiv.org/pdf/2311.12268v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12267v1",
            "title": "Learning Causal Representations from General Environments:\n  Identifiability and Intrinsic Ambiguity",
            "updated": "2023-11-21T01:09:11Z",
            "published": "2023-11-21T01:09:11Z",
            "summary": "This paper studies causal representation learning, the task of recovering\nhigh-level latent variables and their causal relationships from low-level data\nthat we observe, assuming access to observations generated from multiple\nenvironments. While existing works are able to prove full identifiability of\nthe underlying data generating process, they typically assume access to\nsingle-node, hard interventions which is rather unrealistic in practice. The\nmain contribution of this paper is characterize a notion of identifiability\nwhich is provably the best one can achieve when hard interventions are not\navailable. First, for linear causal models, we provide identifiability\nguarantee for data observed from general environments without assuming any\nsimilarities between them. While the causal graph is shown to be fully\nrecovered, the latent variables are only identified up to an effect-domination\nambiguity (EDA). We then propose an algorithm, LiNGCReL which is guaranteed to\nrecover the ground-truth model up to EDA, and we demonstrate its effectiveness\nvia numerical experiments. Moving on to general non-parametric causal models,\nwe prove the same idenfifiability guarantee assuming access to groups of soft\ninterventions. Finally, we provide counterparts of our identifiability results,\nindicating that EDA is basically inevitable in our setting.",
            "author": [
                "Jikai Jin",
                "Vasilis Syrgkanis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12267v1",
                "http://arxiv.org/pdf/2311.12267v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "econ.EM",
                "stat.AP",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12255v2",
            "title": "Exploring Time Granularity on Temporal Graphs for Dynamic Link\n  Prediction in Real-world Networks",
            "updated": "2023-11-22T19:44:10Z",
            "published": "2023-11-21T00:34:53Z",
            "summary": "Dynamic Graph Neural Networks (DGNNs) have emerged as the predominant\napproach for processing dynamic graph-structured data. However, the influence\nof temporal information on model performance and robustness remains\ninsufficiently explored, particularly regarding how models address prediction\ntasks with different time granularities. In this paper, we explore the impact\nof time granularity when training DGNNs on dynamic graphs through extensive\nexperiments. We examine graphs derived from various domains and compare three\ndifferent DGNNs to the baseline model across four varied time granularities. We\nmainly consider the interplay between time granularities, model architectures,\nand negative sampling strategies to obtain general conclusions. Our results\nreveal that a sophisticated memory mechanism and proper time granularity are\ncrucial for a DGNN to deliver competitive and robust performance in the dynamic\nlink prediction task. We also discuss drawbacks in considered models and\ndatasets and propose promising directions for future research on the time\ngranularity of temporal graphs.",
            "author": [
                "Xiangjian Jiang",
                "Yanyi Pu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12255v2",
                "http://arxiv.org/pdf/2311.12255v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12248v1",
            "title": "A generalization of the Kelley--Meka theorem to binary systems of linear\n  forms",
            "updated": "2023-11-21T00:07:03Z",
            "published": "2023-11-21T00:07:03Z",
            "summary": "Kelley and Meka (FOCS'23) recently proved strong bounds on the size of\nsubsets of $\\mathbb{Z}_N$ or $\\mathbb{F}_q^n$ that do not contain 3-term\narithmetic progression. We use their techniques to prove similar bounds for\nsubsets of $\\mathbb{F}_q^n$ that do not contain non-degenerate instances of\naffine binary linear systems whose underlying graph is $2$-degenerate.\n  We show that if a subset of $\\mathbb{F}_q^n$ contains an atypical number of\ninstances of an affine binary linear 2-degenerate system, then it has a\nconstant density increment inside an affine subspace of polylogarithmic\ncodimension. We give a counterexample showing that this kind of result does not\nhold for linear systems whose true complexity exceeds $1$.\n  Using the same techniques, we obtain a counting lemma for sparse quasirandom\ngraphs, improving on the classical result of Chung, Graham, and Wilson\n(Combinatorica 1989), which is only nontrivial for dense graphs.",
            "author": [
                "Yuval Filmus",
                "Hamed Hatami",
                "Kaave Hosseini",
                "Esty Kelman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12248v1",
                "http://arxiv.org/pdf/2311.12248v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12245v1",
            "title": "Towards Accurate Loop Closure Detection in Semantic SLAM with 3D\n  Semantic Covisibility Graphs",
            "updated": "2023-11-21T00:02:25Z",
            "published": "2023-11-21T00:02:25Z",
            "summary": "Loop closure is necessary for correcting errors accumulated in simultaneous\nlocalization and mapping (SLAM) in unknown environments. However, conventional\nloop closure methods based on low-level geometric or image features may cause\nhigh ambiguity by not distinguishing similar scenarios. Thus, incorrect loop\nclosures can occur. Though semantic 2D image information is considered in some\nliterature to detect loop closures, there is little work that compares 3D\nscenes as an integral part of a semantic SLAM system. This paper introduces an\napproach, called SmSLAM+LCD, integrated into a semantic SLAM system to combine\nhigh-level 3D semantic information and low-level feature information to conduct\naccurate loop closure detection and effective drift reduction. The\neffectiveness of our approach is demonstrated in testing results.",
            "author": [
                "Zhentian Qian",
                "Jie Fu",
                "Jing Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12245v1",
                "http://arxiv.org/pdf/2311.12245v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12235v1",
            "title": "Improvements in Interlayer Pipelining of CNN Accelerators Using Genetic\n  Algorithms",
            "updated": "2023-11-20T23:24:58Z",
            "published": "2023-11-20T23:24:58Z",
            "summary": "Deploying Convolutional Neural Networks (CNNs) on edge platforms necessitates\nefficient hardware acceleration. Any unnecessary data movement in such\naccelerators can unacceptably degrade performance and efficiency. To address\nthis, we develop a layer fusion technique targeting CNNs, that reduces off-chip\ndata communication using a Genetic Algorithm (GA) applied to graph-based\ntopological sort. Results show a 1.8$\\times$ increase in energy efficiency and\n1.9$\\times$ improvement in energy-delay product (EDP) for MobileNet-v3 on a\nSIMBA-like mobile architecture. Our approach consistently improves workload\nperformance, averaging 1.4$\\times$ improvement to EDP for SIMBA and\n1.12$\\times$ for Eyeriss.",
            "author": [
                "Mark Horeni",
                "Siddharth Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12235v1",
                "http://arxiv.org/pdf/2311.12235v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "cs.LG",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12883v1",
            "title": "LLVM Static Analysis for Program Characterization and Memory Reuse\n  Profile Estimation",
            "updated": "2023-11-20T23:05:06Z",
            "published": "2023-11-20T23:05:06Z",
            "summary": "Profiling various application characteristics, including the number of\ndifferent arithmetic operations performed, memory footprint, etc., dynamically\nis time- and space-consuming. On the other hand, static analysis methods,\nalthough fast, can be less accurate. This paper presents an LLVM-based\nprobabilistic static analysis method that accurately predicts different program\ncharacteristics and estimates the reuse distance profile of a program by\nanalyzing the LLVM IR file in constant time, regardless of program input size.\nWe generate the basic-block-level control flow graph of the target application\nkernel and determine basic-block execution counts by solving the linear balance\nequation involving the adjacent basic blocks' transition probabilities.\nFinally, we represent the kernel memory accesses in a bracketed format and\nemploy a recursive algorithm to calculate the reuse distance profile. The\nresults show that our approach can predict application characteristics\naccurately compared to another LLVM-based dynamic code analysis tool, Byfl.",
            "author": [
                "Atanu Barai",
                "Nandakishore Santhi",
                "Abdur Razzak",
                "Stephan Eidenbenz",
                "Abdel-Hameed A. Badawy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12883v1",
                "http://arxiv.org/pdf/2311.12883v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.PF",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12176v1",
            "title": "Covert Online Decision Making: From Sequential Hypothesis Testing to\n  Stochastic Bandits",
            "updated": "2023-11-20T20:43:49Z",
            "published": "2023-11-20T20:43:49Z",
            "summary": "We study the problem of covert online decision-making in which an agent\nattempts to identify a parameter governing a system by probing the system while\nescaping detection from an adversary. The system is modeled as a Markov kernel\nwhose input is controlled by the agent and whose two outputs are observed by\nthe agent and the adversary, respectively. This problem is motivated by\napplications such as covert sensing or covert radar, in which one tries to\nperform a sensing task without arousing suspicion by an adversary monitoring\nthe environment for the presence of sensing signals. Specifically, we consider\ntwo situations corresponding to different amounts of knowledge of the system.\nIf the kernel is known but governed by an unknown fixed parameter, we formulate\nthe problem as a sequential hypothesis testing problem. If the kernel\ndetermining the observations of the agent is unknown but the kernel determining\nthose of the adversary is known, we formulate the problem as a best-arm\nidentification problem in a bandit setting. In both situations, we characterize\nthe exponent of the probability of identification error. As expected because of\nthe covertness requirement, the probability of identification error decays\nexponentially with the square root of the blocklength.",
            "author": [
                "Meng-Che Chang",
                "Matthieu R. Bloch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12176v1",
                "http://arxiv.org/pdf/2311.12176v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12167v1",
            "title": "Node classification in random trees",
            "updated": "2023-11-20T20:33:35Z",
            "published": "2023-11-20T20:33:35Z",
            "summary": "We propose a method for the classification of objects that are structured as\nrandom trees. Our aim is to model a distribution over the node label\nassignments in settings where the tree data structure is associated with node\nattributes (typically high dimensional embeddings). The tree topology is not\npredetermined and none of the label assignments are present during inference.\nOther methods that produce a distribution over node label assignment in trees\n(or more generally in graphs) either assume conditional independence of the\nlabel assignment, operate on a fixed graph topology, or require part of the\nnode labels to be observed. Our method defines a Markov Network with the\ncorresponding topology of the random tree and an associated Gibbs distribution.\nWe parameterize the Gibbs distribution with a Graph Neural Network that\noperates on the random tree and the node embeddings. This allows us to estimate\nthe likelihood of node assignments for a given random tree and use MCMC to\nsample from the distribution of node assignments.\n  We evaluate our method on the tasks of node classification in trees on the\nStanford Sentiment Treebank dataset. Our method outperforms the baselines on\nthis dataset, demonstrating its effectiveness for modeling joint distributions\nof node labels in random trees.",
            "author": [
                "Wouter W. L. Nuijten",
                "Vlado Menkovski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12167v1",
                "http://arxiv.org/pdf/2311.12167v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12161v2",
            "title": "ChemScraper: Graphics Extraction, Molecular Diagram Parsing, and\n  Annotated Data Generation for PDF Images",
            "updated": "2023-11-22T03:23:17Z",
            "published": "2023-11-20T20:27:42Z",
            "summary": "Existing visual parsers for molecule diagrams translate pixel-based raster\nimages such as PNGs to chemical structure representations (e.g., SMILES).\nHowever, PDFs created by word processors including LaTeX and Word provide\nexplicit locations and shapes for characters, lines, and polygons. We extract\nsymbols from born-digital PDF molecule images and then apply simple graph\ntransformations to capture both visual and chemical structure in editable\nChemDraw files (CDXML). Our fast ( PDF $\\rightarrow$ visual graph $\\rightarrow$\nchemical graph ) pipeline does not require GPUs, Optical Character Recognition\n(OCR) or vectorization. We evaluate on standard benchmarks using SMILES\nstrings, along with a novel evaluation that provides graph-based metrics and\nerror compilation using LgEval. The geometric information in born-digital PDFs\nproduces a highly accurate parser, motivating generating training data for\nvisual parsers that recognize from raster images, with extracted graphics,\nvisual structure, and chemical structure as annotations. To do this we render\nSMILES strings in Indigo, parse molecule structure, and then validate\nrecognized structure to select correct files.",
            "author": [
                "Ayush Kumar Shah",
                "Bryan Manrique Amador",
                "Abhisek Dey",
                "Ming Creekmore",
                "Blake Ocampo",
                "Scott Denmark",
                "Richard Zanibbi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12161v2",
                "http://arxiv.org/pdf/2311.12161v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12150v1",
            "title": "Fluid statics of a self-gravitating isothermal sphere of van der Waals'\n  gas",
            "updated": "2023-11-20T20:02:30Z",
            "published": "2023-11-20T20:02:30Z",
            "summary": "We subject to scrutiny the physical consistency of adopting the perfect-gas\nthermodynamic model within self-gravitation circumstances by studying the fluid\nstatics of a self-gravitating isothermal sphere with the van der Waals'\nthermodynamic model. The governing equations are formulated for any\nthermodynamic model with two intensive degrees of freedom, applied with the van\nder Waals' model and solved in nondimensional form numerically by\nfinite-difference algorithms. After a brief summary of thermodynamic\ncharacteristics, relevant to the present study, possessed by the van der Waals'\nmodel, we proceed to the description of the results in terms of comparative\ngraphs illustrating radial profiles of density, pressure and gravitational\nfield. We complement them with graphs that compare the dependence of central\nand wall densities on gravitational number for both perfect-gas and van der\nWaals' models and attest dramatically and unequivocally how the presence of the\nmolecular-attraction and -size terms removes questionable fluid-statics results\nsystematically found accompanying the perfect-gas model in standard treatments.\nWe also describe, within a very brief and preliminary digression, how the\nsanitising action of the mentioned terms affects the thermodynamics of the\nisothermal sphere by providing evidence of how the gravitational correction to\nentropy corresponding to the van der Waals' model makes sure that there is no\nrisk of gravothermal catastrophes, negative specific heats, and thermal\ninstabilities. We investigate the phenomenology related to self-gravitationally\ninduced both liquid-gas phase equilibria and metastable-gas states and we\ndescribe how they arise naturally and self-consistently from the governing\nequations. We conclude with a summary of the main results and with a\nchallenging proposal of future work meant to revalorise the perfect gas model.",
            "author": [
                "Domenico Giordano",
                "Pierluigi Amodio",
                "Felice Iavernaro",
                "Francesca Mazzia",
                "P\u00e9ter V\u00e1n",
                "M\u00e1ty\u00e1s Sz\u00fccs"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12150v1",
                "http://arxiv.org/pdf/2311.12150v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12144v6",
            "title": "Applications of Large Scale Foundation Models for Autonomous Driving",
            "updated": "2023-12-04T06:58:33Z",
            "published": "2023-11-20T19:45:27Z",
            "summary": "Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,\nautonomous driving has been the most active field of AI applications. Recently\npowered by large language models (LLMs), chat systems, such as chatGPT and\nPaLM, emerge and rapidly become a promising direction to achieve artificial\ngeneral intelligence (AGI) in natural language processing (NLP). There comes a\nnatural thinking that we could employ these abilities to reformulate autonomous\ndriving. By combining LLM with foundation models, it is possible to utilize the\nhuman knowledge, commonsense and reasoning to rebuild autonomous driving\nsystems from the current long-tailed AI dilemma. In this paper, we investigate\nthe techniques of foundation models and LLMs applied for autonomous driving,\ncategorized as simulation, world model, data annotation and planning or E2E\nsolutions etc.",
            "author": [
                "Yu Huang",
                "Yue Chen",
                "Zhu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12144v6",
                "http://arxiv.org/pdf/2311.12144v6"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12139v1",
            "title": "Software engineering in start-up companies: An analysis of 88 experience\n  reports",
            "updated": "2023-11-20T19:42:37Z",
            "published": "2023-11-20T19:42:37Z",
            "summary": "Context: Start-up companies have become an important supplier of innovation\nand software-intensive products. The flexibility and reactiveness of start-ups\nenables fast development and launch of innovative products. However, a majority\nof software start-up companies fail before achieving any success. Among other\nfactors, poor software engineering could be a significant contributor to the\nchallenges experienced by start-ups. However, the state-of-practice of software\nengineering in start-ups, as well as the utilization of state-of-the-art is\nlargely an unexplored area. Objective: In this study we investigate how\nsoftware engineering is applied in start-up context with a focus to identify\nkey knowledge areas and opportunities for further research. Method: We perform\na multi-vocal exploratory study of 88 start-up experience reports. We develop a\ncustom taxonomy to categorize the reported software engineering practices and\ntheir interrelation with business aspects, and apply qualitative data analysis\nto explore influences and dependencies between the knowledge areas. Results: We\nidentify the most frequently reported software engineering (requirements\nengineering, software design and quality) and business aspect (vision and\nstrategy development) knowledge areas, and illustrate their relationships. We\nalso present a summary of how relevant software engineering knowledge areas are\nimplemented in start-ups and identify potentially useful practices for adoption\nin start-ups. Conclusions: The results enable a more focused research on\nengineering practices in start-ups. We conclude that most engineering\nchallenges in start-ups stem from inadequacies in requirements engineering.\nMany promising practices to address specific engineering challenges exists,\nhowever more research on adaptation of established practices, and validation of\nnew start-up specific practices is needed.",
            "author": [
                "Eriks Klotins",
                "Michael Unterkalmsteiner",
                "Tony Gorschek"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s10664-018-9620-y",
                "http://arxiv.org/abs/2311.12139v1",
                "http://arxiv.org/pdf/2311.12139v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12136v1",
            "title": "Multi-view Graph Convolution for Participant Recommendation",
            "updated": "2023-11-20T19:37:57Z",
            "published": "2023-11-20T19:37:57Z",
            "summary": "Social networks have become essential for people's lives. The proliferation\nof web services further expands social networks at an unprecedented scale,\nleading to immeasurable commercial value for online platforms. Recently, the\ngroup buying (GB) business mode is prevalent and also becoming more popular in\nE-commerce. GB explicitly forms groups of users with similar interests to\nsecure better discounts from the merchants, often operating within social\nnetworks. It is a novel way to further unlock the commercial value by\nexplicitly utilizing the online social network in E-commerce. Participant\nrecommendation, a fundamental problem emerging together with GB, aims to find\nthe participants for a launched group buying process with an initiator and a\ntarget item to increase the GB success rate. This paper proposes Multi-View\nGraph Convolution for Participant Recommendation (MVPRec) to tackle this\nproblem. To differentiate the roles of users (Initiator/Participant) within the\nGB process, we explicitly reconstruct historical GB data into initiator-view\nand participant-view graphs. Together with the social graph, we obtain a\nmulti-view user representation with graph encoders. Then MVPRec fuses the GB\nand social representation with an attention module to obtain the user\nrepresentation and learns a matching score with the initiator's social friends\nvia a multi-head attention mechanism. Social friends with the Top-k matching\nscore are recommended for the corresponding GB process. Experiments on three\ndatasets justify the effectiveness of MVPRec in the emerging participant\nrecommendation problem.",
            "author": [
                "Xiaolong Liu",
                "Liangwei Yang",
                "Chen Wang",
                "Mingdai Yang",
                "Zhiwei Liu",
                "Philip S. Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12136v1",
                "http://arxiv.org/pdf/2311.12136v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12103v1",
            "title": "A tensor network view of multiconfiguration time-dependent Hartree\n  methods",
            "updated": "2023-11-20T19:00:02Z",
            "published": "2023-11-20T19:00:02Z",
            "summary": "The multilayer multiconfiguration time-dependent Hartree (ML-MCTDH) method\nand the density matrix renormalization group (DMRG) are powerful workhorses\napplied mostly in different scientific fields. Although both methods are based\non tensor network states, very different mathematical languages are used for\ndescribing them. This severely limits knowledge transfer and sometimes leads to\nre-inventions of ideas well known in the other field. Here, we review ML-MCTDH\nand DMRG theory using both MCTDH expressions and tensor network diagrams. We\nderive the ML-MCTDH equations of motions using diagrams and compare them with\ntime-dependent and time-independent DMRG algorithms. We further review two\nselected recent advancements. The first advancement is related to optimizing\nunoccupied single-particle functions in MCTDH, which corresponds to subspace\nenrichment in the DMRG. The second one is related to finding optimal tree\nstructures and on highlighting similarities and differences of MCTDH and DMRG\ntheories. We hope that this contribution will foster more fruitful\ncross-fertilization of ideas between ML-MCTDH and DMRG.",
            "author": [
                "Henrik R. Larsson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12103v1",
                "http://arxiv.org/pdf/2311.12103v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph",
                "cond-mat.str-el",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12027v2",
            "title": "Series over fat partitions: matrix models and discrete ensembles",
            "updated": "2023-11-22T18:56:56Z",
            "published": "2023-11-20T18:59:28Z",
            "summary": "We consider series over Young diagrams of products of Schur functions\n$s_{\\lambda\\cup\\lambda}$, marked with ``fat partitions'' $\\lambda\\cup\\lambda$,\nwhich appear in matrix models associated with ensembles of symplectic and\northogonal matrices and quaternion ensemble Ginibre. We consider mixed matrix\nmodels that also contain complex Ginibre ensembles labeled by graphs and the\nthree ensembles mentioned above. Cases are identified when the series of\nperturbations in coupling constants turn out to be tau functions of the DKP\nhierarchy introduced by the Kyoto school. This topic relates matrix models to\nrandom partitions - discrete symplectic ensemble and its modifications.",
            "author": [
                "A. Yu. Orlov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12027v2",
                "http://arxiv.org/pdf/2311.12027v2"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12022v1",
            "title": "GPQA: A Graduate-Level Google-Proof Q&A Benchmark",
            "updated": "2023-11-20T18:57:34Z",
            "published": "2023-11-20T18:57:34Z",
            "summary": "We present GPQA, a challenging dataset of 448 multiple-choice questions\nwritten by domain experts in biology, physics, and chemistry. We ensure that\nthe questions are high-quality and extremely difficult: experts who have or are\npursuing PhDs in the corresponding domains reach 65% accuracy (74% when\ndiscounting clear mistakes the experts identified in retrospect), while highly\nskilled non-expert validators only reach 34% accuracy, despite spending on\naverage over 30 minutes with unrestricted access to the web (i.e., the\nquestions are \"Google-proof\"). The questions are also difficult for\nstate-of-the-art AI systems, with our strongest GPT-4 based baseline achieving\n39% accuracy. If we are to use future AI systems to help us answer very hard\nquestions, for example, when developing new scientific knowledge, we need to\ndevelop scalable oversight methods that enable humans to supervise their\noutputs, which may be difficult even if the supervisors are themselves skilled\nand knowledgeable. The difficulty of GPQA both for skilled non-experts and\nfrontier AI systems should enable realistic scalable oversight experiments,\nwhich we hope can help devise ways for human experts to reliably get truthful\ninformation from AI systems that surpass human capabilities.",
            "author": [
                "David Rein",
                "Betty Li Hou",
                "Asa Cooper Stickland",
                "Jackson Petty",
                "Richard Yuanzhe Pang",
                "Julien Dirani",
                "Julian Michael",
                "Samuel R. Bowman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12022v1",
                "http://arxiv.org/pdf/2311.12022v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12017v1",
            "title": "Public-key pseudoentanglement and the hardness of learning ground state\n  entanglement structure",
            "updated": "2023-11-20T18:54:48Z",
            "published": "2023-11-20T18:54:48Z",
            "summary": "Given a local Hamiltonian, how difficult is it to determine the entanglement\nstructure of its ground state? We show that this problem is computationally\nintractable even if one is only trying to decide if the ground state is\nvolume-law vs near area-law entangled. We prove this by constructing strong\nforms of pseudoentanglement in a public-key setting, where the circuits used to\nprepare the states are public knowledge. In particular, we construct two\nfamilies of quantum circuits which produce volume-law vs near area-law\nentangled states, but nonetheless the classical descriptions of the circuits\nare indistinguishable under the Learning with Errors (LWE) assumption.\nIndistinguishability of the circuits then allows us to translate our\nconstruction to Hamiltonians. Our work opens new directions in Hamiltonian\ncomplexity, for example whether it is difficult to learn certain phases of\nmatter.",
            "author": [
                "Adam Bouland",
                "Bill Fefferman",
                "Soumik Ghosh",
                "Tony Metger",
                "Umesh Vazirani",
                "Chenyi Zhang",
                "Zixin Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12017v1",
                "http://arxiv.org/pdf/2311.12017v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11986v1",
            "title": "A robust numerical strategy for finding surface waves in flows of\n  non-Newtonian liquids",
            "updated": "2023-11-20T18:19:45Z",
            "published": "2023-11-20T18:19:45Z",
            "summary": "Gravity-driven flows of liquid films are frequent in nature and industry,\nsuch as in landslides, lava flow, cooling of nuclear reactors, and coating\nprocesses. In many of these cases, the liquid is non-Newtonian and has\nparticular characteristics. In this paper, we analyze numerically the temporal\nstability of films of non-Newtonian liquids falling by gravity, on the onset of\ninstability. The liquid flows over an incline, where surface waves appear under\ncertain conditions, and we do not fix a priori its rheological behavior. For\nthat, we made used of the Carreau-Yasuda model without assigning specific\nvalues to its constants, and we compute general stability solutions. The\nnumerical strategy is based on expansions of Chebyshev polynomials for\ndiscretizing the Orr-Sommerfeld equation and boundary conditions, and a\nGalerkin method for solving the generalized eigenvalue problem. In addition, an\nInverse Iteration method was implemented to increase accuracy and improve\ncomputational time. The result is a robust and light numerical tool capable of\nfinding the critical conditions for different types of fluids, which we use to\nanalyze some key fluids. We show that the outputs of the general code match\nprevious solutions obtained for specific computations. Besides increasing our\nknowledge on surface-wave instabilities in non-Newtonian liquids, our findings\nprovide a new tool for obtaining comprehensive solutions on the onset of\ninstability.",
            "author": [
                "Bruno Pelisson Chimetta",
                "Erick de Moraes Franklin"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jnnfm.2023.105153",
                "http://arxiv.org/abs/2311.11986v1",
                "http://arxiv.org/pdf/2311.11986v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11980v1",
            "title": "Leveraging Previous Facial Action Units Knowledge for Emotion\n  Recognition on Faces",
            "updated": "2023-11-20T18:14:53Z",
            "published": "2023-11-20T18:14:53Z",
            "summary": "People naturally understand emotions, thus permitting a machine to do the\nsame could open new paths for human-computer interaction. Facial expressions\ncan be very useful for emotion recognition techniques, as these are the biggest\ntransmitters of non-verbal cues capable of being correlated with emotions.\nSeveral techniques are based on Convolutional Neural Networks (CNNs) to extract\ninformation in a machine learning process. However, simple CNNs are not always\nsufficient to locate points of interest on the face that can be correlated with\nemotions. In this work, we intend to expand the capacity of emotion recognition\ntechniques by proposing the usage of Facial Action Units (AUs) recognition\ntechniques to recognize emotions. This recognition will be based on the Facial\nAction Coding System (FACS) and computed by a machine learning system. In\nparticular, our method expands over EmotiRAM, an approach for multi-cue emotion\nrecognition, in which we improve over their facial encoding module.",
            "author": [
                "Pietro B. S. Masur",
                "Willams Costa",
                "Lucas S. Figueredo",
                "Veronica Teichrieb"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11980v1",
                "http://arxiv.org/pdf/2311.11980v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11978v1",
            "title": "Algebras of graph functions",
            "updated": "2023-11-20T18:12:19Z",
            "published": "2023-11-20T18:12:19Z",
            "summary": "Differential operators acting on functions defined on graphs by different\nstudies do not form a consistent framework for the analysis of real or complex\nfunctions in the sense that they do not satisfy the Leibniz rule of any order.\nIn this paper we propose a new family of operators that satisfy the Leibniz\nrule, and as special cases, produce the specific operators defined in the\nliterature, such as the graph difference and the graph Laplacian. We propose a\nframework to define the order of a differential operator consistently using the\nLeibniz rule in Lie algebraic setting.\n  Furthermore by identifying the space of functions defined on graph edges with\nthe tensor product of node functions we construct a Lie bialgebra of graph\nfunctions and reinterpret the difference operator as a co-bracket. As an\napplication, some explicit solutions of Schr\\\"odinger and Fokker-Planck\nequations are given.",
            "author": [
                "F\u00fcl\u00f6p Bazs\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11978v1",
                "http://arxiv.org/pdf/2311.11978v1"
            ],
            "primary_category": "math-ph",
            "category": [
                "math-ph",
                "math.CO",
                "math.MP",
                "math.RA",
                "17B81, 05C25, 81Q99, 35Q99, 17B62"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11971v1",
            "title": "LiDAR-HMR: 3D Human Mesh Recovery from LiDAR",
            "updated": "2023-11-20T17:59:28Z",
            "published": "2023-11-20T17:59:28Z",
            "summary": "In recent years, point cloud perception tasks have been garnering increasing\nattention. This paper presents the first attempt to estimate 3D human body mesh\nfrom sparse LiDAR point clouds. We found that the major challenge in estimating\nhuman pose and mesh from point clouds lies in the sparsity, noise, and\nincompletion of LiDAR point clouds. Facing these challenges, we propose an\neffective sparse-to-dense reconstruction scheme to reconstruct 3D human mesh.\nThis involves estimating a sparse representation of a human (3D human pose) and\ngradually reconstructing the body mesh. To better leverage the 3D structural\ninformation of point clouds, we employ a cascaded graph transformer\n(graphormer) to introduce point cloud features during sparse-to-dense\nreconstruction. Experimental results on three publicly available databases\ndemonstrate the effectiveness of the proposed approach. Code:\nhttps://github.com/soullessrobot/LiDAR-HMR/",
            "author": [
                "Bohao Fan",
                "Wenzhao Zheng",
                "Jianjiang Feng",
                "Jie Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11971v1",
                "http://arxiv.org/pdf/2311.11971v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11969v1",
            "title": "SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20\n  Million masks",
            "updated": "2023-11-20T17:59:03Z",
            "published": "2023-11-20T17:59:03Z",
            "summary": "Segment Anything Model (SAM) has achieved impressive results for natural\nimage segmentation with input prompts such as points and bounding boxes. Its\nsuccess largely owes to massive labeled training data. However, directly\napplying SAM to medical image segmentation cannot perform well because SAM\nlacks medical knowledge -- it does not use medical images for training. To\nincorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a\nlarge-scale segmentation dataset of 2D medical images built upon numerous\npublic and private datasets. It consists of 4.6 million 2D medical images and\n19.7 million corresponding masks, covering almost the whole body and showing\nsignificant diversity. This paper describes all the datasets collected in\nSA-Med2D-20M and details how to process these datasets. Furthermore,\ncomprehensive statistics of SA-Med2D-20M are presented to facilitate the better\nuse of our dataset, which can help the researchers build medical vision\nfoundation models or apply their models to downstream medical applications. We\nhope that the large scale and diversity of SA-Med2D-20M can be leveraged to\ndevelop medical artificial intelligence for enhancing diagnosis, medical image\nanalysis, knowledge sharing, and education. The data with the redistribution\nlicense is publicly available at https://github.com/OpenGVLab/SAM-Med2D.",
            "author": [
                "Jin Ye",
                "Junlong Cheng",
                "Jianpin Chen",
                "Zhongying Deng",
                "Tianbin Li",
                "Haoyu Wang",
                "Yanzhou Su",
                "Ziyan Huang",
                "Jilong Chen",
                "Lei Jiang",
                "Hui Sun",
                "Min Zhu",
                "Shaoting Zhang",
                "Junjun He",
                "Yu Qiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11969v1",
                "http://arxiv.org/pdf/2311.11969v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11965v1",
            "title": "Provably Efficient CVaR RL in Low-rank MDPs",
            "updated": "2023-11-20T17:44:40Z",
            "published": "2023-11-20T17:44:40Z",
            "summary": "We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize\nthe Conditional Value at Risk (CVaR) with a fixed risk tolerance $\\tau$. Prior\ntheoretical work studying risk-sensitive RL focuses on the tabular Markov\nDecision Processes (MDPs) setting. To extend CVaR RL to settings where state\nspace is large, function approximation must be deployed. We study CVaR RL in\nlow-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the\nunderlying transition kernel admits a low-rank decomposition, but unlike prior\nlinear models, low-rank MDPs do not assume the feature or state-action\nrepresentation is known. We propose a novel Upper Confidence Bound (UCB)\nbonus-driven algorithm to carefully balance the interplay between exploration,\nexploitation, and representation learning in CVaR RL. We prove that our\nalgorithm achieves a sample complexity of $\\tilde{O}\\left(\\frac{H^7 A^2\nd^4}{\\tau^2 \\epsilon^2}\\right)$ to yield an $\\epsilon$-optimal CVaR, where $H$\nis the length of each episode, $A$ is the capacity of action space, and $d$ is\nthe dimension of representations. Computational-wise, we design a novel\ndiscretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR\nobjective as the planning oracle and show that we can find the near-optimal\npolicy in a polynomial running time with a Maximum Likelihood Estimation\noracle. To our knowledge, this is the first provably efficient CVaR RL\nalgorithm in low-rank MDPs.",
            "author": [
                "Yulai Zhao",
                "Wenhao Zhan",
                "Xiaoyan Hu",
                "Ho-fung Leung",
                "Farzan Farnia",
                "Wen Sun",
                "Jason D. Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11965v1",
                "http://arxiv.org/pdf/2311.11965v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11908v2",
            "title": "Continual Learning: Applications and the Road Forward",
            "updated": "2023-11-21T15:17:00Z",
            "published": "2023-11-20T16:40:29Z",
            "summary": "Continual learning is a sub-field of machine learning, which aims to allow\nmachine learning models to continuously learn on new data, by accumulating\nknowledge without forgetting what was learned in the past. In this work, we\ntake a step back, and ask: \"Why should one care about continual learning in the\nfirst place?\". We set the stage by surveying recent continual learning papers\npublished at three major machine learning conferences, and show that\nmemory-constrained settings dominate the field. Then, we discuss five open\nproblems in machine learning, and even though they seem unrelated to continual\nlearning at first sight, we show that continual learning will inevitably be\npart of their solution. These problems are model-editing, personalization,\non-device learning, faster (re-)training and reinforcement learning. Finally,\nby comparing the desiderata from these unsolved problems and the current\nassumptions in continual learning, we highlight and discuss four future\ndirections for continual learning research. We hope that this work offers an\ninteresting perspective on the future of continual learning, while displaying\nits potential value and the paths we have to pursue in order to make it\nsuccessful. This work is the result of the many discussions the authors had at\nthe Dagstuhl seminar on Deep Continual Learning, in March 2023.",
            "author": [
                "Eli Verwimp",
                "Rahaf Aljundi",
                "Shai Ben-David",
                "Matthias Bethge",
                "Andrea Cossu",
                "Alexander Gepperth",
                "Tyler L. Hayes",
                "Eyke H\u00fcllermeier",
                "Christopher Kanan",
                "Dhireesha Kudithipudi",
                "Christoph H. Lampert",
                "Martin Mundt",
                "Razvan Pascanu",
                "Adrian Popescu",
                "Andreas S. Tolias",
                "Joost van de Weijer",
                "Bing Liu",
                "Vincenzo Lomonaco",
                "Tinne Tuytelaars",
                "Gido M. van de Ven"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11908v2",
                "http://arxiv.org/pdf/2311.11908v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11898v1",
            "title": "Multimodal Safe Control for Human-Robot Interaction",
            "updated": "2023-11-20T16:33:25Z",
            "published": "2023-11-20T16:33:25Z",
            "summary": "Generating safe behaviors for autonomous systems is important as they\ncontinue to be deployed in the real world, especially around people. In this\nwork, we focus on developing a novel safe controller for systems where there\nare multiple sources of uncertainty. We formulate a novel multimodal safe\ncontrol method, called the Multimodal Safe Set Algorithm (MMSSA) for the case\nwhere the agent has uncertainty over which discrete mode the system is in, and\neach mode itself contains additional uncertainty. To our knowledge, this is the\nfirst energy-function-based safe control method applied to systems with\nmultimodal uncertainty. We apply our controller to a simulated human-robot\ninteraction where the robot is uncertain of the human's true intention and each\npotential intention has its own additional uncertainty associated with it,\nsince the human is not a perfectly rational actor. We compare our proposed safe\ncontroller to existing safe control methods and find that it does not impede\nthe system performance (i.e. efficiency) while also improving the safety of the\nsystem.",
            "author": [
                "Ravi Pandya",
                "Tianhao Wei",
                "Changliu Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11898v1",
                "http://arxiv.org/pdf/2311.11898v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11891v1",
            "title": "AMES: A Differentiable Embedding Space Selection Framework for Latent\n  Graph Inference",
            "updated": "2023-11-20T16:24:23Z",
            "published": "2023-11-20T16:24:23Z",
            "summary": "In real-world scenarios, although data entities may possess inherent\nrelationships, the specific graph illustrating their connections might not be\ndirectly accessible. Latent graph inference addresses this issue by enabling\nGraph Neural Networks (GNNs) to operate on point cloud data, dynamically\nlearning the necessary graph structure. These graphs are often derived from a\nlatent embedding space, which can be modeled using Euclidean, hyperbolic,\nspherical, or product spaces. However, currently, there is no principled\ndifferentiable method for determining the optimal embedding space. In this\nwork, we introduce the Attentional Multi-Embedding Selection (AMES) framework,\na differentiable method for selecting the best embedding space for latent graph\ninference through backpropagation, considering a downstream task. Our framework\nconsistently achieves comparable or superior results compared to previous\nmethods for latent graph inference across five benchmark datasets. Importantly,\nour approach eliminates the need for conducting multiple experiments to\nidentify the optimal embedding space. Furthermore, we explore interpretability\ntechniques that track the gradient contributions of different latent graphs,\nshedding light on how our attention-based, fully differentiable approach learns\nto choose the appropriate latent space. In line with previous works, our\nexperiments emphasize the advantages of hyperbolic spaces in enhancing\nperformance. More importantly, our interpretability framework provides a\ngeneral approach for quantitatively comparing embedding spaces across different\ntasks based on their contributions, a dimension that has been overlooked in\nprevious literature on latent graph inference.",
            "author": [
                "Yuan Lu",
                "Haitz S\u00e1ez de Oc\u00e1riz Borde",
                "Pietro Li\u00f2"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11891v1",
                "http://arxiv.org/pdf/2311.11891v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11887v1",
            "title": "An Almgren monotonicity formula for discrete harmonic functions",
            "updated": "2023-11-20T16:20:48Z",
            "published": "2023-11-20T16:20:48Z",
            "summary": "The celebrated Almgren monotonicity formula for harmonic functions\n$u:\\mathbb{R}^n \\rightarrow \\mathbb{R}$ says that its $L^2-$energy concentrated\non a sphere of radius $r$, when measured in a suitable sense, is\nnon-decreasing: if $u$ oscillates at a certain scale, it has even larger\noscillations at a larger scale. We prove a discrete analogue of the Almgren\nmonotonicity formula for harmonic functions on infinite combinatorial graphs\n$G=(V,E)$. Some applications are discussed.",
            "author": [
                "Mariana Smit Vega Garcia",
                "Stefan Steinerberger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11887v1",
                "http://arxiv.org/pdf/2311.11887v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11879v1",
            "title": "Periodic orbits in general Glass networks",
            "updated": "2023-11-20T16:14:17Z",
            "published": "2023-11-20T16:14:17Z",
            "summary": "Glass networks are piecewise linear ODE systems that models an interactive\nsystem where there are 'switching points': the underlying dynamic changes\nqualitatively when a certain variable pass over a threshold. One of the most\nwell-studied class of models of the original Glass network are the cyclic\nattractor in the orthants (a sequence of orthants where the flow from one\northant to another is unanimous), which was first defined and analysed by Glass\nand Pasternack in 1978. In that paper, the authors gave a complete\nclassification of the topological features of the flow in a full-rank cyclic\nattractor, which is a cyclic attractor that cannot be contained in any sub-cube\nin the graph of orthants.\n  In this paper, we will extend the definition of cyclic attractor to one\ngeneralisation of the Glass network, one that allows for multiple switching\npoints in each variables, and give a complete classification of the topological\nfeatures of the flow for any cyclic attractor, both in the extended network and\nthe original network, including non full-rank ones. We will show that in any\ncyclic attractor, there is either a unique and asymptotically stable periodic\norbit, or that all periodic orbits are degenerated.",
            "author": [
                "Huy K. Vo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11879v1",
                "http://arxiv.org/pdf/2311.11879v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "math.CA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11878v1",
            "title": "A large-scale longitudinal structured dataset of the dark web\n  cryptomarket Evolution (2014-2015)",
            "updated": "2023-11-20T16:13:53Z",
            "published": "2023-11-20T16:13:53Z",
            "summary": "Dark Web Marketplaces (DWM) facilitate the online trade of illicit goods. Due\nto the illicit nature of these marketplaces, quality datasets are scarce and\ndifficult to produce. The Dark Net Market archives (2015) presented raw scraped\nsource files crawled from a selection of DWMs, including Evolution. Here, we\npresent, specifically for the Evolution DWM, a structured dataset extracted\nfrom Dark Net Market archive data. Uniquely, many of the data quality issues\ninherent to crawled data are resolved. The dataset covers over 500 thousand\nforum posts and over 80 thousand listings, providing data on forums, topics,\nposts, forum users, market vendors, listings, and more. Additionally, we\npresent temporal weighted communication networks extracted from this data. The\npresented dataset provides easy access to a high quality DWM dataset to\nfacilitate the study of criminal behaviour and communication on such DWMs,\nwhich may provide a relevant source of knowledge for researchers across\ndisciplines, from social science to law to network science.",
            "author": [
                "Hanjo D. Boekhout",
                "Arjan A. Blokland",
                "Frank W. Takes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11878v1",
                "http://arxiv.org/pdf/2311.11878v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11869v1",
            "title": "A PTAS for Triangle-Free 2-Matching",
            "updated": "2023-11-20T16:05:11Z",
            "published": "2023-11-20T16:05:11Z",
            "summary": "In the Triangle-Free (Simple) 2-Matching problem we are given an undirected\ngraph $G=(V,E)$. Our goal is to compute a maximum-cardinality $M\\subseteq E$\nsatisfying the following properties: (1) at most two edges of $M$ are incident\non each node (i.e., $M$ is a 2-matching) and (2) $M$ does not induce any\ntriangle. In his Ph.D. thesis from 1984, Harvitgsen presents a complex\npolynomial-time algorithm for this problem, with a very complex analysis. This\nresult was never published in a journal nor reproved in a different way, to the\nbest of our knowledge.\n  In this paper we have a fresh look at this problem and present a simple PTAS\nfor it based on local search. Our PTAS exploits the fact that, as long as the\ncurrent solution is far enough from the optimum, there exists a short\naugmenting trail (similar to the maximum matching case).",
            "author": [
                "Miguel Bosch-Calvo",
                "Fabrizio Grandoni",
                "Afrouz Jabal Ameli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11869v1",
                "http://arxiv.org/pdf/2311.11869v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "68W25 (Primary) 68W40, 68R10, 05C40, 05C70 (Secondary)",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11860v2",
            "title": "LION : Empowering Multimodal Large Language Model with Dual-Level Visual\n  Knowledge",
            "updated": "2023-11-26T10:10:55Z",
            "published": "2023-11-20T15:56:44Z",
            "summary": "Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability\nto perceive and understand multi-modal signals. However, most of the existing\nMLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text\npairs, leading to insufficient extraction and reasoning of visual knowledge. To\naddress this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal\nLarge Language Model (LION), which empowers the MLLM by injecting visual\nknowledge in two levels. 1) Progressive incorporation of fine-grained\nspatial-aware visual knowledge. We design a vision aggregator cooperated with\nregion-level vision-language (VL) tasks to incorporate fine-grained\nspatial-aware visual knowledge into the MLLM. To alleviate the conflict between\nimage-level and region-level VL tasks during incorporation, we devise a\ndedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This\nprogressive incorporation scheme contributes to the mutual promotion between\nthese two kinds of VL tasks. 2) Soft prompting of high-level semantic visual\nevidence. We facilitate the MLLM with high-level semantic visual evidence by\nleveraging diverse image tags. To mitigate the potential influence caused by\nimperfect predicted tags, we propose a soft prompting method by embedding a\nlearnable token into the tailored text instruction. Comprehensive experiments\non several multi-modal benchmarks demonstrate the superiority of our model\n(e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over\nInstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).",
            "author": [
                "Gongwei Chen",
                "Leyang Shen",
                "Rui Shao",
                "Xiang Deng",
                "Liqiang Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11860v2",
                "http://arxiv.org/pdf/2311.11860v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11849v1",
            "title": "Multilayer Quantile Graph for Multivariate Time Series Analysis and\n  Dimensionality Reduction",
            "updated": "2023-11-20T15:37:43Z",
            "published": "2023-11-20T15:37:43Z",
            "summary": "In recent years, there has been a surge in the prevalence of high- and\nmulti-dimensional temporal data across various scientific disciplines. These\ndatasets are characterized by their vast size and challenging potential for\nanalysis. Such data typically exhibit serial and cross-dependency and possess\nhigh dimensionality, thereby introducing additional complexities to\nconventional time series analysis methods. To address these challenges, a\nrecent and complementary approach has emerged, known as network-based analysis\nmethods for multivariate time series. In univariate settings, Quantile Graphs\nhave been employed to capture temporal transition properties and reduce data\ndimensionality by mapping observations to a smaller set of sample quantiles.\n  To confront the increasingly prominent issue of high dimensionality, we\npropose an extension of Quantile Graphs into a multivariate variant, which we\nterm \"Multilayer Quantile Graphs\". In this innovative mapping, each time series\nis transformed into a Quantile Graph, and inter-layer connections are\nestablished to link contemporaneous quantiles of pairwise series. This enables\nthe analysis of dynamic transitions across multiple dimensions. In this study,\nwe demonstrate the effectiveness of this new mapping using a synthetic\nmultivariate time series dataset. We delve into the resulting network's\ntopological structures, extract network features, and employ these features for\noriginal dataset analysis. Furthermore, we compare our results with a recent\nmethod from the literature. The resulting multilayer network offers a\nsignificant reduction in the dimensionality of the original data while\ncapturing serial and cross-dimensional transitions. This approach facilitates\nthe characterization and analysis of large multivariate time series datasets\nthrough network analysis techniques.",
            "author": [
                "Vanessa Freitas Silva",
                "Maria Eduarda Silva",
                "Pedro Ribeiro",
                "Fernando Silva"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11849v1",
                "http://arxiv.org/pdf/2311.11849v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11824v2",
            "title": "Neural Graph Collaborative Filtering Using Variational Inference",
            "updated": "2023-12-02T18:43:13Z",
            "published": "2023-11-20T15:01:33Z",
            "summary": "The customization of recommended content to users holds significant\nimportance in enhancing user experiences across a wide spectrum of applications\nsuch as e-commerce, music, and shopping. Graph-based methods have achieved\nconsiderable performance by capturing user-item interactions. However, these\nmethods tend to utilize randomly constructed embeddings in the dataset used for\ntraining the recommender, which lacks any user preferences. Here, we propose\nthe concept of variational embeddings as a means of pre-training the\nrecommender system to improve the feature propagation through the layers of\ngraph convolutional networks (GCNs). The graph variational embedding\ncollaborative filtering (GVECF) is introduced as a novel framework to\nincorporate representations learned through a variational graph auto-encoder\nwhich are embedded into a GCN-based collaborative filtering. This approach\neffectively transforms latent high-order user-item interactions into more\ntrainable vectors, ultimately resulting in better performance in terms of\nrecall and normalized discounted cumulative gain(NDCG) metrics. The experiments\nconducted on benchmark datasets demonstrate that our proposed method achieves\nup to 13.78% improvement in the recall over the test data.",
            "author": [
                "Narges Sadat Fazeli Dehkordi",
                "Hadi Zare",
                "Parham Moradi",
                "Mahdi Jalili"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11824v2",
                "http://arxiv.org/pdf/2311.11824v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11821v1",
            "title": "Cross-View Graph Consistency Learning for Invariant Graph\n  Representations",
            "updated": "2023-11-20T14:58:47Z",
            "published": "2023-11-20T14:58:47Z",
            "summary": "Graph representation learning is fundamental for analyzing graph-structured\ndata. Exploring invariant graph representations remains a challenge for most\nexisting graph representation learning methods. In this paper, we propose a\ncross-view graph consistency learning (CGCL) method that learns invariant graph\nrepresentations for link prediction. First, two complementary augmented views\nare derived from an incomplete graph structure through a bidirectional graph\nstructure augmentation scheme. This augmentation scheme mitigates the potential\ninformation loss that is commonly associated with various data augmentation\ntechniques involving raw graph data, such as edge perturbation, node removal,\nand attribute masking. Second, we propose a CGCL model that can learn invariant\ngraph representations. A cross-view training scheme is proposed to train the\nproposed CGCL model. This scheme attempts to maximize the consistency\ninformation between one augmented view and the graph structure reconstructed\nfrom the other augmented view. Furthermore, we offer a comprehensive\ntheoretical CGCL analysis. This paper empirically and experimentally\ndemonstrates the effectiveness of the proposed CGCL method, achieving\ncompetitive results on graph datasets in comparisons with several\nstate-of-the-art algorithms.",
            "author": [
                "Jie Chen",
                "Zhiming Li",
                "Hua Mao",
                "Wai Lok Woo",
                "Xi Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11821v1",
                "http://arxiv.org/pdf/2311.11821v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11817v1",
            "title": "Quantum Strategies for Rendezvous and Domination Tasks on Graphs with\n  Mobile Agents",
            "updated": "2023-11-20T14:54:37Z",
            "published": "2023-11-20T14:54:37Z",
            "summary": "This paper explores the application of quantum non-locality, a renowned and\nunique phenomenon acknowledged as a valuable resource. Focusing on a novel\napplication, we demonstrate its quantum advantage for mobile agents engaged in\nspecific distributed tasks without communication. The research addresses the\nsignificant challenge of rendezvous on graphs and introduces a new distributed\ntask for mobile agents grounded in the graph domination problem. Through an\ninvestigation across various graph scenarios, we showcase the quantum\nadvantage. Additionally, we scrutinize deterministic strategies, highlighting\ntheir comparatively lower efficiency compared to quantum strategies. The paper\nconcludes with a numerical analysis, providing further insights into our\nfindings.",
            "author": [
                "Giuseppe Viola",
                "Piotr Mironowicz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11817v1",
                "http://arxiv.org/pdf/2311.11817v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11799v1",
            "title": "A classification of Mengerian $4$-uniform hypergraphs derived from\n  graphs",
            "updated": "2023-11-20T14:32:39Z",
            "published": "2023-11-20T14:32:39Z",
            "summary": "In this paper, we give a classification of all Mengerian $4$-uniform\nhypergraphs derived from graphs.",
            "author": [
                "Winfried Hochst\u00e4ttler",
                "Mehrdad Nasernejad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11799v1",
                "http://arxiv.org/pdf/2311.11799v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C65"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11797v1",
            "title": "Igniting Language Intelligence: The Hitchhiker's Guide From\n  Chain-of-Thought Reasoning to Language Agents",
            "updated": "2023-11-20T14:30:55Z",
            "published": "2023-11-20T14:30:55Z",
            "summary": "Large language models (LLMs) have dramatically enhanced the field of language\nintelligence, as demonstrably evidenced by their formidable empirical\nperformance across a spectrum of complex reasoning tasks. Additionally,\ntheoretical proofs have illuminated their emergent reasoning capabilities,\nproviding a compelling showcase of their advanced cognitive abilities in\nlinguistic contexts. Critical to their remarkable efficacy in handling complex\nreasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning\ntechniques, obliging them to formulate intermediate steps en route to deriving\nan answer. The CoT reasoning approach has not only exhibited proficiency in\namplifying reasoning performance but also in enhancing interpretability,\ncontrollability, and flexibility. In light of these merits, recent research\nendeavors have extended CoT reasoning methodologies to nurture the development\nof autonomous language agents, which adeptly adhere to language instructions\nand execute actions within varied environments. This survey paper orchestrates\na thorough discourse, penetrating vital research dimensions, encompassing: (i)\nthe foundational mechanics of CoT techniques, with a focus on elucidating the\ncircumstances and justification behind its efficacy; (ii) the paradigm shift in\nCoT; and (iii) the burgeoning of language agents fortified by CoT approaches.\nProspective research avenues envelop explorations into generalization,\nefficiency, customization, scaling, and safety. This paper caters to a wide\naudience, including beginners seeking comprehensive knowledge of CoT reasoning\nand language agents, as well as experienced researchers interested in\nfoundational mechanics and engaging in cutting-edge discussions on these\ntopics. A repository for the related papers is available at\nhttps://github.com/Zoeyyao27/CoT-Igniting-Agent.",
            "author": [
                "Zhuosheng Zhang",
                "Yao Yao",
                "Aston Zhang",
                "Xiangru Tang",
                "Xinbei Ma",
                "Zhiwei He",
                "Yiming Wang",
                "Mark Gerstein",
                "Rui Wang",
                "Gongshen Liu",
                "Hai Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11797v1",
                "http://arxiv.org/pdf/2311.11797v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.CV",
                "cs.HC",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11796v1",
            "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI\n  Systems",
            "updated": "2023-11-20T14:29:45Z",
            "published": "2023-11-20T14:29:45Z",
            "summary": "Artificial Intelligence (AI) systems such as autonomous vehicles, facial\nrecognition, and speech recognition systems are increasingly integrated into\nour daily lives. However, despite their utility, these AI systems are\nvulnerable to a wide range of attacks such as adversarial, backdoor, data\npoisoning, membership inference, model inversion, and model stealing attacks.\nIn particular, numerous attacks are designed to target a particular model or\nsystem, yet their effects can spread to additional targets, referred to as\ntransferable attacks. Although considerable efforts have been directed toward\ndeveloping transferable attacks, a holistic understanding of the advancements\nin transferable attacks remains elusive. In this paper, we comprehensively\nexplore learning-based attacks from the perspective of transferability,\nparticularly within the context of cyber-physical security. We delve into\ndifferent domains -- the image, text, graph, audio, and video domains -- to\nhighlight the ubiquitous and pervasive nature of transferable attacks. This\npaper categorizes and reviews the architecture of existing attacks from various\nviewpoints: data, process, model, and system. We further examine the\nimplications of transferable attacks in practical scenarios such as autonomous\ndriving, speech recognition, and large language models (LLMs). Additionally, we\noutline the potential research directions to encourage efforts in exploring the\nlandscape of transferable attacks. This survey offers a holistic understanding\nof the prevailing transferable attacks and their impacts across different\ndomains.",
            "author": [
                "Guangjing Wang",
                "Ce Zhou",
                "Yuanda Wang",
                "Bocheng Chen",
                "Hanqing Guo",
                "Qiben Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11796v1",
                "http://arxiv.org/pdf/2311.11796v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.CL",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11794v1",
            "title": "Examples of deformed Spin(7)-instantons/Donaldson-Thomas connections",
            "updated": "2023-11-20T14:26:04Z",
            "published": "2023-11-20T14:26:04Z",
            "summary": "We construct examples of deformed Hermitian Yang-Mills connections and\ndeformed Spin(7)-instantons (also called Spin(7) deformed Donaldson-Thomas\nconnections) on the cotangent bundle of $\\mathbb{C}\\mathbb{P}^2$ endowed with\nthe Calabi hyperK\\\"ahler structure. Deformed Spin(7)-instantons on cones over\n3-Sasakian 7-manifolds are also constructed. We show that these can be used to\ndistinguish between isometric structures and also between Sp(2) and Spin(7)\nholonomy cones. To the best of our knowledge, these are the first non-trivial\nexamples of deformed Spin(7)-instantons.",
            "author": [
                "Udhav Fowdar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11794v1",
                "http://arxiv.org/pdf/2311.11794v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "53C07, 53C26, 53C29"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11793v1",
            "title": "Universal Optimality of Dijkstra via Beyond-Worst-Case Heaps",
            "updated": "2023-11-20T14:21:47Z",
            "published": "2023-11-20T14:21:47Z",
            "summary": "This paper proves that Dijkstra's shortest-path algorithm is universally\noptimal in both its running time and number of comparisons when combined with a\nsufficiently efficient heap data structure.\n  Universal optimality is a powerful beyond-worst-case performance guarantee\nfor graph algorithms that informally states that a single algorithm performs as\nwell as possible for every single graph topology. We give the first application\nof this notion to any sequential algorithm.\n  We design a new heap data structure with a working-set property guaranteeing\nthat the heap takes advantage of locality in heap operations. Our heap matches\nthe optimal (worst-case) bounds of Fibonacci heaps but also provides the\nbeyond-worst-case guarantee that the cost of extracting the minimum element is\nmerely logarithmic in the number of elements inserted after it instead of\nlogarithmic in the number of all elements in the heap. This makes the\nextraction of recently added elements cheaper.\n  We prove that our working-set property is sufficient to guarantee universal\noptimality, specifically, for the problem of ordering vertices by their\ndistance from the source vertex: The locality in the sequence of heap\noperations generated by any run of Dijkstra's algorithm on a fixed topology is\nstrong enough that one can couple the number of comparisons performed by any\nheap with our working-set property to the minimum number of comparisons\nrequired to solve the distance ordering problem on this topology.",
            "author": [
                "Bernhard Haeupler",
                "Richard Hlad\u00edk",
                "V\u00e1clav Rozho\u0148",
                "Robert Tarjan",
                "Jakub T\u011btek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11793v1",
                "http://arxiv.org/pdf/2311.11793v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11782v1",
            "title": "Robust Tumor Segmentation with Hyperspectral Imaging and Graph Neural\n  Networks",
            "updated": "2023-11-20T14:07:38Z",
            "published": "2023-11-20T14:07:38Z",
            "summary": "Segmenting the boundary between tumor and healthy tissue during surgical\ncancer resection poses a significant challenge. In recent years, Hyperspectral\nImaging (HSI) combined with Machine Learning (ML) has emerged as a promising\nsolution. However, due to the extensive information contained within the\nspectral domain, most ML approaches primarily classify individual HSI\n(super-)pixels, or tiles, without taking into account their spatial context. In\nthis paper, we propose an improved methodology that leverages the spatial\ncontext of tiles for more robust and smoother segmentation. To address the\nirregular shapes of tiles, we utilize Graph Neural Networks (GNNs) to propagate\ncontext information across neighboring regions. The features for each tile\nwithin the graph are extracted using a Convolutional Neural Network (CNN),\nwhich is trained simultaneously with the subsequent GNN. Moreover, we\nincorporate local image quality metrics into the loss function to enhance the\ntraining procedure's robustness against low-quality regions in the training\nimages. We demonstrate the superiority of our proposed method using a clinical\nex vivo dataset consisting of 51 HSI images from 30 patients. Despite the\nlimited dataset, the GNN-based model significantly outperforms context-agnostic\napproaches, accurately distinguishing between healthy and tumor tissues, even\nin images from previously unseen patients. Furthermore, we show that our\ncarefully designed loss function, accounting for local image quality, results\nin additional improvements. Our findings demonstrate that context-aware GNN\nalgorithms can robustly find tumor demarcations on HSI images, ultimately\ncontributing to better surgery success and patient outcome.",
            "author": [
                "Mayar Lotfy",
                "Anna Alperovich",
                "Tommaso Giannantonio",
                "Bjorn Barz",
                "Xiaohan Zhang",
                "Felix Holm",
                "Nassir Navab",
                "Felix Boehm",
                "Carolin Schwamborn",
                "Thomas K. Hoffmann",
                "Patrick J. Schuler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11782v1",
                "http://arxiv.org/pdf/2311.11782v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11759v1",
            "title": "Unveiling the Unseen Potential of Graph Learning through MLPs: Effective\n  Graph Learners Using Propagation-Embracing MLPs",
            "updated": "2023-11-20T13:39:19Z",
            "published": "2023-11-20T13:39:19Z",
            "summary": "Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve\nsemi-supervised node classification on graphs, by training a student MLP by\nknowledge distillation (KD) from a teacher graph neural network (GNN). While\nprevious studies have focused mostly on training the student MLP by matching\nthe output probability distributions between the teacher and student models\nduring KD, it has not been systematically studied how to inject the structural\ninformation in an explicit and interpretable manner. Inspired by GNNs that\nseparate feature transformation $T$ and propagation $\\Pi$, we re-frame the KD\nprocess as enabling the student MLP to explicitly learn both $T$ and $\\Pi$.\nAlthough this can be achieved by applying the inverse propagation $\\Pi^{-1}$\nbefore distillation from the teacher GNN, it still comes with a high\ncomputational cost from large matrix multiplications during training. To solve\nthis problem, we propose Propagate & Distill (P&D), which propagates the output\nof the teacher GNN before KD and can be interpreted as an approximate process\nof the inverse propagation $\\Pi^{-1}$. Through comprehensive evaluations using\nreal-world benchmark datasets, we demonstrate the effectiveness of P&D by\nshowing further performance boost of the student MLP.",
            "author": [
                "Yong-Min Shin",
                "Won-Yong Shin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11759v1",
                "http://arxiv.org/pdf/2311.11759v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.IT",
                "cs.NE",
                "cs.SI",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14727v1",
            "title": "Optimal Strategies to Perform Multilingual Analysis of Social Content\n  for a Novel Dataset in the Tourism Domain",
            "updated": "2023-11-20T13:08:21Z",
            "published": "2023-11-20T13:08:21Z",
            "summary": "The rising influence of social media platforms in various domains, including\ntourism, has highlighted the growing need for efficient and automated natural\nlanguage processing (NLP) approaches to take advantage of this valuable\nresource. However, the transformation of multilingual, unstructured, and\ninformal texts into structured knowledge often poses significant challenges.\n  In this work, we evaluate and compare few-shot, pattern-exploiting and\nfine-tuning machine learning techniques on large multilingual language models\n(LLMs) to establish the best strategy to address the lack of annotated data for\n3 common NLP tasks in the tourism domain: (1) Sentiment Analysis, (2) Named\nEntity Recognition, and (3) Fine-grained Thematic Concept Extraction (linked to\na semantic resource). Furthermore, we aim to ascertain the quantity of\nannotated examples required to achieve good performance in those 3 tasks,\naddressing a common challenge encountered by NLP researchers in the\nconstruction of domain-specific datasets.\n  Extensive experimentation on a newly collected and annotated multilingual\n(French, English, and Spanish) dataset composed of tourism-related tweets shows\nthat current few-shot learning techniques allow us to obtain competitive\nresults for all three tasks with very little annotation data: 5 tweets per\nlabel (15 in total) for Sentiment Analysis, 10% of the tweets for location\ndetection (around 160) and 13% (200 approx.) of the tweets annotated with\nthematic concepts, a highly fine-grained sequence labeling task based on an\ninventory of 315 classes.\n  This comparative analysis, grounded in a novel dataset, paves the way for\napplying NLP to new domain-specific applications, reducing the need for manual\nannotations and circumventing the complexities of rule-based, ad hoc solutions.",
            "author": [
                "Maxime Masson",
                "Rodrigo Agerri",
                "Christian Sallaberry",
                "Marie-Noelle Bessagnet",
                "Annig Le Parc Lacayrelle",
                "Philippe Roose"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14727v1",
                "http://arxiv.org/pdf/2311.14727v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11738v1",
            "title": "Linearity and Local Correctness in Weighted Colourings of Random Graphs",
            "updated": "2023-11-20T12:59:46Z",
            "published": "2023-11-20T12:59:46Z",
            "summary": "In this paper, we consider a weighted generalization of the chromatic number\nof a Binomial random graph~\\(G.\\) We equip each edge with a random weight and\nthen colour the vertices in such a way that the absolute colour difference\nbetween any two adjacent vertices is at least as large as their edge weight. We\nshow that with high probability, the weighted colouring number grows linearly\nwith the maximum vertex degree if the edge weights have sufficiently large\nmoments. Conversely, if the edge weight moments are unbounded then the weighted\nchromatic number is much larger than the maximum vertex degree, with high\nprobability. We also obtain a sharp threshold result for locally correct\nweighted colourings for balanced subgraphs of~\\(G.\\)",
            "author": [
                "Ghurumuruhan Ganesan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11738v1",
                "http://arxiv.org/pdf/2311.11738v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11733v1",
            "title": "Linearity Property of Unique Colourings in Random Graphs",
            "updated": "2023-11-20T12:56:45Z",
            "published": "2023-11-20T12:56:45Z",
            "summary": "In this paper, we study unique colourings in random graphs as a\ngeneralization of both conflict-free and injective colourings. Specifically, we\nimpose the condition that a fraction of vertices in the neighbourhood of any\nvertex are assigned unique colours and use vertex partitioning and the\nprobabilistic method to show that the minimum number of colours needed grows\nlinearly with the uniqueness parameter, unlike both conflict-free and injective\ncolourings. We argue how the unboundedness of the vertex neighbourhoods\ninfluences the linearity property and illustrate our case with an example\ninvolving treeunique colourings in random graphs.",
            "author": [
                "Ghurumuruhan Ganesan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11733v1",
                "http://arxiv.org/pdf/2311.11733v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11728v1",
            "title": "Critical Exponent for the Acyclic Chromatic Number of Random Graphs",
            "updated": "2023-11-20T12:47:57Z",
            "published": "2023-11-20T12:47:57Z",
            "summary": "In this paper we study acyclic colouring in the random subgraph $\\mathit{G}$\nof the complete graph $\\mathit{K}_n$ on $\\mathit{n}$ vertices where each edge\nis present with probability $\\mathit{p}$; independent of the other edges. We\nshow that the acyclic chromatic number exhibits a phase transition from\nsublinear to linear growth as the edge probability increases, even in the\nsparse regime and obtain estimates for the critical exponent. Next, we\nintroduce a relaxation by allowing for a small fraction of \"bad\" cycles to\nviolate the acyclic colouring condition and show that the critical exponent in\nthis case is in fact zero, no matter how small the fraction.",
            "author": [
                "Ghurumuruhan Ganesan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11728v1",
                "http://arxiv.org/pdf/2311.11728v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11707v1",
            "title": "Configuring an heterogeneous smartgrid network: complexity and\n  approximations for tree topologies",
            "updated": "2023-11-20T12:22:26Z",
            "published": "2023-11-20T12:22:26Z",
            "summary": "We address the problem of configuring a power distribution network with\nreliability and resilience objectives by satisfying the demands of the\nconsumers and saturating each production source as little as possible. We\nconsider power distribution networks containing source nodes producing\nelectricity, nodes representing electricity consumers and switches between\nthem. Configuring this network consists in deciding the orientation of the\nlinks between the nodes of the network. The electric flow is a direct\nconsequence of the chosen configuration and can be computed in polynomial time.\nIt is valid if it satisfies the demand of each consumer and capacity\nconstraints on the network. In such a case, we study the problem of determining\na feasible solution that balances the loads of the sources, that is their\nproduction rates. We use three metrics to measure the quality of a solution:\nminimizing the maximum load, maximizing the minimum load and minimizing the\ndifference of the maximum and the minimum loads. This defines optimization\nproblems called respectively min-M, max-m and min-R. In the case where the\ngraph of the network is a tree, it is known that the problem of building a\nvalid configuration is polynomial. We show the three optimization variants have\ndistinct properties regarding the theoretical complexity and the\napproximability. Particularly, we show that min-M is polynomial, that max-m is\nNP-Hard but belongs to the class FPTAS and that min-R is NP-Hard, cannot 1 be\napproximated to within any exponential relative ratio but, for any $\\epsilon$ >\n0, there exists an algorithm for which the value of the returned solution\nequals the value of an optimal solution shifted by at most $\\epsilon$.",
            "author": [
                "Dominique Barth",
                "Thierry Mautor",
                "Dimitri Watel",
                "Marc-Antoine Weisser"
            ],
            "link": [
                "http://dx.doi.org/10.1007/s10898-023-01338-0",
                "http://arxiv.org/abs/2311.11707v1",
                "http://arxiv.org/pdf/2311.11707v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11705v1",
            "title": "Trust-based Approaches Towards Enhancing IoT Security: A Systematic\n  Literature Review",
            "updated": "2023-11-20T12:21:35Z",
            "published": "2023-11-20T12:21:35Z",
            "summary": "The continuous rise in the adoption of emerging technologies such as Internet\nof Things (IoT) by businesses has brought unprecedented opportunities for\ninnovation and growth. However, due to the distinct characteristics of these\nemerging IoT technologies like real-time data processing, Self-configuration,\ninteroperability, and scalability, they have also introduced some unique\ncybersecurity challenges, such as malware attacks, advanced persistent threats\n(APTs), DoS /DDoS (Denial of Service & Distributed Denial of Service attacks)\nand insider threats. As a result of these challenges, there is an increased\nneed for improved cybersecurity approaches and efficient management solutions\nto ensure the privacy and security of communication within IoT networks. One\nproposed security approach is the utilization of trust-based systems and is the\nfocus of this study. This research paper presents a systematic literature\nreview on the Trust-based cybersecurity security approaches for IoT. A total of\n23 articles were identified that satisfy the review criteria. We highlighted\nthe common trust-based mitigation techniques in existence for dealing with\nthese threats and grouped them into three major categories, namely:\nObservation-Based, Knowledge-Based & Cluster-Based systems. Finally, several\nopen issues were highlighted, and future research directions presented.",
            "author": [
                "Oghenetejiri Okporokpo",
                "Funminiyi Olajide",
                "Nemitari Ajienka",
                "Xiaoqi Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11705v1",
                "http://arxiv.org/pdf/2311.11705v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11701v1",
            "title": "Control in Hybrid Chatbots",
            "updated": "2023-11-20T12:08:32Z",
            "published": "2023-11-20T12:08:32Z",
            "summary": "Customer data typically is held in database systems, which can be seen as\nrule-based knowledge base, whereas businesses increasingly want to benefit from\nthe capabilities of large, pre-trained language models.\n  In this technical report, we describe a case study of how a commercial rule\nengine and an integrated neural chatbot may be integrated, and what level of\ncontrol that particular integration mode leads to. We also discuss alternative\nways (including past ways realized in other systems) how researchers strive to\nmaintain control and avoid what has recently been called model \"hallucination\".",
            "author": [
                "Thomas R\u00fcdel",
                "Jochen L. Leidner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11701v1",
                "http://arxiv.org/pdf/2311.11701v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL",
                "cs.HC",
                "68T50, 68T07",
                "I.2.7; H.3.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11693v1",
            "title": "Confluence Graphs of Unitals",
            "updated": "2023-11-20T11:45:08Z",
            "published": "2023-11-20T11:45:08Z",
            "summary": "We show that the cliques of maximal size in the confluence graph of an\narbitrary unital of order $q>2$ have size $q^2$, and that these cliques are the\npencils of all blocks through a given point. This solves the Erd\\H{o}s-Ko-Rado\nproblem for all unitals. We also determine all maximal cliques of the\nconfluence graph of the Hermitian unitals. As an application, we show that the\nconfluence graph of an arbitrary unital unambiguously determines the unital.\nAlong the way, we show that each linear space with $q^2$ points such that the\nsizes of both point rows and line pencils are bounded above by $q+1$ embeds in\na projective plane of order $q$.",
            "author": [
                "Theo Grundh\u00f6fer",
                "Markus J. Stroppel",
                "Hendrik Van Maldeghem"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11693v1",
                "http://arxiv.org/pdf/2311.11693v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05B05 05B25 05E30 51A45 51E10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11691v1",
            "title": "Towards Robust Text Retrieval with Progressive Learning",
            "updated": "2023-11-20T11:44:01Z",
            "published": "2023-11-20T11:44:01Z",
            "summary": "Retrieval augmentation has become an effective solution to empower large\nlanguage models (LLMs) with external and verified knowledge sources from the\ndatabase, which overcomes the limitations and hallucinations of LLMs in\nhandling up-to-date and domain-specific information. However, existing\nembedding models for text retrieval usually have three non-negligible\nlimitations. First, the number and diversity of samples in a batch are too\nrestricted to supervise the modeling of textual nuances at scale. Second, the\nhigh proportional noise are detrimental to the semantic correctness and\nconsistency of embeddings. Third, the equal treatment to easy and difficult\nsamples would cause sub-optimum convergence of embeddings with poorer\ngeneralization. In this paper, we propose the PEG, a progressively learned\nembeddings for robust text retrieval. Specifically, we increase the training\nin-batch negative samples to 80,000, and for each query, we extracted five hard\nnegatives. Concurrently, we incorporated a progressive learning mechanism,\nenabling the model to dynamically modulate its attention to the samples\nthroughout the entire training process. Additionally, PEG is trained on more\nthan 100 million data, encompassing a wide range of domains (e.g., finance,\nmedicine, and tourism) and covering various tasks (e.g., question-answering,\nmachine reading comprehension, and similarity matching). Extensive experiments\nconducted on C-MTEB and DuReader demonstrate that PEG surpasses\nstate-of-the-art embeddings in retrieving true positives, highlighting its\nsignificant potential for applications in LLMs. Our model is publicly available\nat https://huggingface.co/TownsWu/PEG.",
            "author": [
                "Tong Wu",
                "Yulei Qin",
                "Enwei Zhang",
                "Zihan Xu",
                "Yuting Gao",
                "Ke Li",
                "Xing Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11691v1",
                "http://arxiv.org/pdf/2311.11691v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11689v1",
            "title": "Causal Structure Learning Supervised by Large Language Model",
            "updated": "2023-11-20T11:43:20Z",
            "published": "2023-11-20T11:43:20Z",
            "summary": "Causal discovery from observational data is pivotal for deciphering complex\nrelationships. Causal Structure Learning (CSL), which focuses on deriving\ncausal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast\nDAG spaces and data sparsity. The integration of Large Language Models (LLMs),\nrecognized for their causal reasoning capabilities, offers a promising\ndirection to enhance CSL by infusing it with knowledge-based causal inferences.\nHowever, existing approaches utilizing LLMs for CSL have encountered issues,\nincluding unreliable constraints from imperfect LLM inferences and the\ncomputational intensity of full pairwise variable analyses. In response, we\nintroduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL\ninnovatively integrates LLM-based causal inference with CSL in an iterative\nprocess, refining the causal DAG using feedback from LLMs. This method not only\nutilizes LLM resources more efficiently but also generates more robust and\nhigh-quality structural constraints compared to previous methodologies. Our\ncomprehensive evaluation across eight real-world datasets demonstrates\nILS-CSL's superior performance, setting a new standard in CSL efficacy and\nshowcasing its potential to significantly advance the field of causal\ndiscovery. The codes are available at\n\\url{https://github.com/tyMadara/ILS-CSL}.",
            "author": [
                "Taiyu Ban",
                "Lyuzhou Chen",
                "Derui Lyu",
                "Xiangyu Wang",
                "Huanhuan Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11689v1",
                "http://arxiv.org/pdf/2311.11689v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11682v1",
            "title": "Combinatorial spectra of graphs",
            "updated": "2023-11-20T11:27:18Z",
            "published": "2023-11-20T11:27:18Z",
            "summary": "In this article we are introducing combinatorial spectra of graphs, this is a\ngeneralization of $H$-Hamiltonian spectra. The main motivation was to made from\n$H$-Hamiltonian spectra an operation and develop some algebra in this field. An\nimproved version of this operation form a commutative monoid. The most\nimportant thing is that most of the basic concepts of graph theory, such as\nmaximum pairing, vertex and edge connectivity and coloring, Ramsey numbers,\nisomorphisms and regularity, can be expressed in the language of this\noperation.",
            "author": [
                "Martin Dz\u00farik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11682v1",
                "http://arxiv.org/pdf/2311.11682v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.RA",
                "05C12, 05C15, 05C40, 05C25",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12085v1",
            "title": "Pyramid Diffusion for Fine 3D Large Scene Generation",
            "updated": "2023-11-20T11:24:21Z",
            "published": "2023-11-20T11:24:21Z",
            "summary": "Directly transferring the 2D techniques to 3D scene generation is challenging\ndue to significant resolution reduction and the scarcity of comprehensive\nreal-world 3D scene datasets. To address these issues, our work introduces the\nPyramid Discrete Diffusion model (PDD) for 3D scene generation. This novel\napproach employs a multi-scale model capable of progressively generating\nhigh-quality 3D scenes from coarse to fine. In this way, the PDD can generate\nhigh-quality scenes within limited resource constraints and does not require\nadditional data sources. To the best of our knowledge, we are the first to\nadopt the simple but effective coarse-to-fine strategy for 3D large scene\ngeneration. Our experiments, covering both unconditional and conditional\ngeneration, have yielded impressive results, showcasing the model's\neffectiveness and robustness in generating realistic and detailed 3D scenes.\nOur code will be available to the public.",
            "author": [
                "Yuheng Liu",
                "Xinke Li",
                "Xueting Li",
                "Lu Qi",
                "Chongshou Li",
                "Ming-Hsuan Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12085v1",
                "http://arxiv.org/pdf/2311.12085v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11657v1",
            "title": "Minimax Two-Stage Gradient Boosting for Parameter Estimation",
            "updated": "2023-11-20T10:48:39Z",
            "published": "2023-11-20T10:48:39Z",
            "summary": "Parameter estimation is an important sub-field in statistics and system\nidentification. Various methods for parameter estimation have been proposed in\nthe literature, among which the Two-Stage (TS) approach is particularly\npromising, due to its ease of implementation and reliable estimates. Among the\ndifferent statistical frameworks used to derive TS estimators, the min-max\nframework is attractive due to its mild dependence on prior knowledge about the\nparameters to be estimated. However, the existing implementation of the minimax\nTS approach has currently limited applicability, due to its heavy computational\nload. In this paper, we overcome this difficulty by using a gradient boosting\nmachine (GBM) in the second stage of TS approach. We call the resulting\nalgorithm the Two-Stage Gradient Boosting Machine (TSGBM) estimator. Finally,\nwe test our proposed TSGBM estimator on several numerical examples including\nmodels of dynamical systems.",
            "author": [
                "Braghadeesh Lakshminarayanan",
                "Cristian R. Rojas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11657v1",
                "http://arxiv.org/pdf/2311.11657v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11654v1",
            "title": "A Hands-On Guide to Shear Force Mixing of Single-Walled Carbon Nanotubes\n  with Conjugated Polymers",
            "updated": "2023-11-20T10:41:44Z",
            "published": "2023-11-20T10:41:44Z",
            "summary": "This guide provides a detailed step-by-step procedure for the dispersion of\n(6,5) single-walled carbon nanotubes by shear force mixing with the conjugated\npolymer PFO-BPy in organic solvents. All processes presented here were\ndeveloped in the Zaumseil group at Heidelberg University since 2015 and\nrepresent best practices to the best of our knowledge. In addition to the\ndetailed instructions, we discuss potential pitfalls and problems, that we have\nencountered over eight years of operation and show how to solve them. This also\nincludes a detailed description of how to maintain and service a shear force\nmixer to ensure long operation lifetime. Finally, we show how to expand our\nprocess to the dispersion other nanotube chiralities in electronic-grade\nquality and how to treat dispersions for subsequent processing (e.g., thin film\ndeposition or functionalization).",
            "author": [
                "Sebastian Lindenthal",
                "Simon Settele",
                "Joshua Hellmann",
                "Klaus Schmitt",
                "Jana Zaumseil"
            ],
            "link": [
                "http://dx.doi.org/10.11588/heidok.00033977",
                "http://arxiv.org/abs/2311.11654v1",
                "http://arxiv.org/pdf/2311.11654v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mtrl-sci",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16171v1",
            "title": "Multi-Agent Learning of Efficient Fulfilment and Routing Strategies in\n  E-Commerce",
            "updated": "2023-11-20T10:32:28Z",
            "published": "2023-11-20T10:32:28Z",
            "summary": "This paper presents an integrated algorithmic framework for minimising\nproduct delivery costs in e-commerce (known as the cost-to-serve or C2S). One\nof the major challenges in e-commerce is the large volume of spatio-temporally\ndiverse orders from multiple customers, each of which has to be fulfilled from\none of several warehouses using a fleet of vehicles. This results in two levels\nof decision-making: (i) selection of a fulfillment node for each order\n(including the option of deferral to a future time), and then (ii) routing of\nvehicles (each of which can carry multiple orders originating from the same\nwarehouse). We propose an approach that combines graph neural networks and\nreinforcement learning to train the node selection and vehicle routing agents.\nWe include real-world constraints such as warehouse inventory capacity, vehicle\ncharacteristics such as travel times, service times, carrying capacity, and\ncustomer constraints including time windows for delivery. The complexity of\nthis problem arises from the fact that outcomes (rewards) are driven both by\nthe fulfillment node mapping as well as the routing algorithms, and are\nspatio-temporally distributed. Our experiments show that this algorithmic\npipeline outperforms pure heuristic policies.",
            "author": [
                "Omkar Shelke",
                "Pranavi Pathakota",
                "Anandsingh Chauhan",
                "Harshad Khadilkar",
                "Hardik Meisheri",
                "Balaraman Ravindran"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16171v1",
                "http://arxiv.org/pdf/2311.16171v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11646v1",
            "title": "CastDet: Toward Open Vocabulary Aerial Object Detection with\n  CLIP-Activated Student-Teacher Learning",
            "updated": "2023-11-20T10:26:04Z",
            "published": "2023-11-20T10:26:04Z",
            "summary": "Object detection in aerial images is a pivotal task for various earth\nobservation applications, whereas current algorithms learn to detect only a\npre-defined set of object categories demanding sufficient bounding-box\nannotated training samples and fail to detect novel object categories. In this\npaper, we consider open-vocabulary object detection (OVD) in aerial images that\nenables the characterization of new objects beyond training categories on the\nearth surface without annotating training images for these new categories. The\nperformance of OVD depends on the quality of class-agnostic region proposals\nand pseudo-labels that can generalize well to novel object categories. To\nsimultaneously generate high-quality proposals and pseudo-labels, we propose\nCastDet, a CLIP-activated student-teacher open-vocabulary object Detection\nframework. Our end-to-end framework within the student-teacher mechanism\nemploys the CLIP model as an extra omniscient teacher of rich knowledge into\nthe student-teacher self-learning process. By doing so, our approach boosts\nnovel object proposals and classification. Furthermore, we design a dynamic\nlabel queue technique to maintain high-quality pseudo labels during batch\ntraining and mitigate label imbalance. We conduct extensive experiments on\nmultiple existing aerial object detection datasets, which are set up for the\nOVD task. Experimental results demonstrate our CastDet achieving superior\nopen-vocabulary detection performance, e.g., reaching 40.0 HM (Harmonic Mean),\nwhich outperforms previous methods Detic/ViLD by 26.9/21.1 on the VisDroneZSD\ndataset.",
            "author": [
                "Yan Li",
                "Weiwei Guo",
                "Dunyun He",
                "Jiaqi Zhou",
                "Yuze Gao",
                "Wenxian Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11646v1",
                "http://arxiv.org/pdf/2311.11646v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14725v1",
            "title": "Unsupervised learning of site percolation based on shuffled\n  configurations",
            "updated": "2023-11-20T10:21:50Z",
            "published": "2023-11-20T10:21:50Z",
            "summary": "In the field of statistical physics, machine learning has gained significant\npopularity and has achieved remarkable results in recent studies on phase\ntransitions.In this paper, we apply Principal Component Analysis (PCA) and\nAutoencoder(AE) based on Unsupervised learning to study the various\nconfigurations of the percolation model in equilibrium phase transition. In\ncertain phase transition models, such as the DP model in non-equilibrium phase\ntransitions, the order parameter is particle density. However, in some other\nphase transition models, such as the percolation model, it is not. This study\ninvolved randomizing and selecting percolation graphs to be used as input for a\nneural network, and analyzed the obtained results, indicating that the outputs\nof the single latent variable of AE and the first principal component of PCA\nare signals related to particle density.",
            "author": [
                "Dian Xu",
                "Shanshan Wang",
                "Feng Gao",
                "Wei Li",
                "Jianmin Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14725v1",
                "http://arxiv.org/pdf/2311.14725v1"
            ],
            "primary_category": "cond-mat.stat-mech",
            "category": [
                "cond-mat.stat-mech",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11638v1",
            "title": "Reti-Diff: Illumination Degradation Image Restoration with Retinex-based\n  Latent Diffusion Model",
            "updated": "2023-11-20T09:55:06Z",
            "published": "2023-11-20T09:55:06Z",
            "summary": "Illumination degradation image restoration (IDIR) techniques aim to improve\nthe visibility of degraded images and mitigate the adverse effects of\ndeteriorated illumination. Among these algorithms, diffusion model (DM)-based\nmethods have shown promising performance but are often burdened by heavy\ncomputational demands and pixel misalignment issues when predicting the\nimage-level distribution. To tackle these problems, we propose to leverage DM\nwithin a compact latent space to generate concise guidance priors and introduce\na novel solution called Reti-Diff for the IDIR task. Reti-Diff comprises two\nkey components: the Retinex-based latent DM (RLDM) and the Retinex-guided\ntransformer (RGformer). To ensure detailed reconstruction and illumination\ncorrection, RLDM is empowered to acquire Retinex knowledge and extract\nreflectance and illumination priors. These priors are subsequently utilized by\nRGformer to guide the decomposition of image features into their respective\nreflectance and illumination components. Following this, RGformer further\nenhances and consolidates the decomposed features, resulting in the production\nof refined images with consistent content and robustness to handle complex\ndegradation scenarios. Extensive experiments show that Reti-Diff outperforms\nexisting methods on three IDIR tasks, as well as downstream applications. Code\nwill be available at \\url{https://github.com/ChunmingHe/Reti-Diff}.",
            "author": [
                "Chunming He",
                "Chengyu Fang",
                "Yulun Zhang",
                "Kai Li",
                "Longxiang Tang",
                "Chenyu You",
                "Fengyang Xiao",
                "Zhenhua Guo",
                "Xiu Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11638v1",
                "http://arxiv.org/pdf/2311.11638v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11626v1",
            "title": "A novel transformer-based approach for soil temperature prediction",
            "updated": "2023-11-20T09:20:26Z",
            "published": "2023-11-20T09:20:26Z",
            "summary": "Soil temperature is one of the most significant parameters that plays a\ncrucial role in glacier energy, dynamics of mass balance, processes of surface\nhydrological, coaction of glacier-atmosphere, nutrient cycling, ecological\nstability, the management of soil, water, and field crop. In this work, we\nintroduce a novel approach using transformer models for the purpose of\nforecasting soil temperature prediction. To the best of our knowledge, the\nusage of transformer models in this work is the very first attempt to predict\nsoil temperature. Experiments are carried out using six different FLUXNET\nstations by modeling them with five different transformer models, namely,\nVanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To\ndemonstrate the effectiveness of the proposed model, experiment results are\ncompared with both deep learning approaches and literature studies. Experiment\nresults show that the utilization of transformer models ensures a significant\ncontribution to the literature, thence determining the new state-of-the-art.",
            "author": [
                "Muhammet Mucahit Enes Yurtsever",
                "Ayhan Kucukmanisa",
                "Zeynep Hilal Kilimci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11626v1",
                "http://arxiv.org/pdf/2311.11626v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CE",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11604v1",
            "title": "CurriculumLoc: Enhancing Cross-Domain Geolocalization through\n  Multi-Stage Refinement",
            "updated": "2023-11-20T08:40:01Z",
            "published": "2023-11-20T08:40:01Z",
            "summary": "Visual geolocalization is a cost-effective and scalable task that involves\nmatching one or more query images, taken at some unknown location, to a set of\ngeo-tagged reference images. Existing methods, devoted to semantic features\nrepresentation, evolving towards robustness to a wide variety between query and\nreference, including illumination and viewpoint changes, as well as scale and\nseasonal variations. However, practical visual geolocalization approaches need\nto be robust in appearance changing and extreme viewpoint variation conditions,\nwhile providing accurate global location estimates. Therefore, inspired by\ncurriculum design, human learn general knowledge first and then delve into\nprofessional expertise. We first recognize semantic scene and then measure\ngeometric structure. Our approach, termed CurriculumLoc, involves a delicate\ndesign of multi-stage refinement pipeline and a novel keypoint detection and\ndescription with global semantic awareness and local geometric verification. We\nrerank candidates and solve a particular cross-domain perspective-n-point (PnP)\nproblem based on these keypoints and corresponding descriptors, position\nrefinement occurs incrementally. The extensive experimental results on our\ncollected dataset, TerraTrack and a benchmark dataset, ALTO, demonstrate that\nour approach results in the aforementioned desirable characteristics of a\npractical visual geolocalization solution. Additionally, we achieve new high\nrecall@1 scores of 62.6% and 94.5% on ALTO, with two different distances\nmetrics, respectively. Dataset, code and trained models are publicly available\non https://github.com/npupilab/CurriculumLoc.",
            "author": [
                "Boni Hu",
                "Lin Chen",
                "Runjian Chen",
                "Shuhui Bu",
                "Pengcheng Han",
                "Haowei Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11604v1",
                "http://arxiv.org/pdf/2311.11604v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11598v1",
            "title": "Filling the Image Information Gap for VQA: Prompting Large Language\n  Models to Proactively Ask Questions",
            "updated": "2023-11-20T08:23:39Z",
            "published": "2023-11-20T08:23:39Z",
            "summary": "Large Language Models (LLMs) demonstrate impressive reasoning ability and the\nmaintenance of world knowledge not only in natural language tasks, but also in\nsome vision-language tasks such as open-domain knowledge-based visual question\nanswering (OK-VQA). As images are invisible to LLMs, researchers convert images\nto text to engage LLMs into the visual question reasoning procedure. This leads\nto discrepancies between images and their textual representations presented to\nLLMs, which consequently impedes final reasoning performance. To fill the\ninformation gap and better leverage the reasoning capability, we design a\nframework that enables LLMs to proactively ask relevant questions to unveil\nmore details in the image, along with filters for refining the generated\ninformation. We validate our idea on OK-VQA and A-OKVQA. Our method\ncontinuously boosts the performance of baselines methods by an average gain of\n2.15% on OK-VQA, and achieves consistent improvements across different LLMs.",
            "author": [
                "Ziyue Wang",
                "Chi Chen",
                "Peng Li",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11598v1",
                "http://arxiv.org/pdf/2311.11598v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12079v1",
            "title": "FreeKD: Knowledge Distillation via Semantic Frequency Prompt",
            "updated": "2023-11-20T08:06:41Z",
            "published": "2023-11-20T08:06:41Z",
            "summary": "Knowledge distillation (KD) has been applied to various tasks successfully,\nand mainstream methods typically boost the student model via spatial imitation\nlosses. However, the consecutive downsamplings induced in the spatial domain of\nteacher model is a type of corruption, hindering the student from analyzing\nwhat specific information needs to be imitated, which results in accuracy\ndegradation. To better understand the underlying pattern of corrupted feature\nmaps, we shift our attention to the frequency domain. During frequency\ndistillation, we encounter a new challenge: the low-frequency bands convey\ngeneral but minimal context, while the high are more informative but also\nintroduce noise. Not each pixel within the frequency bands contributes equally\nto the performance. To address the above problem: (1) We propose the Frequency\nPrompt plugged into the teacher model, absorbing the semantic frequency context\nduring finetuning. (2) During the distillation period, a pixel-wise frequency\nmask is generated via Frequency Prompt, to localize those pixel of interests\n(PoIs) in various frequency bands. Additionally, we employ a position-aware\nrelational frequency loss for dense prediction tasks, delivering a high-order\nspatial enhancement to the student model. We dub our Frequency Knowledge\nDistillation method as FreeKD, which determines the optimal localization and\nextent for the frequency distillation. Extensive experiments demonstrate that\nFreeKD not only outperforms spatial-based distillation methods consistently on\ndense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on\nCOCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys\nmore robustness to the student. Notably, we also validate the generalization of\nour approach on large-scale vision models (e.g., DINO and SAM).",
            "author": [
                "Yuan Zhang",
                "Tao Huang",
                "Jiaming Liu",
                "Tao Jiang",
                "Kuan Cheng",
                "Shanghang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12079v1",
                "http://arxiv.org/pdf/2311.12079v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11591v1",
            "title": "DesignGPT: Multi-Agent Collaboration in Design",
            "updated": "2023-11-20T08:05:52Z",
            "published": "2023-11-20T08:05:52Z",
            "summary": "Generative AI faces many challenges when entering the product design\nworkflow, such as interface usability and interaction patterns. Therefore,\nbased on design thinking and design process, we developed the DesignGPT\nmulti-agent collaboration framework, which uses artificial intelligence agents\nto simulate the roles of different positions in the design company and allows\nhuman designers to collaborate with them in natural language. Experimental\nresults show that compared with separate AI tools, DesignGPT improves the\nperformance of designers, highlighting the potential of applying multi-agent\nsystems that integrate design domain knowledge to product scheme design.",
            "author": [
                "Shiying Ding",
                "Xinyi Chen",
                "Yan Fang",
                "Wenrui Liu",
                "Yiwu Qiu",
                "Chunlei Chai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11591v1",
                "http://arxiv.org/pdf/2311.11591v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11571v1",
            "title": "VyZX: Formal Verification of a Graphical Quantum Language",
            "updated": "2023-11-20T07:12:22Z",
            "published": "2023-11-20T07:12:22Z",
            "summary": "Mathematical representations of graphs often resemble adjacency matrices or\nlists, representations that facilitate whiteboard reasoning and algorithm\ndesign. In the realm of proof assistants, inductive representations effectively\ndefine semantics for formal reasoning. This highlights a gap where algorithm\ndesign and proof assistants require a fundamentally different structure of\ngraphs, particularly for process theories which represent programs using\ngraphs. To address this gap, we present VyZX, a verified library for reasoning\nabout inductively defined graphical languages. These inductive constructs arise\nnaturally from category theory definitions. A key goal for VyZX is to Verify\nthe ZX-calculus, a graphical language for reasoning about quantum computation.\nThe ZX-calculus comes with a collection of diagrammatic rewrite rules that\npreserve the graph's semantic interpretation. We show how inductive graphs in\nVyZX are used to prove the correctness of the ZX-calculus rewrite rules and\napply them in practice using standard proof assistant techniques. VyZX\nintegrates easily with the proof engineer's workflow through visualization and\nautomation.",
            "author": [
                "Adrian Lehmann",
                "Ben Caldwell",
                "Bhakti Shah",
                "Robert Rand"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11571v1",
                "http://arxiv.org/pdf/2311.11571v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11564v1",
            "title": "KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained\n  Language Model",
            "updated": "2023-11-20T07:02:35Z",
            "published": "2023-11-20T07:02:35Z",
            "summary": "Most biomedical pretrained language models are monolingual and cannot handle\nthe growing cross-lingual requirements. The scarcity of non-English domain\ncorpora, not to mention parallel data, poses a significant hurdle in training\nmultilingual biomedical models. Since knowledge forms the core of\ndomain-specific corpora and can be translated into various languages\naccurately, we propose a model called KBioXLM, which transforms the\nmultilingual pretrained model XLM-R into the biomedical domain using a\nknowledge-anchored approach. We achieve a biomedical multilingual corpus by\nincorporating three granularity knowledge alignments (entity, fact, and passage\nlevels) into monolingual corpora. Then we design three corresponding training\ntasks (entity masking, relation masking, and passage relation prediction) and\ncontinue training on top of the XLM-R model to enhance its domain cross-lingual\nability. To validate the effectiveness of our model, we translate the English\nbenchmarks of multiple tasks into Chinese. Experimental results demonstrate\nthat our model significantly outperforms monolingual and multilingual\npretrained models in cross-lingual zero-shot and few-shot scenarios, achieving\nimprovements of up to 10+ points. Our code is publicly available at\nhttps://github.com/ngwlh-gl/KBioXLM.",
            "author": [
                "Lei Geng",
                "Xu Yan",
                "Ziqiang Cao",
                "Juntao Li",
                "Wenjie Li",
                "Sujian Li",
                "Xinjie Zhou",
                "Yang Yang",
                "Jun Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11564v1",
                "http://arxiv.org/pdf/2311.11564v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11553v1",
            "title": "Hartree-Fock-Bogoliubov theory for number-parity--violating fermionic\n  Hamiltonians",
            "updated": "2023-11-20T06:08:54Z",
            "published": "2023-11-20T06:08:54Z",
            "summary": "It is usually asserted that physical Hamiltonians for fermions must contain\nan even number of fermion operators. This is indeed true in electronic\nstructure theory. However, when the Jordan-Wigner transformation is used to map\nspin Hamiltonians to Hamiltonians of spinless fermions, terms which contain an\nodd number of fermion operators may appear. The resulting fermionic Hamiltonian\nthus does not have number parity symmetry, and requires wave functions which do\nnot have this symmetry either. In this work, we discuss the extension of\nstandard Hartree-Fock-Bogoliubov theory to the number-parity--nonconserving\ncase. These ideas had appeared in the literature before but, perhaps for lack\nof practical application, had to the best of our knowledge never been applied.\nWe also show how using these unusual mean-field states can provide significant\nimprovements when studying the Jordan-Wigner transformation of chemically\nrelevant spin Hamiltonians.",
            "author": [
                "Thomas M. Henderson",
                "Shadan Ghassemi Tabrizi",
                "Guo P. Chen",
                "Gustavo E. Scuseria"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11553v1",
                "http://arxiv.org/pdf/2311.11553v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11551v1",
            "title": "Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context\n  Learning",
            "updated": "2023-11-20T06:06:20Z",
            "published": "2023-11-20T06:06:20Z",
            "summary": "Large language models (LLMs) have showcased their capability with few-shot\ninference known as in-context learning. However, in-domain demonstrations are\nnot always readily available in real scenarios, leading to cross-domain\nin-context learning. Besides, LLMs are still facing challenges in long-tail\nknowledge in unseen and unfamiliar domains. The above limitations demonstrate\nthe necessity of Unsupervised Domain Adaptation (UDA). In this paper, we study\nthe UDA problem under an in-context learning setting to adapt language models\nfrom the source domain to the target domain without any target labels. The core\nidea is to retrieve a subset of cross-domain elements that are the most similar\nto the query, and elicit language model to adapt in an in-context manner by\nlearning both target domain distribution and the discriminative task signal\nsimultaneously with the augmented cross-domain in-context examples. We devise\ndifferent prompting and training strategies, accounting for different LM\narchitectures to learn the target distribution via language modeling. With\nextensive experiments on Sentiment Analysis (SA) and Named Entity Recognition\n(NER) tasks, we thoroughly study the effectiveness of ICL for domain transfer\nand demonstrate significant improvements over baseline models.",
            "author": [
                "Quanyu Long",
                "Wenya Wang",
                "Sinno Jialin Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11551v1",
                "http://arxiv.org/pdf/2311.11551v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11536v2",
            "title": "The graph limit for a pairwise competition model",
            "updated": "2023-11-23T22:37:55Z",
            "published": "2023-11-20T04:53:49Z",
            "summary": "This paper is aimed at extending the graph limit with time dependent weights\nobtained in [1] for the case of a pairwise competition model introduced in\n[10], in which the equation governing the weights involves a weak singularity\nat the origin. Well posedness for the graph limit equation associated with the\nODE system of the pairwise competition model is also proved.",
            "author": [
                "Immanuel Ben-Porat",
                "Jos\u00e9 A. Carrillo",
                "Pierre-Emmanuel Jabin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11536v2",
                "http://arxiv.org/pdf/2311.11536v2"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11530v1",
            "title": "Symmetry and asymmetry between positive and negative square energies of\n  graphs",
            "updated": "2023-11-20T04:27:33Z",
            "published": "2023-11-20T04:27:33Z",
            "summary": "The positive and negative square energies of a graph, $s^+(G)$ and $s^-(G)$,\nare the sums of squares of the positive and negative eigenvalues of the\nadjacency matrix, respectively. The first results on square energies revealed\nsymmetry between $s^+(G)$ and $s^-(G)$. This paper reviews examples of\nasymmetry between these parameters, for example using large random graphs and\nthe ratios $s^+/s^-$ and $s^-/s^+$, as well as new examples of symmetry.",
            "author": [
                "Clive Elphick",
                "William Linz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11530v1",
                "http://arxiv.org/pdf/2311.11530v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11525v1",
            "title": "Generalized Category Discovery in Semantic Segmentation",
            "updated": "2023-11-20T04:11:16Z",
            "published": "2023-11-20T04:11:16Z",
            "summary": "This paper explores a novel setting called Generalized Category Discovery in\nSemantic Segmentation (GCDSS), aiming to segment unlabeled images given prior\nknowledge from a labeled set of base classes. The unlabeled images contain\npixels of the base class or novel class. In contrast to Novel Category\nDiscovery in Semantic Segmentation (NCDSS), there is no prerequisite for prior\nknowledge mandating the existence of at least one novel class in each unlabeled\nimage. Besides, we broaden the segmentation scope beyond foreground objects to\ninclude the entire image. Existing NCDSS methods rely on the aforementioned\npriors, making them challenging to truly apply in real-world situations. We\npropose a straightforward yet effective framework that reinterprets the GCDSS\nchallenge as a task of mask classification. Additionally, we construct a\nbaseline method and introduce the Neighborhood Relations-Guided Mask Clustering\nAlgorithm (NeRG-MaskCA) for mask categorization to address the fragmentation in\nsemantic representation. A benchmark dataset, Cityscapes-GCD, derived from the\nCityscapes dataset, is established to evaluate the GCDSS framework. Our method\ndemonstrates the feasibility of the GCDSS problem and the potential for\ndiscovering and segmenting novel object classes in unlabeled images. We employ\nthe generated pseudo-labels from our approach as ground truth to supervise the\ntraining of other models, thereby enabling them with the ability to segment\nnovel classes. It paves the way for further research in generalized category\ndiscovery, broadening the horizons of semantic segmentation and its\napplications. For details, please visit https://github.com/JethroPeng/GCDSS",
            "author": [
                "Zhengyuan Peng",
                "Qijian Tian",
                "Jianqing Xu",
                "Yizhang Jin",
                "Xuequan Lu",
                "Xin Tan",
                "Yuan Xie",
                "Lizhuang Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11525v1",
                "http://arxiv.org/pdf/2311.11525v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12076v1",
            "title": "Towards Few-shot Out-of-Distribution Detection",
            "updated": "2023-11-20T03:51:58Z",
            "published": "2023-11-20T03:51:58Z",
            "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof open-world intelligent systems. Despite the notable advancements in existing\nOOD detection methodologies, our study identifies a significant performance\ndrop under the scarcity of training samples. In this context, we introduce a\nnovel few-shot OOD detection benchmark, carefully constructed to address this\ngap. Our empirical analysis reveals the superiority of ParameterEfficient\nFine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter\ntuning, over conventional techniques, including fully fine-tuning and linear\nprobing tuning in the few-shot OOD detection task. Recognizing some crucial\ninformation from the pre-trained model, which is pivotal for OOD detection, may\nbe lost during the fine-tuning process, we propose a method termed\nDomainSpecific and General Knowledge Fusion (DSGF). This approach is designed\nto be compatible with diverse fine-tuning frameworks. Our experiments show that\nthe integration of DSGF significantly enhances the few-shot OOD detection\ncapabilities across various methods and fine-tuning methodologies, including\nfully fine-tuning, visual adapter tuning, and visual prompt tuning. The code\nwill be released.",
            "author": [
                "Jiuqing Dong",
                "Yongbin Gao",
                "Heng Zhou",
                "Jun Cen",
                "Yifan Yao",
                "Sook Yoon",
                "Park Dong Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12076v1",
                "http://arxiv.org/pdf/2311.12076v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11503v2",
            "title": "A Case for Synthesis of Recursive Quantum Unitary Programs",
            "updated": "2023-12-05T22:49:07Z",
            "published": "2023-11-20T03:01:36Z",
            "summary": "Quantum programs are notoriously difficult to code and verify due to\nunintuitive quantum knowledge associated with quantum programming. Automated\ntools relieving the tedium and errors associated with low-level quantum details\nwould hence be highly desirable. In this paper, we initiate the study of\nprogram synthesis for quantum unitary programs that recursively define a family\nof unitary circuits for different input sizes, which are widely used in\nexisting quantum programming languages. Specifically, we present QSynth, the\nfirst quantum program synthesis framework, including a new inductive quantum\nprogramming language, its specification, a sound logic for reasoning, and an\nencoding of the reasoning procedure into SMT instances. By leveraging existing\nSMT solvers, QSynth successfully synthesizes ten quantum unitary programs\nincluding quantum adder circuits, quantum eigenvalue inversion circuits and\nQuantum Fourier Transformation, which can be readily transpiled to executable\nprograms on major quantum platforms, e.g., Q#, IBM Qiskit, and AWS Braket.",
            "author": [
                "Haowei Deng",
                "Runzhou Tao",
                "Yuxiang Peng",
                "Xiaodi Wu"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3632901",
                "http://arxiv.org/abs/2311.11503v2",
                "http://arxiv.org/pdf/2311.11503v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11501v1",
            "title": "MultiLoRA: Democratizing LoRA for Better Multi-Task Learning",
            "updated": "2023-11-20T02:59:18Z",
            "published": "2023-11-20T02:59:18Z",
            "summary": "LoRA achieves remarkable resource efficiency and comparable performance when\nadapting LLMs for specific tasks. Since ChatGPT demonstrated superior\nperformance on various tasks, there has been a growing desire to adapt one\nmodel for all tasks. However, the explicit low-rank of LoRA limits the\nadaptation performance in complex multi-task scenarios. LoRA is dominated by a\nsmall number of top singular vectors while fine-tuning decomposes into a set of\nless important unitary transforms. In this paper, we propose MultiLoRA for\nbetter multi-task adaptation by reducing the dominance of top singular vectors\nobserved in LoRA. MultiLoRA scales LoRA modules horizontally and change\nparameter initialization of adaptation matrices to reduce parameter dependency,\nthus yields more balanced unitary subspaces. We unprecedentedly construct\nspecialized training data by mixing datasets of instruction follow, natural\nlanguage understanding, world knowledge, to cover semantically and\nsyntactically different samples. With only 2.5% of additional parameters,\nMultiLoRA outperforms single LoRA counterparts and fine-tuning on multiple\nbenchmarks and model scales. Further investigation into weight update matrices\nof MultiLoRA exhibits reduced dependency on top singular vectors and more\ndemocratic unitary transform contributions.",
            "author": [
                "Yiming Wang",
                "Yu Lin",
                "Xiaodong Zeng",
                "Guannan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11501v1",
                "http://arxiv.org/pdf/2311.11501v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11485v1",
            "title": "An NMF-Based Building Block for Interpretable Neural Networks With\n  Continual Learning",
            "updated": "2023-11-20T02:00:33Z",
            "published": "2023-11-20T02:00:33Z",
            "summary": "Existing learning methods often struggle to balance interpretability and\npredictive performance. While models like nearest neighbors and non-negative\nmatrix factorization (NMF) offer high interpretability, their predictive\nperformance on supervised learning tasks is often limited. In contrast, neural\nnetworks based on the multi-layer perceptron (MLP) support the modular\nconstruction of expressive architectures and tend to have better recognition\naccuracy but are often regarded as black boxes in terms of interpretability.\nOur approach aims to strike a better balance between these two aspects through\nthe use of a building block based on NMF that incorporates supervised neural\nnetwork training methods to achieve high predictive performance while retaining\nthe desirable interpretability properties of NMF. We evaluate our Predictive\nFactorized Coupling (PFC) block on small datasets and show that it achieves\ncompetitive predictive performance with MLPs while also offering improved\ninterpretability. We demonstrate the benefits of this approach in various\nscenarios, such as continual learning, training on non-i.i.d. data, and\nknowledge removal after training. Additionally, we show examples of using the\nPFC block to build more expressive architectures, including a fully-connected\nresidual network as well as a factorized recurrent neural network (RNN) that\nperforms competitively with vanilla RNNs while providing improved\ninterpretability. The PFC block uses an iterative inference algorithm that\nconverges to a fixed point, making it possible to trade off accuracy vs\ncomputation after training but also currently preventing its use as a general\nMLP replacement in some scenarios such as training on very large datasets. We\nprovide source code at https://github.com/bkvogel/pfc",
            "author": [
                "Brian K. Vogel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11485v1",
                "http://arxiv.org/pdf/2311.11485v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11473v1",
            "title": "CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection",
            "updated": "2023-11-20T00:57:30Z",
            "published": "2023-11-20T00:57:30Z",
            "summary": "Graph Neural Networks (GNNs) have emerged as a powerful tool for\nrepresentation learning on graphs, but they often suffer from overfitting and\nlabel noise issues, especially when the data is scarce or imbalanced. Different\nfrom the paradigm of previous methods that rely on single-node confidence, in\nthis paper, we introduce a novel Class-wise Selection for Graph Neural\nNetworks, dubbed CSGNN, which employs a neighbor-aggregated latent space to\nadaptively select reliable nodes across different classes. Specifically, 1) to\ntackle the class imbalance issue, we introduce a dynamic class-wise selection\nmechanism, leveraging the clustering technique to identify clean nodes based on\nthe neighbor-aggregated confidences. In this way, our approach can avoid the\npitfalls of biased sampling which is common with global threshold techniques.\n2) To alleviate the problem of noisy labels, built on the concept of the\nmemorization effect, CSGNN prioritizes learning from clean nodes before noisy\nones, thereby iteratively enhancing model performance while mitigating label\nnoise. Through extensive experiments, we demonstrate that CSGNN outperforms\nstate-of-the-art methods in terms of both effectiveness and robustness.",
            "author": [
                "Yifan Li",
                "Zhen Tan",
                "Kai Shu",
                "Zongsheng Cao",
                "Yu Kong",
                "Huan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11473v1",
                "http://arxiv.org/pdf/2311.11473v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11472v1",
            "title": "Portable, Efficient, and Practical Library-Level Choreographic\n  Programming",
            "updated": "2023-11-20T00:55:24Z",
            "published": "2023-11-20T00:55:24Z",
            "summary": "Choreographic programming (CP) is an emerging paradigm for programming\ndistributed applications that run on multiple nodes. In CP, the programmer\nwrites one program, called a choreography, that is then transformed to\nindividual programs for each node via a compilation step called endpoint\nprojection (EPP). While CP languages have existed for over a decade,\nlibrary-level CP -- in which choreographies are expressed as programs in an\nexisting host language, and choreographic language constructs and EPP are\nprovided entirely by a host-language library -- is in its infancy.\nLibrary-level CP has great potential, but existing implementations have\nportability, efficiency, and practicality drawbacks that hinder its adoption.\n  In this paper, we aim to advance the state of the art of library-level CP\nwith two novel techniques for choreographic library design and implementation:\nendpoint projection as dependency injection (EPP-as-DI), and choreographic\nenclaves. EPP-as-DI is a language-agnostic technique for implementing EPP at\nthe library level. Unlike existing library-level approaches, EPP-as-DI asks\nlittle from the host language -- support for higher-order functions is all that\nis required -- making it usable in a wide variety of host languages.\nChoreographic enclaves are a language feature that lets the programmer define\nsub-choreographies within a larger choreography. Within an enclave, \"knowledge\nof choice\" is propagated only among the enclave's participants, enabling the\nseamless use of the host language's conditional constructs while addressing the\nefficiency limitations of existing library-level CP implementations.\n  We implement EPP-as-DI and choreographic enclaves in ChoRus, the first CP\nlibrary for the Rust programming language. Our case studies and benchmarks\ndemonstrate that the usability and performance of ChoRus compares favorably to\ntraditional distributed programming in Rust.",
            "author": [
                "Shun Kashiwa",
                "Gan Shen",
                "Soroush Zare",
                "Lindsey Kuper"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11472v1",
                "http://arxiv.org/pdf/2311.11472v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11468v1",
            "title": "Bounding Lifts of Markoff Triples mod $p$",
            "updated": "2023-11-20T00:36:01Z",
            "published": "2023-11-20T00:36:01Z",
            "summary": "In 2016, Bourgain, Gamburd, and Sarnak proved that Strong Approximation holds\nfor the Markoff surface in most cases. That is, the modulo $p$ solutions to the\nequation $X_1^2+X_2^2+X_3^2=3X_1X_2X_3$ are covered by the integer solutions\nfor most primes $p$. In this paper, we provide upper bounds on lifts of mod $p$\npoints of the Markoff surface by analyzing the growth along paths in the\nMarkoff mod $p$ graphs. Our first upper bound follows the algorithm given in\nthe paper of Bourgain, Gamburd, and Sarnak, which constructs a path of possibly\nlong length but where points grow relatively slowly. Our second bound considers\npaths in these graphs of short length but possibly large growth. We then\nprovide numerical evidence and heuristic arguments for how these bounds might\nbe improved.",
            "author": [
                "Elisa Bellah",
                "Siran Chen",
                "Elena Fuchs",
                "Lynnelle Ye"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11468v1",
                "http://arxiv.org/pdf/2311.11468v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11462v2",
            "title": "LLM aided semi-supervision for Extractive Dialog Summarization",
            "updated": "2023-11-23T12:52:05Z",
            "published": "2023-11-19T23:59:22Z",
            "summary": "Generating high-quality summaries for chat dialogs often requires large\nlabeled datasets. We propose a method to efficiently use unlabeled data for\nextractive summarization of customer-agent dialogs. In our method, we frame\nsummarization as a question-answering problem and use state-of-the-art large\nlanguage models (LLMs) to generate pseudo-labels for a dialog. We then use\nthese pseudo-labels to fine-tune a chat summarization model, effectively\ntransferring knowledge from the large LLM into a smaller specialized model. We\ndemonstrate our method on the \\tweetsumm dataset, and show that using 10% of\nthe original labelled data set we can achieve 65.9/57.0/61.0 ROUGE-1/-2/-L,\nwhereas the current state-of-the-art trained on the entire training data set\nobtains 65.16/55.81/64.37 ROUGE-1/-2/-L. In other words, in the worst case\n(i.e., ROUGE-L) we still effectively retain 94.7% of the performance while\nusing only 10% of the data.",
            "author": [
                "Nishant Mishra",
                "Gaurav Sahu",
                "Iacer Calixto",
                "Ameen Abu-Hanna",
                "Issam H. Laradji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11462v2",
                "http://arxiv.org/pdf/2311.11462v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11441v1",
            "title": "Spot the Bot: Distinguishing Human-Written and Bot-Generated Texts Using\n  Clustering and Information Theory Techniques",
            "updated": "2023-11-19T22:29:15Z",
            "published": "2023-11-19T22:29:15Z",
            "summary": "With the development of generative models like GPT-3, it is increasingly more\nchallenging to differentiate generated texts from human-written ones. There is\na large number of studies that have demonstrated good results in bot\nidentification. However, the majority of such works depend on supervised\nlearning methods that require labelled data and/or prior knowledge about the\nbot-model architecture. In this work, we propose a bot identification algorithm\nthat is based on unsupervised learning techniques and does not depend on a\nlarge amount of labelled data. By combining findings in semantic analysis by\nclustering (crisp and fuzzy) and information techniques, we construct a robust\nmodel that detects a generated text for different types of bot. We find that\nthe generated texts tend to be more chaotic while literary works are more\ncomplex. We also demonstrate that the clustering of human texts results in\nfuzzier clusters in comparison to the more compact and well-separated clusters\nof bot-generated texts.",
            "author": [
                "Vasilii Gromov",
                "Quynh Nhu Dang"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-45170-6_3",
                "http://arxiv.org/abs/2311.11441v1",
                "http://arxiv.org/pdf/2311.11441v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12072v1",
            "title": "Multifractal characterisation of particulate matter (PM10) time series\n  in the Caribbean basin using visibility graphs",
            "updated": "2023-11-19T20:51:30Z",
            "published": "2023-11-19T20:51:30Z",
            "summary": "A good knowledge of pollutant time series behavior is fundamental to\nelaborate strategies and construct tools to protect human health. In Caribbean\narea, air quality is frequently deteriorated by the transport of African dust.\nIn the literature, it is well known that exposure to particulate matter with an\naerodynamic diameter of 10 {\\mu}m or less (PM10) have many adverse health\neffects as respiratory and cardiovascular diseases. To our knowledge, no study\nhas yet performed an analysis of PM10 time series using complex network\nframework. In this study, the so-called Visibility Graph (VG) method is used to\ndescribe PM10 dynamics in Guadeloupe archipelago with a database of 11 years.\nFirstly, the fractal nature of PM10 time series is highlighted using degree\ndistribution for all data, low dust season (October to April) and high dust\nseason (May to September). Thereafter, a profound description of PM10 time\nseries dynamics is made using multifractal analysis through two approaches,\ni.e. Renyi and singularity spectra. Achieved results are consistent with PM10\nbehavior in the Caribbean basin. Both methods showed a higher multifractality\ndegree during the low dust season. In addition, multifractal parameters\nexhibited that the low dust season has the higher recurrence and the lower\nuniformity degrees. Lastly, centrality measures (degree, closeness and\nbetweenness) highlighted PM10 dynamics through the year with a decay of\ncentrality values during the high dust season. To conclude, all these results\nclearly showed that VG is a robust tool to describe times series properties.",
            "author": [
                "T. Plocoste",
                "R. Carmona-Cabezas",
                "F. J. Jimenez-Hornero",
                "E. Gutierrez de Rave",
                "R. Calif"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.apr.2020.08.027",
                "http://arxiv.org/abs/2311.12072v1",
                "http://arxiv.org/pdf/2311.12072v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11399v1",
            "title": "On a metric view of the polynomial shift locus",
            "updated": "2023-11-19T18:48:19Z",
            "published": "2023-11-19T18:48:19Z",
            "summary": "We relate generic points in the shift locus $\\mathcal{S}_D$ of degree $D\\ge\n2$ polynomials to metric graphs. Using thermodynamic metrics on the space of\nmetric graphs, we obtain a distance function $\\rho_D$ on $\\mathcal{S}_D$. We\nstudy the (in)completeness of the metric space $(\\mathcal{S}_D, \\rho_D)$. We\nprove that when $D \\ge 3$, the space $(\\mathcal{S}_D, \\rho_D)$ is incomplete\nand its metric completion contains a subset homeomorphic to the space\n$\\mathbb{P}\\mathcal{ST}_D^*$ introduced by DeMarco and Pilgrim. This provides a\nnew way to understand the space $\\mathbb{P}\\mathcal{ST}_D^*$.",
            "author": [
                "Yan Mary He",
                "Hongming Nie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11399v1",
                "http://arxiv.org/pdf/2311.11399v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11375v1",
            "title": "ML-LMCL: Mutual Learning and Large-Margin Contrastive Learning for\n  Improving ASR Robustness in Spoken Language Understanding",
            "updated": "2023-11-19T16:53:35Z",
            "published": "2023-11-19T16:53:35Z",
            "summary": "Spoken language understanding (SLU) is a fundamental task in the\ntask-oriented dialogue systems. However, the inevitable errors from automatic\nspeech recognition (ASR) usually impair the understanding performance and lead\nto error propagation. Although there are some attempts to address this problem\nthrough contrastive learning, they (1) treat clean manual transcripts and ASR\ntranscripts equally without discrimination in fine-tuning; (2) neglect the fact\nthat the semantically similar pairs are still pushed away when applying\ncontrastive learning; (3) suffer from the problem of Kullback-Leibler (KL)\nvanishing. In this paper, we propose Mutual Learning and Large-Margin\nContrastive Learning (ML-LMCL), a novel framework for improving ASR robustness\nin SLU. Specifically, in fine-tuning, we apply mutual learning and train two\nSLU models on the manual transcripts and the ASR transcripts, respectively,\naiming to iteratively share knowledge between these two models. We also\nintroduce a distance polarization regularizer to avoid pushing away the\nintra-cluster pairs as much as possible. Moreover, we use a cyclical annealing\nschedule to mitigate KL vanishing issue. Experiments on three datasets show\nthat ML-LMCL outperforms existing models and achieves new state-of-the-art\nperformance.",
            "author": [
                "Xuxin Cheng",
                "Bowen Cao",
                "Qichen Ye",
                "Zhihong Zhu",
                "Hongxiang Li",
                "Yuexian Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11375v1",
                "http://arxiv.org/pdf/2311.11375v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11373v1",
            "title": "Ion Dynamics Across a Low Mach Number Bow Shock",
            "updated": "2023-11-19T16:52:40Z",
            "published": "2023-11-19T16:52:40Z",
            "summary": "A thorough understanding of collisionless shocks requires knowledge of how\ndifferent ion species are accelerated across the shock. We investigate a bow\nshock crossing using the Magnetospheric Multiscale spacecraft after a coronal\nmass ejection crossed Earth, which led to solar wind consisting of protons,\nalpha particles, and singly charge helium ions. The low Mach number of the bow\nshock enabled the ions to be distinguished upstream and sometimes downstream of\nthe shock. Some of the protons are specularly reflected and produce\nquasi-periodic fine structures in the velocity distribution functions\ndownstream of the shock. Heavier ions are shown to transit the shock without\nreflection. However, the gyromotion of the heavier ions partially obscures the\nfine structure of proton distributions. Additionally, the calculated proton\nmoments are unreliable when the different ion species are not distinguished by\nthe particle detector. The need to high time-resolution mass-resolving ion\ndetectors when investigating collisionless shocks is discussed.",
            "author": [
                "D. B. Graham",
                "Yu. V. Khotyaintsev",
                "A. P. Dimmock",
                "A. Lalti",
                "J. J. Boldu",
                "S. F. Tigik",
                "S. A. Fuselier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11373v1",
                "http://arxiv.org/pdf/2311.11373v1"
            ],
            "primary_category": "physics.space-ph",
            "category": [
                "physics.space-ph",
                "physics.plasm-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11371v1",
            "title": "SOccDPT: Semi-Supervised 3D Semantic Occupancy from Dense Prediction\n  Transformers trained under memory constraints",
            "updated": "2023-11-19T16:47:51Z",
            "published": "2023-11-19T16:47:51Z",
            "summary": "We present SOccDPT, a memory-efficient approach for 3D semantic occupancy\nprediction from monocular image input using dense prediction transformers. To\naddress the limitations of existing methods trained on structured traffic\ndatasets, we train our model on unstructured datasets including the Indian\nDriving Dataset and Bengaluru Driving Dataset. Our semi-supervised training\npipeline allows SOccDPT to learn from datasets with limited labels by reducing\nthe requirement for manual labelling by substituting it with pseudo-ground\ntruth labels to produce our Bengaluru Semantic Occupancy Dataset. This broader\ntraining enhances our model's ability to handle unstructured traffic scenarios\neffectively. To overcome memory limitations during training, we introduce\npatch-wise training where we select a subset of parameters to train each epoch,\nreducing memory usage during auto-grad graph construction. In the context of\nunstructured traffic and memory-constrained training and inference, SOccDPT\noutperforms existing disparity estimation approaches as shown by the RMSE score\nof 9.1473, achieves a semantic segmentation IoU score of 46.02% and operates at\na competitive frequency of 69.47 Hz. We make our code and semantic occupancy\ndataset public.",
            "author": [
                "Aditya Nalgunda Ganesh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11371v1",
                "http://arxiv.org/pdf/2311.11371v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11368v1",
            "title": "Self-Supervised Pretraining for Heterogeneous Hypergraph Neural Networks",
            "updated": "2023-11-19T16:34:56Z",
            "published": "2023-11-19T16:34:56Z",
            "summary": "Recently, pretraining methods for the Graph Neural Networks (GNNs) have been\nsuccessful at learning effective representations from unlabeled graph data.\nHowever, most of these methods rely on pairwise relations in the graph and do\nnot capture the underling higher-order relations between entities. Hypergraphs\nare versatile and expressive structures that can effectively model higher-order\nrelationships among entities in the data. Despite the efforts to adapt GNNs to\nhypergraphs (HyperGNN), there are currently no fully self-supervised\npretraining methods for HyperGNN on heterogeneous hypergraphs. In this paper,\nwe present SPHH, a novel self-supervised pretraining framework for\nheterogeneous HyperGNNs. Our method is able to effectively capture higher-order\nrelations among entities in the data in a self-supervised manner. SPHH is\nconsist of two self-supervised pretraining tasks that aim to simultaneously\nlearn both local and global representations of the entities in the hypergraph\nby using informative representations derived from the hypergraph structure.\nOverall, our work presents a significant advancement in the field of\nself-supervised pretraining of HyperGNNs, and has the potential to improve the\nperformance of various graph-based downstream tasks such as node classification\nand link prediction tasks which are mapped to hypergraph configuration. Our\nexperiments on two real-world benchmarks using four different HyperGNN models\nshow that our proposed SPHH framework consistently outperforms state-of-the-art\nbaselines in various downstream tasks. The results demonstrate that SPHH is\nable to improve the performance of various HyperGNN models in various\ndownstream tasks, regardless of their architecture or complexity, which\nhighlights the robustness of our framework.",
            "author": [
                "Abdalgader Abubaker",
                "Takanori Maehara",
                "Madhav Nimishakavi",
                "Vassilis Plachouras"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11368v1",
                "http://arxiv.org/pdf/2311.11368v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14722v1",
            "title": "Zero-Shot Question Answering over Financial Documents using Large\n  Language Models",
            "updated": "2023-11-19T16:23:34Z",
            "published": "2023-11-19T16:23:34Z",
            "summary": "We introduce a large language model (LLM) based approach to answer complex\nquestions requiring multi-hop numerical reasoning over financial reports. While\nLLMs have exhibited remarkable performance on various natural language and\nreasoning tasks, complex reasoning problems often rely on few-shot prompts that\nrequire carefully crafted examples. In contrast, our approach uses novel\nzero-shot prompts that guide the LLM to encode the required reasoning into a\nPython program or a domain specific language. The generated program is then\nexecuted by a program interpreter, thus mitigating the limitations of LLM in\nperforming accurate arithmetic calculations.\n  We evaluate the proposed approach on three financial datasets using some of\nthe recently developed generative pretrained transformer (GPT) models and\nperform comparisons with various zero-shot baselines. The experimental results\ndemonstrate that our approach significantly improves the accuracy for all the\nLLMs over their respective baselines. We provide a detailed analysis of the\nresults, generating insights to support our findings. The success of our\napproach demonstrates the enormous potential to extract complex domain specific\nnumerical reasoning by designing zero-shot prompts to effectively exploit the\nknowledge embedded in LLMs.",
            "author": [
                "Karmvir Singh Phogat",
                "Chetan Harsha",
                "Sridhar Dasaratha",
                "Shashishekar Ramakrishna",
                "Sai Akhil Puranam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14722v1",
                "http://arxiv.org/pdf/2311.14722v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11364v1",
            "title": "Local environment-based machine learning for molecular adsorption energy\n  prediction",
            "updated": "2023-11-19T16:20:28Z",
            "published": "2023-11-19T16:20:28Z",
            "summary": "Most machine learning (ML) models in Materials Science are developed by\nglobal geometric features, often falling short in describing localized\ncharacteristics, like molecular adsorption on materials. In this study, we\nintroduce a local environment framework that extracts local features from\ncrystal structures to portray the environment surrounding specific adsorption\nsites. Upon OC20 database (~20,000 3D entries), we apply our local environment\nframework on several ML models, such as random forest, convolutional neural\nnetwork, and graph neural network. It is found that our framework achieves\nremarkable prediction accuracy in predicting molecular adsorption energy,\nsignificantly outperforming other examined global-environment-based models.\nMoreover, the employment of this framework reduces data requirements and\naugments computational speed, specifically for deep learning algorithms.\nFinally, we directly apply our Local Environment ResNet (LERN) on a small\n2DMatPedia database (~2,000 2D entries), which also achieves highly accurate\nprediction, demonstrating the model transferability and remarkable data\nefficiency. Overall, the prediction accuracy, data-utilization efficiency, and\ntransferability of our local-environment-based ML framework hold a promising\nhigh applicability across a broad molecular adsorption field, such as catalysis\nand sensor technologies.",
            "author": [
                "Yifan Li",
                "Yihan Wu",
                "Yuhang Han",
                "Qujie Lyu",
                "Hao Wu",
                "Xiuying Zhang",
                "Lei Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11364v1",
                "http://arxiv.org/pdf/2311.11364v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11347v1",
            "title": "Large-scale Mixed Traffic Control Using Dynamic Vehicle Routing and\n  Privacy-Preserving Crowdsourcing",
            "updated": "2023-11-19T15:18:28Z",
            "published": "2023-11-19T15:18:28Z",
            "summary": "Controlling and coordinating urban traffic flow through robot vehicles is\nemerging as a novel transportation paradigm for the future. While this approach\ngarners growing attention from researchers and practitioners, effectively\nmanaging and coordinating large-scale mixed traffic remains a challenge. We\nintroduce an effective framework for large-scale mixed traffic control via\nprivacy-preserving crowdsourcing and dynamic vehicle routing. Our framework\nconsists of three modules: a privacy-protecting crowdsensing method, a graph\npropagation-based traffic forecasting method, and a privacy-preserving route\nselection mechanism. We evaluate our framework using a real-world road network.\nThe results show that our framework accurately forecasts traffic flow,\nefficiently mitigates network-wide RV shortage issue, and coordinates\nlarge-scale mixed traffic. Compared to other baseline methods, our framework\nnot only reduces the RV shortage issue up to 69.4% but also reduces the average\nwaiting time of all vehicles in the network up to 27%.",
            "author": [
                "Dawei Wang",
                "Weizi Li",
                "Jia Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11347v1",
                "http://arxiv.org/pdf/2311.11347v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11342v1",
            "title": "On the Communication Complexity of Decentralized Bilevel Optimization",
            "updated": "2023-11-19T14:56:26Z",
            "published": "2023-11-19T14:56:26Z",
            "summary": "Decentralized bilevel optimization has been actively studied in the past few\nyears since it has widespread applications in machine learning. However,\nexisting algorithms suffer from large communication complexity caused by the\nestimation of stochastic hypergradient, limiting their application to\nreal-world tasks. To address this issue, we develop a novel decentralized\nstochastic bilevel gradient descent algorithm under the heterogeneous setting,\nwhich enjoys a small communication cost in each round and small communication\nrounds. As such, it can achieve a much better communication complexity than\nexisting algorithms. Moreover, we extend our algorithm to the more challenging\ndecentralized multi-level optimization. To the best of our knowledge, this is\nthe first time achieving these theoretical results under the heterogeneous\nsetting. At last, the experimental results confirm the efficacy of our\nalgorithm.",
            "author": [
                "Yihan Zhang",
                "My T. Thai",
                "Jie Wu",
                "Hongchang Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11342v1",
                "http://arxiv.org/pdf/2311.11342v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11334v1",
            "title": "Using Causal Threads to Explain Changes in a Dynamic System",
            "updated": "2023-11-19T14:32:06Z",
            "published": "2023-11-19T14:32:06Z",
            "summary": "We explore developing rich semantic models of systems. Specifically, we\nconsider structured causal explanations about state changes in those systems.\nEssentially, we are developing process-based dynamic knowledge graphs. As an\nexample, we construct a model of the causal threads for geological changes\nproposed by the Snowball Earth theory. Further, we describe an early prototype\nof a graphical interface to present the explanations. Unlike statistical\napproaches to summarization and explanation such as Large Language Models\n(LLMs), our approach of direct representation can be inspected and verified\ndirectly.",
            "author": [
                "Robert B. Allen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11334v1",
                "http://arxiv.org/pdf/2311.11334v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11332v1",
            "title": "Improved Approximation Algorithms for Cycle and Path Packings",
            "updated": "2023-11-19T14:18:30Z",
            "published": "2023-11-19T14:18:30Z",
            "summary": "Given an edge-weighted (metric/general) complete graph with $n$ vertices, the\nmaximum weight (metric/general) $k$-cycle/path packing problem is to find a set\nof $\\frac{n}{k}$ vertex-disjoint $k$-cycles/paths such that the total weight is\nmaximized. In this paper, we consider approximation algorithms. For metric\n$k$-cycle packing, we improve the previous approximation ratio from $3/5$ to\n$7/10$ for $k=5$, and from $7/8\\cdot(1-1/k)^2$ for $k>5$ to\n$(7/8-0.125/k)(1-1/k)$ for constant odd $k>5$ and to $7/8\\cdot\n(1-1/k+\\frac{1}{k(k-1)})$ for even $k>5$. For metric $k$-path packing, we\nimprove the approximation ratio from $7/8\\cdot (1-1/k)$ to\n$\\frac{27k^2-48k+16}{32k^2-36k-24}$ for even $10\\geq k\\geq 6$. For the case of\n$k=4$, we improve the approximation ratio from $3/4$ to $5/6$ for metric\n4-cycle packing, from $2/3$ to $3/4$ for general 4-cycle packing, and from\n$3/4$ to $14/17$ for metric 4-path packing.",
            "author": [
                "Jingyang Zhao",
                "Mingyu Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11332v1",
                "http://arxiv.org/pdf/2311.11332v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11323v1",
            "title": "Structure and substructure connectivity of folded divide-and-swap cube",
            "updated": "2023-11-19T13:34:45Z",
            "published": "2023-11-19T13:34:45Z",
            "summary": "Let $ H $ be a connected subgraph of a graph $ G $. The structure\nconnectivity of $ G $, denoted by $ \\kappa(G;H) $, is the minimum number of a\nset of connected subgraphs in $ G $, whose removal disconnects $ G $ and each\nelement in the set is isomorphic to $ H $. The substructure connectivity of $ G\n$, denoted by $ \\kappa^s(G;H) $, is the minimum number of a set of connected\nsubgraphs in $ G $, whose removal disconnects $ G $ and each element in the set\nis isomorphic to a connected subgraph of $ H $. In this paper, we determine $ H\n$-structure connectivity and $ H $-substructure connectivity of folded\ndivide-and-swap cube $ FDSC_n $ for $ H\\in\\{K_1, K_{1,1}, K_{1,m} (2\\leq m \\leq\nd+1) \\} $ where $ n=2^d $. We show that\n$\\kappa(FDSC_n;K_1)=\\kappa^s(FDSC_n;K_1)=d+2$,\n$\\kappa(FDSC_n;K_{1,1})=\\kappa^s(FDSC_n;K_{1,1})=d+1 $ for $ d\\geq1 $ and\n$\\kappa(FDSC_n;K_{1,m})=\\kappa^s(FDSC_n;K_{1,m})=\\lfloor\\frac{d}{2}\\rfloor+1$\nfor $d\\geq1 $ and $ 2\\leq m \\leq d+1$.",
            "author": [
                "Muhammed T\u00fcrkmen",
                "Canan \u00c7ift\u00e7i",
                "G\u00fclnaz Boruzanl\u0131 Ekinci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11323v1",
                "http://arxiv.org/pdf/2311.11323v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C40, 94C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11320v2",
            "title": "Equivalence of lattice operators and graph matrices",
            "updated": "2023-11-28T02:23:57Z",
            "published": "2023-11-19T13:30:53Z",
            "summary": "We explore the relationship between lattice field theory and graph theory,\nplacing special emphasis on the interplay between Dirac and scalar lattice\noperators and matrices within the realm of spectral graph theory. Beyond\ndelving into fundamental concepts of spectral graph theory, such as adjacency\nand Laplacian matrices, we introduce a novel matrix named as \"anti-symmetrized\nadjacency matrix\", specifically tailored for cycle digraphs ($T^1$ lattice) and\nsimple directed paths ($B^1$ lattice). The nontrivial relation between graph\ntheory matrices and lattice operators shows that the graph Laplacian matrix\nmirrors the lattice scalar operator and the Wilson term in lattice fermions,\nwhile the anti-symmetrized adjacency matrix, along with its extensions to\nhigher dimensions, are equivalent to naive lattice Dirac operators. Building\nupon these connections, we provide rigorous proofs for two key assertions: (i)\nThe count of zero-modes in a free lattice scalar operator coincides with the\nzeroth Betti number of the underlying graph (lattice). (ii) The maximum count\nof Dirac zero-modes in a free lattice fermion operator is equivalent to the\ncumulative sum of all Betti numbers when the $D$-dimensional graph results from\na cartesian product of cycle digraphs ($T^1$ lattice) and simple directed paths\n($B^1$ lattice).",
            "author": [
                "Jun Yumoto",
                "Tatsuhiro Misumi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11320v2",
                "http://arxiv.org/pdf/2311.11320v2"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "hep-th",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11316v1",
            "title": "Word Measures on Wreath Products II",
            "updated": "2023-11-19T12:40:47Z",
            "published": "2023-11-19T12:40:47Z",
            "summary": "Every word $w$ in $F_r$, the free group of rank $r$, induces a probability\nmeasure (the $w$-measure) on every finite group $G$, by substitution of random\n$G$-elements in the letters. This measure is determined by its Fourier\ncoefficients: the $w$-expectations $E_w[\\chi]$ of the irreducible characters of\n$G$. For every finite group $G$, every stable character $\\chi$ of $G\\wr S_n$\n(trace of a finitely generated $FI_G$-module), and every word $w\\in F_r$, we\napproximate $E_w[\\chi]$ up to an error term of $O(n^{-\\pi(w)})$, where $\\pi(w)$\nis the primitivity rank of $w$. This generalizes previous works by Puder,\nHanany, Magee and the author. As an application we show that random Schreier\ngraphs of representation-stable actions of $G\\wr S_n$ are close-to-optimal\nexpanders. The paper reveals a surprising relation between stable\nrepresentation theory of wreath products and not-necessarily connected\nStallings core graphs.",
            "author": [
                "Yotam Shomroni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11316v1",
                "http://arxiv.org/pdf/2311.11316v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "math.RT",
                "08A50 (Primary) 20B30, 20E06, 05E10, 20E22 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11301v1",
            "title": "CHAMP: Efficient Annotation and Consolidation of Cluster Hierarchies",
            "updated": "2023-11-19T11:22:00Z",
            "published": "2023-11-19T11:22:00Z",
            "summary": "Various NLP tasks require a complex hierarchical structure over nodes, where\neach node is a cluster of items. Examples include generating entailment graphs,\nhierarchical cross-document coreference resolution, annotating event and\nsubevent relations, etc. To enable efficient annotation of such hierarchical\nstructures, we release CHAMP, an open source tool allowing to incrementally\nconstruct both clusters and hierarchy simultaneously over any type of texts.\nThis incremental approach significantly reduces annotation time compared to the\ncommon pairwise annotation approach and also guarantees maintaining\ntransitivity at the cluster and hierarchy levels. Furthermore, CHAMP includes a\nconsolidation mode, where an adjudicator can easily compare multiple cluster\nhierarchy annotations and resolve disagreements.",
            "author": [
                "Arie Cattan",
                "Tom Hope",
                "Doug Downey",
                "Roy Bar-Haim",
                "Lilach Eden",
                "Yoav Kantor",
                "Ido Dagan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11301v1",
                "http://arxiv.org/pdf/2311.11301v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14721v2",
            "title": "AnySyn: A Cost-Generic Logic Synthesis Framework with Customizable Cost\n  Functions",
            "updated": "2023-11-30T15:23:29Z",
            "published": "2023-11-19T10:39:57Z",
            "summary": "Modern technology-independent logic synthesis has been developed to optimize\nfor the size and depth of AND-Inverter Graphs (AIGs) as a proxy of CMOS circuit\narea and delay. However, for non-CMOS-based emerging technologies, AIG size and\ndepth may not be good cost estimations. Dedicated algorithms optimizing for\nmore complex cost functions have been proven effective for their specific\ntarget applications yet require time and experts in both logic synthesis and\nthe targeted technology to develop. In this work, we propose AnySyn, a\ncost-generic optimization framework for agile experimentation and prototyping\nof various customized cost functions before investing in developing specialized\nalgorithms. Experimental results show that AnySyn outperforms non-specialized\nsize and depth optimization algorithms by 14% and 19% on average and achieves\ncomparable results to specialized algorithms within acceptable CPU time.",
            "author": [
                "Hanyu Wang",
                "Siang-Yun Lee",
                "Giovanni De Micheli"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14721v2",
                "http://arxiv.org/pdf/2311.14721v2"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11278v1",
            "title": "Transcending Forgery Specificity with Latent Space Augmentation for\n  Generalizable Deepfake Detection",
            "updated": "2023-11-19T09:41:10Z",
            "published": "2023-11-19T09:41:10Z",
            "summary": "Deepfake detection faces a critical generalization hurdle, with performance\ndeteriorating when there is a mismatch between the distributions of training\nand testing data. A broadly received explanation is the tendency of these\ndetectors to be overfitted to forgery-specific artifacts, rather than learning\nfeatures that are widely applicable across various forgeries. To address this\nissue, we propose a simple yet effective detector called LSDA\n(\\underline{L}atent \\underline{S}pace \\underline{D}ata\n\\underline{A}ugmentation), which is based on a heuristic idea: representations\nwith a wider variety of forgeries should be able to learn a more generalizable\ndecision boundary, thereby mitigating the overfitting of method-specific\nfeatures (see Figure. 1). Following this idea, we propose to enlarge the\nforgery space by constructing and simulating variations within and across\nforgery features in the latent space. This approach encompasses the acquisition\nof enriched, domain-specific features and the facilitation of smoother\ntransitions between different forgery types, effectively bridging domain gaps.\nOur approach culminates in refining a binary classifier that leverages the\ndistilled knowledge from the enhanced features, striving for a generalizable\ndeepfake detector. Comprehensive experiments show that our proposed method is\nsurprisingly effective and transcends state-of-the-art detectors across several\nwidely used benchmarks.",
            "author": [
                "Zhiyuan Yan",
                "Yuhao Luo",
                "Siwei Lyu",
                "Qingshan Liu",
                "Baoyuan Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11278v1",
                "http://arxiv.org/pdf/2311.11278v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11271v1",
            "title": "A Cross-Attention Augmented Model for Event-Triggered Context-Aware\n  Story Generation",
            "updated": "2023-11-19T08:54:47Z",
            "published": "2023-11-19T08:54:47Z",
            "summary": "Despite recent advancements, existing story generation systems continue to\nencounter difficulties in effectively incorporating contextual and event\nfeatures, which greatly influence the quality of generated narratives. To\ntackle these challenges, we introduce a novel neural generation model, EtriCA,\nthat enhances the relevance and coherence of generated stories by employing a\ncross-attention mechanism to map context features onto event sequences through\nresidual mapping. This feature capturing mechanism enables our model to exploit\nlogical relationships between events more effectively during the story\ngeneration process. To further enhance our proposed model, we employ a\npost-training framework for knowledge enhancement (KeEtriCA) on a large-scale\nbook corpus. This allows EtriCA to adapt to a wider range of data samples. This\nresults in approximately 5\\% improvement in automatic metrics and over 10\\%\nimprovement in human evaluation. We conduct extensive experiments, including\ncomparisons with state-of-the-art (SOTA) baseline models, to evaluate the\nperformance of our framework on story generation. The experimental results,\nencompassing both automated metrics and human assessments, demonstrate the\nsuperiority of our model over existing state-of-the-art baselines. These\nresults underscore the effectiveness of our model in leveraging context and\nevent features to improve the quality of generated narratives.",
            "author": [
                "Chen Tang",
                "Tyler Loakman",
                "Chenghua Lin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11271v1",
                "http://arxiv.org/pdf/2311.11271v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11268v1",
            "title": "Towards Real-World Writing Assistance: A Chinese Character Checking\n  Benchmark with Faked and Misspelled Characters",
            "updated": "2023-11-19T08:41:43Z",
            "published": "2023-11-19T08:41:43Z",
            "summary": "Writing assistance is an application closely related to human life and is\nalso a fundamental Natural Language Processing (NLP) research field. Its aim is\nto improve the correctness and quality of input texts, with character checking\nbeing crucial in detecting and correcting wrong characters. From the\nperspective of the real world where handwriting occupies the vast majority,\ncharacters that humans get wrong include faked characters (i.e., untrue\ncharacters created due to writing errors) and misspelled characters (i.e., true\ncharacters used incorrectly due to spelling errors). However, existing datasets\nand related studies only focus on misspelled characters mainly caused by\nphonological or visual confusion, thereby ignoring faked characters which are\nmore common and difficult. To break through this dilemma, we present\nVisual-C$^3$, a human-annotated Visual Chinese Character Checking dataset with\nfaked and misspelled Chinese characters. To the best of our knowledge,\nVisual-C$^3$ is the first real-world visual and the largest human-crafted\ndataset for the Chinese character checking scenario. Additionally, we also\npropose and evaluate novel baseline methods on Visual-C$^3$. Extensive\nempirical results and analyses show that Visual-C$^3$ is high-quality yet\nchallenging. The Visual-C$^3$ dataset and the baseline methods will be publicly\navailable to facilitate further research in the community.",
            "author": [
                "Yinghui Li",
                "Zishan Xu",
                "Shaoshen Chen",
                "Haojing Huang",
                "Yangning Li",
                "Yong Jiang",
                "Zhongli Li",
                "Qingyu Zhou",
                "Hai-Tao Zheng",
                "Ying Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11268v1",
                "http://arxiv.org/pdf/2311.11268v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11262v1",
            "title": "Uncertainty quantification for noisy inputs-outputs in physics-informed\n  neural networks and neural operators",
            "updated": "2023-11-19T08:18:26Z",
            "published": "2023-11-19T08:18:26Z",
            "summary": "Uncertainty quantification (UQ) in scientific machine learning (SciML)\nbecomes increasingly critical as neural networks (NNs) are being widely adopted\nin addressing complex problems across various scientific disciplines.\nRepresentative SciML models are physics-informed neural networks (PINNs) and\nneural operators (NOs). While UQ in SciML has been increasingly investigated in\nrecent years, very few works have focused on addressing the uncertainty caused\nby the noisy inputs, such as spatial-temporal coordinates in PINNs and input\nfunctions in NOs. The presence of noise in the inputs of the models can pose\nsignificantly more challenges compared to noise in the outputs of the models,\nprimarily due to the inherent nonlinearity of most SciML algorithms. As a\nresult, UQ for noisy inputs becomes a crucial factor for reliable and\ntrustworthy deployment of these models in applications involving physical\nknowledge. To this end, we introduce a Bayesian approach to quantify\nuncertainty arising from noisy inputs-outputs in PINNs and NOs. We show that\nthis approach can be seamlessly integrated into PINNs and NOs, when they are\nemployed to encode the physical information. PINNs incorporate physics by\nincluding physics-informed terms via automatic differentiation, either in the\nloss function or the likelihood, and often take as input the spatial-temporal\ncoordinate. Therefore, the present method equips PINNs with the capability to\naddress problems where the observed coordinate is subject to noise. On the\nother hand, pretrained NOs are also commonly employed as equation-free\nsurrogates in solving differential equations and Bayesian inverse problems, in\nwhich they take functions as inputs. The proposed approach enables them to\nhandle noisy measurements for both input and output functions with UQ.",
            "author": [
                "Zongren Zou",
                "Xuhui Meng",
                "George Em Karniadakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11262v1",
                "http://arxiv.org/pdf/2311.11262v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.comp-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11254v3",
            "title": "BOIS: Bayesian Optimization of Interconnected Systems",
            "updated": "2023-11-29T02:32:02Z",
            "published": "2023-11-19T06:44:13Z",
            "summary": "Bayesian optimization (BO) has proven to be an effective paradigm for the\nglobal optimization of expensive-to-sample systems. One of the main advantages\nof BO is its use of Gaussian processes (GPs) to characterize model uncertainty\nwhich can be leveraged to guide the learning and search process. However, BO\ntypically treats systems as black-boxes and this limits the ability to exploit\nstructural knowledge (e.g., physics and sparse interconnections). Composite\nfunctions of the form $f(x, y(x))$, wherein GP modeling is shifted from the\nperformance function $f$ to an intermediate function $y$, offer an avenue for\nexploiting structural knowledge. However, the use of composite functions in a\nBO framework is complicated by the need to generate a probability density for\n$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is\nnonlinear it is not possible to obtain a closed-form expression). Previous work\nhas handled this issue using sampling techniques; these are easy to implement\nand flexible but are computationally intensive. In this work, we introduce a\nnew paradigm which allows for the efficient use of composite functions in BO;\nthis uses adaptive linearizations of $f$ to obtain closed-form expressions for\nthe statistical moments of the composite function. We show that this simple\napproach (which we call BOIS) enables the exploitation of structural knowledge,\nsuch as that arising in interconnected systems as well as systems that embed\nmultiple GP models and combinations of physics and GP models. Using a chemical\nprocess optimization case study, we benchmark the effectiveness of BOIS against\nstandard BO and sampling approaches. Our results indicate that BOIS achieves\nperformance gains and accurately captures the statistics of composite\nfunctions.",
            "author": [
                "Leonardo D. Gonz\u00e1lez",
                "Victor M. Zavala"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11254v3",
                "http://arxiv.org/pdf/2311.11254v3"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11249v1",
            "title": "Open Set Dandelion Network for IoT Intrusion Detection",
            "updated": "2023-11-19T06:28:43Z",
            "published": "2023-11-19T06:28:43Z",
            "summary": "As IoT devices become widely, it is crucial to protect them from malicious\nintrusions. However, the data scarcity of IoT limits the applicability of\ntraditional intrusion detection methods, which are highly data-dependent. To\naddress this, in this paper we propose the Open-Set Dandelion Network (OSDN)\nbased on unsupervised heterogeneous domain adaptation in an open-set manner.\nThe OSDN model performs intrusion knowledge transfer from the knowledge-rich\nsource network intrusion domain to facilitate more accurate intrusion detection\nfor the data-scarce target IoT intrusion domain. Under the open-set setting, it\ncan also detect newly-emerged target domain intrusions that are not observed in\nthe source domain. To achieve this, the OSDN model forms the source domain into\na dandelion-like feature space in which each intrusion category is compactly\ngrouped and different intrusion categories are separated, i.e., simultaneously\nemphasising inter-category separability and intra-category compactness. The\ndandelion-based target membership mechanism then forms the target dandelion.\nThen, the dandelion angular separation mechanism achieves better inter-category\nseparability, and the dandelion embedding alignment mechanism further aligns\nboth dandelions in a finer manner. To promote intra-category compactness, the\ndiscriminating sampled dandelion mechanism is used. Assisted by the intrusion\nclassifier trained using both known and generated unknown intrusion knowledge,\na semantic dandelion correction mechanism emphasises easily-confused categories\nand guides better inter-category separability. Holistically, these mechanisms\nform the OSDN model that effectively performs intrusion knowledge transfer to\nbenefit IoT intrusion detection. Comprehensive experiments on several intrusion\ndatasets verify the effectiveness of the OSDN model, outperforming three\nstate-of-the-art baseline methods by 16.9%.",
            "author": [
                "Jiashu Wu",
                "Hao Dai",
                "Kenneth B. Kent",
                "Jerome Yen",
                "Chengzhong Xu",
                "Yang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11249v1",
                "http://arxiv.org/pdf/2311.11249v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11241v1",
            "title": "Open-Vocabulary Camouflaged Object Segmentation",
            "updated": "2023-11-19T06:00:39Z",
            "published": "2023-11-19T06:00:39Z",
            "summary": "Recently, the emergence of the large-scale vision-language model (VLM), such\nas CLIP, has opened the way towards open-world object perception. Many works\nhas explored the utilization of pre-trained VLM for the challenging\nopen-vocabulary dense prediction task that requires perceive diverse objects\nwith novel classes at inference time. Existing methods construct experiments\nbased on the public datasets of related tasks, which are not tailored for open\nvocabulary and rarely involves imperceptible objects camouflaged in complex\nscenes due to data collection bias and annotation costs. To fill in the gaps,\nwe introduce a new task, open-vocabulary camouflaged object segmentation\n(OVCOS) and construct a large-scale complex scene dataset (\\textbf{OVCamo})\nwhich containing 11,483 hand-selected images with fine annotations and\ncorresponding object classes. Further, we build a strong single-stage\nopen-vocabulary \\underline{c}amouflaged \\underline{o}bject\n\\underline{s}egmentation transform\\underline{er} baseline \\textbf{OVCoser}\nattached to the parameter-fixed CLIP with iterative semantic guidance and\nstructure enhancement. By integrating the guidance of class semantic knowledge\nand the supplement of visual structure cues from the edge and depth\ninformation, the proposed method can efficiently capture camouflaged objects.\nMoreover, this effective framework also surpasses previous state-of-the-arts of\nopen-vocabulary semantic image segmentation by a large margin on our OVCamo\ndataset. With the proposed dataset and baseline, we hope that this new task\nwith more practical value can further expand the research on open-vocabulary\ndense prediction tasks.",
            "author": [
                "Youwei Pang",
                "Xiaoqi Zhao",
                "Jiaming Zuo",
                "Lihe Zhang",
                "Huchuan Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11241v1",
                "http://arxiv.org/pdf/2311.11241v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11239v1",
            "title": "Dependency Relationships-Enhanced Attentive Group Recommendation in HINs",
            "updated": "2023-11-19T05:59:19Z",
            "published": "2023-11-19T05:59:19Z",
            "summary": "Recommending suitable items to a group of users, commonly referred to as the\ngroup recommendation task, is becoming increasingly urgent with the development\nof group activities. The challenges within the group recommendation task\ninvolve aggregating the individual preferences of group members as the group's\npreferences and facing serious sparsity problems due to the lack of\nuser/group-item interactions. To solve these problems, we propose a novel\napproach called Dependency Relationships-Enhanced Attentive Group\nRecommendation (DREAGR) for the recommendation task of occasional groups.\nSpecifically, we introduce the dependency relationship between items as side\ninformation to enhance the user/group-item interaction and alleviate the\ninteraction sparsity problem. Then, we propose a Path-Aware Attention Embedding\n(PAAE) method to model users' preferences on different types of paths. Next, we\ndesign a gated fusion mechanism to fuse users' preferences into their\ncomprehensive preferences. Finally, we develop an attention aggregator that\naggregates users' preferences as the group's preferences for the group\nrecommendation task. We conducted experiments on two datasets to demonstrate\nthe superiority of DREAGR by comparing it with state-of-the-art group\nrecommender models. The experimental results show that DREAGR outperforms other\nmodels, especially HR@N and NDCG@N (N=5, 10), where DREAGR has improved in the\nrange of 3.64% to 7.01% and 2.57% to 3.39% on both datasets, respectively.",
            "author": [
                "Juntao Zhang",
                "Sheng Wang",
                "Zhiyu Chen",
                "Xiandi Yang",
                "Zhiyong Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11239v1",
                "http://arxiv.org/pdf/2311.11239v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11228v1",
            "title": "A Universal Framework for Accurate and Efficient Geometric Deep Learning\n  of Molecular Systems",
            "updated": "2023-11-19T04:52:05Z",
            "published": "2023-11-19T04:52:05Z",
            "summary": "Molecular sciences address a wide range of problems involving molecules of\ndifferent types and sizes and their complexes. Recently, geometric deep\nlearning, especially Graph Neural Networks, has shown promising performance in\nmolecular science applications. However, most existing works often impose\ntargeted inductive biases to a specific molecular system, and are inefficient\nwhen applied to macromolecules or large-scale tasks, thereby limiting their\napplications to many real-world problems. To address these challenges, we\npresent PAMNet, a universal framework for accurately and efficiently learning\nthe representations of three-dimensional (3D) molecules of varying sizes and\ntypes in any molecular system. Inspired by molecular mechanics, PAMNet induces\na physics-informed bias to explicitly model local and non-local interactions\nand their combined effects. As a result, PAMNet can reduce expensive\noperations, making it time and memory efficient. In extensive benchmark\nstudies, PAMNet outperforms state-of-the-art baselines regarding both accuracy\nand efficiency in three diverse learning tasks: small molecule properties, RNA\n3D structures, and protein-ligand binding affinities. Our results highlight the\npotential for PAMNet in a broad range of molecular science applications.",
            "author": [
                "Shuo Zhang",
                "Yang Liu",
                "Lei Xie"
            ],
            "link": [
                "http://dx.doi.org/10.1038/s41598-023-46382-8",
                "http://arxiv.org/abs/2311.11228v1",
                "http://arxiv.org/pdf/2311.11228v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.BM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11221v1",
            "title": "GaussianDiffusion: 3D Gaussian Splatting for Denoising Diffusion\n  Probabilistic Models with Structured Noise",
            "updated": "2023-11-19T04:26:16Z",
            "published": "2023-11-19T04:26:16Z",
            "summary": "Text-to-3D, known for its efficient generation methods and expansive creative\npotential, has garnered significant attention in the AIGC domain. However, the\namalgamation of Nerf and 2D diffusion models frequently yields oversaturated\nimages, posing severe limitations on downstream industrial applications due to\nthe constraints of pixelwise rendering method. Gaussian splatting has recently\nsuperseded the traditional pointwise sampling technique prevalent in NeRF-based\nmethodologies, revolutionizing various aspects of 3D reconstruction. This paper\nintroduces a novel text to 3D content generation framework based on Gaussian\nsplatting, enabling fine control over image saturation through individual\nGaussian sphere transparencies, thereby producing more realistic images. The\nchallenge of achieving multi-view consistency in 3D generation significantly\nimpedes modeling complexity and accuracy. Taking inspiration from SJC, we\nexplore employing multi-view noise distributions to perturb images generated by\n3D Gaussian splatting, aiming to rectify inconsistencies in multi-view\ngeometry. We ingeniously devise an efficient method to generate noise that\nproduces Gaussian noise from diverse viewpoints, all originating from a shared\nnoise source. Furthermore, vanilla 3D Gaussian-based generation tends to trap\nmodels in local minima, causing artifacts like floaters, burrs, or\nproliferative elements. To mitigate these issues, we propose the variational\nGaussian splatting technique to enhance the quality and stability of 3D\nappearance. To our knowledge, our approach represents the first comprehensive\nutilization of Gaussian splatting across the entire spectrum of 3D content\ngeneration processes.",
            "author": [
                "Xinhai Li",
                "Huaibin Wang",
                "Kuo-Kun Tseng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11221v1",
                "http://arxiv.org/pdf/2311.11221v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11212v1",
            "title": "Can We Utilize Pre-trained Language Models within Causal Discovery\n  Algorithms?",
            "updated": "2023-11-19T03:31:30Z",
            "published": "2023-11-19T03:31:30Z",
            "summary": "Scaling laws have allowed Pre-trained Language Models (PLMs) into the field\nof causal reasoning. Causal reasoning of PLM relies solely on text-based\ndescriptions, in contrast to causal discovery which aims to determine the\ncausal relationships between variables utilizing data. Recently, there has been\ncurrent research regarding a method that mimics causal discovery by aggregating\nthe outcomes of repetitive causal reasoning, achieved through specifically\ndesigned prompts. It highlights the usefulness of PLMs in discovering cause and\neffect, which is often limited by a lack of data, especially when dealing with\nmultiple variables. Conversely, the characteristics of PLMs which are that PLMs\ndo not analyze data and they are highly dependent on prompt design leads to a\ncrucial limitation for directly using PLMs in causal discovery. Accordingly,\nPLM-based causal reasoning deeply depends on the prompt design and carries out\nthe risk of overconfidence and false predictions in determining causal\nrelationships. In this paper, we empirically demonstrate the aforementioned\nlimitations of PLM-based causal reasoning through experiments on\nphysics-inspired synthetic data. Then, we propose a new framework that\nintegrates prior knowledge obtained from PLM with a causal discovery algorithm.\nThis is accomplished by initializing an adjacency matrix for causal discovery\nand incorporating regularization using prior knowledge. Our proposed framework\nnot only demonstrates improved performance through the integration of PLM and\ncausal discovery but also suggests how to leverage PLM-extracted prior\nknowledge with existing causal discovery algorithms.",
            "author": [
                "Chanhui Lee",
                "Juhyeon Kim",
                "Yongjun Jeong",
                "Juhyun Lyu",
                "Junghee Kim",
                "Sangmin Lee",
                "Sangjun Han",
                "Hyeokjun Choe",
                "Soyeon Park",
                "Woohyung Lim",
                "Sungbin Lim",
                "Sanghack Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11212v1",
                "http://arxiv.org/pdf/2311.11212v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "I.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11206v1",
            "title": "Robust Network Slicing: Multi-Agent Policies, Adversarial Attacks, and\n  Defensive Strategies",
            "updated": "2023-11-19T03:07:29Z",
            "published": "2023-11-19T03:07:29Z",
            "summary": "In this paper, we present a multi-agent deep reinforcement learning (deep RL)\nframework for network slicing in a dynamic environment with multiple base\nstations and multiple users. In particular, we propose a novel deep RL\nframework with multiple actors and centralized critic (MACC) in which actors\nare implemented as pointer networks to fit the varying dimension of input. We\nevaluate the performance of the proposed deep RL algorithm via simulations to\ndemonstrate its effectiveness. Subsequently, we develop a deep RL based jammer\nwith limited prior information and limited power budget. The goal of the jammer\nis to minimize the transmission rates achieved with network slicing and thus\ndegrade the network slicing agents' performance. We design a jammer with both\nlistening and jamming phases and address jamming location optimization as well\nas jamming channel optimization via deep RL. We evaluate the jammer at the\noptimized location, generating interference attacks in the optimized set of\nchannels by switching between the jamming phase and listening phase. We show\nthat the proposed jammer can significantly reduce the victims' performance\nwithout direct feedback or prior knowledge on the network slicing policies.\nFinally, we devise a Nash-equilibrium-supervised policy ensemble mixed strategy\nprofile for network slicing (as a defensive measure) and jamming. We evaluate\nthe performance of the proposed policy ensemble algorithm by applying on the\nnetwork slicing agents and the jammer agent in simulations to show its\neffectiveness.",
            "author": [
                "Feng Wang",
                "M. Cenk Gursoy",
                "Senem Velipasalar"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TMLCN.2023.3334236",
                "http://arxiv.org/abs/2311.11206v1",
                "http://arxiv.org/pdf/2311.11206v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11198v1",
            "title": "Self-Supervised Versus Supervised Training for Segmentation of Organoid\n  Images",
            "updated": "2023-11-19T01:57:55Z",
            "published": "2023-11-19T01:57:55Z",
            "summary": "The process of annotating relevant data in the field of digital microscopy\ncan be both time-consuming and especially expensive due to the required\ntechnical skills and human-expert knowledge. Consequently, large amounts of\nmicroscopic image data sets remain unlabeled, preventing their effective\nexploitation using deep-learning algorithms. In recent years it has been shown\nthat a lot of relevant information can be drawn from unlabeled data.\nSelf-supervised learning (SSL) is a promising solution based on learning\nintrinsic features under a pretext task that is similar to the main task\nwithout requiring labels. The trained result is transferred to the main task -\nimage segmentation in our case. A ResNet50 U-Net was first trained to restore\nimages of liver progenitor organoids from augmented images using the Structural\nSimilarity Index Metric (SSIM), alone, and using SSIM combined with L1 loss.\nBoth the encoder and decoder were trained in tandem. The weights were\ntransferred to another U-Net model designed for segmentation with frozen\nencoder weights, using Binary Cross Entropy, Dice, and Intersection over Union\n(IoU) losses. For comparison, we used the same U-Net architecture to train two\nsupervised models, one utilizing the ResNet50 encoder as well as a simple CNN.\nResults showed that self-supervised learning models using a 25\\% pixel drop or\nimage blurring augmentation performed better than the other augmentation\ntechniques using the IoU loss. When trained on only 114 images for the main\ntask, the self-supervised learning approach outperforms the supervised method\nachieving an F1-score of 0.85, with higher stability, in contrast to an F1=0.78\nscored by the supervised method. Furthermore, when trained with larger data\nsets (1,000 images), self-supervised learning is still able to perform better,\nachieving an F1-score of 0.92, contrasting to a score of 0.85 for the\nsupervised method.",
            "author": [
                "Asmaa Haja",
                "Eric Brouwer",
                "Lambert Schomaker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11198v1",
                "http://arxiv.org/pdf/2311.11198v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11187v1",
            "title": "Link Streams as a Generalization of Graphs and Time Series",
            "updated": "2023-11-19T00:00:32Z",
            "published": "2023-11-19T00:00:32Z",
            "summary": "A link stream is a set of possibly weighted triplets (t, u, v) modeling that\nu and v interacted at time t. Link streams offer an effective model for\ndatasets containing both temporal and relational information, making their\nproper analysis crucial in many applications. They are commonly regarded as\nsequences of graphs or collections of time series. Yet, a recent seminal work\ndemonstrated that link streams are more general objects of which graphs are\nonly particular cases. It therefore started the construction of a dedicated\nformalism for link streams by extending graph theory. In this work, we\ncontribute to the development of this formalism by showing that link streams\nalso generalize time series. In particular, we show that a link stream\ncorresponds to a time-series extended to a relational dimension, which opens\nthe door to also extend the framework of signal processing to link streams. We\ntherefore develop extensions of numerous signal concepts to link streams: from\nelementary ones like energy, correlation, and differentiation, to more advanced\nones like Fourier transform and filters.",
            "author": [
                "Esteban Bautista",
                "Matthieu Latapy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11187v1",
                "http://arxiv.org/pdf/2311.11187v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11178v2",
            "title": "Active Prompt Learning in Vision Language Models",
            "updated": "2023-11-27T00:58:21Z",
            "published": "2023-11-18T22:42:16Z",
            "summary": "Pre-trained Vision Language Models (VLMs) have demonstrated notable progress\nin various zero-shot tasks, such as classification and retrieval. Despite their\nperformance, because improving performance on new tasks requires task-specific\nknowledge, their adaptation is essential. While labels are needed for the\nadaptation, acquiring them is typically expensive. To overcome this challenge,\nactive learning, a method of achieving a high performance by obtaining labels\nfor a small number of samples from experts, has been studied. Active learning\nprimarily focuses on selecting unlabeled samples for labeling and leveraging\nthem to train models. In this study, we pose the question, \"how can the\npre-trained VLMs be adapted under the active learning framework?\" In response\nto this inquiry, we observe that (1) simply applying a conventional active\nlearning framework to pre-trained VLMs even may degrade performance compared to\nrandom selection because of the class imbalance in labeling candidates, and (2)\nthe knowledge of VLMs can provide hints for achieving the balance before\nlabeling. Based on these observations, we devise a novel active learning\nframework for VLMs, denoted as PCB. To assess the effectiveness of our\napproach, we conduct experiments on seven different real-world datasets, and\nthe results demonstrate that PCB surpasses conventional active learning and\nrandom sampling methods.",
            "author": [
                "Jihwan Bang",
                "Sumyeong Ahn",
                "Jae-Gil Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11178v2",
                "http://arxiv.org/pdf/2311.11178v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11176v1",
            "title": "Morphology-Enhanced CAM-Guided SAM for weakly supervised Breast Lesion\n  Segmentation",
            "updated": "2023-11-18T22:06:04Z",
            "published": "2023-11-18T22:06:04Z",
            "summary": "Breast cancer diagnosis challenges both patients and clinicians, with early\ndetection being crucial for effective treatment. Ultrasound imaging plays a key\nrole in this, but its utility is hampered by the need for precise lesion\nsegmentation-a task that is both time-consuming and labor-intensive. To address\nthese challenges, we propose a new framework: a morphology-enhanced, Class\nActivation Map (CAM)-guided model, which is optimized using a computer vision\nfoundation model known as SAM. This innovative framework is specifically\ndesigned for weakly supervised lesion segmentation in early-stage breast\nultrasound images. Our approach uniquely leverages image-level annotations,\nwhich removes the requirement for detailed pixel-level annotation. Initially,\nwe perform a preliminary segmentation using breast lesion morphology knowledge.\nFollowing this, we accurately localize lesions by extracting semantic\ninformation through a CAM-based heatmap. These two elements are then fused\ntogether, serving as a prompt to guide the SAM in performing refined\nsegmentation. Subsequently, post-processing techniques are employed to rectify\ntopological errors made by the SAM. Our method not only simplifies the\nsegmentation process but also attains accuracy comparable to supervised\nlearning methods that rely on pixel-level annotation. Our framework achieves a\nDice score of 74.39% on the test set, demonstrating compareable performance\nwith supervised learning methods. Additionally, it outperforms a supervised\nlearning model, in terms of the Hausdorff distance, scoring 24.27 compared to\nDeeplabv3+'s 32.22. These experimental results showcase its feasibility and\nsuperior performance in integrating weakly supervised learning with SAM. The\ncode is made available at: https://github.com/YueXin18/MorSeg-CAM-SAM.",
            "author": [
                "Xin Yue",
                "Qing Zhao",
                "Jianqiang Li",
                "Xiaoling Liu",
                "Changwei Song",
                "Suqin Liu",
                "Guanghui Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11176v1",
                "http://arxiv.org/pdf/2311.11176v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11168v1",
            "title": "Bounded quantifier depth spectrum for random uniform hypegraphs",
            "updated": "2023-11-18T21:07:49Z",
            "published": "2023-11-18T21:07:49Z",
            "summary": "The notion of spectrum for first-order properties introduced by J. Spencer\nfor Erdos-Renyi random graph is considered in relation to random uniform\nhypergraphs. In this work we study the set of limit points of the spectrum for\nfirst-order formulae with bounded quantifier depth and obtain bounds for its\nmaximum value. Moreover, we prove zero-one k-laws for the random uniform\nhypergraph and improve the bounds for the maximum value of the spectrum for\nfirst-order formulae with bounded quantifier depth. We obtain that the maximum\nvalue of the spectrum belongs to some two-element set.",
            "author": [
                "Svetlana Popova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11168v1",
                "http://arxiv.org/pdf/2311.11168v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11167v1",
            "title": "Benchmarking Machine Learning Models for Quantum Error Correction",
            "updated": "2023-11-18T21:01:38Z",
            "published": "2023-11-18T21:01:38Z",
            "summary": "Quantum Error Correction (QEC) is one of the fundamental problems in quantum\ncomputer systems, which aims to detect and correct errors in the data qubits\nwithin quantum computers. Due to the presence of unreliable data qubits in\nexisting quantum computers, implementing quantum error correction is a critical\nstep when establishing a stable quantum computer system. Recently, machine\nlearning (ML)-based approaches have been proposed to address this challenge.\nHowever, they lack a thorough understanding of quantum error correction. To\nbridge this research gap, we provide a new perspective to understand machine\nlearning-based QEC in this paper. We find that syndromes in the ancilla qubits\nresult from errors on connected data qubits, and distant ancilla qubits can\nprovide auxiliary information to rule out some incorrect predictions for the\ndata qubits. Therefore, to detect errors in data qubits, we must consider the\ninformation present in the long-range ancilla qubits. To the best of our\nknowledge, machine learning is less explored in the dependency relationship of\nQEC. To fill the blank, we curate a machine learning benchmark to assess the\ncapacity to capture long-range dependencies for quantum error correction. To\nprovide a comprehensive evaluation, we evaluate seven state-of-the-art deep\nlearning algorithms spanning diverse neural network architectures, such as\nconvolutional neural networks, graph neural networks, and graph transformers.\nOur exhaustive experiments reveal an enlightening trend: By enlarging the\nreceptive field to exploit information from distant ancilla qubits, the\naccuracy of QEC significantly improves. For instance, U-Net can improve CNN by\na margin of about 50%. Finally, we provide a comprehensive analysis that could\ninspire future research in this field. We will release the code when the paper\nis published.",
            "author": [
                "Tim Fu",
                "Yue Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11167v1",
                "http://arxiv.org/pdf/2311.11167v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11163v1",
            "title": "Hate speech and hate crimes: a data-driven study of evolving discourse\n  around marginalized groups",
            "updated": "2023-11-18T20:49:15Z",
            "published": "2023-11-18T20:49:15Z",
            "summary": "This study explores the dynamic relationship between online discourse, as\nobserved in tweets, and physical hate crimes, focusing on marginalized groups.\nLeveraging natural language processing techniques, including keyword extraction\nand topic modeling, we analyze the evolution of online discourse after events\naffecting these groups. Examining sentiment and polarizing tweets, we establish\ncorrelations with hate crimes in Black and LGBTQ+ communities. Using a\nknowledge graph, we connect tweets, users, topics, and hate crimes, enabling\nnetwork analyses. Our findings reveal divergent patterns in the evolution of\nuser communities for Black and LGBTQ+ groups, with notable differences in\nsentiment among influential users. This analysis sheds light on distinctive\nonline discourse patterns and emphasizes the need to monitor hate speech to\nprevent hate crimes, especially following significant events impacting\nmarginalized communities.",
            "author": [
                "Malvina Bozhidarova",
                "Jonathn Chang",
                "Aaishah Ale-rasool",
                "Yuxiang Liu",
                "Chongyao Ma",
                "Andrea L. Bertozzi",
                "P. Jeffrey Brantingham",
                "Junyuan Lin",
                "Sanjukta Krishnagopal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11163v1",
                "http://arxiv.org/pdf/2311.11163v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "stat.AP",
                "stat.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11157v1",
            "title": "Contextualizing Internet Memes Across Social Media Platforms",
            "updated": "2023-11-18T20:18:18Z",
            "published": "2023-11-18T20:18:18Z",
            "summary": "Internet memes have emerged as a novel format for communication and\nexpressing ideas on the web. Their fluidity and creative nature are reflected\nin their widespread use, often across platforms and occasionally for unethical\nor harmful purposes. While computational work has already analyzed their\nhigh-level virality over time and developed specialized classifiers for hate\nspeech detection, there have been no efforts to date that aim to holistically\ntrack, identify, and map internet memes posted on social media. To bridge this\ngap, we investigate whether internet memes across social media platforms can be\ncontextualized by using a semantic repository of knowledge, namely, a knowledge\ngraph. We collect thousands of potential internet meme posts from two social\nmedia platforms, namely Reddit and Discord, and perform an\nextract-transform-load procedure to create a data lake with candidate meme\nposts. By using vision transformer-based similarity, we match these candidates\nagainst the memes cataloged in a recently released knowledge graph of internet\nmemes, IMKG. We provide evidence that memes published online can be identified\nby mapping them to IMKG. We leverage this grounding to study the prevalence of\nmemes on different platforms, discover popular memes, and select common meme\nchannels and subreddits. Finally, we illustrate how the grounding can enable\nusers to get context about memes on social media thanks to their link to the\nknowledge graph.",
            "author": [
                "Saurav Joshi",
                "Filip Ilievski",
                "Luca Luceri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11157v1",
                "http://arxiv.org/pdf/2311.11157v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11154v1",
            "title": "Weighted tree games",
            "updated": "2023-11-18T19:52:43Z",
            "published": "2023-11-18T19:52:43Z",
            "summary": "We consider a variation on Maker-Breaker games on graphs or digraphs where\nthe edges have random costs. We assume that Maker wishes to choose the edges of\na spanning tree, but wishes to minimise his cost. Meanwhile Breaker wants to\nmake Maker's cost as large as possible.",
            "author": [
                "Patrick Bennett",
                "Alan Frieze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11154v1",
                "http://arxiv.org/pdf/2311.11154v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11143v1",
            "title": "Goal-Oriented Communications for Remote Inference with Two-Way Delay",
            "updated": "2023-11-18T18:41:34Z",
            "published": "2023-11-18T18:41:34Z",
            "summary": "We study the design of goal-oriented communication strategies for remote\ninference, where an inferrer (e.g., a trained neural network) on the receiver\nside predicts a time-varying target signal (e.g., the position of the car in\nfront) using the data packet (e.g., video clip) most recently received from a\nsensor (e.g., camera). The communication between the sensor and the receiver is\ncarried out over a two-way channel. The objective is to minimize the expected\ninference error per time slot by exploiting the memory of the delay in the\nchannel. It turns out that the optimal policy is an index-based threshold\npolicy. The scheduler submits a packet at suitable time slots for which the\nindex function exceeds a threshold. The index function depends on the current\nage of information on the receiver side and the prior knowledge about the delay\nin the subsequent packet transmission.",
            "author": [
                "Cagri Ari",
                "Md Kamran Chowdhury Shisher",
                "Elif Uysal",
                "Yin Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11143v1",
                "http://arxiv.org/pdf/2311.11143v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11135v1",
            "title": "A Principled Framework for Knowledge-enhanced Large Language Model",
            "updated": "2023-11-18T18:10:02Z",
            "published": "2023-11-18T18:10:02Z",
            "summary": "Large Language Models (LLMs) are versatile, yet they often falter in tasks\nrequiring deep and reliable reasoning due to issues like hallucinations,\nlimiting their applicability in critical scenarios. This paper introduces a\nrigorously designed framework for creating LLMs that effectively anchor\nknowledge and employ a closed-loop reasoning process, enhancing their\ncapability for in-depth analysis. We dissect the framework to illustrate the\ncontribution of each component to the LLMs' performance, offering a theoretical\nassurance of improved reasoning under well-defined assumptions.",
            "author": [
                "Saizhuo Wang",
                "Zhihan Liu",
                "Zhaoran Wang",
                "Jian Guo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11135v1",
                "http://arxiv.org/pdf/2311.11135v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11132v3",
            "title": "Low-dimensional controllability of brain networks",
            "updated": "2023-11-28T14:48:55Z",
            "published": "2023-11-18T17:46:32Z",
            "summary": "Network controllability is a powerful tool to study causal relationships in\ncomplex systems and identify the driver nodes for steering the network dynamics\ninto desired states. However, due to ill-posed conditions, results become\nunreliable when the number of drivers becomes too small compared to the network\nsize. This is a very common situation, particularly in real-world applications,\nwhere the possibility to access multiple nodes at the same time is limited by\ntechnological constraints, such as in the human brain. Although targeting\nsmaller network parts might improve accuracy, challenges may remain for\nextremely unbalanced situations, when for example there is one single driver.\nTo address this problem, we developed a mathematical framework that combines\nconcepts from spectral graph theory and modern network science. Instead of\ncontrolling the original network dynamics, we aimed to control its\nlow-dimensional embedding into the topological space derived from the network\nLaplacian. By performing extensive simulations on synthetic networks, we showed\nthat a relatively low number of projected components is enough to improve the\noverall control accuracy, notably when dealing with very few drivers. Based on\nthese findings, we introduced alternative low-dimensional controllability\nmetrics and used them to identify the main driver areas of the human connectome\nobtained from N=6134 healthy individuals in the UK-biobank cohort. Results\nrevealed previously unappreciated influential regions compared to standard\napproaches, enabled to draw control maps between distinct specialized\nlarge-scale brain systems, and yielded an anatomically-based understanding of\nhemispheric functional lateralization. Taken together, our results offered a\ntheoretically-grounded solution to deal with network controllability in\nreal-life applications and provided insights into the causal interactions of\nthe human brain.",
            "author": [
                "Remy Ben Messaoud",
                "Vincent Le Du",
                "Brigitte Charlotte Kaufmann",
                "Baptiste Couvy-Duchesne",
                "Lara Migliaccio",
                "Paolo Bartolomeo",
                "Mario Chavez",
                "Fabrizio De Vico Fallani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11132v3",
                "http://arxiv.org/pdf/2311.11132v3"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11114v1",
            "title": "Environment-Aware Dynamic Graph Learning for Out-of-Distribution\n  Generalization",
            "updated": "2023-11-18T16:31:10Z",
            "published": "2023-11-18T16:31:10Z",
            "summary": "Dynamic graph neural networks (DGNNs) are increasingly pervasive in\nexploiting spatio-temporal patterns on dynamic graphs. However, existing works\nfail to generalize under distribution shifts, which are common in real-world\nscenarios. As the generation of dynamic graphs is heavily influenced by latent\nenvironments, investigating their impacts on the out-of-distribution (OOD)\ngeneralization is critical. However, it remains unexplored with the following\ntwo major challenges: (1) How to properly model and infer the complex\nenvironments on dynamic graphs with distribution shifts? (2) How to discover\ninvariant patterns given inferred spatio-temporal environments? To solve these\nchallenges, we propose a novel Environment-Aware dynamic Graph LEarning (EAGLE)\nframework for OOD generalization by modeling complex coupled environments and\nexploiting spatio-temporal invariant patterns. Specifically, we first design\nthe environment-aware EA-DGNN to model environments by multi-channel\nenvironments disentangling. Then, we propose an environment instantiation\nmechanism for environment diversification with inferred distributions. Finally,\nwe discriminate spatio-temporal invariant patterns for out-of-distribution\nprediction by the invariant pattern recognition mechanism and perform\nfine-grained causal interventions node-wisely with a mixture of instantiated\nenvironment samples. Experiments on real-world and synthetic dynamic graph\ndatasets demonstrate the superiority of our method against state-of-the-art\nbaselines under distribution shifts. To the best of our knowledge, we are the\nfirst to study OOD generalization on dynamic graphs from the environment\nlearning perspective.",
            "author": [
                "Haonan Yuan",
                "Qingyun Sun",
                "Xingcheng Fu",
                "Ziwei Zhang",
                "Cheng Ji",
                "Hao Peng",
                "Jianxin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11114v1",
                "http://arxiv.org/pdf/2311.11114v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11086v1",
            "title": "LightBTSeg: A lightweight breast tumor segmentation model using\n  ultrasound images via dual-path joint knowledge distillation",
            "updated": "2023-11-18T14:25:40Z",
            "published": "2023-11-18T14:25:40Z",
            "summary": "The accurate segmentation of breast tumors is an important prerequisite for\nlesion detection, which has significant clinical value for breast tumor\nresearch. The mainstream deep learning-based methods have achieved a\nbreakthrough. However, these high-performance segmentation methods are\nformidable to implement in clinical scenarios since they always embrace high\ncomputation complexity, massive parameters, slow inference speed, and huge\nmemory consumption. To tackle this problem, we propose LightBTSeg, a dual-path\njoint knowledge distillation framework, for lightweight breast tumor\nsegmentation. Concretely, we design a double-teacher model to represent the\nfine-grained feature of breast ultrasound according to different semantic\nfeature realignments of benign and malignant breast tumors. Specifically, we\nleverage the bottleneck architecture to reconstruct the original Attention\nU-Net. It is regarded as a lightweight student model named Simplified U-Net.\nThen, the prior knowledge of benign and malignant categories is utilized to\ndesign the teacher network combined dual-path joint knowledge distillation,\nwhich distills the knowledge from cumbersome benign and malignant teachers to a\nlightweight student model. Extensive experiments conducted on breast ultrasound\nimages (Dataset BUSI) and Breast Ultrasound Dataset B (Dataset B) datasets\ndemonstrate that LightBTSeg outperforms various counterparts.",
            "author": [
                "Hongjiang Guo",
                "Shengwen Wang",
                "Hao Dang",
                "Kangle Xiao",
                "Yaru Yang",
                "Wenpei Liu",
                "Tongtong Liu",
                "Yiying Wan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11086v1",
                "http://arxiv.org/pdf/2311.11086v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11085v1",
            "title": "Compositional Fusion of Signals in Data Embedding",
            "updated": "2023-11-18T14:20:56Z",
            "published": "2023-11-18T14:20:56Z",
            "summary": "Embeddings in AI convert symbolic structures into fixed-dimensional vectors,\neffectively fusing multiple signals. However, the nature of this fusion in\nreal-world data is often unclear. To address this, we introduce two methods:\n(1) Correlation-based Fusion Detection, measuring correlation between known\nattributes and embeddings, and (2) Additive Fusion Detection, viewing\nembeddings as sums of individual vectors representing attributes.\n  Applying these methods, word embeddings were found to combine semantic and\nmorphological signals. BERT sentence embeddings were decomposed into individual\nword vectors of subject, verb and object. In the knowledge graph-based\nrecommender system, user embeddings, even without training on demographic data,\nexhibited signals of demographics like age and gender.\n  This study highlights that embeddings are fusions of multiple signals, from\nWord2Vec components to demographic hints in graph embeddings.",
            "author": [
                "Zhijin Guo",
                "Zhaozhen Xu",
                "Martha Lewis",
                "Nello Cristianini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11085v1",
                "http://arxiv.org/pdf/2311.11085v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11083v1",
            "title": "ECLM: Efficient Edge-Cloud Collaborative Learning with Continuous\n  Environment Adaptation",
            "updated": "2023-11-18T14:10:09Z",
            "published": "2023-11-18T14:10:09Z",
            "summary": "Pervasive mobile AI applications primarily employ one of the two learning\nparadigms: cloud-based learning (with powerful large models) or on-device\nlearning (with lightweight small models). Despite their own advantages, neither\nparadigm can effectively handle dynamic edge environments with frequent data\ndistribution shifts and on-device resource fluctuations, inevitably suffering\nfrom performance degradation. In this paper, we propose ECLM, an edge-cloud\ncollaborative learning framework for rapid model adaptation for dynamic edge\nenvironments. We first propose a novel block-level model decomposition design\nto decompose the original large cloud model into multiple combinable modules.\nBy flexibly combining a subset of the modules, this design enables the\nderivation of compact, task-specific sub-models for heterogeneous edge devices\nfrom the large cloud model, and the seamless integration of new knowledge\nlearned on these devices into the cloud model periodically. As such, ECLM\nensures that the cloud model always provides up-to-date sub-models for edge\ndevices. We further propose an end-to-end learning framework that incorporates\nthe modular model design into an efficient model adaptation pipeline including\nan offline on-cloud model prototyping and training stage, and an online\nedge-cloud collaborative adaptation stage. Extensive experiments over various\ndatasets demonstrate that ECLM significantly improves model performance (e.g.,\n18.89% accuracy increase) and resource efficiency (e.g., 7.12x communication\ncost reduction) in adapting models to dynamic edge environments by efficiently\ncollaborating the edge and the cloud models.",
            "author": [
                "Yan Zhuang",
                "Zhenzhe Zheng",
                "Yunfeng Shao",
                "Bingshuai Li",
                "Fan Wu",
                "Guihai Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11083v1",
                "http://arxiv.org/pdf/2311.11083v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11075v1",
            "title": "A note on the uniqueness of minimal maps into $\\mathbb{R}^n$ via\n  singular values",
            "updated": "2023-11-18T13:49:54Z",
            "published": "2023-11-18T13:49:54Z",
            "summary": "In this note, we derive a uniqueness theorem for minimal graphs of general\ncodimension under certain restrictions closed related to the convexity (not\nstrict convexity) of the area functional with respect to singular values,\nimproving the result in \\cite{L-O-T}. The crucial step of the proof is to show\nthe local linearity of the singular value vectors along the geodesic homotopy\nof two given minimal maps.",
            "author": [
                "Minghao Li",
                "Ling Yang",
                "Taiyang Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11075v1",
                "http://arxiv.org/pdf/2311.11075v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "49Q05, 53A07, 53C42"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11073v1",
            "title": "Community-Aware Efficient Graph Contrastive Learning via Personalized\n  Self-Training",
            "updated": "2023-11-18T13:45:21Z",
            "published": "2023-11-18T13:45:21Z",
            "summary": "In recent years, graph contrastive learning (GCL) has emerged as one of the\noptimal solutions for various supervised tasks at the node level. However, for\nunsupervised and structure-related tasks such as community detection, current\nGCL algorithms face difficulties in acquiring the necessary community-level\ninformation, resulting in poor performance. In addition, general contrastive\nlearning algorithms improve the performance of downstream tasks by increasing\nthe number of negative samples, which leads to severe class collision and\nunfairness of community detection. To address above issues, we propose a novel\nCommunity-aware Efficient Graph Contrastive Learning Framework (CEGCL) to\njointly learn community partition and node representations in an end-to-end\nmanner. Specifically, we first design a personalized self-training (PeST)\nstrategy for unsupervised scenarios, which enables our model to capture precise\ncommunity-level personalized information in a graph. With the benefit of the\nPeST, we alleviate class collision and unfairness without sacrificing the\noverall model performance. Furthermore, the aligned graph clustering (AlGC) is\nemployed to obtain the community partition. In this module, we align the\nclustering space of our downstream task with that in PeST to achieve more\nconsistent node embeddings. Finally, we demonstrate the effectiveness of our\nmodel for community detection both theoretically and experimentally. Extensive\nexperimental results also show that our CEGCL exhibits state-of-the-art\nperformance on three benchmark datasets with different scales.",
            "author": [
                "Yuecheng Li",
                "Yanming Hu",
                "Lele Fu",
                "Chuan Chen",
                "Lei Yang",
                "Zibin Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11073v1",
                "http://arxiv.org/pdf/2311.11073v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11060v1",
            "title": "AIMS-EREA -- A framework for AI-accelerated Innovation of Materials for\n  Sustainability -- for Environmental Remediation and Energy Applications",
            "updated": "2023-11-18T12:35:45Z",
            "published": "2023-11-18T12:35:45Z",
            "summary": "Many environmental remediation and energy applications (conversion and\nstorage) for sustainability need design and development of green novel\nmaterials. Discovery processes of such novel materials are time taking and\ncumbersome due to large number of possible combinations and permutations of\nmaterials structures. Often theoretical studies based on Density Functional\nTheory (DFT) and other theories, coupled with Simulations are conducted to\nnarrow down sample space of candidate materials, before conducting\nlaboratory-based synthesis and analytical process. With the emergence of\nartificial intelligence (AI), AI techniques are being tried in this process too\nto ease out simulation time and cost. However tremendous values of previously\npublished research from various parts of the world are still left as\nlabor-intensive manual effort and discretion of individual researcher and prone\nto human omissions. AIMS-EREA is our novel framework to blend best of breed of\nMaterial Science theory with power of Generative AI to give best impact and\nsmooth and quickest discovery of material for sustainability. This also helps\nto eliminate the possibility of production of hazardous residues and\nbye-products of the reactions. AIMS-EREA uses all available resources --\nPredictive and Analytical AI on large collection of chemical databases along\nwith automated intelligent assimilation of deep materials knowledge from\npreviously published research works through Generative AI. We demonstrate use\nof our own novel framework with an example, how this framework can be\nsuccessfully applied to achieve desired success in development of\nthermoelectric material for waste heat conversion.",
            "author": [
                "Sudarson Roy Pratihar",
                "Deepesh Pai",
                "Manaswita Nag"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11060v1",
                "http://arxiv.org/pdf/2311.11060v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12054v1",
            "title": "Wet scavenging process of particulate matter (PM10): A multivariate\n  complex network approach",
            "updated": "2023-11-18T12:21:51Z",
            "published": "2023-11-18T12:21:51Z",
            "summary": "This paper reports the results of research on PM10 wet scavenging by rainfall\nusing a new multilayer complex networks called Multiplex Visibility Graphs\n(MVG). To the best of our knowledge, this work is the first to assess PM10 wet\ndeposition using multivariate time series according to African dust\nseasonality. We considered 11 years of daily PM10 and rainfall data from the\nGuadeloupe archipelago. To analyse the impact of rainfall on PM10 behaviour,\ntwo MVG parameters were computed: the average edge overlap ({\\omega}) and the\ninterlayer mutual information (IPM). On the 1-d scale, the {\\omega} results\nshowed that the wet scavenging process was higher during the second half of the\nyear when the high dust season and the rainy season are juxtaposed. This\nhighlights a greater correlation between the microscopic structure of the\nsignal, and the impact of rainfall on PM10 concentrations is more significant\nwhen the atmosphere is loaded with dust. The joint probability computed between\nthe PM10 and rainfall nodes confirmed this trend. The IPM results indicated a\ncorrelation between PM10 and rainfall structures throughout the year.\nFurthermore, IPM values were higher during the transition periods between\nwinter and summer (and vice versa). Our study showed that MVG is a powerful\ntechnique for investigating the relationship between at least two nonlinear\ntime series using a multivariate time series.",
            "author": [
                "T. Plocoste",
                "R. Carmona-Cabezas",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.apr.2021.101095",
                "http://arxiv.org/abs/2311.12054v1",
                "http://arxiv.org/pdf/2311.12054v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11039v1",
            "title": "Synthetic Data Generation for Bridging Sim2Real Gap in a Production\n  Environment",
            "updated": "2023-11-18T11:15:08Z",
            "published": "2023-11-18T11:15:08Z",
            "summary": "Synthetic data is being used lately for training deep neural networks in\ncomputer vision applications such as object detection, object segmentation and\n6D object pose estimation. Domain randomization hereby plays an important role\nin reducing the simulation to reality gap. However, this generalization might\nnot be effective in specialized domains like a production environment involving\ncomplex assemblies. Either the individual parts, trained with synthetic images,\nare integrated in much larger assemblies making them indistinguishable from\ntheir counterparts and result in false positives or are partially occluded just\nenough to give rise to false negatives. Domain knowledge is vital in these\ncases and if conceived effectively while generating synthetic data, can show a\nconsiderable improvement in bridging the simulation to reality gap. This paper\nfocuses on synthetic data generation procedures for parts and assemblies used\nin a production environment. The basic procedures for synthetic data generation\nand their various combinations are evaluated and compared on images captured in\na production environment, where results show up to 15% improvement using\ncombinations of basic procedures. Reducing the simulation to reality gap in\nthis way can aid to utilize the true potential of robot assisted production\nusing artificial intelligence.",
            "author": [
                "Parth Rawal",
                "Mrunal Sompura",
                "Wolfgang Hintze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11039v1",
                "http://arxiv.org/pdf/2311.11039v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12053v1",
            "title": "Background PM10 atmosphere: In the seek of a multifractal\n  characterization using complex networks",
            "updated": "2023-11-18T10:50:59Z",
            "published": "2023-11-18T10:50:59Z",
            "summary": "In the literature, several epidemiological studies have already associated\nrespiratory and cardiovascular diseases to acute exposure of mineral dust.\nHowever, frail people are also sensitive to chronic exposure to particulate\nmatter with an aerodynamic diameter 10{\\mu}m or less (PM10). Consequently, it\nis crucial to better understand PM10 fluctuations at all scales. This study\ninvestigates PM10 background atmosphere in the Caribbean area according to\nAfrican dust seasonality with complex network framework. For that purpose, the\nregular Visibility Graph (VG) and the new Upside-Down Visibility Graph (UDVG)\nare used for a multifractal analysis. Firstly, concentration vs degree (v-k)\nplots highlighted that high degree values (hubs behavior) are related to the\nhighest PM10 concentrations in VG while hubs is associated to the lowest\nconcentrations in UDVG, i.e. probably the background atmosphere. Then, the\ndegree distribution analysis showed that VG and UDVG difference is reduced for\nhigh dust season contrary to the low one. As regards the multifractal analysis,\nthe multifractal degree is higher for the low season in VG while it is higher\nfor the high season in UDVG. The degree distribution behavior and the opposite\ntrend in multifractal degree for UDVG are due to the increase of PM10\nbackground atmosphere during the high season, i.e. from May to September. To\nsum up, UDGV is an efficient tool to perform noise fluctuations analysis in\nenvironmental time series where low concentrations play an important role as\nwell.",
            "author": [
                "T. Plocoste",
                "R. Carmona-Cabezas",
                "F. J. Jimenez-Hornero",
                "E. Gutierrez de Rave"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.jaerosci.2021.105777",
                "http://arxiv.org/abs/2311.12053v1",
                "http://arxiv.org/pdf/2311.12053v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11034v1",
            "title": "Almost complex structure on finite points from bidirected graphs",
            "updated": "2023-11-18T10:49:51Z",
            "published": "2023-11-18T10:49:51Z",
            "summary": "We show that there is an almost complex structure on a differential calculus\non finite points coming from a bidirected finite graph without multiple edges\nor loops. We concentrate on a polygon as a concrete case. In particular, a\n`holomorphic structure on the exterior bundle' built from the polygon is\nstudied. Also a positive Hochschild 2-cocycle on the vertex set of the polygon,\nalbeit a trivial one, is shown to arise naturally from the almost complex\nstructure.",
            "author": [
                "Soumalya Joardar",
                "Atibur Rahaman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11034v1",
                "http://arxiv.org/pdf/2311.11034v1"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA",
                "46L87, 05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11029v1",
            "title": "Geometric Data Augmentations to Mitigate Distribution Shifts in Pollen\n  Classification from Microscopic Images",
            "updated": "2023-11-18T10:35:18Z",
            "published": "2023-11-18T10:35:18Z",
            "summary": "Distribution shifts are characterized by differences between the training and\ntest data distributions. They can significantly reduce the accuracy of machine\nlearning models deployed in real-world scenarios. This paper explores the\ndistribution shift problem when classifying pollen grains from microscopic\nimages collected in the wild with a low-cost camera sensor. We leverage the\ndomain knowledge that geometric features are highly important for accurate\npollen identification and introduce two novel geometric image augmentation\ntechniques to significantly narrow the accuracy gap between the model\nperformance on the train and test datasets. In particular, we show that\nTenengrad and ImageToSketch filters are highly effective to balance the shape\nand texture information while leaving out unimportant details that may confuse\nthe model. Extensive evaluations on various model architectures demonstrate a\nconsistent improvement of the model generalization to field data of up to 14%\nachieved by the geometric augmentation techniques when compared to a wide range\nof standard image augmentations. The approach is validated through an ablation\nstudy using pollen hydration tests to recover the shape of dry pollen grains.\nThe proposed geometric augmentations also receive the highest scores according\nto the affinity and diversity measures from the literature.",
            "author": [
                "Nam Cao",
                "Olga Saukh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11029v1",
                "http://arxiv.org/pdf/2311.11029v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12052v1",
            "title": "MagicDance: Realistic Human Dance Video Generation with Motions & Facial\n  Expressions Transfer",
            "updated": "2023-11-18T10:22:44Z",
            "published": "2023-11-18T10:22:44Z",
            "summary": "In this work, we propose MagicDance, a diffusion-based model for 2D human\nmotion and facial expression transfer on challenging human dance videos.\nSpecifically, we aim to generate human dance videos of any target identity\ndriven by novel pose sequences while keeping the identity unchanged. To this\nend, we propose a two-stage training strategy to disentangle human motions and\nappearance (e.g., facial expressions, skin tone and dressing), consisting of\nthe pretraining of an appearance-control block and fine-tuning of an\nappearance-pose-joint-control block over human dance poses of the same dataset.\nOur novel design enables robust appearance control with temporally consistent\nupper body, facial attributes, and even background. The model also generalizes\nwell on unseen human identities and complex motion sequences without the need\nfor any fine-tuning with additional data with diverse human attributes by\nleveraging the prior knowledge of image diffusion models. Moreover, the\nproposed model is easy to use and can be considered as a plug-in\nmodule/extension to Stable Diffusion. We also demonstrate the model's ability\nfor zero-shot 2D animation generation, enabling not only the appearance\ntransfer from one identity to another but also allowing for cartoon-like\nstylization given only pose inputs. Extensive experiments demonstrate our\nsuperior performance on the TikTok dataset.",
            "author": [
                "Di Chang",
                "Yichun Shi",
                "Quankai Gao",
                "Jessica Fu",
                "Hongyi Xu",
                "Guoxian Song",
                "Qing Yan",
                "Xiao Yang",
                "Mohammad Soleymani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12052v1",
                "http://arxiv.org/pdf/2311.12052v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12050v2",
            "title": "3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing",
            "updated": "2023-11-23T13:06:33Z",
            "published": "2023-11-18T09:55:56Z",
            "summary": "The current GAN inversion methods typically can only edit the appearance and\nshape of a single object and background while overlooking spatial information.\nIn this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted\nediting of affine information (scale, translation, and rotation) on multiple\nobjects. 3D-GOI realizes the complex editing function by inverting the\nabundance of attribute codes (object\nshape/appearance/scale/rotation/translation, background shape/appearance, and\ncamera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all\nthe codes is challenging, 3D-GOI solves this challenge following three main\nsteps. First, we segment the objects and the background in a multi-object\nimage. Second, we use a custom Neural Inversion Encoder to obtain coarse codes\nof each object. Finally, we use a round-robin optimization algorithm to get\nprecise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is\nthe first framework to enable multifaceted editing on multiple objects. Both\nqualitative and quantitative experiments demonstrate that 3D-GOI holds immense\npotential for flexible, multifaceted editing in complex multi-object scenes.",
            "author": [
                "Haoran Li",
                "Long Ma",
                "Yong Liao",
                "Lechao Cheng",
                "Yanbin Hao",
                "Pengyuan Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12050v2",
                "http://arxiv.org/pdf/2311.12050v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12047v1",
            "title": "Multimodal Machine Unlearning",
            "updated": "2023-11-18T08:30:38Z",
            "published": "2023-11-18T08:30:38Z",
            "summary": "Machine Unlearning is the process of removing specific training data samples\nand their corresponding effects from an already trained model. It has\nsignificant practical benefits, such as purging private, inaccurate, or\noutdated information from trained models without the need for complete\nre-training. Unlearning within a multimodal setting presents unique challenges\ndue to the intrinsic dependencies between different data modalities and the\nexpensive cost of training on large multimodal datasets and architectures.\nCurrent approaches to machine unlearning have not fully addressed these\nchallenges. To bridge this gap, we introduce MMUL, a machine unlearning\napproach specifically designed for multimodal data and models. MMUL formulates\nthe multimodal unlearning task by focusing on three key properties: (a):\nmodality decoupling, which effectively decouples the association between\nindividual unimodal data points within multimodal inputs marked for deletion,\nrendering them as unrelated data points within the model's context, (b):\nunimodal knowledge retention, which retains the unimodal representation\ncapability of the model post-unlearning, and (c): multimodal knowledge\nretention, which retains the multimodal representation capability of the model\npost-unlearning. MMUL is efficient to train and is not constrained by the\nrequirement of using a strongly convex loss. Experiments on two multimodal\nmodels and four multimodal benchmark datasets, including vision-language and\ngraph-language datasets, show that MMUL outperforms existing baselines, gaining\nan average improvement of +17.6 points against the best-performing unimodal\nbaseline in distinguishing between deleted and remaining data. In addition,\nMMUL can largely maintain pre-existing knowledge of the original model post\nunlearning, with a performance gap of only 0.3 points compared to retraining a\nnew model from scratch.",
            "author": [
                "Jiali Cheng",
                "Hadi Amiri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12047v1",
                "http://arxiv.org/pdf/2311.12047v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.11009v1",
            "title": "Joyful: Joint Modality Fusion and Graph Contrastive Learning for\n  Multimodal Emotion Recognition",
            "updated": "2023-11-18T08:21:42Z",
            "published": "2023-11-18T08:21:42Z",
            "summary": "Multimodal emotion recognition aims to recognize emotions for each utterance\nof multiple modalities, which has received increasing attention for its\napplication in human-machine interaction. Current graph-based methods fail to\nsimultaneously depict global contextual features and local diverse uni-modal\nfeatures in a dialogue. Furthermore, with the number of graph layers\nincreasing, they easily fall into over-smoothing. In this paper, we propose a\nmethod for joint modality fusion and graph contrastive learning for multimodal\nemotion recognition (Joyful), where multimodality fusion, contrastive learning,\nand emotion recognition are jointly optimized. Specifically, we first design a\nnew multimodal fusion mechanism that can provide deep interaction and fusion\nbetween the global contextual and uni-modal specific features. Then, we\nintroduce a graph contrastive learning framework with inter-view and intra-view\ncontrastive losses to learn more distinguishable representations for samples\nwith different sentiments. Extensive experiments on three benchmark datasets\nindicate that Joyful achieved state-of-the-art (SOTA) performance compared to\nall baselines.",
            "author": [
                "Dongyuan Li",
                "Yusong Wang",
                "Kotaro Funakoshi",
                "Manabu Okumura"
            ],
            "link": [
                "http://arxiv.org/abs/2311.11009v1",
                "http://arxiv.org/pdf/2311.11009v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10998v1",
            "title": "Learning Scene Context Without Images",
            "updated": "2023-11-18T07:27:25Z",
            "published": "2023-11-18T07:27:25Z",
            "summary": "Teaching machines of scene contextual knowledge would enable them to interact\nmore effectively with the environment and to anticipate or predict objects that\nmay not be immediately apparent in their perceptual field. In this paper, we\nintroduce a novel transformer-based approach called $LMOD$ ( Label-based\nMissing Object Detection) to teach scene contextual knowledge to machines using\nan attention mechanism. A distinctive aspect of the proposed approach is its\nreliance solely on labels from image datasets to teach scene context, entirely\neliminating the need for the actual image itself. We show how scene-wide\nrelationships among different objects can be learned using a self-attention\nmechanism. We further show that the contextual knowledge gained from label\nbased learning can enhance performance of other visual based object detection\nalgorithm.",
            "author": [
                "Amirreza Rouhi",
                "David Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10998v1",
                "http://arxiv.org/pdf/2311.10998v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10995v1",
            "title": "Behavior Optimized Image Generation",
            "updated": "2023-11-18T07:07:38Z",
            "published": "2023-11-18T07:07:38Z",
            "summary": "The last few years have witnessed great success on image generation, which\nhas crossed the acceptance thresholds of aesthetics, making it directly\napplicable to personal and commercial applications. However, images, especially\nin marketing and advertising applications, are often created as a means to an\nend as opposed to just aesthetic concerns. The goal can be increasing sales,\ngetting more clicks, likes, or image sales (in the case of stock businesses).\nTherefore, the generated images need to perform well on these key performance\nindicators (KPIs), in addition to being aesthetically good. In this paper, we\nmake the first endeavor to answer the question of \"How can one infuse the\nknowledge of the end-goal within the image generation process itself to create\nnot just better-looking images but also \"better-performing'' images?''. We\npropose BoigLLM, an LLM that understands both image content and user behavior.\nBoigLLM knows how an image should look to get a certain required KPI. We show\nthat BoigLLM outperforms 13x larger models such as GPT-3.5 and GPT-4 in this\ntask, demonstrating that while these state-of-the-art models can understand\nimages, they lack information on how these images perform in the real world. To\ngenerate actual pixels of behavior-conditioned images, we train a\ndiffusion-based model (BoigSD) to align with a proposed BoigLLM-defined reward.\nWe show the performance of the overall pipeline on two datasets covering two\ndifferent behaviors: a stock dataset with the number of forward actions as the\nKPI and a dataset containing tweets with the total likes as the KPI, denoted as\nBoigBench. To advance research in the direction of utility-driven image\ngeneration and understanding, we release BoigBench, a benchmark dataset\ncontaining 168 million enterprise tweets with their media, brand account names,\ntime of post, and total likes.",
            "author": [
                "Varun Khurana",
                "Yaman K Singla",
                "Jayakumar Subramanian",
                "Rajiv Ratn Shah",
                "Changyou Chen",
                "Zhiqiang Xu",
                "Balaji Krishnamurthy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10995v1",
                "http://arxiv.org/pdf/2311.10995v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10993v1",
            "title": "Multiplex Visibility Graphs as a complementary tool for describing the\n  relation between ground level O3 and NO2",
            "updated": "2023-11-18T07:02:41Z",
            "published": "2023-11-18T07:02:41Z",
            "summary": "The usage of multilayer complex networks for the analysis of correlations\namong environmental variables (such as O3 and NO2 concentrations from the\nphotochemical smog) is investigated in this work. The mentioned technique is\ncalled Multiplex Visibility Graphs (MVG). By performing the joint analysis of\nthose layers, the parameters named Average Edge Overlap and Interlayer Mutual\nInformation are extracted, which accounts for the microscopical time coherence\nand the correlation between the time series behavior, respectively. These\nparameters point to the possibility of using them independently to describe the\ncorrelation between atmospheric pollutants (which could be extended to\nenvironmental time series). More precisely the first one of them is considered\nto be a potential new approach to determine the time required for the\ncorrelation of NO2 and O3 to be observed, since it is obtained from the\ncorrelation of the pollutants at the smallest time scale. As for the second\none, it has been checked that the proposed technique can be used to describe\nthe variation of the correlation between the two gases along the seasons. In\nshort, MVGs parameters are introduced and results show that they could be\npotentially used in a future for correlation studies, supplementing already\nexisting techniques.",
            "author": [
                "R. Carmona-Cabezas",
                "J. Gomez-Gomez",
                "A. B. Ariza-Villaverde",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.apr.2019.10.011",
                "http://arxiv.org/abs/2311.10993v1",
                "http://arxiv.org/pdf/2311.10993v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10991v1",
            "title": "Improving graph-based detection of singular events for photochemical\n  smog agents",
            "updated": "2023-11-18T06:54:47Z",
            "published": "2023-11-18T06:54:47Z",
            "summary": "Recently, a set of graph-based tools have been introduced for the\nidentification of singular events of O3, NO2 and temperature time series, as\nwell as description of their dynamics. These are based on the use of the\nVisibility Graphs (VG). In this work, an improvement of the original approach\nis proposed, being called Upside-Down Visibility Graph (UDVG). It adds the\npossibility of investigating the singular lowest episodes, instead of the\nhighest. Results confirm the applicability of the new method for describing the\nmultifractal nature of the underlying O3, NO2, and temperature. Asymmetries in\nthe NO2 degree distribution are observed, possibly due to the interaction with\ndifferent chemicals. Furthermore, a comparison of VG and UDVG has been\nperformed and the outcomes show that they describe opposite subsets of the time\nseries (low and high values) as expected. The combination of the results from\nthe two networks is proposed and evaluated, with the aim of obtaining all the\ninformation at once. It turns out to be a more complete tool for singularity\ndetection in photochemical time series, which could be a valuable asset for\nfuture research.",
            "author": [
                "R. Carmona-Cabezas",
                "J. Gomez-Gomez",
                "E. Gutierrez de Rave",
                "E. Sanchez-Lopez",
                "J. Serrano",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.chemosphere.2020.126660",
                "http://arxiv.org/abs/2311.10991v1",
                "http://arxiv.org/pdf/2311.10991v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10989v1",
            "title": "Checking complex networks indicators in search of singular episodes of\n  the photochemical smog",
            "updated": "2023-11-18T06:49:40Z",
            "published": "2023-11-18T06:49:40Z",
            "summary": "A set of indicators derived from the analysis of complex networks have been\nintroduced to identify singularities on a time series. To that end, the\nVisibility Graphs (VG) from three different signals related to photochemical\nsmog (O3, NO2 concentration and temperature) have been computed. From the\nresulting complex network, the centrality parameters have been obtained and\ncompared among them. Besides, they have been contrasted to two others that\narise from a multifractal point of view, that have been widely used for\nsingularity detection in many fields: the Holder and singularity exponents\n(specially the first one of them). The outcomes show that the complex network\nindicators give equivalent results to those already tested, even exhibiting\nsome advantages such as the unambiguity and the more selective results. This\nsuggest a favorable position as supplementary sources of information when\ndetecting singularities in several environmental variables, such as pollutant\nconcentration or temperature.",
            "author": [
                "R. Carmona-Cabezas",
                "J. Gomez-Gomez",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.chemosphere.2019.125085",
                "http://arxiv.org/abs/2311.10989v1",
                "http://arxiv.org/pdf/2311.10989v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10988v1",
            "title": "Expanding Scene Graph Boundaries: Fully Open-vocabulary Scene Graph\n  Generation via Visual-Concept Alignment and Retention",
            "updated": "2023-11-18T06:49:17Z",
            "published": "2023-11-18T06:49:17Z",
            "summary": "Scene Graph Generation (SGG) offers a structured representation critical in\nmany computer vision applications. Traditional SGG approaches, however, are\nlimited by a closed-set assumption, restricting their ability to recognize only\npredefined object and relation categories. To overcome this, we categorize SGG\nscenarios into four distinct settings based on the node and edge: Closed-set\nSGG, Open Vocabulary (object) Detection-based SGG (OvD-SGG), Open Vocabulary\nRelation-based SGG (OvR-SGG), and Open Vocabulary Detection + Relation-based\nSGG (OvD+R-SGG). While object-centric open vocabulary SGG has been studied\nrecently, the more challenging problem of relation-involved open-vocabulary SGG\nremains relatively unexplored. To fill this gap, we propose a unified framework\nnamed OvSGTR towards fully open vocabulary SGG from a holistic view. The\nproposed framework is an end-toend transformer architecture, which learns a\nvisual-concept alignment for both nodes and edges, enabling the model to\nrecognize unseen categories. For the more challenging settings of\nrelation-involved open vocabulary SGG, the proposed approach integrates\nrelation-aware pre-training utilizing image-caption data and retains\nvisual-concept alignment through knowledge distillation. Comprehensive\nexperimental results on the Visual Genome benchmark demonstrate the\neffectiveness and superiority of the proposed framework.",
            "author": [
                "Zuyao Chen",
                "Jinlin Wu",
                "Zhen Lei",
                "Zhaoxiang Zhang",
                "Changwen Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10988v1",
                "http://arxiv.org/pdf/2311.10988v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10986v3",
            "title": "EdgeFM: Leveraging Foundation Model for Open-set Learning on the Edge",
            "updated": "2023-11-23T04:44:00Z",
            "published": "2023-11-18T06:40:39Z",
            "summary": "Deep Learning (DL) models have been widely deployed on IoT devices with the\nhelp of advancements in DL algorithms and chips. However, the limited resources\nof edge devices make these on-device DL models hard to be generalizable to\ndiverse environments and tasks. Although the recently emerged foundation models\n(FMs) show impressive generalization power, how to effectively leverage the\nrich knowledge of FMs on resource-limited edge devices is still not explored.\nIn this paper, we propose EdgeFM, a novel edge-cloud cooperative system with\nopen-set recognition capability. EdgeFM selectively uploads unlabeled data to\nquery the FM on the cloud and customizes the specific knowledge and\narchitectures for edge models. Meanwhile, EdgeFM conducts dynamic model\nswitching at run-time taking into account both data uncertainty and dynamic\nnetwork variations, which ensures the accuracy always close to the original FM.\nWe implement EdgeFM using two FMs on two edge platforms. We evaluate EdgeFM on\nthree public datasets and two self-collected datasets. Results show that EdgeFM\ncan reduce the end-to-end latency up to 3.2x and achieve 34.3% accuracy\nincrease compared with the baseline.",
            "author": [
                "Bufang Yang",
                "Lixing He",
                "Neiwen Ling",
                "Zhenyu Yan",
                "Guoliang Xing",
                "Xian Shuai",
                "Xiaozhe Ren",
                "Xin Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10986v3",
                "http://arxiv.org/pdf/2311.10986v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10984v1",
            "title": "Black Hole Search in Dynamic Cactus Graph",
            "updated": "2023-11-18T06:32:56Z",
            "published": "2023-11-18T06:32:56Z",
            "summary": "We study the problem of black hole search by a set of mobile agents, where\nthe underlying graph is a dynamic cactus. A black hole is a dangerous vertex in\nthe graph that eliminates any visiting agent without leaving any trace behind.\nKey parameters that dictate the complexity of finding the black hole include:\nthe number of agents required (termed as \\textit{size}), the number of moves\nperformed by the agents in order to determine the black hole location (termed\nas \\textit{move}) and the \\textit{time} (or round) taken to terminate. This\nproblem has already been studied where the underlying graph is a dynamic ring\n\\cite{di2021black}. In this paper, we extend the same problem to a dynamic\ncactus. We introduce two categories of dynamicity, but still the underlying\ngraph needs to be connected: first, we examine the scenario where, at most, one\ndynamic edge can disappear or reappear at any round. Secondly, we consider the\nproblem for at most $k$ dynamic edges. In both scenarios, we establish lower\nand upper bounds for the necessary number of agents, moves and rounds.",
            "author": [
                "Adri Bhattacharya",
                "Giuseppe F. Italiano",
                "Partha Sarathi Mandal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10984v1",
                "http://arxiv.org/pdf/2311.10984v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10979v1",
            "title": "Asymptotic distributions of the average clustering coefficient and its\n  variant",
            "updated": "2023-11-18T06:04:02Z",
            "published": "2023-11-18T06:04:02Z",
            "summary": "In network data analysis, summary statistics of a network can provide us with\nmeaningful insight into the structure of the network. The average clustering\ncoefficient is one of the most popular and widely used network statistics. In\nthis paper, we investigate the asymptotic distributions of the average\nclustering coefficient and its variant of a heterogeneous Erd\\\"{o}s-R\\'{e}nyi\nrandom graph. We show that the standardized average clustering coefficient\nconverges in distribution to the standard normal distribution. Interestingly,\nthe variance of the average clustering coefficient exhibits a phase transition\nphenomenon. The sum of weighted triangles is a variant of the average\nclustering coefficient. It is recently introduced to detect geometry in a\nnetwork. We also derive the asymptotic distribution of the sum weighted\ntriangles, which does not exhibit a phase transition phenomenon as the average\nclustering coefficient. This result signifies the difference between the two\nsummary statistics.",
            "author": [
                "Mingao Yuan",
                "Xiaofeng Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10979v1",
                "http://arxiv.org/pdf/2311.10979v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "physics.soc-ph",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10975v1",
            "title": "Engage Wider Audience or Facilitate Quality Answers? a Mixed-methods\n  Analysis of Questioning Strategies for Research Sensemaking on a Community\n  Q&A Site",
            "updated": "2023-11-18T05:05:21Z",
            "published": "2023-11-18T05:05:21Z",
            "summary": "Discussing research-sensemaking questions on Community Question and Answering\n(CQA) platforms has been an increasingly common practice for the public to\nparticipate in science communication. Nonetheless, how users strategically\ncraft research-sensemaking questions to engage public participation and\nfacilitate knowledge construction is a significant yet less understood problem.\nTo fill this gap, we collected 837 science-related questions and 157,684\nanswers from Zhihu, and conducted a mixed-methods study to explore\nuser-developed strategies in proposing research-sensemaking questions, and\ntheir potential effects on public engagement and knowledge construction.\nThrough open coding, we captured a comprehensive taxonomy of question-crafting\nstrategies, such as eyecatching narratives with counter-intuitive claims and\nrigorous descriptions with data use. Regression analysis indicated that these\nstrategies correlated with user engagement and answer construction in different\nways (e.g., emotional questions attracted more views and answers), yet there\nexisted a general divergence between wide participation and quality knowledge\nestablishment, when most questioning strategies could not ensure both. Based on\nlog analysis, we further found that collaborative editing afforded unique\nvalues in refining research-sensemaking questions regarding accuracy, rigor,\ncomprehensiveness and attractiveness. We propose design implications to\nfacilitate accessible, accurate and engaging science communication on CQA\nplatforms.",
            "author": [
                "Changyang He",
                "Yue Deng",
                "Lu He",
                "Qingyu Guo",
                "Yu Zhang",
                "Zhicong Lu",
                "Bo Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10975v1",
                "http://arxiv.org/pdf/2311.10975v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10972v1",
            "title": "Polynomial-Time Solutions for ReLU Network Training: A Complexity\n  Classification via Max-Cut and Zonotopes",
            "updated": "2023-11-18T04:41:07Z",
            "published": "2023-11-18T04:41:07Z",
            "summary": "We investigate the complexity of training a two-layer ReLU neural network\nwith weight decay regularization. Previous research has shown that the optimal\nsolution of this problem can be found by solving a standard cone-constrained\nconvex program. Using this convex formulation, we prove that the hardness of\napproximation of ReLU networks not only mirrors the complexity of the Max-Cut\nproblem but also, in certain special cases, exactly corresponds to it. In\nparticular, when $\\epsilon\\leq\\sqrt{84/83}-1\\approx 0.006$, we show that it is\nNP-hard to find an approximate global optimizer of the ReLU network objective\nwith relative error $\\epsilon$ with respect to the objective value. Moreover,\nwe develop a randomized algorithm which mirrors the Goemans-Williamson rounding\nof semidefinite Max-Cut relaxations. To provide polynomial-time approximations,\nwe classify training datasets into three categories: (i) For orthogonal\nseparable datasets, a precise solution can be obtained in polynomial-time. (ii)\nWhen there is a negative correlation between samples of different classes, we\ngive a polynomial-time approximation with relative error $\\sqrt{\\pi/2}-1\\approx\n0.253$. (iii) For general datasets, the degree to which the problem can be\napproximated in polynomial-time is governed by a geometric factor that controls\nthe diameter of two zonotopes intrinsic to the dataset. To our knowledge, these\nresults present the first polynomial-time approximation guarantees along with\nfirst hardness of approximation results for regularized ReLU networks.",
            "author": [
                "Yifei Wang",
                "Mert Pilanci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10972v1",
                "http://arxiv.org/pdf/2311.10972v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10969v1",
            "title": "MATILDA: Inclusive Data Science Pipelines Design through Computational\n  Creativity",
            "updated": "2023-11-18T04:37:07Z",
            "published": "2023-11-18T04:37:07Z",
            "summary": "We argue for the need for a new generation of data science solutions that can\ndemocratize recent advances in data engineering and artificial intelligence for\nnon-technical users from various disciplines, enabling them to unlock the full\npotential of these solutions. To do so, we adopt an approach whereby\ncomputational creativity and conversational computing are combined to guide\nnon-specialists intuitively to explore and extract knowledge from data\ncollections. The paper introduces MATILDA, a creativity-based data science\ndesign platform, showing how it can support the design process of data science\npipelines guided by human and computational creativity.",
            "author": [
                "Genoveva Vargas-Solar",
                "Santiago Negrete-Yankelevich",
                "Javier A. Espinosa-Oviedo",
                "Khalid Belhajjame",
                "Jos\u00e9-Luis Zechinelli-Martini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10969v1",
                "http://arxiv.org/pdf/2311.10969v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10968v1",
            "title": "2d Quantum Breakdown Model with Krylov Subspace Many-Body Localization",
            "updated": "2023-11-18T04:33:26Z",
            "published": "2023-11-18T04:33:26Z",
            "summary": "We propose a two-dimensional (2d) quantum breakdown model of hardcore bosons\ninteracting with disordered spins, which resembles particles incident into\nsupersaturated vapor. The model exhibits a strong fragmentation into Krylov\nsubspaces in each symmetry sector. The Hamiltonian in each Krylov subspace maps\nto a single-particle problem in a Cayley tree-like graph, which enters a 2d\nmany-body localization (MBL) phase beyond certain disorder strength $W_*$, as\nindicated by Poisson level spacing statistics and entanglement entropy growing\nas $\\log t$ with time $t$. Our theoretical arguments suggest $W_*$ is finite or\nzero for boson number $N_b\\lesssim L^\\gamma/\\log L$ ($1/2\\le \\gamma \\le 1$) as\nsystem size $L\\rightarrow\\infty$. At zero disorder, the model also exhibits\nfully or partially solvable features, such as degenerate quantum scar states.",
            "author": [
                "Xinyu Liu",
                "Biao Lian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10968v1",
                "http://arxiv.org/pdf/2311.10968v1"
            ],
            "primary_category": "cond-mat.str-el",
            "category": [
                "cond-mat.str-el",
                "cond-mat.dis-nn",
                "cond-mat.quant-gas",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10963v1",
            "title": "Learning Deterministic Finite Automata from Confidence Oracles",
            "updated": "2023-11-18T04:21:05Z",
            "published": "2023-11-18T04:21:05Z",
            "summary": "We discuss the problem of learning a deterministic finite automaton (DFA)\nfrom a confidence oracle. That is, we are given access to an oracle $Q$ with\nincomplete knowledge of some target language $L$ over an alphabet $\\Sigma$; the\noracle maps a string $x\\in\\Sigma^*$ to a score in the interval $[-1,1]$\nindicating its confidence that the string is in the language. The\ninterpretation is that the sign of the score signifies whether $x\\in L$, while\nthe magnitude $|Q(x)|$ represents the oracle's confidence. Our goal is to learn\na DFA representation of the oracle that preserves the information that it is\nconfident in. The learned DFA should closely match the oracle wherever it is\nhighly confident, but it need not do this when the oracle is less sure of\nitself.",
            "author": [
                "Wilson Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10963v1",
                "http://arxiv.org/pdf/2311.10963v1"
            ],
            "primary_category": "cs.FL",
            "category": [
                "cs.FL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10947v1",
            "title": "RecExplainer: Aligning Large Language Models for Recommendation Model\n  Interpretability",
            "updated": "2023-11-18T03:05:43Z",
            "published": "2023-11-18T03:05:43Z",
            "summary": "Recommender systems are widely used in various online services, with\nembedding-based models being particularly popular due to their expressiveness\nin representing complex signals. However, these models often lack\ninterpretability, making them less reliable and transparent for both users and\ndevelopers. With the emergence of large language models (LLMs), we find that\ntheir capabilities in language expression, knowledge-aware reasoning, and\ninstruction following are exceptionally powerful. Based on this, we propose a\nnew model interpretation approach for recommender systems, by using LLMs as\nsurrogate models and learn to mimic and comprehend target recommender models.\nSpecifically, we introduce three alignment methods: behavior alignment,\nintention alignment, and hybrid alignment. Behavior alignment operates in the\nlanguage space, representing user preferences and item information as text to\nlearn the recommendation model's behavior; intention alignment works in the\nlatent space of the recommendation model, using user and item representations\nto understand the model's behavior; hybrid alignment combines both language and\nlatent spaces for alignment training. To demonstrate the effectiveness of our\nmethods, we conduct evaluation from two perspectives: alignment effect, and\nexplanation generation ability on three public datasets. Experimental results\nindicate that our approach effectively enables LLMs to comprehend the patterns\nof recommendation models and generate highly credible recommendation\nexplanations.",
            "author": [
                "Yuxuan Lei",
                "Jianxun Lian",
                "Jing Yao",
                "Xu Huang",
                "Defu Lian",
                "Xing Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10947v1",
                "http://arxiv.org/pdf/2311.10947v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10941v1",
            "title": "Comparison among Classical, Probabilistic and Quantum Algorithms for\n  Hamiltonian Cycle problem",
            "updated": "2023-11-18T02:36:10Z",
            "published": "2023-11-18T02:36:10Z",
            "summary": "The Hamiltonian cycle problem (HCP), which is an NP-complete problem,\nconsists of having a graph G with n nodes and m edges and finding the path that\nconnects each node exactly once. In this paper we compare some algorithms to\nsolve a Hamiltonian cycle problem, using different models of computations and\nespecially the probabilistic and quantum ones. Starting from the classical\nprobabilistic approach of random walks, we take a step to the quantum direction\nby involving an ad hoc designed Quantum Turing Machine (QTM), which can be a\nuseful conceptual project tool for quantum algorithms. Introducing several\nconstraints to the graphs, our analysis leads to not-exponential speedup\nimprovements to the best-known algorithms. In particular, the results are based\non bounded degree graphs (graphs with nodes having a maximum number of edges)\nand graphs with the right limited number of nodes and edges to allow them to\noutperform the other algorithms.",
            "author": [
                "Giuseppe Corrente",
                "Carlo Vincenzo Stanzione",
                "Vittoria Stanzione"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10941v1",
                "http://arxiv.org/pdf/2311.10941v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DS",
                "cs.ET",
                "F.1.m"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10937v1",
            "title": "Bridging Data-Driven and Knowledge-Driven Approaches for Safety-Critical\n  Scenario Generation in Automated Vehicle Validation",
            "updated": "2023-11-18T02:11:14Z",
            "published": "2023-11-18T02:11:14Z",
            "summary": "Automated driving vehicles~(ADV) promise to enhance driving efficiency and\nsafety, yet they face intricate challenges in safety-critical scenarios. As a\nresult, validating ADV within generated safety-critical scenarios is essential\nfor both development and performance evaluations. This paper investigates the\ncomplexities of employing two major scenario-generation solutions: data-driven\nand knowledge-driven methods. Data-driven methods derive scenarios from\nrecorded datasets, efficiently generating scenarios by altering the existing\nbehavior or trajectories of traffic participants but often falling short in\nconsidering ADV perception; knowledge-driven methods provide effective coverage\nthrough expert-designed rules, but they may lead to inefficiency in generating\nsafety-critical scenarios within that coverage. To overcome these challenges,\nwe introduce BridgeGen, a safety-critical scenario generation framework,\ndesigned to bridge the benefits of both methodologies. Specifically, by\nutilizing ontology-based techniques, BridgeGen models the five scenario layers\nin the operational design domain (ODD) from knowledge-driven methods, ensuring\nbroad coverage, and incorporating data-driven strategies to efficiently\ngenerate safety-critical scenarios. An optimized scenario generation toolkit is\ndeveloped within BridgeGen. This expedites the crafting of safety-critical\nscenarios through a combination of traditional optimization and reinforcement\nlearning schemes. Extensive experiments conducted using Carla simulator\ndemonstrate the effectiveness of BridgeGen in generating diverse\nsafety-critical scenarios.",
            "author": [
                "Kunkun Hao",
                "Lu Liu",
                "Wen Cui",
                "Jianxing Zhang",
                "Songyang Yan",
                "Yuxi Pan",
                "Zijiang Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10937v1",
                "http://arxiv.org/pdf/2311.10937v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10933v1",
            "title": "Representing visual classification as a linear combination of words",
            "updated": "2023-11-18T02:00:20Z",
            "published": "2023-11-18T02:00:20Z",
            "summary": "Explainability is a longstanding challenge in deep learning, especially in\nhigh-stakes domains like healthcare. Common explainability methods highlight\nimage regions that drive an AI model's decision. Humans, however, heavily rely\non language to convey explanations of not only \"where\" but \"what\".\nAdditionally, most explainability approaches focus on explaining individual AI\npredictions, rather than describing the features used by an AI model in\ngeneral. The latter would be especially useful for model and dataset auditing,\nand potentially even knowledge generation as AI is increasingly being used in\nnovel tasks. Here, we present an explainability strategy that uses a\nvision-language model to identify language-based descriptors of a visual\nclassification task. By leveraging a pre-trained joint embedding space between\nimages and text, our approach estimates a new classification task as a linear\ncombination of words, resulting in a weight for each word that indicates its\nalignment with the vision-based classifier. We assess our approach using two\nmedical imaging classification tasks, where we find that the resulting\ndescriptors largely align with clinical knowledge despite a lack of\ndomain-specific language training. However, our approach also identifies the\npotential for 'shortcut connections' in the public datasets used. Towards a\nfunctional measure of explainability, we perform a pilot reader study where we\nfind that the AI-identified words can enable non-expert humans to perform a\nspecialized medical task at a non-trivial level. Altogether, our results\nemphasize the potential of using multimodal foundational models to deliver\nintuitive, language-based explanations of visual tasks.",
            "author": [
                "Shobhit Agarwal",
                "Yevgeniy R. Semenov",
                "William Lotter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10933v1",
                "http://arxiv.org/pdf/2311.10933v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.CV",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12871v1",
            "title": "An Embodied Generalist Agent in 3D World",
            "updated": "2023-11-18T01:21:38Z",
            "published": "2023-11-18T01:21:38Z",
            "summary": "Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.",
            "author": [
                "Jiangyong Huang",
                "Silong Yong",
                "Xiaojian Ma",
                "Xiongkun Linghu",
                "Puhao Li",
                "Yan Wang",
                "Qing Li",
                "Song-Chun Zhu",
                "Baoxiong Jia",
                "Siyuan Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12871v1",
                "http://arxiv.org/pdf/2311.12871v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10924v1",
            "title": "Faster Streaming and Scalable Algorithms for Finding Directed Dense\n  Subgraphs in Large Graphs",
            "updated": "2023-11-18T00:58:05Z",
            "published": "2023-11-18T00:58:05Z",
            "summary": "Finding dense subgraphs is a fundamental algorithmic tool in data mining,\ncommunity detection, and clustering. In this problem, one aims to find an\ninduced subgraph whose edge-to-vertex ratio is maximized.\n  We study the directed case of this question in the context of semi-streaming\nand massively parallel algorithms. In particular, we show that it is possible\nto find a $(2+\\epsilon)$ approximation on randomized streams even in a single\npass by using $O(n \\cdot {\\rm poly} \\log n)$ memory on $n$-vertex graphs. Our\nresult improves over prior works, which were designed for arbitrary-ordered\nstreams: the algorithm by Bahmani et al. (VLDB 2012) which uses $O(\\log n)$\npasses, and the work by Esfandiari et al. (2015) which makes one pass but uses\n$O(n^{3/2})$ memory. Moreover, our techniques extend to the Massively Parallel\nComputation model yielding $O(1)$ rounds in the super-linear and $O(\\sqrt{\\log\nn})$ rounds in the nearly-linear memory regime. This constitutes a quadratic\nimprovement over state-of-the-art bounds by Bahmani et al. (VLDB 2012 and WAW\n2014), which require $O(\\log n)$ rounds even in the super-linear memory regime.\n  Finally, we empirically evaluate our single-pass semi-streaming algorithm on\n$6$ benchmarks and show that, even on non-randomly ordered streams, the quality\nof its output is essentially the same as that of Bahmani et al. (VLDB 2012)\nwhile it is $2$ times faster on large graphs.",
            "author": [
                "Slobodan Mitrovi\u0107",
                "Theodore Pan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10924v1",
                "http://arxiv.org/pdf/2311.10924v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10919v1",
            "title": "PACOL: Poisoning Attacks Against Continual Learners",
            "updated": "2023-11-18T00:20:57Z",
            "published": "2023-11-18T00:20:57Z",
            "summary": "Continual learning algorithms are typically exposed to untrusted sources that\ncontain training data inserted by adversaries and bad actors. An adversary can\ninsert a small number of poisoned samples, such as mislabeled samples from\npreviously learned tasks, or intentional adversarial perturbed samples, into\nthe training datasets, which can drastically reduce the model's performance. In\nthis work, we demonstrate that continual learning systems can be manipulated by\nmalicious misinformation and present a new category of data poisoning attacks\nspecific for continual learners, which we refer to as {\\em Poisoning Attacks\nAgainst Continual Learners} (PACOL). The effectiveness of labeling flipping\nattacks inspires PACOL; however, PACOL produces attack samples that do not\nchange the sample's label and produce an attack that causes catastrophic\nforgetting. A comprehensive set of experiments shows the vulnerability of\ncommonly used generative replay and regularization-based continual learning\napproaches against attack methods. We evaluate the ability of label-flipping\nand a new adversarial poison attack, namely PACOL proposed in this work, to\nforce the continual learning system to forget the knowledge of a learned\ntask(s). More specifically, we compared the performance degradation of\ncontinual learning systems trained on benchmark data streams with and without\npoisoning attacks. Moreover, we discuss the stealthiness of the attacks in\nwhich we test the success rate of data sanitization defense and other outlier\ndetection-based defenses for filtering out adversarial samples.",
            "author": [
                "Huayu Li",
                "Gregory Ditzler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10919v1",
                "http://arxiv.org/pdf/2311.10919v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10908v1",
            "title": "Equivariant Neural Operator Learning with Graphon Convolution",
            "updated": "2023-11-17T23:28:22Z",
            "published": "2023-11-17T23:28:22Z",
            "summary": "We propose a general architecture that combines the coefficient learning\nscheme with a residual operator layer for learning mappings between continuous\nfunctions in the 3D Euclidean space. Our proposed model is guaranteed to\nachieve SE(3)-equivariance by design. From the graph spectrum view, our method\ncan be interpreted as convolution on graphons (dense graphs with infinitely\nmany nodes), which we term InfGCN. By leveraging both the continuous graphon\nstructure and the discrete graph structure of the input data, our model can\neffectively capture the geometric information while preserving equivariance.\nThrough extensive experiments on large-scale electron density datasets, we\nobserved that our model significantly outperformed the current state-of-the-art\narchitectures. Multiple ablation studies were also carried out to demonstrate\nthe effectiveness of the proposed architecture.",
            "author": [
                "Chaoran Cheng",
                "Jian Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10908v1",
                "http://arxiv.org/pdf/2311.10908v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.12045v1",
            "title": "Using Guided Transfer Learning to Predispose AI Agent to Learn\n  Efficiently from Small RNA-sequencing Datasets",
            "updated": "2023-11-17T22:47:46Z",
            "published": "2023-11-17T22:47:46Z",
            "summary": "Given the increasing availability of RNA-seq data and its complex and\nheterogeneous nature, there has been growing interest in applying AI/machine\nlearning methodologies to work with such data modalities. However, because\nomics data is characterized by high dimensionality and low sample size (HDLSS),\ncurrent attempts at integrating AI in this domain require significant human\nguidance and expertise to mitigate overfitting. In this work we look at how\ntransfer learning can be improved to learn from small RNA-seq sample sizes\nwithout significant human interference. The strategy is to gain general prior\nknowledge about a particular domain of data (e.g. RNA-seq data) by pre-training\non a general task with a large aggregate of data, then fine-tuning to various\nspecific, downstream target tasks in the same domain. Because previous attempts\nhave shown traditional transfer learning failing on HLDSS, we propose to\nimprove performance by using Guided Transfer Learning (GTL). Collaborating with\nRobots Go Mental, the AI we deploy here not only learns good initial parameters\nduring pre-training, but also learns inductive biases that affect how the AI\nlearns downstream tasks. In this approach, we first pre-trained on recount3\ndata, a collection of over 400,000 mouse RNA-seq samples sourced from thousands\nof individual studies. With such a large collection, patterns of expression\nbetween the ~30,000 genes in mammalian systems were pre-determined. Such\npatterns were sufficient for the pre-trained AI agent to efficiently learn new\ndownstream tasks involving RNA-seq datasets with very low sample sizes and\nperformed notably better on few-shot learning tasks compared to the same model\nwithout pre-training.",
            "author": [
                "Kevin Li",
                "Danko Nikoli\u0107",
                "Vjekoslav Nikoli\u0107",
                "Davor Andri\u0107",
                "Lauren M. Sanders",
                "Sylvain V. Costes"
            ],
            "link": [
                "http://arxiv.org/abs/2311.12045v1",
                "http://arxiv.org/pdf/2311.12045v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10863v3",
            "title": "Verified Compositional Neuro-Symbolic Control for Stochastic Systems\n  with Temporal Logic Tasks",
            "updated": "2023-11-22T03:01:59Z",
            "published": "2023-11-17T20:51:24Z",
            "summary": "Several methods have been proposed recently to learn neural network (NN)\ncontrollers for autonomous agents, with unknown and stochastic dynamics, tasked\nwith complex missions captured by Linear Temporal Logic (LTL). Due to the\nsample-inefficiency of the majority of these works, compositional learning\nmethods have been proposed decomposing the LTL specification into smaller\nsub-tasks. Then, separate controllers are learned and composed to satisfy the\noriginal task. A key challenge within these approaches is that they often lack\nsafety guarantees or the provided guarantees are impractical. This paper aims\nto address this challenge. Particularly, we consider autonomous systems with\nunknown and stochastic dynamics and LTL-encoded tasks. We assume that the\nsystem is equipped with a finite set of base skills modeled by trained NN\nfeedback controllers. Our goal is to check if there exists a temporal\ncomposition of the trained NN controllers - and if so, to compute it - that\nwill yield a composite system behavior that satisfies the assigned LTL task\nwith probability one. We propose a new approach that relies on a novel\nintegration of automata theory and data-driven reachability analysis tools for\nNN-controlled stochastic systems. The resulting neuro-symbolic controller\nallows the agent to generate safe behaviors for unseen complex temporal logic\ntasks in a zero-shot fashion by leveraging its base skills. We show correctness\nof the proposed method and we provide conditions under which it is complete. To\nthe best of our knowledge, this is the first work that designs verified\ntemporal compositions of NN controllers for unknown and stochastic systems.\nFinally, we provide extensive numerical simulations and hardware experiments on\nrobot navigation tasks to demonstrate the proposed method.",
            "author": [
                "Jun Wang",
                "Haojun Chen",
                "Zihe Sun",
                "Yiannis Kantaros"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10863v3",
                "http://arxiv.org/pdf/2311.10863v3"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10856v1",
            "title": "Exploring the Consistency, Quality and Challenges in Manual and\n  Automated Coding of Free-text Diagnoses from Hospital Outpatient Letters",
            "updated": "2023-11-17T20:32:24Z",
            "published": "2023-11-17T20:32:24Z",
            "summary": "Coding of unstructured clinical free-text to produce interoperable structured\ndata is essential to improve direct care, support clinical communication and to\nenable clinical research.However, manual clinical coding is difficult and time\nconsuming, which motivates the development and use of natural language\nprocessing for automated coding. This work evaluates the quality and\nconsistency of both manual and automated clinical coding of diagnoses from\nhospital outpatient letters. Using 100 randomly selected letters, two human\nclinicians performed coding of diagnosis lists to SNOMED CT. Automated coding\nwas also performed using IMO's Concept Tagger. A gold standard was constructed\nby a panel of clinicians from a subset of the annotated diagnoses. This was\nused to evaluate the quality and consistency of both manual and automated\ncoding via (1) a distance-based metric, treating SNOMED CT as a graph, and (2)\na qualitative metric agreed upon by the panel of clinicians. Correlation\nbetween the two metrics was also evaluated. Comparing human and\ncomputer-generated codes to the gold standard, the results indicate that humans\nslightly out-performed automated coding, while both performed notably better\nwhen there was only a single diagnosis contained in the free-text description.\nAutomated coding was considered acceptable by the panel of clinicians in\napproximately 90% of cases.",
            "author": [
                "Warren Del-Pinto",
                "George Demetriou",
                "Meghna Jani",
                "Rikesh Patel",
                "Leanne Gray",
                "Alex Bulcock",
                "Niels Peek",
                "Andrew S. Kanter",
                "William G Dixon",
                "Goran Nenadic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10856v1",
                "http://arxiv.org/pdf/2311.10856v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10822v1",
            "title": "Gradients and frequency profiles of quantum re-uploading models",
            "updated": "2023-11-17T19:01:43Z",
            "published": "2023-11-17T19:01:43Z",
            "summary": "Quantum re-uploading models have been extensively investigated as a form of\nmachine learning within the context of variational quantum algorithms. Their\ntrainability and expressivity are not yet fully understood and are critical to\ntheir performance. In this work, we address trainability through the lens of\nthe magnitude of the gradients of the cost function. We prove bounds for the\ndifferences between gradients of the better-studied data-less parameterized\nquantum circuits and re-uploading models. We coin the concept of {\\sl\nabsorption witness} to quantify such difference. For the expressivity, we prove\nthat quantum re-uploading models output functions with vanishing high-frequency\ncomponents and upper-bounded derivatives with respect to data. As a\nconsequence, such functions present limited sensitivity to fine details, which\nprotects against overfitting. We performed numerical experiments extending the\ntheoretical results to more relaxed and realistic conditions. Overall, future\ndesigns of quantum re-uploading models will benefit from the strengthened\nknowledge delivered by the uncovering of absorption witnesses and vanishing\nhigh frequencies.",
            "author": [
                "Alice Barthe",
                "Adri\u00e1n P\u00e9rez-Salinas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10822v1",
                "http://arxiv.org/pdf/2311.10822v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10711v1",
            "title": "Direct Optimal Mapping Image Power Spectrum and its Window Functions",
            "updated": "2023-11-17T18:59:59Z",
            "published": "2023-11-17T18:59:59Z",
            "summary": "The key to detecting neutral hydrogen during the epoch of reionization (EoR)\nis to separate the cosmological signal from the dominating foreground\nradiation. We developed direct optimal mapping (Xu et al. 2022) to map\ninterferometric visibilities; it contains only linear operations, with full\nknowledge of point spread functions from visibilities to images. Here we\npresent an FFT-based image power spectrum and its window functions based on\ndirect optimal mapping. We use noiseless simulation, based on the Hydrogen\nEpoch of Reionization Array (HERA) Phase I configuration, to study the image\npower spectrum properties. The window functions show $<10^{-11}$ power leakage\nfrom the foreground-dominated region into the EoR window; the 2D and 1D power\nspectra also verify the separation between the foregrounds and the EoR.\nFurthermore, we simulated visibilities from a $uv$-complete array and\ncalculated its image power spectrum. The result shows that the foreground--EoR\nleakage is further suppressed below $10^{-12}$, dominated by the tapering\nfunction sidelobes; the 2D power spectrum does not show signs of the horizon\nwedge. The $uv$-complete result provides a reference case for future 21cm\ncosmology array designs.",
            "author": [
                "Zhilei Xu",
                "Honggeun Kim",
                "Jacqueline N. Hewitt",
                "Kai-Feng Chen",
                "Nicholas S. Kern",
                "Elizabeth Rath",
                "Ruby Byrne",
                "Ad\u00e9lie Gorce",
                "Zachary E. Martinot",
                "Joshua S. Dillon",
                "Bryna J. Hazelton",
                "Adrian Liu",
                "Miguel F. Morales",
                "Zara Abdurashidova",
                "Tyrone Adams",
                "James E. Aguirre",
                "Paul Alexander",
                "Zaki S. Ali",
                "Rushelle Baartman",
                "Yanga Balfour",
                "Adam P. Beardsley",
                "Gianni Bernardi",
                "Tashalee S. Billings",
                "Judd D. Bowman",
                "Richard F. Bradley",
                "Philip Bull",
                "Jacob Burba",
                "Steven Carey",
                "Chris L. Carilli",
                "Carina Cheng",
                "David R. DeBoer",
                "Eloy de Lera Acedo",
                "Matt Dexter",
                "Nico Eksteen",
                "John Ely",
                "Aaron Ewall-Wice",
                "Nicolas Fagnoni",
                "Randall Fritz",
                "Steven R. Furlanetto",
                "Kingsley Gale-Sides",
                "Brian Glendenning",
                "Deepthi Gorthi",
                "Bradley Greig",
                "Jasper Grobbelaar",
                "Ziyaad Halday",
                "Jack Hickish",
                "Daniel C. Jacobs",
                "Austin Julius",
                "MacCalvin Kariseb",
                "Joshua Kerrigan",
                "Piyanat Kittiwisit",
                "Saul A. Kohn",
                "Matthew Kolopanis",
                "Adam Lanman",
                "Paul La Plante",
                "Anita Loots",
                "David Harold Edward MacMahon",
                "Lourence Malan",
                "Cresshim Malgas",
                "Keith Malgas",
                "Bradley Marero",
                "Andrei Mesinger",
                "Mathakane Molewa",
                "Tshegofalang Mosiane",
                "Steven G. Murray",
                "Abraham R. Neben",
                "Bojan Nikolic",
                "Hans Nuwegeld",
                "Aaron R. Parsons",
                "Nipanjana Patra",
                "Samantha Pieterse",
                "Nima Razavi-Ghods",
                "James Robnett",
                "Kathryn Rosie",
                "Peter Sims",
                "Craig Smith",
                "Hilton Swarts",
                "Nithyanandan Thyagarajan",
                "Pieter van Wyngaarden",
                "Peter K. G. Williams",
                "Haoxuan Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10711v1",
                "http://arxiv.org/pdf/2311.10711v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10813v3",
            "title": "A Language Agent for Autonomous Driving",
            "updated": "2023-11-27T20:53:35Z",
            "published": "2023-11-17T18:59:56Z",
            "summary": "Human-level driving is an ultimate goal of autonomous driving. Conventional\napproaches formulate autonomous driving as a perception-prediction-planning\nframework, yet their systems do not capitalize on the inherent reasoning\nability and experiential knowledge of humans. In this paper, we propose a\nfundamental paradigm shift from current pipelines, exploiting Large Language\nModels (LLMs) as a cognitive agent to integrate human-like intelligence into\nautonomous driving systems. Our approach, termed Agent-Driver, transforms the\ntraditional autonomous driving pipeline by introducing a versatile tool library\naccessible via function calls, a cognitive memory of common sense and\nexperiential knowledge for decision-making, and a reasoning engine capable of\nchain-of-thought reasoning, task planning, motion planning, and\nself-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive\ncommon sense and robust reasoning capabilities, thus enabling a more nuanced,\nhuman-like approach to autonomous driving. We evaluate our approach on the\nlarge-scale nuScenes benchmark, and extensive experiments substantiate that our\nAgent-Driver significantly outperforms the state-of-the-art driving methods by\na large margin. Our approach also demonstrates superior interpretability and\nfew-shot learning ability to these methods. Code will be released.",
            "author": [
                "Jiageng Mao",
                "Junjie Ye",
                "Yuxi Qian",
                "Marco Pavone",
                "Yue Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10813v3",
                "http://arxiv.org/pdf/2311.10813v3"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10708v1",
            "title": "SelfEval: Leveraging the discriminative nature of generative models for\n  evaluation",
            "updated": "2023-11-17T18:58:16Z",
            "published": "2023-11-17T18:58:16Z",
            "summary": "In this work, we show that text-to-image generative models can be 'inverted'\nto assess their own text-image understanding capabilities in a completely\nautomated manner.\n  Our method, called SelfEval, uses the generative model to compute the\nlikelihood of real images given text prompts, making the generative model\ndirectly applicable to discriminative tasks.\n  Using SelfEval, we repurpose standard datasets created for evaluating\nmultimodal text-image discriminative models to evaluate generative models in a\nfine-grained manner: assessing their performance on attribute binding, color\nrecognition, counting, shape recognition, spatial understanding.\n  To the best of our knowledge SelfEval is the first automated metric to show a\nhigh degree of agreement for measuring text-faithfulness with the gold-standard\nhuman evaluations across multiple models and benchmarks.\n  Moreover, SelfEval enables us to evaluate generative models on challenging\ntasks such as Winoground image-score where they demonstrate competitive\nperformance to discriminative models.\n  We also show severe drawbacks of standard automated metrics such as\nCLIP-score to measure text faithfulness on benchmarks such as DrawBench, and\nhow SelfEval sidesteps these issues.\n  We hope SelfEval enables easy and reliable automated evaluation for diffusion\nmodels.",
            "author": [
                "Sai Saketh Rambhatla",
                "Ishan Misra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10708v1",
                "http://arxiv.org/pdf/2311.10708v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10706v1",
            "title": "Cactus Representations in Polylogarithmic Max-flow via Maximal Isolating\n  Mincuts",
            "updated": "2023-11-17T18:53:56Z",
            "published": "2023-11-17T18:53:56Z",
            "summary": "A cactus representation of a graph, introduced by Dinitz et al. in 1976, is\nan edge sparsifier of $O(n)$ size that exactly captures all global minimum cuts\nof the graph. It is a central combinatorial object that has been a key\ningredient in almost all algorithms for the connectivity augmentation problems\nand for maintaining minimum cuts under edge insertions (e.g. [NGM97], [CKL+22],\n[Hen97]). This sparsifier was generalized to Steiner cactus for a vertex set\n$T$, which can be seen as a vertex sparsifier of $O(|T|)$ size that captures\nall partitions of $T$ corresponding to a $T$-Steiner minimum cut, and also\nhypercactus, an analogous concept in hypergraphs. These generalizations further\nextend the applications of cactus to the Steiner and hypergraph settings.\n  In a long line of work on fast constructions of cactus and its\ngeneralizations, a near-linear time construction of cactus was shown by [Karger\nand Panigrahi 2009]. Unfortunately, their technique based on tree packing\ninherently does not generalize. The state-of-the-art algorithms for Steiner\ncactus and hypercactus are still slower than linear time by a factor of\n$\\Omega(|T|)$ [DV94] and $\\Omega(n)$ [CX17], respectively.\n  We show how to construct both Steiner cactus and hypercactus using\npolylogarithmic calls to max flow, which gives the first almost-linear time\nalgorithms of both problems. The constructions immediately imply\nalmost-linear-time connectivity augmentation algorithms in the Steiner and\nhypergraph settings, as well as speed up the incremental algorithm for\nmaintaining minimum cuts in hypergraphs by a factor of $n$.\n  The key technique behind our result is a novel variant of the influential\nisolating mincut technique [LP20, AKL+21] which we called maximal isolating\nmincuts. This technique makes the isolating mincuts to be \"more balanced\"\nwhich, we believe, will likely be useful in future applications.",
            "author": [
                "Zhongtian He",
                "Shang-En Huang",
                "Thatchaphol Saranurak"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10706v1",
                "http://arxiv.org/pdf/2311.10706v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10696v1",
            "title": "Versatile Medical Image Segmentation Learned from Multi-Source Datasets\n  via Model Self-Disambiguation",
            "updated": "2023-11-17T18:28:32Z",
            "published": "2023-11-17T18:28:32Z",
            "summary": "A versatile medical image segmentation model applicable to imaging data\ncollected with diverse equipment and protocols can facilitate model deployment\nand maintenance. However, building such a model typically requires a large,\ndiverse, and fully annotated dataset, which is rarely available due to the\nlabor-intensive and costly data curation. In this study, we develop a\ncost-efficient method by harnessing readily available data with partially or\neven sparsely annotated segmentation labels. We devise strategies for model\nself-disambiguation, prior knowledge incorporation, and imbalance mitigation to\naddress challenges associated with inconsistently labeled data from various\nsources, including label ambiguity and imbalances across modalities, datasets,\nand segmentation labels. Experimental results on a multi-modal dataset compiled\nfrom eight different sources for abdominal organ segmentation have demonstrated\nour method's effectiveness and superior performance over alternative\nstate-of-the-art methods, highlighting its potential for optimizing the use of\nexisting annotated data and reducing the annotation efforts for new data to\nfurther enhance model capability.",
            "author": [
                "Xiaoyang Chen",
                "Hao Zheng",
                "Yuemeng Li",
                "Yuncong Ma",
                "Liang Ma",
                "Hongming Li",
                "Yong Fan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10696v1",
                "http://arxiv.org/pdf/2311.10696v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10688v1",
            "title": "A sliding window-based algorithm for faster transformation of time\n  series into complex networks",
            "updated": "2023-11-17T18:16:28Z",
            "published": "2023-11-17T18:16:28Z",
            "summary": "A new alternative method to approximate the Visibility Graph (VG) of a time\nseries has been introduced here. It exploits the fact that most of the nodes in\nthe resulting network are not connected to those that are far away from them.\nThis means that the adjacency matrix is almost empty, and its nonzero values\nare close to the main diagonal. This new method is called Sliding Visibility\nGraph (SVG). Numerical tests have been performed for several time series,\nshowing a time efficiency that scales linearly with the size of the series\n[O(N)], in contrast to the original VG that does so quadratically [O(N2)]. This\nfact is noticeably convenient when dealing with very large time series. The\nresults obtained from the SVG of the studied time series have been compared to\nthe exact values of the original VG. As expected, the SVG outcomes converge\nvery rapidly to the desired ones, especially for random and stochastic series.\nAlso, this method can be extended to the analysis of time series that evolve in\nreal time, since it does not require the entire dataset to perform the analysis\nbut a shorter segment of it. The length segment can remain constant, making\npossible a simple analysis as the series evolves in time.",
            "author": [
                "R. Carmona-Cabezas",
                "J. Gomez-Gomez",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1063/1.5112782",
                "http://arxiv.org/abs/2311.10688v1",
                "http://arxiv.org/pdf/2311.10688v1"
            ],
            "primary_category": "physics.data-an",
            "category": [
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10684v1",
            "title": "Total Skin Electron Therapy Stanford Technique Evolution With Monte\n  Carlo Simulation Toward Personalized Treatments For Cutaneous Lymphoma",
            "updated": "2023-11-17T18:09:19Z",
            "published": "2023-11-17T18:09:19Z",
            "summary": "Current Total Skin Electron Therapy (TSET) Stanford technique for cutaneous\nlymphoma, established in the 70's, involves a unique irradiation setup, i.e.\npatient's position and beam arrangement, for all patients with ensuing great\nvariability in dose distribution and difficult dose optimization. A\nGeant4-based simulation has been developed to explore the possibility of\npersonalizing the dose to each patient's anatomy. To achieve this optimization\nof the treatment method, this project enrolls different aspects of the clinical\nand computational techniques: starting with the knowledge of the experimental\nparameters involving TSET practice, passing through an innovative approach to\nmodel the patient's anatomy, a precise description of the electron beam and a\nvalidated configuration of the physics models handling the interactions of the\nelectrons and of secondary particles. The Geant4-based simulation models the\npatient as a tessellated solid derived from the optical scan of her/his body,\nrealistically reproduces the irradiation environment in detail and calculates\nthe energy deposition corresponding to each facet of the patient's scanned\nsurface. The resulting three-dimensional dose distribution constitutes the\nbasis for the personalization of the medical treatement as appropriate to each\npatient's specific characteristics.",
            "author": [
                "Tullio Basaglia",
                "Patrizia Boccacci",
                "Stephane Chauvie",
                "Manuela Chessa",
                "Daniele DAgostino",
                "Monica Gambaro",
                "Filippo Grillo Ruggieri",
                "Gabriela Hoff",
                "Maria Grazia Pia",
                "Paolo Saracco",
                "Piero Schiapparelli",
                "Giuseppe Scielzo",
                "Evgueni Tcherniaev",
                "Daniele Zefiro"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10684v1",
                "http://arxiv.org/pdf/2311.10684v1"
            ],
            "primary_category": "physics.comp-ph",
            "category": [
                "physics.comp-ph",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10681v1",
            "title": "An efficient quantum parallel repetition theorem and applications",
            "updated": "2023-11-17T18:06:36Z",
            "published": "2023-11-17T18:06:36Z",
            "summary": "We prove a tight parallel repetition theorem for $3$-message\ncomputationally-secure quantum interactive protocols between an efficient\nchallenger and an efficient adversary. We also prove under plausible\nassumptions that the security of $4$-message computationally secure protocols\ndoes not generally decrease under parallel repetition. These mirror the\nclassical results of Bellare, Impagliazzo, and Naor [BIN97]. Finally, we prove\nthat all quantum argument systems can be generically compiled to an equivalent\n$3$-message argument system, mirroring the transformation for quantum proof\nsystems [KW00, KKMV07].\n  As immediate applications, we show how to derive hardness amplification\ntheorems for quantum bit commitment schemes (answering a question of Yan\n[Yan22]), EFI pairs (answering a question of Brakerski, Canetti, and Qian\n[BCQ23]), public-key quantum money schemes (answering a question of Aaronson\nand Christiano [AC13]), and quantum zero-knowledge argument systems. We also\nderive an XOR lemma [Yao82] for quantum predicates as a corollary.",
            "author": [
                "John Bostanci",
                "Luowen Qian",
                "Nicholas Spooner",
                "Henry Yuen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10681v1",
                "http://arxiv.org/pdf/2311.10681v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10678v1",
            "title": "Distilling and Retrieving Generalizable Knowledge for Robot Manipulation\n  via Language Corrections",
            "updated": "2023-11-17T18:00:20Z",
            "published": "2023-11-17T18:00:20Z",
            "summary": "Today's robot policies exhibit subpar performance when faced with the\nchallenge of generalizing to novel environments. Human corrective feedback is a\ncrucial form of guidance to enable such generalization. However, adapting to\nand learning from online human corrections is a non-trivial endeavor: not only\ndo robots need to remember human feedback over time to retrieve the right\ninformation in new settings and reduce the intervention rate, but also they\nwould need to be able to respond to feedback that can be arbitrary corrections\nabout high-level human preferences to low-level adjustments to skill\nparameters. In this work, we present Distillation and Retrieval of Online\nCorrections (DROC), a large language model (LLM)-based system that can respond\nto arbitrary forms of language feedback, distill generalizable knowledge from\ncorrections, and retrieve relevant past experiences based on textual and visual\nsimilarity for improving performance in novel settings. DROC is able to respond\nto a sequence of online language corrections that address failures in both\nhigh-level task plans and low-level skill primitives. We demonstrate that DROC\neffectively distills the relevant information from the sequence of online\ncorrections in a knowledge base and retrieves that knowledge in settings with\nnew task or object instances. DROC outperforms other techniques that directly\ngenerate robot code via LLMs by using only half of the total number of\ncorrections needed in the first round and requires little to no corrections\nafter two iterations. We show further results, videos, prompts and code on\nhttps://sites.google.com/stanford.edu/droc .",
            "author": [
                "Lihan Zha",
                "Yuchen Cui",
                "Li-Heng Lin",
                "Minae Kwon",
                "Montserrat Gonzalez Arenas",
                "Andy Zeng",
                "Fei Xia",
                "Dorsa Sadigh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10678v1",
                "http://arxiv.org/pdf/2311.10678v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10808v1",
            "title": "Multiparameter Persistent Homology for Molecular Property Prediction",
            "updated": "2023-11-17T17:57:56Z",
            "published": "2023-11-17T17:57:56Z",
            "summary": "In this study, we present a novel molecular fingerprint generation method\nbased on multiparameter persistent homology. This approach reveals the latent\nstructures and relationships within molecular geometry, and detects topological\nfeatures that exhibit persistence across multiple scales along multiple\nparameters, such as atomic mass, partial charge, and bond type, and can be\nfurther enhanced by incorporating additional parameters like ionization energy,\nelectron affinity, chirality and orbital hybridization. The proposed\nfingerprinting method provides fresh perspectives on molecular structure that\nare not easily discernible from single-parameter or single-scale analysis.\nBesides, in comparison with traditional graph neural networks, multiparameter\npersistent homology has the advantage of providing a more comprehensive and\ninterpretable characterization of the topology of the molecular data. We have\nestablished theoretical stability guarantees for multiparameter persistent\nhomology, and have conducted extensive experiments on the Lipophilicity,\nFreeSolv, and ESOL datasets to demonstrate its effectiveness in predicting\nmolecular properties.",
            "author": [
                "Andac Demir",
                "Bulent Kiziltan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10808v1",
                "http://arxiv.org/pdf/2311.10808v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10670v1",
            "title": "Target-based Distributionally Robust Minimum Spanning Tree Problem",
            "updated": "2023-11-17T17:43:07Z",
            "published": "2023-11-17T17:43:07Z",
            "summary": "Due to its broad applications in practice, the minimum spanning tree problem\nand its all kinds of variations have been studied extensively during the last\ndecades, for which a host of efficient exact and heuristic algorithms have been\nproposed. Meanwhile, motivated by realistic applications, the minimum spanning\ntree problem in stochastic network has attracted considerable attention of\nresearchers, with respect to which stochastic and robust spanning tree models\nand related algorithms have been continuingly developed. However, all of them\nwould be either too restricted by the types of the edge weight random variables\nor computationally intractable, especially in large-scale networks. In this\npaper, we introduce a target-based distributionally robust optimization\nframework to solve the minimum spanning tree problem in stochastic graphs where\nthe probability distribution function of the edge weight is unknown but some\nstatistical information could be utilized to prevent the optimal solution from\nbeing too conservative. We propose two exact algorithms to solve it, based on\nBenders decomposition framework and a modified classical greedy algorithm of\nMST problem (Prim algorithm),respectively. Compared with the NP-hard stochastic\nand robust spanning tree problems,The proposed target-based distributionally\nrobust minimum spanning tree problem enjoys more satisfactory algorithmic\naspect and robustness, when faced with uncertainty in input data.",
            "author": [
                "Yang Xu",
                "Lianmin Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10670v1",
                "http://arxiv.org/pdf/2311.10670v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10656v1",
            "title": "LE-SSL-MOS: Self-Supervised Learning MOS Prediction with Listener\n  Enhancement",
            "updated": "2023-11-17T17:20:45Z",
            "published": "2023-11-17T17:20:45Z",
            "summary": "Recently, researchers have shown an increasing interest in automatically\npredicting the subjective evaluation for speech synthesis systems. This\nprediction is a challenging task, especially on the out-of-domain test set. In\nthis paper, we proposed a novel fusion model for MOS prediction that combines\nsupervised and unsupervised approaches. In the supervised aspect, we developed\nan SSL-based predictor called LE-SSL-MOS. The LE-SSL-MOS utilizes pre-trained\nself-supervised learning models and further improves prediction accuracy by\nutilizing the opinion scores of each utterance in the listener enhancement\nbranch. In the unsupervised aspect, two steps are contained: we fine-tuned the\nunit language model (ULM) using highly intelligible domain data to improve the\ncorrelation of an unsupervised metric - SpeechLMScore. Another is that we\nutilized ASR confidence as a new metric with the help of ensemble learning. To\nour knowledge, this is the first architecture that fuses supervised and\nunsupervised methods for MOS prediction. With these approaches, our\nexperimental results on the VoiceMOS Challenge 2023 show that LE-SSL-MOS\nperforms better than the baseline. Our fusion system achieved an absolute\nimprovement of 13% over LE-SSL-MOS on the noisy and enhanced speech track. Our\nsystem ranked 1st and 2nd, respectively, in the French speech synthesis track\nand the challenge's noisy and enhanced speech track.",
            "author": [
                "Zili Qi",
                "Xinhui Hu",
                "Wangjin Zhou",
                "Sheng Li",
                "Hao Wu",
                "Jian Lu",
                "Xinkang Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10656v1",
                "http://arxiv.org/pdf/2311.10656v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10651v1",
            "title": "3D-TexSeg: Unsupervised Segmentation of 3D Texture using Mutual\n  Transformer Learning",
            "updated": "2023-11-17T17:13:14Z",
            "published": "2023-11-17T17:13:14Z",
            "summary": "Analysis of the 3D Texture is indispensable for various tasks, such as\nretrieval, segmentation, classification, and inspection of sculptures, knitted\nfabrics, and biological tissues. A 3D texture is a locally repeated surface\nvariation independent of the surface's overall shape and can be determined\nusing the local neighborhood and its characteristics. Existing techniques\ntypically employ computer vision techniques that analyze a 3D mesh globally,\nderive features, and then utilize the obtained features for retrieval or\nclassification. Several traditional and learning-based methods exist in the\nliterature, however, only a few are on 3D texture, and nothing yet, to the best\nof our knowledge, on the unsupervised schemes. This paper presents an original\nframework for the unsupervised segmentation of the 3D texture on the mesh\nmanifold. We approach this problem as binary surface segmentation, partitioning\nthe mesh surface into textured and non-textured regions without prior\nannotation. We devise a mutual transformer-based system comprising a label\ngenerator and a cleaner. The two models take geometric image representations of\nthe surface mesh facets and label them as texture or non-texture across an\niterative mutual learning scheme. Extensive experiments on three publicly\navailable datasets with diverse texture patterns demonstrate that the proposed\nframework outperforms standard and SOTA unsupervised techniques and competes\nreasonably with supervised methods.",
            "author": [
                "Iyyakutti Iyappan Ganapathi",
                "Fayaz Ali",
                "Sajid Javed",
                "Syed Sadaf Ali",
                "Naoufel Werghi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10651v1",
                "http://arxiv.org/pdf/2311.10651v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10642v3",
            "title": "Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as\n  an Alternative to Attention Layers in Transformers",
            "updated": "2023-11-29T10:41:36Z",
            "published": "2023-11-17T16:58:52Z",
            "summary": "This work presents an analysis of the effectiveness of using standard shallow\nfeed-forward networks to mimic the behavior of the attention mechanism in the\noriginal Transformer model, a state-of-the-art architecture for\nsequence-to-sequence tasks. We substitute key elements of the attention\nmechanism in the Transformer with simple feed-forward networks, trained using\nthe original components via knowledge distillation. Our experiments, conducted\non the IWSLT2017 dataset, reveal the capacity of these \"attentionless\nTransformers\" to rival the performance of the original architecture. Through\nrigorous ablation studies, and experimenting with various replacement network\ntypes and sizes, we offer insights that support the viability of our approach.\nThis not only sheds light on the adaptability of shallow feed-forward networks\nin emulating attention mechanisms but also underscores their potential to\nstreamline complex architectures for sequence-to-sequence tasks.",
            "author": [
                "Vukasin Bozic",
                "Danilo Dordevic",
                "Daniele Coppola",
                "Joseph Thommes",
                "Sidak Pal Singh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10642v3",
                "http://arxiv.org/pdf/2311.10642v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10638v1",
            "title": "Concept-free Causal Disentanglement with Variational Graph Auto-Encoder",
            "updated": "2023-11-17T16:50:00Z",
            "published": "2023-11-17T16:50:00Z",
            "summary": "In disentangled representation learning, the goal is to achieve a compact\nrepresentation that consists of all interpretable generative factors in the\nobservational data. Learning disentangled representations for graphs becomes\nincreasingly important as graph data rapidly grows. Existing approaches often\nrely on Variational Auto-Encoder (VAE) or its causal structure learning-based\nrefinement, which suffer from sub-optimality in VAEs due to the independence\nfactor assumption and unavailability of concept labels, respectively. In this\npaper, we propose an unsupervised solution, dubbed concept-free causal\ndisentanglement, built on a theoretically provable tight upper bound\napproximating the optimal factor. This results in an SCM-like causal structure\nmodeling that directly learns concept structures from data. Based on this idea,\nwe propose Concept-free Causal VGAE (CCVGAE) by incorporating a novel causal\ndisentanglement layer into Variational Graph Auto-Encoder. Furthermore, we\nprove concept consistency under our concept-free causal disentanglement\nframework, hence employing it to enhance the meta-learning framework, called\nconcept-free causal Meta-Graph (CC-Meta-Graph). We conduct extensive\nexperiments to demonstrate the superiority of the proposed models: CCVGAE and\nCC-Meta-Graph, reaching up to $29\\%$ and $11\\%$ absolute improvements over\nbaselines in terms of AUC, respectively.",
            "author": [
                "Jingyun Feng",
                "Lin Zhang",
                "Lili Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10638v1",
                "http://arxiv.org/pdf/2311.10638v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10637v2",
            "title": "On the Rectangles Induced by Points",
            "updated": "2023-12-04T17:22:01Z",
            "published": "2023-11-17T16:48:32Z",
            "summary": "$ \\newcommand{\\Re}{\\mathbb{R}} \\newcommand{\\reals}{\\mathbb{R}}\n\\newcommand{\\SetX}{\\mathsf{X}} \\newcommand{\\rad}{r} \\newcommand{\\Mh}[1]{#1}\n\\newcommand{\\query}{q} \\newcommand{\\eps}{\\varepsilon}\n\\newcommand{\\VorX}[1]{\\mathcal{V} \\pth{#1}} \\newcommand{\\Polygon}{\\mathsf{P}}\n\\newcommand{\\IntRange}[1]{[ #1 ]} \\newcommand{\\Space}{\\overline{\\mathsf{m}}}\n\\newcommand{\\pth}[2][\\!]{#1\\left({#2}\\right)}\n\\newcommand{\\polylog}{\\mathrm{polylog}} \\newcommand{\\N}{\\mathbb N}\n\\newcommand{\\Z}{\\mathbb Z} \\newcommand{\\pt}{p} \\newcommand{\\distY}[2]{\\left\\|\n{#1} - {#2} \\right\\|} \\newcommand{\\ptq}{q} \\newcommand{\\pts}{s}$ A set $P$ of\n$n$ points in the plane, induces a set of Delaunay-type axis-parallel\nrectangles $\\mathcal{R}$, potentially of quadratic size, where an axis-parallel\nrectangle is in $\\mathcal{R}$, if it has two points of $P$ as corners, and no\nother point of $P$ in it. We study various algorithmic problems related to this\nset of rectangles, including how to compute it, in near linear time, and handle\nvarious algorithmic tasks on it, such as computing its union and depth. The set\nof rectangles $\\mathcal{R}$ induces the rectangle influence graph $G =\n(P,\\mathcal{R})$, which we also study. Potentially our most interesting result\nis showing that this graph can be described as the union of $O(n)$ bicliques,\nwhere the total weight of the bicliques is $O(n \\log^2 n)$. Here, the weight of\na bicliques is the cardinality of its vertices.",
            "author": [
                "Stav Ashur",
                "Sariel Har-Peled"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10637v2",
                "http://arxiv.org/pdf/2311.10637v2"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10633v1",
            "title": "Predicting the Probability of Collision of a Satellite with Space\n  Debris: A Bayesian Machine Learning Approach",
            "updated": "2023-11-17T16:41:35Z",
            "published": "2023-11-17T16:41:35Z",
            "summary": "Space is becoming more crowded in Low Earth Orbit due to increased space\nactivity. Such a dense space environment increases the risk of collisions\nbetween space objects endangering the whole space population. Therefore, the\nneed to consider collision avoidance as part of routine operations is evident\nto satellite operators. Current procedures rely on the analysis of multiple\ncollision warnings by human analysts. However, with the continuous growth of\nthe space population, this manual approach may become unfeasible, highlighting\nthe importance of automation in risk assessment. In 2019, ESA launched a\ncompetition to study the feasibility of applying machine learning in collision\nrisk estimation and released a dataset that contained sequences of Conjunction\nData Messages (CDMs) in support of real close encounters. The competition\nresults showed that the naive forecast and its variants are strong predictors\nfor this problem, which suggests that the CDMs may follow the Markov property.\nThe proposed work investigates this theory by benchmarking Hidden Markov Models\n(HMM) in predicting the risk of collision between two resident space objects by\nusing one feature of the entire dataset: the sequence of the probability in the\nCDMs. In addition, Bayesian statistics are used to infer a joint distribution\nfor the parameters of the models, which allows the development of robust and\nreliable probabilistic predictive models that can incorporate physical or prior\nknowledge about the problem within a rigorous theoretical framework and\nprovides prediction uncertainties that nicely reflect the accuracy of the\npredicted risk. This work shows that the implemented HMM outperforms the naive\nsolution in some metrics, which further adds to the idea that the collision\nwarnings may be Markovian and suggests that this is a powerful method to be\nfurther explored.",
            "author": [
                "Jo\u00e3o Sim\u00f5es Catulo",
                "Cl\u00e1udia Soares",
                "Marta Guimar\u00e3es"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10633v1",
                "http://arxiv.org/pdf/2311.10633v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10627v1",
            "title": "Gravity-Induced Ice Compaction and Subsurface Porosity on Icy Moons",
            "updated": "2023-11-17T16:28:33Z",
            "published": "2023-11-17T16:28:33Z",
            "summary": "Our understanding of the surface porosity of icy moons and its evolution with\ndepth remains limited, including the precise scale at which ice compaction\noccurs under self-weight pressure. This parameter is of crucial interest for\nthe correct interpretation of current remote sensing data (spectroscopy in the\nvisible, infrared to passive microwave) but also for planetary exploration when\ndesigning a lander, a rover or a cryobot. In situ exploration of the ice crust\nwould require knowledge about subsurface porosity. This study employs a\ncompaction model solely driven by overburden pressure based on prior research.\nThe formulation for density as a function of depth, incorporates an essential\nparameter: the ice compaction coefficient. To determine this coefficient, we\nfit our depth-dependent density model to existing data obtained from\nEarth-based measurements of ice cores in Antarctica and North Greenland. Our\nresults yield a typical lengthscale for ice compaction on Earth of\napproximately 20.1 $\\pm$ 0.6 m , consistent with the existing literature. We\napply the model to Europa, which due to its lower gravity, has a typical ice\ncompaction scale of 150 $\\pm$ 4 m. We compare it with the depths scanned by\ncurrent spaceborne data and find that porosity can be considered constant when\naccounting only for gravity-induced compaction.",
            "author": [
                "Cyril Mergny",
                "Fr\u00e9d\u00e9ric Schmidt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10627v1",
                "http://arxiv.org/pdf/2311.10627v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10625v1",
            "title": "Central limit theorems for Soft random simplicial complexes",
            "updated": "2023-11-17T16:27:40Z",
            "published": "2023-11-17T16:27:40Z",
            "summary": "A soft random graph $G(n,r,p)$ can be obtained from the random geometric\ngraph $G(n,r)$ by keeping every edge in $G(n,r)$ with probability $p$. The soft\nrandom simplicial complexes is a model for random simplicial complexes built\nover the soft random graph $G(n,r,p)$. This new model depends on a probability\nvector $\\rho$ which allows the simplicial complexes to present randomness in\nall dimensions. In this article, we use a normal approximation theorem to prove\ncentral limit theorems for the number of $k$-faces and for the Euler's\ncharacteristic for soft random simplicial complexes.",
            "author": [
                "Juli\u00e1n David Candela"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10625v1",
                "http://arxiv.org/pdf/2311.10625v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10616v1",
            "title": "Sparsity-Parameterised Dynamic Edge Colouring",
            "updated": "2023-11-17T16:13:01Z",
            "published": "2023-11-17T16:13:01Z",
            "summary": "We study the edge-colouring problem, and give efficient algorithms where the\nnumber of colours is parameterised by the graph's arboricity, $\\alpha$. In a\ndynamic graph, subject to insertions and deletions, we give a deterministic\nalgorithm that updates a proper $\\Delta + O(\\alpha)$ edge~colouring in\n$\\operatorname{poly}(\\log n)$ amortised time. Our algorithm is fully adaptive\nto the current value of the maximum degree and arboricity.\n  In this fully-dynamic setting, the state-of-the-art edge-colouring algorithms\nare either a randomised algorithm using $(1 + \\varepsilon)\\Delta$ colours in\n$\\operatorname{poly}(\\log n, \\epsilon^{-1})$ time per update, or the naive\ngreedy algorithm which is a deterministic $2\\Delta -1$ edge colouring with\n$\\log(\\Delta)$ update time.\n  Compared to the $(1+\\varepsilon)\\Delta$ algorithm, our algorithm is\ndeterministic and asymptotically faster, and when $\\alpha$ is sufficiently\nsmall compared to $\\Delta$, it even uses fewer colours. In particular, ours is\nthe first $\\Delta+O(1)$ edge-colouring algorithm for dynamic forests, and\ndynamic planar graphs, with polylogarithmic update time.\n  Additionally, in the static setting, we show that we can find a proper edge\ncolouring with $\\max\\{deg(u), deg(v)\\} + 2\\alpha$ colours in $O(m\\log n)$ time.\nThis time bound matches that of the greedy algorithm that computes a\n$2\\Delta-1$ colouring of the graph's edges, and improves the number of colours\nwhen $\\alpha$ is sufficiently small compared to $\\Delta$.",
            "author": [
                "Aleksander B. J. Christiansen",
                "Eva Rotenberg",
                "Juliette Vlieghe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10616v1",
                "http://arxiv.org/pdf/2311.10616v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10614v1",
            "title": "A Self-enhancement Approach for Domain-specific Chatbot Training via\n  Knowledge Mining and Digest",
            "updated": "2023-11-17T16:09:10Z",
            "published": "2023-11-17T16:09:10Z",
            "summary": "Large Language Models (LLMs), despite their great power in language\ngeneration, often encounter challenges when dealing with intricate and\nknowledge-demanding queries in specific domains. This paper introduces a novel\napproach to enhance LLMs by effectively extracting the relevant knowledge from\ndomain-specific textual sources, and the adaptive training of a chatbot with\ndomain-specific inquiries. Our two-step approach starts from training a\nknowledge miner, namely LLMiner, which autonomously extracts Question-Answer\npairs from relevant documents through a chain-of-thought reasoning process.\nSubsequently, we blend the mined QA pairs with a conversational dataset to\nfine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise\nand conversational capabilities. We also developed a new evaluation benchmark\nwhich comprises four domain-specific text corpora and associated human-crafted\nQA pairs for testing. Our model shows remarkable performance improvement over\ngenerally aligned LLM and surpasses domain-adapted models directly fine-tuned\non domain corpus. In particular, LLMiner achieves this with minimal human\nintervention, requiring only 600 seed instances, thereby providing a pathway\ntowards self-improvement of LLMs through model-synthesized training data.",
            "author": [
                "Ruohong Zhang",
                "Luyu Gao",
                "Chen Zheng",
                "Zhen Fan",
                "Guokun Lai",
                "Zheng Zhang",
                "Fangzhou Ai",
                "Yiming Yang",
                "Hongxia Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10614v1",
                "http://arxiv.org/pdf/2311.10614v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10611v1",
            "title": "TacFR-Gripper: A Reconfigurable Fin Ray-Based Compliant Robotic Gripper\n  with Tactile Skin for In-Hand Manipulation",
            "updated": "2023-11-17T16:04:41Z",
            "published": "2023-11-17T16:04:41Z",
            "summary": "This paper introduces the TacFR-Gripper, a reconfigurable Fin Ray-based soft\nand compliant robotic gripper equipped with tactile skin, which can be used for\ndexterous in-hand manipulation tasks. This gripper can adaptively grasp objects\nof diverse shapes and stiffness levels. An array of Force Sensitive Resistor\n(FSR) sensors is embedded within the robotic finger to serve as the tactile\nskin, enabling the robot to perceive contact information during manipulation.\nWe provide theoretical analysis for gripper design, including kinematic\nanalysis, workspace analysis, and finite element analysis to identify the\nrelationship between the gripper's load and its deformation. Moreover, we\nimplemented a Graph Neural Network (GNN)-based tactile perception approach to\nenable reliable grasping without accidental slip or excessive force.\n  Three physical experiments were conducted to quantify the performance of the\nTacFR-Gripper. These experiments aimed to i) assess the grasp success rate\nacross various everyday objects through different configurations, ii) verify\nthe effectiveness of tactile skin with the GNN algorithm in grasping, iii)\nevaluate the gripper's in-hand manipulation capabilities for object pose\ncontrol. The experimental results indicate that the TacFR-Gripper can grasp a\nwide range of complex-shaped objects with a high success rate and deliver\ndexterous in-hand manipulation. Additionally, the integration of tactile skin\nwith the GNN algorithm enhances grasp stability by incorporating tactile\nfeedback during manipulations. For more details of this project, please view\nour website: https://sites.google.com/view/tacfr-gripper/homepage.",
            "author": [
                "Qingzheng Cong",
                "Wen Fan",
                "Dandan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10611v1",
                "http://arxiv.org/pdf/2311.10611v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10610v1",
            "title": "A Poincar\u00e9 Inequality and Consistency Results for Signal Sampling on\n  Large Graphs",
            "updated": "2023-11-17T16:04:31Z",
            "published": "2023-11-17T16:04:31Z",
            "summary": "Large-scale graph machine learning is challenging as the complexity of\nlearning models scales with the graph size. Subsampling the graph is a viable\nalternative, but sampling on graphs is nontrivial as graphs are non-Euclidean.\nExisting graph sampling techniques require not only computing the spectra of\nlarge matrices but also repeating these computations when the graph changes,\ne.g., grows. In this paper, we introduce a signal sampling theory for a type of\ngraph limit -- the graphon. We prove a Poincar\\'e inequality for graphon\nsignals and show that complements of node subsets satisfying this inequality\nare unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting\nconnections with spectral clustering and Gaussian elimination, we prove that\nsuch sampling sets are consistent in the sense that unique sampling sets on a\nconvergent graph sequence converge to unique sampling sets on the graphon. We\nthen propose a related graphon signal sampling algorithm for large graphs, and\ndemonstrate its good empirical performance on graph machine learning tasks.",
            "author": [
                "Thien Le",
                "Luana Ruiz",
                "Stefanie Jegelka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10610v1",
                "http://arxiv.org/pdf/2311.10610v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10600v1",
            "title": "Robust universal quantum processors in spin systems via Walsh pulse\n  sequences",
            "updated": "2023-11-17T15:55:04Z",
            "published": "2023-11-17T15:55:04Z",
            "summary": "We propose a protocol to realize quantum simulation and computation in spin\nsystems with long-range interactions. Our approach relies on the local\naddressing of single spins with external fields parametrized by Walsh\nfunctions. This enables a mapping from a class of target Hamiltonians, defined\nby the graph structure of their interactions, to pulse sequences. We then\nobtain a recipe to implement arbitrary two-body Hamiltonians and universal\nquantum circuits. Performance guarantees are provided in terms of bounds on\nTrotter errors and total number of pulses, and robustness to experimental\nimperfections. We demonstrate and numerically benchmark our protocol with\nexamples from the dynamical of spin models, quantum error correction and\nquantum optimization algorithms.",
            "author": [
                "Matteo Votto",
                "Johannes Zeiher",
                "Beno\u00eet Vermersch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10600v1",
                "http://arxiv.org/pdf/2311.10600v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.quant-gas"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10593v1",
            "title": "Generation and New Infinite Families of $K_2$-hypohamiltonian Graphs",
            "updated": "2023-11-17T15:47:02Z",
            "published": "2023-11-17T15:47:02Z",
            "summary": "We present an algorithm which can generate all pairwise non-isomorphic\n$K_2$-hypohamiltonian graphs, i.e. non-hamiltonian graphs in which the removal\nof any pair of adjacent vertices yields a hamiltonian graph, of a given order.\nWe introduce new bounding criteria specifically designed for\n$K_2$-hypohamiltonian graphs, allowing us to improve upon earlier computational\nresults. Specifically, we characterise the orders for which\n$K_2$-hypohamiltonian graphs exist and improve existing lower bounds on the\norders of the smallest planar and the smallest bipartite $K_2$-hypohamiltonian\ngraphs. Furthermore, we describe a new operation for creating\n$K_2$-hypohamiltonian graphs that preserves planarity under certain conditions\nand use it to prove the existence of a planar $K_2$-hypohamiltonian graph of\norder $n$ for every integer $n\\geq 134$. Additionally, motivated by a theorem\nof Thomassen on hypohamiltonian graphs, we show the existence\n$K_2$-hypohamiltonian graphs with large maximum degree and size.",
            "author": [
                "Jan Goedgebeur",
                "Jarne Renders",
                "Carol T. Zamfirescu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10593v1",
                "http://arxiv.org/pdf/2311.10593v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C38, 05C45, 05C85, 05C30, 05C76, 05C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10579v1",
            "title": "Graph Neural Networks for Pressure Estimation in Water Distribution\n  Systems",
            "updated": "2023-11-17T15:30:12Z",
            "published": "2023-11-17T15:30:12Z",
            "summary": "Pressure and flow estimation in Water Distribution Networks (WDN) allows\nwater management companies to optimize their control operations. For many\nyears, mathematical simulation tools have been the most common approach to\nreconstructing an estimate of the WDN hydraulics. However, pure physics-based\nsimulations involve several challenges, e.g. partially observable data, high\nuncertainty, and extensive manual configuration. Thus, data-driven approaches\nhave gained traction to overcome such limitations. In this work, we combine\nphysics-based modeling and Graph Neural Networks (GNN), a data-driven approach,\nto address the pressure estimation problem. First, we propose a new data\ngeneration method using a mathematical simulation but not considering temporal\npatterns and including some control parameters that remain untouched in\nprevious works; this contributes to a more diverse training data. Second, our\ntraining strategy relies on random sensor placement making our GNN-based\nestimation model robust to unexpected sensor location changes. Third, a\nrealistic evaluation protocol considers real temporal patterns and additionally\ninjects the uncertainties intrinsic to real-world scenarios. Finally, a\nmulti-graph pre-training strategy allows the model to be reused for pressure\nestimation in unseen target WDNs. Our GNN-based model estimates the pressure of\na large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of\n7%, surpassing the performance of previous studies. Likewise, it outperformed\nprevious approaches on other WDN benchmarks, showing a reduction of absolute\nerror up to approximately 52% in the best cases.",
            "author": [
                "Huy Truong",
                "Andr\u00e9s Tello",
                "Alexander Lazovik",
                "Victoria Degeler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10579v1",
                "http://arxiv.org/pdf/2311.10579v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10545v1",
            "title": "The $A_\u03b1$-spectra of graph operations based on generalized (edge)\n  corona",
            "updated": "2023-11-17T14:13:27Z",
            "published": "2023-11-17T14:13:27Z",
            "summary": "Let $G, H_{i}$ be simple graphs with $n=|V(G)|$, $m=|E(G)|$ and $i=1, 2,\n\\ldots, n(m)$. The generalized corona, denoted $G\\tilde{o}\\wedge^{n}_{i=1}\nH_{i}$, is the graph obtained by taking one copy of graphs $G, H_{1},\\ldots,\nH_{n}$ and joining the $i$th vertex of $G$ to every vertex of $H_{i}$ for $1\n\\leq i \\leq n$. The generalized edge corona, denoted by $G[H_i]_1^m$, is the\ngraph obtained by taking one copy of graphs $G, H_{1},\\ldots, H_{m}$ and then\njoining two end-vertices of the $i$th edge of $G$ to every vertex of $H_{i}$\nfor $1 \\leq i \\leq m$. For any real $\\alpha\\in[0,1]$, the matrix\n$A_{\\alpha}(G)=\\alpha D(G)+(1-\\alpha)A(G)$, where $A(G)$ and $D(G)$ are the\nadjacency matrix and the degree matrix of a graph $G$, respectively. In this\npaper, we obtain the $A_{\\alpha}$-characteristic polynomial of\n$G\\tilde{o}\\wedge^{n}_{i=1} H_{i}$, which extends some known results.\nMeanwhile, we determine the $A_{\\alpha}$-characteristic polynomial of\n$G[H_i]_1^m$ and get the $A_{\\alpha}$-spectrum of $G[H_i]_1^m$ when $G$ and\n$H_i$ are regular graphs for $1\\le i\\le m$. As an application of the above\nconclusions, we construct infinitely many pairs of non-regular\n$A_{\\alpha}$-cospectral graphs.",
            "author": [
                "Xiaxia Zhang",
                "Xiaoling Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10545v1",
                "http://arxiv.org/pdf/2311.10545v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10807v1",
            "title": "SENetV2: Aggregated dense layer for channelwise and global\n  representations",
            "updated": "2023-11-17T14:10:57Z",
            "published": "2023-11-17T14:10:57Z",
            "summary": "Convolutional Neural Networks (CNNs) have revolutionized image classification\nby extracting spatial features and enabling state-of-the-art accuracy in\nvision-based tasks. The squeeze and excitation network proposed module gathers\nchannelwise representations of the input. Multilayer perceptrons (MLP) learn\nglobal representation from the data and in most image classification models\nused to learn extracted features of the image. In this paper, we introduce a\nnovel aggregated multilayer perceptron, a multi-branch dense layer, within the\nSqueeze excitation residual module designed to surpass the performance of\nexisting architectures. Our approach leverages a combination of squeeze\nexcitation network module with dense layers. This fusion enhances the network's\nability to capture channel-wise patterns and have global knowledge, leading to\na better feature representation. This proposed model has a negligible increase\nin parameters when compared to SENet. We conduct extensive experiments on\nbenchmark datasets to validate the model and compare them with established\narchitectures. Experimental results demonstrate a remarkable increase in the\nclassification accuracy of the proposed model.",
            "author": [
                "Mahendran Narayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10807v1",
                "http://arxiv.org/pdf/2311.10807v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10540v1",
            "title": "A large and natural Class of $\u03a3^p_2$- and $\u03a3^p_3$-complete\n  Problems in Bilevel and Robust Optimization",
            "updated": "2023-11-17T14:07:49Z",
            "published": "2023-11-17T14:07:49Z",
            "summary": "Because $\\Sigma^p_2$- and $\\Sigma^p_3$-hardness proofs are usually tedious\nand difficult, not so many complete problems for these classes are known. This\nis especially true in the areas of min-max regret robust optimization, network\ninterdiction, most vital vertex problems, blocker problems, and two-stage\nadjustable robust optimization problems. Even though these areas are\nwell-researched for over two decades and one would naturally expect many (if\nnot most) of the problems occurring in these areas to be complete for the above\nclasses, almost no completeness results exist in the literature. We address\nthis lack of knowledge by introducing over 70 new $\\Sigma^p_2$-complete and\n$\\Sigma^p_3$-complete problems. We achieve this result by proving a new\nmeta-theorem, which shows $\\Sigma^p_2$- and $\\Sigma^p_3$-completeness\nsimultaneously for a huge class of problems. The majority of all earlier\npublications on $\\Sigma^p_2$- and $\\Sigma^p_3$-completeness in said areas are\nspecial cases of our meta-theorem. Our precise result is the following: We\nintroduce a large list of problems for which the meta-theorem is applicable\n(including clique, vertex cover, knapsack, TSP, facility location and many\nmore). For every problem on this list, we show: The interdiction/minimum cost\nblocker/most vital nodes problem (with element costs) is $\\Sigma^p_2$-complete.\nThe min-max-regret problem with interval uncertainty is $\\Sigma^p_2$-complete.\nThe two-stage adjustable robust optimization problem with discrete budgeted\nuncertainty is $\\Sigma^p_3$-complete. In summary, our work reveals the\ninteresting insight that a large amount of NP-complete problems have the\nproperty that their min-max versions are 'automatically' $\\Sigma^p_2$-complete.",
            "author": [
                "Christoph Gr\u00fcne",
                "Lasse Wulf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10540v1",
                "http://arxiv.org/pdf/2311.10540v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DM",
                "math.OC",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10536v1",
            "title": "Well-posedness of first-order acoustic wave equations and space-time\n  finite element approximation",
            "updated": "2023-11-17T14:01:31Z",
            "published": "2023-11-17T14:01:31Z",
            "summary": "We study a first-order system formulation of the (acoustic) wave equation and\nprove that the operator of this system is an isomorphsim from an appropriately\ndefined graph space to L^2. The results rely on well-posedness and stability of\nthe weak and ultraweak formulation of the second-order wave equation. As an\napplication we define and analyze a space-time least-squares finite element\nmethod for solving the wave equation. Some numerical examples for one- and two-\ndimensional spatial domains are presented.",
            "author": [
                "Thomas F\u00fchrer",
                "Roberto Gonz\u00e1lez",
                "Michael Karkulik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10536v1",
                "http://arxiv.org/pdf/2311.10536v1"
            ],
            "primary_category": "math.NA",
            "category": [
                "math.NA",
                "cs.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10806v1",
            "title": "SEA++: Multi-Graph-based High-Order Sensor Alignment for Multivariate\n  Time-Series Unsupervised Domain Adaptation",
            "updated": "2023-11-17T13:54:18Z",
            "published": "2023-11-17T13:54:18Z",
            "summary": "Unsupervised Domain Adaptation (UDA) methods have been successful in reducing\nlabel dependency by minimizing the domain discrepancy between a labeled source\ndomain and an unlabeled target domain. However, these methods face challenges\nwhen dealing with Multivariate Time-Series (MTS) data. MTS data typically\nconsist of multiple sensors, each with its own unique distribution. This\ncharacteristic makes it hard to adapt existing UDA methods, which mainly focus\non aligning global features while overlooking the distribution discrepancies at\nthe sensor level, to reduce domain discrepancies for MTS data. To address this\nissue, a practical domain adaptation scenario is formulated as Multivariate\nTime-Series Unsupervised Domain Adaptation (MTS-UDA). In this paper, we propose\nSEnsor Alignment (SEA) for MTS-UDA, aiming to reduce domain discrepancy at both\nthe local and global sensor levels. At the local sensor level, we design\nendo-feature alignment, which aligns sensor features and their correlations\nacross domains. To reduce domain discrepancy at the global sensor level, we\ndesign exo-feature alignment that enforces restrictions on global sensor\nfeatures. We further extend SEA to SEA++ by enhancing the endo-feature\nalignment. Particularly, we incorporate multi-graph-based high-order alignment\nfor both sensor features and their correlations. Extensive empirical results\nhave demonstrated the state-of-the-art performance of our SEA and SEA++ on\npublic MTS datasets for MTS-UDA.",
            "author": [
                "Yucheng Wang",
                "Yuecong Xu",
                "Jianfei Yang",
                "Min Wu",
                "Xiaoli Li",
                "Lihua Xie",
                "Zhenghua Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10806v1",
                "http://arxiv.org/pdf/2311.10806v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10528v1",
            "title": "Hyperbolic Circle Packings and Total Geodesic Curvatures on Surfaces\n  with Boundary",
            "updated": "2023-11-17T13:48:09Z",
            "published": "2023-11-17T13:48:09Z",
            "summary": "This paper investigates a generalized hyperbolic circle packing (including\ncircles, horocycles or hypercycles) with respect to the total geodesic\ncurvatures on the surface with boundary. We mainly focus on the existence and\nrigidity of circle packing whose contact graph is the $1$-skeleton of a finite\npolygonal cellular decomposition, which is analogous to the construction of\nBobenko and Springborn [4]. Motivated by Colin de Verdi\\`ere's method [6], we\nintroduce the variational principle for generalized hyperbolic circle packings\non polygons. By analyzing limit behaviours of generalized circle packings on\npolygons, we give an existence and rigidity for the generalized hyperbolic\ncircle packing with conical singularities regarding the total geodesic\ncurvature on each vertex of the contact graph. As a consequence, we introduce\nthe combinatoral Ricci flow to find a desired circle packing with a prescribed\ntotal geodesic curvature on each vertex of the contact graph.",
            "author": [
                "Guangming Hu",
                "Yi Qi",
                "Yu Sun",
                "Puchun Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10528v1",
                "http://arxiv.org/pdf/2311.10528v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "52C25, 52C26, 53A70"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10512v1",
            "title": "Causal Fairness-Guided Dataset Reweighting using Neural Networks",
            "updated": "2023-11-17T13:31:19Z",
            "published": "2023-11-17T13:31:19Z",
            "summary": "The importance of achieving fairness in machine learning models cannot be\noverstated. Recent research has pointed out that fairness should be examined\nfrom a causal perspective, and several fairness notions based on the on Pearl's\ncausal framework have been proposed. In this paper, we construct a reweighting\nscheme of datasets to address causal fairness. Our approach aims at mitigating\nbias by considering the causal relationships among variables and incorporating\nthem into the reweighting process. The proposed method adopts two neural\nnetworks, whose structures are intentionally used to reflect the structures of\na causal graph and of an interventional graph. The two neural networks can\napproximate the causal model of the data, and the causal model of\ninterventions. Furthermore, reweighting guided by a discriminator is applied to\nachieve various fairness notions. Experiments on real-world datasets show that\nour method can achieve causal fairness on the data while remaining close to the\noriginal data for downstream tasks.",
            "author": [
                "Xuan Zhao",
                "Klaus Broelemann",
                "Salvatore Ruggieri",
                "Gjergji Kasneci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10512v1",
                "http://arxiv.org/pdf/2311.10512v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10503v1",
            "title": "Witnessing quantum coherence with prior knowledge of observables",
            "updated": "2023-11-17T13:06:05Z",
            "published": "2023-11-17T13:06:05Z",
            "summary": "Quantum coherence is the key resource in quantum technologies including\nfaster computing, secure communication and advanced sensing. Its quantification\nand detection are, therefore, paramount within the context of quantum\ninformation processing. Having certain priori knowledge on the observables may\nenhance the efficiency of coherence detection. In this work, we posit that the\ntrace of the observables is a known quantity. Our investigation confirms that\nthis assumption indeed extends the scope of coherence detection capabilities.\nUtilizing this prior knowledge of the trace of the observables, we establish a\nseries of coherence detection criteria. We investigate the detection\ncapabilities of these coherence criteria from diverse perspectives and\nultimately ascertain the existence of four distinct and inequivalent criteria.\nThese findings contribute to the deepening of our understanding of coherence\ndetection methodologies, thereby potentially opening new avenues for\nadvancements in quantum technologies.",
            "author": [
                "Mao-Sheng Li",
                "Wen Xu",
                "Shao-Ming Fei",
                "Zhu-Jun Zheng",
                "Yan-Ling Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10503v1",
                "http://arxiv.org/pdf/2311.10503v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10502v1",
            "title": "Fast Estimations of Hitting Time of Elitist Evolutionary Algorithms from\n  Fitness Levels",
            "updated": "2023-11-17T13:04:42Z",
            "published": "2023-11-17T13:04:42Z",
            "summary": "The fitness level method is an easy-to-use tool for estimating the hitting\ntime of elitist EAs. Recently, general linear lower and upper bounds from\nfitness levels have been constructed. However, the construction of these bounds\nrequires recursive computation, which makes them difficult to use in practice.\nWe address this shortcoming with a new directed graph (digraph) method that\ndoes not require recursive computation and significantly simplifies the\ncalculation of coefficients in linear bounds. In this method, an EA is modeled\nas a Markov chain on a digraph. Lower and upper bounds are directly calculated\nusing conditional transition probabilities on the digraph. This digraph method\nprovides straightforward and explicit expressions of lower and upper time bound\nfor elitist EAs. In particular, it can be used to derive tight lower bound on\nboth fitness landscapes without and with shortcuts. This is demonstrated\nthrough four examples: the (1+1) EA on OneMax, FullyDeceptive, TwoMax1 and\nDeceptive. Our work extends the fitness level method from addressing simple\nfitness functions without shortcuts to more realistic functions with shortcuts.",
            "author": [
                "Jun He",
                "Siang Yew Chong",
                "Xin Yao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10502v1",
                "http://arxiv.org/pdf/2311.10502v1"
            ],
            "primary_category": "cs.NE",
            "category": [
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10501v1",
            "title": "Collaborative Word-based Pre-trained Item Representation for\n  Transferable Recommendation",
            "updated": "2023-11-17T13:02:25Z",
            "published": "2023-11-17T13:02:25Z",
            "summary": "Item representation learning (IRL) plays an essential role in recommender\nsystems, especially for sequential recommendation. Traditional sequential\nrecommendation models usually utilize ID embeddings to represent items, which\nare not shared across different domains and lack the transferable ability.\nRecent studies use pre-trained language models (PLM) for item text embeddings\n(text-based IRL) that are universally applicable across domains. However, the\nexisting text-based IRL is unaware of the important collaborative filtering\n(CF) information. In this paper, we propose CoWPiRec, an approach of\nCollaborative Word-based Pre-trained item representation for Recommendation. To\neffectively incorporate CF information into text-based IRL, we convert the\nitem-level interaction data to a word graph containing word-level\ncollaborations. Subsequently, we design a novel pre-training task to align the\nword-level semantic- and CF-related item representation. Extensive experimental\nresults on multiple public datasets demonstrate that compared to\nstate-of-the-art transferable sequential recommenders, CoWPiRec achieves\nsignificantly better performances in both fine-tuning and zero-shot settings\nfor cross-scenario recommendation and effectively alleviates the cold-start\nissue. The code is available at: https://github.com/ysh-1998/CoWPiRec.",
            "author": [
                "Shenghao Yang",
                "Chenyang Wang",
                "Yankai Liu",
                "Kangping Xu",
                "Weizhi Ma",
                "Yiqun Liu",
                "Min Zhang",
                "Haitao Zeng",
                "Junlan Feng",
                "Chao Deng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10501v1",
                "http://arxiv.org/pdf/2311.10501v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10489v2",
            "title": "Handling Overlapping Asymmetric Datasets -- A Twice Penalized P-Spline\n  Approach",
            "updated": "2023-11-20T13:37:58Z",
            "published": "2023-11-17T12:41:07Z",
            "summary": "Overlapping asymmetric datasets are common in data science and pose questions\nof how they can be incorporated together into a predictive analysis. In\nhealthcare datasets there is often a small amount of information that is\navailable for a larger number of patients such as an electronic health record,\nhowever a small number of patients may have had extensive further testing.\nCommon solutions such as missing imputation can often be unwise if the smaller\ncohort is significantly different in scale to the larger sample, therefore the\naim of this research is to develop a new method which can model the smaller\ncohort against a particular response, whilst considering the larger cohort\nalso. Motivated by non-parametric models, and specifically flexible smoothing\ntechniques via generalized additive models, we model a twice penalized P-Spline\napproximation method to firstly prevent over/under-fitting of the smaller\ncohort and secondly to consider the larger cohort. This second penalty is\ncreated through discrepancies in the marginal value of covariates that exist in\nboth the smaller and larger cohorts. Through data simulations, parameter\ntunings and model adaptations to consider a continuous and binary response, we\nfind our twice penalized approach offers an enhanced fit over a linear B-Spline\nand once penalized P-Spline approximation. Applying to a real-life dataset\nrelating to a person's risk of developing Non-Alcoholic Steatohepatitis, we see\nan improved model fit performance of over 65%. Areas for future work within\nthis space include adapting our method to not require dimensionality reduction\nand also consider parametric modelling methods. However, to our knowledge this\nis the first work to propose additional marginal penalties in a flexible\nregression of which we can report a vastly improved model fit that is able to\nconsider asymmetric datasets, without the need for missing data imputation.",
            "author": [
                "Matthew McTeer",
                "Robin Henderson",
                "Quentin M Anstee",
                "Paolo Missier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10489v2",
                "http://arxiv.org/pdf/2311.10489v2"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME",
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10484v1",
            "title": "Learning Agile Locomotion on Risky Terrains",
            "updated": "2023-11-17T12:32:57Z",
            "published": "2023-11-17T12:32:57Z",
            "summary": "Quadruped robots have shown remarkable mobility on various terrains through\nreinforcement learning. Yet, in the presence of sparse footholds and risky\nterrains such as stepping stones and balance beams, which require precise foot\nplacement to avoid falls, model-based approaches are often used. In this paper,\nwe show that end-to-end reinforcement learning can also enable the robot to\ntraverse risky terrains with dynamic motions. To this end, our approach\ninvolves training a generalist policy for agile locomotion on disorderly and\nsparse stepping stones before transferring its reusable knowledge to various\nmore challenging terrains by finetuning specialist policies from it. Given that\nthe robot needs to rapidly adapt its velocity on these terrains, we formulate\nthe task as a navigation task instead of the commonly used velocity tracking\nwhich constrains the robot's behavior and propose an exploration strategy to\novercome sparse rewards and achieve high robustness. We validate our proposed\nmethod through simulation and real-world experiments on an ANYmal-D robot\nachieving peak forward velocity of >= 2.5 m/s on sparse stepping stones and\nnarrow balance beams. Video: youtu.be/Z5X0J8OH6z4",
            "author": [
                "Chong Zhang",
                "Nikita Rudin",
                "David Hoeller",
                "Marco Hutter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10484v1",
                "http://arxiv.org/pdf/2311.10484v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10480v1",
            "title": "(Quantum) complexity of testing signed graph clusterability",
            "updated": "2023-11-17T12:22:10Z",
            "published": "2023-11-17T12:22:10Z",
            "summary": "This study examines clusterability testing for a signed graph in the\nbounded-degree model. Our contributions are two-fold. First, we provide a\nquantum algorithm with query complexity $\\tilde{O}(N^{1/3})$ for testing\nclusterability, which yields a polynomial speedup over the best classical\nclusterability tester known [arXiv:2102.07587]. Second, we prove an\n$\\tilde{\\Omega}(\\sqrt{N})$ classical query lower bound for testing\nclusterability, which nearly matches the upper bound from [arXiv:2102.07587].\nThis settles the classical query complexity of clusterability testing, and it\nshows that our quantum algorithm has an advantage over any classical algorithm.",
            "author": [
                "Kuo-Chin Chen",
                "Simon Apers",
                "Min-Hsiu Hsieh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10480v1",
                "http://arxiv.org/pdf/2311.10480v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10475v1",
            "title": "Conway's law, revised from a mathematical viewpoint",
            "updated": "2023-11-17T12:08:37Z",
            "published": "2023-11-17T12:08:37Z",
            "summary": "In this article, we revise Conway's Law from a mathematical point of view. By\nintroducing a task graph, we first rigorously state Conway's Law based on the\nhomomorphisms in graph theory for the software system and the organizations\nthat created it. Though Conway did not mention it, the task graph shows the\ngeometric structure of tasks, which plays a crucial role. Furthermore, due to\nrecent requirements for high-level treatment of communication (due to security,\nknowledge hiding, etc.) in organizations and hierarchical treatment of\norganizations, we have reformulated these statements in terms of weakened\nhomomorphisms, and the continuous maps in graph topology. In order to use graph\ntopology and the continuous map in Conway's law, we have prepared them as\nmathematical tools, and then we show the natural expression of Conway's\ncorrespondences with hierarchical structures.",
            "author": [
                "Shigeki Matsutani",
                "Shousuke Ohmori",
                "Kenji Hiranabe",
                "Eiichi Hanyuda"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10475v1",
                "http://arxiv.org/pdf/2311.10475v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CY",
                "math.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10470v2",
            "title": "On minimal extended representations of generalized power cones",
            "updated": "2023-11-22T10:50:39Z",
            "published": "2023-11-17T11:51:28Z",
            "summary": "In this paper, we analyze minimal representations of\n$(p,\\boldsymbol{\\alpha})$-power cones as simpler cones. We derive some new\nresults on the complexity of the representations, and we provide a procedure to\nconstruct a minimal representation by means of second order cones in case\n$\\boldsymbol{\\alpha}$ and $p$ are rational. The construction is based on the\nidentification of the cones with a graph, the mediated graph. Then, we develop\na mixed integer linear optimization formulation to obtain the optimal mediated\ngraph, and then, the minimal representation. We present the results of a series\nof computational experiments in order to analyze the computational performance\nof the approach, both to obtain the representation and its incorporation into a\npractical conic optimization model that arises in facility location.",
            "author": [
                "V\u00edctor Blanco",
                "Miguel Mart\u00ednez-Ant\u00f3n"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10470v2",
                "http://arxiv.org/pdf/2311.10470v2"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "90C23, 90C25, 91B32, 90C10, 90C27, 90C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10463v1",
            "title": "Correlation-Distance Graph Learning for Treatment Response Prediction\n  from rs-fMRI",
            "updated": "2023-11-17T11:34:01Z",
            "published": "2023-11-17T11:34:01Z",
            "summary": "Resting-state fMRI (rs-fMRI) functional connectivity (FC) analysis provides\nvaluable insights into the relationships between different brain regions and\ntheir potential implications for neurological or psychiatric disorders.\nHowever, specific design efforts to predict treatment response from rs-fMRI\nremain limited due to difficulties in understanding the current brain state and\nthe underlying mechanisms driving the observed patterns, which limited the\nclinical application of rs-fMRI. To overcome that, we propose a graph learning\nframework that captures comprehensive features by integrating both correlation\nand distance-based similarity measures under a contrastive loss. This approach\nresults in a more expressive framework that captures brain dynamic features at\ndifferent scales and enables more accurate prediction of treatment response.\nOur experiments on the chronic pain and depersonalization disorder datasets\ndemonstrate that our proposed method outperforms current methods in different\nscenarios. To the best of our knowledge, we are the first to explore the\nintegration of distance-based and correlation-based neural similarity into\ngraph learning for treatment response prediction.",
            "author": [
                "Xiatian Zhang",
                "Sisi Zheng",
                "Hubert P. H. Shum",
                "Haozheng Zhang",
                "Nan Song",
                "Mingkang Song",
                "Hongxiao Jia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10463v1",
                "http://arxiv.org/pdf/2311.10463v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10459v1",
            "title": "Hermitian Pseudospectral Shattering, Cholesky, Hermitian Eigenvalues,\n  and Density Functional Theory in Nearly Matrix Multiplication Time",
            "updated": "2023-11-17T11:28:34Z",
            "published": "2023-11-17T11:28:34Z",
            "summary": "Floating point algorithms are studied for computational problems arising in\nDensity Functional Theory (DFT), a powerful technique to determine the\nelectronic structure of solids, e.g., metals, oxides, or semiconductors.\nSpecifically, we seek algorithms with provable properties for the density\nmatrix and the corresponding electron density in atomic systems described by\nthe Kohn-Sham equations expressed in a localized basis set. The underlying\nproblem is a Hermitian generalized eigenvalue problem of the form $HC=SCE$,\nwhere $H$ is Hermitian and $S$ is Hermitian positive-definite (HPD). Different\nmethods are developed and combined to solve this problem. We first describe a\nHermitian pseudospectral shattering method in finite precision, and use it to\nobtain a new gap-independent floating point algorithm to compute all\neigenvalues of a Hermitian matrix within an additive error $\\delta$ in\n$O(T_{MM}(n)\\log^2(\\tfrac{n}{\\delta}))$. Here $T_{MM}(n) = O(n^{\\omega+\\eta})$,\nfor any $\\eta>0$, and $\\omega\\leq 2.371552$ is the matrix multiplication\nexponent.To the best of our knowledge, this is the first algorithm to achieve\nnearly $O(n^\\omega)$ bit complexity for all Hermitian eigenvalues. As\nby-products, we also demonstrate additive error approximations for all singular\nvalues of rectangular matrices, and, for full-rank matrices, relative error\napproximations for all eigenvalues, all singular values, the spectral norm, and\nthe condition number. We finally provide a novel analysis of a\nlogarithmically-stable Cholesky factorization algorithm, and show that it can\nbe used to accurately transform the HPD generalized eigenproblem to a Hermitian\neigenproblem in $O(T_{MM}(n))$. All these tools are combined to obtain the\nfirst provably accurate floating point algorithms with nearly $O(T_{MM}(n))$\nbit complexity for the density matrix and the electron density of atomic\nsystems.",
            "author": [
                "Aleksandros Sobczyk",
                "Marko Mladenovi\u0107",
                "Mathieu Luisier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10459v1",
                "http://arxiv.org/pdf/2311.10459v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.NA",
                "math.NA",
                "65F15",
                "F.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10450v1",
            "title": "A Novel VAPT Algorithm: Enhancing Web Application Security Trough OWASP\n  top 10 Optimization",
            "updated": "2023-11-17T11:11:21Z",
            "published": "2023-11-17T11:11:21Z",
            "summary": "This research study is built upon cybersecurity audits and investigates the\noptimization of an Open Web Application Security Project (OWASP) Top 10\nalgorithm for Web Applications (WA) security audits using Vulnerability\nAssessment and Penetration Testing (VAPT) processes. The study places\nparticular emphasis on enhancing the VAPT process by optimizing the OWASP\nalgorithm. To achieve this, the research utilizes desk documents to gain\nknowledge of WA cybersecurity audits and their associated tools. It also delves\ninto archives to explore VAPT processes and identify techniques, methods, and\ntools for VAPT automation. Furthermore, the research proposes a prototype\noptimization that streamlines the two steps of VAPT using the OWASP Top 10\nalgorithm through an experimental procedure. The results are obtained within a\nvirtual environment, which employs black box testing methods as the primary\nmeans of data acquisition and analysis. In this experimental setting, the OWASP\nalgorithm demonstrates an impressive level of precision, achieving a precision\nrate exceeding 90%. It effectively covers all researched vulnerabilities, thus\njustifying its optimization. This research contributes significantly to the\nenhancement of the OWASP algorithm and benefits the offensive security\ncommunity. It plays a crucial role in ensuring compliance processes for\nprofessionals and analysts in the security and software development fields.",
            "author": [
                "Rui Ventura",
                "Daniel Jose Franco",
                "Omar Khasro Akram"
            ],
            "link": [
                "http://dx.doi.org/10.5121/csit.2023.132002",
                "http://arxiv.org/abs/2311.10450v1",
                "http://arxiv.org/pdf/2311.10450v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10440v1",
            "title": "Parallel Verification of Natural Deduction Proof Graphs",
            "updated": "2023-11-17T10:33:23Z",
            "published": "2023-11-17T10:33:23Z",
            "summary": "Graph-based interactive theorem provers offer a visual representation of\nproofs, explicitly representing the dependencies and inferences between each of\nthe proof steps in a graph or hypergraph format. The number and complexity of\nthese dependency links can determine how long it takes to verify the validity\nof the entire proof. Towards this end, we present a set of parallel algorithms\nfor the formal verification of graph-based natural-deduction (ND) style proofs.\nWe introduce a definition of layering that captures dependencies between the\nproof steps (nodes). Nodes in each layer can then be verified in parallel as\nlong as prior layers have been verified. To evaluate the performance of our\nalgorithms on proof graphs, we propose a framework for finding the performance\nbounds and patterns using directed acyclic network topologies (DANTs). This\nframework allows us to create concrete instances of DANTs for empirical\nevaluation of our algorithms. With this, we compare our set of parallel\nalgorithms against a serial implementation with two experiments: one scaling\nboth the problem size and the other scaling the number of threads. Our findings\nshow that parallelization results in improved verification performance for\ncertain DANT instances. We also show that our algorithms scale for certain DANT\ninstances with respect to the number of threads.",
            "author": [
                "James T. Oswald",
                "Brandon Rozek"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.396.4",
                "http://arxiv.org/abs/2311.10440v1",
                "http://arxiv.org/pdf/2311.10440v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10437v1",
            "title": "DUA-DA: Distillation-based Unbiased Alignment for Domain Adaptive Object\n  Detection",
            "updated": "2023-11-17T10:26:26Z",
            "published": "2023-11-17T10:26:26Z",
            "summary": "Though feature-alignment based Domain Adaptive Object Detection (DAOD) have\nachieved remarkable progress, they ignore the source bias issue, i.e. the\naligned features are more favorable towards the source domain, leading to a\nsub-optimal adaptation. Furthermore, the presence of domain shift between the\nsource and target domains exacerbates the problem of inconsistent\nclassification and localization in general detection pipelines. To overcome\nthese challenges, we propose a novel Distillation-based Unbiased Alignment\n(DUA) framework for DAOD, which can distill the source features towards a more\nbalanced position via a pre-trained teacher model during the training process,\nalleviating the problem of source bias effectively. In addition, we design a\nTarget-Relevant Object Localization Network (TROLN), which can mine\ntarget-related knowledge to produce two classification-free metrics (IoU and\ncenterness). Accordingly, we implement a Domain-aware Consistency Enhancing\n(DCE) strategy that utilizes these two metrics to further refine classification\nconfidences, achieving a harmonization between classification and localization\nin cross-domain scenarios. Extensive experiments have been conducted to\nmanifest the effectiveness of this method, which consistently improves the\nstrong baseline by large margins, outperforming existing alignment-based works.",
            "author": [
                "Yongchao Feng",
                "Shiwei Li",
                "Yingjie Gao",
                "Ziyue Huang",
                "Yanan Zhang",
                "Qingjie Liu",
                "Yunhong Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10437v1",
                "http://arxiv.org/pdf/2311.10437v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10431v1",
            "title": "Causal Graph in Language Model Rediscovers Cortical Hierarchy in Human\n  Narrative Processing",
            "updated": "2023-11-17T10:09:12Z",
            "published": "2023-11-17T10:09:12Z",
            "summary": "Understanding how humans process natural language has long been a vital\nresearch direction. The field of natural language processing (NLP) has recently\nexperienced a surge in the development of powerful language models. These\nmodels have proven to be invaluable tools for studying another complex system\nknown to process human language: the brain. Previous studies have demonstrated\nthat the features of language models can be mapped to fMRI brain activity. This\nraises the question: is there a commonality between information processing in\nlanguage models and the human brain? To estimate information flow patterns in a\nlanguage model, we examined the causal relationships between different layers.\nDrawing inspiration from the workspace framework for consciousness, we\nhypothesized that features integrating more information would more accurately\npredict higher hierarchical brain activity. To validate this hypothesis, we\nclassified language model features into two categories based on causal network\nmeasures: 'low in-degree' and 'high in-degree'. We subsequently compared the\nbrain prediction accuracy maps for these two groups. Our results reveal that\nthe difference in prediction accuracy follows a hierarchical pattern,\nconsistent with the cortical hierarchy map revealed by activity time constants.\nThis finding suggests a parallel between how language models and the human\nbrain process linguistic information.",
            "author": [
                "Zhengqi He",
                "Taro Toyoizumi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10431v1",
                "http://arxiv.org/pdf/2311.10431v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10407v1",
            "title": "Quantum Counting by Quantum Walks",
            "updated": "2023-11-17T09:22:28Z",
            "published": "2023-11-17T09:22:28Z",
            "summary": "Quantum counting is a key quantum algorithm that aims to determine the number\nof marked elements in a database. This algorithm is based on the quantum phase\nestimation algorithm and uses the evolution operator of Grover's algorithm\nbecause its non-trivial eigenvalues are dependent on the number of marked\nelements. Since Grover's algorithm can be viewed as a quantum walk on a\ncomplete graph, a natural way to extend quantum counting is to use the\nevolution operator of quantum-walk-based search on non-complete graphs instead\nof Grover's operator. In this paper, we explore this extension by analyzing the\ncoined quantum walk on the complete bipartite graph with an arbitrary number of\nmarked vertices. We show that some eigenvalues of the evolution operator depend\non the number of marked vertices and using this fact we show that the quantum\nphase estimation can be used to obtain the number of marked vertices. The time\ncomplexity for estimating the number of marked vertices in the bipartite graph\nwith our algorithm aligns closely with that of the original quantum counting\nalgorithm.",
            "author": [
                "Gustavo A. Bezerra",
                "Raqueline A. M. Santos",
                "Renato Portugal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10407v1",
                "http://arxiv.org/pdf/2311.10407v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10401v1",
            "title": "LLM-based Control Code Generation using Image Recognition",
            "updated": "2023-11-17T09:04:44Z",
            "published": "2023-11-17T09:04:44Z",
            "summary": "LLM-based code generation could save significant manual efforts in industrial\nautomation, where control engineers manually produce control logic for\nsophisticated production processes. Previous attempts in control logic code\ngeneration lacked methods to interpret schematic drawings from process\nengineers. Recent LLMs now combine image recognition, trained domain knowledge,\nand coding skills. We propose a novel LLM-based code generation method that\ngenerates IEC 61131-3 Structure Text control logic source code from\nPiping-and-Instrumentation Diagrams (P&IDs) using image recognition. We have\nevaluated the method in three case study with industrial P&IDs and provide\nfirst evidence on the feasibility of such a code generation besides experiences\non image recognition glitches.",
            "author": [
                "Heiko Koziolek",
                "Anne Koziolek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10401v1",
                "http://arxiv.org/pdf/2311.10401v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "D.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10392v1",
            "title": "Maximal cocliques and the chromatic number of the Kneser graph on\n  chambers of PG$(3,q)$",
            "updated": "2023-11-17T08:46:11Z",
            "published": "2023-11-17T08:46:11Z",
            "summary": "Let $\\Gamma$ be the graph whose vertices are the chambers of the finite\nprojective $3$-space PG$(3,q)$, with two vertices being adjacent if and only if\nthe corresponding chambers are in general position. We show that a maximal\nindependent set of vertices of $\\Gamma$ contains $q^4+3q^3+4q^2+3q+1$, or\n$3q^3+5q^2+3q+1$, or at most $3q^3+4q^2+3q+2$ elements. For $q\\geq 4$ the\nstructure of the largest maximal independent sets is described. For $q\\geq 7$\nthe structure of the maximal independent sets of the three largest\ncardinalities is described. Using the cardinality of the second largest maximal\nindependent sets, we show that the chromatic number of $\\Gamma$ is $q^2+q$.",
            "author": [
                "Philipp Heering",
                "Klaus Metsch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10392v1",
                "http://arxiv.org/pdf/2311.10392v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C69 (Primary) 51E20, 05C15 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10388v1",
            "title": "Automatic Smart Contract Comment Generation via Large Language Models\n  and In-Context Learning",
            "updated": "2023-11-17T08:31:09Z",
            "published": "2023-11-17T08:31:09Z",
            "summary": "The previous smart contract code comment (SCC) generation approaches can be\ndivided into two categories: fine-tuning paradigm-based approaches and\ninformation retrieval-based approaches. However, for the fine-tuning\nparadigm-based approaches, the performance may be limited by the quality of the\ngathered dataset for the downstream task and they may have knowledge-forgetting\nissues. While for the information retrieval-based approaches, it is difficult\nfor them to generate high-quality comments if similar code does not exist in\nthe historical repository. Therefore we want to utilize the domain knowledge\nrelated to SCC generation in large language models (LLMs) to alleviate the\ndisadvantages of these two types of approaches. In this study, we propose an\napproach SCCLLM based on LLMs and in-context learning. Specifically, in the\ndemonstration selection phase, SCCLLM retrieves the top-k code snippets from\nthe historical corpus by considering syntax, semantics, and lexical\ninformation. In the in-context learning phase, SCCLLM utilizes the retrieved\ncode snippets as demonstrations, which can help to utilize the related\nknowledge for this task. We select a large corpus from a smart contract\ncommunity Etherscan.io as our experimental subject. Extensive experimental\nresults show the effectiveness of SCCLLM when compared with baselines in\nautomatic evaluation and human evaluation.",
            "author": [
                "Junjie Zhao",
                "Xiang Chen",
                "Guang Yang",
                "Yiheng Shen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10388v1",
                "http://arxiv.org/pdf/2311.10388v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10379v2",
            "title": "Achromatic colorings of polarity graphs",
            "updated": "2023-11-21T10:08:00Z",
            "published": "2023-11-17T08:07:51Z",
            "summary": "A complete partition of a graph $G$ is a partition of the vertex set such\nthat there is at least one edge between any two parts. The largest $r$ such\nthat $G$ has a complete partition into $r$ parts, each of which is an\nindependent set, is the achromatic number of $G$. We determine the achromatic\nnumber of polarity graphs coming from generalized polygons. Our colorings of a\nfamily of unitary polarity graphs are used to solve a problem of Axenovich and\nMartin on complete partitions of $C_4$-free graphs. Furthermore, these\ncolorings prove that there are sequences of graphs which are optimally complete\nand have unbounded degree, a problem that had been studied for the sequence of\nhypercubes independently by Roichman, and Ahlswede, Bezrukov, Blokhuis, Metsch,\nand Moorhouse.",
            "author": [
                "Vladislav Taranchuk",
                "Craig Timmons"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10379v2",
                "http://arxiv.org/pdf/2311.10379v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10375v1",
            "title": "Quantum Data Encoding: A Comparative Analysis of Classical-to-Quantum\n  Mapping Techniques and Their Impact on Machine Learning Accuracy",
            "updated": "2023-11-17T08:00:08Z",
            "published": "2023-11-17T08:00:08Z",
            "summary": "This research explores the integration of quantum data embedding techniques\ninto classical machine learning (ML) algorithms, aiming to assess the\nperformance enhancements and computational implications across a spectrum of\nmodels. We explore various classical-to-quantum mapping methods, ranging from\nbasis encoding, angle encoding to amplitude encoding for encoding classical\ndata, we conducted an extensive empirical study encompassing popular ML\nalgorithms, including Logistic Regression, K-Nearest Neighbors, Support Vector\nMachines and ensemble methods like Random Forest, LightGBM, AdaBoost, and\nCatBoost. Our findings reveal that quantum data embedding contributes to\nimproved classification accuracy and F1 scores, particularly notable in models\nthat inherently benefit from enhanced feature representation. We observed\nnuanced effects on running time, with low-complexity models exhibiting moderate\nincreases and more computationally intensive models experiencing discernible\nchanges. Notably, ensemble methods demonstrated a favorable balance between\nperformance gains and computational overhead. This study underscores the\npotential of quantum data embedding in enhancing classical ML models and\nemphasizes the importance of weighing performance improvements against\ncomputational costs. Future research directions may involve refining quantum\nencoding processes to optimize computational efficiency and exploring\nscalability for real-world applications. Our work contributes to the growing\nbody of knowledge at the intersection of quantum computing and classical\nmachine learning, offering insights for researchers and practitioners seeking\nto harness the advantages of quantum-inspired techniques in practical\nscenarios.",
            "author": [
                "Minati Rath",
                "Hema Date"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10375v1",
                "http://arxiv.org/pdf/2311.10375v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10373v1",
            "title": "FOAL: Fine-grained Contrastive Learning for Cross-domain Aspect\n  Sentiment Triplet Extraction",
            "updated": "2023-11-17T07:56:01Z",
            "published": "2023-11-17T07:56:01Z",
            "summary": "Aspect Sentiment Triplet Extraction (ASTE) has achieved promising results\nwhile relying on sufficient annotation data in a specific domain. However, it\nis infeasible to annotate data for each individual domain. We propose to\nexplore ASTE in the cross-domain setting, which transfers knowledge from a\nresource-rich source domain to a resource-poor target domain, thereby\nalleviating the reliance on labeled data in the target domain. To effectively\ntransfer the knowledge across domains and extract the sentiment triplets\naccurately, we propose a method named Fine-grained cOntrAstive Learning (FOAL)\nto reduce the domain discrepancy and preserve the discriminability of each\ncategory. Experiments on six transfer pairs show that FOAL achieves 6%\nperformance gains and reduces the domain discrepancy significantly compared\nwith strong baselines. Our code will be publicly available once accepted.",
            "author": [
                "Ting Xu",
                "Zhen Wu",
                "Huiyun Yang",
                "Xinyu Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10373v1",
                "http://arxiv.org/pdf/2311.10373v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10370v1",
            "title": "Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly\n  Detection",
            "updated": "2023-11-17T07:49:20Z",
            "published": "2023-11-17T07:49:20Z",
            "summary": "Graph anomaly detection plays a crucial role in identifying exceptional\ninstances in graph data that deviate significantly from the majority. It has\ngained substantial attention in various domains of information security,\nincluding network intrusion, financial fraud, and malicious comments, et al.\nExisting methods are primarily developed in an unsupervised manner due to the\nchallenge in obtaining labeled data. For lack of guidance from prior knowledge\nin unsupervised manner, the identified anomalies may prove to be data noise or\nindividual data instances. In real-world scenarios, a limited batch of labeled\nanomalies can be captured, making it crucial to investigate the few-shot\nproblem in graph anomaly detection. Taking advantage of this potential, we\npropose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot\nMessage-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a\nself-supervised contrastive learning strategy within and across views to\ncapture intrinsic and transferable structural representations. Furthermore, we\npropose the Deep-GNN message-enhanced reconstruction module, which extensively\nexploits the few-shot label information and enables long-range propagation to\ndisseminate supervision signals to deeper unlabeled nodes. This module in turn\nassists in the training of self-supervised contrastive learning. Comprehensive\nexperimental results on six real-world datasets demonstrate that FMGAD can\nachieve better performance than other state-of-the-art methods, regardless of\nartificially injected anomalies or domain-organic anomalies.",
            "author": [
                "Fan Xu",
                "Nan Wang",
                "Xuezhi Wen",
                "Meiqi Gao",
                "Chaoqun Guo",
                "Xibin Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10370v1",
                "http://arxiv.org/pdf/2311.10370v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10798v1",
            "title": "INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and\n  Prognosis",
            "updated": "2023-11-17T07:28:16Z",
            "published": "2023-11-17T07:28:16Z",
            "summary": "Synthesizing information from multiple data sources plays a crucial role in\nthe practice of modern medicine. Current applications of artificial\nintelligence in medicine often focus on single-modality data due to a lack of\npublicly available, multimodal medical datasets. To address this limitation, we\nintroduce INSPECT, which contains de-identified longitudinal records from a\nlarge cohort of patients at risk for pulmonary embolism (PE), along with ground\ntruth labels for multiple outcomes. INSPECT contains data from 19,402 patients,\nincluding CT images, radiology report impression sections, and structured\nelectronic health record (EHR) data (i.e. demographics, diagnoses, procedures,\nvitals, and medications). Using INSPECT, we develop and release a benchmark for\nevaluating several baseline modeling approaches on a variety of important PE\nrelated tasks. We evaluate image-only, EHR-only, and multimodal fusion models.\nTrained models and the de-identified dataset are made available for\nnon-commercial use under a data use agreement. To the best of our knowledge,\nINSPECT is the largest multimodal dataset integrating 3D medical imaging and\nEHR for reproducible methods evaluation and research.",
            "author": [
                "Shih-Cheng Huang",
                "Zepeng Huo",
                "Ethan Steinberg",
                "Chia-Chun Chiang",
                "Matthew P. Lungren",
                "Curtis P. Langlotz",
                "Serena Yeung",
                "Nigam H. Shah",
                "Jason A. Fries"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10798v1",
                "http://arxiv.org/pdf/2311.10798v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10341v1",
            "title": "Federated Knowledge Graph Completion via Latent Embedding Sharing and\n  Tensor Factorization",
            "updated": "2023-11-17T06:03:56Z",
            "published": "2023-11-17T06:03:56Z",
            "summary": "Knowledge graphs (KGs), which consist of triples, are inherently incomplete\nand always require completion procedure to predict missing triples. In\nreal-world scenarios, KGs are distributed across clients, complicating\ncompletion tasks due to privacy restrictions. Many frameworks have been\nproposed to address the issue of federated knowledge graph completion. However,\nthe existing frameworks, including FedE, FedR, and FEKG, have certain\nlimitations. = FedE poses a risk of information leakage, FedR's optimization\nefficacy diminishes when there is minimal overlap among relations, and FKGE\nsuffers from computational costs and mode collapse issues. To address these\nissues, we propose a novel method, i.e., Federated Latent Embedding Sharing\nTensor factorization (FLEST), which is a novel approach using federated tensor\nfactorization for KG completion. FLEST decompose the embedding matrix and\nenables sharing of latent dictionary embeddings to lower privacy risks.\nEmpirical results demonstrate FLEST's effectiveness and efficiency, offering a\nbalanced solution between performance and privacy. FLEST expands the\napplication of federated tensor factorization in KG completion tasks.",
            "author": [
                "Maolin Wang",
                "Dun Zeng",
                "Zenglin Xu",
                "Ruocheng Guo",
                "Xiangyu Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10341v1",
                "http://arxiv.org/pdf/2311.10341v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10337v1",
            "title": "Scalable Edge Clustering of Dynamic Graphs via Weighted Line Graphs",
            "updated": "2023-11-17T05:48:20Z",
            "published": "2023-11-17T05:48:20Z",
            "summary": "Timestamped relational datasets consisting of records between pairs of\nentities are ubiquitous in data and network science. For applications like\npeer-to-peer communication, email, social network interactions, and computer\nnetwork security, it makes sense to organize these records into groups based on\nhow and when they are occurring. Weighted line graphs offer a natural way to\nmodel how records are related in such datasets but for large real-world graph\ntopologies the complexity of building and utilizing the line graph is\nprohibitive. We present an algorithm to cluster the edges of a dynamic graph\nvia the associated line graph without forming it explicitly.\n  We outline a novel hierarchical dynamic graph edge clustering approach that\nefficiently breaks massive relational datasets into small sets of edges\ncontaining events at various timescales. This is in stark contrast to\ntraditional graph clustering algorithms that prioritize highly connected\ncommunity structures. Our approach relies on constructing a sufficient subgraph\nof a weighted line graph and applying a hierarchical agglomerative clustering.\nThis work draws particular inspiration from HDBSCAN.\n  We present a parallel algorithm and show that it is able to break\nbillion-scale dynamic graphs into small sets that correlate in topology and\ntime. The entire clustering process for a graph with $O(10 \\text{ billion})$\nedges takes just a few minutes of run time on 256 nodes of a distributed\ncompute environment. We argue how the output of the edge clustering is useful\nfor a multitude of data visualization and powerful machine learning tasks, both\ninvolving the original massive dynamic graph data and/or the non-relational\nmetadata. Finally, we demonstrate its use on a real-world large-scale directed\ndynamic graph and describe how it can be extended to dynamic hypergraphs and\ngraphs with unstructured data living on vertices and edges.",
            "author": [
                "Michael Ostroski",
                "Geoffrey Sanders",
                "Trevor Steil",
                "Roger Pearce"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10337v1",
                "http://arxiv.org/pdf/2311.10337v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC",
                "cs.SI",
                "05C90, 68R10, 68W10, 68W15, 68W40, 68T09",
                "I.5.3; E.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10335v1",
            "title": "Antimagic Labeling of Generalized Edge Corona Graphs",
            "updated": "2023-11-17T05:37:16Z",
            "published": "2023-11-17T05:37:16Z",
            "summary": "An antimagic labeling of a graph $G$ is a one-to-one correspondence between\nthe edge set $E(G)$ and $\\lbrace 1,2,...,|E(G)|\\rbrace$ in which the sum of the\nedge labels incident on the distinct vertices are distinct. Let\n$G$,$H_1$,$H_2$,...,$H_{m-1}$, and $H_m$ be simple graphs where $|E(G)|=m$. A\ngeneralized edge corona of the graph $G$ and $(H_1,H_2,...,H_m)$ (denoted by\n$G\\diamond (H_1,H_2,...H_m)$) is a graph obtained by taking a copy of\n$G,H_1,H_2,...,H_m$ and joining the end vertices of $i^{th}$ edge of $G$ to\nevery vertex of $H_i$, $i\\in\\lbrace 1,2,...,m\\rbrace$. In this paper, we\nconsider $G$ as a connected graph with exactly one vertex of maximum degree 3\n(excluding the spider graph with exactly one vertex of maximum degree 3\ncontaining uneven legs) and each $H_i$, $1\\leq i \\leq m$ as a connected graph\non at least two vertices. We provide an algorithmic approach to prove that $G$\n$\\diamond$ $(H_1,H_2,...H_m)$ is antimagic under certain conditions.",
            "author": [
                "Nivedha D",
                "Devi Yamini S"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10335v1",
                "http://arxiv.org/pdf/2311.10335v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10330v1",
            "title": "A Complete Characterization of all Magic Constants Arising from Distance\n  Magic Graphs",
            "updated": "2023-11-17T05:19:54Z",
            "published": "2023-11-17T05:19:54Z",
            "summary": "A positive integer $k$ is called a magic constant if there is a graph $G$\nalong with a bijective function $f$ from $V(G)$ to first $|V(G)|$ natural\nnumbers such that the weight of the vertex $w(v) = \\sum_{uv \\in E}f(v) =k$ for\nall $v \\in V$. It is known that all odd positive integers greater equal $3$ and\nthe integer powers of $2$, $2^{t}$, $t \\ge 6$ are magic constants. In this\npaper we characterise all positive integers which are magic constants.",
            "author": [
                "Ravindra Pawar",
                "Tarkeshwar Singh",
                "Himadri Mukherjee",
                "Jay Bagga"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10330v1",
                "http://arxiv.org/pdf/2311.10330v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C 78"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10327v1",
            "title": "Dimensionality Reduction of Dynamics on Lie Manifolds via\n  Structure-Aware Canonical Correlation Analysis",
            "updated": "2023-11-17T04:46:58Z",
            "published": "2023-11-17T04:46:58Z",
            "summary": "Incorporating prior knowledge into a data-driven modeling problem can\ndrastically improve performance, reliability, and generalization outside of the\ntraining sample. The stronger the structural properties, the more effective\nthese improvements become. Manifolds are a powerful nonlinear generalization of\nEuclidean space for modeling finite dimensions. Structural impositions in\nconstrained systems increase when applying group structure, converting them\ninto Lie manifolds. The range of their applications is very wide and includes\nthe important case of robotic tasks. Canonical Correlation Analysis (CCA) can\nconstruct a hierarchical sequence of maximal correlations of up to two paired\ndata sets in these Euclidean spaces. We present a method to generalize this\nconcept to Lie Manifolds and demonstrate its efficacy through the substantial\nimprovements it achieves in making structure-consistent predictions about\nchanges in the state of a robotic hand.",
            "author": [
                "Wooyoung Chung",
                "Daniel Polani",
                "Stas Tiomkin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10327v1",
                "http://arxiv.org/pdf/2311.10327v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10323v1",
            "title": "Solar-Energetic-Particle Track-Production Rates at 1 au: Comparing\n  In-situ Particle Fluxes with Lunar Sample-Derived Track Densities",
            "updated": "2023-11-17T04:28:14Z",
            "published": "2023-11-17T04:28:14Z",
            "summary": "Heavy (Z>26) solar energetic particles (SEPs) with energies ~1 MeV/nucleon\nare known to leave visible damage tracks in meteoritic materials. The density\nof such solar flare tracks in lunar and asteroidal samples has been used as a\nmeasure of a sample's exposure time to space, yielding critical information on\nplanetary space weathering rates, the dynamics and lifetimes of interplanetary\ndust grains, and the long-term history of solar particle fluxes. Knowledge of\nthe SEP track accumulation rate in planetary materials at 1 au is critical for\nproperly interpreting observed track densities. Here, we use in-situ particle\nobservations of the 0.50-3.0 MeV/nuc Fe-group SEP flux taken by NASA's Advanced\nComposition Explorer (ACE) to calculate a flux of track-inducing particles at 1\nau of 6.0x10^5 /cm2/yr/str. Using the observed energy spectrum of Fe-group\nSEPs, we find that the depth distribution of SEP-induced damage tracks inferred\nfrom ACE measurements matches closely to that recently measured in lunar sample\n64455; however, the magnitude of the ACE-inferred rate is approximately 25x\nhigher than that observed in the lunar sample. We discuss several hypotheses\nfor the nature of this discrepancy, including inefficiencies in track\nformation, thermal annealing of lunar samples, erosion via space weathering\nprocessing, and variations in the SEP flux at the Moon, yet find no\nsatisfactory explanation. We encourage further research on both the nature of\nSEP track formation in meteoritic materials and the flux of Fe-group SEPs at\nthe lunar surface in recent and geologic times to resolve this discrepancy.",
            "author": [
                "A. R. Poppe",
                "P. S. Szabo",
                "E. R. Imata",
                "L. P. Keller",
                "R. Christoffersen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10323v1",
                "http://arxiv.org/pdf/2311.10323v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP",
                "astro-ph.SR",
                "physics.geo-ph",
                "physics.space-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10320v1",
            "title": "Learning transformer-based heterogeneously salient graph representation\n  for multimodal fusion classification of hyperspectral image and LiDAR data",
            "updated": "2023-11-17T04:06:20Z",
            "published": "2023-11-17T04:06:20Z",
            "summary": "Data collected by different modalities can provide a wealth of complementary\ninformation, such as hyperspectral image (HSI) to offer rich spectral-spatial\nproperties, synthetic aperture radar (SAR) to provide structural information\nabout the Earth's surface, and light detection and ranging (LiDAR) to cover\naltitude information about ground elevation. Therefore, a natural idea is to\ncombine multimodal images for refined and accurate land-cover interpretation.\nAlthough many efforts have been attempted to achieve multi-source remote\nsensing image classification, there are still three issues as follows: 1)\nindiscriminate feature representation without sufficiently considering modal\nheterogeneity, 2) abundant features and complex computations associated with\nmodeling long-range dependencies, and 3) overfitting phenomenon caused by\nsparsely labeled samples. To overcome the above barriers, a transformer-based\nheterogeneously salient graph representation (THSGR) approach is proposed in\nthis paper. First, a multimodal heterogeneous graph encoder is presented to\nencode distinctively non-Euclidean structural features from heterogeneous data.\nThen, a self-attention-free multi-convolutional modulator is designed for\neffective and efficient long-term dependency modeling. Finally, a mean forward\nis put forward in order to avoid overfitting. Based on the above structures,\nthe proposed model is able to break through modal gaps to obtain differentiated\ngraph representation with competitive time cost, even for a small fraction of\ntraining samples. Experiments and analyses on three benchmark datasets with\nvarious state-of-the-art (SOTA) methods show the performance of the proposed\napproach.",
            "author": [
                "Jiaqi Yang",
                "Bo Du",
                "Liangpei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10320v1",
                "http://arxiv.org/pdf/2311.10320v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10318v1",
            "title": "Nonparametric Teaching for Multiple Learners",
            "updated": "2023-11-17T04:04:11Z",
            "published": "2023-11-17T04:04:11Z",
            "summary": "We study the problem of teaching multiple learners simultaneously in the\nnonparametric iterative teaching setting, where the teacher iteratively\nprovides examples to the learner for accelerating the acquisition of a target\nconcept. This problem is motivated by the gap between current single-learner\nteaching setting and the real-world scenario of human instruction where a\nteacher typically imparts knowledge to multiple students. Under the new problem\nformulation, we introduce a novel framework -- Multi-learner Nonparametric\nTeaching (MINT). In MINT, the teacher aims to instruct multiple learners, with\neach learner focusing on learning a scalar-valued target model. To achieve\nthis, we frame the problem as teaching a vector-valued target model and extend\nthe target model space from a scalar-valued reproducing kernel Hilbert space\nused in single-learner scenarios to a vector-valued space. Furthermore, we\ndemonstrate that MINT offers significant teaching speed-up over repeated\nsingle-learner teaching, particularly when the multiple learners can\ncommunicate with each other. Lastly, we conduct extensive experiments to\nvalidate the practicality and efficiency of MINT.",
            "author": [
                "Chen Zhang",
                "Xiaofeng Cao",
                "Weiyang Liu",
                "Ivor Tsang",
                "James Kwok"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10318v1",
                "http://arxiv.org/pdf/2311.10318v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10316v1",
            "title": "Graph Sparsifications using Neural Network Assisted Monte Carlo Tree\n  Search",
            "updated": "2023-11-17T03:59:50Z",
            "published": "2023-11-17T03:59:50Z",
            "summary": "Graph neural networks have been successful for machine learning, as well as\nfor combinatorial and graph problems such as the Subgraph Isomorphism Problem\nand the Traveling Salesman Problem. We describe an approach for computing graph\nsparsifiers by combining a graph neural network and Monte Carlo Tree Search. We\nfirst train a graph neural network that takes as input a partial solution and\nproposes a new node to be added as output. This neural network is then used in\na Monte Carlo search to compute a sparsifier. The proposed method consistently\noutperforms several standard approximation algorithms on different types of\ngraphs and often finds the optimal solution.",
            "author": [
                "Alvin Chiu",
                "Mithun Ghosh",
                "Reyan Ahmed",
                "Kwang-Sung Jun",
                "Stephen Kobourov",
                "Michael T. Goodrich"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10316v1",
                "http://arxiv.org/pdf/2311.10316v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10308v1",
            "title": "The Rainbow Connection Number of The Commuting Graph of a Finite\n  Nonabelian Group",
            "updated": "2023-11-17T03:39:24Z",
            "published": "2023-11-17T03:39:24Z",
            "summary": "A path in an edge-colored graph is called a rainbow path if every two\ndistinct edges of the path have different colors. A graph whose every pair of\nvertices are linked by a rainbow path is called a rainbow-connected graph. The\nrainbow connection number of a graph is the minimum number of colors that are\nneeded to color the edges of the graph such that the graph is\nrainbow-connected. In this paper, we determine the rainbow connection number of\nthe commuting graph of a finite nonabelian group, which is a graph whose vertex\nset is a nonabelian group of finite order and two distinct elements of the\ngroup is adjacent in the graph if they commute. We also show that the rainbow\nconnection number of the commuting graph of a finite group is related to the\nnumber of maximal abelian subgroups or the number of involutions that do not\ncommute with any other nonidentity element of the group.",
            "author": [
                "Rian Febrian Umbara",
                "A. N. M. Salman",
                "Pritta Etriana Putri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10308v1",
                "http://arxiv.org/pdf/2311.10308v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10305v1",
            "title": "Semi-supervised ViT knowledge distillation network with style transfer\n  normalization for colorectal liver metastases survival prediction",
            "updated": "2023-11-17T03:32:11Z",
            "published": "2023-11-17T03:32:11Z",
            "summary": "Colorectal liver metastases (CLM) significantly impact colon cancer patients,\ninfluencing survival based on systemic chemotherapy response. Traditional\nmethods like tumor grading scores (e.g., tumor regression grade - TRG) for\nprognosis suffer from subjectivity, time constraints, and expertise demands.\nCurrent machine learning approaches often focus on radiological data, yet the\nrelevance of histological images for survival predictions, capturing intricate\ntumor microenvironment characteristics, is gaining recognition. To address\nthese limitations, we propose an end-to-end approach for automated prognosis\nprediction using histology slides stained with H&E and HPS. We first employ a\nGenerative Adversarial Network (GAN) for slide normalization to reduce staining\nvariations and improve the overall quality of the images that are used as input\nto our prediction pipeline. We propose a semi-supervised model to perform\ntissue classification from sparse annotations, producing feature maps. We use\nan attention-based approach that weighs the importance of different slide\nregions in producing the final classification results. We exploit the extracted\nfeatures for the metastatic nodules and surrounding tissue to train a prognosis\nmodel. In parallel, we train a vision Transformer (ViT) in a knowledge\ndistillation framework to replicate and enhance the performance of the\nprognosis prediction. In our evaluation on a clinical dataset of 258 patients,\nour approach demonstrates superior performance with c-indexes of 0.804 (0.014)\nfor OS and 0.733 (0.014) for TTR. Achieving 86.9% to 90.3% accuracy in\npredicting TRG dichotomization and 78.5% to 82.1% accuracy for the 3-class TRG\nclassification task, our approach outperforms comparative methods. Our proposed\npipeline can provide automated prognosis for pathologists and oncologists, and\ncan greatly promote precision medicine progress in managing CLM patients.",
            "author": [
                "Mohamed El Amine Elforaici",
                "Emmanuel Montagnon",
                "Francisco Perdigon Romero",
                "William Trung Le",
                "Feryel Azzi",
                "Dominique Trudel",
                "Bich Nguyen",
                "Simon Turcotte",
                "An Tang",
                "Samuel Kadoury"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10305v1",
                "http://arxiv.org/pdf/2311.10305v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10290v1",
            "title": "Scalable Algorithms for Laplacian Pseudo-inverse Computation",
            "updated": "2023-11-17T02:36:44Z",
            "published": "2023-11-17T02:36:44Z",
            "summary": "The pseudo-inverse of a graph Laplacian matrix, denoted as $L^\\dagger$, finds\nextensive application in various graph analysis tasks. Notable examples include\nthe calculation of electrical closeness centrality, determination of Kemeny's\nconstant, and evaluation of resistance distance. However, existing algorithms\nfor computing $L^\\dagger$ are often computationally expensive when dealing with\nlarge graphs. To overcome this challenge, we propose novel solutions for\napproximating $L^\\dagger$ by establishing a connection with the inverse of a\nLaplacian submatrix $L_v$. This submatrix is obtained by removing the $v$-th\nrow and column from the original Laplacian matrix $L$. The key advantage of\nthis connection is that $L_v^{-1}$ exhibits various interesting combinatorial\ninterpretations. We present two innovative interpretations of $L_v^{-1}$ based\non spanning trees and loop-erased random walks, which allow us to develop\nefficient sampling algorithms. Building upon these new theoretical insights, we\npropose two novel algorithms for efficiently approximating both electrical\ncloseness centrality and Kemeny's constant. We extensively evaluate the\nperformance of our algorithms on five real-life datasets. The results\ndemonstrate that our novel approaches significantly outperform the\nstate-of-the-art methods by several orders of magnitude in terms of both\nrunning time and estimation errors for these two graph analysis tasks. To\nfurther illustrate the effectiveness of electrical closeness centrality and\nKemeny's constant, we present two case studies that showcase the practical\napplications of these metrics.",
            "author": [
                "Meihao Liao",
                "Rong-Hua Li",
                "Qiangqiang Dai",
                "Hongyang Chen",
                "Guoren Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10290v1",
                "http://arxiv.org/pdf/2311.10290v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10270v1",
            "title": "Multiscale Hodge Scattering Networks for Data Analysis",
            "updated": "2023-11-17T01:30:43Z",
            "published": "2023-11-17T01:30:43Z",
            "summary": "We propose new scattering networks for signals measured on simplicial\ncomplexes, which we call \\emph{Multiscale Hodge Scattering Networks} (MHSNs).\nOur construction is based on multiscale basis dictionaries on simplicial\ncomplexes, i.e., the $\\kappa$-GHWT and $\\kappa$-HGLET, which we recently\ndeveloped for simplices of dimension $\\kappa \\in \\N$ in a given simplicial\ncomplex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT)\nand Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\\kappa$-GHWT and\nthe $\\kk$-HGLET both form redundant sets (i.e., dictionaries) of multiscale\nbasis vectors and the corresponding expansion coefficients of a given signal.\nOur MHSNs use a layered structure analogous to a convolutional neural network\n(CNN) to cascade the moments of the modulus of the dictionary coefficients. The\nresulting features are invariant to reordering of the simplices (i.e., node\npermutation of the underlying graphs). Importantly, the use of multiscale basis\ndictionaries in our MHSNs admits a natural pooling operation that is akin to\nlocal pooling in CNNs, and which may be performed either locally or per-scale.\nThese pooling operations are harder to define in both traditional scattering\nnetworks based on Morlet wavelets, and geometric scattering networks based on\nDiffusion Wavelets. As a result, we are able to extract a rich set of\ndescriptive yet robust features that can be used along with very simple machine\nlearning methods (i.e., logistic regression or support vector machines) to\nachieve high-accuracy classification systems with far fewer parameters to train\nthan most modern graph neural networks. Finally, we demonstrate the usefulness\nof our MHSNs in three distinct types of problems: signal classification, domain\n(i.e., graph/simplex) classification, and molecular dynamics prediction.",
            "author": [
                "Naoki Saito",
                "Stefan C. Schonsheck",
                "Eugene Shvarts"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10270v1",
                "http://arxiv.org/pdf/2311.10270v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.NA",
                "cs.SI",
                "eess.SP",
                "math.NA",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10263v1",
            "title": "Stable Differentiable Causal Discovery",
            "updated": "2023-11-17T01:14:24Z",
            "published": "2023-11-17T01:14:24Z",
            "summary": "Inferring causal relationships as directed acyclic graphs (DAGs) is an\nimportant but challenging problem. Differentiable Causal Discovery (DCD) is a\npromising approach to this problem, framing the search as a continuous\noptimization. But existing DCD methods are numerically unstable, with poor\nperformance beyond tens of variables. In this paper, we propose Stable\nDifferentiable Causal Discovery (SDCD), a new method that improves previous DCD\nmethods in two ways: (1) It employs an alternative constraint for acyclicity;\nthis constraint is more stable, both theoretically and empirically, and fast to\ncompute. (2) It uses a training procedure tailored for sparse causal graphs,\nwhich are common in real-world scenarios. We first derive SDCD and prove its\nstability and correctness. We then evaluate it with both observational and\ninterventional data and on both small-scale and large-scale settings. We find\nthat SDCD outperforms existing methods in both convergence speed and accuracy\nand can scale to thousands of variables.",
            "author": [
                "Achille Nazaret",
                "Justin Hong",
                "Elham Azizi",
                "David Blei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10263v1",
                "http://arxiv.org/pdf/2311.10263v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10260v1",
            "title": "igraph enables fast and robust network analysis across programming\n  languages",
            "updated": "2023-11-17T01:05:13Z",
            "published": "2023-11-17T01:05:13Z",
            "summary": "Networks or graphs are widely used across the sciences to represent\nrelationships of many kinds. igraph (https://igraph.org) is a general-purpose\nsoftware library for graph construction, analysis, and visualisation, combining\nfast and robust performance with a low entry barrier. igraph pairs a fast core\nwritten in C with beginner-friendly interfaces in Python, R, and Mathematica.\nOver the last two decades, igraph has expanded substantially. It now scales to\nbillions of edges, supports Mathematica and interactive plotting, integrates\nwith Jupyter notebooks and other network libraries, includes new graph layouts\nand community detection algorithms, and has streamlined the documentation with\nexamples and Spanish translations. Modern testing features such as continuous\nintegration, address sanitizers, stricter typing, and memory-managed vectors\nhave also increased robustness. Hundreds of bug reports have been fixed and a\ncommunity forum has been opened to connect users and developers. Specific\neffort has been made to broaden use and community participation by women,\nnon-binary people, and other demographic groups typically underrepresented in\nopen source software.",
            "author": [
                "Michael Antonov",
                "G\u00e1bor Cs\u00e1rdi",
                "Szabolcs Horv\u00e1t",
                "Kirill M\u00fcller",
                "Tam\u00e1s Nepusz",
                "Daniel Noom",
                "Ma\u00eblle Salmon",
                "Vincent Traag",
                "Brooke Foucault Welles",
                "Fabio Zanini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10260v1",
                "http://arxiv.org/pdf/2311.10260v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "q-bio.MN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10235v1",
            "title": "Data-Driven LQR using Reinforcement Learning and Quadratic Neural\n  Networks",
            "updated": "2023-11-16T23:49:43Z",
            "published": "2023-11-16T23:49:43Z",
            "summary": "This paper introduces a novel data-driven approach to design a linear\nquadratic regulator (LQR) using a reinforcement learning (RL) algorithm that\ndoes not require a system model. The key contribution is to perform policy\niteration (PI) by designing the policy evaluator as a two-layer quadratic\nneural network (QNN). This network is trained through convex optimization. To\nthe best of our knowledge, this is the first time that a QNN trained through\nconvex optimization is employed as the Q-function approximator (QFA). The main\nadvantage is that the QNN's input-output mapping has an analytical expression\nas a quadratic form, which can then be used to obtain an analytical expression\nfor policy improvement. This is in stark contrast to the available techniques\nin the literature that must train a second neural network to obtain policy\nimprovement. The article establishes the convergence of the learning algorithm\nto the optimal control, provided the system is controllable and one starts from\na stabilitzing policy. A quadrotor example demonstrates the effectiveness of\nthe proposed approach.",
            "author": [
                "Soroush Asri",
                "Luis Rodrigues"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10235v1",
                "http://arxiv.org/pdf/2311.10235v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10217v2",
            "title": "A Language and Its Dimensions: Intrinsic Dimensions of Language Fractal\n  Structures",
            "updated": "2023-11-20T17:08:11Z",
            "published": "2023-11-16T22:15:15Z",
            "summary": "The present paper introduces a novel object of study - a language fractal\nstructure. We hypothesize that a set of embeddings of all $n$-grams of a\nnatural language constitutes a representative sample of this fractal set. (We\nuse the term Hailonakea to refer to the sum total of all language fractal\nstructures, over all $n$). The paper estimates intrinsic (genuine) dimensions\nof language fractal structures for the Russian and English languages. To this\nend, we employ methods based on (1) topological data analysis and (2) a minimum\nspanning tree of a data graph for a cloud of points considered (Steele\ntheorem). For both languages, for all $n$, the intrinsic dimensions appear to\nbe non-integer values (typical for fractal sets), close to 9 for both of the\nRussian and English language.",
            "author": [
                "Vasilii A. Gromov",
                "Nikita S. Borodin",
                "Asel S. Yerbolova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10217v2",
                "http://arxiv.org/pdf/2311.10217v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "math.AT",
                "nlin.CD"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10208v1",
            "title": "A geometric approach to apriori estimates for optimal transport maps",
            "updated": "2023-11-16T21:43:10Z",
            "published": "2023-11-16T21:43:10Z",
            "summary": "A key inequality which underpins the regularity theory of optimal transport\nfor costs satisfying the Ma--Trudinger--Wang condition is the Pogorelov second\nderivative bound. This translates to an apriori interior $C^1$ estimate for\nsmooth optimal maps. Here we give a new derivation of this estimate which\nrelies in part on Kim, McCann and Warren's observation that the graph of an\noptimal map becomes a volume maximizing spacelike submanifold when the product\nof the source and target domains is endowed with a suitable pseudo-Riemannian\ngeometry that combines both the marginal densities and the cost.",
            "author": [
                "Simon Brendle",
                "Flavien L\u00e9ger",
                "Robert J. McCann",
                "Cale Rankin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10208v1",
                "http://arxiv.org/pdf/2311.10208v1"
            ],
            "primary_category": "math.DG",
            "category": [
                "math.DG",
                "math.AP",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10204v1",
            "title": "The NFA Acceptance Hypothesis: Non-Combinatorial and Dynamic Lower\n  Bounds",
            "updated": "2023-11-16T21:30:05Z",
            "published": "2023-11-16T21:30:05Z",
            "summary": "We pose the fine-grained hardness hypothesis that the textbook algorithm for\nthe NFA Acceptance problem is optimal up to subpolynomial factors, even for\ndense NFAs and fixed alphabets.\n  We show that this barrier appears in many variations throughout the\nalgorithmic literature by introducing a framework of Colored Walk problems.\nThese yield fine-grained equivalent formulations of the NFA Acceptance problem\nas problems concerning detection of an $s$-$t$-walk with a prescribed color\nsequence in a given edge- or node-colored graph. For NFA Acceptance on sparse\nNFAs (or equivalently, Colored Walk in sparse graphs), a tight lower bound\nunder the Strong Exponential Time Hypothesis has been rediscovered several\ntimes in recent years. We show that our hardness hypothesis, which concerns\ndense NFAs, has several interesting implications:\n  - It gives a tight lower bound for Context-Free Language Reachability. This\nproves conditional optimality for the class of 2NPDA-complete problems,\nexplaining the cubic bottleneck of interprocedural program analysis.\n  - It gives a tight $(n+nm^{1/3})^{1-o(1)}$ lower bound for the Word Break\nproblem on strings of length $n$ and dictionaries of total size $m$.\n  - It implies the popular OMv hypothesis. Since the NFA acceptance problem is\na static (i.e., non-dynamic) problem, this provides a static reason for the\nhardness of many dynamic problems.\n  Thus, a proof of the NFA Acceptance hypothesis would resolve several\ninteresting barriers. Conversely, a refutation of the NFA Acceptance hypothesis\nmay lead the way to attacking the current barriers observed for Context-Free\nLanguage Reachability, the Word Break problem and the growing list of dynamic\nproblems proven hard under the OMv hypothesis.",
            "author": [
                "Karl Bringmann",
                "Allan Gr\u00f8nlund",
                "Marvin K\u00fcnnemann",
                "Kasper Green Larsen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10204v1",
                "http://arxiv.org/pdf/2311.10204v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DS",
                "cs.FL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10201v1",
            "title": "Fused Breadth-First Probabilistic Traversals on Distributed GPU Systems",
            "updated": "2023-11-16T21:17:42Z",
            "published": "2023-11-16T21:17:42Z",
            "summary": "Probabilistic breadth-first traversals (BPTs) are used in many network\nscience and graph machine learning applications. In this paper, we are\nmotivated by the application of BPTs in stochastic diffusion-based graph\nproblems such as influence maximization. These applications heavily rely on\nBPTs to implement a Monte-Carlo sampling step for their approximations. Given\nthe large sampling complexity, stochasticity of the diffusion process, and the\ninherent irregularity in real-world graph topologies, efficiently parallelizing\nthese BPTs remains significantly challenging.\n  In this paper, we present a new algorithm to fuse massive number of\nconcurrently executing BPTs with random starts on the input graph. Our\nalgorithm is designed to fuse BPTs by combining separate traversals into a\nunified frontier on distributed multi-GPU systems. To show the general\napplicability of the fused BPT technique, we have incorporated it into two\nstate-of-the-art influence maximization parallel implementations (gIM and\nRipples). Our experiments on up to 4K nodes of the OLCF Frontier supercomputer\n($32,768$ GPUs and $196$K CPU cores) show strong scaling behavior, and that\nfused BPTs can improve the performance of these implementations up to\n34$\\times$ (for gIM) and ~360$\\times$ (for Ripples).",
            "author": [
                "Reece Neff",
                "Mostafa Eghbali Zarch",
                "Marco Minutoli",
                "Mahantesh Halappanavar",
                "Antonino Tumeo",
                "Ananth Kalyanaraman",
                "Michela Becchi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10201v1",
                "http://arxiv.org/pdf/2311.10201v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10189v1",
            "title": "TAPA-CS: Enabling Scalable Accelerator Design on Distributed HBM-FPGAs",
            "updated": "2023-11-16T20:40:59Z",
            "published": "2023-11-16T20:40:59Z",
            "summary": "Despite the increasing adoption of Field-Programmable Gate Arrays (FPGAs) in\ncompute clouds, there remains a significant gap in programming tools and\nabstractions which can leverage network-connected, cloud-scale, multi-die FPGAs\nto generate accelerators with high frequency and throughput. To this end, we\npropose TAPA-CS, a task-parallel dataflow programming framework which\nautomatically partitions and compiles a large design across a cluster of FPGAs\nwith no additional user effort while achieving high frequency and throughput.\nTAPA-CS has three main contributions. First, it is an open-source framework\nwhich allows users to leverage virtually \"unlimited\" accelerator fabric,\nhigh-bandwidth memory (HBM), and on-chip memory, by abstracting away the\nunderlying hardware. This reduces the user's programming burden to a logical\none, enabling software developers and researchers with limited FPGA domain\nknowledge to deploy larger designs than possible earlier. Second, given as\ninput a large design, TAPA-CS automatically partitions the design to map to\nmultiple FPGAs, while ensuring congestion control, resource balancing, and\noverlapping of communication and computation. Third, TAPA-CS couples\ncoarse-grained floorplanning with automated interconnect pipelining at the\ninter- and intra-FPGA levels to ensure high frequency. We have tested TAPA-CS\non our multi-FPGA testbed where the FPGAs communicate through a high-speed\n100GBps Ethernet infrastructure. We have evaluated the performance and\nscalability of our tool on designs, including systolic-array based\nconvolutional neural networks (CNNs), graph processing workloads such as page\nrank, stencil applications like the Dilate kernel, and K-nearest neighbors\n(KNN). TAPA-CS has the potential to accelerate development of increasingly\ncomplex and large designs on the low power and reconfigurable FPGAs.",
            "author": [
                "Neha Prakriya",
                "Yuze Chi",
                "Suhail Basalama",
                "Linghao Song",
                "Jason Cong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10189v1",
                "http://arxiv.org/pdf/2311.10189v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10182v1",
            "title": "Can complex networks describe the urban and rural tropospheric O3\n  dynamics?",
            "updated": "2023-11-16T20:15:07Z",
            "published": "2023-11-16T20:15:07Z",
            "summary": "Tropospheric ozone (O3) time series have been converted into complex networks\nthrough the recent so-called Visibility Graph (VG), using the data from air\nquality stations located in the western part of Andalusia (Spain). The aim is\nto apply this novel method to differentiate the behavior between rural and\nurban regions when it comes to the ozone dynamics. To do so, some centrality\nparameters of the resulting complex networks have been investigated: the\ndegree, betweenness and shortest path. Some of them are expected to corroborate\nprevious works in order to support the use of this technique; while others to\nsupply new information. Results coincide when describing the difference that\ntropospheric ozone exhibits seasonally and geographically. It is seen that\nozone behavior is fractal, in accordance to previous works. Also, it has been\ndemonstrated that this methodology is able to characterize the divergence\nencountered between measurements in urban environments and countryside. In\naddition to that, the promising outcomes of this technique support the use of\ncomplex networks for the study of air pollutants dynamics. Particularly, new\nnuances are offered such as the identification and description of singularities\nin the signal.",
            "author": [
                "R. Carmona-Cabezas",
                "J. Gomez-Gomez",
                "A. B. Ariza-Villaverde",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.chemosphere.2019.05.057",
                "http://arxiv.org/abs/2311.10182v1",
                "http://arxiv.org/pdf/2311.10182v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10175v1",
            "title": "An automated procedure for the detection of the Yarkovsky effect and\n  results from the ESA NEO Coordination Centre",
            "updated": "2023-11-16T20:04:18Z",
            "published": "2023-11-16T20:04:18Z",
            "summary": "Context: The measurement of the Yarkovsky effect on near-Earth asteroids\n(NEAs) is common practice in orbit determination today, and the number of\ndetections will increase with the developments of new and more accurate\ntelescopic surveys. However, the process of finding new detections and\nidentifying spurious ones is not yet automated, and it often relies on personal\njudgment. Aims: We aim to introduce a more automated procedure that can search\nfor NEA candidates to measure the Yarkovsky effect, and that can identify\nspurious detections. Methods: The expected semi-major axis drift on an NEA\ncaused by the Yarkovsky effect was computed with a Monte Carlo method on a\nstatistical model of the physical parameters of the asteroid that relies on the\nmost recent NEA population models and data. The expected drift was used to\nselect candidates in which the Yarkovsky effect might be detected, according to\nthe current knowledge of their orbit and the length of their observational arc.\nThen, a nongravitational acceleration along the transverse direction was\nestimated through orbit determination for each candidate. If the detected\nacceleration was statistically significant, we performed a statistical test to\ndetermine whether it was compatible with the Yarkovsky effect model. Finally,\nwe determined the dependence on an isolated tracklet. Results: Among the known\nNEAs, our procedure automatically found 348 detections of the Yarkovsky effect\nthat were accepted. The results are overall compatible with the predicted trend\nwith the the inverse of the diameter, and the procedure appears to be efficient\nin identifying and rejecting spurious detections. This algorithm is now adopted\nby the ESA NEO Coordination Centre to periodically update the catalogue of NEAs\nwith a measurable Yarkovsky effect, and the results are automatically posted on\nthe web portal.",
            "author": [
                "Marco Fenucci",
                "Marco Micheli",
                "Francesco Gianotto",
                "Laura Faggioli",
                "Dario Oliviero",
                "Andrea Porru",
                "Regina Rudawska",
                "Juan Luis Cano",
                "Luca Conversi",
                "Richard Moissl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10175v1",
                "http://arxiv.org/pdf/2311.10175v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10172v1",
            "title": "Visibility graphs of ground-level ozone time series: A multifractal\n  analysis",
            "updated": "2023-11-16T20:00:38Z",
            "published": "2023-11-16T20:00:38Z",
            "summary": "A recent method based on the concurrence of complex networks and multifractal\nanalyses is applied for the first time to explore ground-level ozone behavior.\nOzone time series are converted into complex networks for their posterior\nanalysis. The searched purpose is to check the suitability of this\ntransformation and to see whether some features of these complex networks could\nconstitute a preliminary analysis before the more thorough multifractal\nformalism. Results show effectively that the exposed transformation stores the\noriginal information about the ozone dynamics and gives meaningful knowledge\nabout the time series. Based on these results, the multifractal analysis of the\ncomplex networks is performed. Looking at the physical meaning of the\nmultifractal properties (such as fractal dimensions and singularity spectrum),\na relationship between those and the degree distribution of the complex\nnetworks is found. In addition to all the promising results, this novel\nconnection between time series and complex networks can deal with both\nstationary and non-stationary time series, overcoming one of the main\nlimitations of multifractal analysis. Therefore, this technique can be regarded\nas an alternative to give supplementary information within the study of complex\nsignals.",
            "author": [
                "R. Carmona-Cabezas",
                "A. B. Ariza-Villaverde",
                "E. Gutierrez de Rave",
                "F. J. Jimenez-Hornero"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.scitotenv.2019.01.147",
                "http://arxiv.org/abs/2311.10172v1",
                "http://arxiv.org/pdf/2311.10172v1"
            ],
            "primary_category": "physics.ao-ph",
            "category": [
                "physics.ao-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10170v1",
            "title": "Improving Unimodal Inference with Multimodal Transformers",
            "updated": "2023-11-16T19:53:35Z",
            "published": "2023-11-16T19:53:35Z",
            "summary": "This paper proposes an approach for improving performance of unimodal models\nwith multimodal training. Our approach involves a multi-branch architecture\nthat incorporates unimodal models with a multimodal transformer-based branch.\nBy co-training these branches, the stronger multimodal branch can transfer its\nknowledge to the weaker unimodal branches through a multi-task objective,\nthereby improving the performance of the resulting unimodal models. We evaluate\nour approach on tasks of dynamic hand gesture recognition based on RGB and\nDepth, audiovisual emotion recognition based on speech and facial video, and\naudio-video-text based sentiment analysis. Our approach outperforms the\nconventionally trained unimodal counterparts. Interestingly, we also observe\nthat optimization of the unimodal branches improves the multimodal branch,\ncompared to a similar multimodal model trained from scratch.",
            "author": [
                "Kateryna Chumachenko",
                "Alexandros Iosifidis",
                "Moncef Gabbouj"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10170v1",
                "http://arxiv.org/pdf/2311.10170v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10165v1",
            "title": "Practical Cybersecurity Ethics: Mapping CyBOK to Ethical Concerns",
            "updated": "2023-11-16T19:44:03Z",
            "published": "2023-11-16T19:44:03Z",
            "summary": "Research into the ethics of cybersecurity is an established and growing topic\nof investigation, however the translation of this research into practice is\nlacking: there exists a small number of professional codes of ethics or codes\nof practice in cybersecurity, however these are very broad and do not offer\nmuch insight into the ethical dilemmas that can be faced while performing\nspecific cybersecurity activities. In order to address this gap, we leverage\nongoing work on the Cyber Security Body of Knowledge (CyBOK) to help elicit and\ndocument the responsibilities and ethics of the profession. Based on a\nliterature review of the ethics of cybersecurity, we use CyBOK to frame the\nexploration of ethical challenges in the cybersecurity profession through a\nseries of 15 interviews with cybersecurity experts. Our approach is qualitative\nand exploratory, aiming to answer the research question \"What ethical\nchallenges, insights, and solutions arise in different areas of\ncybersecurity?\". Our findings indicate that there are broad ethical challenges\nacross the whole of cybersecurity, but also that different areas of\ncybersecurity can face specific ethical considerations for which more detailed\nguidance can help professionals in those areas. In particular, our findings\nindicate that security decision-making is expected of all security\nprofessionals, but that this requires them to balance a complex mix of\ntechnical, objective and subjective points of view, and that resolving\nconflicts raises challenging ethical dilemmas. We conclude that more work is\nneeded to explore, map, and integrate ethical considerations into cybersecurity\npractice; the urgent need to conduct further research into the ethics of\ncybersecurity AI; and highlight the importance of this work for individuals and\nprofessional bodies who seek to develop and mature the cybersecurity profession\nin a responsible manner.",
            "author": [
                "Ivan Flechais",
                "George Chalhoub"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10165v1",
                "http://arxiv.org/pdf/2311.10165v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10156v1",
            "title": "Algebraic Topological Networks via the Persistent Local Homology Sheaf",
            "updated": "2023-11-16T19:24:20Z",
            "published": "2023-11-16T19:24:20Z",
            "summary": "In this work, we introduce a novel approach based on algebraic topology to\nenhance graph convolution and attention modules by incorporating local\ntopological properties of the data. To do so, we consider the framework of\nsheaf neural networks, which has been previously leveraged to incorporate\nadditional structure into graph neural networks' features and construct more\nexpressive, non-isotropic messages. Specifically, given an input simplicial\ncomplex (e.g. generated by the cliques of a graph or the neighbors in a point\ncloud), we construct its local homology sheaf, which assigns to each node the\nvector space of its local homology. The intermediate features of our networks\nlive in these vector spaces and we leverage the associated sheaf Laplacian to\nconstruct more complex linear messages between them. Moreover, we extend this\napproach by considering the persistent version of local homology associated\nwith a weighted simplicial complex (e.g., built from pairwise distances of\nnodes embeddings). This i) solves the problem of the lack of a natural choice\nof basis for the local homology vector spaces and ii) makes the sheaf itself\ndifferentiable, which enables our models to directly optimize the topology of\ntheir intermediate features.",
            "author": [
                "Gabriele Cesa",
                "Arash Behboodi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10156v1",
                "http://arxiv.org/pdf/2311.10156v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10149v1",
            "title": "Improving fairness for spoken language understanding in atypical speech\n  with Text-to-Speech",
            "updated": "2023-11-16T19:09:28Z",
            "published": "2023-11-16T19:09:28Z",
            "summary": "Spoken language understanding (SLU) systems often exhibit suboptimal\nperformance in processing atypical speech, typically caused by neurological\nconditions and motor impairments. Recent advancements in Text-to-Speech (TTS)\nsynthesis-based augmentation for more fair SLU have struggled to accurately\ncapture the unique vocal characteristics of atypical speakers, largely due to\ninsufficient data. To address this issue, we present a novel data augmentation\nmethod for atypical speakers by finetuning a TTS model, called Aty-TTS. Aty-TTS\nmodels speaker and atypical characteristics via knowledge transferring from a\nvoice conversion model. Then, we use the augmented data to train SLU models\nadapted to atypical speech. To train these data augmentation models and\nevaluate the resulting SLU systems, we have collected a new atypical speech\ndataset containing intent annotation. Both objective and subjective assessments\nvalidate that Aty-TTS is capable of generating high-quality atypical speech.\nFurthermore, it serves as an effective data augmentation strategy, contributing\nto more fair SLU systems that can better accommodate individuals with atypical\nspeech patterns.",
            "author": [
                "Helin Wang",
                "Venkatesh Ravichandran",
                "Milind Rao",
                "Becky Lammers",
                "Myra Sydnor",
                "Nicholas Maragakis",
                "Ankur A. Butala",
                "Jayne Zhang",
                "Lora Clawson",
                "Victoria Chovaz",
                "Laureano Moro-Velazquez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10149v1",
                "http://arxiv.org/pdf/2311.10149v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10142v1",
            "title": "Worldsheet from worldline",
            "updated": "2023-11-16T19:00:04Z",
            "published": "2023-11-16T19:00:04Z",
            "summary": "We take a step toward a \"microscopic\" derivation of gauge-string duality. In\nparticular, using mathematical techniques of Strebel differentials and discrete\nexterior calculus, we obtain a bosonic string worldsheet action for a string\nembedded in d+1 dimensional asymptotically AdS space from multi-loop Feynman\ngraphs of a quantum field theory of scalar matrices in d-dimensions in the\nlimit of diverging loop number. Our work is building on the program started by\n't Hooft in 1974, this time including the holographic dimension which we show\nto emerge from the continuum of Schwinger parameters of Feynman diagrams.",
            "author": [
                "Umut Gursoy",
                "Guim Planella Planas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10142v1",
                "http://arxiv.org/pdf/2311.10142v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10069v1",
            "title": "The fractional chromatic number of the plane is at least 4",
            "updated": "2023-11-16T18:09:12Z",
            "published": "2023-11-16T18:09:12Z",
            "summary": "We prove that the fractional chromatic number $\\chi_f(\\mathbb{R}^2)$ of the\nunit distance graph of the Euclidean plane is greater than or equal to $4$. A\nfundamental ingredient of the proof is the notion of geometric fractional\nchromatic number $\\chi_{gf}(\\mathbb{R}^2)$ introduced recently by Ambrus et al.\nFirst, we establish that $\\chi_f(\\mathbb{R}^2)=\\chi_{gf}(\\mathbb{R}^2)$ by\nexploiting the amenability of the group of Euclidean transformations in\ndimension 2. Second, we provide a specific unit distance graph $G$ on 27\nvertices such that $\\chi_{gf}(G)=4$.\n  We also provide a natural connection of $\\chi_f(\\mathbb{R}^2)$ to the maximal\nsize of independent sets in finite unit distance graphs.",
            "author": [
                "M\u00e1t\u00e9 Matolcsi",
                "Imre Z. Ruzsa",
                "D\u00e1niel Varga",
                "P\u00e1l Zs\u00e1mboki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10069v1",
                "http://arxiv.org/pdf/2311.10069v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.MG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10055v1",
            "title": "The Minimum Clique Routing Problem on Cycles",
            "updated": "2023-11-16T17:49:00Z",
            "published": "2023-11-16T17:49:00Z",
            "summary": "In the Minimum Clique Routing Problem on Cycles \\textsc{MCRPC} we are given a\ncycle together with a set of demands (weighted origin-destination pairs) and\nthe goal is to route all the pairs minimizing the maximum weighted clique of\nthe intersection graph induced by the routing. The vertices of this graph are\nthe demands with their corresponding weights and two demands are adjacent when\ntheir routes share at least one arc. In this work we are not only interested in\nthe \\textsc{MCRPC} but also in two natural subproblems. First, we consider the\nsituation where the demands are disjoint, in the sense that every two demands\ndo not share any of their corresponding ends. Second, we analyze the subproblem\nwhere the weights of the routes are all equal. We first show that the problem\nis NP-complete even in the subproblem of disjoint demands. For the case of\narbitrary weights, we exhibit a simple combinatorial 2-approximation algorithm\nand a $\\frac{3}{2}$-approximation algorithm based on rounding a solution of a\nrelaxation of an integer linear programming formulation of our problem.\nFinally, we give a Fixed Parameter Tractable algorithm for the case of uniform\nweights, whose parameter is related to the maximum degree of the intersection\ngraph induced by any routing.",
            "author": [
                "Mariana Escalante",
                "Mart\u00edn Matamala",
                "Iv\u00e1n Rapaport",
                "Paola Tolomei",
                "Luis Miguel Torres"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10055v1",
                "http://arxiv.org/pdf/2311.10055v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10051v1",
            "title": "Tabular Few-Shot Generalization Across Heterogeneous Feature Spaces",
            "updated": "2023-11-16T17:45:59Z",
            "published": "2023-11-16T17:45:59Z",
            "summary": "Despite the prevalence of tabular datasets, few-shot learning remains\nunder-explored within this domain. Existing few-shot methods are not directly\napplicable to tabular datasets due to varying column relationships, meanings,\nand permutational invariance. To address these challenges, we propose FLAT-a\nnovel approach to tabular few-shot learning, encompassing knowledge sharing\nbetween datasets with heterogeneous feature spaces. Utilizing an encoder\ninspired by Dataset2Vec, FLAT learns low-dimensional embeddings of datasets and\ntheir individual columns, which facilitate knowledge transfer and\ngeneralization to previously unseen datasets. A decoder network parametrizes\nthe predictive target network, implemented as a Graph Attention Network, to\naccommodate the heterogeneous nature of tabular datasets. Experiments on a\ndiverse collection of 118 UCI datasets demonstrate FLAT's successful\ngeneralization to new tabular datasets and a considerable improvement over the\nbaselines.",
            "author": [
                "Max Zhu",
                "Katarzyna Kobalczyk",
                "Andrija Petrovic",
                "Mladen Nikolic",
                "Mihaela van der Schaar",
                "Boris Delibasic",
                "Petro Lio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10051v1",
                "http://arxiv.org/pdf/2311.10051v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10050v1",
            "title": "Graph models for Cybersecurity -- A Survey",
            "updated": "2023-11-16T17:45:49Z",
            "published": "2023-11-16T17:45:49Z",
            "summary": "Graph models are helpful means of analyzing computer networks as well as\ncomplex system architectures for security. In this paper we evaluate the\ncurrent state of research for representing and analysing cyber-attack using\ngraph models, i.e. attack graph (AG) formalisms. We propose a taxonomy on\nattack graph formalisms, based on 70 models, which we analysed with respect to\ntheir \\textit{graph semantic}, involved agents and analysis features.\nAdditionally, we adress which formalisms allow for automatic attack graph\ngeneration from raw or processes data inputs. Our taxonomy is especially\ndesigned to help users and applied researchers identify a suitable AG model for\ntheir needs. A summary of the individual AG formalisms is provided as\nsupplementary material.",
            "author": [
                "Jasmin Wachter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10050v1",
                "http://arxiv.org/pdf/2311.10050v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10049v2",
            "title": "Inherently Interpretable Time Series Classification via Multiple\n  Instance Learning",
            "updated": "2023-11-23T14:46:49Z",
            "published": "2023-11-16T17:45:37Z",
            "summary": "Conventional Time Series Classification (TSC) methods are often black boxes\nthat obscure inherent interpretation of their decision-making processes. In\nthis work, we leverage Multiple Instance Learning (MIL) to overcome this issue,\nand propose a new framework called MILLET: Multiple Instance Learning for\nLocally Explainable Time series classification. We apply MILLET to existing\ndeep learning TSC models and show how they become inherently interpretable\nwithout compromising (and in some cases, even improving) predictive\nperformance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel\nsynthetic dataset that is specially designed to facilitate interpretability\nevaluation. On these datasets, we show MILLET produces sparse explanations\nquickly that are of higher quality than other well-known interpretability\nmethods. To the best of our knowledge, our work with MILLET, which is available\non GitHub (https://github.com/JAEarly/MILTimeSeriesClassification), is the\nfirst to develop general MIL methods for TSC and apply them to an extensive\nvariety of domains",
            "author": [
                "Joseph Early",
                "Gavin KC Cheung",
                "Kurt Cutajar",
                "Hanting Xie",
                "Jas Kandola",
                "Niall Twomey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10049v2",
                "http://arxiv.org/pdf/2311.10049v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10046v1",
            "title": "Computation of invariant densities for continued fraction algorithms",
            "updated": "2023-11-16T17:42:05Z",
            "published": "2023-11-16T17:42:05Z",
            "summary": "We introduce the notion of matrices graph, defining continued fraction\nalgorithms where the past and the future are almost independent. We provide an\nalgorithm to convert more general algorithms into matrices graphs. We present\nan algorithm that computes exact invariant densities of certain continued\nfraction algorithms, including classical ones and some of their extensions. For\nfinite extensions of the classical additive algorithm with two coordinates, we\nprovide a more precise algorithm that decides whether the invariant density is\ncomposed of rational fractions and computes it. For any finite set of quadratic\nnumbers, we construct a continued fraction algorithm whose invariant density\nare rational fractions containing the quadratic numbers.",
            "author": [
                "Paul Mercat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10046v1",
                "http://arxiv.org/pdf/2311.10046v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "28D05, 37A05, 37E99"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10043v1",
            "title": "On the potential of Carbon-Enhanced Metal-Poor stars for Galactic\n  Archaeology",
            "updated": "2023-11-16T17:39:04Z",
            "published": "2023-11-16T17:39:04Z",
            "summary": "The low-mass metal-poor stars in the Galaxy that preserve in their\natmosphere, the chemical imprints of the gas clouds from which they were formed\ncan be used as probes to get insight into the origin and evolution of elements\nin the early galaxy, early star formation and nucleosynthesis. Among the\nmetal-poor stars, a large fraction, the so-called carbon-enhanced metal-poor\n(CEMP) stars exhibits high abundance of carbon. These stars exhibit diverse\nabundance patterns, particularly for heavy elements, based on which they are\nclassified into different groups. The diversity of abundance patterns points at\ndifferent formation scenarios. Hence, accurate classification of CEMP stars and\nknowledge of their distribution is essential to understand the role and\ncontribution of each group. While CEMP-s and CEMP-r/s stars can be used to get\ninsight into binary interactions at very low metallicity, CEMP-no stars can be\nused to probe the properties of the first stars and early nucleosynthesis. To\nexploit the full potential of CEMP stars for Galactic archaeology a homogeneous\nanalysis of each class is extremely important. Our efforts towards, and\ncontributions to providing an improved classification scheme for accurate\nclassification of CEMP-s and CEMP-r/s stars and in characterizing the companion\nasymptotic giant branch (AGB) stars of CH, CEMP-no, CEMP-s and CEMP-r/s binary\nsystems are discussed. Some recent results obtained based on low- and\nhigh-resolution spectroscopic analysis of a large number of potential CH and\nCEMP star candidates are highlighted.",
            "author": [
                "Aruna Goswami",
                "J Shejeelammal",
                "Partha Pratim Goswami",
                "Meenakshi Purandardas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10043v1",
                "http://arxiv.org/pdf/2311.10043v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10040v1",
            "title": "A characterization of efficiently compilable constraint languages",
            "updated": "2023-11-16T17:37:13Z",
            "published": "2023-11-16T17:37:13Z",
            "summary": "A central task in knowledge compilation is to compile a CNF-SAT instance into\na succinct representation format that allows efficient operations such as\ntesting satisfiability, counting, or enumerating all solutions. Useful\nrepresentation formats studied in this area range from ordered binary decision\ndiagrams (OBDDs) to circuits in decomposable negation normal form (DNNFs).\n  While it is known that there exist CNF formulas that require exponential size\nrepresentations, the situation is less well studied for other types of\nconstraints than Boolean disjunctive clauses. The constraint satisfaction\nproblem (CSP) is a powerful framework that generalizes CNF-SAT by allowing\narbitrary sets of constraints over any finite domain. The main goal of our work\nis to understand for which type of constraints (also called the constraint\nlanguage) it is possible to efficiently compute representations of polynomial\nsize. We answer this question completely and prove two tight characterizations\nof efficiently compilable constraint languages, depending on whether target\nformat is structured.\n  We first identify the combinatorial property of ``strong blockwise\ndecomposability'' and show that if a constraint language has this property, we\ncan compute DNNF representations of linear size. For all other constraint\nlanguages we construct families of CSP-instances that provably require DNNFs of\nexponential size. For a subclass of ``strong uniformly blockwise decomposable''\nconstraint languages we obtain a similar dichotomy for structured DNNFs. In\nfact, strong (uniform) blockwise decomposability even allows efficient\ncompilation into multi-valued analogs of OBDDs and FBDDs, respectively. Thus,\nwe get complete characterizations for all knowledge compilation classes between\nO(B)DDs and DNNFs.",
            "author": [
                "Christoph Berkholz",
                "Stefan Mengel",
                "Hermann Wilhelm"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10040v1",
                "http://arxiv.org/pdf/2311.10040v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.CC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10036v2",
            "title": "Dynamic CBCT Imaging using Prior Model-Free Spatiotemporal Implicit\n  Neural Representation (PMF-STINR)",
            "updated": "2023-12-04T15:59:47Z",
            "published": "2023-11-16T17:34:32Z",
            "summary": "Dynamic cone-beam computed tomography (CBCT) can capture\nhigh-spatial-resolution, time-varying images for motion monitoring, patient\nsetup, and adaptive planning of radiotherapy. However, dynamic CBCT\nreconstruction is an extremely ill-posed spatiotemporal inverse problem, as\neach CBCT volume in the dynamic sequence is only captured by one or a few X-ray\nprojections. We developed a machine learning-based technique, prior-model-free\nspatiotemporal implicit neural representation (PMF-STINR), to reconstruct\ndynamic CBCTs from sequentially acquired X-ray projections. PMF-STINR employs a\njoint image reconstruction and registration approach to address the\nunder-sampling challenge. Specifically, PMF-STINR uses spatial implicit neural\nrepresentation to reconstruct a reference CBCT volume, and it applies temporal\nINR to represent the intra-scan dynamic motion with respect to the reference\nCBCT to yield dynamic CBCTs. PMF-STINR couples the temporal INR with a\nlearning-based B-spline motion model to capture time-varying deformable motion\nduring the reconstruction. Compared with previous methods, the spatial INR, the\ntemporal INR, and the B-spline model of PMF-STINR are all learned on the fly\nduring reconstruction in a one-shot fashion, without using any patient-specific\nprior knowledge or motion sorting/binning. PMF-STINR was evaluated via digital\nphantom simulations, physical phantom measurements, and a multi-institutional\npatient dataset featuring various imaging protocols (half-fan/full-fan, full\nsampling/sparse sampling, different energy and mAs settings, etc.). The results\nshowed that the one-shot learning-based PMF-STINR can accurately and robustly\nreconstruct dynamic CBCTs and capture highly irregular motion with high\ntemporal (~0.1s) resolution and sub-millimeter accuracy. It can be a promising\ntool for motion management by offering richer motion information than\ntraditional 4D-CBCTs.",
            "author": [
                "Hua-Chieh Shao",
                "Mengke Tielige",
                "Tinsu Pan",
                "You Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10036v2",
                "http://arxiv.org/pdf/2311.10036v2"
            ],
            "primary_category": "physics.med-ph",
            "category": [
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10014v1",
            "title": "On the Number of Shortest Paths in Graphs",
            "updated": "2023-11-16T16:56:30Z",
            "published": "2023-11-16T16:56:30Z",
            "summary": "It is proved that the number of shortest paths between two vertices of\ndistance $t$ in a graph with degrees bounded by $\\Delta$ is at most $2 \\cdot\n(\\frac{\\Delta}{2})^t$. This improves upon the na\\\"ive $\\Delta (\\Delta-1)\n^{t-1}$ bound.",
            "author": [
                "Itai Benjamini",
                "Elad Tzalik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10014v1",
                "http://arxiv.org/pdf/2311.10014v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.MG",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10005v1",
            "title": "Towards Flexibility and Robustness of LSM Trees",
            "updated": "2023-11-16T16:36:37Z",
            "published": "2023-11-16T16:36:37Z",
            "summary": "Log-Structured Merge trees (LSM trees) are increasingly used as part of the\nstorage engine behind several data systems, and are frequently deployed in the\ncloud. As the number of applications relying on LSM-based storage backends\nincreases, the problem of performance tuning of LSM trees receives increasing\nattention. We consider both nominal tunings - where workload and execution\nenvironment are accurately known a priori - and robust tunings - which consider\nuncertainty in the workload knowledge. This type of workload uncertainty is\ncommon in modern applications, notably in shared infrastructure environments\nlike the public cloud.\n  To address this problem, we introduce ENDURE, a new paradigm for tuning LSM\ntrees in the presence of workload uncertainty. Specifically, we focus on the\nimpact of the choice of compaction policy, size ratio, and memory allocation on\nthe overall performance. ENDURE considers a robust formulation of the\nthroughput maximization problem and recommends a tuning that offers\nnear-optimal throughput when the executed workload is not the same, instead in\na neighborhood of the expected workload. Additionally, we explore the\nrobustness of flexible LSM designs by proposing a new unified design called\nK-LSM that encompasses existing designs. We deploy our robust tuning system,\nENDURE, on a state-of-the-art key-value store, RocksDB, and demonstrate\nthroughput improvements of up to 5x in the presence of uncertainty. Our results\nindicate that the tunings obtained by ENDURE are more robust than tunings\nobtained under our expanded LSM design space. This indicates that robustness\nmay not be inherent to a design, instead, it is an outcome of a tuning process\nthat explicitly accounts for uncertainty.",
            "author": [
                "Andy Huynh",
                "Harshal A. Chaudhari",
                "Evimaria Terzi",
                "Manos Athanassoulis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10005v1",
                "http://arxiv.org/pdf/2311.10005v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09991v1",
            "title": "Market Research on IIoT Standard Compliance Monitoring Providers and\n  deriving Attributes for IIoT Compliance Monitoring",
            "updated": "2023-11-16T16:08:52Z",
            "published": "2023-11-16T16:08:52Z",
            "summary": "Adapting security architectures to common standards like IEC 62443 or ISO\n27000 in the Industrial Internet of Things (IIoT) involves complex processes\nand compliance reports. Automatic monitoring of compliance status would enhance\nthis process. Despite limited research, practical applications exist. This\npaper conducts a market study on providers implementing IEC 62443 in IIoT,\naiming to formulate a catalog of monitorable attributes aligned with the\nstandard. The study reveals challenges, such as a lack of formal separation in\nsecurity architectures, limiting visibility. Despite these challenges,\npractical implementations share commonalities, providing insights into viable\nmonitoring properties. The research serves as a crucial entry point into\ndeveloping a comprehensive catalog of monitorable attributes for IEC 62443\nstandards in IIoT.\n  Aligned with the IEC 62443 SR catalog of document 3-3, monitorable attributes\nare derived based on current research about IIoT security and Expert Knowledge.\nThe provided tables serve as an exemplary extract, not exhaustive, defining\nthree types of attributes based on their origin of creation.",
            "author": [
                "Daniel Oberhofer",
                "Markus Hornsteiner",
                "Stefan Sch\u00f6nig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09991v1",
                "http://arxiv.org/pdf/2311.09991v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09976v1",
            "title": "Revolutionizing Customer Interactions: Insights and Challenges in\n  Deploying ChatGPT and Generative Chatbots for FAQs",
            "updated": "2023-11-16T15:50:02Z",
            "published": "2023-11-16T15:50:02Z",
            "summary": "In the rapidly evolving domain of artificial intelligence, chatbots have\nemerged as a potent tool for various applications ranging from e-commerce to\nhealthcare. This research delves into the intricacies of chatbot technology,\nfrom its foundational concepts to advanced generative models like ChatGPT. We\npresent a comprehensive taxonomy of existing chatbot approaches, distinguishing\nbetween rule-based, retrieval-based, generative, and hybrid models. A specific\nemphasis is placed on ChatGPT, elucidating its merits for frequently asked\nquestions (FAQs)-based chatbots, coupled with an exploration of associated\nNatural Language Processing (NLP) techniques such as named entity recognition,\nintent classification, and sentiment analysis. The paper further delves into\nthe customization and fine-tuning of ChatGPT, its integration with knowledge\nbases, and the consequent challenges and ethical considerations that arise.\nThrough real-world applications in domains such as online shopping, healthcare,\nand education, we underscore the transformative potential of chatbots. However,\nwe also spotlight open challenges and suggest future research directions,\nemphasizing the need for optimizing conversational flow, advancing dialogue\nmechanics, improving domain adaptability, and enhancing ethical considerations.\nThe research culminates in a call for further exploration in ensuring\ntransparent, ethical, and user-centric chatbot systems.",
            "author": [
                "Feriel Khennouche",
                "Youssef Elmir",
                "Nabil Djebari",
                "Yassine Himeur",
                "Abbes Amira"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09976v1",
                "http://arxiv.org/pdf/2311.09976v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09958v1",
            "title": "VertDetect: Fully End-to-End 3D Vertebral Instance Segmentation Model",
            "updated": "2023-11-16T15:29:21Z",
            "published": "2023-11-16T15:29:21Z",
            "summary": "Vertebral detection and segmentation are critical steps for treatment\nplanning in spine surgery and radiation therapy. Accurate identification and\nsegmentation are complicated in imaging that does not include the full spine,\nin cases with variations in anatomy (T13 and/or L6 vertebrae), and in the\npresence of fracture or hardware. This paper proposes VertDetect, a fully\nautomated end-to-end 3D vertebral instance segmentation Convolutional Neural\nNetwork (CNN) model to predict vertebral level labels and segmentations for all\nvertebrae present in a CT scan. The utilization of a shared CNN backbone\nprovides the detection and segmentation branches of the network with feature\nmaps containing both spinal and vertebral level information. A Graph\nConvolutional Network (GCN) layer is used to improve vertebral labelling by\nusing the known structure of the spine. This model achieved a Dice Similarity\nCoefficient (DSC) of 0.883 (95% CI, 0.843-0.906) and 0.882 (95% CI,\n0.835-0.909) in the VerSe 2019 and 0.868 (95\\% CI, 0.834-0.890) and 0.869 (95\\%\nCI, 0.832-0.891) in the VerSe 2020 public and hidden test sets, respectively.\nThis model achieved state-of-the-art performance for an end-to-end\narchitecture, whose design facilitates the extraction of features that can be\nsubsequently used for downstream tasks.",
            "author": [
                "Geoff Klein",
                "Michael Hardisty",
                "Cari Whyne",
                "Anne L. Martel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09958v1",
                "http://arxiv.org/pdf/2311.09958v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09942v1",
            "title": "Harnessing Transformers: A Leap Forward in Lung Cancer Image Detection",
            "updated": "2023-11-16T14:50:42Z",
            "published": "2023-11-16T14:50:42Z",
            "summary": "This paper discusses the role of Transfer Learning (TL) and transformers in\ncancer detection based on image analysis. With the enormous evolution of cancer\npatients, the identification of cancer cells in a patient's body has emerged as\na trend in the field of Artificial Intelligence (AI). This process involves\nanalyzing medical images, such as Computed Tomography (CT) scans and Magnetic\nResonance Imaging (MRIs), to identify abnormal growths that may help in cancer\ndetection. Many techniques and methods have been realized to improve the\nquality and performance of cancer classification and detection, such as TL,\nwhich allows the transfer of knowledge from one task to another with the same\ntask or domain. TL englobes many methods, particularly those used in image\nanalysis, such as transformers and Convolutional Neural Network (CNN) models\ntrained on the ImageNet dataset. This paper analyzes and criticizes each method\nof TL based on image analysis and compares the results of each method, showing\nthat transformers have achieved the best results with an accuracy of 97.41% for\ncolon cancer detection and 94.71% for Histopathological Lung cancer. Future\ndirections for cancer detection based on image analysis are also discussed.",
            "author": [
                "Amine Bechar",
                "Youssef Elmir",
                "Rafik Medjoudj",
                "Yassine Himeur",
                "Abbes Amira"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09942v1",
                "http://arxiv.org/pdf/2311.09942v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09941v2",
            "title": "Ghost Value Augmentation for $k$-ECSS and $k$-ECSM",
            "updated": "2023-11-19T14:25:39Z",
            "published": "2023-11-16T14:45:02Z",
            "summary": "We give a poly-time algorithm for the $k$-edge-connected spanning subgraph\n($k$-ECSS) problem that returns a solution of cost no greater than the cheapest\n$(k+10)$-ECSS on the same graph. Our approach enhances the iterative relaxation\nframework with a new ingredient, which we call ghost values, that allows for\nhigh sparsity in intermediate problems.\n  Our guarantees improve upon the best-known approximation factor of $2$ for\n$k$-ECSS whenever the optimal value of $(k+10)$-ECSS is close to that of\n$k$-ECSS. This is a property that holds for the closely related problem\n$k$-edge-connected spanning multi-subgraph ($k$-ECSM), which is identical to\n$k$-ECSS except edges can be selected multiple times at the same cost. As a\nconsequence, we obtain a\n$\\left(1+O\\left(\\frac{1}{k}\\right)\\right)$-approximation algorithm for\n$k$-ECSM, which resolves a conjecture of Pritchard and improves upon a recent\n$\\left(1+O\\left(\\frac{1}{\\sqrt{k}}\\right)\\right)$-approximation algorithm of\nKarlin, Klein, Oveis Gharan, and Zhang. Moreover, we present a matching lower\nbound for $k$-ECSM, showing that our approximation ratio is tight up to the\nconstant factor in $O\\left(\\frac{1}{k}\\right)$, unless $P=NP$.",
            "author": [
                "D Ellis Hershkowitz",
                "Nathan Klein",
                "Rico Zenklusen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09941v2",
                "http://arxiv.org/pdf/2311.09941v2"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09940v1",
            "title": "On the Weisfeiler algorithm of depth-$1$ stabilization",
            "updated": "2023-11-16T14:44:09Z",
            "published": "2023-11-16T14:44:09Z",
            "summary": "An origin of the multidimensional Weisfeiler-Leman algorithm goes back to a\nrefinement procedure of deep stabilization, introduced by B. Weisfeiler in a\npaper included in the collective monograph ``On construction and identification\nof graphs\"(1976). This procedure is recursive and the recursion starts from an\nalgorithm of depth-$1$ stabilization, which has never been discussed in the\nliterature. A goal of the present paper is to show that a simplified algorithm\nof the depth-$1$ stabilization has the same power as the $3$-dimensional\nWeisfeiler-Leman algorithm. It is proved that the class of coherent\nconfigurations obtained at the output of this simplified algorithm coincides\nwith the class introduced earlier by the third author. As an application we\nalso prove that if there exist at least two nonisomorphic projective planes of\norder $q$, then the Weisfeiler-Leman dimension of the incidence graph of any\nprojective plane of order $q$ is at least $4$.",
            "author": [
                "Gang Chen",
                "Qing Ren",
                "Ilia Ponomarenko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09940v1",
                "http://arxiv.org/pdf/2311.09940v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05E16"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09934v1",
            "title": "Echo Chambers within the Russo-Ukrainian War: The Role of Bipartisan\n  Users",
            "updated": "2023-11-16T14:39:37Z",
            "published": "2023-11-16T14:39:37Z",
            "summary": "The ongoing Russia-Ukraine war has been extensively discussed on social\nmedia. One commonly observed problem in such discussions is the emergence of\necho chambers, where users are rarely exposed to opinions outside their\nworldview. Prior literature on this topic has assumed that such users hold a\nsingle consistent view. However, recent work has revealed that complex topics\n(such as the war) often trigger bipartisanship among certain people. With this\nin mind, we study the presence of echo chambers on Twitter related to the\nRusso-Ukrainian war. We measure their presence and identify an important subset\nof bipartisan users who vary their opinions during the invasion. We explore the\nrole they play in the communications graph and identify features that\ndistinguish them from remaining users. We conclude by discussing their\nimportance and how they can improve the quality of discourse surrounding the\nwar.",
            "author": [
                "Peixian Zhang",
                "Ehsan-Ul Haq",
                "Yiming Zhu",
                "Pan Hui",
                "Gareth Tyson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09934v1",
                "http://arxiv.org/pdf/2311.09934v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09923v1",
            "title": "Branch-and-Price for the Stochastic TSP with Generalized Latency",
            "updated": "2023-11-16T14:22:05Z",
            "published": "2023-11-16T14:22:05Z",
            "summary": "Motivated by the tactical planning level of demand adaptive public\ntransportation systems, we present the stochastic symmetric traveling salesman\nproblem with generalized latency (STSP-GL), a stochastic extension to the\nsymmetric traveling salesman problem with generalized latency (TSP-GL). The\nSTSP-GL aims to choose a subset of nodes of an undirected graph and determines\na Hamiltonian tour amongst those nodes, minimizing an objective function that\nis a weighted combination of route design and passenger routing costs. These\nnodes are selected to ensure that a predefined percentage of uncertain\npassenger demand is served with a given probability. We formulate the STSP-GL\nas a stochastic program and propose a branch-and-price algorithm for solving\nits deterministic equivalent. We also develop a local search approach with\nwhich we improve the performance of the B&P approach. We assess the efficiency\nof the proposed methods on a set of instances from the literature. We\ndemonstrate that the proposed methods outperform a known benchmark, improving\nupper bounds by up to 85% and lower bounds by up to 55%. Finally, we show that\nsolutions of the stochastic model are both more cost-effective and robust than\nthose of the deterministic model.",
            "author": [
                "Benedikt Lienkamp",
                "Mike Hewitt",
                "Maximilian Schiffer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09923v1",
                "http://arxiv.org/pdf/2311.09923v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14713v2",
            "title": "The Rise of the AI Co-Pilot: Lessons for Design from Aviation and Beyond",
            "updated": "2023-11-29T16:52:34Z",
            "published": "2023-11-16T13:58:15Z",
            "summary": "The fast pace of advances in AI promises to revolutionize various aspects of\nknowledge work, extending its influence to daily life and professional fields\nalike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,\nworking under human guidance rather than as a mere tool. Drawing from relevant\nresearch and literature in the disciplines of Human-Computer Interaction and\nHuman Factors Engineering, we highlight the criticality of maintaining human\noversight in AI interactions. Reflecting on lessons from aviation, we address\nthe dangers of over-relying on automation, such as diminished human vigilance\nand skill erosion. Our paper proposes a design approach that emphasizes active\nhuman engagement, control, and skill enhancement in the AI partnership, aiming\nto foster a harmonious, effective, and empowering human-AI relationship. We\nparticularly call out the critical need to design AI interaction capabilities\nand software applications to enable and celebrate the primacy of human agency.\nThis calls for designs for human-AI partnership that cede ultimate control and\nresponsibility to the human user as pilot, with the AI co-pilot acting in a\nwell-defined supporting role.",
            "author": [
                "Abigail Sellen",
                "Eric Horvitz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14713v2",
                "http://arxiv.org/pdf/2311.14713v2"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09904v1",
            "title": "Capacitated Network Bargaining Games: Stability and Structure",
            "updated": "2023-11-16T13:56:14Z",
            "published": "2023-11-16T13:56:14Z",
            "summary": "Capacitated network bargaining games are popular combinatorial games that\ninvolve the structure of matchings in graphs. We show that it is always\npossible to stabilize unweighted instances of this problem (that is, ensure\nthat they admit a stable outcome) via capacity-reduction and edge-removal\noperations, without decreasing the total value that the players can get.\n  Furthermore, for general weighted instances, we show that computing a minimum\namount of vertex-capacity to reduce to make an instance stable is a\npolynomial-time solvable problem. We then exploit this to give approximation\nresults for the NP-hard problem of stabilizing a graph via edge-removal\noperations.\n  Our work extends and generalizes previous results in the literature that\ndealt with an uncapacitated version of the problem, using several new\narguments. In particular, while previous results mainly used combinatorial\ntechniques, we here rely on polyhedral arguments and, more specifically, on the\nnotion of circuits of a polytope.",
            "author": [
                "Laura Sanit\u00e0",
                "Lucy Verberk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09904v1",
                "http://arxiv.org/pdf/2311.09904v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "math.CO",
                "05C57",
                "G.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09891v1",
            "title": "On some elusive aspects of databases hindering AI based discovery: A\n  case study on superconducting materials",
            "updated": "2023-11-16T13:38:00Z",
            "published": "2023-11-16T13:38:00Z",
            "summary": "It stands to reason that the amount and the quality of big data is of key\nimportance for setting up accurate AI-driven models. Nonetheless, we believe\nthere are still critical roadblocks in the inherent generation of databases,\nthat are often underestimated and poorly discussed in the literature. In our\nview, such issues can seriously hinder the AI-based discovery process, even\nwhen high quality, sufficiently large and highly reputable data sources are\navailable. Here, considering superconducting and thermoelectric materials as\ntwo representative case studies, we specifically discuss three aspects, namely\nintrinsically biased sample selection, possible hidden variables, disparate\ndata age. Importantly, to our knowledge, we suggest and test a first strategy\ncapable of detecting and quantifying the presence of the intrinsic data bias.",
            "author": [
                "Giovanni Trezza",
                "Eliodoro Chiavazzo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09891v1",
                "http://arxiv.org/pdf/2311.09891v1"
            ],
            "primary_category": "cond-mat.other",
            "category": [
                "cond-mat.other",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09889v2",
            "title": "Language Generation from Human Brain Activities",
            "updated": "2023-11-19T15:23:17Z",
            "published": "2023-11-16T13:37:21Z",
            "summary": "Generating human language through non-invasive brain-computer interfaces\n(BCIs) has the potential to unlock many applications, such as serving disabled\npatients and improving communication. Currently, however, generating language\nvia BCIs has been previously successful only within a classification setup for\nselecting pre-generated sentence continuation candidates with the most likely\ncortical semantic representation. Inspired by recent research that revealed\nassociations between the brain and the large computational language models, we\npropose a generative language BCI that utilizes the capacity of a large\nlanguage model (LLM) jointly with a semantic brain decoder to directly generate\nlanguage from functional magnetic resonance imaging (fMRI) input. The proposed\nmodel can generate coherent language sequences aligned with the semantic\ncontent of visual or auditory language stimuli perceived, without prior\nknowledge of any pre-generated candidates. We compare the language generated\nfrom the presented model with a random control, pre-generated language\nselection approach, and a standard LLM, which generates common coherent text\nsolely based on the next word likelihood according to statistical language\ntraining data. The proposed model is found to generate language that is more\naligned with semantic stimulus in response to which brain input is sampled. Our\nfindings demonstrate the potential and feasibility of employing BCIs in direct\nlanguage generation.",
            "author": [
                "Ziyi Ye",
                "Qingyao Ai",
                "Yiqun Liu",
                "Min Zhang",
                "Christina Lioma",
                "Tuukka Ruotsalo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09889v2",
                "http://arxiv.org/pdf/2311.09889v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09872v1",
            "title": "A matroidal perspective on the tropical Prym variety",
            "updated": "2023-11-16T13:02:17Z",
            "published": "2023-11-16T13:02:17Z",
            "summary": "We associate a matroid $M(\\widetilde{\\Gamma}/\\Gamma)$ to a harmonic double\ncover $\\pi:\\widetilde{\\Gamma}\\to \\Gamma$ of metric graphs. The matroid\n$M(\\widetilde{\\Gamma}/\\Gamma)$ is a geometric interpretation of Zaslavsky's\nsigned graphic matroid. We show that the principalization\n$\\mathrm{Prym}_p(\\widetilde{\\Gamma}/\\Gamma)$ of the tropical Prym variety of\nthe double cover can be reconstructed from $M(\\widetilde{\\Gamma}/\\Gamma)$,\nequipped with certain additional decorations. We describe the simplification of\nthe matroid $M(\\widetilde{\\Gamma}/\\Gamma)$ and show that the Prym variety does\nnot change under simplification.",
            "author": [
                "Felix R\u00f6hrle",
                "Dmitry Zakharov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09872v1",
                "http://arxiv.org/pdf/2311.09872v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AG",
                "14H40 (primary), 14T20, 05B35, 05C22 (secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09862v1",
            "title": "Which Modality should I use -- Text, Motif, or Image? : Understanding\n  Graphs with Large Language Models",
            "updated": "2023-11-16T12:45:41Z",
            "published": "2023-11-16T12:45:41Z",
            "summary": "Large language models (LLMs) are revolutionizing various fields by leveraging\nlarge text corpora for context-aware intelligence. Due to the context size,\nhowever, encoding an entire graph with LLMs is fundamentally limited. This\npaper explores how to better integrate graph data with LLMs and presents a\nnovel approach using various encoding modalities (e.g., text, image, and motif)\nand approximation of global connectivity of a graph using different prompting\nmethods to enhance LLMs' effectiveness in handling complex graph structures.\nThe study also introduces GraphTMI, a new benchmark for evaluating LLMs in\ngraph structure analysis, focusing on factors such as homophily, motif\npresence, and graph difficulty. Key findings reveal that image modality,\nsupported by advanced vision-language models like GPT-4V, is more effective\nthan text in managing token limits while retaining critical information. The\nresearch also examines the influence of different factors on each encoding\nmodality's performance. This study highlights the current limitations and\ncharts future directions for LLMs in graph understanding and reasoning tasks.",
            "author": [
                "Debarati Das",
                "Ishaan Gupta",
                "Jaideep Srivastava",
                "Dongyeop Kang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09862v1",
                "http://arxiv.org/pdf/2311.09862v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09861v2",
            "title": "PsyBench: a balanced and in-depth Psychological Chinese Evaluation\n  Benchmark for Foundation Models",
            "updated": "2023-11-17T03:17:05Z",
            "published": "2023-11-16T12:43:18Z",
            "summary": "As Large Language Models (LLMs) are becoming prevalent in various fields,\nthere is an urgent need for improved NLP benchmarks that encompass all the\nnecessary knowledge of individual discipline. Many contemporary benchmarks for\nfoundational models emphasize a broad range of subjects but often fall short in\npresenting all the critical subjects and encompassing necessary professional\nknowledge of them. This shortfall has led to skewed results, given that LLMs\nexhibit varying performance across different subjects and knowledge areas. To\naddress this issue, we present psybench, the first comprehensive Chinese\nevaluation suite that covers all the necessary knowledge required for graduate\nentrance exams. psybench offers a deep evaluation of a model's strengths and\nweaknesses in psychology through multiple-choice questions. Our findings show\nsignificant differences in performance across different sections of a subject,\nhighlighting the risk of skewed results when the knowledge in test sets is not\nbalanced. Notably, only the ChatGPT model reaches an average accuracy above\n$70\\%$, indicating that there is still plenty of room for improvement. We\nexpect that psybench will help to conduct thorough evaluations of base models'\nstrengths and weaknesses and assist in practical application in the field of\npsychology.",
            "author": [
                "Junlei Zhang",
                "Hongliang He",
                "Nirui Song",
                "Shuyuan He",
                "Shuai Zhang",
                "Huachuan Qiu",
                "Anqi Li",
                "Lizhi Ma",
                "Zhenzhong Lan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09861v2",
                "http://arxiv.org/pdf/2311.09861v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09855v1",
            "title": "Scoring Anomalous Vertices Through Quantum Walks",
            "updated": "2023-11-16T12:32:13Z",
            "published": "2023-11-16T12:32:13Z",
            "summary": "With the explosion of data over the past decades there has been a respective\nexplosion of techniques to extract information from the data from labeled data,\nquasi-labeled data, and data with no labels known a priori. For data with at\nbest quasi-labels, graphs are a natural structure to connect points to further\nextract information. In particular, anomaly detection in graphs is a method to\ndetermine which data points do not posses the latent characteristics of the\nother data. There have been a variety of classical methods to score vertices on\ntheir anomalous level with respect to the graph, spanning straightforward\nmethods of checking the local topology of a node to intricate neural networks.\nLeveraging the structure of the graph, we propose a first ever quantum-based\ntechnique to calculate the anomaly score of each node by continuously\ntraversing the graph in a particular manner. The proposed algorithm\nincorporates well-known characteristics of quantum random walks, and an\nadjustment to the algorithm is given to mitigate the increasing depth of the\ncircuit. This algorithm is rigorously shown to converge to the expected\nprobability, with respect to the initial condition.",
            "author": [
                "Andrew Vlasic",
                "Anh Pham"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09855v1",
                "http://arxiv.org/pdf/2311.09855v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09841v1",
            "title": "Leveraging LLMs in Scholarly Knowledge Graph Question Answering",
            "updated": "2023-11-16T12:13:49Z",
            "published": "2023-11-16T12:13:49Z",
            "summary": "This paper presents a scholarly Knowledge Graph Question Answering (KGQA)\nthat answers bibliographic natural language questions by leveraging a large\nlanguage model (LLM) in a few-shot manner. The model initially identifies the\ntop-n similar training questions related to a given test question via a\nBERT-based sentence encoder and retrieves their corresponding SPARQL. Using the\ntop-n similar question-SPARQL pairs as an example and the test question creates\na prompt. Then pass the prompt to the LLM and generate a SPARQL. Finally, runs\nthe SPARQL against the underlying KG - ORKG (Open Research KG) endpoint and\nreturns an answer. Our system achieves an F1 score of 99.0%, on SciQA - one of\nthe Scholarly-QALD-23 challenge benchmarks.",
            "author": [
                "Tilahun Abedissa Taffa",
                "Ricardo Usbeck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09841v1",
                "http://arxiv.org/pdf/2311.09841v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09836v1",
            "title": "PELMS: Pre-training for Effective Low-Shot Multi-Document Summarization",
            "updated": "2023-11-16T12:05:23Z",
            "published": "2023-11-16T12:05:23Z",
            "summary": "We investigate pre-training techniques for abstractive multi-document\nsummarization (MDS), which is much less studied than summarizing single\ndocuments. Though recent work has demonstrated the effectiveness of\nhighlighting information salience for pre-training strategy design, it\nstruggles to generate abstractive and reflective summaries, which are critical\nproperties for MDS. To this end, we present PELMS, a pre-trained model that\nuses objectives based on semantic coherence heuristics and faithfulness\nconstraints with un-labeled multi-document inputs, to promote the generation of\nconcise, fluent, and faithful summaries. To support the training of PELMS, we\ncompile MultiPT, a multi-document pre-training corpus containing over 93\nmillion documents to form more than 3 million unlabeled topic-centric document\nclusters, covering diverse genres such as product reviews, news, and general\nknowledge. We perform extensive evaluation of PELMS in low-shot settings on a\nwide range of MDS datasets. Our approach consistently outperforms competitive\ncomparisons with respect to overall informativeness, abstractiveness,\ncoherence, and faithfulness.",
            "author": [
                "Joseph J. Peper",
                "Wenzhao Qiu",
                "Lu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09836v1",
                "http://arxiv.org/pdf/2311.09836v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09832v1",
            "title": "X-Mark: Towards Lossless Watermarking Through Lexical Redundancy",
            "updated": "2023-11-16T11:58:31Z",
            "published": "2023-11-16T11:58:31Z",
            "summary": "Text watermarking has emerged as an important technique for detecting\nmachine-generated text. However, existing methods can severely degrade text\nquality due to arbitrary vocabulary partitioning, which disrupts the language\nmodel's expressiveness and impedes textual coherence. To mitigate this, we\nintroduce XMark, a novel approach that capitalizes on text redundancy within\nthe lexical space. Specifically, XMark incorporates a mutually exclusive rule\nfor synonyms during the language model decoding process, thereby integrating\nprior knowledge into vocabulary partitioning and preserving the capabilities of\nlanguage generation. We present theoretical analyses and empirical evidence\ndemonstrating that XMark substantially enhances text generation fluency while\nmaintaining watermark detectability. Furthermore, we investigate watermarking's\nimpact on the emergent abilities of large language models, including zero-shot\nand few-shot knowledge recall, logical reasoning, and instruction following.\nOur comprehensive experiments confirm that XMark consistently outperforms\nexisting methods in retaining these crucial capabilities of LLMs.",
            "author": [
                "Liang Chen",
                "Yatao Bian",
                "Yang Deng",
                "Shuaiyi Li",
                "Bingzhe Wu",
                "Peilin Zhao",
                "Kam-fai Wong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09832v1",
                "http://arxiv.org/pdf/2311.09832v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09827v1",
            "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded\n  Logical Thinking",
            "updated": "2023-11-16T11:52:22Z",
            "published": "2023-11-16T11:52:22Z",
            "summary": "While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.",
            "author": [
                "Nan Xu",
                "Fei Wang",
                "Ben Zhou",
                "Bang Zheng Li",
                "Chaowei Xiao",
                "Muhao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09827v1",
                "http://arxiv.org/pdf/2311.09827v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09825v1",
            "title": "Human Still Wins over LLM: An Empirical Study of Active Learning on\n  Domain-Specific Annotation Tasks",
            "updated": "2023-11-16T11:51:13Z",
            "published": "2023-11-16T11:51:13Z",
            "summary": "Large Language Models (LLMs) have demonstrated considerable advances, and\nseveral claims have been made about their exceeding human performance. However,\nin real-world tasks, domain knowledge is often required. Low-resource learning\nmethods like Active Learning (AL) have been proposed to tackle the cost of\ndomain expert annotation, raising this question: Can LLMs surpass compact\nmodels trained with expert annotations in domain-specific tasks? In this work,\nwe conduct an empirical experiment on four datasets from three different\ndomains comparing SOTA LLMs with small models trained on expert annotations\nwith AL. We found that small models can outperform GPT-3.5 with a few hundreds\nof labeled data, and they achieve higher or similar performance with GPT-4\ndespite that they are hundreds time smaller. Based on these findings, we posit\nthat LLM predictions can be used as a warmup method in real-world applications\nand human experts remain indispensable in tasks involving data annotation\ndriven by domain-specific knowledge.",
            "author": [
                "Yuxuan Lu",
                "Bingsheng Yao",
                "Shao Zhang",
                "Yun Wang",
                "Peng Zhang",
                "Tun Lu",
                "Toby Jia-Jun Li",
                "Dakuo Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09825v1",
                "http://arxiv.org/pdf/2311.09825v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09821v1",
            "title": "Towards Robust Temporal Reasoning of Large Language Models via a\n  Multi-Hop QA Dataset and Pseudo-Instruction Tuning",
            "updated": "2023-11-16T11:49:29Z",
            "published": "2023-11-16T11:49:29Z",
            "summary": "Knowledge in the real world is being updated constantly. However, it is\ncostly to frequently update large language models (LLMs). Therefore, it is\ncrucial for LLMs to understand the concept of temporal knowledge. However,\nprior works on temporal question answering did not emphasize multi-answer and\nmulti-hop types of temporal reasoning. In this paper, we propose a complex\ntemporal question-answering (QA) dataset Complex-TR that focuses on\nmulti-answer and multi-hop temporal reasoning. Besides, we also propose a novel\ndata augmentation strategy to improve the complex temporal reasoning capability\nand robustness of LLMs. We conducted experiments on multiple temporal QA\ndatasets. Experimental results show that our method is able to improve LLMs'\nperformance on temporal QA benchmarks by significant margins.",
            "author": [
                "Qingyu Tan",
                "Hwee Tou Ng",
                "Lidong Bing"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09821v1",
                "http://arxiv.org/pdf/2311.09821v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09818v1",
            "title": "SUQL: Conversational Search over Structured and Unstructured Data with\n  Large Language Models",
            "updated": "2023-11-16T11:48:17Z",
            "published": "2023-11-16T11:48:17Z",
            "summary": "Many knowledge sources consist of both structured information such as\nrelational databases as well as unstructured free text. Building a\nconversational interface to such data sources is challenging.\n  This paper introduces SUQL, Structured and Unstructured Query Language, the\nfirst formal executable representation that naturally covers compositions of\nstructured and unstructured data queries. Specifically, it augments SQL with\nseveral free-text primitives to form a precise, succinct, and expressive\nrepresentation. This paper also presents a conversational search agent based on\nlarge language models, including a few-shot contextual semantic parser for\nSUQL.\n  To validate our approach, we introduce a dataset consisting of crowdsourced\nquestions and conversations about real restaurants. Over 51% of the questions\nin the dataset require both structured and unstructured data, suggesting that\nit is a common phenomenon. We show that our few-shot conversational agent based\non SUQL finds an entity satisfying all user requirements 89.3% of the time,\ncompared to just 65.0% for a strong and commonly used baseline.",
            "author": [
                "Shicheng Liu",
                "Jialiang Xu",
                "Wesley Tjangnaka",
                "Sina J. Semnani",
                "Chen Jie Yu",
                "Gui D\u00e1vid",
                "Monica S. Lam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09818v1",
                "http://arxiv.org/pdf/2311.09818v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10537v1",
            "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical\n  Reasoning",
            "updated": "2023-11-16T11:47:58Z",
            "published": "2023-11-16T11:47:58Z",
            "summary": "Large Language Models (LLMs), despite their remarkable progress across\nvarious general domains, encounter significant barriers in medicine and\nhealthcare. This field faces unique challenges such as domain-specific\nterminologies and the reasoning over specialized knowledge. To address these\nobstinate issues, we propose a novel Multi-disciplinary Collaboration (MC)\nframework for the medical domain that leverages role-playing LLM-based agents\nwho participate in a collaborative multi-round discussion, thereby enhancing\nLLM proficiency and reasoning capabilities. This training-free and\ninterpretable framework encompasses five critical steps: gathering domain\nexperts, proposing individual analyses, summarising these analyses into a\nreport, iterating over discussions until a consensus is reached, and ultimately\nmaking a decision. Our work particularly focuses on the zero-shot scenario, our\nresults on nine data sets (MedQA, MedMCQA, PubMedQA, and six subtasks from\nMMLU) establish that our proposed MC framework excels at mining and harnessing\nthe medical expertise in LLMs, as well as extending its reasoning abilities.\nBased on these outcomes, we further conduct a human evaluation to pinpoint and\ncategorize common errors within our method, as well as ablation studies aimed\nat understanding the impact of various factors on overall performance. Our code\ncan be found at \\url{https://github.com/gersteinlab/MedAgents}.",
            "author": [
                "Xiangru Tang",
                "Anni Zou",
                "Zhuosheng Zhang",
                "Yilun Zhao",
                "Xingyao Zhang",
                "Arman Cohan",
                "Mark Gerstein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10537v1",
                "http://arxiv.org/pdf/2311.10537v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10123v1",
            "title": "MetaDreamer: Efficient Text-to-3D Creation With Disentangling Geometry\n  and Texture",
            "updated": "2023-11-16T11:35:10Z",
            "published": "2023-11-16T11:35:10Z",
            "summary": "Generative models for 3D object synthesis have seen significant advancements\nwith the incorporation of prior knowledge distilled from 2D diffusion models.\nNevertheless, challenges persist in the form of multi-view geometric\ninconsistencies and slow generation speeds within the existing 3D synthesis\nframeworks. This can be attributed to two factors: firstly, the deficiency of\nabundant geometric a priori knowledge in optimization, and secondly, the\nentanglement issue between geometry and texture in conventional 3D generation\nmethods.In response, we introduce MetaDreammer, a two-stage optimization\napproach that leverages rich 2D and 3D prior knowledge. In the first stage, our\nemphasis is on optimizing the geometric representation to ensure multi-view\nconsistency and accuracy of 3D objects. In the second stage, we concentrate on\nfine-tuning the geometry and optimizing the texture, thereby achieving a more\nrefined 3D object. Through leveraging 2D and 3D prior knowledge in two stages,\nrespectively, we effectively mitigate the interdependence between geometry and\ntexture. MetaDreamer establishes clear optimization objectives for each stage,\nresulting in significant time savings in the 3D generation process. Ultimately,\nMetaDreamer can generate high-quality 3D objects based on textual prompts\nwithin 20 minutes, and to the best of our knowledge, it is the most efficient\ntext-to-3D generation method. Furthermore, we introduce image control into the\nprocess, enhancing the controllability of 3D generation. Extensive empirical\nevidence confirms that our method is not only highly efficient but also\nachieves a quality level that is at the forefront of current state-of-the-art\n3D generation techniques.",
            "author": [
                "Lincong Feng",
                "Muyu Wang",
                "Maoyu Wang",
                "Kuo Xu",
                "Xiaoli Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10123v1",
                "http://arxiv.org/pdf/2311.10123v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09809v1",
            "title": "Comparing Differentiable Logics for Learning Systems: A Research Preview",
            "updated": "2023-11-16T11:33:08Z",
            "published": "2023-11-16T11:33:08Z",
            "summary": "Extensive research on formal verification of machine learning (ML) systems\nindicates that learning from data alone often fails to capture underlying\nbackground knowledge. A variety of verifiers have been developed to ensure that\na machine-learnt model satisfies correctness and safety properties, however,\nthese verifiers typically assume a trained network with fixed weights.\nML-enabled autonomous systems are required to not only detect incorrect\npredictions, but should also possess the ability to self-correct, continuously\nimproving and adapting. A promising approach for creating ML models that\ninherently satisfy constraints is to encode background knowledge as logical\nconstraints that guide the learning process via so-called differentiable\nlogics. In this research preview, we compare and evaluate various logics from\nthe literature in weakly-supervised contexts, presenting our findings and\nhighlighting open problems for future work. Our experimental results are\nbroadly consistent with results reported previously in literature; however,\nlearning with differentiable logics introduces a new hyperparameter that is\ndifficult to tune and has significant influence on the effectiveness of the\nlogics.",
            "author": [
                "Thomas Flinkow",
                "Barak A. Pearlmutter",
                "Rosemary Monahan"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.395.3",
                "http://arxiv.org/abs/2311.09809v1",
                "http://arxiv.org/pdf/2311.09809v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09804v1",
            "title": "Average Jaccard Index of Random Graphs",
            "updated": "2023-11-16T11:29:03Z",
            "published": "2023-11-16T11:29:03Z",
            "summary": "The asymptotic behavior of the Jaccard index in $G(n,p)$, the classical\nErd\\\"{o}s-R\\'{e}nyi random graphs model, is studied in this paper, as $n$ goes\nto infinity. We first derive the asymptotic distribution of the Jaccard index\nof any pair of distinct vertices, as well as the first two moments of this\nindex. Then the average of the Jaccard indices over all vertex pairs in\n$G(n,p)$ is shown to be asymptotically normal under an additional mild\ncondition that $np\\to\\infty$ and $n^2(1-p)\\to\\infty$.",
            "author": [
                "Qunqiang Feng",
                "Shuai Guo",
                "Zhishui Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09804v1",
                "http://arxiv.org/pdf/2311.09804v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09802v1",
            "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
            "updated": "2023-11-16T11:26:21Z",
            "published": "2023-11-16T11:26:21Z",
            "summary": "Though prompting LLMs with various reasoning structures produces reasoning\nproofs along with answers, these proofs are not ensured to be causal and\nreliable due to the inherent defects of LLMs. Tracking such deficiencies, we\npresent a neuro-symbolic integration method, in which a neural LLM is used to\nrepresent the knowledge of the problem while an LLM-free symbolic solver is\nadopted to do deliberative reasoning using the knowledge. Specifically, our\ncustomized meta-interpreters allow the production of reasoning proofs and\nsupport flexible search strategies. These reasoning proofs are ensured to be\ncausal and reliable because of the deterministic executing nature of the\nsymbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT\nbaseline by nearly double in accuracy and more than triple in proof similarity.\nOn GSM8K, our method also shows accuracy improvements and nearly doubled proof\nsimilarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
            "author": [
                "Sen Yang",
                "Xin Li",
                "Leyang Cui",
                "Lidong Bing",
                "Wai Lam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09802v1",
                "http://arxiv.org/pdf/2311.09802v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09800v1",
            "title": "$\\textit{Dial BeInfo for Faithfulness}$: Improving Factuality of\n  Information-Seeking Dialogue via Behavioural Fine-Tuning",
            "updated": "2023-11-16T11:25:44Z",
            "published": "2023-11-16T11:25:44Z",
            "summary": "Factuality is a crucial requirement in information seeking dialogue: the\nsystem should respond to the user's queries so that the responses are\nmeaningful and aligned with the knowledge provided to the system. However, most\nmodern large language models suffer from hallucinations, that is, they generate\nresponses not supported by or contradicting the knowledge source. To mitigate\nthe issue and increase faithfulness of information-seeking dialogue systems, we\nintroduce BeInfo, a simple yet effective method that applies behavioural tuning\nto aid information-seeking dialogue. Relying on three standard datasets, we\nshow that models tuned with BeInfo} become considerably more faithful to the\nknowledge source both for datasets and domains seen during BeInfo-tuning, as\nwell as on unseen domains, when applied in a zero-shot manner. In addition, we\nshow that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo\ndemonstrate strong performance on data from real `production' conversations and\noutperform GPT4 when tuned on a limited amount of such realistic in-domain\ndialogues.",
            "author": [
                "Evgeniia Razumovskaia",
                "Ivan Vuli\u0107",
                "Pavle Markovi\u0107",
                "Tomasz Cichy",
                "Qian Zheng",
                "Tsung-Hsien Wen",
                "Pawe\u0142 Budzianowski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09800v1",
                "http://arxiv.org/pdf/2311.09800v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09801v1",
            "title": "A Study of Abstract Elementary Classes in the context of Graphs",
            "updated": "2023-11-16T11:25:44Z",
            "published": "2023-11-16T11:25:44Z",
            "summary": "In the framework of graphs, we study abstract elementary classes (aecs). In\nthis work we analyze several properties of Forb(G) and versions of Forb-Con(G)\nin the context of aecs and we present some examples of classes of graphs which\ncontradicts amalgamation property.",
            "author": [
                "Navaneetha Madaparambu Rajan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09801v1",
                "http://arxiv.org/pdf/2311.09801v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.CO",
                "03C48, 03C45, 03C50, 03C52, 05C63"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09797v1",
            "title": "KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance\n  Domains",
            "updated": "2023-11-16T11:22:08Z",
            "published": "2023-11-16T11:22:08Z",
            "summary": "We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs'\ncapabilities in applying financial knowledge to solve complex math word\nproblems. Compared to prior works, this study features three core advancements.\nFirst, KnowledgeMath includes 1,259 problems with a hybrid of textual and\ntabular content and require college-level knowledge in the finance domain for\neffective resolution. Second, we provide expert-annotated, detailed solution\nreferences in Python program format, ensuring a high-quality benchmark for LLM\nassessment. Finally, we evaluate a wide spectrum of 14 LLMs with different\nprompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The\ncurrent best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves\nonly 45.4% accuracy, leaving substantial room for improvement. While\nknowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0%\nfor GPT-3.5), it is still significantly lower the estimated human expert\nperformance of 94%. We believe that KnowledgeMath can facilitate future\nresearch on domain-specific knowledge retrieval and augmentation into the math\nword problem-solving process. We will release the benchmark and code at\nhttps://github.com/yale-nlp/KnowledgeMath.",
            "author": [
                "Yilun Zhao",
                "Hongjun Liu",
                "Yitao Long",
                "Rui Zhang",
                "Chen Zhao",
                "Arman Cohan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09797v1",
                "http://arxiv.org/pdf/2311.09797v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09792v1",
            "title": "PSR J$0952-0607$ and GW170817: Direct Multimessenger Constraints on\n  Neutron Star Equation of State through a Novel Wide-ranging Correlation",
            "updated": "2023-11-16T11:16:41Z",
            "published": "2023-11-16T11:16:41Z",
            "summary": "Our knowledge about neutron star (NS) masses is renewed once again due to the\nrecognition of the heaviest NS PSR J$ 0952-0607 $. By taking advantage of both\nmass observations of super massive neutron stars and the tidal deformability\nderived from event GW170817, a joint constraint on tidal deformability is\nobtained. A wide-ranging correlation between NS pressure and tidal\ndeformability within the density range from saturation density $\\rho_0$ to\n$5.6\\rho_0$ is discovered, which directly yields a constrained NS EoS. The\nnewly constrained EoS has a small uncertainty and a softer behavior at high\ndensities without the inclusion of extra degrees of freedom, which shows its\npotential to be used as an indicator for the component of NS core.",
            "author": [
                "Lan Guo",
                "YiFei Niu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09792v1",
                "http://arxiv.org/pdf/2311.09792v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09789v1",
            "title": "Arrow's Impossibility Theorem: Computability in Social Choice Theory",
            "updated": "2023-11-16T11:06:13Z",
            "published": "2023-11-16T11:06:13Z",
            "summary": "Arrow's Impossibility Theorem establishes bounds on what we can require from\nvoting systems. Given satisfaction of a small collection of \"fairness\" axioms,\nit shows votes can only exist as dictatorships in which one voter determines\nall outcomes. Votes are modelled as maps from a collection of partial orders,\nthe preferences of voters, to a single verdict which is another aggregated\npartial ordering. This result is classic and has an extension called the\nPossibility Theorem that shows these dictatorships needn't exist with infinite\nvoter sets. Mihara extends this work by examining the computability of each of\nthese results. He found that the only voting systems that are in any sense\ncomputable are necessarily dictatorial, which takes away from the usefulness of\nthe Possibility Theorem.\n  In this paper we primarily survey the results of Mihara, focusing not on\napplied consequences, as much of the surrounding literature does, but going\ninto greater details on the underlying Mathematics and Computability of the\nproofs. We give detailed exposition on the methods used and introduce all\nnotation. We first see complete proofs of the classical results and a\nsufficient introduction to computability that an unfamiliar reader should be\nable to follow without prior knowledge of the field. We then expand into\nMihara's results, and using our established knowledge of computability show the\nproblems with trying to compute non-dictatorial social welfare functions. This\ninvolves introducing an extended definition of computability called pairwise\ncomputability.",
            "author": [
                "Alex Hall"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09789v1",
                "http://arxiv.org/pdf/2311.09789v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09777v1",
            "title": "Trust Modelling and Verification Using Event-B",
            "updated": "2023-11-16T11:01:20Z",
            "published": "2023-11-16T11:01:20Z",
            "summary": "Trust is a crucial component in collaborative multiagent systems (MAS)\ninvolving humans and autonomous AI agents. Rather than assuming trust based on\npast system behaviours, it is important to formally verify trust by modelling\nthe current state and capabilities of agents. We argue for verifying actual\ntrust relations based on agents abilities to deliver intended outcomes in\nspecific contexts. To enable reasoning about different notions of trust, we\npropose using the refinement-based formal method Event-B. Refinement allows\nprogressively introducing new aspects of trust from abstract to concrete models\nincorporating knowledge and runtime states. We demonstrate modelling three\ntrust concepts and verifying associated trust properties in MAS. The formal,\ncorrectness-by-construction approach allows to deduce guarantees about\ntrustworthy autonomy in human-AI partnerships. Overall, our contribution\nfacilitates rigorous verification of trust in multiagent systems.",
            "author": [
                "Asieh Salehi Fathabadi",
                "Vahid Yazdanpanah"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.395.2",
                "http://arxiv.org/abs/2311.09777v1",
                "http://arxiv.org/pdf/2311.09777v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09775v1",
            "title": "MEGA: A Memory-Efficient GNN Accelerator Exploiting Degree-Aware\n  Mixed-Precision Quantization",
            "updated": "2023-11-16T10:58:34Z",
            "published": "2023-11-16T10:58:34Z",
            "summary": "Graph Neural Networks (GNNs) are becoming a promising technique in various\ndomains due to their excellent capabilities in modeling non-Euclidean data.\nAlthough a spectrum of accelerators has been proposed to accelerate the\ninference of GNNs, our analysis demonstrates that the latency and energy\nconsumption induced by DRAM access still significantly impedes the improvement\nof performance and energy efficiency. To address this issue, we propose a\nMemory-Efficient GNN Accelerator (MEGA) through algorithm and hardware\nco-design in this work. Specifically, at the algorithm level, through an\nin-depth analysis of the node property, we observe that the data-independent\nquantization in previous works is not optimal in terms of accuracy and memory\nefficiency. This motivates us to propose the Degree-Aware mixed-precision\nquantization method, in which a proper bitwidth is learned and allocated to a\nnode according to its in-degree to compress GNNs as much as possible while\nmaintaining accuracy. At the hardware level, we employ a heterogeneous\narchitecture design in which the aggregation and combination phases are\nimplemented separately with different dataflows. In order to boost the\nperformance and energy efficiency, we also present an Adaptive-Package format\nto alleviate the storage overhead caused by the fine-grained bitwidth and\ndiverse sparsity, and a Condense-Edge scheduling method to enhance the data\nlocality and further alleviate the access irregularity induced by the extremely\nsparse adjacency matrix in the graph. We implement our MEGA accelerator in a\n28nm technology node. Extensive experiments demonstrate that MEGA can achieve\nan average speedup of 38.3x, 7.1x, 4.0x, 3.6x and 47.6x, 7.2x, 5.4x, 4.5x\nenergy savings over four state-of-the-art GNN accelerators, HyGCN, GCNAX, GROW,\nand SGCN, respectively, while retaining task accuracy.",
            "author": [
                "Zeyu Zhu",
                "Fanrong Li",
                "Gang Li",
                "Zejian Liu",
                "Zitao Mo",
                "Qinghao Hu",
                "Xiaoyao Liang",
                "Jian Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09775v1",
                "http://arxiv.org/pdf/2311.09775v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09774v1",
            "title": "HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs",
            "updated": "2023-11-16T10:56:24Z",
            "published": "2023-11-16T10:56:24Z",
            "summary": "Adapting a language model into a specific domain, a.k.a `domain adaption', is\na common practice when specialized knowledge, e.g. medicine, is not\nencapsulated in a general language model like Llama2. The challenge lies in the\nheterogeneity of data across the two training stages, as it varies in\nlanguages, genres, or formats. To tackle this and simplify the learning\nprotocol, we propose to transform heterogeneous data, from the both\npre-training and supervised stages, into a unified, simple input-output pair\nformat. We validate the new protocol in the domains where proprietary LLMs like\nChatGPT perform relatively poorly, such as Traditional Chinese Medicine. The\ndeveloped model, HuatuoGPT-II, has shown state-of-the-art performance in\nChinese medicine domain on a number of benchmarks, e.g. medical licensing\nexams. It even outperforms proprietary models like ChatGPT and GPT-4 in some\naspects, especially in Traditional Chinese Medicine. Expert manual evaluations\nfurther validate HuatuoGPT-II's advantages over existing LLMs. Notably,\nHuatuoGPT-II was benchmarked in a fresh Chinese National Medical Licensing\nExamination where it achieved the best performance, showcasing not only its\neffectiveness but also its generalization capabilities.",
            "author": [
                "Junying Chen",
                "Xidong Wang",
                "Anningzhe Gao",
                "Feng Jiang",
                "Shunian Chen",
                "Hongbo Zhang",
                "Dingjie Song",
                "Wenya Xie",
                "Chuyi Kong",
                "Jianquan Li",
                "Xiang Wan",
                "Haizhou Li",
                "Benyou Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09774v1",
                "http://arxiv.org/pdf/2311.09774v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09762v1",
            "title": "Graph-Guided Reasoning for Multi-Hop Question Answering in Large\n  Language Models",
            "updated": "2023-11-16T10:36:08Z",
            "published": "2023-11-16T10:36:08Z",
            "summary": "Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning\ncapabilities of Large Language Models (LLMs) by generating a series of\nrationales before the final answer. We analyze the reasoning paths generated by\nCoT and find two issues in multi-step reasoning: (i) Generating rationales\nirrelevant to the question, (ii) Unable to compose subquestions or queries for\ngenerating/retrieving all the relevant information. To address them, we propose\na graph-guided CoT prompting method, which guides the LLMs to reach the correct\nanswer with graph representation/verification steps. Specifically, we first\nleverage LLMs to construct a \"question/rationale graph\" by using knowledge\nextraction prompting given the initial question and the rationales generated in\nthe previous steps. Then, the graph verification step diagnoses the current\nrationale triplet by comparing it with the existing question/rationale graph to\nfilter out irrelevant rationales and generate follow-up questions to obtain\nrelevant information. Additionally, we generate CoT paths that exclude the\nextracted graph information to represent the context information missed from\nthe graph extraction. Our graph-guided reasoning method shows superior\nperformance compared to previous CoT prompting and the variants on multi-hop\nquestion answering benchmark datasets.",
            "author": [
                "Jinyoung Park",
                "Ameen Patel",
                "Omar Zia Khan",
                "Hyunwoo J. Kim",
                "Joo-Kyung Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09762v1",
                "http://arxiv.org/pdf/2311.09762v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09760v1",
            "title": "Eventually Lattice-Linear Algorithms",
            "updated": "2023-11-16T10:34:08Z",
            "published": "2023-11-16T10:34:08Z",
            "summary": "Lattice-linear systems allow nodes to execute asynchronously. We introduce\neventually lattice-linear algorithms, where lattices are induced only among the\nstates in a subset of the state space. The algorithm guarantees that the system\ntransitions to a state in one of the lattices. Then, the algorithm behaves\nlattice linearly while traversing to an optimal state through that lattice.\n  We present a lattice-linear self-stabilizing algorithm for service demand\nbased minimal dominating set (SDMDS) problem. Using this as an example, we\nelaborate the working of, and define, eventually lattice-linear algorithms.\nThen, we present eventually lattice-linear self-stabilizing algorithms for\nminimal vertex cover (\\mvc), maximal independent set (\\mis), graph colouring\n(\\gc) and 2-dominating set problems (\\tds).\n  Algorithms for SDMDS, \\mvc and \\mis converge in 1 round plus $n$ moves\n(within $2n$ moves), \\gc in $n+4m$ moves, and \\tds in 1 round plus $2n$ moves\n(within $3n$ moves). These results are an improvement over the existing\nliterature. We also present experimental results to show performance gain\ndemonstrating the benefit of lattice-linearity.",
            "author": [
                "Arya Tanmay Gupta",
                "Sandeep S Kulkarni"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09760v1",
                "http://arxiv.org/pdf/2311.09760v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09758v1",
            "title": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue\n  State Tracking",
            "updated": "2023-11-16T10:30:55Z",
            "published": "2023-11-16T10:30:55Z",
            "summary": "Large language models (LLMs) have revolutionized the landscape of Natural\nLanguage Processing systems, but are computationally expensive. To reduce the\ncost without sacrificing performance, previous studies have explored various\napproaches to harness the potential of Small Language Models (SLMs) as\ncost-effective alternatives to their larger counterparts. Driven by findings\nthat SLMs and LLMs exhibit complementary strengths in a structured knowledge\nextraction task, this work presents a novel SLM/LLM routing framework designed\nto improve computational efficiency and enhance task performance. First,\nexemplar pools are created to represent the types of contexts where each LM\nprovides a more reliable answer, leveraging a sentence embedding fine-tuned so\nthat context similarity is close to dialogue state similarity. Then, during\ninference, the k-nearest exemplars to the testing instance are retrieved, and\nthe instance is routed according to majority vote. In dialogue state tracking\ntasks, the proposed routing framework enhances performance substantially\ncompared to relying solely on LLMs, while reducing the computational costs by\nover 50%.",
            "author": [
                "Chia-Hsuan Lee",
                "Hao Cheng",
                "Mari Ostendorf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09758v1",
                "http://arxiv.org/pdf/2311.09758v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09756v1",
            "title": "FairytaleCQA: Integrating a Commonsense Knowledge Graph into Children's\n  Storybook Narratives",
            "updated": "2023-11-16T10:30:26Z",
            "published": "2023-11-16T10:30:26Z",
            "summary": "AI models (including LLM) often rely on narrative question-answering (QA)\ndatasets to provide customized QA functionalities to support downstream\nchildren education applications; however, existing datasets only include QA\npairs that are grounded within the given storybook content, but children can\nlearn more when teachers refer the storybook content to real-world knowledge\n(e.g., commonsense knowledge). We introduce the FairytaleCQA dataset, which is\nannotated by children education experts, to supplement 278 storybook narratives\nwith educationally appropriate commonsense knowledge. The dataset has 5,868 QA\npairs that not only originate from the storybook narrative but also contain the\ncommonsense knowledge grounded by an external knowledge graph (i.e.,\nConceptNet). A follow-up experiment shows that a smaller model (T5-large)\nfine-tuned with FairytaleCQA reliably outperforms much larger prompt-engineered\nLLM (e.g., GPT-4) in this new QA-pair generation task (QAG). This result\nsuggests that: 1) our dataset brings novel challenges to existing LLMs, and 2)\nhuman experts' data annotation are still critical as they have much nuanced\nknowledge that LLMs do not know in the children educational domain.",
            "author": [
                "Jiaju Chen",
                "Yuxuan Lu",
                "Shao Zhang",
                "Bingsheng Yao",
                "Yuanzhe Dong",
                "Ying Xu",
                "Yunyao Li",
                "Qianwen Wang",
                "Dakuo Wang",
                "Yuling Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09756v1",
                "http://arxiv.org/pdf/2311.09756v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09751v2",
            "title": "Folding median graphs",
            "updated": "2023-11-29T08:45:26Z",
            "published": "2023-11-16T10:28:07Z",
            "summary": "Extending Stallings' foldings of trees, we show in this article that every\nparallel-preserving map between median graphs factors as an isometric embedding\nthrough a sequence of elementary transformations which we call foldings and\nswellings. This new construction proposes a unified point of view on Beeker and\nLazarovich's work on folding pocsets and on Ben-Zvi, Kropholler, and Lyman's\nwork on folding nonpositively curved cube complexes.",
            "author": [
                "Anthony Genevois",
                "Yassine Guerch",
                "Romain Tessera"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09751v2",
                "http://arxiv.org/pdf/2311.09751v2"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20F65, 20F67, 05C25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09733v1",
            "title": "MOKA: Moral Knowledge Augmentation for Moral Event Extraction",
            "updated": "2023-11-16T10:04:49Z",
            "published": "2023-11-16T10:04:49Z",
            "summary": "News media employ moral language to create memorable stories, and readers\noften engage with the content that align with their values. Moral theories have\nbeen applied to news analysis studying moral values in isolation, while the\nintricate dynamics among participating entities in shaping moral events have\nbeen overlooked. This is mainly due to the use of obscure language to conceal\nevident ideology and values, coupled with the insufficient moral reasoning\ncapability in most existing NLP systems, where LLMs are no exception. To study\nthis phenomenon, we first annotate a new dataset, MORAL EVENTS, consisting of\n5,494 structured annotations on 474 news articles by diverse US media across\nthe political spectrum. We further propose MOKA, a moral event extraction\nframework with MOral Knowledge Augmentation, that leverages knowledge derived\nfrom moral words and moral scenarios. Experimental results show that MOKA\noutperforms competitive baselines across three moral event understanding tasks.\nFurther analyses illuminate the selective reporting of moral events by media\noutlets of different ideological leanings, suggesting the significance of\nevent-level morality analysis in news. Our datasets and codebase are available\nat https://github.com/launchnlp/MOKA.",
            "author": [
                "Xinliang Frederick Zhang",
                "Winston Wu",
                "Nick Beauchamp",
                "Lu Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09733v1",
                "http://arxiv.org/pdf/2311.09733v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09731v1",
            "title": "Prudent Silence or Foolish Babble? Examining Large Language Models'\n  Responses to the Unknown",
            "updated": "2023-11-16T10:02:40Z",
            "published": "2023-11-16T10:02:40Z",
            "summary": "Large Language Models (LLMs) often struggle when faced with situations where\nthey lack the prerequisite knowledge to generate a sensical response. In these\ncases, models tend to fabricate and hallucinate, rather than appropriately\nsignaling uncertainty as humans would. This behavior misaligns with human\nconversational norms and presents challenges surrounding responsible and\nethical AI development. This work aims to systematically investigate LLMs'\nbehaviors in such situations. We curate an adversarial question-answering\nbenchmark containing unanswerable questions targeting information absent from\nthe LLM's training data. Concretely, these unanswerable questions contain\nnon-existent concepts or false premises. When presented with such unanswerable\nquestions, an LLM should appropriately convey uncertainty, and be able to\nchallenge the premise and refuse to generate a response. While facing\nanswerable valid questions, a model should demonstrate a positive correlation\nbetween accuracy and confidence. Using a model-agnostic unified confidence\nelicitation approach, we observe that LLMs that have gone through instruction\nfinetuning and reinforcement learning from human feedback (RLHF) perform\nsignificantly better than their counterparts that do not. Moreover, uncertainty\nexpression 1 through our elicitation method does not always stay consistent\nwith the perceived confidence of the direct response of an LLM. Our findings\ncall for further research into teaching LLMs to proactively and reliably\nexpress uncertainty.",
            "author": [
                "Genglin Liu",
                "Xingyao Wang",
                "Lifan Yuan",
                "Yangyi Chen",
                "Hao Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09731v1",
                "http://arxiv.org/pdf/2311.09731v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09726v1",
            "title": "MS-Former: Memory-Supported Transformer for Weakly Supervised Change\n  Detection with Patch-Level Annotations",
            "updated": "2023-11-16T09:57:29Z",
            "published": "2023-11-16T09:57:29Z",
            "summary": "Fully supervised change detection methods have achieved significant\nadvancements in performance, yet they depend severely on acquiring costly\npixel-level labels. Considering that the patch-level annotations also contain\nabundant information corresponding to both changed and unchanged objects in\nbi-temporal images, an intuitive solution is to segment the changes with\npatch-level annotations. How to capture the semantic variations associated with\nthe changed and unchanged regions from the patch-level annotations to obtain\npromising change results is the critical challenge for the weakly supervised\nchange detection task. In this paper, we propose a memory-supported transformer\n(MS-Former), a novel framework consisting of a bi-directional attention block\n(BAB) and a patch-level supervision scheme (PSS) tailored for weakly supervised\nchange detection with patch-level annotations. More specifically, the BAM\ncaptures contexts associated with the changed and unchanged regions from the\ntemporal difference features to construct informative prototypes stored in the\nmemory bank. On the other hand, the BAM extracts useful information from the\nprototypes as supplementary contexts to enhance the temporal difference\nfeatures, thereby better distinguishing changed and unchanged regions. After\nthat, the PSS guides the network learning valuable knowledge from the\npatch-level annotations, thus further elevating the performance. Experimental\nresults on three benchmark datasets demonstrate the effectiveness of our\nproposed method in the change detection task. The demo code for our work will\nbe publicly available at \\url{https://github.com/guanyuezhen/MS-Former}.",
            "author": [
                "Zhenglai Li",
                "Chang Tang",
                "Xinwang Liu",
                "Changdong Li",
                "Xianju Li",
                "Wei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09726v1",
                "http://arxiv.org/pdf/2311.09726v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09685v1",
            "title": "Chromatic functions, interval orders and increasing forests",
            "updated": "2023-11-16T08:55:25Z",
            "published": "2023-11-16T08:55:25Z",
            "summary": "The chromatic quasisymmetric functions (csf) of Shareshian and Wachs\nassociated to unit interval orders have attracted a lot of interest since their\nintroduction in 2016, both in combinatorics and geometry, because of their\nrelation to the famous Stanley-Stembridge conjecture (1993) and to the topology\nof Hessenberg varieties, respectively. In the present work we study the csf\nassociated to the larger class of interval orders with no restriction on the\nlength of the intervals. Inspired by an article of Abreu and Nigro, we show\nthat these csf are weighted sums of certain quasisymmetric functions associated\nto the increasing spanning forests of the associated incomparability graphs.\nFurthermore, we define quasisymmetric functions that include the unicellular\nLLT symmetric functions and generalize an identity due to Carlsson and Mellit.\nFinally we conjecture a formula giving their expansion in the type 1 power sum\nquasisymmetric functions which should extend a theorem of Athanasiadis.",
            "author": [
                "Michele D'Adderio",
                "Roberto Riccardi",
                "Viola Siconolfi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09685v1",
                "http://arxiv.org/pdf/2311.09685v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05E05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09682v1",
            "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
            "updated": "2023-11-16T08:52:27Z",
            "published": "2023-11-16T08:52:27Z",
            "summary": "We explore the creative problem-solving capabilities of modern large language\nmodels (LLMs) in a constrained setting. The setting requires circumventing a\ncognitive bias known in psychology as ''functional fixedness'' to use familiar\nobjects in innovative or unconventional ways. To this end, we create MacGyver,\nan automatically generated dataset consisting of 1,600 real-world problems that\ndeliberately trigger functional fixedness and require thinking\n'out-of-the-box'. We then present our collection of problems to both LLMs and\nhumans to compare and contrast their problem-solving abilities. We show that\nMacGyver is challenging for both groups, but in unique and complementary ways.\nFor example, humans typically excel in solving problems that they are familiar\nwith but may struggle with tasks requiring domain-specific knowledge, leading\nto a higher variance. On the other hand, LLMs, being exposed to a variety of\nhighly specialized knowledge, attempt broader problems but are prone to\noverconfidence and propose actions that are physically infeasible or\ninefficient. We also provide a detailed error analysis of LLMs, and demonstrate\nthe potential of enhancing their problem-solving ability with novel prompting\ntechniques such as iterative step-wise reflection and divergent-convergent\nthinking. This work provides insight into the creative problem-solving\ncapabilities of humans and AI and illustrates how psychological paradigms can\nbe extended into large-scale tasks for comparing humans and machines.",
            "author": [
                "Yufei Tian",
                "Abhilasha Ravichander",
                "Lianhui Qin",
                "Ronan Le Bras",
                "Raja Marjieh",
                "Nanyun Peng",
                "Yejin Choi",
                "Thomas L. Griffiths",
                "Faeze Brahman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09682v1",
                "http://arxiv.org/pdf/2311.09682v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09677v1",
            "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions",
            "updated": "2023-11-16T08:45:44Z",
            "published": "2023-11-16T08:45:44Z",
            "summary": "Large language models (LLMs) have revolutionized numerous domains with their\nimpressive performance but still face their challenges. A predominant issue is\nthe propensity for these models to generate non-existent facts, a concern\ntermed hallucination. Our research is motivated by the observation that\nprevious instruction tuning methods force the model to complete a sentence no\nmatter whether the model knows the knowledge or not. When the question is out\nof the parametric knowledge, it will try to make up something and fail to\nindicate when it lacks knowledge. In this paper, we present a new approach\ncalled Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized\nby first identifying the knowledge gap between parametric knowledge and the\ninstruction tuning data. Then, we construct the refusal-aware data based on the\nknowledge intersection, to tune LLMs to refrain from responding to questions\nbeyond its parametric knowledge. Experimental results demonstrate this new\ninstruction tuning approach effectively improves a model's ability to answer\nknown questions and refrain from answering unknown questions. Furthermore, when\ntested on out-of-domain datasets, the refusal ability was found to be a\nmeta-skill that could be generalized to other tasks. Further analysis\nsurprisingly finds that learning the uncertainty during training displays a\nbetter ability to estimate uncertainty than uncertainty-based testing. Our code\nwill be released at https://github.com/shizhediao/R-Tuning.",
            "author": [
                "Hanning Zhang",
                "Shizhe Diao",
                "Yong Lin",
                "Yi R. Fung",
                "Qing Lian",
                "Xingyao Wang",
                "Yangyi Chen",
                "Heng Ji",
                "Tong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09677v1",
                "http://arxiv.org/pdf/2311.09677v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09672v1",
            "title": "Facilitating the sharing of electrophysiology data analysis results\n  through in-depth provenance capture",
            "updated": "2023-11-16T08:41:43Z",
            "published": "2023-11-16T08:41:43Z",
            "summary": "Scientific research demands reproducibility and transparency, particularly in\ndata-intensive fields like electrophysiology. Electrophysiology data is\ntypically analyzed using scripts that generate output files, including figures.\nHandling these results poses several challenges due to the complexity and\ninteractivity of the analysis process. These stem from the difficulty to\ndiscern the analysis steps, parameters, and data flow from the results, making\nknowledge transfer and findability challenging in collaborative settings.\nProvenance information tracks data lineage and processes applied to it, and\nprovenance capture during the execution of an analysis script can address those\nchallenges. We present Alpaca (Automated Lightweight Provenance Capture), a\ntool that captures fine-grained provenance information with minimal user\nintervention when running data analysis pipelines implemented in Python\nscripts. Alpaca records inputs, outputs, and function parameters and structures\ninformation according to the W3C PROV standard. We demonstrate the tool using a\nrealistic use case involving multichannel local field potential recordings of a\nneurophysiological experiment, highlighting how the tool makes result details\nknown in a standardized manner in order to address the challenges of the\nanalysis process. Ultimately, using Alpaca will help to represent results\naccording to the FAIR principles, which will improve research reproducibility\nand facilitate sharing the results of data analyses.",
            "author": [
                "Cristiano Andr\u00e9 K\u00f6hler",
                "Danylo Ulianych",
                "Sonja Gr\u00fcn",
                "Stefan Decker",
                "Michael Denker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09672v1",
                "http://arxiv.org/pdf/2311.09672v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09667v1",
            "title": "Repetitive nonoverlapping sequential pattern mining",
            "updated": "2023-11-16T08:35:36Z",
            "published": "2023-11-16T08:35:36Z",
            "summary": "Sequential pattern mining (SPM) is an important branch of knowledge discovery\nthat aims to mine frequent sub-sequences (patterns) in a sequential database.\nVarious SPM methods have been investigated, and most of them are classical SPM\nmethods, since these methods only consider whether or not a given pattern\noccurs within a sequence. Classical SPM can only find the common features of\nsequences, but it ignores the number of occurrences of the pattern in each\nsequence, i.e., the degree of interest of specific users. To solve this\nproblem, this paper addresses the issue of repetitive nonoverlapping sequential\npattern (RNP) mining and proposes the RNP-Miner algorithm. To reduce the number\nof candidate patterns, RNP-Miner adopts an itemset pattern join strategy. To\nimprove the efficiency of support calculation, RNP-Miner utilizes the candidate\nsupport calculation algorithm based on the position dictionary. To validate the\nperformance of RNP-Miner, 10 competitive algorithms and 20 sequence databases\nwere selected. The experimental results verify that RNP-Miner outperforms the\nother algorithms, and using RNPs can achieve a better clustering performance\nthan raw data and classical frequent patterns. All the algorithms were\ndeveloped using the PyCharm environment and can be downloaded from\nhttps://github.com/wuc567/Pattern-Mining/tree/master/RNP-Miner.",
            "author": [
                "Meng Geng",
                "Youxi Wu",
                "Yan Li",
                "Jing Liu",
                "Philippe Fournier-Viger",
                "Xingquan Zhu",
                "Xindong Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09667v1",
                "http://arxiv.org/pdf/2311.09667v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09666v1",
            "title": "Orientably-regular embeddings of complete multigraphs",
            "updated": "2023-11-16T08:32:42Z",
            "published": "2023-11-16T08:32:42Z",
            "summary": "An embedding of a graph on an orientable surface is orientably-regular (or\nrotary, in an equivalent terminology) if the group of orientation-preserving\nautomorphisms of the embedding is transitive (and hence regular) on incident\nvertex-edge pairs of the graph. A classification of orientably-regular\nembeddings of complete graphs was obtained by L. D. James and G. A. Jones [in\n\"Regular orientable imbeddings of complete graphs\", J. Combinatorial Theory\nSer. B 39 (1985), 353-367], pointing out interesting connections to finite\nfields and Frobenius groups. By a combination of graph-theoretic methods and\ntools from combinatorial group theory we extend results of James and Jones to\nclassification of orientably-regular embeddings of complete multigraphs with\narbitrary edge-multiplicity.",
            "author": [
                "Stefan Gyurki",
                "Sona Pavlikova",
                "Jozef Siran"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09666v1",
                "http://arxiv.org/pdf/2311.09666v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09656v1",
            "title": "Structured Chemistry Reasoning with Large Language Models",
            "updated": "2023-11-16T08:20:36Z",
            "published": "2023-11-16T08:20:36Z",
            "summary": "This paper studies the problem of solving complex chemistry problems with\nlarge language models (LLMs). Despite the extensive general knowledge in LLMs\n(such as GPT-4), they struggle with chemistry reasoning that requires faithful\ngrounded reasoning with diverse chemical knowledge and an integrative\nunderstanding of chemical interactions. We propose InstructChem, a new\nstructured reasoning approach that substantially boosts the LLMs' chemical\nreasoning capabilities. InstructChem explicitly decomposes the reasoning into\nthree critical phrases, including chemical formulae generation by LLMs that\noffers the basis for subsequent grounded reasoning, step-by-step reasoning that\nmakes multi-step derivations with the identified formulae for a preliminary\nanswer, and iterative review-and-refinement that steers LLMs to progressively\nrevise the previous phases for increasing confidence, leading to the final\nhigh-confidence answer. We conduct extensive experiments on four different\nchemistry challenges, including quantum chemistry, quantum mechanics, physical\nchemistry, and chemistry kinetics. Our approach significantly enhances GPT-4 on\nchemistry reasoning, yielding an 8% average absolute improvement and a 30% peak\nimprovement. We further use the generated reasoning by GPT-4 to fine-tune\nsmaller LMs (e.g., Vicuna) and observe strong improvement of the smaller LMs.\nThis validates our approach and enables LLMs to generate high-quality\nreasoning.",
            "author": [
                "Siru Ouyang",
                "Zhuosheng Zhang",
                "Bing Yan",
                "Xuan Liu",
                "Jiawei Han",
                "Lianhui Qin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09656v1",
                "http://arxiv.org/pdf/2311.09656v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09646v1",
            "title": "Reconstructing Continuous Light Field From Single Coded Image",
            "updated": "2023-11-16T07:59:01Z",
            "published": "2023-11-16T07:59:01Z",
            "summary": "We propose a method for reconstructing a continuous light field of a target\nscene from a single observed image. Our method takes the best of two worlds:\njoint aperture-exposure coding for compressive light-field acquisition, and a\nneural radiance field (NeRF) for view synthesis. Joint aperture-exposure coding\nimplemented in a camera enables effective embedding of 3-D scene information\ninto an observed image, but in previous works, it was used only for\nreconstructing discretized light-field views. NeRF-based neural rendering\nenables high quality view synthesis of a 3-D scene from continuous viewpoints,\nbut when only a single image is given as the input, it struggles to achieve\nsatisfactory quality. Our method integrates these two techniques into an\nefficient and end-to-end trainable pipeline. Trained on a wide variety of\nscenes, our method can reconstruct continuous light fields accurately and\nefficiently without any test time optimization. To our knowledge, this is the\nfirst work to bridge two worlds: camera design for efficiently acquiring 3-D\ninformation and neural rendering.",
            "author": [
                "Yuya Ishikawa",
                "Keita Takahashi",
                "Chihiro Tsutake",
                "Toshiaki Fujii"
            ],
            "link": [
                "http://dx.doi.org/10.1109/ACCESS.2023.3314340",
                "http://arxiv.org/abs/2311.09646v1",
                "http://arxiv.org/pdf/2311.09646v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.GR",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09632v1",
            "title": "Online Continual Knowledge Learning for Language Models",
            "updated": "2023-11-16T07:31:03Z",
            "published": "2023-11-16T07:31:03Z",
            "summary": "Large Language Models (LLMs) serve as repositories of extensive world\nknowledge, enabling them to perform tasks such as question-answering and\nfact-checking. However, this knowledge can become obsolete as global contexts\nchange. In this paper, we introduce a novel problem in the realm of continual\nlearning: Online Continual Knowledge Learning (OCKL). This problem formulation\naims to manage the dynamic nature of world knowledge in LMs under real-time\nconstraints. We propose a new benchmark and evaluation metric designed to\nmeasure both the rate of new knowledge acquisition and the retention of\npreviously learned knowledge. Our empirical evaluation, conducted using a\nvariety of state-of-the-art methods, establishes robust base-lines for OCKL.\nOur results reveal that existing continual learning approaches are\nunfortunately insufficient for tackling the unique challenges posed by OCKL. We\nidentify key factors that influence the trade-off between knowledge acquisition\nand retention, thereby advancing our understanding of how to train LMs in a\ncontinually evolving environment.",
            "author": [
                "Yuhao Wu",
                "Tongjun Shi",
                "Karthick Sharma",
                "Chun Wei Seah",
                "Shuhao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09632v1",
                "http://arxiv.org/pdf/2311.09632v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09627v1",
            "title": "CRISPR: Eliminating Bias Neurons from an Instruction-following Language\n  Model",
            "updated": "2023-11-16T07:16:55Z",
            "published": "2023-11-16T07:16:55Z",
            "summary": "Large language models (LLMs) executing tasks through instruction-based\nprompts often face challenges stemming from distribution differences between\nuser instructions and training instructions. This leads to distractions and\nbiases, especially when dealing with inconsistent dynamic labels. In this\npaper, we introduces a novel bias mitigation method, CRISPR, designed to\nalleviate instruction-label biases in LLMs. CRISPR utilizes attribution methods\nto identify bias neurons influencing biased outputs and employs pruning to\neliminate the bias neurons. Experimental results demonstrate the method's\neffectiveness in mitigating biases in instruction-based prompting, enhancing\nlanguage model performance on social bias benchmarks without compromising\npre-existing knowledge. CRISPR proves highly practical, model-agnostic,\noffering flexibility in adapting to evolving social biases.",
            "author": [
                "Nakyeong Yang",
                "Taegwan Kang",
                "Kyomin Jung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09627v1",
                "http://arxiv.org/pdf/2311.09627v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09623v1",
            "title": "Apoptosis classification using attention based spatio temporal graph\n  convolution neural network",
            "updated": "2023-11-16T07:14:50Z",
            "published": "2023-11-16T07:14:50Z",
            "summary": "Accurate classification of apoptosis plays an important role in cell biology\nresearch. There are many state-of-the-art approaches which use deep CNNs to\nperform the apoptosis classification but these approaches do not account for\nthe cell interaction. Our paper proposes the Attention Graph spatio-temporal\ngraph convolutional network to classify the cell death based on the target\ncells in the video. This method considers the interaction of multiple target\ncells at each time stamp. We model the whole video sequence as a set of graphs\nand classify the target cell in the video as dead or alive. Our method\nencounters both spatial and temporal relationships.",
            "author": [
                "Akash Awasthi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09623v1",
                "http://arxiv.org/pdf/2311.09623v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10779v1",
            "title": "Knowledge Plugins: Enhancing Large Language Models for Domain-Specific\n  Recommendations",
            "updated": "2023-11-16T07:09:38Z",
            "published": "2023-11-16T07:09:38Z",
            "summary": "The significant progress of large language models (LLMs) provides a promising\nopportunity to build human-like systems for various practical applications.\nHowever, when applied to specific task domains, an LLM pre-trained on a\ngeneral-purpose corpus may exhibit a deficit or inadequacy in two types of\ndomain-specific knowledge. One is a comprehensive set of domain data that is\ntypically large-scale and continuously evolving. The other is specific working\npatterns of this domain reflected in the data. The absence or inadequacy of\nsuch knowledge impacts the performance of the LLM. In this paper, we propose a\ngeneral paradigm that augments LLMs with DOmain-specific KnowledgE to enhance\ntheir performance on practical applications, namely DOKE. This paradigm relies\non a domain knowledge extractor, working in three steps: 1) preparing effective\nknowledge for the task; 2) selecting the knowledge for each specific sample;\nand 3) expressing the knowledge in an LLM-understandable way. Then, the\nextracted knowledge is incorporated through prompts, without any computational\ncost of model fine-tuning. We instantiate the general paradigm on a widespread\napplication, i.e. recommender systems, where critical item attributes and\ncollaborative filtering signals are incorporated. Experimental results\ndemonstrate that DOKE can substantially improve the performance of LLMs in\nspecific domains.",
            "author": [
                "Jing Yao",
                "Wei Xu",
                "Jianxun Lian",
                "Xiting Wang",
                "Xiaoyuan Yi",
                "Xing Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10779v1",
                "http://arxiv.org/pdf/2311.10779v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09619v1",
            "title": "Take One Step at a Time to Know Incremental Utility of Demonstration: An\n  Analysis on Reranking for Few-Shot In-Context Learning",
            "updated": "2023-11-16T07:03:54Z",
            "published": "2023-11-16T07:03:54Z",
            "summary": "In-Context Learning (ICL) is an emergent capability of Large Language Models\n(LLMs). Only a few demonstrations enable LLMs to be used as blackbox for new\ntasks. Previous studies have shown that using LLMs' outputs as labels is\neffective in training models to select demonstrations. Such a label is expected\nto estimate utility of a demonstration in ICL; however, it has not been well\nunderstood how different labeling strategies affect results on target tasks.\nThis paper presents an analysis on different utility functions by focusing on\nLLMs' output probability given ground-truth output, and task-specific reward\ngiven LLMs' prediction. Unlike the previous work, we introduce a novel labeling\nmethod, incremental utility, which estimates how much incremental knowledge is\nbrought into the LLMs by a demonstration. We conduct experiments with\ninstruction-tuned LLMs on binary/multi-class classification, segmentation, and\ntranslation across Arabic, English, Finnish, Japanese, and Spanish. Our results\nshow that (1) the probability is effective when the probability values are\ndistributed across the whole value range (on the classification tasks), and (2)\nthe downstream metric is more robust when nuanced reward values are provided\nwith long outputs (on the segmentation and translation tasks). We then show\nthat the proposed incremental utility further helps ICL by contrasting how the\nLLMs perform with and without the demonstrations.",
            "author": [
                "Kazuma Hashimoto",
                "Karthik Raman",
                "Michael Bendersky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09619v1",
                "http://arxiv.org/pdf/2311.09619v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09608v1",
            "title": "Deep Neural Helmholtz Operators for 3D Elastic Wave Propagation and\n  Inversion",
            "updated": "2023-11-16T06:37:47Z",
            "published": "2023-11-16T06:37:47Z",
            "summary": "Numerical simulations of seismic wave propagation in heterogeneous 3D media\nare central to investigating subsurface structures and understanding earthquake\nprocesses, yet are computationally expensive for large problems. This is\nparticularly problematic for full waveform inversion, which typically involves\nnumerous runs of the forward process. In machine learning there has been\nconsiderable recent work in the area of operator learning, with a new class of\nmodels called neural operators allowing for data-driven solutions to partial\ndifferential equations. Recent works in seismology have shown that when neural\noperators are adequately trained, they can significantly shorten the compute\ntime for wave propagation. However, the memory required for the 3D time domain\nequations may be prohibitive. In this study, we show that these limitations can\nbe overcome by solving the wave equations in the frequency domain, also known\nas the Helmholtz equations, since the solutions for a set of frequencies can be\ndetermined in parallel. The 3D Helmholtz neural operator is 40 times more\nmemory-efficient than an equivalent time-domain version. We employ a U-shaped\nneural operator for 2D and 3D elastic wave modeling, achieving two orders of\nmagnitude acceleration compared to a baseline spectral element method. The\nneural operator accurately generalizes to variable velocity structures and can\nbe evaluated on denser input meshes than used in the training simulations. We\nalso show that when solving for wavefields strictly on the surface, the\naccuracy can be significantly improved via a graph neural operator layer. In\nleveraging automatic differentiation, the proposed method can serve as an\nalternative to the adjoint-state approach for 3D full-waveform inversion,\nreducing the computation time by a factor of 350.",
            "author": [
                "Caifeng Zou",
                "Kamyar Azizzadenesheli",
                "Zachary E. Ross",
                "Robert W. Clayton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09608v1",
                "http://arxiv.org/pdf/2311.09608v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09599v1",
            "title": "Gradual Source Domain Expansion for Unsupervised Domain Adaptation",
            "updated": "2023-11-16T06:18:35Z",
            "published": "2023-11-16T06:18:35Z",
            "summary": "Unsupervised domain adaptation (UDA) tries to overcome the need for a large\nlabeled dataset by transferring knowledge from a source dataset, with lots of\nlabeled data, to a target dataset, that has no labeled data. Since there are no\nlabels in the target domain, early misalignment might propagate into the later\nstages and lead to an error build-up. In order to overcome this problem, we\npropose a gradual source domain expansion (GSDE) algorithm. GSDE trains the UDA\ntask several times from scratch, each time reinitializing the network weights,\nbut each time expands the source dataset with target data. In particular, the\nhighest-scoring target data of the previous run are employed as pseudo-source\nsamples with their respective pseudo-label. Using this strategy, the\npseudo-source samples induce knowledge extracted from the previous run directly\nfrom the start of the new training. This helps align the two domains better,\nespecially in the early training epochs. In this study, we first introduce a\nstrong baseline network and apply our GSDE strategy to it. We conduct\nexperiments and ablation studies on three benchmarks (Office-31, OfficeHome,\nand DomainNet) and outperform state-of-the-art methods. We further show that\nthe proposed GSDE strategy can improve the accuracy of a variety of different\nstate-of-the-art UDA approaches.",
            "author": [
                "Thomas Westfechtel",
                "Hao-Wei Yeh",
                "Dexuan Zhang",
                "Tatsuya Harada"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09599v1",
                "http://arxiv.org/pdf/2311.09599v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09596v1",
            "title": "Generating Drug Repurposing Hypotheses through the Combination of\n  Disease-Specific Hypergraphs",
            "updated": "2023-11-16T06:09:14Z",
            "published": "2023-11-16T06:09:14Z",
            "summary": "The drug development pipeline for a new compound can last 10-20 years and\ncost over 10 billion. Drug repurposing offers a more time- and cost-effective\nalternative. Computational approaches based on biomedical knowledge graph\nrepresentations have recently yielded new drug repurposing hypotheses. In this\nstudy, we present a novel, disease-specific hypergraph representation learning\ntechnique to derive contextual embeddings of biological pathways of various\nlengths but that all start at any given drug and all end at the disease of\ninterest. Further, we extend this method to multi-disease hypergraphs. To\ndetermine the repurposing potential of each of the 1,522 drugs, we derive\ndrug-specific distributions of cosine similarity values and ultimately consider\nthe median for ranking. Cosine similarity values are computed between (1) all\nbiological pathways starting at the considered drug and ending at the disease\nof interest and (2) all biological pathways starting at drugs currently\nprescribed against that disease and ending at the disease of interest. We\nillustrate our approach with Alzheimer's disease (AD) and two of its risk\nfactors: hypertension (HTN) and type 2 diabetes (T2D). We compare each drug's\nrank across four hypergraph settings (single- or multi-disease): AD only, AD +\nHTN, AD + T2D, and AD + HTN + T2D. Notably, our framework led to the\nidentification of two promising drugs whose repurposing potential was\nsignificantly higher in hypergraphs combining two diseases: dapagliflozin\n(antidiabetic; moved up, from top 32$\\%$ to top 7$\\%$, across all considered\ndrugs) and debrisoquine (antihypertensive; moved up, from top 76$\\%$ to top\n23$\\%$). Our approach serves as a hypothesis generation tool, to be paired with\na validation pipeline relying on laboratory experiments and semi-automated\nparsing of the biomedical literature.",
            "author": [
                "Ayush Jain",
                "Marie Laure-Charpignon",
                "Irene Y. Chen",
                "Anthony Philippakis",
                "Ahmed Alaa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09596v1",
                "http://arxiv.org/pdf/2311.09596v1"
            ],
            "primary_category": "q-bio.BM",
            "category": [
                "q-bio.BM",
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09593v1",
            "title": "Multi-Step Dialogue Workflow Action Prediction",
            "updated": "2023-11-16T06:05:47Z",
            "published": "2023-11-16T06:05:47Z",
            "summary": "In task-oriented dialogue, a system often needs to follow a sequence of\nactions, called a workflow, that complies with a set of guidelines in order to\ncomplete a task. In this paper, we propose the novel problem of multi-step\nworkflow action prediction, in which the system predicts multiple future\nworkflow actions. Accurate prediction of multiple steps allows for multi-turn\nautomation, which can free up time to focus on more complex tasks. We propose\nthree modeling approaches that are simple to implement yet lead to more action\nautomation: 1) fine-tuning on a training dataset, 2) few-shot in-context\nlearning leveraging retrieval and large language model prompting, and 3)\nzero-shot graph traversal, which aggregates historical action sequences into a\ngraph for prediction. We show that multi-step action prediction produces\nfeatures that improve accuracy on downstream dialogue tasks like predicting\ntask success, and can increase automation of steps by 20% without requiring as\nmuch feedback from a human overseeing the system.",
            "author": [
                "Ramya Ramakrishnan",
                "Ethan Elenberg",
                "Hashan Narangodage",
                "Ryan McDonald"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09593v1",
                "http://arxiv.org/pdf/2311.09593v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10777v3",
            "title": "A Systematic Review of Aspect-based Sentiment Analysis (ABSA): Domains,\n  Methods, and Trends",
            "updated": "2023-12-03T05:50:36Z",
            "published": "2023-11-16T06:01:47Z",
            "summary": "Aspect-based Sentiment Analysis (ABSA) is a type of fine-grained sentiment\nanalysis (SA) that identifies aspects and the associated opinions from a given\ntext. In the digital era, ABSA gained increasing popularity and applications in\nmining opinionated text data to obtain insights and support decisions. ABSA\nresearch employs linguistic, statistical, and machine-learning approaches and\nutilises resources such as labelled datasets, aspect and sentiment lexicons and\nontology. By its nature, ABSA is domain-dependent and can be sensitive to the\nimpact of misalignment between the resource and application domains. However,\nto our knowledge, this topic has not been explored by the existing ABSA\nliterature reviews. In this paper, we present a Systematic Literature Review\n(SLR) of ABSA studies with a focus on the research application domain, dataset\ndomain, and the research methods to examine their relationships and identify\ntrends over time. Our results suggest a number of potential systemic issues in\nthe ABSA research literature, including the predominance of the\n``product/service review'' dataset domain among the majority of studies that\ndid not have a specific research application domain, coupled with the\nprevalence of dataset-reliant methods such as supervised machine learning. This\nreview makes a number of unique contributions to the ABSA research field: 1) To\nour knowledge, it is the first SLR that links the research domain, dataset\ndomain, and research method through a systematic perspective; 2) it is one of\nthe largest scoped SLR on ABSA, with 519 eligible studies filtered from 4191\nsearch results without time constraint; and 3) our review methodology adopted\nan innovative automatic filtering process based on PDF-mining, which enhanced\nscreening quality and reliability. Suggestions and our review limitations are\nalso discussed.",
            "author": [
                "Yan Cathy Hua",
                "Paul Denny",
                "Katerina Taskova",
                "J\u00f6rg Wicker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10777v3",
                "http://arxiv.org/pdf/2311.10777v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09584v1",
            "title": "A Dichotomy Hierarchy Characterizing Linear Time Subgraph Counting in\n  Bounded Degeneracy Graphs",
            "updated": "2023-11-16T05:40:23Z",
            "published": "2023-11-16T05:40:23Z",
            "summary": "Subgraph and homomorphism counting are fundamental algorithmic problems.\nGiven a constant-sized pattern graph $H$ and a large input graph $G$, we wish\nto count the number of $H$-homomorphisms/subgraphs in $G$. Given the massive\nsizes of real-world graphs and the practical importance of counting problems,\nwe focus on when (near) linear time algorithms are possible. The seminal work\nof Chiba-Nishizeki (SICOMP 1985) shows that for bounded degeneracy graphs $G$,\nclique and $4$-cycle counting can be done linear time. Recent works (Bera et\nal, SODA 2021, JACM 2022) show a dichotomy theorem characterizing the patterns\n$H$ for which $H$-homomorphism counting is possible in linear time, for bounded\ndegeneracy inputs $G$. At the other end, Ne\\v{s}et\\v{r}il and Ossona de Mendez\nused their deep theory of \"sparsity\" to define bounded expansion graphs. They\nprove that, for all $H$, $H$-homomorphism counting can be done in linear time\nfor bounded expansion inputs. What lies between? For a specific $H$, can we\ncharacterize input classes where $H$-homomorphism counting is possible in\nlinear time?\n  We discover a hierarchy of dichotomy theorems that precisely answer the above\nquestions. We show the existence of an infinite sequence of graph classes\n$\\mathcal{G}_0$ $\\supseteq$ $\\mathcal{G}_1$ $\\supseteq$ ... $\\supseteq$\n$\\mathcal{G}_\\infty$ where $\\mathcal{G}_0$ is the class of bounded degeneracy\ngraphs, and $\\mathcal{G}_\\infty$ is the class of bounded expansion graphs. Fix\nany constant sized pattern graph $H$. Let $LICL(H)$ denote the length of the\nlongest induced cycle in $H$. We prove the following. If $LICL(H) < 3(r+2)$,\nthen $H$-homomorphisms can be counted in linear time for inputs in\n$\\mathcal{G}_r$. If $LICL(H) \\geq 3(r+2)$, then $H$-homomorphism counting on\ninputs from $\\mathcal{G}_r$ takes $\\Omega(m^{1+\\gamma})$ time. We prove similar\ndichotomy theorems for subgraph counting.",
            "author": [
                "Daniel Paul-Pena",
                "C. Seshadhri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09584v1",
                "http://arxiv.org/pdf/2311.09584v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09579v1",
            "title": "Crafting In-context Examples according to LMs' Parametric Knowledge",
            "updated": "2023-11-16T05:30:07Z",
            "published": "2023-11-16T05:30:07Z",
            "summary": "In-context learning has been applied to knowledge-rich tasks such as question\nanswering. In such scenarios, in-context examples are used to trigger a\nbehaviour in the language model: namely, it should surface information stored\nin its parametric knowledge. We study the construction of in-context example\nsets, with a focus on the parametric knowledge of the model regarding\nin-context examples. We identify 'known' examples, where models can correctly\nanswer from its parametric knowledge, and 'unknown' ones. Our experiments show\nthat prompting with 'unknown' examples decreases the performance, potentially\nas it encourages hallucination rather than searching its parametric knowledge.\nConstructing an in-context example set that presents both known and unknown\ninformation performs the best across diverse settings. We perform analysis on\nthree multi-answer question answering datasets, which allows us to further\nstudy answer set ordering strategies based on the LM's knowledge about each\nanswer. Together, our study sheds lights on how to best construct in-context\nexample sets for knowledge-rich tasks.",
            "author": [
                "Yoonsang Lee",
                "Pranav Atreya",
                "Xi Ye",
                "Eunsol Choi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09579v1",
                "http://arxiv.org/pdf/2311.09579v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09575v1",
            "title": "Exact solution of Boltzmann equation in a longitudinal expanding system",
            "updated": "2023-11-16T05:17:54Z",
            "published": "2023-11-16T05:17:54Z",
            "summary": "Analytical solutions to the microscopic Boltzmann equation are useful in\ntesting the applicability and accuracy of macroscopic hydrodynamic theory. In\nthis work, we present exact solutions of the relativistic Boltzmann equation,\nbased on a new family of exact solutions of the relativistic ideal hydrodynamic\nequations [Phys. Rev. C 105, L021902]. To the best of our knowledge, this is\nthe first exact solution that allows either symmetric or asymmetric\nlongitudinal expansion with broken boost invariance.",
            "author": [
                "Shile Chen",
                "Shuzhe Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09575v1",
                "http://arxiv.org/pdf/2311.09575v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th",
                "hep-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09566v1",
            "title": "A Knowledge Distillation Approach for Sepsis Outcome Prediction from\n  Multivariate Clinical Time Series",
            "updated": "2023-11-16T05:06:51Z",
            "published": "2023-11-16T05:06:51Z",
            "summary": "Sepsis is a life-threatening condition triggered by an extreme infection\nresponse. Our objective is to forecast sepsis patient outcomes using their\nmedical history and treatments, while learning interpretable state\nrepresentations to assess patients' risks in developing various adverse\noutcomes. While neural networks excel in outcome prediction, their limited\ninterpretability remains a key issue. In this work, we use knowledge\ndistillation via constrained variational inference to distill the knowledge of\na powerful \"teacher\" neural network model with high predictive power to train a\n\"student\" latent variable model to learn interpretable hidden state\nrepresentations to achieve high predictive performance for sepsis outcome\nprediction. Using real-world data from the MIMIC-IV database, we trained an\nLSTM as the \"teacher\" model to predict mortality for sepsis patients, given\ninformation about their recent history of vital signs, lab values and\ntreatments. For our student model, we use an autoregressive hidden Markov model\n(AR-HMM) to learn interpretable hidden states from patients' clinical time\nseries, and use the posterior distribution of the learned state representations\nto predict various downstream outcomes, including hospital mortality, pulmonary\nedema, need for diuretics, dialysis, and mechanical ventilation. Our results\nshow that our approach successfully incorporates the constraint to achieve high\npredictive power similar to the teacher model, while maintaining the generative\nperformance.",
            "author": [
                "Anna Wong",
                "Shu Ge",
                "Nassim Oufattole",
                "Adam Dejl",
                "Megan Su",
                "Ardavan Saeedi",
                "Li-wei H. Lehman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09566v1",
                "http://arxiv.org/pdf/2311.09566v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09560v1",
            "title": "An experimental study of flow near an advancing contact line: a rigorous\n  test of theoretical models",
            "updated": "2023-11-16T04:34:46Z",
            "published": "2023-11-16T04:34:46Z",
            "summary": "The flow near a moving contact line depends on the dynamic contact angle,\nviscosity ratio, and capillary number. We report experiments involving\nimmersing a plate into a liquid bath, concurrently measuring the interface\nshape, interfacial velocity, and fluid flow using digital image processing and\nparticle image velocimetry. All experiments were performed at low plate speeds\nto maintain small Reynolds and capillary numbers for comparison with viscous\ntheories. The dynamic contact angle, measured in the viscous phase, was kept\nbelow $90^{\\circ}$, an unexplored region of parameter space. An important aim\nof the present study is to provide valuable experimental data using which new\ncontact line models can be developed and validated. Interface shapes reveal\nthat the strong viscous bending predicted by theoretical models is absent in\nthe experimental data. The flow field is directly compared against the\nprediction from the viscous theory of \\cite{huh1971hydrodynamic} but with a\nslight modification involving the curved interface. Remarkable agreement is\nfound between experiments and theory across a wide parameter range. The\nprediction for interfacial speed from \\cite{huh1971hydrodynamic} is also in\nexcellent agreement with experiments except in the vicinity of the contact\nline. Material points along the interface were found to rapidly slow down near\nthe contact line, thus alleviating the singularity at the moving contact line.\nTo the best of our knowledge, such a detailed test of theoretical models has\nnot been performed before and we hope the present study will spur new modeling\nefforts in the field.",
            "author": [
                "Charul Gupta",
                "Anjishnu Choudhury",
                "Lakshmana D Chandrala",
                "Harish N Dixit"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09560v1",
                "http://arxiv.org/pdf/2311.09560v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cond-mat.soft"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09557v1",
            "title": "Products of boundary classes on M_0,n-bar via balanced weights",
            "updated": "2023-11-16T04:23:57Z",
            "published": "2023-11-16T04:23:57Z",
            "summary": "In this note, we give a simple closed formula for an arbitrary product,\nlanding in dimension 0, of boundary classes on the Deligne--Mumford moduli\nspace M_0,n-bar. For any such boundary strata $X_{T_1}, \\ldots, X_{T_\\ell}$, we\nshow the intersection product $\\int \\prod_{i=1}^\\ell [X_{T_i}]$ is either a\nsigned product of multinomial coefficients, or zero, and provide a simple\ncriterion for determining when it is nonzero.\n  We do not claim originality for our product formula, but to our knowledge it\ndoes not appear elsewhere in the literature.",
            "author": [
                "Maria Gillespie",
                "Jake Levinson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09557v1",
                "http://arxiv.org/pdf/2311.09557v1"
            ],
            "primary_category": "math.AG",
            "category": [
                "math.AG",
                "math.CO",
                "14H10, 14N10, 05A10, 05C05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09548v1",
            "title": "Universally Optimal Information Dissemination and Shortest Paths in the\n  HYBRID Distributed Model",
            "updated": "2023-11-16T04:00:56Z",
            "published": "2023-11-16T04:00:56Z",
            "summary": "In this work we consider the HYBRID model of distributed computing,\nintroduced recently by Augustine, Hinnenthal, Kuhn, Scheideler, and Schneider\n(SODA 2020), where nodes have access to two different communication modes:\nhigh-bandwidth local communication along the edges of the graph and\nlow-bandwidth all-to-all communication, capturing the non-uniform nature of\nmodern communication networks.\n  Prior work in HYBRID has focused on showing existentially optimal algorithms,\nmeaning there exists a pathological family of instances on which no algorithm\ncan do better. This neglects the fact that such worst-case instances often do\nnot appear or can be actively avoided in practice. In this work, we focus on\nthe notion of universal optimality, first raised by Garay, Kutten, and Peleg\n(FOCS 1993). Roughly speaking, a universally optimal algorithm is one that,\ngiven any input graph, runs as fast as the best algorithm designed specifically\nfor that graph.\n  We show the first universally optimal algorithms in HYBRID. We present\nuniversally optimal solutions for fundamental information dissemination tasks,\nsuch as broadcasting and unicasting multiple messages in HYBRID. Furthermore,\nwe apply these tools to obtain universally optimal solutions for various\nshortest paths problems in HYBRID.\n  A main conceptual contribution of this work is the conception of a new graph\nparameter called neighborhood quality that captures the inherent complexity of\nmany fundamental graph problems in HYBRID.\n  We also show new existentially optimal shortest paths algorithms in HYBRID,\nwhich are utilized as key subroutines in our universally optimal algorithms and\nare of independent interest. Our new algorithms for $k$-source shortest paths\nmatch the existing $\\tilde{\\Omega}(\\sqrt{k})$ lower bound for all $k$.\nPreviously, the lower bound was only known to be tight when $k \\in\n\\tilde{\\Omega}(n^{2/3})$.",
            "author": [
                "Yi-Jun Chang",
                "Oren Hecht",
                "Dean Leitersdorf",
                "Philipp Schneider"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09548v1",
                "http://arxiv.org/pdf/2311.09548v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09546v1",
            "title": "Stability estimates in a partial data inverse boundary value problem for\n  biharmonic operators with first order perturbation",
            "updated": "2023-11-16T03:56:21Z",
            "published": "2023-11-16T03:56:21Z",
            "summary": "We establish logarithmic stability estimates for an inverse boundary value\nproblem for the biharmonic operator with first order perturbation from partial\nboundary measurements. Our geometric setting is that of a bounded domain in the\nEuclidean space of dimension three or higher. In particular, the inaccessible\nportion of the boundary is flat, and we have knowledge of the\nDirichlet-to-Neumann map on the complement.",
            "author": [
                "Boya Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09546v1",
                "http://arxiv.org/pdf/2311.09546v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "35R30, 35J40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09535v2",
            "title": "FunctionMarker: Watermarking Language Datasets via Knowledge Injection",
            "updated": "2023-11-17T05:00:21Z",
            "published": "2023-11-16T03:22:53Z",
            "summary": "Large Language Models (LLMs) have demonstrated superior performance in\nvarious natural language processing tasks. Meanwhile, they require extensive\ntraining data, raising concerns related to dataset copyright protection.\nBackdoor-based watermarking is a viable approach to protect the copyright of\nclassification datasets. However, these methods may introduce malicious\nmisclassification behaviors into watermarked LLMs by attackers and also affect\nthe semantic information of the watermarked text. To address these issues, we\npropose FunctionMarker, a novel copyright protection method for language\ndatasets via knowledge injection. FunctionMarker enables LLMs to learn specific\nknowledge through fine-tuning on watermarked datasets, and we can extract the\nembedded watermark by obtaining the responses of LLMs to specific\nknowledge-related queries. Considering watermark capacity and stealthness, we\nselect customizable functions as specific knowledge for LLMs to learn and embed\nthe watermark into them. Moreover, FunctionMarker can embed multi-bit\nwatermarks while preserving the original semantic information, thereby\nincreasing the difficulty of adaptive attacks. We take mathematical functions\nas an instance to evaluate the effectiveness of FunctionMarker, and experiments\nshow that only 0.3% of watermarked text achieves a 90% watermark extraction\naccuracy in most cases, validating our method's effectiveness.",
            "author": [
                "Shuai Li",
                "Kejiang Chen",
                "Kunsheng Tang",
                "Wen Huang",
                "Jie Zhang",
                "Weiming Zhang",
                "Nenghai Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09535v2",
                "http://arxiv.org/pdf/2311.09535v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09529v1",
            "title": "TransCrimeNet: A Transformer-Based Model for Text-Based Crime Prediction\n  in Criminal Networks",
            "updated": "2023-11-16T03:14:58Z",
            "published": "2023-11-16T03:14:58Z",
            "summary": "This paper presents TransCrimeNet, a novel transformer-based model for\npredicting future crimes in criminal networks from textual data. Criminal\nnetwork analysis has become vital for law enforcement agencies to prevent\ncrimes. However, existing graph-based methods fail to effectively incorporate\ncrucial textual data like social media posts and interrogation transcripts that\nprovide valuable insights into planned criminal activities. To address this\nlimitation, we develop TransCrimeNet which leverages the representation\nlearning capabilities of transformer models like BERT to extract features from\nunstructured text data. These text-derived features are fused with graph\nembeddings of the criminal network for accurate prediction of future crimes.\nExtensive experiments on real-world criminal network datasets demonstrate that\nTransCrimeNet outperforms previous state-of-the-art models by 12.7\\% in F1\nscore for crime prediction. The results showcase the benefits of combining\ntextual and graph-based features for actionable insights to disrupt criminal\nenterprises.",
            "author": [
                "Chen Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09529v1",
                "http://arxiv.org/pdf/2311.09529v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09521v1",
            "title": "AMRFact: Enhancing Summarization Factuality Evaluation with AMR-driven\n  Training Data Generation",
            "updated": "2023-11-16T02:56:29Z",
            "published": "2023-11-16T02:56:29Z",
            "summary": "Ensuring factual consistency is crucial in various natural language\nprocessing tasks, particularly in abstractive summarization, where preserving\nthe integrity of information is paramount. Prior entailment-based approaches\noften generate factually inconsistent summaries and then train a classifier on\nthe generated data. However, summaries produced by these approaches are either\nof low coherence or lack error-type coverage. To address these issues, we\npropose AMRFact, a novel framework that generates factually inconsistent\nsummaries using Abstract Meaning Representation (AMR). Our approach parses\nfactually correct summaries into AMR graphs and injects controlled factual\ninconsistencies to create negative examples, allowing for coherent factually\ninconsistent summaries to be generated with high error-type coverage.\nAdditionally, we present a data selection module NegFilter based on natural\nlanguage inference and BARTScore to ensure the quality of the generated\nnegative samples. Experimental results demonstrate that our approach\nsignificantly outperforms previous systems on the AggreFact-SOTA dataset,\nshowcasing its efficacy in assessing factuality in abstractive summarization.",
            "author": [
                "Haoyi Qiu",
                "Kung-Hsiang Huang",
                "Jingnong Qu",
                "Nanyun Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09521v1",
                "http://arxiv.org/pdf/2311.09521v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09518v1",
            "title": "From GPT-3 to GPT-4: On the Evolving Efficacy of LLMs to Answer\n  Multiple-choice Questions for Programming Classes in Higher Education",
            "updated": "2023-11-16T02:46:15Z",
            "published": "2023-11-16T02:46:15Z",
            "summary": "We explore the evolving efficacy of three generative pre-trained transformer\n(GPT) models in generating answers for multiple-choice questions (MCQ) from\nintroductory and intermediate Python programming courses in higher education.\nWe focus on the differences in capabilities of the models prior to the release\nof ChatGPT (Nov '22), at the time of the release, and today (i.e., Aug '23).\nRecent studies have established that the abilities of the OpenAI's GPT models\nto handle assessments originally designed for humans keep increasing as the\nnewer more capable models are released. However, the qualitative differences in\nthe capabilities and limitations of these models to reason about and/or analyze\nprogramming MCQs have been under-explored. We evaluated three OpenAI's GPT\nmodels on formative and summative MCQ assessments from three Python courses\n(530 questions) focusing on the qualitative differences in the evolving\nefficacy of the subsequent models. This study provides further evidence and\ninsight into the trajectory of the current developments where there already\nexists a technology that can be utilized by students to collect passing scores,\nwith no effort whatsoever, on what today counts as viable programming knowledge\nand skills assessments. This study could be leveraged by educators and\ninstitutions to better understand the recent technological developments in\norder to adapt the design of programming assessments as well as to fuel the\nnecessary discussions into how assessments in future programming classes should\nbe updated.",
            "author": [
                "Jaromir Savelka",
                "Arav Agarwal",
                "Christopher Bogart",
                "Majd Sakr"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09518v1",
                "http://arxiv.org/pdf/2311.09518v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09515v1",
            "title": "On the range of fractal interpolation functions",
            "updated": "2023-11-16T02:43:39Z",
            "published": "2023-11-16T02:43:39Z",
            "summary": "In this paper, based on the results from [On the localization of\nHutchinson-Barnsley fractals, Chaos Solitons Fractals, 173 (2023), 113674], we\ngenerate coverings (consisting of finite families of rhombi) of the graph of\nfractal interpolation functions. As a by-product we obtain estimations for the\nrange of such functions. Some concrete examples and graphical representations\nare provided.",
            "author": [
                "Bogdan Anghelina",
                "Radu Miculescu",
                "Mar\u00eda Antonia Navascu\u00e9s"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09515v1",
                "http://arxiv.org/pdf/2311.09515v1"
            ],
            "primary_category": "math.DS",
            "category": [
                "math.DS",
                "28A80, 41A05"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09514v1",
            "title": "Know Thy Neighbors: A Graph Based Approach for Effective Sensor-Based\n  Human Activity Recognition in Smart Homes",
            "updated": "2023-11-16T02:43:13Z",
            "published": "2023-11-16T02:43:13Z",
            "summary": "There has been a resurgence of applications focused on Human Activity\nRecognition (HAR) in smart homes, especially in the field of ambient\nintelligence and assisted living technologies. However, such applications\npresent numerous significant challenges to any automated analysis system\noperating in the real world, such as variability, sparsity, and noise in sensor\nmeasurements. Although state-of-the-art HAR systems have made considerable\nstrides in addressing some of these challenges, they especially suffer from a\npractical limitation: they require successful pre-segmentation of continuous\nsensor data streams before automated recognition, i.e., they assume that an\noracle is present during deployment, which is capable of identifying time\nwindows of interest across discrete sensor events. To overcome this limitation,\nwe propose a novel graph-guided neural network approach that performs activity\nrecognition by learning explicit co-firing relationships between sensors. We\naccomplish this by learning a more expressive graph structure representing the\nsensor network in a smart home, in a data-driven manner. Our approach maps\ndiscrete input sensor measurements to a feature space through the application\nof attention mechanisms and hierarchical pooling of node embeddings. We\ndemonstrate the effectiveness of our proposed approach by conducting several\nexperiments on CASAS datasets, showing that the resulting graph-guided neural\nnetwork outperforms the state-of-the-art method for HAR in smart homes across\nmultiple datasets and by large margins. These results are promising because\nthey push HAR for smart homes closer to real-world applications.",
            "author": [
                "Srivatsa P",
                "Thomas Pl\u00f6tz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09514v1",
                "http://arxiv.org/pdf/2311.09514v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09506v2",
            "title": "Investigating the Impact of Weight Sharing Decisions on Knowledge\n  Transfer in Continual Learning",
            "updated": "2023-11-28T05:31:06Z",
            "published": "2023-11-16T02:06:23Z",
            "summary": "Continual Learning (CL) has generated attention as a method of avoiding\nCatastrophic Forgetting (CF) in the sequential training of neural networks,\nimproving network efficiency and adaptability to different tasks. Additionally,\nCL serves as an ideal setting for studying network behavior and Forward\nKnowledge Transfer (FKT) between tasks. Pruning methods for CL train\nsubnetworks to handle the sequential tasks which allows us to take a structured\napproach to investigating FKT. Sharing prior subnetworks' weights leverages\npast knowledge for the current task through FKT. Understanding which weights to\nshare is important as sharing all weights can yield sub-optimal accuracy. This\npaper investigates how different sharing decisions affect the FKT between\ntasks. Through this lens we demonstrate how task complexity and similarity\ninfluence the optimal weight sharing decisions, giving insights into the\nrelationships between tasks and helping inform decision making in similar CL\nmethods. We implement three sequential datasets designed to emphasize variation\nin task complexity and similarity, reporting results for both ResNet-18 and\nVGG-16. By sharing in accordance with the decisions supported by our findings,\nwe show that we can improve task accuracy compared to other sharing decisions.",
            "author": [
                "Josh Andle",
                "Ali Payani",
                "Salimeh Yasaei-Sekeh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09506v2",
                "http://arxiv.org/pdf/2311.09506v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10776v2",
            "title": "Towards an Automatic AI Agent for Reaction Condition Recommendation in\n  Chemical Synthesis",
            "updated": "2023-11-28T02:21:40Z",
            "published": "2023-11-16T01:21:33Z",
            "summary": "Artificial intelligence (AI) for reaction condition optimization has become\nan important topic in the pharmaceutical industry, given that a data-driven AI\nmodel can assist drug discovery and accelerate reaction design. However,\nexisting AI models lack the chemical insights and real-time knowledge\nacquisition abilities of experienced human chemists. This paper proposes a\nLarge Language Model (LLM) empowered AI agent to bridge this gap. We put forth\na novel three-phase paradigm and applied advanced intelligence-enhancement\nmethods like in-context learning and multi-LLM debate so that the AI agent can\nborrow human insight and update its knowledge by searching the latest chemical\nliterature. Additionally, we introduce a novel Coarse-label Contrastive\nLearning (CCL) based chemical fingerprint that greatly enhances the agent's\nperformance in optimizing the reaction condition. With the above efforts, the\nproposed AI agent can autonomously generate the optimal reaction condition\nrecommendation without any human interaction. Further, the agent is highly\nprofessional in terms of chemical reactions. It demonstrates close-to-human\nperformance and strong generalization capability in both dry-lab and wet-lab\nexperiments. As the first attempt in the chemical AI agent, this work goes a\nstep further in the field of \"AI for chemistry\" and opens up new possibilities\nfor computer-aided synthesis planning.",
            "author": [
                "Kexin Chen",
                "Junyou Li",
                "Kunyi Wang",
                "Yuyang Du",
                "Jiahui Yu",
                "Jiamin Lu",
                "Lanqing Li",
                "Jiezhong Qiu",
                "Qun Fang",
                "Pheng Ann Heng",
                "Guangyong Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10776v2",
                "http://arxiv.org/pdf/2311.10776v2"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09482v1",
            "title": "Robust Conformal Prediction for STL Runtime Verification under\n  Distribution Shift",
            "updated": "2023-11-16T00:54:34Z",
            "published": "2023-11-16T00:54:34Z",
            "summary": "Cyber-physical systems (CPS) designed in simulators behave differently in the\nreal-world. Once they are deployed in the real-world, we would hence like to\npredict system failures during runtime. We propose robust predictive runtime\nverification (RPRV) algorithms under signal temporal logic (STL) tasks for\ngeneral stochastic CPS. The RPRV problem faces several challenges: (1) there\nmay not be sufficient data of the behavior of the deployed CPS, (2) predictive\nmodels are based on a distribution over system trajectories encountered during\nthe design phase, i.e., there may be a distribution shift during deployment. To\naddress these challenges, we assume to know an upper bound on the statistical\ndistance (in terms of an f-divergence) between the distributions at deployment\nand design time, and we utilize techniques based on robust conformal\nprediction. Motivated by our results in [1], we construct an accurate and an\ninterpretable RPRV algorithm. We use a trajectory prediction model to estimate\nthe system behavior at runtime and robust conformal prediction to obtain\nprobabilistic guarantees by accounting for distribution shifts. We precisely\nquantify the relationship between calibration data, desired confidence, and\npermissible distribution shift. To the best of our knowledge, these are the\nfirst statistically valid algorithms under distribution shift in this setting.\nWe empirically validate our algorithms on a Franka manipulator within the\nNVIDIA Isaac sim environment.",
            "author": [
                "Yiqi Zhao",
                "Bardh Hoxha",
                "Georgios Fainekos",
                "Jyotirmoy V. Deshmukh",
                "Lars Lindemann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09482v1",
                "http://arxiv.org/pdf/2311.09482v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LO",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09481v1",
            "title": "Personalized Jargon Identification for Enhanced Interdisciplinary\n  Communication",
            "updated": "2023-11-16T00:51:25Z",
            "published": "2023-11-16T00:51:25Z",
            "summary": "Scientific jargon can impede researchers when they read materials from other\ndomains. Current methods of jargon identification mainly use corpus-level\nfamiliarity indicators (e.g., Simple Wikipedia represents plain language).\nHowever, researchers' familiarity of a term can vary greatly based on their own\nbackground. We collect a dataset of over 10K term familiarity annotations from\n11 computer science researchers for terms drawn from 100 paper abstracts.\nAnalysis of this data reveals that jargon familiarity and information needs\nvary widely across annotators, even within the same sub-domain (e.g., NLP). We\ninvestigate features representing individual, sub-domain, and domain knowledge\nto predict individual jargon familiarity. We compare supervised and\nprompt-based approaches, finding that prompt-based methods including personal\npublications yields the highest accuracy, though zero-shot prompting provides a\nstrong baseline. This research offers insight into features and methods to\nintegrate personal data into scientific jargon identification.",
            "author": [
                "Yue Guo",
                "Joseph Chee Chang",
                "Maria Antoniak",
                "Erin Bransom",
                "Trevor Cohen",
                "Lucy Lu Wang",
                "Tal August"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09481v1",
                "http://arxiv.org/pdf/2311.09481v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09476v1",
            "title": "ARES: An Automated Evaluation Framework for Retrieval-Augmented\n  Generation Systems",
            "updated": "2023-11-16T00:39:39Z",
            "published": "2023-11-16T00:39:39Z",
            "summary": "Evaluating retrieval-augmented generation (RAG) systems traditionally relies\non hand annotations for input queries, passages to retrieve, and responses to\ngenerate. We introduce ARES, an Automated RAG Evaluation System, for evaluating\nRAG systems along the dimensions of context relevance, answer faithfulness, and\nanswer relevance. Using synthetic training data, ARES finetunes lightweight LM\njudges to assess the quality of individual RAG components. To mitigate\npotential prediction errors, ARES utilizes a small set of human-annotated\ndatapoints for prediction-powered inference (PPI). Across six different\nknowledge-intensive tasks in KILT and SuperGLUE, ARES accurately evaluates RAG\nsystems while using a few hundred human annotations during evaluation.\nFurthermore, ARES judges remain effective across domain shifts, proving\naccurate even after changing the type of queries and/or documents used in the\nevaluated RAG systems. We make our datasets and code for replication and\ndeployment available at https://github.com/stanford-futuredata/ARES.",
            "author": [
                "Jon Saad-Falcon",
                "Omar Khattab",
                "Christopher Potts",
                "Matei Zaharia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09476v1",
                "http://arxiv.org/pdf/2311.09476v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09467v1",
            "title": "Think While You Write: Hypothesis Verification Promotes Faithful\n  Knowledge-to-Text Generation",
            "updated": "2023-11-16T00:13:19Z",
            "published": "2023-11-16T00:13:19Z",
            "summary": "Neural knowledge-to-text generation models often struggle to faithfully\ngenerate descriptions for the input facts: they may produce hallucinations that\ncontradict the given facts, or describe facts not present in the input. To\nreduce hallucinations, we propose a novel decoding method, TWEAK (Think While\nEffectively Articulating Knowledge). TWEAK treats the generated sequences at\neach decoding step and its future sequences as hypotheses, and ranks each\ngeneration candidate based on how well their corresponding hypotheses support\nthe input facts using a Hypothesis Verification Model (HVM). We first\ndemonstrate the effectiveness of TWEAK by using a Natural Language Inference\n(NLI) model as the HVM and report improved faithfulness with minimal impact on\nthe quality. We then replace the NLI model with our task-specific HVM trained\nwith a first-of-a-kind dataset, FATE (Fact-Aligned Textual Entailment), which\npairs input facts with their faithful and hallucinated descriptions with the\nhallucinated spans marked. The new HVM improves the faithfulness and the\nquality further and runs faster. Overall the best TWEAK variants improve on\naverage 2.22/7.17 points on faithfulness measured by FactKB over WebNLG and\nTekGen/GenWiki, respectively, with only 0.14/0.32 points degradation on quality\nmeasured by BERTScore over the same datasets. Since TWEAK is a decoding-only\napproach, it can be integrated with any neural generative model without\nretraining.",
            "author": [
                "Yifu Qiu",
                "Varun Embar",
                "Shay B. Cohen",
                "Benjamin Han"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09467v1",
                "http://arxiv.org/pdf/2311.09467v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09411v1",
            "title": "Heteroskedasticity as a Signature of Association for Age-Related Genes",
            "updated": "2023-11-15T22:26:32Z",
            "published": "2023-11-15T22:26:32Z",
            "summary": "Human aging is a process controlled by both genetics and environment. Many\nstudies have been conducted to identify a subset of genes related to aging from\nthe human genome. Biologists implicitly categorize age-related genes into genes\nthat cause aging and genes that are influenced by aging, which resulted in both\ncausal inference and inference of associations studies. While inference of\nassociation is better explored, causal inference and computational causal\ninference, remains less explored. In this work, we are primarily motivated to\ntackle the problem of identifying genes associated with aging, while having a\nbrief look into genes with probable causal relations, both from a computational\nperspective. Specifically, we form a set of hypotheses and accordingly,\nintroduce a data-tailored framework for inference. First we perform linear\nmodeling on the expression values of age-related genes, and then examine the\npresence of heteroskedastic properties in the residual of the model. We\nevaluate this framework and our results suggest that, 1) presence of\nheteroskedasticity in these residuals is a potential signature of association\nfor age-related genes, and 2) consistent heteroskedasticity along the human\nlife span could imply some sort of causality. To our knowledge, along with\nidentifying age-associated genes, this is the first work to propose a framework\nfor computational causal inference on age-related genes, using a dataset of\nhuman dermal fibroblast gene expression data. Hence the results of our simple,\nyet effective approach can be used not only to assess future age-related genes,\nbut also as a possible criterion to select new associative or potential causal\ngenes with respect to aging.",
            "author": [
                "Salman Mohamadi",
                "Donald A. Adjeroh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09411v1",
                "http://arxiv.org/pdf/2311.09411v1"
            ],
            "primary_category": "q-bio.GN",
            "category": [
                "q-bio.GN",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10112v1",
            "title": "Zero-Shot Relational Learning on Temporal Knowledge Graphs with Large\n  Language Models",
            "updated": "2023-11-15T21:25:15Z",
            "published": "2023-11-15T21:25:15Z",
            "summary": "In recent years, modeling evolving knowledge over temporal knowledge graphs\n(TKGs) has become a heated topic. Various methods have been proposed to\nforecast links on TKGs. Most of them are embedding-based, where hidden\nrepresentations are learned to represent knowledge graph (KG) entities and\nrelations based on the observed graph contexts. Although these methods show\nstrong performance on traditional TKG forecasting (TKGF) benchmarks, they\nnaturally face a strong challenge when they are asked to model the unseen\nzero-shot relations that has no prior graph context. In this paper, we try to\nmitigate this problem as follows. We first input the text descriptions of KG\nrelations into large language models (LLMs) for generating relation\nrepresentations, and then introduce them into embedding-based TKGF methods.\nLLM-empowered representations can capture the semantic information in the\nrelation descriptions. This makes the relations, whether seen or unseen, with\nsimilar semantic meanings stay close in the embedding space, enabling TKGF\nmodels to recognize zero-shot relations even without any observed graph\ncontext. Experimental results show that our approach helps TKGF models to\nachieve much better performance in forecasting the facts with previously unseen\nrelations, while still maintaining their ability in link forecasting regarding\nseen relations.",
            "author": [
                "Zifeng Ding",
                "Heling Cai",
                "Jingpei Wu",
                "Yunpu Ma",
                "Ruotong Liao",
                "Bo Xiong",
                "Volker Tresp"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10112v1",
                "http://arxiv.org/pdf/2311.10112v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09383v1",
            "title": "Long-form Question Answering: An Iterative Planning-Retrieval-Generation\n  Approach",
            "updated": "2023-11-15T21:22:27Z",
            "published": "2023-11-15T21:22:27Z",
            "summary": "Long-form question answering (LFQA) poses a challenge as it involves\ngenerating detailed answers in the form of paragraphs, which go beyond simple\nyes/no responses or short factual answers. While existing QA models excel in\nquestions with concise answers, LFQA requires handling multiple topics and\ntheir intricate relationships, demanding comprehensive explanations. Previous\nattempts at LFQA focused on generating long-form answers by utilizing relevant\ncontexts from a corpus, relying solely on the question itself. However, they\noverlooked the possibility that the question alone might not provide sufficient\ninformation to identify the relevant contexts. Additionally, generating\ndetailed long-form answers often entails aggregating knowledge from diverse\nsources. To address these limitations, we propose an LFQA model with iterative\nPlanning, Retrieval, and Generation. This iterative process continues until a\ncomplete answer is generated for the given question. From an extensive\nexperiment on both an open domain and a technical domain QA dataset, we find\nthat our model outperforms the state-of-the-art models on various textual and\nfactual metrics for the LFQA task.",
            "author": [
                "Pritom Saha Akash",
                "Kashob Kumar Roy",
                "Lucian Popa",
                "Kevin Chen-Chuan Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09383v1",
                "http://arxiv.org/pdf/2311.09383v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09375v1",
            "title": "HypOp: Distributed Constrained Combinatorial Optimization leveraging\n  Hypergraph Neural Networks",
            "updated": "2023-11-15T21:06:49Z",
            "published": "2023-11-15T21:06:49Z",
            "summary": "Scalable addressing of high dimensional constrained combinatorial\noptimization problems is a challenge that arises in several science and\nengineering disciplines. Recent work introduced novel application of graph\nneural networks for solving polynomial-cost unconstrained combinatorial\noptimization problems. This paper proposes a new framework, called HypOp, which\ngreatly advances the state of the art for solving combinatorial optimization\nproblems in several aspects: (i) it generalizes the prior results to\nconstrained optimization problems with an arbitrary cost function; (ii) it\nbroadens the application to higher dimensional problems by leveraging a\nhypergraph neural network structure; (iii) it enables scalability to much\nlarger problems by introducing a new distributed and parallel architecture for\nhypergraph neural network training; (iv) it demonstrates generalizability to\nother problem formulations by knowledge transfer from the learned experience of\naddressing one set of cost/constraints to another set for the same hypergraph;\n(v) it significantly boosts the solution accuracy compared with the prior art\nby suggesting a fine-tuning step using simulated annealing; (vi) HypOp shows a\nremarkable progress on benchmark examples, with run times improved by up to\nfivefold using a combination of fine-tuning and distributed training\ntechniques. The framework allows addressing a novel set of scientific problems\nincluding hypergraph MaxCut problem, satisfiability problems (3SAT), and\nresource allocation. We showcase the application of HypOp in scientific\ndiscovery by solving a hypergraph MaxCut problem on the NDC drug-substance\nhypergraph. Through extensive experimentation on a variety of combinatorial\noptimization problems, HypOp demonstrates superiority over existing\nunsupervised learning-based solvers and generic optimization methods.",
            "author": [
                "Nasimeh Heydaribeni",
                "Xinrui Zhan",
                "Ruisi Zhang",
                "Tina Eliassi-Rad",
                "Farinaz Koushanfar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09375v1",
                "http://arxiv.org/pdf/2311.09375v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09366v1",
            "title": "LOKE: Linked Open Knowledge Extraction for Automated Knowledge Graph\n  Construction",
            "updated": "2023-11-15T20:57:44Z",
            "published": "2023-11-15T20:57:44Z",
            "summary": "While the potential of Open Information Extraction (Open IE) for Knowledge\nGraph Construction (KGC) may seem promising, we find that the alignment of Open\nIE extraction results with existing knowledge graphs to be inadequate. The\nadvent of Large Language Models (LLMs), especially the commercially available\nOpenAI models, have reset expectations for what is possible with deep learning\nmodels and have created a new field called prompt engineering. We investigate\nthe use of GPT models and prompt engineering for knowledge graph construction\nwith the Wikidata knowledge graph to address a similar problem to Open IE,\nwhich we call Open Knowledge Extraction (OKE) using an approach we call the\nLinked Open Knowledge Extractor (LOKE, pronounced like \"Loki\"). We consider the\nentity linking task essential to construction of real world knowledge graphs.\nWe merge the CaRB benchmark scoring approach with data from the TekGen dataset\nfor the LOKE task. We then show that a well engineered prompt, paired with a\nnaive entity linking approach (which we call LOKE-GPT), outperforms AllenAI's\nOpenIE 4 implementation on the OKE task, although it over-generates triples\ncompared to the reference set due to overall triple scarcity in the TekGen set.\nThrough an analysis of entity linkability in the CaRB dataset, as well as\noutputs from OpenIE 4 and LOKE-GPT, we see that LOKE-GPT and the \"silver\"\nTekGen triples show that the task is significantly different in content from\nOIE, if not structure. Through this analysis and a qualitative analysis of\nsentence extractions via all methods, we found that LOKE-GPT extractions are of\nhigh utility for the KGC task and suitable for use in semi-automated extraction\nsettings.",
            "author": [
                "Jamie McCusker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09366v1",
                "http://arxiv.org/pdf/2311.09366v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09364v1",
            "title": "The Structure of Metrizable Graphs",
            "updated": "2023-11-15T20:54:41Z",
            "published": "2023-11-15T20:54:41Z",
            "summary": "A consistent path system in a graph $G$ is an intersection-closed collection\nof paths, with exactly one path between any two vertices in $G$. We call $G$\nmetrizable if every consistent path system in it is the system of geodesic\npaths defined by assigning some positive lengths to its edges. We show that\nmetrizable graphs are, in essence, subdivisions of a small family of basic\ngraphs with additional compliant edges. In particular, we show that every\nmetrizable graph with 11 vertices or more is outerplanar plus one vertex.",
            "author": [
                "Maria Chudnovsky",
                "Daniel Cizma",
                "Nati Linial"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09364v1",
                "http://arxiv.org/pdf/2311.09364v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09359v1",
            "title": "Local Computation Algorithms for Maximum Matching: New Lower Bounds",
            "updated": "2023-11-15T20:43:46Z",
            "published": "2023-11-15T20:43:46Z",
            "summary": "We study local computation algorithms (LCA) for maximum matching. An LCA does\nnot return its output entirely, but reveals parts of it upon query. For\nmatchings, each query is a vertex $v$; the LCA should return whether $v$ is\nmatched -- and if so to which neighbor -- while spending a small time per\nquery.\n  In this paper, we prove that any LCA that computes a matching that is at most\nan additive of $\\epsilon n$ smaller than the maximum matching in $n$-vertex\ngraphs of maximum degree $\\Delta$ must take at least\n$\\Delta^{\\Omega(1/\\varepsilon)}$ time. This comes close to the existing upper\nbounds that take $(\\Delta/\\epsilon)^{O(1/\\epsilon^2)} polylog(n)$ time.\n  In terms of sublinear time algorithms, our techniques imply that any\nalgorithm that estimates the size of maximum matching up to an additive error\nof $\\epsilon n$ must take $\\Delta^{\\Omega(1/\\epsilon)}$ time. This negatively\nresolves a decade old open problem of the area (see Open Problem 39 of\nsublinear.info) on whether such estimates can be achieved in\n$poly(\\Delta/\\epsilon)$ time.",
            "author": [
                "Soheil Behnezhad",
                "Mohammad Roghani",
                "Aviad Rubinstein"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09359v1",
                "http://arxiv.org/pdf/2311.09359v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09358v1",
            "title": "Empirical evaluation of Uncertainty Quantification in\n  Retrieval-Augmented Language Models for Science",
            "updated": "2023-11-15T20:42:11Z",
            "published": "2023-11-15T20:42:11Z",
            "summary": "Large language models (LLMs) have shown remarkable achievements in natural\nlanguage processing tasks, producing high-quality outputs. However, LLMs still\nexhibit limitations, including the generation of factually incorrect\ninformation. In safety-critical applications, it is important to assess the\nconfidence of LLM-generated content to make informed decisions. Retrieval\nAugmented Language Models (RALMs) is relatively a new area of research in NLP.\nRALMs offer potential benefits for scientific NLP tasks, as retrieved\ndocuments, can serve as evidence to support model-generated content. This\ninclusion of evidence enhances trustworthiness, as users can verify and explore\nthe retrieved documents to validate model outputs. Quantifying uncertainty in\nRALM generations further improves trustworthiness, with retrieved text and\nconfidence scores contributing to a comprehensive and reliable model for\nscientific applications. However, there is limited to no research on UQ for\nRALMs, particularly in scientific contexts. This study aims to address this gap\nby conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific\ntasks. This research investigates how uncertainty scores vary when scientific\nknowledge is incorporated as pretraining and retrieval data and explores the\nrelationship between uncertainty scores and the accuracy of model-generated\noutputs. We observe that an existing RALM finetuned with scientific knowledge\nas the retrieval data tends to be more confident in generating predictions\ncompared to the model pretrained only with scientific knowledge. We also found\nthat RALMs are overconfident in their predictions, making inaccurate\npredictions more confidently than accurate ones. Scientific knowledge provided\neither as pretraining or retrieval corpus does not help alleviate this issue.\nWe released our code, data and dashboards at https://github.com/pnnl/EXPERT2.",
            "author": [
                "Sridevi Wagle",
                "Sai Munikoti",
                "Anurag Acharya",
                "Sara Smith",
                "Sameera Horawalavithana"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09358v1",
                "http://arxiv.org/pdf/2311.09358v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09353v1",
            "title": "Flexible and Adaptive Manufacturing by Complementing Knowledge\n  Representation, Reasoning and Planning with Reinforcement Learning",
            "updated": "2023-11-15T20:28:27Z",
            "published": "2023-11-15T20:28:27Z",
            "summary": "This paper describes a novel approach to adaptive manufacturing in the\ncontext of small batch production and customization. It focuses on integrating\ntask-level planning and reasoning with reinforcement learning (RL) in the\nSkiROS2 skill-based robot control platform. This integration enhances the\nefficiency and adaptability of robotic systems in manufacturing, enabling them\nto adjust to task variations and learn from interaction data. The paper\nhighlights the architecture of SkiROS2, particularly its world model, skill\nlibraries, and task management. It demonstrates how combining RL with robotic\nmanipulators can learn and improve the execution of industrial tasks. It\nadvocates a multi-objective learning model that eases the learning problem\ndesign. The approach can incorporate user priors or previous experiences to\naccelerate learning and increase safety.\n  Spotlight video: https://youtu.be/H5PmZl2rRbs?si=8wmZ-gbwuSJRxe3S&t=1422\n  SkiROS2 code: https://github.com/RVMI/skiros2\n  SkiROS2 talk at ROSCon: https://vimeo.com/879001825/2a0e9d5412\n  SkiREIL code: https://github.com/matthias-mayr/SkiREIL",
            "author": [
                "Matthias Mayr",
                "Faseeh Ahmad",
                "Volker Krueger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09353v1",
                "http://arxiv.org/pdf/2311.09353v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09335v1",
            "title": "Lighter, yet More Faithful: Investigating Hallucinations in Pruned Large\n  Language Models for Abstractive Summarization",
            "updated": "2023-11-15T19:49:24Z",
            "published": "2023-11-15T19:49:24Z",
            "summary": "Despite their remarkable performance on abstractive summarization, large\nlanguage models (LLMs) face two significant challenges: their considerable size\nand tendency to hallucinate. Hallucinations are concerning because they erode\nthe reliability of LLMs and raise safety issues. Pruning is a technique that\nreduces model size by removing redundant weights to create sparse models that\nenable more efficient inference. Pruned models yield comparable performance to\ntheir counterpart full-sized models, making them ideal alternatives when\noperating on a limited budget. However, the effect that pruning has upon\nhallucinations in abstractive summarization with LLMs has yet to be explored.\nIn this paper, we provide an extensive empirical study on the hallucinations\nproduced by pruned models across three standard summarization tasks, two\npruning approaches, three instruction-tuned LLMs, and three hallucination\nevaluation metrics. Surprisingly, we find that pruned LLMs hallucinate less\ncompared to their full-sized counterparts. Our follow-up analysis suggests that\npruned models tend to depend more on the source input and less on their\nparametric knowledge from pre-training for generation. This greater dependency\non the source input leads to a higher lexical overlap between generated content\nand the source input, which can be a reason for the reduction in\nhallucinations.",
            "author": [
                "George Chrysostomou",
                "Zhixue Zhao",
                "Miles Williams",
                "Nikolaos Aletras"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09335v1",
                "http://arxiv.org/pdf/2311.09335v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09317v1",
            "title": "Connectivity threshold for superpositions of Bernoulli random graphs. II",
            "updated": "2023-11-15T19:23:08Z",
            "published": "2023-11-15T19:23:08Z",
            "summary": "Let $G_1,\\dots, G_m$ be independent Bernoulli random subgraphs of the\ncomplete graph ${\\cal K}_n$ having variable sizes $X_1,\\dots, X_m\\in\n\\{0,1,2,\\dots\\}$ and densities $Q_1,\\dots, Q_m\\in [0,1]$. Letting\n$n,m\\to+\\infty$ we establish the connectivity threshold for the union\n$\\cup_{i=1}^mG_i$ defined on the vertex set of ${\\cal K}_n$. Assuming that\n$(X_1,Q_1), (X_2,Q_2),\\dots, (X_m,Q_m)$ are independent identically distributed\nbivariate random variables and $\\ln n\n-\\frac{m}{n}E\\bigl(X_1(1-(1-Q_1)^{|X_1-1|}\\bigr)\\to c$ we show that\n$P\\{\\cup_{i=1}^mG_i$ is connected$\\}\\to e^{-e^c}$.The result extends to the\ncase of non-identically distributed random variables $(X_1,Q_1),\\dots,\n(X_m,Q_m)$ as well.",
            "author": [
                "Mindaugas Bloznelis",
                "Dominykas Marma",
                "Rimantas Vaicekauskas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09317v1",
                "http://arxiv.org/pdf/2311.09317v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO",
                "05C80, 05C82, 05C40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09314v1",
            "title": "Multimatroids and rational curves with cyclic action",
            "updated": "2023-11-15T19:15:07Z",
            "published": "2023-11-15T19:15:07Z",
            "summary": "We study the connection between multimatroids and moduli spaces of rational\ncurves with cyclic action. Multimatroids are generalizations of matroids and\ndelta-matroids introduced by Bouchet, which naturally arise in topological\ngraph theory. The vantage point of moduli of curves provides a tropical\nframework for studying multimatroids, generalizing the previous connection\nbetween type-A permutohedral varieties (Losev--Manin moduli spaces) and\nmatroids, and the connection between type-B permutohedral varieties\n(Batyrev--Blume moduli spaces) and delta-matroids. Specifically, we equate a\ncombinatorial nef cone of the moduli space with the space of\n$\\mathbb{R}$-multimatroids, a slight generalization of multimatroids, and we\nintroduce the independence polytopal complex of a multimatroid, whose volume is\nidentified with an intersection number on the moduli space. As an application,\nfor the generating set of the Chow ring of the moduli space consisting of all\npsi-classes and their pullbacks along certain forgetful maps, we give a\ncombinatorial formula for their intersection numbers by relating to the volumes\nof independence polytopal complexes of multimatroids.",
            "author": [
                "Emily Clader",
                "Chiara Damiolini",
                "Christopher Eur",
                "Daoji Huang",
                "Shiyue Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09314v1",
                "http://arxiv.org/pdf/2311.09314v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09308v1",
            "title": "Divergences between Language Models and Human Brains",
            "updated": "2023-11-15T19:02:40Z",
            "published": "2023-11-15T19:02:40Z",
            "summary": "Do machines and humans process language in similar ways? A recent line of\nresearch has hinted in the affirmative, demonstrating that human brain signals\ncan be effectively predicted using the internal representations of language\nmodels (LMs). This is thought to reflect shared computational principles\nbetween LMs and human language processing. However, there are also clear\ndifferences in how LMs and humans acquire and use language, even if the final\ntask they are performing is the same. Despite this, there is little work\nexploring systematic differences between human and machine language processing\nusing brain data. To address this question, we examine the differences between\nLM representations and the human brain's responses to language, specifically by\nexamining a dataset of Magnetoencephalography (MEG) responses to a written\nnarrative. In doing so we identify three phenomena that, in prior work, LMs\nhave been found to not capture well: emotional understanding, figurative\nlanguage processing, and physical commonsense. By fine-tuning LMs on datasets\nrelated to these phenomena, we observe that fine-tuned LMs show improved\nalignment with human brain responses across these tasks. Our study implies that\nthe observed divergences between LMs and human brains may stem from LMs'\ninadequate representation of these specific types of knowledge.",
            "author": [
                "Yuchen Zhou",
                "Emmy Liu",
                "Graham Neubig",
                "Leila Wehbe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09308v1",
                "http://arxiv.org/pdf/2311.09308v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG",
                "q-bio.NC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09278v1",
            "title": "Symbol-LLM: Towards Foundational Symbol-centric Interface For Large\n  Language Models",
            "updated": "2023-11-15T18:59:56Z",
            "published": "2023-11-15T18:59:56Z",
            "summary": "Large Language Models (LLMs) have greatly propelled the progress in natural\nlanguage(NL)-centric tasks based on NL interface. However, the NL form is not\nenough for world knowledge. Current works focus on this question by injecting\nspecific symbolic knowledge into LLM, which ignore two critical challenges: the\ninterrelations between various symbols and the balance between symbolic-centric\nand NL-centric capabilities. In this work, we tackle these challenges from both\na data and framework perspective and introduce Symbol-LLM series models. First,\nwe collect 34 symbolic tasks, covering ~20 different forms, which are unified\nto capture symbol interrelations. Then, a two-stage tuning framework succeeds\nin injecting symbolic knowledge without loss of the generality ability.\nExtensive experiments on both symbol- and NL-centric tasks demonstrate the\nbalanced and superior performances of Symbol-LLM series models.",
            "author": [
                "Fangzhi Xu",
                "Zhiyong Wu",
                "Qiushi Sun",
                "Siyu Ren",
                "Fei Yuan",
                "Shuai Yuan",
                "Qika Lin",
                "Yu Qiao",
                "Jun Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09278v1",
                "http://arxiv.org/pdf/2311.09278v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09214v1",
            "title": "Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive\n  Thinking from Large Language Models",
            "updated": "2023-11-15T18:56:23Z",
            "published": "2023-11-15T18:56:23Z",
            "summary": "Large language models (LLMs) have achieved remarkable advancements in the\nfield of natural language processing. However, the sheer scale and\ncomputational demands of these models present formidable challenges when\nconsidering their practical deployment in resource-constrained contexts. While\ntechniques such as chain-of-thought (CoT) distillation have displayed promise\nin distilling LLMs into small language models (SLMs), there is a risk that\ndistilled SLMs may still carry over flawed reasoning or hallucinations\ninherited from their LLM counterparts. To address these issues, we propose a\ntwofold methodology: First, we introduce a novel method for distilling the\nself-evaluation capability inherent in LLMs into SLMs, which aims to mitigate\nthe adverse effects of erroneous reasoning and reduce hallucinations. Second,\nwe advocate for a comprehensive distillation process that incorporates multiple\ndistinct chain-of-thought and self-evaluation paradigms and ensures a more\nholistic and robust knowledge transfer into SLMs. Experiments on three NLP\nbenchmarks demonstrate that our method significantly improves the performance\nof distilled SLMs and sheds light on the path towards developing smaller models\nclosely aligned with human cognition.",
            "author": [
                "Weize Liu",
                "Guocong Li",
                "Kai Zhang",
                "Bang Du",
                "Qiyuan Chen",
                "Xuming Hu",
                "Hongxia Xu",
                "Jintai Chen",
                "Jian Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09214v1",
                "http://arxiv.org/pdf/2311.09214v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09213v1",
            "title": "GRIM: GRaph-based Interactive narrative visualization for gaMes",
            "updated": "2023-11-15T18:55:45Z",
            "published": "2023-11-15T18:55:45Z",
            "summary": "Dialogue-based Role Playing Games (RPGs) require powerful storytelling. The\nnarratives of these may take years to write and typically involve a large\ncreative team. In this work, we demonstrate the potential of large generative\ntext models to assist this process. \\textbf{GRIM}, a prototype\n\\textbf{GR}aph-based \\textbf{I}nteractive narrative visualization system for\nga\\textbf{M}es, generates a rich narrative graph with branching storylines that\nmatch a high-level narrative description and constraints provided by the\ndesigner. Game designers can interactively edit the graph by automatically\ngenerating new sub-graphs that fit the edits within the original narrative and\nconstraints. We illustrate the use of \\textbf{GRIM} in conjunction with GPT-4,\ngenerating branching narratives for four well-known stories with different\ncontextual constraints.",
            "author": [
                "Jorge Leandro",
                "Sudha Rao",
                "Michael Xu",
                "Weijia Xu",
                "Nebosja Jojic",
                "Chris Brockett",
                "Bill Dolan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09213v1",
                "http://arxiv.org/pdf/2311.09213v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09210v1",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language\n  Models",
            "updated": "2023-11-15T18:54:53Z",
            "published": "2023-11-15T18:54:53Z",
            "summary": "Retrieval-augmented language models (RALMs) represent a substantial\nadvancement in the capabilities of large language models, notably in reducing\nfactual hallucination by leveraging external knowledge sources. However, the\nreliability of the retrieved information is not always guaranteed. The\nretrieval of irrelevant data can lead to misguided responses, and potentially\ncausing the model to overlook its inherent knowledge, even when it possesses\nadequate information to address the query. Moreover, standard RALMs often\nstruggle to assess whether they possess adequate knowledge, both intrinsic and\nretrieved, to provide an accurate answer. In situations where knowledge is\nlacking, these systems should ideally respond with \"unknown\" when the answer is\nunattainable. In response to these challenges, we introduces Chain-of-Noting\n(CoN), a novel approach aimed at improving the robustness of RALMs in facing\nnoisy, irrelevant documents and in handling unknown scenarios. The core idea of\nCoN is to generate sequential reading notes for retrieved documents, enabling a\nthorough evaluation of their relevance to the given question and integrating\nthis information to formulate the final answer. We employed ChatGPT to create\ntraining data for CoN, which was subsequently trained on an LLaMa-2 7B model.\nOur experiments across four open-domain QA benchmarks show that RALMs equipped\nwith CoN significantly outperform standard RALMs. Notably, CoN achieves an\naverage improvement of +7.9 in EM score given entirely noisy retrieved\ndocuments and +10.5 in rejection rates for real-time questions that fall\noutside the pre-training knowledge scope.",
            "author": [
                "Wenhao Yu",
                "Hongming Zhang",
                "Xiaoman Pan",
                "Kaixin Ma",
                "Hongwei Wang",
                "Dong Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09210v1",
                "http://arxiv.org/pdf/2311.09210v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09201v1",
            "title": "Topology of Pulsar Profiles (ToPP). I. Graph theory method and\n  classification of the EPN",
            "updated": "2023-11-15T18:44:19Z",
            "published": "2023-11-15T18:44:19Z",
            "summary": "Some of the most important information on a radio pulsar is derived from its\naverage pulse profile. Many early pulsar studies were necessarily based on only\nfew such profiles. There, discrete profile components were linked to emission\nmechanism models for individual stars through human interpretation. For the\npopulation as a whole, profiles morphology must reflect the geometry and\noverall evolution of the radio emitting regions. The problem, however, is that\nthis population is becoming too large for intensive studies of all sources\nindividually. Moreover, connecting profiles from a large collection of pulsars\nrapidly becomes cumbersome. In this article, we present ToPP, the first-ever\nunsupervised method to sort pulsars by profile-shape similarity, using graph\ntopology. We apply ToPP to the publicly available European Pulsar Network\nprofile database, providing the first organised visual overview of\nmulti-frequency profiles representing 90 individual pulsars. We find discrete\nevolutionary tracks, varying from simple, single component profiles at all\nfrequencies, towards diverse mixtures of more complex profiles with frequency\nevolution. The profile evolution is continuous, extending out to millisecond\npulsars, and does not fall in sharp classes. We interpret the profiles as a\nmixture of pulsar core/cone emission type, spin-down energetics, and the\nline-of-sight impact angle towards the magnetic axis. We show how ToPP can\nsystematically classify sources into the Rankin empirical profile scheme. ToPP\nforms one of the key unsupervised methods that will be essential to explore\nupcoming pulsar census data such as expected by the Square Kilometer Array.",
            "author": [
                "D. Vohl",
                "J. van Leeuwen",
                "Y. Maan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09201v1",
                "http://arxiv.org/pdf/2311.09201v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09197v1",
            "title": "A Unified Approach to Learning Ising Models: Beyond Independence and\n  Bounded Width",
            "updated": "2023-11-15T18:41:19Z",
            "published": "2023-11-15T18:41:19Z",
            "summary": "We revisit the problem of efficiently learning the underlying parameters of\nIsing models from data. Current algorithmic approaches achieve essentially\noptimal sample complexity when given i.i.d. samples from the stationary measure\nand the underlying model satisfies \"width\" bounds on the total $\\ell_1$\ninteraction involving each node. We show that a simple existing approach based\non node-wise logistic regression provably succeeds at recovering the underlying\nmodel in several new settings where these assumptions are violated:\n  (1) Given dynamically generated data from a wide variety of local Markov\nchains, like block or round-robin dynamics, logistic regression recovers the\nparameters with optimal sample complexity up to $\\log\\log n$ factors. This\ngeneralizes the specialized algorithm of Bresler, Gamarnik, and Shah [IEEE\nTrans. Inf. Theory'18] for structure recovery in bounded degree graphs from\nGlauber dynamics.\n  (2) For the Sherrington-Kirkpatrick model of spin glasses, given\n$\\mathsf{poly}(n)$ independent samples, logistic regression recovers the\nparameters in most of the known high-temperature regime via a simple reduction\nto weaker structural properties of the measure. This improves on recent work of\nAnari, Jain, Koehler, Pham, and Vuong [ArXiv'23] which gives distribution\nlearning at higher temperature.\n  (3) As a simple byproduct of our techniques, logistic regression achieves an\nexponential improvement in learning from samples in the M-regime of data\nconsidered by Dutt, Lokhov, Vuffray, and Misra [ICML'21] as well as novel\nguarantees for learning from the adversarial Glauber dynamics of Chin, Moitra,\nMossel, and Sandon [ArXiv'23].\n  Our approach thus significantly generalizes the elegant analysis of Wu,\nSanghavi, and Dimakis [Neurips'19] without any algorithmic modification.",
            "author": [
                "Jason Gaitonde",
                "Elchanan Mossel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09197v1",
                "http://arxiv.org/pdf/2311.09197v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DS",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09195v1",
            "title": "Self-Supervised Curriculum Generation for Autonomous Reinforcement\n  Learning without Task-Specific Knowledge",
            "updated": "2023-11-15T18:40:10Z",
            "published": "2023-11-15T18:40:10Z",
            "summary": "A significant bottleneck in applying current reinforcement learning\nalgorithms to real-world scenarios is the need to reset the environment between\nevery episode. This reset process demands substantial human intervention,\nmaking it difficult for the agent to learn continuously and autonomously.\nSeveral recent works have introduced autonomous reinforcement learning (ARL)\nalgorithms that generate curricula for jointly training reset and forward\npolicies. While their curricula can reduce the number of required manual resets\nby taking into account the agent's learning progress, they rely on\ntask-specific knowledge, such as predefined initial states or reset reward\nfunctions. In this paper, we propose a novel ARL algorithm that can generate a\ncurriculum adaptive to the agent's learning progress without task-specific\nknowledge. Our curriculum empowers the agent to autonomously reset to diverse\nand informative initial states. To achieve this, we introduce a success\ndiscriminator that estimates the success probability from each initial state\nwhen the agent follows the forward policy. The success discriminator is trained\nwith relabeled transitions in a self-supervised manner. Our experimental\nresults demonstrate that our ARL algorithm can generate an adaptive curriculum\nand enable the agent to efficiently bootstrap to solve sparse-reward maze\nnavigation tasks, outperforming baselines with significantly fewer manual\nresets.",
            "author": [
                "Sang-Hyun Lee",
                "Seung-Woo Seo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09195v1",
                "http://arxiv.org/pdf/2311.09195v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09194v1",
            "title": "Structural Priming Demonstrates Abstract Grammatical Representations in\n  Multilingual Language Models",
            "updated": "2023-11-15T18:39:56Z",
            "published": "2023-11-15T18:39:56Z",
            "summary": "Abstract grammatical knowledge - of parts of speech and grammatical patterns\n- is key to the capacity for linguistic generalization in humans. But how\nabstract is grammatical knowledge in large language models? In the human\nliterature, compelling evidence for grammatical abstraction comes from\nstructural priming. A sentence that shares the same grammatical structure as a\npreceding sentence is processed and produced more readily. Because confounds\nexist when using stimuli in a single language, evidence of abstraction is even\nmore compelling from crosslingual structural priming, where use of a syntactic\nstructure in one language primes an analogous structure in another language. We\nmeasure crosslingual structural priming in large language models, comparing\nmodel behavior to human experimental results from eight crosslingual\nexperiments covering six languages, and four monolingual structural priming\nexperiments in three non-English languages. We find evidence for abstract\nmonolingual and crosslingual grammatical representations in the models that\nfunction similarly to those found in humans. These results demonstrate that\ngrammatical representations in multilingual language models are not only\nsimilar across languages, but they can causally influence text produced in\ndifferent languages.",
            "author": [
                "James A. Michaelov",
                "Catherine Arnett",
                "Tyler A. Chang",
                "Benjamin K. Bergen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09194v1",
                "http://arxiv.org/pdf/2311.09194v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09183v1",
            "title": "The union of independent USFs on $\\mathbb{Z}^d$ is transient",
            "updated": "2023-11-15T18:24:33Z",
            "published": "2023-11-15T18:24:33Z",
            "summary": "We show that the union of two or more independent uniform spanning forests\n(USF) on $\\mathbb{Z}^d$ with $d\\geq 3$ almost surely forms a connected\ntransient graph. In fact, this also holds when taking the union of a\ndeterministic everywhere percolating set and an independent\n$\\epsilon$-Bernoulli percolation on a single USF sample.",
            "author": [
                "Eleanor Archer",
                "Asaf Nachmias",
                "Matan Shalev",
                "Pengfei Tang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09183v1",
                "http://arxiv.org/pdf/2311.09183v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09180v1",
            "title": "PEARL: Personalizing Large Language Model Writing Assistants with\n  Generation-Calibrated Retrievers",
            "updated": "2023-11-15T18:19:58Z",
            "published": "2023-11-15T18:19:58Z",
            "summary": "Powerful large language models have facilitated the development of writing\nassistants that promise to significantly improve the quality and efficiency of\ncomposition and communication. However, a barrier to effective assistance is\nthe lack of personalization in LLM outputs to the author's communication style\nand specialized knowledge. In this paper, we address this challenge by\nproposing PEARL, a retrieval-augmented LLM writing assistant personalized with\na generation-calibrated retriever. Our retriever is trained to select historic\nuser-authored documents for prompt augmentation, such that they are likely to\nbest personalize LLM generations for a user request. We propose two key\nnovelties for training our retriever: 1) A training data selection method that\nidentifies user requests likely to benefit from personalization and documents\nthat provide that benefit; and 2) A scale-calibrating KL-divergence objective\nthat ensures that our retriever closely tracks the benefit of a document for\npersonalized generation. We demonstrate the effectiveness of PEARL in\ngenerating personalized workplace social media posts and Reddit comments.\nFinally, we showcase the potential of a generation-calibrated retriever to\ndouble as a performance predictor and further improve low-quality generations\nvia LLM chaining.",
            "author": [
                "Sheshera Mysore",
                "Zhuoran Lu",
                "Mengting Wan",
                "Longqi Yang",
                "Steve Menezes",
                "Tina Baghaee",
                "Emmanuel Barajas Gonzalez",
                "Jennifer Neville",
                "Tara Safavi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09180v1",
                "http://arxiv.org/pdf/2311.09180v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.HC",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10768v1",
            "title": "Memory Augmented Language Models through Mixture of Word Experts",
            "updated": "2023-11-15T18:19:56Z",
            "published": "2023-11-15T18:19:56Z",
            "summary": "Scaling up the number of parameters of language models has proven to be an\neffective approach to improve performance. For dense models, increasing model\nsize proportionally increases the model's computation footprint. In this work,\nwe seek to aggressively decouple learning capacity and FLOPs through\nMixture-of-Experts (MoE) style models with large knowledge-rich vocabulary\nbased routing functions and experts. Our proposed approach, dubbed Mixture of\nWord Experts (MoWE), can be seen as a memory augmented model, where a large set\nof word-specific experts play the role of a sparse memory. We demonstrate that\nMoWE performs significantly better than the T5 family of models with similar\nnumber of FLOPs in a variety of NLP tasks. Additionally, MoWE outperforms\nregular MoE models on knowledge intensive tasks and has similar performance to\nmore complex memory augmented approaches that often require to invoke custom\nmechanisms to search the sparse memory.",
            "author": [
                "Cicero Nogueira dos Santos",
                "James Lee-Thorp",
                "Isaac Noble",
                "Chung-Ching Chang",
                "David Uthus"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10768v1",
                "http://arxiv.org/pdf/2311.10768v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09174v2",
            "title": "AbsPyramid: Benchmarking the Abstraction Ability of Language Models with\n  a Unified Entailment Graph",
            "updated": "2023-11-16T08:20:19Z",
            "published": "2023-11-15T18:11:23Z",
            "summary": "Cognitive research indicates that abstraction ability is essential in human\nintelligence, which remains under-explored in language models. In this paper,\nwe present AbsPyramid, a unified entailment graph of 221K textual descriptions\nof abstraction knowledge. While existing resources only touch nouns or verbs\nwithin simplified events or specific domains, AbsPyramid collects abstract\nknowledge for three components of diverse events to comprehensively evaluate\nthe abstraction ability of language models in the open domain. Experimental\nresults demonstrate that current LLMs face challenges comprehending abstraction\nknowledge in zero-shot and few-shot settings. By training on our rich\nabstraction knowledge, we find LLMs can acquire basic abstraction abilities and\ngeneralize to unseen events. In the meantime, we empirically show that our\nbenchmark is comprehensive to enhance LLMs across two previous abstraction\ntasks.",
            "author": [
                "Zhaowei Wang",
                "Haochen Shi",
                "Weiqi Wang",
                "Tianqing Fang",
                "Hongming Zhang",
                "Sehyun Choi",
                "Xin Liu",
                "Yangqiu Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09174v2",
                "http://arxiv.org/pdf/2311.09174v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09172v1",
            "title": "Enhancing AmBC Systems with Deep Learning for Joint Channel Estimation\n  and Signal Detection",
            "updated": "2023-11-15T18:09:52Z",
            "published": "2023-11-15T18:09:52Z",
            "summary": "The era of ubiquitous, affordable wireless connectivity has opened doors to\ncountless practical applications. In this context, ambient backscatter\ncommunication (AmBC) stands out, utilizing passive tags to establish\nconnections with readers by harnessing reflected ambient radio frequency (RF)\nsignals. However, conventional data detectors face limitations due to their\ninadequate knowledge of channel and RF-source parameters. To address this\nchallenge, we propose an innovative approach using a deep neural network (DNN)\nfor channel state estimation (CSI) and signal detection within AmBC systems.\nUnlike traditional methods that separate CSI estimation and data detection, our\napproach leverages a DNN to implicitly estimate CSI and simultaneously detect\ndata. The DNN model, trained offline using simulated data derived from channel\nstatistics, excels in online data recovery, ensuring robust performance in\npractical scenarios. Comprehensive evaluations validate the superiority of our\nproposed DNN method over traditional detectors, particularly in terms of bit\nerror rate (BER). In high signal-to-noise ratio (SNR) conditions, our method\nexhibits an impressive approximately 20% improvement in BER performance\ncompared to the maximum likelihood (ML) approach. These results underscore the\neffectiveness of our developed approach for AmBC channel estimation and signal\ndetection. In summary, our method outperforms traditional detectors, bolstering\nthe reliability and efficiency of AmBC systems, even in challenging channel\nconditions.",
            "author": [
                "S. Zargari",
                "A. Hakimi",
                "C. Tellambura",
                "A. Maaref"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09172v1",
                "http://arxiv.org/pdf/2311.09172v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09162v1",
            "title": "Leaving No Branches Behind: Predicting Baryonic Properties of Galaxies\n  from Merger Trees",
            "updated": "2023-11-15T18:00:17Z",
            "published": "2023-11-15T18:00:17Z",
            "summary": "Galaxies play a key role in our endeavor to understand how structure\nformation proceeds in the Universe. For any precision study of cosmology or\ngalaxy formation, there is a strong demand for huge sets of realistic mock\ngalaxy catalogs, spanning cosmologically significant volumes. For such a\ndaunting task, methods that can produce a direct mapping between dark matter\nhalos from dark matter-only simulations and galaxies are strongly preferred, as\nproducing mocks from full-fledged hydrodynamical simulations or semi-analytical\nmodels is too expensive. Here we present a Graph Neural Network-based model\nthat is able to accurately predict key properties of galaxies such as stellar\nmass, $g-r$ color, star formation rate, gas mass, stellar metallicity, and gas\nmetallicity, purely from dark matter properties extracted from halos along the\nfull assembly history of the galaxies. Tests based on the TNG300 simulation of\nthe IllustrisTNG project show that our model can recover the baryonic\nproperties of galaxies to high accuracy, over a wide redshift range ($z =\n0-5$), for all galaxies with stellar masses more massive than $10^9\\,M_\\odot$\nand their progenitors, with strong improvements over the state-of-the-art\nmethods. We further show that our method makes substantial strides toward\nproviding an understanding of the implications of the IllustrisTNG galaxy\nformation model.",
            "author": [
                "Chen-Yu Chuang",
                "Christian Kragh Jespersen",
                "Yen-Ting Lin",
                "Shirley Ho",
                "Shy Genel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09162v1",
                "http://arxiv.org/pdf/2311.09162v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09149v1",
            "title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction",
            "updated": "2023-11-15T17:46:39Z",
            "published": "2023-11-15T17:46:39Z",
            "summary": "In this paper, we tackle the significant challenge of temporal knowledge\nreasoning in Large Language Models (LLMs), an area where such models frequently\nencounter difficulties. These difficulties often result in the generation of\nmisleading or incorrect information, primarily due to their limited capacity to\nprocess evolving factual knowledge and complex temporal logic. In response, we\npropose a novel, constructivism-based approach that advocates for a paradigm\nshift in LLM learning towards an active, ongoing process of knowledge synthesis\nand customization. At the heart of our proposal is the Abstract Reasoning\nInduction ARI framework, which divides temporal reasoning into two distinct\nphases: Knowledge-agnostic and Knowledge-based. This division aims to reduce\ninstances of hallucinations and improve LLMs' capacity for integrating abstract\nmethodologies derived from historical data. Our approach achieves remarkable\nimprovements, with relative gains of 29.7\\% and 9.27\\% on two temporal QA\ndatasets, underscoring its efficacy in advancing temporal reasoning in LLMs.\nThe code will be released at https://github.com/czy1999/ARI.",
            "author": [
                "Ziyang Chen",
                "Dongfang Li",
                "Xiang Zhao",
                "Baotian Hu",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09149v1",
                "http://arxiv.org/pdf/2311.09149v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09141v1",
            "title": "Prophet Inequalities Require Only a Constant Number of Samples",
            "updated": "2023-11-15T17:35:04Z",
            "published": "2023-11-15T17:35:04Z",
            "summary": "In a prophet inequality problem, $n$ independent random variables are\npresented to a gambler one by one. The gambler decides when to stop the\nsequence and obtains the most recent value as reward. We evaluate a stopping\nrule by the worst-case ratio between its expected reward and the expectation of\nthe maximum variable. In the classic setting, the order is fixed, and the\noptimal ratio is known to be 1/2. Three variants of this problem have been\nextensively studied: the prophet-secretary model, where variables arrive in\nuniformly random order; the free-order model, where the gambler chooses the\narrival order; and the i.i.d. model, where the distributions are all the same,\nrendering the arrival order irrelevant.\n  Most of the literature assumes that distributions are known to the gambler.\nRecent work has considered the question of what is achievable when the gambler\nhas access only to a few samples per distribution. Surprisingly, in the\nfixed-order case, a single sample from each distribution is enough to\napproximate the optimal ratio, but this is not the case in any of the three\nvariants.\n  We provide a unified proof that for all three variants of the problem, a\nconstant number of samples (independent of n) for each distribution is good\nenough to approximate the optimal ratios.\n  Prior to our work, this was known to be the case only in the i.i.d. variant.\nWe complement our result showing that our algorithms can be implemented in\npolynomial time.\n  A key ingredient in our proof is an existential result based on a minimax\nargument, which states that there must exist an algorithm that attains the\noptimal ratio and does not rely on the knowledge of the upper tail of the\ndistributions. A second key ingredient is a refined sample-based version of a\ndecomposition of the instance into \"small\" and \"large\" variables, first\nintroduced by Liu et al. [EC'21].",
            "author": [
                "Andr\u00e9s Cristi",
                "Bruno Ziliotto"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09141v1",
                "http://arxiv.org/pdf/2311.09141v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09128v1",
            "title": "Fast Detection of Phase Transitions with Multi-Task\n  Learning-by-Confusion",
            "updated": "2023-11-15T17:17:49Z",
            "published": "2023-11-15T17:17:49Z",
            "summary": "Machine learning has been successfully used to study phase transitions. One\nof the most popular approaches to identifying critical points from data without\nprior knowledge of the underlying phases is the learning-by-confusion scheme.\nAs input, it requires system samples drawn from a grid of the parameter whose\nchange is associated with potential phase transitions. Up to now, the scheme\nrequired training a distinct binary classifier for each possible splitting of\nthe grid into two sides, resulting in a computational cost that scales linearly\nwith the number of grid points. In this work, we propose and showcase an\nalternative implementation that only requires the training of a single\nmulti-class classifier. Ideally, such multi-task learning eliminates the\nscaling with respect to the number of grid points. In applications to the Ising\nmodel and an image dataset generated with Stable Diffusion, we find significant\nspeedups that closely correspond to the ideal case, with only minor deviations.",
            "author": [
                "Julian Arnold",
                "Frank Sch\u00e4fer",
                "Niels L\u00f6rch"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09128v1",
                "http://arxiv.org/pdf/2311.09128v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09120v1",
            "title": "The least eigenvalues of integral circulant graphs",
            "updated": "2023-11-15T17:09:08Z",
            "published": "2023-11-15T17:09:08Z",
            "summary": "The integral circulant graph $ICG_n (D)$ has the vertex set $Z_n = \\{0, 1, 2,\n\\ldots, n - 1\\}$, where vertices $a$ and $b$ are adjacent if $\\gcd(a-b,n)\\in\nD$, with $D \\subseteq \\{d : d \\mid n,\\ 1\\leq d<n\\}$. In this paper, we\nestablish that the minimal value of the least eigenvalues (minimal least\neigenvalue) of integral circulant graphs $ICG_n(D)$, given an order $n$ with\nits prime factorization $p_1^{\\alpha_1}\\cdots p_k^{\\alpha_k}$, is equal to\n$-\\frac{n}{p_1}$. Moreover, we show that the minimal least eigenvalue of\nconnected integral circulant graphs $ICG_n(D)$ of order $n$ whose complements\nare also connected is equal to $-\\frac{n}{p_1}+p_1^{\\alpha_1-1}$. Finally, we\ndetermine the second minimal eigenvalue among all least eigenvalues within the\nclass of connected integral circulant graphs of a prescribed order $n$ and show\nit to be equal to $-\\frac{n}{p_1}+p_1-1$ or $-\\frac{n}{p_1}+1$, depending on\nwhether $\\alpha_1>1$ or not, respectively. In all the aforementioned tasks, we\nprovide a complete characterization of graphs whose spectra contain these\ndetermined minimal least eigenvalues.",
            "author": [
                "Milan Basic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09120v1",
                "http://arxiv.org/pdf/2311.09120v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50 05C35 11A25"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09118v1",
            "title": "WildlifeDatasets: An open-source toolkit for animal re-identification",
            "updated": "2023-11-15T17:08:09Z",
            "published": "2023-11-15T17:08:09Z",
            "summary": "In this paper, we present WildlifeDatasets\n(https://github.com/WildlifeDatasets/wildlife-datasets) - an open-source\ntoolkit intended primarily for ecologists and computer-vision /\nmachine-learning researchers. The WildlifeDatasets is written in Python, allows\nstraightforward access to publicly available wildlife datasets, and provides a\nwide variety of methods for dataset pre-processing, performance analysis, and\nmodel fine-tuning. We showcase the toolkit in various scenarios and baseline\nexperiments, including, to the best of our knowledge, the most comprehensive\nexperimental comparison of datasets and methods for wildlife re-identification,\nincluding both local descriptors and deep learning approaches. Furthermore, we\nprovide the first-ever foundation model for individual re-identification within\na wide range of species - MegaDescriptor - that provides state-of-the-art\nperformance on animal re-identification datasets and outperforms other\npre-trained models such as CLIP and DINOv2 by a significant margin. To make the\nmodel available to the general public and to allow easy integration with any\nexisting wildlife monitoring applications, we provide multiple MegaDescriptor\nflavors (i.e., Small, Medium, and Large) through the HuggingFace hub\n(https://huggingface.co/BVRA).",
            "author": [
                "Vojt\u011bch \u010cerm\u00e1k",
                "Lukas Picek",
                "Luk\u00e1\u0161 Adam",
                "Kostas Papafitsoros"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09118v1",
                "http://arxiv.org/pdf/2311.09118v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09115v2",
            "title": "HEALNet -- Hybrid Multi-Modal Fusion for Heterogeneous Biomedical Data",
            "updated": "2023-11-20T13:55:04Z",
            "published": "2023-11-15T17:06:26Z",
            "summary": "Technological advances in medical data collection such as high-resolution\nhistopathology and high-throughput genomic sequencing have contributed to the\nrising requirement for multi-modal biomedical modelling, specifically for\nimage, tabular, and graph data. Most multi-modal deep learning approaches use\nmodality-specific architectures that are trained separately and cannot capture\nthe crucial cross-modal information that motivates the integration of different\ndata sources. This paper presents the Hybrid Early-fusion Attention Learning\nNetwork (HEALNet): a flexible multi-modal fusion architecture, which a)\npreserves modality-specific structural information, b) captures the cross-modal\ninteractions and structural information in a shared latent space, c) can\neffectively handle missing modalities during training and inference, and d)\nenables intuitive model inspection by learning on the raw data input instead of\nopaque embeddings. We conduct multi-modal survival analysis on Whole Slide\nImages and Multi-omic data on four cancer cohorts of The Cancer Genome Atlas\n(TCGA). HEALNet achieves state-of-the-art performance, substantially improving\nover both uni-modal and recent multi-modal baselines, whilst being robust in\nscenarios with missing modalities.",
            "author": [
                "Konstantin Hemker",
                "Nikola Simidjievski",
                "Mateja Jamnik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09115v2",
                "http://arxiv.org/pdf/2311.09115v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09111v1",
            "title": "Graph Compression with Side Information at the Decoder",
            "updated": "2023-11-15T17:01:25Z",
            "published": "2023-11-15T17:01:25Z",
            "summary": "In this paper, we study the problem of graph compression with side\ninformation at the decoder. The focus is on the situation when an unlabelled\ngraph (which is also referred to as a structure) is to be compressed or is\navailable as side information. For correlated Erd\\H{o}s-R\\'enyi weighted random\ngraphs, we give a precise characterization of the smallest rate at which a\nlabelled graph or its structure can be compressed with aid of a correlated\nlabelled graph or its structure at the decoder. We approach this problem by\nusing the entropy-spectrum framework and establish some convergence results for\nconditional distributions involving structures, which play a key role in the\nconstruction of an optimal encoding and decoding scheme. Our proof essentially\nuses the fact that, in the considered correlated Erd\\H{o}s-R\\'enyi model, the\nstructure retains most of the information about the labelled graph.\nFurthermore, we consider the case of unweighted graphs and present how the\noptimal decoding can be done using the notion of graph alignment.",
            "author": [
                "Praneeth Kumar Vippathalla",
                "Mihai-Alin Badiu",
                "Justin P. Coon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09111v1",
                "http://arxiv.org/pdf/2311.09111v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09109v1",
            "title": "Does Pre-trained Language Model Actually Infer Unseen Links in Knowledge\n  Graph Completion?",
            "updated": "2023-11-15T16:56:49Z",
            "published": "2023-11-15T16:56:49Z",
            "summary": "Knowledge graphs (KGs) consist of links that describe relationships between\nentities. Due to the difficulty of manually enumerating all relationships\nbetween entities, automatically completing them is essential for KGs. Knowledge\nGraph Completion (KGC) is a task that infers unseen relationships between\nentities in a KG. Traditional embedding-based KGC methods, such as RESCAL,\nTransE, DistMult, ComplEx, RotatE, HAKE, HousE, etc., infer missing links using\nonly the knowledge from training data. In contrast, the recent Pre-trained\nLanguage Model (PLM)-based KGC utilizes knowledge obtained during pre-training.\nTherefore, PLM-based KGC can estimate missing links between entities by reusing\nmemorized knowledge from pre-training without inference. This approach is\nproblematic because building KGC models aims to infer unseen links between\nentities. However, conventional evaluations in KGC do not consider inference\nand memorization abilities separately. Thus, a PLM-based KGC method, which\nachieves high performance in current KGC evaluations, may be ineffective in\npractical applications. To address this issue, we analyze whether PLM-based KGC\nmethods make inferences or merely access memorized knowledge. For this purpose,\nwe propose a method for constructing synthetic datasets specified in this\nanalysis and conclude that PLMs acquire the inference abilities required for\nKGC through pre-training, even though the performance improvements mostly come\nfrom textual information of entities and relations.",
            "author": [
                "Yusuke Sakai",
                "Hidetaka Kamigaito",
                "Katsuhiko Hayashi",
                "Taro Watanabe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09109v1",
                "http://arxiv.org/pdf/2311.09109v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09095v1",
            "title": "New Graph Decompositions and Combinatorial Boolean Matrix Multiplication\n  Algorithms",
            "updated": "2023-11-15T16:42:18Z",
            "published": "2023-11-15T16:42:18Z",
            "summary": "We revisit the fundamental Boolean Matrix Multiplication (BMM) problem. With\nthe invention of algebraic fast matrix multiplication over 50 years ago, it\nalso became known that BMM can be solved in truly subcubic $O(n^\\omega)$ time,\nwhere $\\omega<3$; much work has gone into bringing $\\omega$ closer to $2$.\nSince then, a parallel line of work has sought comparably fast combinatorial\nalgorithms but with limited success. The naive $O(n^3)$-time algorithm was\ninitially improved by a $\\log^2{n}$ factor [Arlazarov et al.; RAS'70], then by\n$\\log^{2.25}{n}$ [Bansal and Williams; FOCS'09], then by $\\log^3{n}$ [Chan;\nSODA'15], and finally by $\\log^4{n}$ [Yu; ICALP'15].\n  We design a combinatorial algorithm for BMM running in time $n^3 /\n2^{\\Omega(\\sqrt[7]{\\log n})}$ -- a speed-up over cubic time that is stronger\nthan any poly-log factor. This comes tantalizingly close to refuting the\nconjecture from the 90s that truly subcubic combinatorial algorithms for BMM\nare impossible. This popular conjecture is the basis for dozens of fine-grained\nhardness results.\n  Our main technical contribution is a new regularity decomposition theorem for\nBoolean matrices (or equivalently, bipartite graphs) under a notion of\nregularity that was recently introduced and analyzed analytically in the\ncontext of communication complexity [Kelley, Lovett, Meka; arXiv'23], and is\nrelated to a similar notion from the recent work on $3$-term arithmetic\nprogression free sets [Kelley, Meka; FOCS'23].",
            "author": [
                "Amir Abboud",
                "Nick Fischer",
                "Zander Kelley",
                "Shachar Lovett",
                "Raghu Meka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09095v1",
                "http://arxiv.org/pdf/2311.09095v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09078v1",
            "title": "Majority dynamics on random graphs: the multiple states case",
            "updated": "2023-11-15T16:19:36Z",
            "published": "2023-11-15T16:19:36Z",
            "summary": "We study the evolution of majority dynamics with more than two states on the\nbinomial random graph $G(n,p)$. In this process, each vertex has a state in\n$\\{1,\\ldots, k\\}$, with $k\\geq 3$, and at each round every vertex adopts state\n$i$ if it has more neighbours in state $i$ that in any other state. Ties are\nresolved randomly. We show that with high probability the process reaches\nunanimity in at most three rounds, if $np\\gg n^{2/3}$.",
            "author": [
                "Jordan Chellig",
                "Nikolaos Fountoulakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09078v1",
                "http://arxiv.org/pdf/2311.09078v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO",
                "05C80"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09075v1",
            "title": "Self-stabilizing Byzantine Multivalued Consensus",
            "updated": "2023-11-15T16:18:17Z",
            "published": "2023-11-15T16:18:17Z",
            "summary": "Consensus, abstracting a myriad of problems in which processes have to agree\non a single value, is one of the most celebrated problems of fault-tolerant\ndistributed computing. Consensus applications include fundamental services for\nthe environments of the Cloud and Blockchain, and in such challenging\nenvironments, malicious behaviors are often modeled as adversarial Byzantine\nfaults.\n  At OPODIS 2010, Mostefaoui and Raynal (in short MR) presented a\nByzantine-tolerant solution to consensus in which the decided value cannot be a\nvalue proposed only by Byzantine processes. MR has optimal resilience coping\nwith up to t < n/3 Byzantine nodes over n processes. MR provides this\nmultivalued consensus object (which accepts proposals taken from a finite set\nof values) assuming the availability of a single Binary consensus object (which\naccepts proposals taken from the set {0,1}).\n  This work, which focuses on multivalued consensus, aims at the design of an\neven more robust solution than MR. Our proposal expands MR's fault-model with\nself-stabilization, a vigorous notion of fault-tolerance. In addition to\ntolerating Byzantine, self-stabilizing systems can automatically recover after\nthe occurrence of arbitrary transient-faults. These faults represent any\nviolation of the assumptions according to which the system was designed to\noperate (provided that the algorithm code remains intact).\n  To the best of our knowledge, we propose the first self-stabilizing solution\nfor intrusion-tolerant multivalued consensus for asynchronous message-passing\nsystems prone to Byzantine failures. Our solution has a O(t) stabilization time\nfrom arbitrary transient faults.",
            "author": [
                "Romaric Duvignau",
                "Michel Raynal",
                "Elad Michael Schiller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09075v1",
                "http://arxiv.org/pdf/2311.09075v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09069v1",
            "title": "How Well Do Large Language Models Truly Ground?",
            "updated": "2023-11-15T16:11:27Z",
            "published": "2023-11-15T16:11:27Z",
            "summary": "Reliance on the inherent knowledge of Large Language Models (LLMs) can cause\nissues such as hallucinations, lack of control, and difficulties in integrating\nvariable knowledge. To mitigate this, LLMs can be probed to generate responses\nby grounding on external context, often given as input (knowledge-augmented\nmodels). Yet, previous research is often confined to a narrow view of the term\n\"grounding\", often only focusing on whether the response contains the correct\nanswer or not, which does not ensure the reliability of the entire response. To\naddress this limitation, we introduce a strict definition of grounding: a model\nis considered truly grounded when its responses (1) fully utilize necessary\nknowledge from the provided context, and (2) don't exceed the knowledge within\nthe contexts. We introduce a new dataset and a grounding metric to assess this\nnew definition and perform experiments across 13 LLMs of different sizes and\ntraining methods to provide insights into the factors that influence grounding\nperformance. Our findings contribute to a better understanding of how to\nimprove grounding capabilities and suggest an area of improvement toward more\nreliable and controllable LLM applications.",
            "author": [
                "Hyunji Lee",
                "Sejune Joo",
                "Chaeeun Kim",
                "Joel Jang",
                "Doyoung Kim",
                "Kyoung-Woon On",
                "Minjoon Seo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09069v1",
                "http://arxiv.org/pdf/2311.09069v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09068v1",
            "title": "Learning Fair Division from Bandit Feedback",
            "updated": "2023-11-15T16:10:34Z",
            "published": "2023-11-15T16:10:34Z",
            "summary": "This work addresses learning online fair division under uncertainty, where a\ncentral planner sequentially allocates items without precise knowledge of\nagents' values or utilities. Departing from conventional online algorithm, the\nplanner here relies on noisy, estimated values obtained after allocating items.\nWe introduce wrapper algorithms utilizing \\textit{dual averaging}, enabling\ngradual learning of both the type distribution of arriving items and agents'\nvalues through bandit feedback. This approach enables the algorithms to\nasymptotically achieve optimal Nash social welfare in linear Fisher markets\nwith agents having additive utilities. We establish regret bounds in Nash\nsocial welfare and empirically validate the superior performance of our\nproposed algorithms across synthetic and empirical datasets.",
            "author": [
                "Hakuei Yamada",
                "Junpei Komiyama",
                "Kenshi Abe",
                "Atsushi Iwasaki"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09068v1",
                "http://arxiv.org/pdf/2311.09068v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09064v1",
            "title": "Imagine the Unseen World: A Benchmark for Systematic Generalization in\n  Visual World Models",
            "updated": "2023-11-15T16:02:13Z",
            "published": "2023-11-15T16:02:13Z",
            "summary": "Systematic compositionality, or the ability to adapt to novel situations by\ncreating a mental model of the world using reusable pieces of knowledge,\nremains a significant challenge in machine learning. While there has been\nconsiderable progress in the language domain, efforts towards systematic visual\nimagination, or envisioning the dynamical implications of a visual observation,\nare in their infancy. We introduce the Systematic Visual Imagination Benchmark\n(SVIB), the first benchmark designed to address this problem head-on. SVIB\noffers a novel framework for a minimal world modeling problem, where models are\nevaluated based on their ability to generate one-step image-to-image\ntransformations under a latent world dynamics. The framework provides benefits\nsuch as the possibility to jointly optimize for systematic perception and\nimagination, a range of difficulty levels, and the ability to control the\nfraction of possible factor combinations used during training. We provide a\ncomprehensive evaluation of various baseline models on SVIB, offering insight\ninto the current state-of-the-art in systematic visual imagination. We hope\nthat this benchmark will help advance visual systematic compositionality.",
            "author": [
                "Yeongbin Kim",
                "Gautam Singh",
                "Junyeong Park",
                "Caglar Gulcehre",
                "Sungjin Ahn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09064v1",
                "http://arxiv.org/pdf/2311.09064v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09054v1",
            "title": "On the Crazy Knight's Tour Problem",
            "updated": "2023-11-15T15:45:49Z",
            "published": "2023-11-15T15:45:49Z",
            "summary": "Heffter arrays are partially filled arrays that have been introduced in [1]\nas a tool to construct regular embeddings of graphs on surfaces. These\nconstructions can be achieved from the solution of a tour problem on the filled\ncells of the array, introduced in [3] and called Crazy Knight's Tour Problem.\nIn particular, the knight's move is seen as the composition of an horizontal\nmove and a vertical one, where the directions (leftward or rightward, upward or\ndownward respectively) are prescribed in advance for each row and each column\nof the array and each filled cell is mapped to the first filled cell\nencountered along that direction. Then, a solution to the Crazy Knight's Tour\nProblem is a set of directions such that the resulting move function is a tour\nover the array. Here, we consider a particular class of square arrays, and we\nconstruct solutions to the tour problem in some infinite families of these\narrays.",
            "author": [
                "Lorenzo Mella"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09054v1",
                "http://arxiv.org/pdf/2311.09054v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09053v1",
            "title": "Assessing Knowledge Editing in Language Models via Relation Perspective",
            "updated": "2023-11-15T15:44:42Z",
            "published": "2023-11-15T15:44:42Z",
            "summary": "Knowledge Editing (KE) for modifying factual knowledge in Large Language\nModels (LLMs) has been receiving increasing attention. However, existing\nknowledge editing methods are entity-centric, and it is unclear whether this\napproach is suitable for a relation-centric perspective. To address this gap,\nthis paper constructs a new benchmark named RaKE, which focuses on Relation\nbased Knowledge Editing. In this paper, we establish a suite of innovative\nmetrics for evaluation and conduct comprehensive experiments involving various\nknowledge editing baselines. We notice that existing knowledge editing methods\nexhibit the potential difficulty in their ability to edit relations. Therefore,\nwe further explore the role of relations in factual triplets within the\ntransformer. Our research results confirm that knowledge related to relations\nis not only stored in the FFN network but also in the attention layers. This\nprovides experimental support for future relation-based knowledge editing\nmethods.",
            "author": [
                "Yifan Wei",
                "Xiaoyan Yu",
                "Huanhuan Ma",
                "Fangyu Lei",
                "Yixuan Weng",
                "Ran Song",
                "Kang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09053v1",
                "http://arxiv.org/pdf/2311.09053v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09042v1",
            "title": "A necessary and sufficient condition for the existence of a properly\n  coloured $f$-factor in an edge-coloured graph",
            "updated": "2023-11-15T15:33:59Z",
            "published": "2023-11-15T15:33:59Z",
            "summary": "The main result of this paper is an edge-coloured version of Tutte's\n$f$-factor theorem. We give a necessary and sufficient condition for an\nedge-coloured graph $G^c$ to have a properly coloured $f$-factor. We state and\nprove our result in terms of an auxiliary graph $G_f^c$ which has a 1-factor if\nand only if $G^c$ has a properly coloured $f$-factor; this is analogous to the\n\"short proof\" of the $f$-factor theorem given by Tutte in 1954. An alternative\nstatement, analogous to the original $f$-factor theorem, is also given. We show\nthat our theorem generalises the $f$-factor theorem; that is, the former\nimplies the latter. We consider other properties of edge-coloured graphs, and\nshow that similar results are unlikely for $f$-factors with rainbow components\nand distance-$d$-coloured $f$-factors, even when $d=2$ and the number of\ncolours used is asymptotically minimal.",
            "author": [
                "Roman \u010cada",
                "Michitaka Furuya",
                "Kenji Kimura",
                "Kenta Ozeki",
                "Christopher Purcell",
                "Takamasa Yashima"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09042v1",
                "http://arxiv.org/pdf/2311.09042v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C70, 68Q17",
                "G.2.2; F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09020v1",
            "title": "Explaining Explanation: An Empirical Study on Explanation in Code\n  Reviews",
            "updated": "2023-11-15T15:08:38Z",
            "published": "2023-11-15T15:08:38Z",
            "summary": "Code review is an important process for quality assurance in software\ndevelopment. For an effective code review, the reviewers must explain their\nfeedback to enable the authors of the code change to act on them. However, the\nexplanation needs may differ among developers, who may require different types\nof explanations. It is therefore crucial to understand what kind of\nexplanations reviewers usually use in code reviews. To the best of our\nknowledge, no study published to date has analyzed the types of explanations\nused in code review. In this study, we present the first analysis of\nexplanations in useful code reviews. We extracted a set of code reviews based\non their usefulness and labeled them based on whether they contained an\nexplanation, a solution, or both a proposed solution and an explanation\nthereof.\n  Based on our analysis, we found that a significant portion of the code review\ncomments (46%) only include solutions without providing an explanation. We\nfurther investigated the remaining 54% of code review comments containing an\nexplanation and conducted an open card sorting to categorize the reviewers'\nexplanations. We distilled seven distinct categories of explanations based on\nthe expression forms developers used. Then, we utilize large language models,\nspecifically ChatGPT, to assist developers in getting a code review explanation\nthat suits their preferences. Specifically, we created prompts to transform a\ncode review explanation into a specific type of explanation. Our evaluation\nresults show that ChatGPT correctly generated the specified type of explanation\nin 88/90 cases and that 89/90 of the cases have the correct explanation.\nOverall, our study provides insights into the types of explanations that\ndevelopers use in code review and showcases how ChatGPT can be leveraged during\nthe code review process to generate a specific type of explanation.",
            "author": [
                "Ratnadira Widyasari",
                "Ting Zhang",
                "Abir Bouraffa",
                "David Lo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09020v1",
                "http://arxiv.org/pdf/2311.09020v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09008v1",
            "title": "End-to-end Task-oriented Dialogue: A Survey of Tasks, Methods, and\n  Future Directions",
            "updated": "2023-11-15T14:50:16Z",
            "published": "2023-11-15T14:50:16Z",
            "summary": "End-to-end task-oriented dialogue (EToD) can directly generate responses in\nan end-to-end fashion without modular training, which attracts escalating\npopularity. The advancement of deep neural networks, especially the successful\nuse of large pre-trained models, has further led to significant progress in\nEToD research in recent years. In this paper, we present a thorough review and\nprovide a unified perspective to summarize existing approaches as well as\nrecent trends to advance the development of EToD research. The contributions of\nthis paper can be summarized: (1) \\textbf{\\textit{First survey}}: to our\nknowledge, we take the first step to present a thorough survey of this research\nfield; (2) \\textbf{\\textit{New taxonomy}}: we first introduce a unified\nperspective for EToD, including (i) \\textit{Modularly EToD} and (ii)\n\\textit{Fully EToD}; (3) \\textbf{\\textit{New Frontiers}}: we discuss some\npotential frontier areas as well as the corresponding challenges, hoping to\nspur breakthrough research in EToD field; (4) \\textbf{\\textit{Abundant\nresources}}: we build a public website\\footnote{We collect the related papers,\nbaseline projects, and leaderboards for the community at\n\\url{https://etods.net/}.}, where EToD researchers could directly access the\nrecent progress. We hope this work can serve as a thorough reference for the\nEToD research community.",
            "author": [
                "Libo Qin",
                "Wenbo Pan",
                "Qiguang Chen",
                "Lizi Liao",
                "Zhou Yu",
                "Yue Zhang",
                "Wanxiang Che",
                "Min Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09008v1",
                "http://arxiv.org/pdf/2311.09008v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09001v1",
            "title": "Non-geometric distance-regular graphs of diameter at least $3$ with\n  smallest eigenvalue at least $-3$",
            "updated": "2023-11-15T14:42:49Z",
            "published": "2023-11-15T14:42:49Z",
            "summary": "In this paper, we classify non-geometric distance-regular graphs of diameter\nat least $3$ with smallest eigenvalue at least $-3$. This is progress towards\nwhat is hoped to be an eventual complete classification of distance-regular\ngraphs with smallest eigenvalue at least $-3$, analogous to existing\nclassification results available in the case that the smallest eigenvalue is at\nleast $-2$.",
            "author": [
                "Jack Koolen",
                "Kefan Yu",
                "Xiaoye Liang",
                "Harrison Choi",
                "Greg Markowsky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09001v1",
                "http://arxiv.org/pdf/2311.09001v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08995v1",
            "title": "Simple but Effective Unsupervised Classification for Specified Domain\n  Images: A Case Study on Fungi Images",
            "updated": "2023-11-15T14:33:22Z",
            "published": "2023-11-15T14:33:22Z",
            "summary": "High-quality labeled datasets are essential for deep learning. Traditional\nmanual annotation methods are not only costly and inefficient but also pose\nchallenges in specialized domains where expert knowledge is needed.\nSelf-supervised methods, despite leveraging unlabeled data for feature\nextraction, still require hundreds or thousands of labeled instances to guide\nthe model for effective specialized image classification. Current unsupervised\nlearning methods offer automatic classification without prior annotation but\noften compromise on accuracy. As a result, efficiently procuring high-quality\nlabeled datasets remains a pressing challenge for specialized domain images\ndevoid of annotated data. Addressing this, an unsupervised classification\nmethod with three key ideas is introduced: 1) dual-step feature dimensionality\nreduction using a pre-trained model and manifold learning, 2) a voting\nmechanism from multiple clustering algorithms, and 3) post-hoc instead of prior\nmanual annotation. This approach outperforms supervised methods in\nclassification accuracy, as demonstrated with fungal image data, achieving\n94.1% and 96.7% on public and private datasets respectively. The proposed\nunsupervised classification method reduces dependency on pre-annotated\ndatasets, enabling a closed-loop for data classification. The simplicity and\nease of use of this method will also bring convenience to researchers in\nvarious fields in building datasets, promoting AI applications for images in\nspecialized domains.",
            "author": [
                "Zhaocong liu",
                "Fa Zhang",
                "Lin Cheng",
                "Huanxi Deng",
                "Xiaoyan Yang",
                "Zhenyu Zhang",
                "Chichun Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08995v1",
                "http://arxiv.org/pdf/2311.08995v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08988v1",
            "title": "Counting Small Induced Subgraphs with Edge-monotone Properties",
            "updated": "2023-11-15T14:22:08Z",
            "published": "2023-11-15T14:22:08Z",
            "summary": "We study the parameterized complexity of #IndSub($\\Phi$), where given a graph\n$G$ and an integer $k$, the task is to count the number of induced subgraphs on\n$k$ vertices that satisfy the graph property $\\Phi$. Focke and Roth [STOC 2022]\ncompletely characterized the complexity for each $\\Phi$ that is a hereditary\nproperty (that is, closed under vertex deletions): #IndSub($\\Phi$) is\n#W[1]-hard except in the degenerate cases when every graph satisfies $\\Phi$ or\nonly finitely many graphs satisfy $\\Phi$. We complement this result with a\nclassification for each $\\Phi$ that is edge monotone (that is, closed under\nedge deletions): #IndSub($\\Phi$) is #W[1]-hard except in the degenerate case\nwhen there are only finitely many integers $k$ such that $\\Phi$ is nontrivial\non $k$-vertex graphs. Our result generalizes earlier results for specific\nproperties $\\Phi$ that are related to the connectivity or density of the graph.\n  Further, we extend the #W[1]-hardness result by a lower bound which shows\nthat #IndSub($\\Phi$) cannot be solved in time $f(k) \\cdot |V(G)|^{o(\\sqrt{\\log\nk/\\log\\log k})}$ for any function $f$, unless the Exponential-Time Hypothesis\n(ETH) fails. For many natural properties, we obtain even a tight bound $f(k)\n\\cdot |V(G)|^{o(k)}$; for example, this is the case for every property $\\Phi$\nthat is nontrivial on $k$-vertex graphs for each $k$ greater than some $k_0$.",
            "author": [
                "Simon D\u00f6ring",
                "D\u00e1niel Marx",
                "Philip Wellnitz"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08988v1",
                "http://arxiv.org/pdf/2311.08988v1"
            ],
            "primary_category": "cs.CC",
            "category": [
                "cs.CC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08958v1",
            "title": "Locally Asymptotically Minimax Statistical Treatment Rules Under Partial\n  Identification",
            "updated": "2023-11-15T13:47:24Z",
            "published": "2023-11-15T13:47:24Z",
            "summary": "Policymakers often desire a statistical treatment rule (STR) that determines\na treatment assignment rule deployed in a future population from available\ndata. With the true knowledge of the data generating process, the average\ntreatment effect (ATE) is the key quantity characterizing the optimal treatment\nrule. Unfortunately, the ATE is often not point identified but partially\nidentified. Presuming the partial identification of the ATE, this study\nconducts a local asymptotic analysis and develops the locally asymptotically\nminimax (LAM) STR. The analysis does not assume the full differentiability but\nthe directional differentiability of the boundary functions of the\nidentification region of the ATE. Accordingly, the study shows that the LAM STR\ndiffers from the plug-in STR. A simulation study also demonstrates that the LAM\nSTR outperforms the plug-in STR.",
            "author": [
                "Daido Kido"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08958v1",
                "http://arxiv.org/pdf/2311.08958v1"
            ],
            "primary_category": "econ.EM",
            "category": [
                "econ.EM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08955v1",
            "title": "A Spectral Diffusion Prior for Hyperspectral Image Super-Resolution",
            "updated": "2023-11-15T13:40:58Z",
            "published": "2023-11-15T13:40:58Z",
            "summary": "Fusion-based hyperspectral image (HSI) super-resolution aims to produce a\nhigh-spatial-resolution HSI by fusing a low-spatial-resolution HSI and a\nhigh-spatial-resolution multispectral image. Such a HSI super-resolution\nprocess can be modeled as an inverse problem, where the prior knowledge is\nessential for obtaining the desired solution. Motivated by the success of\ndiffusion models, we propose a novel spectral diffusion prior for fusion-based\nHSI super-resolution. Specifically, we first investigate the spectrum\ngeneration problem and design a spectral diffusion model to model the spectral\ndata distribution. Then, in the framework of maximum a posteriori, we keep the\ntransition information between every two neighboring states during the reverse\ngenerative process, and thereby embed the knowledge of trained spectral\ndiffusion model into the fusion problem in the form of a regularization term.\nAt last, we treat each generation step of the final optimization problem as its\nsubproblem, and employ the Adam to solve these subproblems in a reverse\nsequence. Experimental results conducted on both synthetic and real datasets\ndemonstrate the effectiveness of the proposed approach. The code of the\nproposed approach will be available on https://github.com/liuofficial/SDP.",
            "author": [
                "Jianjun Liu",
                "Zebin Wu",
                "Liang Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08955v1",
                "http://arxiv.org/pdf/2311.08955v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08941v1",
            "title": "Reasoning over Description Logic-based Contexts with Transformers",
            "updated": "2023-11-15T13:23:24Z",
            "published": "2023-11-15T13:23:24Z",
            "summary": "One way that the current state of the art measures the reasoning ability of\ntransformer-based models is by evaluating accuracy in downstream tasks like\nlogical question answering or proof generation over synthetic contexts\nexpressed in natural language. However, most of the contexts used are in\npractice very simple; in most cases, they are generated from short first-order\nlogic sentences with only a few logical operators and quantifiers. In this\nwork, we seek to answer the question how well a transformer-based model will\nperform reasoning over expressive contexts. For this purpose, we construct a\nsynthetic natural language question-answering dataset, generated by description\nlogic knowledge bases. For the generation of the knowledge bases, we use the\nexpressive language $\\mathcal{ALCQ}$. The resulting dataset contains 384K\nexamples, and increases in two dimensions: i) reasoning depth, and ii) length\nof sentences. We show that the performance of our DeBERTa-based model,\nDELTA$_M$, is marginally affected when the reasoning depth is increased and it\nis not affected at all when the length of the sentences is increasing. We also\nevaluate the generalization ability of the model on reasoning depths unseen at\ntraining, both increasing and decreasing, revealing interesting insights into\nthe model's adaptive generalization abilities.",
            "author": [
                "Angelos Poulis",
                "Eleni Tsalapati",
                "Manolis Koubarakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08941v1",
                "http://arxiv.org/pdf/2311.08941v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08936v2",
            "title": "Confident Naturalness Explanation (CNE): A Framework to Explain and\n  Assess Patterns Forming Naturalness",
            "updated": "2023-11-22T14:25:55Z",
            "published": "2023-11-15T13:19:02Z",
            "summary": "Protected natural areas are regions that have been minimally affected by\nhuman activities such as urbanization, agriculture, and other human\ninterventions. To better understand and map the naturalness of these areas,\nmachine learning models can be used to analyze satellite imagery. Specifically,\nexplainable machine learning methods show promise in uncovering patterns that\ncontribute to the concept of naturalness within these protected environments.\nAdditionally, addressing the uncertainty inherent in machine learning models is\ncrucial for a comprehensive understanding of this concept. However, existing\napproaches have limitations. They either fail to provide explanations that are\nboth valid and objective or struggle to offer a quantitative metric that\naccurately measures the contribution of specific patterns to naturalness, along\nwith the associated confidence. In this paper, we propose a novel framework\ncalled the Confident Naturalness Explanation (CNE) framework. This framework\ncombines explainable machine learning and uncertainty quantification to assess\nand explain naturalness. We introduce a new quantitative metric that describes\nthe confident contribution of patterns to the concept of naturalness.\nFurthermore, we generate an uncertainty-aware segmentation mask for each input\nsample, highlighting areas where the model lacks knowledge. To demonstrate the\neffectiveness of our framework, we apply it to a study site in Fennoscandia\nusing two open-source satellite datasets.",
            "author": [
                "Ahmed Emam",
                "Mohamed Farag",
                "Ribana Roscher"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08936v2",
                "http://arxiv.org/pdf/2311.08936v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08923v1",
            "title": "Leveraging Activation Maximization and Generative Adversarial Training\n  to Recognize and Explain Patterns in Natural Areas in Satellite Imagery",
            "updated": "2023-11-15T12:55:19Z",
            "published": "2023-11-15T12:55:19Z",
            "summary": "Natural protected areas are vital for biodiversity, climate change\nmitigation, and supporting ecological processes. Despite their significance,\ncomprehensive mapping is hindered by a lack of understanding of their\ncharacteristics and a missing land cover class definition. This paper aims to\nadvance the explanation of the designating patterns forming protected and wild\nareas. To this end, we propose a novel framework that uses activation\nmaximization and a generative adversarial model. With this, we aim to generate\nsatellite images that, in combination with domain knowledge, are capable of\noffering complete and valid explanations for the spatial and spectral patterns\nthat define the natural authenticity of these regions. Our proposed framework\nproduces more precise attribution maps pinpointing the designating patterns\nforming the natural authenticity of protected areas. Our approach fosters our\nunderstanding of the ecological integrity of the protected natural areas and\nmay contribute to future monitoring and preservation efforts.",
            "author": [
                "Ahmed Emam",
                "Timo T. Stomberg",
                "Ribana Roscher"
            ],
            "link": [
                "http://dx.doi.org/10.1109/LGRS.2023.3335473",
                "http://arxiv.org/abs/2311.08923v1",
                "http://arxiv.org/pdf/2311.08923v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08919v1",
            "title": "FCS-HGNN: Flexible Multi-type Community Search in Heterogeneous\n  Information Networks",
            "updated": "2023-11-15T12:46:33Z",
            "published": "2023-11-15T12:46:33Z",
            "summary": "Community Search (CS), a crucial task in network science, has attracted\nconsiderable interest owing to its prowess in unveiling personalized\ncommunities, thereby finding applications across diverse domains. Existing\nresearch primarily focuses on traditional homogeneous networks, which cannot be\ndirectly applied to heterogeneous information networks (HINs). However,\nexisting research also has some limitations. For instance, either they solely\nfocus on single-type or multi-type community search, which severely lacking\nflexibility, or they require users to specify meta-paths or predefined\ncommunity structures, which poses significant challenges for users who are\nunfamiliar with community search and HINs. In this paper, we propose an\ninnovative method, FCS-HGNN, that can flexibly identify either single-type or\nmulti-type communities in HINs based on user preferences. We propose the\nheterogeneous information transformer to handle node heterogeneity, and the\nedge-semantic attention mechanism to address edge heterogeneity. This not only\nconsiders the varying contributions of edges when identifying different\ncommunities, but also expertly circumvents the challenges presented by\nmeta-paths, thereby elegantly unifying the single-type and multi-type community\nsearch problems. Moreover, to enhance the applicability on large-scale graphs,\nwe propose the neighbor sampling and depth-based heuristic search strategies,\nresulting in LS-FCS-HGNN. This algorithm significantly improves training and\nquery efficiency while maintaining outstanding community effectiveness. We\nconducted extensive experiments on five real-world large-scale HINs, and the\nresults demonstrated that the effectiveness and efficiency of our proposed\nmethod, which significantly outperforms state-of-the-art methods.",
            "author": [
                "Guoxin Chen",
                "Fangda Guo",
                "Yongqing Wang",
                "Yanghao Liu",
                "Peiying Yu",
                "Huawei Shen",
                "Xueqi Cheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08919v1",
                "http://arxiv.org/pdf/2311.08919v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08911v1",
            "title": "Connection Incentives in Cost Sharing Mechanisms with Budgets",
            "updated": "2023-11-15T12:34:19Z",
            "published": "2023-11-15T12:34:19Z",
            "summary": "In a cost sharing problem on a weighted undirected graph, all other nodes\nwant to connect to the source node for some service. Each edge has a cost\ndenoted by a weight and all the connected nodes should share the total cost for\nthe connectivity. The goal of the existing solutions (e.g. folk solution and\ncycle-complete solution) is to design cost sharing rules with nice properties,\ne.g. budget balance and cost monotonicity. However, they did not consider the\ncases that each non-source node has a budget which is the maximum it can pay\nfor its cost share and may cut its adjacent edges to reduce its cost share. In\nthis paper, we design two cost sharing mechanisms taking into account the\nnodes' budgets and incentivizing all nodes to report all their adjacent edges\nso that we can minimize the total cost for the connectivity.",
            "author": [
                "Tianyi Zhang",
                "Dengji Zhao",
                "Junyu Zhang",
                "Sizhe Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08911v1",
                "http://arxiv.org/pdf/2311.08911v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08903v1",
            "title": "Cost Sharing under Private Costs and Connection Control on Directed\n  Acyclic Graphs",
            "updated": "2023-11-15T12:19:52Z",
            "published": "2023-11-15T12:19:52Z",
            "summary": "We consider a cost sharing problem on a weighted directed acyclic graph (DAG)\nwith a source node to which all the other nodes want to connect. The cost\n(weight) of each edge is private information reported by multiple contractors,\nand among them, only one contractor is selected as the builder. All the nodes\nexcept for the source need to share the total cost of the used edges. However,\nthey may block others' connections to the source by strategically cutting their\noutgoing edges to reduce their cost share, which may increase the total cost of\nconnectivity. To minimize the total cost of connectivity, we design a cost\nsharing mechanism to incentivize each node to offer all its outgoing edges and\neach contractor to report all the edges' weights truthfully, and show the\nproperties of the proposed mechanism. In addition, our mechanism outperforms\nthe two benchmark mechanisms.",
            "author": [
                "Tianyi Zhang",
                "Dengji Zhao",
                "Junyu Zhang",
                "Sizhe Gu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08903v1",
                "http://arxiv.org/pdf/2311.08903v1"
            ],
            "primary_category": "cs.GT",
            "category": [
                "cs.GT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08894v1",
            "title": "Combining Transfer Learning with In-context Learning using Blackbox LLMs\n  for Zero-shot Knowledge Base Question Answering",
            "updated": "2023-11-15T11:56:56Z",
            "published": "2023-11-15T11:56:56Z",
            "summary": "We address the zero-shot transfer learning setting for the knowledge base\nquestion answering (KBQA) problem, where a large volume of labeled training\ndata is available for the source domain, but no such labeled examples are\navailable for the target domain. Transfer learning for KBQA makes use of large\nvolumes of unlabeled data in the target in addition to the labeled data in the\nsource. More recently, few-shot in-context learning using Black-box Large\nLanguage Models (BLLMs) has been adapted for KBQA without considering any\nsource domain data. In this work, we show how to meaningfully combine these two\nparadigms for KBQA so that their benefits add up. Specifically, we preserve the\ntwo stage retrieve-then-generate pipeline of supervised KBQA and introduce\ninteraction between in-context learning using BLLMs and transfer learning from\nthe source for both stages. In addition, we propose execution-guided\nself-refinement using BLLMs, decoupled from the transfer setting. With the help\nof experiments using benchmark datasets GrailQA as the source and WebQSP as the\ntarget, we show that the proposed combination brings significant improvements\nto both stages and also outperforms by a large margin state-of-the-art\nsupervised KBQA models trained on the source. We also show that in the\nin-domain setting, the proposed BLLM augmentation significantly outperforms\nstate-of-the-art supervised models, when the volume of labeled data is limited,\nand also outperforms these marginally even when using the entire large training\ndataset.",
            "author": [
                "Mayur Patidar",
                "Avinash Singh",
                "Riya Sawhney",
                "Indrajit Bhattacharya",
                "Mausam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08894v1",
                "http://arxiv.org/pdf/2311.08894v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08892v1",
            "title": "Parameters of nucleon densities and the Coulomb barrier in heavy-ion\n  collisions",
            "updated": "2023-11-15T11:55:01Z",
            "published": "2023-11-15T11:55:01Z",
            "summary": "When modeling nuclear processes which occur in heavy-ion reactions, it is\nnecessary to calculate the potential energy of interaction between two nuclei.\nOne of the main features determining the dynamics of the nucleus-nucleus\ncollision is the Coulomb barrier, the knowledge of which is especially\nimportant when low- and intermediate-energy reactions are being studied. Our\ngoal is to establish a parameterization of nucleon density distributions for\ncalculation of the nucleus-nucleus double-folding potential in nuclear\nreactions. Special attention is paid to the description of the Coulomb barrier.\nThe study analyzes experimental data on charge radii, diffuseness, and neutron\nskin thickness of atomic nuclei. The nucleus-nucleus potential is calculated in\nthe framework of the double-folding method with the effective nucleon-nucleon\ninteraction taken in the form of the zero-range Migdal potential. Based on this\nanalysis and comparison with the Bass potential, parameters of nucleon density\ndistributions are fitted to reproduce the Coulomb barrier. A method for\ncorrecting the parameters of nucleon densities to reproduce the Coulomb barrier\nwith the double-folding potential is proposed. The presented way to correct\nnucleon densities allows obtaining a satisfactory description of the Coulomb\nbarrier that is important for modeling near-barrier collisions of heavy ions.",
            "author": [
                "Makar Simonov",
                "Alexander Karpov",
                "Tatiana Tretyakova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08892v1",
                "http://arxiv.org/pdf/2311.08892v1"
            ],
            "primary_category": "nucl-th",
            "category": [
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08883v1",
            "title": "Enabling Large Language Models to Learn from Rules",
            "updated": "2023-11-15T11:42:41Z",
            "published": "2023-11-15T11:42:41Z",
            "summary": "Large language models (LLMs) have shown incredible performance in completing\nvarious real-world tasks. The current knowledge learning paradigm of LLMs is\nmainly based on learning from examples, in which LLMs learn the internal rule\nimplicitly from a certain number of supervised examples. However, the learning\nparadigm may not well learn those complicated rules, especially when the\ntraining examples are limited. We are inspired that humans can learn the new\ntasks or knowledge in another way by learning from rules. That is, humans can\ngrasp the new tasks or knowledge quickly and generalize well given only a\ndetailed rule and a few optional examples. Therefore, in this paper, we aim to\nexplore the feasibility of this new learning paradigm, which encodes the\nrule-based knowledge into LLMs. We propose rule distillation, which first uses\nthe strong in-context abilities of LLMs to extract the knowledge from the\ntextual rules and then explicitly encode the knowledge into LLMs' parameters by\nlearning from the above in-context signals produced inside the model. Our\nexperiments show that making LLMs learn from rules by our method is much more\nefficient than example-based learning in both the sample size and\ngeneralization ability.",
            "author": [
                "Wenkai Yang",
                "Yankai Lin",
                "Jie Zhou",
                "Jirong Wen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08883v1",
                "http://arxiv.org/pdf/2311.08883v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08870v2",
            "title": "One-Shot Federated Learning with Classifier-Guided Diffusion Models",
            "updated": "2023-11-16T15:43:52Z",
            "published": "2023-11-15T11:11:25Z",
            "summary": "One-shot federated learning (OSFL) has gained attention in recent years due\nto its low communication cost. However, most of the existing methods require\nauxiliary datasets or training generators, which hinders their practicality in\nreal-world scenarios. In this paper, we explore the novel opportunities that\ndiffusion models bring to OSFL and propose FedCADO, utilizing guidance from\nclient classifiers to generate data that complies with clients' distributions\nand subsequently training the aggregated model on the server. Specifically, our\nmethod involves targeted optimizations in two aspects. On one hand, we\nconditionally edit the randomly sampled initial noises, embedding them with\nspecified semantics and distributions, resulting in a significant improvement\nin both the quality and stability of generation. On the other hand, we employ\nthe BN statistics from the classifiers to provide detailed guidance during\ngeneration. These tailored optimizations enable us to limitlessly generate\ndatasets, which closely resemble the distribution and quality of the original\nclient dataset. Our method effectively handles the heterogeneous client models\nand the problems of non-IID features or labels. In terms of privacy protection,\nour method avoids training any generator or transferring any auxiliary\ninformation on clients, eliminating any additional privacy leakage risks.\nLeveraging the extensive knowledge stored in the pre-trained diffusion model,\nthe synthetic datasets can assist us in surpassing the knowledge limitations of\nthe client samples, resulting in aggregation models that even outperform the\nperformance ceiling of centralized training in some cases, which is\nconvincingly demonstrated in the sufficient quantification and visualization\nexperiments conducted on three large-scale multi-domain image datasets.",
            "author": [
                "Mingzhao Yang",
                "Shangchao Su",
                "Bin Li",
                "Xiangyang Xue"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08870v2",
                "http://arxiv.org/pdf/2311.08870v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08868v1",
            "title": "Color Fault-Tolerant Spanners",
            "updated": "2023-11-15T11:05:06Z",
            "published": "2023-11-15T11:05:06Z",
            "summary": "We initiate the study of spanners in arbitrarily vertex- or edge-colored\ngraphs (with no \"legality\" restrictions), that are resilient to failures of\nentire color classes. When a color fails, all vertices/edges of that color\ncrash. An $f$-color fault-tolerant ($f$-CFT) $t$-spanner of an $n$-vertex\ncolored graph $G$ is a subgraph $H$ that preserves distances up to factor $t$,\neven in the presence of at most $f$ color faults. This notion generalizes the\nwell-studied $f$-vertex/edge fault-tolerant ($f$-V/EFT) spanners. The size of\nan $f$-V/EFT spanner crucially depends on the number $f$ of vertex/edge faults\nto be tolerated. In the colored variants, even a single color fault can\ncorrespond to an unbounded number of vertex/edge faults. The key conceptual\ncontribution of this work is in showing that the size (number of edges)\nrequired by an $f$-CFT spanner is in fact comparable to its uncolored\ncounterpart, with no dependency on the size of color classes. We provide\noptimal bounds on the size required by $f$-CFT spanners, revealing an\ninteresting phenomenon: while (individual) edge faults are \"easier\" than vertex\nfaults in terms of spanner size, edge-color faults are \"harder\" than\nvertex-color faults. Our upper bounds are based on a generalization of the\nblocking set technique of [Bodwin and Patel, PODC 2019] for analyzing the\n(exponential-time) greedy algorithm for FT spanners. We complement them by\nproviding efficient constructions of CFT spanners with similar size guarantees,\nbased on the algorithm of [Dinitz and Robelle, PODC 2020].",
            "author": [
                "Asaf Petruschka",
                "Shay Sapir",
                "Elad Tzalik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08868v1",
                "http://arxiv.org/pdf/2311.08868v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08858v1",
            "title": "Formal Verification of Zero-Knowledge Circuits",
            "updated": "2023-11-15T10:47:28Z",
            "published": "2023-11-15T10:47:28Z",
            "summary": "Zero-knowledge circuits are sets of equality constraints over arithmetic\nexpressions interpreted in a prime field; they are used to encode computations\nin cryptographic zero-knowledge proofs. We make the following contributions to\nthe problem of ensuring that a circuit correctly encodes a computation: a\nformal framework for circuit correctness; an ACL2 library for prime fields; an\nACL2 model of the existing R1CS (Rank-1 Constraint Systems) formalism to\nrepresent circuits, along with ACL2 and Axe tools to verify circuits of this\nform; a novel PFCS (Prime Field Constraint Systems) formalism to represent\nhierarchically structured circuits, along with an ACL2 model of it and ACL2\ntools to verify circuits of this form in a compositional and scalable way;\nverification of circuits, ranging from simple to complex; and discovery of bugs\nand optimizations in existing zero-knowledge systems.",
            "author": [
                "Alessandro Coglio",
                "Eric McCarthy",
                "Eric W. Smith"
            ],
            "link": [
                "http://dx.doi.org/10.4204/EPTCS.393.9",
                "http://arxiv.org/abs/2311.08858v1",
                "http://arxiv.org/pdf/2311.08858v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "cs.CR",
                "cs.SC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08849v1",
            "title": "OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient\n  Large-scale Multilingual Continued Pretraining",
            "updated": "2023-11-15T10:40:45Z",
            "published": "2023-11-15T10:40:45Z",
            "summary": "Pretraining multilingual language models from scratch requires considerable\ncomputational resources and substantial training data. Therefore, a more\nefficient method is to adapt existing pretrained language models (PLMs) to new\nlanguages via vocabulary extension and continued pretraining. However, this\nmethod usually randomly initializes the embeddings of new subwords and\nintroduces substantially more embedding parameters to the language model, thus\nweakening the efficiency. To address these issues, we propose a novel\nframework: \\textbf{O}ne \\textbf{F}or \\textbf{A}ll (\\textbf{\\textsc{Ofa}}),\nwhich wisely initializes the embeddings of unseen subwords from target\nlanguages and thus can adapt a PLM to multiple languages efficiently and\neffectively. \\textsc{Ofa} takes advantage of external well-aligned multilingual\nword embeddings and injects the alignment knowledge into the new embeddings. In\naddition, \\textsc{Ofa} applies matrix factorization and replaces the cumbersome\nembeddings with two lower-dimensional matrices, which significantly reduces the\nnumber of parameters while not sacrificing the performance. Through extensive\nexperiments, we show models initialized by \\textsc{Ofa} are efficient and\noutperform several baselines. \\textsc{Ofa} not only accelerates the convergence\nof continued pretraining, which is friendly to a limited computation budget,\nbut also improves the zero-shot crosslingual transfer on a wide range of\ndownstream tasks. We make our code and models publicly available.",
            "author": [
                "Yihong Liu",
                "Peiqin Lin",
                "Mingyang Wang",
                "Hinrich Sch\u00fctze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08849v1",
                "http://arxiv.org/pdf/2311.08849v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08837v2",
            "title": "On the distance spectral radius, fractional matching and factors of\n  graphs with given minimum degree",
            "updated": "2023-12-05T03:12:16Z",
            "published": "2023-11-15T10:25:16Z",
            "summary": "Let $D(G)=(d_{ij})_{n\\times n}$ be the distance matrix of a graph $G$ of\norder $n$. The largest eigenvalue of $D(G)$, denoted by $\\mu(G)$, is called the\ndistance spectral radius of $G$. A fractional matching of $G$ is a function $f:\nE(G)\\to [0,1]$ such that for any $v_i\\in V(G)$, $\\sum_{e\\in E_G(v_i)}f(e)\\le\n1$, where $E_G(v_i)=\\{e:e\\in E(G) \\ \\textrm{and}\\ e \\ \\textrm{is incident with}\n\\ v_i\\}$. Let $\\alpha_f(G)$ denote the fractional matching number of $G$ which\nis defined as $\\alpha_f(G)=\\max\\{\\sum_{e\\in E(G)}f(e): f\\ \\textrm{is a\nfractional matching of} \\ G\\}$. In this paper, we establish a sharp upper bound\nfor the distance spectral radius to guarantee $\\alpha_f(G)>\\frac{n-k}{2}$ in\ngraph $G$ with given minimum degree $\\delta$.\n  Let $\\{G_1,G_2,G_3,\\cdots\\}$ be a set of graphs, an\n$\\{G_1,G_2,G_3,\\cdots\\}$-factor of a graph $G$ is a spanning subgraph of $G$\nsuch that each component of which is isomorphic to one of\n$\\{G_1,G_2,G_3,\\cdots\\}$. In this paper, we give a sharp upper bound on\ndistance spectral radius of graph $G$ with given minimum degree $\\delta$ to\nensure that $G$ has a $\\{K_2, C_k\\}$-factor, where $k\\ge3$ is an integer. In\naddition, we also obtain a sharp upper bound on distance spectral radius for\nthe existence of a $\\{K_{1,1},K_{1,2},\\cdots,K_{1,k}\\}$-factor with $k\\ge2$ in\na graph $G$ with given minimum degree $\\delta$.",
            "author": [
                "Zengzhao Xu",
                "Weige Xi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08837v2",
                "http://arxiv.org/pdf/2311.08837v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 15A18"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08830v1",
            "title": "Quantity versus quality in publication activity: knowledge production at\n  the regional level",
            "updated": "2023-11-15T10:18:56Z",
            "published": "2023-11-15T10:18:56Z",
            "summary": "This study contributes to the ongoing debate regarding the balance between\nquality and quantity in research productivity and publication activity. Using\nempirical regional knowledge production functions, we establish a significant\ncorrelation between R&D spending and research output, specifically publication\nproductivity, while controlling for patenting activity and socioeconomic\nfactors. Our focus is on the dilemma of research quantity versus quality, which\nis analysed in the context of regional thematic specialization using spatial\nlags. When designing policies and making forecasts, it is important to consider\nthe quality of research measured by established indicators. In this study, we\nexamine the dual effect of research quality on publication activity. We\nidentify two groups of quality factors: those related to the quality of\njournals and those related to the impact of publications. On average, these\nfactors have different influences on quantitative measures. The quality of\njournals shows a negative relationship with quantity, indicating that as\njournal quality increases, the number of publications decreases. On the other\nhand, the impact of publications can be approximated by an inverse parabolic\nshape, with a positive decreasing slope within a common range of values. This\nduality in the relationship between quality factors and quantitative measures\nmay explain some of the significant variations in conclusions found in the\nliterature. We compare several models that explore factors influencing\npublication activity using a balanced panel dataset of Russian regions from\n2009 to 2021. Additionally, we propose a novel approach using thematic\nscientometric parameters as a special type of proximity measure between regions\nin thematic space. Incorporating spatial spillovers in thematic space allows us\nto account for potential cross-sectional dependence in regional data.",
            "author": [
                "Timur Gareev",
                "Irina Peker"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08830v1",
                "http://arxiv.org/pdf/2311.08830v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08829v1",
            "title": "Autoencoder with Group-based Decoder and Multi-task Optimization for\n  Anomalous Sound Detection",
            "updated": "2023-11-15T10:15:37Z",
            "published": "2023-11-15T10:15:37Z",
            "summary": "In industry, machine anomalous sound detection (ASD) is in great demand.\nHowever, collecting enough abnormal samples is difficult due to the high cost,\nwhich boosts the rapid development of unsupervised ASD algorithms. Autoencoder\n(AE) based methods have been widely used for unsupervised ASD, but suffer from\nproblems including 'shortcut', poor anti-noise ability and sub-optimal quality\nof features. To address these challenges, we propose a new AE-based framework\ntermed AEGM. Specifically, we first insert an auxiliary classifier into AE to\nenhance ASD in a multi-task learning manner. Then, we design a group-based\ndecoder structure, accompanied by an adaptive loss function, to endow the model\nwith domain-specific knowledge. Results on the DCASE 2021 Task 2 development\nset show that our methods achieve a relative improvement of 13.11% and 15.20%\nrespectively in average AUC over the official AE and MobileNetV2 across test\nsets of seven machines.",
            "author": [
                "Yifan Zhou",
                "Dongxing Xu",
                "Haoran Wei",
                "Yanhua Long"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08829v1",
                "http://arxiv.org/pdf/2311.08829v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08798v1",
            "title": "X-GRL: An Empirical Assessment of Explainable GNN-DRL in B5G/6G Networks",
            "updated": "2023-11-15T09:11:37Z",
            "published": "2023-11-15T09:11:37Z",
            "summary": "The rapid development of artificial intelligence (AI) techniques has\ntriggered a revolution in beyond fifth-generation (B5G) and upcoming\nsixth-generation (6G) mobile networks. Despite these advances, efficient\nresource allocation in dynamic and complex networks remains a major challenge.\nThis paper presents an experimental implementation of deep reinforcement\nlearning (DRL) enhanced with graph neural networks (GNNs) on a real 5G testbed.\nThe method addresses the explainability of GNNs by evaluating the importance of\neach edge in determining the model's output. The custom sampling functions feed\nthe data into the proposed GNN-driven Monte Carlo policy gradient (REINFORCE)\nagent to optimize the gNodeB (gNB) radio resources according to the specific\ntraffic demands. The demo demonstrates real-time visualization of network\nparameters and superior performance compared to benchmarks.",
            "author": [
                "Farhad Rezazadeh",
                "Sergio Barrachina-MuNoz",
                "Engin Zeydan",
                "Houbing Song",
                "K. P. Subbalakshmi",
                "Josep Mangues-Bafalluy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08798v1",
                "http://arxiv.org/pdf/2311.08798v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08796v1",
            "title": "Interacting Edge-Reinforced Random Walks",
            "updated": "2023-11-15T09:09:16Z",
            "published": "2023-11-15T09:09:16Z",
            "summary": "We consider the edge-reinforced random walk with multiple (but finitely many)\nwalkers which influence the edge weights together. The walker which moves at a\ngiven time step is chosen uniformly at random, or according to a fixed order.\nFirst, we consider 2 walkers with linear reinforcement on a line graph\ncomprising three nodes. We show that the edge weights evolve similarly to the\nsetting with a single walker which corresponds to a P\\'olya urn. In particular,\nthe left edge weight proportion is a martingale at certain stopping times,\nshowing that a (random) limiting proportion exists. We then look at an\narbitrary number of walkers on Z with very general reinforcement. We show that\nin this case, the behaviour is also the same as for a single walker: either all\nwalkers are recurrent or all walkers have finite range. In the particular case\nof reinforcements of \"sequence type\", we give a criterion for recurrence.",
            "author": [
                "Nina Gantert",
                "Fabian Michel",
                "Guilherme Reis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08796v1",
                "http://arxiv.org/pdf/2311.08796v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60K35, 60K37, 60G42 (Primary) 60F05, 60B10, 60G57 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08792v1",
            "title": "Matroids in OSCAR",
            "updated": "2023-11-15T09:07:08Z",
            "published": "2023-11-15T09:07:08Z",
            "summary": "OSCAR is an innovative new computer algebra system which combines and extends\nthe power of its four cornerstone systems - GAP (group theory), Singular\n(algebra and algebraic geometry), Polymake (polyhedral geometry), and Antic\n(number theory). Here, we present parts of the module handeling matroids in\nOSCAR, which will appear as a chapter of the upcoming OSCAR book. A matroid is\na fundamental and actively studied object in combinatorics. Matroids generalize\nlinear dependency in vector spaces as well as many aspects of graph theory.\nMoreover, matroids form a cornerstone of tropical geometry and a deep link\nbetween algebraic geometry and combinatorics. Our focus lies in particular on\ncomputing the realization space and the Chow ring of a matroid.",
            "author": [
                "Daniel Corey",
                "Lukas K\u00fchne",
                "Benjamin Schr\u00f6ter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08792v1",
                "http://arxiv.org/pdf/2311.08792v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AG",
                "05-04 (05-02, 05E14)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08782v1",
            "title": "Language Semantic Graph Guided Data-Efficient Learning",
            "updated": "2023-11-15T08:54:57Z",
            "published": "2023-11-15T08:54:57Z",
            "summary": "Developing generalizable models that can effectively learn from limited data\nand with minimal reliance on human supervision is a significant objective\nwithin the machine learning community, particularly in the era of deep neural\nnetworks. Therefore, to achieve data-efficient learning, researchers typically\nexplore approaches that can leverage more related or unlabeled data without\nnecessitating additional manual labeling efforts, such as Semi-Supervised\nLearning (SSL), Transfer Learning (TL), and Data Augmentation (DA). SSL\nleverages unlabeled data in the training process, while TL enables the transfer\nof expertise from related data distributions. DA broadens the dataset by\nsynthesizing new data from existing examples. However, the significance of\nadditional knowledge contained within labels has been largely overlooked in\nresearch. In this paper, we propose a novel perspective on data efficiency that\ninvolves exploiting the semantic information contained in the labels of the\navailable data. Specifically, we introduce a Language Semantic Graph (LSG)\nwhich is constructed from labels manifest as natural language descriptions.\nUpon this graph, an auxiliary graph neural network is trained to extract\nhigh-level semantic relations and then used to guide the training of the\nprimary model, enabling more adequate utilization of label knowledge. Across\nimage, video, and audio modalities, we utilize the LSG method in both TL and\nSSL scenarios and illustrate its versatility in significantly enhancing\nperformance compared to other data-efficient learning approaches. Additionally,\nour in-depth analysis shows that the LSG method also expedites the training\nprocess.",
            "author": [
                "Wenxuan Ma",
                "Shuang Li",
                "Lincan Cai",
                "Jingxuan Kang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08782v1",
                "http://arxiv.org/pdf/2311.08782v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08778v2",
            "title": "Gitor: Scalable Code Clone Detection by Building Global Sample Graph",
            "updated": "2023-11-18T13:46:07Z",
            "published": "2023-11-15T08:48:50Z",
            "summary": "Code clone detection is about finding out similar code fragments, which has\ndrawn much attention in software engineering since it is important for software\nmaintenance and evolution. Researchers have proposed many techniques and tools\nfor source code clone detection, but current detection methods concentrate on\nanalyzing or processing code samples individually without exploring the\nunderlying connections among code samples. In this paper, we propose Gitor to\ncapture the underlying connections among different code samples. Specifically,\ngiven a source code database, we first tokenize all code samples to extract the\npre-defined individual information. After obtaining all samples individual\ninformation, we leverage them to build a large global sample graph where each\nnode is a code sample or a type of individual information. Then we apply a node\nembedding technique on the global sample graph to extract all the samples\nvector representations. After collecting all code samples vectors, we can\nsimply compare the similarity between any two samples to detect possible clone\npairs. More importantly, since the obtained vector of a sample is from a global\nsample graph, we can combine it with its own code features to improve the code\nclone detection performance. To demonstrate the effectiveness of Gitor, we\nevaluate it on a widely used dataset namely BigCloneBench. Our experimental\nresults show that Gitor has higher accuracy in terms of code clone detection\nand excellent execution time for inputs of various sizes compared to existing\nstate-of-the-art tools. Moreover, we also evaluate the combination of Gitor\nwith other traditional vector-based clone detection methods, the results show\nthat the use of Gitor enables them detect more code clones with higher F1.",
            "author": [
                "Junjie Shan",
                "Shihan Dou",
                "Yueming Wu",
                "Hairu Wu",
                "Yang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08778v2",
                "http://arxiv.org/pdf/2311.08778v2"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08774v2",
            "title": "Two-stage Joint Transductive and Inductive learning for Nuclei\n  Segmentation",
            "updated": "2023-11-17T19:26:53Z",
            "published": "2023-11-15T08:37:11Z",
            "summary": "AI-assisted nuclei segmentation in histopathological images is a crucial task\nin the diagnosis and treatment of cancer diseases. It decreases the time\nrequired to manually screen microscopic tissue images and can resolve the\nconflict between pathologists during diagnosis. Deep Learning has proven useful\nin such a task. However, lack of labeled data is a significant barrier for deep\nlearning-based approaches. In this study, we propose a novel approach to nuclei\nsegmentation that leverages the available labelled and unlabelled data. The\nproposed method combines the strengths of both transductive and inductive\nlearning, which have been previously attempted separately, into a single\nframework. Inductive learning aims at approximating the general function and\ngeneralizing to unseen test data, while transductive learning has the potential\nof leveraging the unlabelled test data to improve the classification. To the\nbest of our knowledge, this is the first study to propose such a hybrid\napproach for medical image segmentation. Moreover, we propose a novel two-stage\ntransductive inference scheme. We evaluate our approach on MoNuSeg benchmark to\ndemonstrate the efficacy and potential of our method.",
            "author": [
                "Hesham Ali",
                "Idriss Tondji",
                "Mennatullah Siam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08774v2",
                "http://arxiv.org/pdf/2311.08774v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08772v1",
            "title": "Borodin-Kostochka conjecture and Partitioning a graph into classes with\n  no clique of specified size",
            "updated": "2023-11-15T08:35:34Z",
            "published": "2023-11-15T08:35:34Z",
            "summary": "For a given graph $H$ and the graphical properties $P_1, P_2,\\ldots,P_k$, a\ngraph $H$ is said to be $(V_1, V_2,\\ldots,V_k)$-partitionable if there exists a\npartition of $V(H)$ into $k$-sets $V_1, V_2\\ldots,V_k$, such that for each\n$i\\in[k]$, the subgraph induced by $V_i$ has the property $P_i$. In $1979$,\nBollob\\'{a}s and Manvel showed that for a graph $H$ with maximum degree\n$\\Delta(H)\\geq 3$ and clique number $\\omega(H)\\leq \\Delta(H)$, if $\\Delta(H)=\np+q$, then there exists a $(V_1,V_2)$-partition of $V(H)$, such that\n$\\Delta(H[V_1])\\leq p$, $\\Delta(H[V_2])\\leq q$, $H[V_1]$ is $(p-1)$-degenerate,\nand $H[V_2]$ is $(q-1)$-degenerate.\n  Assume that $p_1\\geq p_2\\geq\\cdots\\geq p_k\\geq 2$ are $k$ positive integers\nand $\\sum_{i=1}^k p_i=\\Delta(H)-1+k$. Assume that for each $i\\in[k]$ the\nproperties $P_i$ means that $\\omega(H[V_i])\\leq p_i-1$. Is $H$ a\n$(V_1,\\ldots,V_k)$-partitionable graph?\n  In 1977, Borodin and Kostochka conjectured that any graph $H$ with maximum\ndegree $\\Delta(H)\\geq 9$ and without $K_{\\Delta(H)}$ as a subgraph, has\nchromatic number at most $\\Delta(H)-1$. Reed proved that the conjecture holds\nwhenever $ \\Delta(G) \\geq 10^{14} $.\n  When $p_1=2$ and $\\Delta(H)\\geq 9$, the above question is the Borodin and\nKostochka conjecture. Therefore, when all $p_i$s are equal to $2$ and\n$\\Delta(H)\\leq 8$, the answer to the above question is negative. Let $H$ is a\ngraph with maximum degree $\\Delta$, and clique number $\\omega(H)$, where\n$\\omega(H)\\leq \\Delta-1$. In this article, we intend to study this question\nwhen $k\\geq 2$ and $\\Delta\\geq 13$. In particular as an analogue of the\nBorodin-Kostochka conjecture, for the case that $\\Delta\\geq 13$ and $p_i\\geq 2$\nwe prove that the above question is true.",
            "author": [
                "Yaser Rowshan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08772v1",
                "http://arxiv.org/pdf/2311.08772v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08764v1",
            "title": "Combining Past, Present and Future: A Self-Supervised Approach for Class\n  Incremental Learning",
            "updated": "2023-11-15T08:13:52Z",
            "published": "2023-11-15T08:13:52Z",
            "summary": "Class Incremental Learning (CIL) aims to handle the scenario where data of\nnovel classes occur continuously and sequentially. The model should recognize\nthe sequential novel classes while alleviating the catastrophic forgetting. In\nthe self-supervised manner, it becomes more challenging to avoid the conflict\nbetween the feature embedding spaces of novel classes and old ones without any\nclass labels. To address the problem, we propose a self-supervised CIL\nframework CPPF, meaning Combining Past, Present and Future. In detail, CPPF\nconsists of a prototype clustering module (PC), an embedding space reserving\nmodule (ESR) and a multi-teacher distillation module (MTD). 1) The PC and the\nESR modules reserve embedding space for subsequent phases at the prototype\nlevel and the feature level respectively to prepare for knowledge learned in\nthe future. 2) The MTD module maintains the representations of the current\nphase without the interference of past knowledge. One of the teacher networks\nretains the representations of the past phases, and the other teacher network\ndistills relation information of the current phase to the student network.\nExtensive experiments on CIFAR100 and ImageNet100 datasets demonstrate that our\nproposed method boosts the performance of self-supervised class incremental\nlearning. We will release code in the near future.",
            "author": [
                "Xiaoshuang Chen",
                "Zhongyi Sun",
                "Ke Yan",
                "Shouhong Ding",
                "Hongtao Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08764v1",
                "http://arxiv.org/pdf/2311.08764v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09264v1",
            "title": "Cross-domain feature disentanglement for interpretable modeling of tumor\n  microenvironment impact on drug response",
            "updated": "2023-11-15T07:50:54Z",
            "published": "2023-11-15T07:50:54Z",
            "summary": "High-throughput screening technology has facilitated the generation of\nlarge-scale drug responses across hundreds of cancer cell lines. However, there\nexists significant discrepancy between in vitro cell lines and actual tumors in\nvivo in terms of their response to drug treatments, because of tumors comprise\nof complex cellular compositions and histopathology structure, known as tumor\nmicroenvironment (TME), which greatly influences the drug cytotoxicity against\ntumor cells. To date, no study has focused on modeling the impact of the TME on\nclinical drug response. This paper proposed a domain adaptation network for\nfeature disentanglement to separate representations of cancer cells and TME of\na tumor in patients. Two denoising autoencoders were separately used to extract\nfeatures from cell lines (source domain) and tumors (target domain) for partial\ndomain alignment and feature decoupling. The specific encoder was enforced to\nextract information only about TME. Moreover, to ensure generalizability to\nnovel drugs, we applied a graph attention network to learn the latent\nrepresentation of drugs, allowing us to linearly model the drug perturbation on\ncellular state in latent space. We calibrated our model on a benchmark dataset\nand demonstrated its superior performance in predicting clinical drug response\nand dissecting the influence of the TME on drug efficacy.",
            "author": [
                "Jia Zhai",
                "Hui Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09264v1",
                "http://arxiv.org/pdf/2311.09264v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08745v3",
            "title": "Using Stochastic Gradient Descent to Smooth Nonconvex Functions:\n  Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling",
            "updated": "2023-11-29T03:12:00Z",
            "published": "2023-11-15T07:27:40Z",
            "summary": "The graduated optimization approach is a heuristic method for finding\nglobally optimal solutions for nonconvex functions and has been theoretically\nanalyzed in several studies. This paper defines a new family of nonconvex\nfunctions for graduated optimization, discusses their sufficient conditions,\nand provides a convergence analysis of the graduated optimization algorithm for\nthem. It shows that stochastic gradient descent (SGD) with mini-batch\nstochastic gradients has the effect of smoothing the function, the degree of\nwhich is determined by the learning rate and batch size. This finding provides\ntheoretical insights on why large batch sizes fall into sharp local minima, why\ndecaying learning rates and increasing batch sizes are superior to fixed\nlearning rates and batch sizes, and what the optimal learning rate scheduling\nis. To the best of our knowledge, this is the first paper to provide a\ntheoretical explanation for these aspects. Moreover, a new graduated\noptimization framework that uses a decaying learning rate and increasing batch\nsize is analyzed and experimental results of image classification that support\nour theoretical findings are reported.",
            "author": [
                "Naoki Sato",
                "Hideaki Iiduka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08745v3",
                "http://arxiv.org/pdf/2311.08745v3"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08744v1",
            "title": "Towards Graph-Aware Diffusion Modeling for Collaborative Filtering",
            "updated": "2023-11-15T07:25:14Z",
            "published": "2023-11-15T07:25:14Z",
            "summary": "Recovering masked feedback with neural models is a popular paradigm in\nrecommender systems. Seeing the success of diffusion models in solving\nill-posed inverse problems, we introduce a conditional diffusion framework for\ncollaborative filtering that iteratively reconstructs a user's hidden\npreferences guided by its historical interactions. To better align with the\nintrinsic characteristics of implicit feedback data, we implement forward\ndiffusion by applying synthetic smoothing filters to interaction signals on an\nitem-item graph. The resulting reverse diffusion can be interpreted as a\npersonalized process that gradually refines preference scores. Through graph\nFourier transform, we equivalently characterize this model as an anisotropic\nGaussian diffusion in the graph spectral domain, establishing both forward and\nreverse formulations. Our model outperforms state-of-the-art methods by a large\nmargin on one dataset and yields competitive results on the others.",
            "author": [
                "Yunqin Zhu",
                "Chao Wang",
                "Hui Xiong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08744v1",
                "http://arxiv.org/pdf/2311.08744v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09262v1",
            "title": "Disentangling the Potential Impacts of Papers into Diffusion,\n  Conformity, and Contribution Values",
            "updated": "2023-11-15T07:21:11Z",
            "published": "2023-11-15T07:21:11Z",
            "summary": "The potential impact of an academic paper is determined by various factors,\nincluding its popularity and contribution. Existing models usually estimate\noriginal citation counts based on static graphs and fail to differentiate\nvalues from nuanced perspectives. In this study, we propose a novel graph\nneural network to Disentangle the Potential impacts of Papers into Diffusion,\nConformity, and Contribution values (called DPPDCC). Given a target paper,\nDPPDCC encodes temporal and structural features within the constructed dynamic\nheterogeneous graph. Particularly, to capture the knowledge flow, we emphasize\nthe importance of comparative and co-cited/citing information between papers\nand aggregate snapshots evolutionarily. To unravel popularity, we contrast\naugmented graphs to extract the essence of diffusion and predict the\naccumulated citation binning to model conformity. We further apply orthogonal\nconstraints to encourage distinct modeling of each perspective and preserve the\ninherent value of contribution. To evaluate models' generalization for papers\npublished at various times, we reformulate the problem by partitioning data\nbased on specific time points to mirror real-world conditions. Extensive\nexperimental results on three datasets demonstrate that DPPDCC significantly\noutperforms baselines for previously, freshly, and immediately published\npapers. Further analyses confirm its robust capabilities. We will make our\ndatasets and codes publicly available.",
            "author": [
                "Zhikai Xue",
                "Guoxiu He",
                "Zhuoren Jiang",
                "Yangyang Kang",
                "Star Zhao",
                "Wei Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09262v1",
                "http://arxiv.org/pdf/2311.09262v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08732v1",
            "title": "Enhancing Emergency Decision-making with Knowledge Graphs and Large\n  Language Models",
            "updated": "2023-11-15T06:48:50Z",
            "published": "2023-11-15T06:48:50Z",
            "summary": "Emergency management urgently requires comprehensive knowledge while having a\nhigh possibility to go beyond individuals' cognitive scope. Therefore,\nartificial intelligence(AI) supported decision-making under that circumstance\nis of vital importance. Recent emerging large language models (LLM) provide a\nnew direction for enhancing targeted machine intelligence. However, the\nutilization of LLM directly would inevitably introduce unreliable output for\nits inherent issue of hallucination and poor reasoning skills. In this work, we\ndevelop a system called Enhancing Emergency decision-making with Knowledge\nGraph and LLM (E-KELL), which provides evidence-based decision-making in\nvarious emergency stages. The study constructs a structured emergency knowledge\ngraph and guides LLMs to reason over it via a prompt chain. In real-world\nevaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in\ncomprehensibility, accuracy, conciseness, and instructiveness from a group of\nemergency commanders and firefighters, demonstrating a significant improvement\nacross various situations compared to baseline models. This work introduces a\nnovel approach to providing reliable emergency decision support.",
            "author": [
                "Minze Chen",
                "Zhenxiang Tao",
                "Weitong Tang",
                "Tingxin Qin",
                "Rui Yang",
                "Chunli Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08732v1",
                "http://arxiv.org/pdf/2311.08732v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08724v1",
            "title": "Method for Text Entity Linking in Power Distribution Scheduling Oriented\n  to Power Distribution Network Knowledge Graph",
            "updated": "2023-11-15T06:35:01Z",
            "published": "2023-11-15T06:35:01Z",
            "summary": "The proposed method for linking entities in power distribution dispatch texts\nto a power distribution network knowledge graph is based on a deep\nunderstanding of these networks. This method leverages the unique features of\nentities in both the power distribution network's knowledge graph and the\ndispatch texts, focusing on their semantic, phonetic, and syntactic\ncharacteristics. An enhanced model, the Lexical Semantic Feature-based Skip\nConvolutional Neural Network (LSF-SCNN), is utilized for effectively matching\ndispatch text entities with those in the knowledge graph. The efficacy of this\nmodel, compared to a control model, is evaluated through cross-validation\nmethods in real-world power distribution dispatch scenarios. The results\nindicate that the LSF-SCNN model excels in accurately linking a variety of\nentity types, demonstrating high overall accuracy in entity linking when the\nprocess is conducted in English.",
            "author": [
                "Xiang Li",
                "Che Wang",
                "Bing Li",
                "Hao Chen",
                "Sizhe Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08724v1",
                "http://arxiv.org/pdf/2311.08724v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09261v1",
            "title": "Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural\n  Network with Biomedical Network",
            "updated": "2023-11-15T06:34:00Z",
            "published": "2023-11-15T06:34:00Z",
            "summary": "Accurately predicting drug-drug interactions (DDI) for emerging drugs, which\noffer possibilities for treating and alleviating diseases, with computational\nmethods can improve patient care and contribute to efficient drug development.\nHowever, many existing computational methods require large amounts of known DDI\ninformation, which is scarce for emerging drugs. In this paper, we propose\nEmerGNN, a graph neural network (GNN) that can effectively predict interactions\nfor emerging drugs by leveraging the rich information in biomedical networks.\nEmerGNN learns pairwise representations of drugs by extracting the paths\nbetween drug pairs, propagating information from one drug to the other, and\nincorporating the relevant biomedical concepts on the paths. The different\nedges on the biomedical network are weighted to indicate the relevance for the\ntarget DDI prediction. Overall, EmerGNN has higher accuracy than existing\napproaches in predicting interactions for emerging drugs and can identify the\nmost relevant information on the biomedical network.",
            "author": [
                "Yongqi Zhang",
                "Quanming Yao",
                "Ling Yue",
                "Xian Wu",
                "Ziheng Zhang",
                "Zhenxi Lin",
                "Yefeng Zheng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09261v1",
                "http://arxiv.org/pdf/2311.09261v1"
            ],
            "primary_category": "q-bio.QM",
            "category": [
                "q-bio.QM",
                "cs.AI",
                "cs.CE",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08718v1",
            "title": "Decomposing Uncertainty for Large Language Models through Input\n  Clarification Ensembling",
            "updated": "2023-11-15T05:58:35Z",
            "published": "2023-11-15T05:58:35Z",
            "summary": "Uncertainty decomposition refers to the task of decomposing the total\nuncertainty of a model into data (aleatoric) uncertainty, resulting from the\ninherent complexity or ambiguity of the data, and model (epistemic)\nuncertainty, resulting from the lack of knowledge in the model. Performing\nuncertainty decomposition for large language models (LLMs) is an important step\ntoward improving the reliability, trustworthiness, and interpretability of\nLLMs, but this research task is very challenging and remains unresolved. The\nexisting canonical method, Bayesian Neural Network (BNN), cannot be applied to\nLLMs, because BNN requires training and ensembling multiple variants of models,\nwhich is infeasible or prohibitively expensive for LLMs. In this paper, we\nintroduce an uncertainty decomposition framework for LLMs, called input\nclarifications ensemble, which bypasses the need to train new models. Rather\nthan ensembling models with different parameters, our approach generates a set\nof clarifications for the input, feeds them into the fixed LLMs, and ensembles\nthe corresponding predictions. We show that our framework shares a symmetric\ndecomposition structure with BNN. Empirical evaluations demonstrate that the\nproposed framework provides accurate and reliable uncertainty quantification on\nvarious tasks. Code will be made publicly available at\nhttps://github.com/UCSB-NLP-Chang/llm_uncertainty .",
            "author": [
                "Bairu Hou",
                "Yujian Liu",
                "Kaizhi Qian",
                "Jacob Andreas",
                "Shiyu Chang",
                "Yang Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08718v1",
                "http://arxiv.org/pdf/2311.08718v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08702v1",
            "title": "Debate Helps Supervise Unreliable Experts",
            "updated": "2023-11-15T05:05:40Z",
            "published": "2023-11-15T05:05:40Z",
            "summary": "As AI systems are used to answer more difficult questions and potentially\nhelp create new knowledge, judging the truthfulness of their outputs becomes\nmore difficult and more important. How can we supervise unreliable experts,\nwhich have access to the truth but may not accurately report it, to give\nanswers that are systematically true and don't just superficially seem true,\nwhen the supervisor can't tell the difference between the two on their own? In\nthis work, we show that debate between two unreliable experts can help a\nnon-expert judge more reliably identify the truth. We collect a dataset of\nhuman-written debates on hard reading comprehension questions where the judge\nhas not read the source passage, only ever seeing expert arguments and short\nquotes selectively revealed by 'expert' debaters who have access to the\npassage. In our debates, one expert argues for the correct answer, and the\nother for an incorrect answer. Comparing debate to a baseline we call\nconsultancy, where a single expert argues for only one answer which is correct\nhalf of the time, we find that debate performs significantly better, with 84%\njudge accuracy compared to consultancy's 74%. Debates are also more efficient,\nbeing 68% of the length of consultancies. By comparing human to AI debaters, we\nfind evidence that with more skilled (in this case, human) debaters, the\nperformance of debate goes up but the performance of consultancy goes down. Our\nerror analysis also supports this trend, with 46% of errors in human debate\nattributable to mistakes by the honest debater (which should go away with\nincreased skill); whereas 52% of errors in human consultancy are due to\ndebaters obfuscating the relevant evidence from the judge (which should become\nworse with increased skill). Overall, these results show that debate is a\npromising approach for supervising increasingly capable but potentially\nunreliable AI systems.",
            "author": [
                "Julian Michael",
                "Salsabila Mahdi",
                "David Rein",
                "Jackson Petty",
                "Julien Dirani",
                "Vishakh Padmakumar",
                "Samuel R. Bowman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08702v1",
                "http://arxiv.org/pdf/2311.08702v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "I.2.0"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08690v1",
            "title": "Enabling CMF Estimation in Data-Constrained Scenarios: A\n  Semantic-Encoding Knowledge Mining Model",
            "updated": "2023-11-15T04:37:27Z",
            "published": "2023-11-15T04:37:27Z",
            "summary": "Precise estimation of Crash Modification Factors (CMFs) is central to\nevaluating the effectiveness of various road safety treatments and prioritizing\ninfrastructure investment accordingly. While customized study for each\ncountermeasure scenario is desired, the conventional CMF estimation approaches\nrely heavily on the availability of crash data at given sites. This not only\nmakes the estimation costly, but the results are also less transferable, since\nthe intrinsic similarities between different safety countermeasure scenarios\nare not fully explored. Aiming to fill this gap, this study introduces a novel\nknowledge-mining framework for CMF prediction. This framework delves into the\nconnections of existing countermeasures and reduces the reliance of CMF\nestimation on crash data availability and manual data collection. Specifically,\nit draws inspiration from human comprehension processes and introduces advanced\nNatural Language Processing (NLP) techniques to extract intricate variations\nand patterns from existing CMF knowledge. It effectively encodes unstructured\ncountermeasure scenarios into machine-readable representations and models the\ncomplex relationships between scenarios and CMF values. This new data-driven\nframework provides a cost-effective and adaptable solution that complements the\ncase-specific approaches for CMF estimation, which is particularly beneficial\nwhen availability of crash data or time imposes constraints. Experimental\nvalidation using real-world CMF Clearinghouse data demonstrates the\neffectiveness of this new approach, which shows significant accuracy\nimprovements compared to baseline methods. This approach provides insights into\nnew possibilities of harnessing accumulated transportation knowledge in various\napplications.",
            "author": [
                "Yanlin Qi",
                "Jia Li",
                "Michael Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08690v1",
                "http://arxiv.org/pdf/2311.08690v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08666v1",
            "title": "It Takes Two to Negotiate: Modeling Social Exchange in Online\n  Multiplayer Games",
            "updated": "2023-11-15T03:21:04Z",
            "published": "2023-11-15T03:21:04Z",
            "summary": "Online games are dynamic environments where players interact with each other,\nwhich offers a rich setting for understanding how players negotiate their way\nthrough the game to an ultimate victory. This work studies online player\ninteractions during the turn-based strategy game, Diplomacy. We annotated a\ndataset of over 10,000 chat messages for different negotiation strategies and\nempirically examined their importance in predicting long- and short-term game\noutcomes. Although negotiation strategies can be predicted reasonably\naccurately through the linguistic modeling of the chat messages, more is needed\nfor predicting short-term outcomes such as trustworthiness. On the other hand,\nthey are essential in graph-aware reinforcement learning approaches to predict\nlong-term outcomes, such as a player's success, based on their prior\nnegotiation history. We close with a discussion of the implications and impact\nof our work. The dataset is available at\nhttps://github.com/kj2013/claff-diplomacy.",
            "author": [
                "Kokil Jaidka",
                "Hansin Ahuja",
                "Lynnette Ng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08666v1",
                "http://arxiv.org/pdf/2311.08666v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.GT",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09259v1",
            "title": "ChemDuino Calorimetry to Determine the Enthalpy Change of Neutralization\n  of an Acid_Base Reaction: Making a Familiar Experiment Greener",
            "updated": "2023-11-15T03:12:35Z",
            "published": "2023-11-15T03:12:35Z",
            "summary": "In thermochemistry experiments, a large volume and relatively high\nconcentration of chemicals are often used to get a significant temperature\nchange using an analogue thermometer. A greener experimental protocol is\ndeveloped as an alternative using modern microcontroller boards, ChemDuino. The\naim of this research is to develop a low cost and pocket sized prototype for\nmeasuring temperature of solutions using Arduino Uno and sensor based on green\nchemistry and Do It Yourself methods. The construction of ChemDuino for\ntemperature sensing is made in the simplest manner for individuals with little\nknowledge of electronics. The details of all components are stipulated in this\npaper. This research was conducted in three phases, which includes preliminary,\nprototype development, and assessment phase. The device is used to determine\nthe enthalpy change of neutralization of NaOH and HCl solution. The enthalpy\nchange of neutralization, kJ/mol at 25.0 {\\deg}C from literature is -57.13\nkJ/mol, whereas the enthalpy change of neutralization, kJ/mol at 25.0 {\\deg}C\nfrom the experiment using ChemDuino Calorimetry is -56.87 kJ/mol with the\nestimated standard deviation of 1.9. The device has successfully measured the\ntemperature change of the reaction at a relatively lower concentration.\nChemDuino Calorimetry has a great potential, because of its reliability and\naccuracy in measurements, inexpensive setup, and interconnectivity.",
            "author": [
                "N. K. Prabowo",
                "M. Paristiowati",
                "I. Irwanto",
                "Afrizal",
                "Yusmaniar"
            ],
            "link": [
                "http://dx.doi.org/10.1063/5.0175294",
                "http://arxiv.org/abs/2311.09259v1",
                "http://arxiv.org/pdf/2311.09259v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph",
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08661v1",
            "title": "Deep Neural Network Identification of Limnonectes Species and New Class\n  Detection Using Image Data",
            "updated": "2023-11-15T02:57:59Z",
            "published": "2023-11-15T02:57:59Z",
            "summary": "As is true of many complex tasks, the work of discovering, describing, and\nunderstanding the diversity of life on Earth (viz., biological systematics and\ntaxonomy) requires many tools. Some of this work can be accomplished as it has\nbeen done in the past, but some aspects present us with challenges which\ntraditional knowledge and tools cannot adequately resolve. One such challenge\nis presented by species complexes in which the morphological similarities among\nthe group members make it difficult to reliably identify known species and\ndetect new ones. We address this challenge by developing new tools using the\nprinciples of machine learning to resolve two specific questions related to\nspecies complexes. The first question is formulated as a classification problem\nin statistics and machine learning and the second question is an\nout-of-distribution (OOD) detection problem. We apply these tools to a species\ncomplex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)\nand employ a morphological character (hind limb skin texture) traditionally\ntreated qualitatively in a quantitative and objective manner. We demonstrate\nthat deep neural networks can successfully automate the classification of an\nimage into a known species group for which it has been trained. We further\ndemonstrate that the algorithm can successfully classify an image into a new\nclass if the image does not belong to the existing classes. Additionally, we\nuse the larger MNIST dataset to test the performance of our OOD detection\nalgorithm. We finish our paper with some concluding remarks regarding the\napplication of these methods to species complexes and our efforts to document\ntrue biodiversity. This paper has online supplementary materials.",
            "author": [
                "Li Xu",
                "Yili Hong",
                "Eric P. Smith",
                "David S. McLeod",
                "Xinwei Deng",
                "Laura J. Freeman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08661v1",
                "http://arxiv.org/pdf/2311.08661v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08653v1",
            "title": "Quantum Locally Recoverable Codes",
            "updated": "2023-11-15T02:27:01Z",
            "published": "2023-11-15T02:27:01Z",
            "summary": "Classical locally recoverable codes, which permit highly efficient recovery\nfrom localized errors as well as global recovery from larger errors, provide\nsome of the most useful codes for distributed data storage in practice. In this\npaper, we initiate the study of quantum locally recoverable codes (qLRCs). In\nthe long term, like their classical counterparts, such qLRCs may be used for\nlarge-scale quantum data storage. Our results also have concrete implications\nfor quantum LDPC codes, which are applicable to near-term quantum\nerror-correction.\n  After defining quantum local recoverability, we provide an explicit\nconstruction of qLRCs based on the classical LRCs of Tamo and Barg (2014),\nwhich we show have (1) a close-to-optimal rate-distance tradeoff (i.e. near the\nSingleton bound), (2) an efficient decoder, and (3) permit good spatial\nlocality in a physical implementation. Although the analysis is significantly\nmore involved than in the classical case, we obtain close-to-optimal parameters\nby introducing a \"folded\" version of our quantum Tamo-Barg (qTB) codes, which\nwe then analyze using a combination of algebraic techniques. We furthermore\npresent and analyze two additional constructions using more basic techniques,\nnamely random qLRCs, and qLRCs from AEL distance amplification. Each of these\nconstructions has some advantages, but neither achieves all 3 properties of our\nfolded qTB codes described above.\n  We complement these constructions with Singleton-like bounds that show our\nqLRC constructions achieve close-to-optimal parameters. We also apply these\nresults to obtain Singleton-like bounds for qLDPC codes, which to the best of\nour knowledge are novel. We then show that even the weakest form of a stronger\nlocality property called local correctability, which permits more robust local\nrecovery and is achieved by certain classical codes, is impossible quantumly.",
            "author": [
                "Louis Golowich",
                "Venkatesan Guruswami"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08653v1",
                "http://arxiv.org/pdf/2311.08653v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08640v1",
            "title": "Multistage Collaborative Knowledge Distillation from Large Language\n  Models",
            "updated": "2023-11-15T01:28:28Z",
            "published": "2023-11-15T01:28:28Z",
            "summary": "We study semi-supervised sequence prediction tasks where labeled data are too\nscarce to effectively finetune a model and at the same time few-shot prompting\nof a large language model (LLM) has suboptimal performance. This happens when a\ntask, such as parsing, is expensive to annotate and also unfamiliar to a\npretrained LLM. In this paper, we present a discovery that student models\ndistilled from a prompted LLM can often generalize better than their teacher on\nsuch tasks. Leveraging this finding, we propose a new distillation method,\nmultistage collaborative knowledge distillation from an LLM (MCKD), for such\ntasks. MCKD first prompts an LLM using few-shot in-context learning to produce\npseudolabels for unlabeled data. Then, at each stage of distillation, a pair of\nstudents are trained on disjoint partitions of the pseudolabeled data. Each\nstudent subsequently produces new and improved pseudolabels for the unseen\npartition to supervise the next round of student(s) with. We show the benefit\nof multistage cross-partition labeling on two constituency parsing tasks. On\nCRAFT biomedical parsing, 3-stage MCKD with 50 labeled examples matches the\nperformance of supervised finetuning with 500 examples and outperforms the\nprompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively.",
            "author": [
                "Jiachen Zhao",
                "Wenlong Zhao",
                "Andrew Drozdov",
                "Benjamin Rozonoyer",
                "Md Arafat Sultan",
                "Jay-Yoon Lee",
                "Mohit Iyyer",
                "Andrew McCallum"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08640v1",
                "http://arxiv.org/pdf/2311.08640v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08639v1",
            "title": "ColorTrace: Fungible token coloring and attribution",
            "updated": "2023-11-15T01:26:03Z",
            "published": "2023-11-15T01:26:03Z",
            "summary": "We formally define the fungible token coloring problem of attributing\n(coloring) fungible tokens to originating entities (minters), and present, to\nour knowledge, the first practical onchain algorithm to solve it. Tracking\nattribution of colored tokens losslessly using existing approaches such as the\nColored Coins protocol is computationally intractable due to the per-wallet\nstorage requirements growing in proportion to the number of minters. Our first\ncontribution is an elegant solution to the single-chain token coloring problem,\nwhere colored tokens are atomically burned and minted to ensure each wallet\nonly contains tokens of a single color. Our second contribution is an extension\nto this single-chain token coloring algorithm to allow safe and efficient\ncrosschain token transfers. We present ColorTrace, an onchain algorithm to\nachieve globally consistent, economically feasible, fungible token coloring.",
            "author": [
                "Ryan Zarick",
                "Bryan Pellegrino",
                "Isaac Zhang",
                "Thomas Kim",
                "Caleb Banister"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08639v1",
                "http://arxiv.org/pdf/2311.08639v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08635v1",
            "title": "Spatio-Temporal Graph Neural Point Process for Traffic Congestion Event\n  Prediction",
            "updated": "2023-11-15T01:22:47Z",
            "published": "2023-11-15T01:22:47Z",
            "summary": "Traffic congestion event prediction is an important yet challenging task in\nintelligent transportation systems. Many existing works about traffic\nprediction integrate various temporal encoders and graph convolution networks\n(GCNs), called spatio-temporal graph-based neural networks, which focus on\npredicting dense variables such as flow, speed and demand in time snapshots,\nbut they can hardly forecast the traffic congestion events that are sparsely\ndistributed on the continuous time axis. In recent years, neural point process\n(NPP) has emerged as an appropriate framework for event prediction in\ncontinuous time scenarios. However, most conventional works about NPP cannot\nmodel the complex spatio-temporal dependencies and congestion evolution\npatterns. To address these limitations, we propose a spatio-temporal graph\nneural point process framework, named STGNPP for traffic congestion event\nprediction. Specifically, we first design the spatio-temporal graph learning\nmodule to fully capture the long-range spatio-temporal dependencies from the\nhistorical traffic state data along with the road network. The extracted\nspatio-temporal hidden representation and congestion event information are then\nfed into a continuous gated recurrent unit to model the congestion evolution\npatterns. In particular, to fully exploit the periodic information, we also\nimprove the intensity function calculation of the point process with a periodic\ngated mechanism. Finally, our model simultaneously predicts the occurrence time\nand duration of the next congestion. Extensive experiments on two real-world\ndatasets demonstrate that our method achieves superior performance in\ncomparison to existing state-of-the-art approaches.",
            "author": [
                "Guangyin Jin",
                "Lingbo Liu",
                "Fuxian Li",
                "Jincai Huang"
            ],
            "link": [
                "http://dx.doi.org/10.1609/aaai.v37i12.26669",
                "http://arxiv.org/abs/2311.08635v1",
                "http://arxiv.org/pdf/2311.08635v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08634v1",
            "title": "On the minimum degree of minimally $ t $-tough, claw-free graphs",
            "updated": "2023-11-15T01:14:15Z",
            "published": "2023-11-15T01:14:15Z",
            "summary": "A graph $ G $ is minimally $ t $-tough if the toughness of $ G $ is $ t $ and\ndeletion of any edge from $ G $ decreases its toughness. Katona et al.\nconjectured that the minimum degree of any minimally $ t $-tough graph is $\n\\lceil 2t\\rceil $ and proved that the minimum degree of minimally $ \\frac{1}2\n$-tough and $ 1 $-tough, claw-free graphs is 1 and 2, respectively. We have\nshow that every minimally $ 3/2 $-tough, claw-free graph has a vertex of degree\nof $ 3 $. In this paper, we give an upper bound on the minimum degree of\nminimally $t$-tough, claw-free graphs for $ t\\geq 2 $.",
            "author": [
                "Hui Ma",
                "Xiaomin Hu",
                "Weihua Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08634v1",
                "http://arxiv.org/pdf/2311.08634v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08622v1",
            "title": "Multiple-Question Multiple-Answer Text-VQA",
            "updated": "2023-11-15T01:00:02Z",
            "published": "2023-11-15T01:00:02Z",
            "summary": "We present Multiple-Question Multiple-Answer (MQMA), a novel approach to do\ntext-VQA in encoder-decoder transformer models. The text-VQA task requires a\nmodel to answer a question by understanding multi-modal content: text\n(typically from OCR) and an associated image. To the best of our knowledge,\nalmost all previous approaches for text-VQA process a single question and its\nassociated content to predict a single answer. In order to answer multiple\nquestions from the same image, each question and content are fed into the model\nmultiple times. In contrast, our proposed MQMA approach takes multiple\nquestions and content as input at the encoder and predicts multiple answers at\nthe decoder in an auto-regressive manner at the same time. We make several\nnovel architectural modifications to standard encoder-decoder transformers to\nsupport MQMA. We also propose a novel MQMA denoising pre-training task which is\ndesigned to teach the model to align and delineate multiple questions and\ncontent with associated answers. MQMA pre-trained model achieves\nstate-of-the-art results on multiple text-VQA datasets, each with strong\nbaselines. Specifically, on OCR-VQA (+2.5%), TextVQA (+1.4%), ST-VQA (+0.6%),\nDocVQA (+1.1%) absolute improvements over the previous state-of-the-art\napproaches.",
            "author": [
                "Peng Tang",
                "Srikar Appalaraju",
                "R. Manmatha",
                "Yusheng Xie",
                "Vijay Mahadevan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08622v1",
                "http://arxiv.org/pdf/2311.08622v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08617v1",
            "title": "Bank Performance Determinants: State of the Art and Future Research\n  Avenues",
            "updated": "2023-11-15T00:47:19Z",
            "published": "2023-11-15T00:47:19Z",
            "summary": "Banks' performance is an important topic for both professionals and\nresearchers. Given the important literature on this subject, this paper aims to\nbring an up-to-date and organized review of literature on the determinants of\nbanks performance. This paper discusses the main approaches that molded the\ndebate on banks performance and their main determinants. An in-depth\nunderstanding of these latter may allow on the one hand, bank managers and\nregulators to improve the sector efficiency and to deal with the new trends\nshaping the future of their industry and on the other hand, academicians to\nenrich research and knowledge on this field. Through the analysis of 54 studies\npublished in 42 peer-reviewed journals, we show that despite the importance of\nthe existent literature, the subject of bank performance factors did not reveal\nall its secrets and still constitute a fertile field for critical debates,\nespecially since the COVID-19 and the increasingly pressing rise in power of\ndigital transformation and artificial intelligence in general and FinTechs in\nparticular. The study concludes by suggesting new promising research avenues.",
            "author": [
                "Anas Azzabi",
                "Younes Lahrichi"
            ],
            "link": [
                "http://dx.doi.org/10.32038/NCAF.2023.09.03",
                "http://arxiv.org/abs/2311.08617v1",
                "http://arxiv.org/pdf/2311.08617v1"
            ],
            "primary_category": "q-fin.GN",
            "category": [
                "q-fin.GN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08614v1",
            "title": "XplainLLM: A QA Explanation Dataset for Understanding LLM\n  Decision-Making",
            "updated": "2023-11-15T00:34:28Z",
            "published": "2023-11-15T00:34:28Z",
            "summary": "Large Language Models (LLMs) have recently made impressive strides in natural\nlanguage understanding tasks. Despite their remarkable performance,\nunderstanding their decision-making process remains a big challenge. In this\npaper, we look into bringing some transparency to this process by introducing a\nnew explanation dataset for question answering (QA) tasks that integrates\nknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\nquestion-answer-explanation (QAE) triples. Each explanation in the dataset\nlinks the LLM's reasoning to entities and relations in the KGs. The explanation\ncomponent includes a why-choose explanation, a why-not-choose explanation, and\na set of reason-elements that underlie the LLM's decision. We leverage KGs and\ngraph attention networks (GAT) to find the reason-elements and transform them\ninto why-choose and why-not-choose explanations that are comprehensible to\nhumans. Through quantitative and qualitative evaluations, we demonstrate the\npotential of our dataset to improve the in-context learning of LLMs, and\nenhance their interpretability and explainability. Our work contributes to the\nfield of explainable AI by enabling a deeper understanding of the LLMs\ndecision-making process to make them more transparent and thereby, potentially\nmore reliable, to researchers and practitioners alike. Our dataset is available\nat: https://github.com/chen-zichen/XplainLLM_dataset.git",
            "author": [
                "Zichen Chen",
                "Jianda Chen",
                "Mitali Gaidhani",
                "Ambuj Singh",
                "Misha Sra"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08614v1",
                "http://arxiv.org/pdf/2311.08614v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08610v1",
            "title": "Converting Transformers to Polynomial Form for Secure Inference Over\n  Homomorphic Encryption",
            "updated": "2023-11-15T00:23:58Z",
            "published": "2023-11-15T00:23:58Z",
            "summary": "Designing privacy-preserving deep learning models is a major challenge within\nthe deep learning community. Homomorphic Encryption (HE) has emerged as one of\nthe most promising approaches in this realm, enabling the decoupling of\nknowledge between the model owner and the data owner. Despite extensive\nresearch and application of this technology, primarily in convolutional neural\nnetworks, incorporating HE into transformer models has been challenging because\nof the difficulties in converting these models into a polynomial form. We break\nnew ground by introducing the first polynomial transformer, providing the first\ndemonstration of secure inference over HE with transformers. This includes a\ntransformer architecture tailored for HE, alongside a novel method for\nconverting operators to their polynomial equivalent. This innovation enables us\nto perform secure inference on LMs with WikiText-103. It also allows us to\nperform image classification with CIFAR-100 and Tiny-ImageNet. Our models yield\nresults comparable to traditional methods, bridging the performance gap with\ntransformers of similar scale and underscoring the viability of HE for\nstate-of-the-art applications. Finally, we assess the stability of our models\nand conduct a series of ablations to quantify the contribution of each model\ncomponent.",
            "author": [
                "Itamar Zimerman",
                "Moran Baruch",
                "Nir Drucker",
                "Gilad Ezov",
                "Omri Soceanu",
                "Lior Wolf"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08610v1",
                "http://arxiv.org/pdf/2311.08610v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR",
                "F.2.2; I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08585v1",
            "title": "Unsupervised segmentation of irradiation$\\unicode{x2010}$induced\n  order$\\unicode{x2010}$disorder phase transitions in electron microscopy",
            "updated": "2023-11-14T23:13:59Z",
            "published": "2023-11-14T23:13:59Z",
            "summary": "We present a method for the unsupervised segmentation of electron microscopy\nimages, which are powerful descriptors of materials and chemical systems.\nImages are oversegmented into overlapping chips, and similarity graphs are\ngenerated from embeddings extracted from a domain$\\unicode{x2010}$pretrained\nconvolutional neural network (CNN). The Louvain method for community detection\nis then applied to perform segmentation. The graph representation provides an\nintuitive way of presenting the relationship between chips and communities. We\ndemonstrate our method to track irradiation$\\unicode{x2010}$induced amorphous\nfronts in thin films used for catalysis and electronics. This method has\npotential for \"on$\\unicode{x2010}$the$\\unicode{x2010}$fly\" segmentation to\nguide emerging automated electron microscopes.",
            "author": [
                "Arman H Ter-Petrosyan",
                "Jenna A Bilbrey",
                "Christina M Doty",
                "Bethany E Matthews",
                "Le Wang",
                "Yingge Du",
                "Eric Lang",
                "Khalid Hattar",
                "Steven R Spurgeon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08585v1",
                "http://arxiv.org/pdf/2311.08585v1"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci",
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08579v1",
            "title": "Graph-Induced Syntactic-Semantic Spaces in Transformer-Based Variational\n  AutoEncoders",
            "updated": "2023-11-14T22:47:23Z",
            "published": "2023-11-14T22:47:23Z",
            "summary": "The injection of syntactic information in Variational AutoEncoders (VAEs) has\nbeen shown to result in an overall improvement of performances and\ngeneralisation. An effective strategy to achieve such a goal is to separate the\nencoding of distributional semantic features and syntactic structures into\nheterogeneous latent spaces via multi-task learning or dual encoder\narchitectures. However, existing works employing such techniques are limited to\nLSTM-based VAEs. In this paper, we investigate latent space separation methods\nfor structural syntactic injection in Transformer-based VAE architectures\n(i.e., Optimus). Specifically, we explore how syntactic structures can be\nleveraged in the encoding stage through the integration of graph-based and\nsequential models, and how multiple, specialised latent representations can be\ninjected into the decoder's attention mechanism via low-rank operators. Our\nempirical evaluation, carried out on natural language sentences and\nmathematical expressions, reveals that the proposed end-to-end VAE architecture\ncan result in a better overall organisation of the latent space, alleviating\nthe information loss occurring in standard VAE setups, resulting in enhanced\nperformances on language modelling and downstream generation tasks.",
            "author": [
                "Yingji Zhang",
                "Marco Valentino",
                "Danilo S. Carvalho",
                "Ian Pratt-Hartmann",
                "Andr\u00e9 Freitas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08579v1",
                "http://arxiv.org/pdf/2311.08579v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08573v1",
            "title": "Topological Symmetries of the Heawood family",
            "updated": "2023-11-14T22:32:40Z",
            "published": "2023-11-14T22:32:40Z",
            "summary": "The {\\em topological symmetry group} of an embedding $\\Gamma$ of an abstract\ngraph $\\gamma$ in $S^3$ is the group of automorphisms of $\\gamma$ which can be\nrealized by homeomorphisms of the pair $(S^3, \\Gamma)$. These groups are\nmotivated by questions about the symmetries of molecules in space. In this\npaper, we find all the groups which can be realized as topological symmetry\ngroups for each of the graphs in the Heawood family. This is an important\ncollection of spatial graphs, containing the only intrinsically knotted graphs\nwith 21 or fewer edges. Unexpectedly, we discover that the graphs in this\nfamily are all intrinsically chiral, leading to conjectures about how intrinsic\nchirality is related to $\\nabla Y$ moves.",
            "author": [
                "Blake Mellor",
                "Robin Wilson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08573v1",
                "http://arxiv.org/pdf/2311.08573v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57M15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08563v1",
            "title": "Clustering coefficients for networks with higher order interactions",
            "updated": "2023-11-14T21:53:37Z",
            "published": "2023-11-14T21:53:37Z",
            "summary": "We introduce a clustering coefficient for nondirected and directed\nhypergraphs, which we call the quad clustering coefficient. We determine the\naverage quad clustering coefficient and its distribution in real-world\nhypergraphs and compare its value with those of random hypergraphs drawn from\nthe configuration model. We find that clustering in real-world hypergraphs is\nsignificantly different from those of random hypergraphs. Notably, we find that\nreal-world hypergraphs exhibit a nonnegligible fraction of nodes with a maximal\nvalue of the quad clustering coefficient, while we do not find such nodes in\nrandom hypergraphs. Moreover, these highly clustered nodes are not observed in\nan analysis based on the pairwise clustering coefficient of the associated\nprojected graph that has binary interactions, and hence higher order\ninteractions are required to identify nodes with a large quad clustering\ncoefficient.",
            "author": [
                "Gyeong-Gyun Ha",
                "Izaak Neri",
                "Alessia Annibale"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08563v1",
                "http://arxiv.org/pdf/2311.08563v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08560v1",
            "title": "Linear Colouring of Binomial Random Graphs",
            "updated": "2023-11-14T21:43:24Z",
            "published": "2023-11-14T21:43:24Z",
            "summary": "We investigate the linear chromatic number $\\chi_{\\text{lin}}(G(n,p))$ of the\nbinomial random graph $G(n,p)$ on $n$ vertices in which each edge appears\nindependently with probability $p=p(n)$. For dense random graphs ($np \\to\n\\infty$ as $n \\to \\infty$), we show that asymptotically almost surely\n$\\chi_{\\text{lin}}(G(n,p)) \\ge n (1 - O( (np)^{-1/2} ) ) = n(1-o(1))$.\nUnderstanding the order of the linear chromatic number for subcritical random\ngraphs ($np < 1$) and critical ones ($np=1$) is relatively easy. However,\nsupercritical sparse random graphs ($np = c$ for some constant $c > 1$) remain\nto be investigated.",
            "author": [
                "Austin Eide",
                "Pawe\u0142 Pra\u0142at"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08560v1",
                "http://arxiv.org/pdf/2311.08560v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08557v1",
            "title": "Low-light Pedestrian Detection in Visible and Infrared Image Feeds:\n  Issues and Challenges",
            "updated": "2023-11-14T21:39:15Z",
            "published": "2023-11-14T21:39:15Z",
            "summary": "Pedestrian detection has become a cornerstone for several high-level tasks,\nincluding autonomous driving, intelligent transportation, and traffic\nsurveillance. There are several works focussed on pedestrian detection using\nvisible images, mainly in the daytime. However, this task is very intriguing\nwhen the environmental conditions change to poor lighting or nighttime.\nRecently, new ideas have been spurred to use alternative sources, such as Far\nInfraRed (FIR) temperature sensor feeds for detecting pedestrians in low-light\nconditions. This study comprehensively reviews recent developments in low-light\npedestrian detection approaches. It systematically categorizes and analyses\nvarious algorithms from region-based to non-region-based and graph-based\nlearning methodologies by highlighting their methodologies, implementation\nissues, and challenges. It also outlines the key benchmark datasets that can be\nused for research and development of advanced pedestrian detection algorithms,\nparticularly in low-light situations",
            "author": [
                "Hrishikesh Vachhani",
                "Thangarajah Akilan",
                "Yash Devmurari",
                "Nisharaff Shaik",
                "Dhruvisha Patel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08557v1",
                "http://arxiv.org/pdf/2311.08557v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08551v1",
            "title": "Fast Partitioning of Pauli Strings into Commuting Families for\n  Expectation Value Measurements of Dense Operators",
            "updated": "2023-11-14T21:22:54Z",
            "published": "2023-11-14T21:22:54Z",
            "summary": "The cost of measuring quantum expectation values of an operator can be\nreduced by grouping the Pauli string ($SU(2)$ tensor product) decomposition of\nthe operator into maximally commuting sets. We detail an algorithm, presented\nin [1], to partition the full set of $m$-qubit Pauli strings into the minimal\nnumber of commuting families, and benchmark the performance with dense\nHamiltonians on IBM hardware. Here we also compare how our method scales\ncompared to graph-theoretic techniques for the generally commuting case.",
            "author": [
                "Nouman Butt",
                "Andrew Lytle",
                "Ben Reggio",
                "Patrick Draper"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08551v1",
                "http://arxiv.org/pdf/2311.08551v1"
            ],
            "primary_category": "hep-lat",
            "category": [
                "hep-lat",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08549v1",
            "title": "Manifold learning in Wasserstein space",
            "updated": "2023-11-14T21:21:35Z",
            "published": "2023-11-14T21:21:35Z",
            "summary": "This paper aims at building the theoretical foundations for manifold learning\nalgorithms in the space of absolutely continuous probability measures on a\ncompact and convex subset of $\\mathbb{R}^d$, metrized with the Wasserstein-2\ndistance $W$. We begin by introducing a natural construction of submanifolds\n$\\Lambda$ of probability measures equipped with metric $W_\\Lambda$, the\ngeodesic restriction of $W$ to $\\Lambda$. In contrast to other constructions,\nthese submanifolds are not necessarily flat, but still allow for local\nlinearizations in a similar fashion to Riemannian submanifolds of\n$\\mathbb{R}^d$. We then show how the latent manifold structure of\n$(\\Lambda,W_{\\Lambda})$ can be learned from samples $\\{\\lambda_i\\}_{i=1}^N$ of\n$\\Lambda$ and pairwise extrinsic Wasserstein distances $W$ only. In particular,\nwe show that the metric space $(\\Lambda,W_{\\Lambda})$ can be asymptotically\nrecovered in the sense of Gromov--Wasserstein from a graph with nodes\n$\\{\\lambda_i\\}_{i=1}^N$ and edge weights $W(\\lambda_i,\\lambda_j)$. In addition,\nwe demonstrate how the tangent space at a sample $\\lambda$ can be\nasymptotically recovered via spectral analysis of a suitable \"covariance\noperator\" using optimal transport maps from $\\lambda$ to sufficiently close and\ndiverse samples $\\{\\lambda_i\\}_{i=1}^N$. The paper closes with some explicit\nconstructions of submanifolds $\\Lambda$ and numerical examples on the recovery\nof tangent spaces through spectral analysis.",
            "author": [
                "Keaton Hamm",
                "Caroline Moosm\u00fcller",
                "Bernhard Schmitzer",
                "Matthew Thorpe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08549v1",
                "http://arxiv.org/pdf/2311.08549v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "math.DG",
                "49Q22, 41A65, 58B20, 53Z50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08546v1",
            "title": "A Category of Genes",
            "updated": "2023-11-14T21:19:14Z",
            "published": "2023-11-14T21:19:14Z",
            "summary": "Understanding how genes interact and relate to each other is a fundamental\nquestion in biology. However, current practices for describing these\nrelationships, such as drawing diagrams or graphs in a somewhat arbitrary\nmanner, limit our ability to integrate various aspects of the gene functions\nand view the genome holistically. To overcome these limitations, we need a more\nappropriate way to describe the intricate relationships between genes.\nInterestingly, category theory, an abstract field of mathematics seemingly\nunrelated to biology, has emerged as a powerful language for describing\nrelations in general. We propose that category theory could provide a framework\nfor unifying our knowledge of genes and their relationships.\n  As a starting point, we construct a category of genes, with its morphisms\nabstracting various aspects of the relationships betweens genes. These\nrelationships include, but not limited to, the order of genes on the\nchromosomes, the physical or genetic interactions, the signalling pathways, the\ngene ontology causal activity models (GO-CAM) and gene groups. Previously, they\nwere encoded by miscellaneous networks or graphs, while our work unifies them\nin a consistent manner as a category. By doing so, we hope to view the\nrelationships between genes systematically. In the long run, this paves a\npromising way for us to understand the fundamental principles that govern gene\nregulation and function.",
            "author": [
                "Yanying Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08546v1",
                "http://arxiv.org/pdf/2311.08546v1"
            ],
            "primary_category": "q-bio.OT",
            "category": [
                "q-bio.OT",
                "math.CT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08543v1",
            "title": "2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection",
            "updated": "2023-11-14T21:16:40Z",
            "published": "2023-11-14T21:16:40Z",
            "summary": "Orthogonal time frequency space (OTFS) is a promising modulation scheme for\nwireless communication in high-mobility scenarios. Recently, a reservoir\ncomputing (RC) based approach has been introduced for online subframe-based\nsymbol detection in the OTFS system, where only a limited number of\nover-the-air (OTA) pilot symbols are utilized for training. However, this\napproach does not leverage the domain knowledge specific to the OTFS system.\nThis paper introduces a novel two-dimensional RC (2D-RC) method that\nincorporates the structural knowledge of the OTFS system into the design for\nonline symbol detection on a subframe basis. Specifically, as the channel\nresponse acts as a two-dimensional (2D) operation over the transmitted\ninformation symbols in the delay-Doppler (DD) domain, the 2D-RC is designed to\nhave a 2D structure to equalize the channel. With the introduced architecture,\nthe 2D-RC can benefit from the predictable channel representation in the DD\ndomain. Moreover, unlike the previous work that requires multiple RCs to learn\nthe channel feature, the 2D-RC only requires a single neural network for\ndetection. Experimental results demonstrate the effectiveness of the 2D-RC\napproach across different OTFS system variants and modulation orders.",
            "author": [
                "Jiarui Xu",
                "Karim Said",
                "Lizhong Zheng",
                "Lingjia Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08543v1",
                "http://arxiv.org/pdf/2311.08543v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08541v1",
            "title": "Three invariants of geometrically vertex decomposable ideals",
            "updated": "2023-11-14T21:14:46Z",
            "published": "2023-11-14T21:14:46Z",
            "summary": "We study three invariants of geometrically vertex decomposable ideals: the\nCastelnuovo-Mumford regularity, the multiplicity, and the $a$-invariant. We\nshow that these invariants can be computed recursively using the ideals that\nappear in the geometric vertex decomposition process. As an application, we\nprove that the $a$-invariant of a geometrically vertex decomposable ideal is\nnon-positive. We also recover some previously known results in the literature\nincluding a formula for the regularity of the Stanley--Reisner ideal of a pure\nvertex decomposable simplicial complex, and proofs that some well-known\nfamilies of ideals are Hilbertian. Finally, we apply our recursions to the\nstudy of toric ideals of bipartite graphs. Included among our results on this\ntopic is a new proof for a known bound on the $a$-invariant of a toric ideal of\na bipartite graph.",
            "author": [
                "Thai Thanh Nguyen",
                "Jenna Rajchgot",
                "Adam Van Tuyl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08541v1",
                "http://arxiv.org/pdf/2311.08541v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO",
                "13P10, 14M25, 05E40"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08530v1",
            "title": "SceneScore: Learning a Cost Function for Object Arrangement",
            "updated": "2023-11-14T20:55:40Z",
            "published": "2023-11-14T20:55:40Z",
            "summary": "Arranging objects correctly is a key capability for robots which unlocks a\nwide range of useful tasks. A prerequisite for creating successful arrangements\nis the ability to evaluate the desirability of a given arrangement. Our method\n\"SceneScore\" learns a cost function for arrangements, such that desirable,\nhuman-like arrangements have a low cost. We learn the distribution of training\narrangements offline using an energy-based model, solely from example images\nwithout requiring environment interaction or human supervision. Our model is\nrepresented by a graph neural network which learns object-object relations,\nusing graphs constructed from images. Experiments demonstrate that the learned\ncost function can be used to predict poses for missing objects, generalise to\nnovel objects using semantic features, and can be composed with other cost\nfunctions to satisfy constraints at inference time.",
            "author": [
                "Ivan Kapelyukh",
                "Edward Johns"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08530v1",
                "http://arxiv.org/pdf/2311.08530v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08521v1",
            "title": "On Hamilton cycles in graphs defined by intersecting set systems",
            "updated": "2023-11-14T20:33:36Z",
            "published": "2023-11-14T20:33:36Z",
            "summary": "In 1970 Lov\\'asz conjectured that every connected vertex-transitive graph\nadmits a Hamilton cycle, apart from five exceptional graphs. This conjecture\nhas recently been settled for graphs defined by intersecting set systems, which\nfeature prominently throughout combinatorics. In this expository article, we\nretrace these developments and give an overview of the many different\ningredients in the proofs.",
            "author": [
                "Torsten M\u00fctze"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08521v1",
                "http://arxiv.org/pdf/2311.08521v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08514v1",
            "title": "An algorithm for Tambara-Yamagami quantum invariants of 3-manifolds,\n  parameterized by the first Betti number",
            "updated": "2023-11-14T20:09:58Z",
            "published": "2023-11-14T20:09:58Z",
            "summary": "Quantum topology provides various frameworks for defining and computing\ninvariants of manifolds. One such framework of substantial interest in both\nmathematics and physics is the Turaev-Viro-Barrett-Westbury state sum\nconstruction, which uses the data of a spherical fusion category to define\ntopological invariants of triangulated 3-manifolds via tensor network\ncontractions. In this work we consider a restricted class of state sum\ninvariants of 3-manifolds derived from Tambara-Yamagami categories. These\ncategories are particularly simple, being entirely specified by three pieces of\ndata: a finite abelian group, a bicharacter of that group, and a sign $\\pm 1$.\nDespite being one of the simplest sources of state sum invariants, the\ncomputational complexities of Tambara-Yamagami invariants are yet to be fully\nunderstood.\n  We make substantial progress on this problem. Our main result is the\nexistence of a general fixed parameter tractable algorithm for all such\ntopological invariants, where the parameter is the first Betti number of the\n3-manifold with $\\mathbb{Z}/2\\mathbb{Z}$ coefficients. We also explain that\nthese invariants are sometimes #P-hard to compute (and we expect that this is\nalmost always the case).\n  Contrary to other domains of computational topology, such as graphs on\nsurfaces, very few hard problems in 3-manifold topology are known to admit FPT\nalgorithms with a topological parameter. However, such algorithms are of\nparticular interest as their complexity depends only polynomially on the\ncombinatorial representation of the input, regardless of size or combinatorial\nwidth. Additionally, in the case of Betti numbers, the parameter itself is\neasily computable in polynomial time.",
            "author": [
                "Colleen Delaney",
                "Cl\u00e9ment Maria",
                "Eric Samperton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08514v1",
                "http://arxiv.org/pdf/2311.08514v1"
            ],
            "primary_category": "cs.CG",
            "category": [
                "cs.CG",
                "math.GT",
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08513v1",
            "title": "Query Efficient Weighted Stochastic Matching",
            "updated": "2023-11-14T20:08:58Z",
            "published": "2023-11-14T20:08:58Z",
            "summary": "In this paper, we study the weighted stochastic matching problem. Let $G=(V,\nE)$ be a given edge-weighted graph and let its realization $\\mathcal{G}$ be a\nrandom subgraph of $G$ that includes each edge $e\\in E$ independently with a\nknown probability $p_e$. The goal in this problem is to pick a sparse subgraph\n$Q$ of $G$ without prior knowledge of $G$'s realization, such that the maximum\nweight matching among the realized edges of $Q$ (i.e. the subgraph $Q\\cap\n\\mathcal{G}$) in expectation approximates the maximum weight matching of the\nentire realization $\\mathcal{G}$.\n  Attaining any constant approximation ratio for this problem requires\nselecting a subgraph of max-degree $\\Omega(1/p)$ where $p=\\min_{e\\in E} p_e$.\nOn the positive side, there exists a $(1-\\epsilon)$-approximation algorithm by\nBehnezhad and Derakhshan, albeit at the cost of max-degree having exponential\ndependence on $1/p$. Within the $\\text{poly}(1/p)$ regime, however, the\nbest-known algorithm achieves a $0.536$ approximation ratio due to Dughmi,\nKalayci, and Patel improving over the $0.501$ approximation algorithm by\nBehnezhad, Farhadi, Hajiaghayi, and Reyhani.\n  In this work, we present a 0.68 approximation algorithm with $O(1/p)$ queries\nper vertex, which is asymptotically tight. This is even an improvement over the\nbest-known approximation ratio of $2/3$ for unweighted graphs within the\n$\\text{poly}(1/p)$ regime due to Assadi and Bernstein. The $2/3$ approximation\nratio is proven tight in the presence of a few correlated edges in\n$\\mathcal{G}$, indicating that surpassing the $2/3$ barrier should rely on the\nindependent realization of edges. Our analysis involves reducing the problem to\ndesigning a randomized matching algorithm on a given stochastic graph with some\nvariance-bounding properties.",
            "author": [
                "Mahsa Derakhshan",
                "Mohammad Saneian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08513v1",
                "http://arxiv.org/pdf/2311.08513v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08511v1",
            "title": "CoRE-CoG: Conversational Recommendation of Entities using Constrained\n  Generation",
            "updated": "2023-11-14T20:07:34Z",
            "published": "2023-11-14T20:07:34Z",
            "summary": "End-to-end conversational recommendation systems (CRS) generate responses by\nleveraging both dialog history and a knowledge base (KB). A CRS mainly faces\nthree key challenges: (1) at each turn, it must decide if recommending a KB\nentity is appropriate; if so, it must identify the most relevant KB entity to\nrecommend; and finally, it must recommend the entity in a fluent utterance that\nis consistent with the conversation history. Recent CRSs do not pay sufficient\nattention to these desiderata, often generating unfluent responses or not\nrecommending (relevant) entities at the right turn. We introduce a new CRS we\ncall CoRE-CoG. CoRE-CoG addresses the limitations in prior systems by\nimplementing (1) a recommendation trigger that decides if the system utterance\nshould include an entity, (2) a type pruning module that improves the relevance\nof recommended entities, and (3) a novel constrained response generator to make\nrecommendations while maintaining fluency. Together, these modules ensure\nsimultaneous accurate recommendation decisions and fluent system utterances.\nExperiments with recent benchmarks show the superiority particularly on\nconditional generation sub-tasks with close to 10 F1 and 4 Recall@1 percent\npoints gain over baselines.",
            "author": [
                "Harshvardhan Srivastava",
                "Kanav Pruthi",
                "Soumen Chakrabarti",
                "Mausam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08511v1",
                "http://arxiv.org/pdf/2311.08511v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08505v1",
            "title": "Semi-Structured Chain-of-Thought: Integrating Multiple Sources of\n  Knowledge for Improved Language Model Reasoning",
            "updated": "2023-11-14T19:53:53Z",
            "published": "2023-11-14T19:53:53Z",
            "summary": "An important open question pertaining to the use of large language models for\nknowledge-intensive tasks is how to effectively integrate knowledge from three\nsources: the model's parametric memory, external structured knowledge, and\nexternal unstructured knowledge. Most existing prompting methods either rely\nsolely on one or two of these sources, or require repeatedly invoking large\nlanguage models to generate similar or identical content. In this work, we\novercome these limitations by introducing a novel semi-structured prompting\napproach that seamlessly integrates the model's parametric memory with\nunstructured knowledge from text documents and structured knowledge from\nknowledge graphs. Experimental results on open-domain multi-hop question\nanswering datasets demonstrate that our prompting method significantly\nsurpasses existing techniques, even exceeding those which require fine-tuning.",
            "author": [
                "Xin Su",
                "Tiep Le",
                "Steven Bethard",
                "Phillip Howard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08505v1",
                "http://arxiv.org/pdf/2311.08505v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08476v3",
            "title": "Deconstructing Alien Hunting",
            "updated": "2023-11-26T22:41:43Z",
            "published": "2023-11-14T19:08:25Z",
            "summary": "The search for extraterrestrial (alien) life is one of the greatest\nscientific quests yet raises fundamental questions about just what we should be\nlooking for and how. We approach alien hunting from the perspective of an\nexperimenter engaging in binary classification with some true and confounding\npositive probability (TPP and CPP). We derive the Bayes factor in such a\nframework between two competing hypotheses, which we use to classify\nexperiments as either impotent, imperfect or ideal. Similarly, the experimenter\ncan be classified as dogmatic, biased or agnostic. We show how the unbounded\nexplanatory and evasion capability of aliens poses fundamental problems to\nexperiments directly seeking aliens. Instead, we advocate framing the\nexperiments as looking for that outside of known processes, which means the\nhypotheses we test do not directly concern aliens per se. To connect back to\naliens requires a second level of model selection, for which we derive the\nfinal odds ratio in a Bayesian framework. This reveals that it is fundamentally\nimpossible to ever establish alien life at some threshold odds ratio,\n$\\mathcal{O}_{\\mathrm{crit}}$, unless we deem the prior probability that some\nas-yet-undiscovered natural process could explain the event is less than\n$(1+\\mathcal{O}_{\\mathrm{crit}})^{-1}$. This elucidates how alien hunters need\nto carefully consider the challenging problem of how probable unknown unknowns\nare, such as new physics or chemistry, and how it is arguably most fruitful to\nfocus on experiments for which our domain knowledge is thought to be\nasymptotically complete.",
            "author": [
                "David Kipping",
                "Jason Wright"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08476v3",
                "http://arxiv.org/pdf/2311.08476v3"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08401v1",
            "title": "Fine-tuning Language Models for Factuality",
            "updated": "2023-11-14T18:59:15Z",
            "published": "2023-11-14T18:59:15Z",
            "summary": "The fluency and creativity of large pre-trained language models (LLMs) have\nled to their widespread use, sometimes even as a replacement for traditional\nsearch engines. Yet language models are prone to making convincing but\nfactually inaccurate claims, often referred to as 'hallucinations.' These\nerrors can inadvertently spread misinformation or harmfully perpetuate\nmisconceptions. Further, manual fact-checking of model responses is a\ntime-consuming process, making human factuality labels expensive to acquire. In\nthis work, we fine-tune language models to be more factual, without human\nlabeling and targeting more open-ended generation settings than past work. We\nleverage two key recent innovations in NLP to do so. First, several recent\nworks have proposed methods for judging the factuality of open-ended text by\nmeasuring consistency with an external knowledge base or simply a large model's\nconfidence scores. Second, the direct preference optimization algorithm enables\nstraightforward fine-tuning of language models on objectives other than\nsupervised imitation, using a preference ranking over possible model responses.\nWe show that learning from automatically generated factuality preference\nrankings, generated either through existing retrieval systems or our novel\nretrieval-free approach, significantly improves the factuality (percent of\ngenerated claims that are correct) of Llama-2 on held-out topics compared with\nRLHF or decoding strategies targeted at factuality. At 7B scale, compared to\nLlama-2-chat, we observe 58% and 40% reduction in factual error rate when\ngenerating biographies and answering medical questions, respectively.",
            "author": [
                "Katherine Tian",
                "Eric Mitchell",
                "Huaxiu Yao",
                "Christopher D. Manning",
                "Chelsea Finn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08401v1",
                "http://arxiv.org/pdf/2311.08401v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08400v1",
            "title": "Towards Open-Ended Visual Recognition with Large Language Model",
            "updated": "2023-11-14T18:59:01Z",
            "published": "2023-11-14T18:59:01Z",
            "summary": "Localizing and recognizing objects in the open-ended physical world poses a\nlong-standing challenge within the domain of machine perception. Recent methods\nhave endeavored to address the issue by employing a class-agnostic mask (or\nbox) proposal model, complemented by an open-vocabulary classifier (e.g., CLIP)\nusing pre-extracted text embeddings. However, it is worth noting that these\nopen-vocabulary recognition models still exhibit limitations in practical\napplications. On one hand, they rely on the provision of class names during\ntesting, where the recognition performance heavily depends on this predefined\nset of semantic classes by users. On the other hand, when training with\nmultiple datasets, human intervention is required to alleviate the label\ndefinition conflict between them. In this paper, we introduce the OmniScient\nModel (OSM), a novel Large Language Model (LLM) based mask classifier, as a\nstraightforward and effective solution to the aforementioned challenges.\nSpecifically, OSM predicts class labels in a generative manner, thus removing\nthe supply of class names during both training and testing. It also enables\ncross-dataset training without any human interference, exhibiting robust\ngeneralization capabilities due to the world knowledge acquired from the LLM.\nBy combining OSM with an off-the-shelf mask proposal model, we present\npromising results on various benchmarks, and demonstrate its effectiveness in\nhandling novel concepts. Code/model are available at\nhttps://github.com/bytedance/OmniScient-Model.",
            "author": [
                "Qihang Yu",
                "Xiaohui Shen",
                "Liang-Chieh Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08400v1",
                "http://arxiv.org/pdf/2311.08400v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08398v2",
            "title": "Are Large Language Models Temporally Grounded?",
            "updated": "2023-11-16T09:41:28Z",
            "published": "2023-11-14T18:57:15Z",
            "summary": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot\nperceive and interact with the environment, it is impossible to answer this\nquestion directly. Instead, we provide LLMs with textual narratives and probe\nthem with respect to their common-sense knowledge of the structure and duration\nof events, their ability to order events along a timeline, and self-consistency\nwithin their temporal model (e.g., temporal relations such as after and before\nare mutually exclusive for any pair of events). We evaluate state-of-the-art\nLLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities.\nGenerally, we find that LLMs lag significantly behind both human performance as\nwell as small-scale, specialised LMs. In-context learning, instruction tuning,\nand chain-of-thought prompting reduce this gap only to a limited degree.\nCrucially, LLMs struggle the most with self-consistency, displaying incoherent\nbehaviour in at least 27.23% of their predictions. Contrary to expectations, we\nalso find that scaling the model size does not guarantee positive gains in\nperformance. To explain these results, we study the sources from which LLMs may\ngather temporal information: we find that sentence ordering in unlabelled\ntexts, available during pre-training, is only weakly correlated with event\nordering. Moreover, public instruction tuning mixtures contain few temporal\ntasks. Hence, we conclude that current LLMs lack a consistent temporal model of\ntextual narratives. Code, datasets, and LLM outputs are available at\nhttps://github.com/yfqiu-nlp/temporal-llms.",
            "author": [
                "Yifu Qiu",
                "Zheng Zhao",
                "Yftah Ziser",
                "Anna Korhonen",
                "Edoardo M. Ponti",
                "Shay B. Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08398v2",
                "http://arxiv.org/pdf/2311.08398v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08381v1",
            "title": "Automated detection of laser cooling schemes for ultracold molecules",
            "updated": "2023-11-14T18:44:24Z",
            "published": "2023-11-14T18:44:24Z",
            "summary": "One of the demanding frontiers in ultracold science is identifying laser\ncooling schemes for complex atoms and molecules, out of their vast spectra of\ninternal states. Motivated by a need to expand the set of available ultracold\nmolecules for applications in fundamental physics, chemistry, astrochemistry,\nand quantum simulation, we propose and demonstrate an automated graph-based\nsearch approach for viable laser cooling schemes. The method is time efficient\nand the outcomes greatly surpass the results of manual searches used so far. We\ndiscover new laser cooling schemes for C$_2$, OH$^+$, CN, YO, and CO$_2$ that\ncan be viewed as surprising or counterintuitive compared to previously\nidentified laser cooling schemes. In addition, a central insight of this work\nis that the reinterpretation of quantum states and transitions between them as\na graph can dramatically enhance our ability to identify new quantum control\nschemes for complex quantum systems. As such, this approach will also be\napplicable to complex atoms and, in fact, any complex many-body quantum system\nwith a discrete spectrum of internal states.",
            "author": [
                "Anna Dawid",
                "Niccol\u00f2 Bigagli",
                "Daniel W. Savin",
                "Sebastian Will"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08381v1",
                "http://arxiv.org/pdf/2311.08381v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.quant-gas",
                "physics.atm-clus"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08377v1",
            "title": "Learning to Filter Context for Retrieval-Augmented Generation",
            "updated": "2023-11-14T18:41:54Z",
            "published": "2023-11-14T18:41:54Z",
            "summary": "On-the-fly retrieval of relevant knowledge has proven an essential element of\nreliable systems for tasks such as open-domain question answering and fact\nverification. However, because retrieval systems are not perfect, generation\nmodels are required to generate outputs given partially or entirely irrelevant\npassages. This can cause over- or under-reliance on context, and result in\nproblems in the generated output such as hallucinations. To alleviate these\nproblems, we propose FILCO, a method that improves the quality of the context\nprovided to the generator by (1) identifying useful context based on lexical\nand information-theoretic approaches, and (2) training context filtering models\nthat can filter retrieved contexts at test time. We experiment on six\nknowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our\nmethod outperforms existing approaches on extractive question answering (QA),\ncomplex multi-hop and long-form QA, fact verification, and dialog generation\ntasks. FILCO effectively improves the quality of context, whether or not it\nsupports the canonical output.",
            "author": [
                "Zhiruo Wang",
                "Jun Araki",
                "Zhengbao Jiang",
                "Md Rizwan Parvez",
                "Graham Neubig"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08377v1",
                "http://arxiv.org/pdf/2311.08377v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08367v1",
            "title": "Arboricity-Dependent Algorithms for Edge Coloring",
            "updated": "2023-11-14T18:26:50Z",
            "published": "2023-11-14T18:26:50Z",
            "summary": "The problem of edge coloring has been extensively studied over the years. The\nmain conceptual contribution of this work is in identifying a surprisingly\nsimple connection between the problem of $(\\Delta +O(\\alpha))$-edge coloring\nand a certain canonical graph decomposition in graphs of arboricity $\\alpha$,\nfor which efficient algorithms are known across various computational models.\n  We first leverage such graph decompositions to provide fast $(\\Delta\n+O(\\alpha))$-edge coloring algorithms in the standard {\\em static} (sequential\nand distributed) settings. Further, as our main technical contribution, we show\nhow to efficiently maintain a $(\\Delta +O(\\alpha))$-edge coloring in the\nstandard {\\em dynamic} model. Consequently, we improve over the\nstate-of-the-art edge coloring algorithms in these models for graphs of\nsufficiently small arboricity.",
            "author": [
                "Sayan Bhattacharya",
                "Mart\u00edn Costa",
                "Nadav Panski",
                "Shay Solomon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08367v1",
                "http://arxiv.org/pdf/2311.08367v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08355v1",
            "title": "Mustango: Toward Controllable Text-to-Music Generation",
            "updated": "2023-11-14T17:54:38Z",
            "published": "2023-11-14T17:54:38Z",
            "summary": "With recent advancements in text-to-audio and text-to-music based on latent\ndiffusion models, the quality of generated content has been reaching new\nheights. The controllability of musical aspects, however, has not been\nexplicitly explored in text-to-music systems yet. In this paper, we present\nMustango, a music-domain-knowledge-inspired text-to-music system based on\ndiffusion, that expands the Tango text-to-audio model. Mustango aims to control\nthe generated music, not only with general text captions, but from more rich\ncaptions that could include specific instructions related to chords, beats,\ntempo, and key. As part of Mustango, we propose MuNet, a\nMusic-Domain-Knowledge-Informed UNet sub-module to integrate these\nmusic-specific features, which we predict from the text prompt, as well as the\ngeneral text embedding, into the diffusion denoising process. To overcome the\nlimited availability of open datasets of music with text captions, we propose a\nnovel data augmentation method that includes altering the harmonic, rhythmic,\nand dynamic aspects of music audio and using state-of-the-art Music Information\nRetrieval methods to extract the music features which will then be appended to\nthe existing descriptions in text format. We release the resulting MusicBench\ndataset which contains over 52K instances and includes music-theory-based\ndescriptions in the caption text. Through extensive experiments, we show that\nthe quality of the music generated by Mustango is state-of-the-art, and the\ncontrollability through music-specific text prompts greatly outperforms other\nmodels in terms of desired chords, beat, key, and tempo, on multiple datasets.",
            "author": [
                "Jan Melechovsky",
                "Zixun Guo",
                "Deepanway Ghosal",
                "Navonil Majumder",
                "Dorien Herremans",
                "Soujanya Poria"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08355v1",
                "http://arxiv.org/pdf/2311.08355v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08332v1",
            "title": "Graph Curve Matroids",
            "updated": "2023-11-14T17:25:05Z",
            "published": "2023-11-14T17:25:05Z",
            "summary": "We introduce a new class of matroids, called graph curve matroids. A graph\ncurve matroid is associated to a graph and defined on the vertices of the graph\nas a ground set. We prove that these matroids provide a combinatorial\ndescription of hyperplane sections of degenerate canonical curves in algebraic\ngeometry. Our focus lies on graphs that are 2-connected and trivalent, which\ndefine identically self-dual graph curve matroids, but we also develop\ngeneralizations. Finally, we provide an algorithm to compute the graph curve\nmatroid associated to a given graph, as well as an implementation and data of\nexamples that can be used in Macaulay2.",
            "author": [
                "Alheydis Geiger",
                "Kevin Kuehn",
                "Raluca Vlad"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08332v1",
                "http://arxiv.org/pdf/2311.08332v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AG",
                "05E14"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08329v3",
            "title": "KTRL+F: Knowledge-Augmented In-Document Search",
            "updated": "2023-11-16T09:38:39Z",
            "published": "2023-11-14T17:18:08Z",
            "summary": "We introduce a new problem KTRL+F, a knowledge-augmented in-document search\ntask that necessitates real-time identification of all semantic targets within\na document with the awareness of external sources through a single natural\nquery. This task addresses following unique challenges for in-document search:\n1) utilizing knowledge outside the document for extended use of additional\ninformation about targets to bridge the semantic gap between the query and the\ntargets, and 2) balancing between real-time applicability with the performance.\nWe analyze various baselines in KTRL+F and find there are limitations of\nexisting models, such as hallucinations, low latency, or difficulties in\nleveraging external knowledge. Therefore we propose a Knowledge-Augmented\nPhrase Retrieval model that shows a promising balance between speed and\nperformance by simply augmenting external knowledge embedding in phrase\nembedding. Additionally, we conduct a user study to verify whether solving\nKTRL+F can enhance search experience of users. It demonstrates that even with\nour simple model users can reduce the time for searching with less queries and\nreduced extra visits to other sources for collecting evidence. We encourage the\nresearch community to work on KTRL+F to enhance more efficient in-document\ninformation access.",
            "author": [
                "Hanseok Oh",
                "Haebin Shin",
                "Miyoung Ko",
                "Hyunji Lee",
                "Minjoon Seo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08329v3",
                "http://arxiv.org/pdf/2311.08329v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08441v1",
            "title": "MasterRTL: A Pre-Synthesis PPA Estimation Framework for Any RTL Design",
            "updated": "2023-11-14T17:17:32Z",
            "published": "2023-11-14T17:17:32Z",
            "summary": "In modern VLSI design flow, the register-transfer level (RTL) stage is a\ncritical point, where designers define precise design behavior with hardware\ndescription languages (HDLs) like Verilog. Since the RTL design is in the\nformat of HDL code, the standard way to evaluate its quality requires\ntime-consuming subsequent synthesis steps with EDA tools. This time-consuming\nprocess significantly impedes design optimization at the early RTL stage.\nDespite the emergence of some recent ML-based solutions, they fail to maintain\nhigh accuracy for any given RTL design. In this work, we propose an innovative\npre-synthesis PPA estimation framework named MasterRTL. It first converts the\nHDL code to a new bit-level design representation named the simple operator\ngraph (SOG). By only adopting single-bit simple operators, this SOG proves to\nbe a general representation that unifies different design types and styles. The\nSOG is also more similar to the target gate-level netlist, reducing the gap\nbetween RTL representation and netlist. In addition to the new SOG\nrepresentation, MasterRTL proposes new ML methods for the RTL-stage modeling of\ntiming, power, and area separately. Compared with state-of-the-art solutions,\nthe experiment on a comprehensive dataset with 90 different designs shows\naccuracy improvement by 0.33, 0.22, and 0.15 in correlation for total negative\nslack (TNS), worst negative slack (WNS), and power, respectively.",
            "author": [
                "Wenji Fang",
                "Yao Lu",
                "Shang Liu",
                "Qijun Zhang",
                "Ceyu Xu",
                "Lisa Wu Wills",
                "Hongce Zhang",
                "Zhiyao Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08441v1",
                "http://arxiv.org/pdf/2311.08441v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08328v1",
            "title": "A PRISMA-driven systematic mapping study on system assurance weakeners",
            "updated": "2023-11-14T17:17:16Z",
            "published": "2023-11-14T17:17:16Z",
            "summary": "Context: An assurance case is a structured hierarchy of claims aiming at\ndemonstrating that a given mission-critical system supports specific\nrequirements (e.g., safety, security, privacy). The presence of assurance\nweakeners (i.e., assurance deficits, logical fallacies) in assurance cases\nreflects insufficient evidence, knowledge, or gaps in reasoning. These\nweakeners can undermine confidence in assurance arguments, potentially\nhindering the verification of mission-critical system capabilities.\n  Objectives: As a stepping stone for future research on assurance weakeners,\nwe aim to initiate the first comprehensive systematic mapping study on this\nsubject. Methods: We followed the well-established PRISMA 2020 and SEGRESS\nguidelines to conduct our systematic mapping study. We searched for primary\nstudies in five digital libraries and focused on the 2012-2023 publication year\nrange. Our selection criteria focused on studies addressing assurance weakeners\nat the modeling level, resulting in the inclusion of 39 primary studies in our\nsystematic review.\n  Results: Our systematic mapping study reports a taxonomy (map) that provides\na uniform categorization of assurance weakeners and approaches proposed to\nmanage them at the modeling level.\n  Conclusion: Our study findings suggest that the SACM (Structured Assurance\nCase Metamodel) -- a standard specified by the OMG (Object Management Group) --\nmay be the best specification to capture structured arguments and reason about\ntheir potential assurance weakeners.",
            "author": [
                "Kimya Khakzad Shahandashti",
                "Alvine B. Belle",
                "Timothy C. Lethbridge",
                "Oluwafemi Odu",
                "Mithila Sivakumar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08328v1",
                "http://arxiv.org/pdf/2311.08328v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08320v1",
            "title": "CV32RT: Enabling Fast Interrupt and Context Switching for RISC-V\n  Microcontrollers",
            "updated": "2023-11-14T17:04:17Z",
            "published": "2023-11-14T17:04:17Z",
            "summary": "Processors using the open RISC-V ISA are finding increasing adoption in the\nembedded world. Many embedded use cases have real-time constraints and require\nflexible, predictable, and fast reactive handling of incoming events. However,\nRISC- V processors are still lagging in this area compared to more mature\nproprietary architectures, such as ARM Cortex-M and TriCore, which have been\ntuned for years. The default interrupt controller standardized by RISC-V, the\nCore Local Interruptor (CLINT), lacks configurability in prioritization and\npreemption of interrupts. The RISC-V Core Local Interrupt Controller (CLIC)\nspecification addresses this concern by enabling pre-emptible, low-latency\nvectored interrupts while also envisioning optional extensions to improve\ninterrupt latency. In this work, we implement a CLIC for the CV32E40P, an\nindustrially supported open-source 32-bit MCU-class RISC-V core, and enhance it\nwith fastirq: a custom extension that provides interrupt latency as low as 6\ncycles. We call CV32RT our enhanced core. To the best of our knowledge, CV32RT\nis the first fully open-source RV32 core with competitive interrupt-handling\nfeatures compared to the Arm Cortex-M series and TriCore. The proposed\nextensions are also demonstrated to improve task context switching in real-time\noperating systems.",
            "author": [
                "Robert Balas",
                "Alessandro Ottaviano",
                "Luca Benini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08320v1",
                "http://arxiv.org/pdf/2311.08320v1"
            ],
            "primary_category": "cs.AR",
            "category": [
                "cs.AR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08315v1",
            "title": "Total Empiricism: Learning from Data",
            "updated": "2023-11-14T16:59:37Z",
            "published": "2023-11-14T16:59:37Z",
            "summary": "Statistical analysis is an important tool to distinguish systematic from\nchance findings. Current statistical analyses rely on distributional\nassumptions reflecting the structure of some underlying model, which if not met\nlead to problems in the analysis and interpretation of the results. Instead of\ntrying to fix the model or \"correct\" the data, we here describe a totally\nempirical statistical approach that does not rely on ad hoc distributional\nassumptions in order to overcome many problems in contemporary statistics.\nStarting from elementary combinatorics, we motivate an information-guided\nformalism to quantify knowledge extracted from the given data. Subsequently, we\nderive model-agnostic methods to identify patterns that are solely evidenced by\nthe data based on our prior knowledge. The data-centric character of empiricism\nallows for its universal applicability, particularly as sample size grows\nlarger. In this comprehensive framework, we re-interpret and extend model\ndistributions, scores and statistical tests used in different schools of\nstatistics.",
            "author": [
                "Orestis Loukas",
                "Ho Ryun Chung"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08315v1",
                "http://arxiv.org/pdf/2311.08315v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "physics.data-an",
                "stat.ME",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08309v1",
            "title": "Introducing an Improved Information-Theoretic Measure of Predictive\n  Uncertainty",
            "updated": "2023-11-14T16:55:12Z",
            "published": "2023-11-14T16:55:12Z",
            "summary": "Applying a machine learning model for decision-making in the real world\nrequires to distinguish what the model knows from what it does not. A critical\nfactor in assessing the knowledge of a model is to quantify its predictive\nuncertainty. Predictive uncertainty is commonly measured by the entropy of the\nBayesian model average (BMA) predictive distribution. Yet, the properness of\nthis current measure of predictive uncertainty was recently questioned. We\nprovide new insights regarding those limitations. Our analyses show that the\ncurrent measure erroneously assumes that the BMA predictive distribution is\nequivalent to the predictive distribution of the true model that generated the\ndataset. Consequently, we introduce a theoretically grounded measure to\novercome these limitations. We experimentally verify the benefits of our\nintroduced measure of predictive uncertainty. We find that our introduced\nmeasure behaves more reasonably in controlled synthetic tasks. Moreover, our\nevaluations on ImageNet demonstrate that our introduced measure is advantageous\nin real-world applications utilizing predictive uncertainty.",
            "author": [
                "Kajetan Schweighofer",
                "Lukas Aichberger",
                "Mykyta Ielanskyi",
                "Sepp Hochreiter"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08309v1",
                "http://arxiv.org/pdf/2311.08309v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08295v1",
            "title": "A postgraduate laboratory experiment to set up a single-photon detector\n  using MKIDs",
            "updated": "2023-11-14T16:40:44Z",
            "published": "2023-11-14T16:40:44Z",
            "summary": "This paper presents a laboratory activity aimed at developing knowledge and\nexpertise in microwave applications at cryogenic temperatures. The experience\nfocuses on the detection of single infrared photons through Microwave Kinetic\nInductance Detectors (MKIDs). The experimental setup, theoretical concepts, and\nactivities involved are detailed, highlighting the skills and knowledge gained\nthrough the experience. This experiment is designed for postgraduate students\nin the field of quantum technologies.",
            "author": [
                "Pietro Campana",
                "Rodolfo Carobene",
                "Eleonora Cipelli",
                "Marco Gobbo",
                "Aurora Perego",
                "Davide Vertemati"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08295v1",
                "http://arxiv.org/pdf/2311.08295v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08287v1",
            "title": "How Well Do Large Language Models Understand Syntax? An Evaluation by\n  Asking Natural Language Questions",
            "updated": "2023-11-14T16:30:36Z",
            "published": "2023-11-14T16:30:36Z",
            "summary": "While recent advancements in large language models (LLMs) bring us closer to\nachieving artificial general intelligence, the question persists: Do LLMs truly\nunderstand language, or do they merely mimic comprehension through pattern\nrecognition? This study seeks to explore this question through the lens of\nsyntax, a crucial component of sentence comprehension. Adopting a natural\nlanguage question-answering (Q&A) scheme, we craft questions targeting nine\nsyntactic knowledge points that are most closely related to sentence\ncomprehension. Experiments conducted on 24 LLMs suggest that most have a\nlimited grasp of syntactic knowledge, exhibiting notable discrepancies across\ndifferent syntactic knowledge points. In particular, questions involving\nprepositional phrase attachment pose the greatest challenge, whereas those\nconcerning adjectival modifier and indirect object are relatively easier for\nLLMs to handle. Furthermore, a case study on the training dynamics of the LLMs\nreveals that the majority of syntactic knowledge is learned during the initial\nstages of training, hinting that simply increasing the number of training\ntokens may not be the `silver bullet' for improving the comprehension ability\nof LLMs.",
            "author": [
                "Houquan Zhou",
                "Yang Hou",
                "Zhenghua Li",
                "Xuebin Wang",
                "Zhefeng Wang",
                "Xinyu Duan",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08287v1",
                "http://arxiv.org/pdf/2311.08287v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08283v1",
            "title": "Noise-Resilient Group Testing with Order-Optimal Tests and\n  Fast-and-Reliable Decoding",
            "updated": "2023-11-14T16:27:16Z",
            "published": "2023-11-14T16:27:16Z",
            "summary": "Group testing (GT) is the Boolean counterpart of compressed sensing and the\nmarketplace of new ideas for related problems such as cognitive radio and heavy\nhitter. A GT scheme is considered good if it is nonadaptive, uses $O(k \\log n)$\ntests, resists noise, can be decoded in $O(k \\operatorname{poly}(\\log n))$\ntime, and makes nearly no mistakes. In this paper, we propose \"Gacha GT\", an\nelementary, self-contained, and unified randomized scheme that, for the first\ntime, satisfies all criteria for a fairly large region of parameters, namely\nwhen $\\log k < \\log(n)^{1-1/O(1)}$. Outside this parameter region, Gacha can be\nspecialized to outperform the state-of-the-art partial-recovery GTs,\nexact-recovery GTs, and worst-case GTs.\n  The new idea that runs through this paper, using an analogy, is to ask every\nperson to break her $9$-digit \"phone number\" into three $3$-digit numbers $x$,\n$y$, and $z$ and write $(b, x)$, $(b, y)$, and $(b, z)$ on three pieces of\nsticky notes, where $b$ is her \"birthday\". This way, one can sort the sticky\nnotes by birthday to reassemble the phone numbers. This birthday--number code\nand other coded constructions can be stacked like a multipartite graph pyramid.\nGacha's encoder will synthesize the test results from the bottom up; and\nGacha's decoder will reassemble the phone numbers from the top down.",
            "author": [
                "Venkatesan Guruswami",
                "Hsin-Po Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08283v1",
                "http://arxiv.org/pdf/2311.08283v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "cs.DS",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08271v1",
            "title": "Mobility-Induced Graph Learning for WiFi Positioning",
            "updated": "2023-11-14T16:06:11Z",
            "published": "2023-11-14T16:06:11Z",
            "summary": "A smartphone-based user mobility tracking could be effective in finding\nhis/her location, while the unpredictable error therein due to low\nspecification of built-in inertial measurement units (IMUs) rejects its\nstandalone usage but demands the integration to another positioning technique\nlike WiFi positioning. This paper aims to propose a novel integration technique\nusing a graph neural network called Mobility-INduced Graph LEarning (MINGLE),\nwhich is designed based on two types of graphs made by capturing different user\nmobility features. Specifically, considering sequential measurement points\n(MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor\nMPs as edges, called time-driven mobility graph (TMG). Second, a user's\nrelatively straight transition at a constant pace when moving from one position\nto another can be captured by connecting the nodes on each path, called a\ndirection-driven mobility graph (DMG). Then, we can design graph convolution\nnetwork (GCN)-based cross-graph learning, where two different GCN models for\nTMG and DMG are jointly trained by feeding different input features created by\nWiFi RTTs yet sharing their weights. Besides, the loss function includes a\nmobility regularization term such that the differences between adjacent\nlocation estimates should be less variant due to the user's stable moving pace.\nNoting that the regularization term does not require ground-truth location,\nMINGLE can be designed under semi- and self-supervised learning frameworks. The\nproposed MINGLE's effectiveness is extensively verified through field\nexperiments, showing a better positioning accuracy than benchmarks, say root\nmean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and\nsemi-supervised learning cases, respectively.",
            "author": [
                "Kyuwon Han",
                "Seung Min Yu",
                "Seong-Lyun Kim",
                "Seung-Woo Ko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08271v1",
                "http://arxiv.org/pdf/2311.08271v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.IT",
                "cs.NI",
                "eess.SP",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08253v1",
            "title": "The Impact of Dust on Cepheid and Type Ia Supernova Distances",
            "updated": "2023-11-14T15:48:39Z",
            "published": "2023-11-14T15:48:39Z",
            "summary": "Milky-Way and intergalactic dust extinction and reddening must be accounted\nfor in measurements of distances throughout the universe. This work provides a\ncomprehensive review of the various impacts of cosmic dust focusing\nspecifically on its effects on two key distance indicators used in the distance\nladder: Cepheid variable stars and Type Ia supernovae. We review the formalism\nused for computing and accounting for dust extinction and reddening as a\nfunction of wavelength. We also detail the current state of the art knowledge\nof dust properties in the Milky Way and in host galaxies. We discuss how dust\nhas been accounted for in both the Cepheid and SN distance measurements.\nFinally, we show how current uncertainties on dust modeling impact the inferred\nluminosities and distances, but that measurements of the Hubble constant remain\nrobust to these uncertainties.",
            "author": [
                "Dillon Brout",
                "Adam Riess"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08253v1",
                "http://arxiv.org/pdf/2311.08253v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08252v1",
            "title": "REST: Retrieval-Based Speculative Decoding",
            "updated": "2023-11-14T15:43:47Z",
            "published": "2023-11-14T15:43:47Z",
            "summary": "We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm\ndesigned to speed up language model generation. The key insight driving the\ndevelopment of REST is the observation that the process of text generation\noften includes certain common phases and patterns. Unlike previous methods that\nrely on a draft language model for speculative decoding, REST harnesses the\npower of retrieval to generate draft tokens. This method draws from the\nreservoir of existing knowledge, retrieving and employing relevant tokens based\non the current context. Its plug-and-play nature allows for seamless\nintegration and acceleration of any language models, all without necessitating\nadditional training. When benchmarked on 7B and 13B language models in a\nsingle-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on\ncode or text generation. The code of REST is available at\nhttps://github.com/FasterDecoding/REST.",
            "author": [
                "Zhenyu He",
                "Zexuan Zhong",
                "Tianle Cai",
                "Jason D Lee",
                "Di He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08252v1",
                "http://arxiv.org/pdf/2311.08252v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08240v1",
            "title": "Investigating the Encoding of Words in BERT's Neurons using Feature\n  Textualization",
            "updated": "2023-11-14T15:21:49Z",
            "published": "2023-11-14T15:21:49Z",
            "summary": "Pretrained language models (PLMs) form the basis of most state-of-the-art NLP\ntechnologies. Nevertheless, they are essentially black boxes: Humans do not\nhave a clear understanding of what knowledge is encoded in different parts of\nthe models, especially in individual neurons. The situation is different in\ncomputer vision, where feature visualization provides a decompositional\ninterpretability technique for neurons of vision models. Activation\nmaximization is used to synthesize inherently interpretable visual\nrepresentations of the information encoded in individual neurons. Our work is\ninspired by this but presents a cautionary tale on the interpretability of\nsingle neurons, based on the first large-scale attempt to adapt activation\nmaximization to NLP, and, more specifically, large PLMs. We propose feature\ntextualization, a technique to produce dense representations of neurons in the\nPLM word embedding space. We apply feature textualization to the BERT model\n(Devlin et al., 2019) to investigate whether the knowledge encoded in\nindividual neurons can be interpreted and symbolized. We find that the produced\nrepresentations can provide insights about the knowledge encoded in individual\nneurons, but that individual neurons do not represent clearcut symbolic units\nof language such as words. Additionally, we use feature textualization to\ninvestigate how many neurons are needed to encode words in BERT.",
            "author": [
                "Tanja Baeumel",
                "Soniya Vijayakumar",
                "Josef van Genabith",
                "Guenter Neumann",
                "Simon Ostermann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08240v1",
                "http://arxiv.org/pdf/2311.08240v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08232v1",
            "title": "Revealing Transition in Fall-off Rates of spin-s Ising Model through\n  Multiqudit Graph states",
            "updated": "2023-11-14T15:12:21Z",
            "published": "2023-11-14T15:12:21Z",
            "summary": "A variable-range interacting Ising model with spin-1/2 particles exhibits\ndistinct behavior depending on the fall-off rates in the range of interactions,\nnotably non-local (NL), quasi-local (QL), and local. It is unknown if such a\ntransition occurs in this model with an arbitrary spin quantum number. We\nestablish its existence by analyzing the profiles of entanglement entropy,\nmutual information, and genuine multipartite entanglement (GME) of the weighted\ngraph state (WGS), which is prepared when the multi-level maximally coherent\nstate at each site evolves according to the spin-s Ising Hamiltonian.\nSpecifically, we demonstrate that the scaling of time-averaged mutual\ninformation and the divergence in the first derivative of GME with respect to\nthe fall-off rate in the WGS can indicate the transition point from NL to QL,\nwhich scales logarithmically with individual spin dimension. Additionally, we\nsuggest that the existence of a saturation value of a finite number of qudits\ncapable of mimicking the GME pattern of an arbitrarily large system-size can\nreveal the second transition point between quasi-local and local regions.",
            "author": [
                "Debkanta Ghosh",
                "Keshav Das Agarwal",
                "Pritam Halder",
                "Aditi Sen De"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08232v1",
                "http://arxiv.org/pdf/2311.08232v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cond-mat.stat-mech",
                "cond-mat.str-el"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08223v2",
            "title": "Improving Image Captioning via Predicting Structured Concepts",
            "updated": "2023-11-28T04:05:03Z",
            "published": "2023-11-14T15:01:58Z",
            "summary": "Having the difficulty of solving the semantic gap between images and texts\nfor the image captioning task, conventional studies in this area paid some\nattention to treating semantic concepts as a bridge between the two modalities\nand improved captioning performance accordingly. Although promising results on\nconcept prediction were obtained, the aforementioned studies normally ignore\nthe relationship among concepts, which relies on not only objects in the image,\nbut also word dependencies in the text, so that offers a considerable potential\nfor improving the process of generating good descriptions. In this paper, we\npropose a structured concept predictor (SCP) to predict concepts and their\nstructures, then we integrate them into captioning, so as to enhance the\ncontribution of visual signals in this task via concepts and further use their\nrelations to distinguish cross-modal semantics for better description\ngeneration. Particularly, we design weighted graph convolutional networks\n(W-GCN) to depict concept relations driven by word dependencies, and then\nlearns differentiated contributions from these concepts for following decoding\nprocess. Therefore, our approach captures potential relations among concepts\nand discriminatively learns different concepts, so that effectively facilitates\nimage captioning with inherited information across modalities. Extensive\nexperiments and their results demonstrate the effectiveness of our approach as\nwell as each proposed module in this work.",
            "author": [
                "Ting Wang",
                "Weidong Chen",
                "Yuanhe Tian",
                "Yan Song",
                "Zhendong Mao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08223v2",
                "http://arxiv.org/pdf/2311.08223v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08217v1",
            "title": "Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot\n  Image Generation",
            "updated": "2023-11-14T14:55:42Z",
            "published": "2023-11-14T14:55:42Z",
            "summary": "Few-shot image generation aims to train generative models using a small\nnumber of training images. When there are few images available for training\n(e.g. 10 images), Learning From Scratch (LFS) methods often generate images\nthat closely resemble the training data while Transfer Learning (TL) methods\ntry to improve performance by leveraging prior knowledge from GANs pre-trained\non large-scale datasets. However, current TL methods may not allow for\nsufficient control over the degree of knowledge preservation from the source\nmodel, making them unsuitable for setups where the source and target domains\nare not closely related. To address this, we propose a novel pipeline called\nPeer is your Pillar (PIP), which combines a target few-shot dataset with a peer\ndataset to create a data-unbalanced conditional generation. Our approach\nincludes a class embedding method that separates the class space from the\nlatent space, and we use a direction loss based on pre-trained CLIP to improve\nimage diversity. Experiments on various few-shot datasets demonstrate the\nadvancement of the proposed PIP, especially reduces the training requirements\nof few-shot image generation.",
            "author": [
                "Ziqiang Li",
                "Chaoyue Wang",
                "Xue Rui",
                "Chao Xue",
                "Jiaxu Leng",
                "Bin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08217v1",
                "http://arxiv.org/pdf/2311.08217v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08214v1",
            "title": "Frequentist Guarantees of Distributed (Non)-Bayesian Inference",
            "updated": "2023-11-14T14:50:46Z",
            "published": "2023-11-14T14:50:46Z",
            "summary": "Motivated by the need to analyze large, decentralized datasets, distributed\nBayesian inference has become a critical research area across multiple fields,\nincluding statistics, electrical engineering, and economics. This paper\nestablishes Frequentist properties, such as posterior consistency, asymptotic\nnormality, and posterior contraction rates, for the distributed (non-)Bayes\nInference problem among agents connected via a communication network. Our\nresults show that, under appropriate assumptions on the communication graph,\ndistributed Bayesian inference retains parametric efficiency while enhancing\nrobustness in uncertainty quantification. We also explore the trade-off between\nstatistical efficiency and communication efficiency by examining how the design\nand size of the communication graph impact the posterior contraction rate.\nFurthermore, We extend our analysis to time-varying graphs and apply our\nresults to exponential family models, distributed logistic regression, and\ndecentralized detection models.",
            "author": [
                "Bohan Wu",
                "C\u00e9sar A. Uribe"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08214v1",
                "http://arxiv.org/pdf/2311.08214v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08213v1",
            "title": "Unlock the Power: Competitive Distillation for Multi-Modal Large\n  Language Models",
            "updated": "2023-11-14T14:49:46Z",
            "published": "2023-11-14T14:49:46Z",
            "summary": "Recently, multi-modal content generation has attracted lots of attention from\nresearchers by investigating the utilization of visual instruction tuning based\non large language models (LLMs). To enhance the performance and generalization\nability of such LLMs, the practice of distilling knowledge from pretrained\nmulti-modal models (a.k.a. teachers) to more compact multi-modal LLMs\n(students) has gained considerable interest. However, the prevailing paradigm\nof instructiontuning in multi-modal LLMs knowledge distillation is\nresource-intensive and unidirectional, neglecting the potential for mutual\nfeedback between the student and teacher models. Thus, we propose an innovative\nCompetitive Multi-modal Distillation framework (CoMD), which captures\nbidirectional feedback between teacher and student models and continually\nupdates the multi-modal capabilities that the student model has learned. It\ncomprises two stages: multi-modal pre-training and multi-modal competitive\ndistillation. The first stage pre-trains the student model on a large number of\nfiltered multi-modal datasets. The second stage facilitates a bidirectional\nknowledge transfer between the student and teacher models. Our experimental\nanalysis of diverse datasets shows that our knowledge transfer method\nconsistently improves the capabilities of the student model. Finally, the\n7B-sized student model after four distillations surpassed the current\nstate-of-the-art model LLaVA-13B on the ScienceQA and LLaVA Test dataset, also\noutperforms other strong baselines in the zero-shot setting.",
            "author": [
                "Xinwei Li",
                "Li Lin",
                "Shuai Wang",
                "Chen Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08213v1",
                "http://arxiv.org/pdf/2311.08213v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08205v1",
            "title": "Increasing the Efficiency of Cryptoasset Investigations by Connecting\n  the Cases",
            "updated": "2023-11-14T14:41:39Z",
            "published": "2023-11-14T14:41:39Z",
            "summary": "Law enforcement agencies are confronted with a rapidly growing number of\ncryptoasset-related cases, often redundantly investigating the same cases\nwithout mutual knowledge or shared insights. In this paper, we explore the\nhypothesis that recognizing and acting upon connections between these cases can\nsignificantly streamline investigative processes. Through an analysis of a\ndataset comprising 34 cyberfraud and 1793 sextortion spam cases, we discovered\nthat 41% of the cyberfraud and 96.9% of the sextortion spam incidents can be\ninterconnected. We introduce a straightforward yet effective tool, which is\nintegrated into a broader cryptoasset forensics workflow and allows\ninvestigators to highlight and share case connections. Our research\nunequivocally demonstrates that recognizing case connections can lead to\nremarkable efficiencies, especially when extended across crime areas,\ninternational borders, and jurisdictions.",
            "author": [
                "Bernhard Haslhofer",
                "Christiane Hanslbauer",
                "Michael Fr\u00f6wis",
                "Thomas Goger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08205v1",
                "http://arxiv.org/pdf/2311.08205v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08194v1",
            "title": "On the Quantum Chromatic Numbers of Small Graphs",
            "updated": "2023-11-14T14:27:03Z",
            "published": "2023-11-14T14:27:03Z",
            "summary": "We make two contributions pertaining to the study of the quantum chromatic\nnumbers of small graphs. Firstly, in an elegant paper, Man\\v{c}inska and\nRoberson [\\textit{Baltic Journal on Modern Computing}, 4(4), 846-859, 2016]\ngave an example of a graph $G_{14}$ on 14 vertices with quantum chromatic\nnumber 4 and classical chromatic number 5, and conjectured that this is the\nsmallest graph exhibiting a separation between the two parameters. We describe\na computer-assisted proof of this conjecture, thereby resolving a longstanding\nopen problem in quantum graph theory. Our second contribution pertains to the\nstudy of the rank-$r$ quantum chromatic numbers. While it can now be shown that\nfor every $r$, $\\chi_q$ and $\\chi^{(r)}_q$ are distinct, few small examples of\nseparations between these parameters are known. We give the smallest known\nexample of such a separation in the form of a graph $G_{21}$ on 21 vertices\nwith $\\chi_q(G_{21}) = \\chi^{(2)}_q(G_{21}) = 4$ and $ \\xi(G_{21}) =\n\\chi^{(1)}_q(G_{21}) = \\chi(G_{21}) = 5$. The previous record was held by a\ngraph $G_{msg}$ on 57 vertices that was first considered in the aforementioned\npaper of Man\\v{c}inska and Roberson and which satisfies $\\chi_q(G_{msg}) = 3$\nand $\\chi^{(1)}_q(G_{msg}) = 4$. In addition, $G_{21}$ provides the first\nprovable separation between the parameters $\\chi^{(1)}_q$ and $\\chi^{(2)}_q$.\nWe believe that our techniques for constructing $G_{21}$ and lower bounding its\northogonal rank could be of independent interest.",
            "author": [
                "Olivier Lalonde"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08194v1",
                "http://arxiv.org/pdf/2311.08194v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08167v2",
            "title": "SeDe: Balancing Blockchain Privacy and Regulatory Compliance by\n  Selective De-Anonymization",
            "updated": "2023-11-16T12:38:12Z",
            "published": "2023-11-14T13:49:13Z",
            "summary": "Privacy is one of the essential pillars for the widespread adoption of\nblockchains, but public blockchains are transparent by nature. Modern analytics\ntechniques can easily subdue the pseudonymity feature of a blockchain user.\nSome applications have been able to provide practical privacy protections using\nprivacy-preserving cryptography techniques. However, malicious actors have\nabused them illicitly, discouraging honest actors from using privacy-preserving\napplications as \"mixing\" user interactions and funds with anonymous bad actors,\ncausing compliance and regulatory concerns.\n  In this paper, we propose a framework that balances privacy-preserving\nfeatures by establishing a regulatory and compliant framework called Selective\nDe-Anonymization (SeDe). The adoption of this framework allows\nprivacy-preserving applications on blockchains to de-anonymize illicit\ntransactions by recursive traversal of subgraphs of linked transactions. Our\ntechnique achieves this without leaving de-anonymization decisions or control\nin the hands of a single entity but distributing it among multiple entities\nwhile holding them accountable for their respective actions. To instantiate,\nour framework uses threshold encryption schemes and Zero-Knowledge Proofs\n(ZKPs).",
            "author": [
                "Naveen Sahu",
                "Mitul Gajera",
                "Amit Chaudhary",
                "Hamish Ivey-Law"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08167v2",
                "http://arxiv.org/pdf/2311.08167v2"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08166v1",
            "title": "MechAgents: Large language model multi-agent collaborations can solve\n  mechanics problems, generate new data, and integrate knowledge",
            "updated": "2023-11-14T13:49:03Z",
            "published": "2023-11-14T13:49:03Z",
            "summary": "Solving mechanics problems using numerical methods requires comprehensive\nintelligent capability of retrieving relevant knowledge and theory,\nconstructing and executing codes, analyzing the results, a task that has thus\nfar mainly been reserved for humans. While emerging AI methods can provide\neffective approaches to solve end-to-end problems, for instance via the use of\ndeep surrogate models or various data analytics strategies, they often lack\nphysical intuition since knowledge is baked into the parametric complement\nthrough training, offering less flexibility when it comes to incorporating\nmathematical or physical insights. By leveraging diverse capabilities of\nmultiple dynamically interacting large language models (LLMs), we can overcome\nthe limitations of conventional approaches and develop a new class of\nphysics-inspired generative machine learning platform, here referred to as\nMechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for\nelasticity problems, via autonomous collaborations. A two-agent team can\neffectively write, execute and self-correct code, in order to apply finite\nelement methods to solve classical elasticity problems in various flavors\n(different boundary conditions, domain geometries, meshes, small/finite\ndeformation and linear/hyper-elastic constitutive laws, and others). For more\ncomplex tasks, we construct a larger group of agents with enhanced division of\nlabor among planning, formulating, coding, executing and criticizing the\nprocess and results. The agents mutually correct each other to improve the\noverall team-work performance in understanding, formulating and validating the\nsolution. Our framework shows the potential of synergizing the intelligence of\nlanguage models, the reliability of physics-based modeling, and the dynamic\ncollaborations among diverse agents, opening novel avenues for automation of\nsolving engineering problems.",
            "author": [
                "Bo Ni",
                "Markus J. Buehler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08166v1",
                "http://arxiv.org/pdf/2311.08166v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cond-mat.dis-nn",
                "cond-mat.mtrl-sci",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08149v2",
            "title": "Modeling Complex Disease Trajectories using Deep Generative Models with\n  Semi-Supervised Latent Processes",
            "updated": "2023-11-17T11:51:18Z",
            "published": "2023-11-14T13:25:41Z",
            "summary": "In this paper, we propose a deep generative time series approach using latent\ntemporal processes for modeling and holistically analyzing complex disease\ntrajectories. We aim to find meaningful temporal latent representations of an\nunderlying generative process that explain the observed disease trajectories in\nan interpretable and comprehensive way. To enhance the interpretability of\nthese latent temporal processes, we develop a semi-supervised approach for\ndisentangling the latent space using established medical concepts. By combining\nthe generative approach with medical knowledge, we leverage the ability to\ndiscover novel aspects of the disease while integrating medical concepts into\nthe model. We show that the learned temporal latent processes can be utilized\nfor further data analysis and clinical hypothesis testing, including finding\nsimilar patients and clustering the disease into new sub-types. Moreover, our\nmethod enables personalized online monitoring and prediction of multivariate\ntime series including uncertainty quantification. We demonstrate the\neffectiveness of our approach in modeling systemic sclerosis, showcasing the\npotential of our machine learning model to capture complex disease trajectories\nand acquire new medical knowledge.",
            "author": [
                "C\u00e9cile Trottet",
                "Manuel Sch\u00fcrch",
                "Ahmed Allam",
                "Imon Barua",
                "Liubov Petelytska",
                "Oliver Distler",
                "Anna-Maria Hoffmann-Vold",
                "Michael Krauthammer",
                "the EUSTAR collaborators"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08149v2",
                "http://arxiv.org/pdf/2311.08149v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08147v1",
            "title": "RECALL: A Benchmark for LLMs Robustness against External Counterfactual\n  Knowledge",
            "updated": "2023-11-14T13:24:19Z",
            "published": "2023-11-14T13:24:19Z",
            "summary": "LLMs and AI chatbots have improved people's efficiency in various fields.\nHowever, the necessary knowledge for answering the question may be beyond the\nmodels' knowledge boundaries. To mitigate this issue, many researchers try to\nintroduce external knowledge, such as knowledge graphs and Internet contents,\ninto LLMs for up-to-date information. However, the external information from\nthe Internet may include counterfactual information that will confuse the model\nand lead to an incorrect response. Thus there is a pressing need for LLMs to\npossess the ability to distinguish reliable information from external\nknowledge. Therefore, to evaluate the ability of LLMs to discern the\nreliability of external knowledge, we create a benchmark from existing\nknowledge bases. Our benchmark consists of two tasks, Question Answering and\nText Generation, and for each task, we provide models with a context containing\ncounterfactual information. Evaluation results show that existing LLMs are\nsusceptible to interference from unreliable external knowledge with\ncounterfactual information, and simple intervention methods make limited\ncontributions to the alleviation of this issue.",
            "author": [
                "Yi Liu",
                "Lianzhe Huang",
                "Shicheng Li",
                "Sishuo Chen",
                "Hao Zhou",
                "Fandong Meng",
                "Jie Zhou",
                "Xu Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08147v1",
                "http://arxiv.org/pdf/2311.08147v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08141v1",
            "title": "GMTR: Graph Matching Transformers",
            "updated": "2023-11-14T13:12:47Z",
            "published": "2023-11-14T13:12:47Z",
            "summary": "Vision transformers (ViTs) have recently been used for visual matching beyond\nobject detection and segmentation. However, the original grid dividing strategy\nof ViTs neglects the spatial information of the keypoints, limiting the\nsensitivity to local information. Therefore, we propose \\textbf{QueryTrans}\n(Query Transformer), which adopts a cross-attention module and keypoints-based\ncenter crop strategy for better spatial information extraction. We further\nintegrate the graph attention module and devise a transformer-based graph\nmatching approach \\textbf{GMTR} (Graph Matching TRansformers) whereby the\ncombinatorial nature of GM is addressed by a graph transformer neural GM\nsolver. On standard GM benchmarks, GMTR shows competitive performance against\nthe SOTA frameworks. Specifically, on Pascal VOC, GMTR achieves\n$\\mathbf{83.6\\%}$ accuracy, $\\mathbf{0.9\\%}$ higher than the SOTA framework. On\nSpair-71k, GMTR shows great potential and outperforms most of the previous\nworks. Meanwhile, on Pascal VOC, QueryTrans improves the accuracy of NGMv2 from\n$80.1\\%$ to $\\mathbf{83.3\\%}$, and BBGM from $79.0\\%$ to $\\mathbf{84.5\\%}$. On\nSpair-71k, QueryTrans improves NGMv2 from $80.6\\%$ to $\\mathbf{82.5\\%}$, and\nBBGM from $82.1\\%$ to $\\mathbf{83.9\\%}$. Source code will be made publicly\navailable.",
            "author": [
                "Jinpei Guo",
                "Shaofeng Zhang",
                "Runzhong Wang",
                "Chang Liu",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08141v1",
                "http://arxiv.org/pdf/2311.08141v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08133v2",
            "title": "Exogenous-endogenous surfactant interaction yields heterogeneous\n  spreading in complex branching networks",
            "updated": "2023-11-15T08:27:40Z",
            "published": "2023-11-14T12:54:18Z",
            "summary": "Experiments have shown that surfactant introduced to a liquid-filled maze can\nfind the solution path. We reveal how the maze-solving dynamics arise from\ninteractions between the added surfactant and endogenous surfactant present at\nthe liquid surface. We simulate the dynamics using a nonlinear model solved\nwith a discrete mimetic scheme on a graph. Endogenous surfactant transforms\nlocal spreading into a non-local problem with an omniscient view of the maze\ngeometry, key to the maze-solving dynamics. Our results offer insight into\nsurfactant-driven transport in complex networks such as lung airways.",
            "author": [
                "Richard Mcnair",
                "Fernando Temprano-Coleto",
                "Fran\u00e7ois J. Peaudecerf",
                "Fr\u00e9d\u00e9ric Gibou",
                "Paolo Luzzatto-Fegiz",
                "Oliver E. Jensen",
                "Julien R. Landel"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08133v2",
                "http://arxiv.org/pdf/2311.08133v2"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "math-ph",
                "math.MP",
                "76"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08128v1",
            "title": "Distance-regular Cayley graphs over (pseudo-) semi-dihedral groups",
            "updated": "2023-11-14T12:48:12Z",
            "published": "2023-11-14T12:48:12Z",
            "summary": "Distance-regular graphs are a class of regualr graphs with pretty\ncombinatorial symmetry. In 2007, Miklavi\\v{c} and Poto\\v{c}nik proposed the\nproblem of charaterizing distance-regular Cayley graphs, which can be viewed as\na natural extension of the problem of characterizing strongly-regular Cayley\ngraphs (or equivalently, regular partial difference sets). In this paper, we\nprovide a partial characterization for distance-regular Cayley graphs over\nsemi-dihedral groups and pseudo-semi-dihedral groups, both of which are\n$2$-groups with a cyclic subgroup of index $2$.",
            "author": [
                "Xueyi Huang",
                "Lu Lu",
                "Xiongfeng Zhan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08128v1",
                "http://arxiv.org/pdf/2311.08128v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05E30, 05C25, 05C50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08118v1",
            "title": "Evaluating Neighbor Explainability for Graph Neural Networks",
            "updated": "2023-11-14T12:33:19Z",
            "published": "2023-11-14T12:33:19Z",
            "summary": "Explainability in Graph Neural Networks (GNNs) is a new field growing in the\nlast few years. In this publication we address the problem of determining how\nimportant is each neighbor for the GNN when classifying a node and how to\nmeasure the performance for this specific task. To do this, various known\nexplainability methods are reformulated to get the neighbor importance and four\nnew metrics are presented. Our results show that there is almost no difference\nbetween the explanations provided by gradient-based techniques in the GNN\ndomain. In addition, many explainability techniques failed to identify\nimportant neighbors when GNNs without self-loops are used.",
            "author": [
                "Oscar Llorente",
                "P\u00e9ter Vaderna",
                "S\u00e1ndor Laki",
                "Roland Kotrocz\u00f3",
                "Rita Csoma",
                "J\u00e1nos M\u00e1rk Szalai-Gindl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08118v1",
                "http://arxiv.org/pdf/2311.08118v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08117v1",
            "title": "Insights into Classifying and Mitigating LLMs' Hallucinations",
            "updated": "2023-11-14T12:30:28Z",
            "published": "2023-11-14T12:30:28Z",
            "summary": "The widespread adoption of large language models (LLMs) across diverse AI\napplications is proof of the outstanding achievements obtained in several\ntasks, such as text mining, text generation, and question answering. However,\nLLMs are not exempt from drawbacks. One of the most concerning aspects regards\nthe emerging problematic phenomena known as \"Hallucinations\". They manifest in\ntext generation systems, particularly in question-answering systems reliant on\nLLMs, potentially resulting in false or misleading information propagation.\nThis paper delves into the underlying causes of AI hallucination and elucidates\nits significance in artificial intelligence. In particular, Hallucination\nclassification is tackled over several tasks (Machine Translation, Question and\nAnswer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and\nVisual Question Answer). Additionally, we explore potential strategies to\nmitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our\nresearch addresses this critical issue within the HeReFaNMi (Health-Related\nFake News Mitigation) project, generously supported by NGI Search, dedicated to\ncombating Health-Related Fake News dissemination on the Internet. This\nendeavour represents a concerted effort to safeguard the integrity of\ninformation dissemination in an age of evolving AI technologies.",
            "author": [
                "Alessandro Bruno",
                "Pier Luigi Mazzeo",
                "Aladine Chetouani",
                "Marouane Tliba",
                "Mohamed Amine Kerkouri"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08117v1",
                "http://arxiv.org/pdf/2311.08117v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08106v1",
            "title": "Carpe Diem: On the Evaluation of World Knowledge in Lifelong Language\n  Models",
            "updated": "2023-11-14T12:12:02Z",
            "published": "2023-11-14T12:12:02Z",
            "summary": "In an ever-evolving world, the dynamic nature of knowledge presents\nchallenges for language models that are trained on static data, leading to\noutdated encoded information. However, real-world scenarios require models not\nonly to acquire new knowledge but also to overwrite outdated information into\nupdated ones. To address this under-explored issue, we introduce the temporally\nevolving question answering benchmark, EvolvingQA - a novel benchmark designed\nfor training and evaluating LMs on an evolving Wikipedia database, where the\nconstruction of our benchmark is automated with our pipeline using large\nlanguage models. Our benchmark incorporates question-answering as a downstream\ntask to emulate real-world applications. Through EvolvingQA, we uncover that\nexisting continual learning baselines have difficulty in updating and\nforgetting outdated knowledge. Our findings suggest that the models fail to\nlearn updated knowledge due to the small weight gradient. Furthermore, we\nelucidate that the models struggle mostly on providing numerical or temporal\nanswers to questions asking for updated knowledge. Our work aims to model the\ndynamic nature of real-world information, offering a robust measure for the\nevolution-adaptability of language models.",
            "author": [
                "Yujin Kim",
                "Jaehong Yoon",
                "Seonghyeon Ye",
                "Sung Ju Hwang",
                "Se-young Yun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08106v1",
                "http://arxiv.org/pdf/2311.08106v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08104v1",
            "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice\n  Conversion",
            "updated": "2023-11-14T12:03:46Z",
            "published": "2023-11-14T12:03:46Z",
            "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech\nscenarios is getting increasingly popular. Although many of the works in the\nfield of voice conversion share a common global pipeline, there is a\nconsiderable diversity in the underlying structures, methods, and neural\nsub-blocks used across research efforts. Thus, obtaining a comprehensive\nunderstanding of the reasons behind the choice of the different methods in the\nvoice conversion pipeline can be challenging, and the actual hurdles in the\nproposed solutions are often unclear. To shed light on these aspects, this\npaper presents a scoping review that explores the use of deep learning in\nspeech analysis, synthesis, and disentangled speech representation learning\nwithin modern voice conversion systems. We screened 621 publications from more\nthan 38 different venues between the years 2017 and 2023, followed by an\nin-depth review of a final database consisting of 123 eligible studies. Based\non the review, we summarise the most frequently used approaches to voice\nconversion based on deep learning and highlight common pitfalls within the\ncommunity. Lastly, we condense the knowledge gathered, identify main challenges\nand provide recommendations for future research directions.",
            "author": [
                "Anders R. Bargum",
                "Stefania Serafin",
                "Cumhur Erkut"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08104v1",
                "http://arxiv.org/pdf/2311.08104v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08086v1",
            "title": "CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and\n  Cognitive Theory",
            "updated": "2023-11-14T11:13:00Z",
            "published": "2023-11-14T11:13:00Z",
            "summary": "Active safety systems on vehicles often face problems with false alarms. Most\nactive safety systems predict the driver's trajectory with the assumption that\nthe driver is always in a normal emotion, and then infer risks. However, the\ndriver's trajectory uncertainty increases under abnormal emotions. This paper\nproposes a new trajectory prediction model: CPSOR-GCN, which predicts vehicle\ntrajectories under abnormal emotions. At the physical level, the interaction\nfeatures between vehicles are extracted by the physical GCN module. At the\ncognitive level, SOR cognitive theory is used as prior knowledge to build a\nDynamic Bayesian Network (DBN) structure. The conditional probability and state\ntransition probability of nodes from the calibrated SOR-DBN quantify the causal\nrelationship between cognitive factors, which is embedded into the cognitive\nGCN module to extract the characteristics of the influence mechanism of\nemotions on driving behavior. The CARLA-SUMO joint driving simulation platform\nwas built to develop dangerous pre-crash scenarios. Methods of recreating\ntraffic scenes were used to naturally induce abnormal emotions. The experiment\ncollected data from 26 participants to verify the proposed model. Compared with\nthe model that only considers physical motion features, the prediction accuracy\nof the proposed model is increased by 68.70%. Furthermore,considering the\nSOR-DBN reduces the prediction error of the trajectory by 15.93%. Compared with\nother advanced trajectory prediction models, the results of CPSOR-GCN also have\nlower errors. This model can be integrated into active safety systems to better\nadapt to the driver's emotions, which could effectively reduce false alarms.",
            "author": [
                "L. Tang",
                "Y. Li",
                "J. Yuan",
                "A. Fu",
                "J. Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08086v1",
                "http://arxiv.org/pdf/2311.08086v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08075v1",
            "title": "GlanceSeg: Real-time microaneurysm lesion segmentation with\n  gaze-map-guided foundation model for early detection of diabetic retinopathy",
            "updated": "2023-11-14T10:59:45Z",
            "published": "2023-11-14T10:59:45Z",
            "summary": "Early-stage diabetic retinopathy (DR) presents challenges in clinical\ndiagnosis due to inconspicuous and minute microangioma lesions, resulting in\nlimited research in this area. Additionally, the potential of emerging\nfoundation models, such as the segment anything model (SAM), in medical\nscenarios remains rarely explored. In this work, we propose a\nhuman-in-the-loop, label-free early DR diagnosis framework called GlanceSeg,\nbased on SAM. GlanceSeg enables real-time segmentation of microangioma lesions\nas ophthalmologists review fundus images. Our human-in-the-loop framework\nintegrates the ophthalmologist's gaze map, allowing for rough localization of\nminute lesions in fundus images. Subsequently, a saliency map is generated\nbased on the located region of interest, which provides prompt points to assist\nthe foundation model in efficiently segmenting microangioma lesions. Finally, a\ndomain knowledge filter refines the segmentation of minute lesions. We\nconducted experiments on two newly-built public datasets, i.e., IDRiD and\nRetinal-Lesions, and validated the feasibility and superiority of GlanceSeg\nthrough visualized illustrations and quantitative measures. Additionally, we\ndemonstrated that GlanceSeg improves annotation efficiency for clinicians and\nenhances segmentation performance through fine-tuning using annotations. This\nstudy highlights the potential of GlanceSeg-based annotations for self-model\noptimization, leading to enduring performance advancements through continual\nlearning.",
            "author": [
                "Hongyang Jiang",
                "Mengdi Gao",
                "Zirong Liu",
                "Chen Tang",
                "Xiaoqing Zhang",
                "Shuai Jiang",
                "Wu Yuan",
                "Jiang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08075v1",
                "http://arxiv.org/pdf/2311.08075v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08053v1",
            "title": "Communication-Constrained Bayesian Active Knowledge Distillation",
            "updated": "2023-11-14T10:23:00Z",
            "published": "2023-11-14T10:23:00Z",
            "summary": "Consider an active learning setting in which a learner has a training set\nwith few labeled examples and a pool set with many unlabeled inputs, while a\nremote teacher has a pre-trained model that is known to perform well for the\nlearner's task. The learner actively transmits batches of unlabeled inputs to\nthe teacher through a constrained communication channel for labeling. This\npaper addresses the following key questions: (i) Active batch selection: Which\nbatch of inputs should be sent to the teacher to acquire the most useful\ninformation and thus reduce the number of required communication rounds? (ii)\nBatch encoding: How do we encode the batch of inputs for transmission to the\nteacher to reduce the communication resources required at each round? We\nintroduce Communication-Constrained Bayesian Active Knowledge Distillation\n(CC-BAKD), a novel protocol that integrates Bayesian active learning with\ncompression via a linear mix-up mechanism. Bayesian active learning selects the\nbatch of inputs based on their epistemic uncertainty, addressing the\n\"confirmation bias\" that is known to increase the number of required\ncommunication rounds. Furthermore, the proposed mix-up compression strategy is\nintegrated with the epistemic uncertainty-based active batch selection process\nto reduce the communication overhead per communication round.",
            "author": [
                "Victor Croisfelt",
                "Shashi Raj Pandey",
                "Osvaldo Simeone",
                "Petar Popovski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08053v1",
                "http://arxiv.org/pdf/2311.08053v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08042v1",
            "title": "Quantum Algorithms for Graph Coloring and other Partitioning, Covering,\n  and Packing Problems",
            "updated": "2023-11-14T10:07:14Z",
            "published": "2023-11-14T10:07:14Z",
            "summary": "Let U be a universe on n elements, let k be a positive integer, and let F be\na family of (implicitly defined) subsets of U. We consider the problems of\npartitioning U into k sets from F, covering U with k sets from F, and packing k\nnon-intersecting sets from F into U. Classically, these problems can be solved\nvia inclusion-exclusion in O*(2^n) time [BjorklundHK09]. Quantumly, there are\nfaster algorithms for graph coloring with running time O(1.9140^n) [ShimizuM22]\nand for Set Cover with a small number of sets with running time O(1.7274^n\n|F|^O(1)) [AmbainisBIKPV19]. In this paper, we give a quantum speedup for Set\nPartition, Set Cover, and Set Packing whenever there is a classical enumeration\nalgorithm that lends itself to a quadratic quantum speedup, which, for any\nsubinstance on a subset X of U, enumerates at least one member of a\nk-partition, k-cover, or k-packing (if one exists) restricted to (or projected\nonto, in the case of k-cover) the set X in O*(c^{|X|}) time with c<2.\n  Our bounded-error quantum algorithm runs in O*((2+c)^(n/2)) for Set\nPartition, Set Cover, and Set Packing. When c<=1.147899, our algorithm is\nslightly faster than O*((2+c)^(n/2)); when c approaches 1, it matches the\nrunning time of [AmbainisBIKPV19] for Set Cover when |F| is subexponential in\nn.\n  For Graph Coloring, we further improve the running time to O(1.7956^n) by\nleveraging faster algorithms for coloring with a small number of colors to\nbetter balance our divide-and-conquer steps. For Domatic Number, we obtain a\nO((2-\\epsilon)^n) running time for some \\epsilon>0.",
            "author": [
                "Serge Gaspers",
                "Jerry Zirui Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08042v1",
                "http://arxiv.org/pdf/2311.08042v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "quant-ph",
                "68Q12",
                "F.2.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08021v1",
            "title": "Silhouettes and generic properties of subgroups of the modular group",
            "updated": "2023-11-14T09:29:26Z",
            "published": "2023-11-14T09:29:26Z",
            "summary": "We show that the probability for a finitely generated subgroup of the modular\ngroup, of size $n$, to be almost malnormal or non-parabolic, tends to 0 as $n$\ntends to infinity -- where the notion of the size of a subgroup is based on a\nnatural graph-theoretic representation of the subgroup.\n  The proofs of these results rely on the combinatorial and asymptotic study of\na natural map, which associates with any finitely generated subgroup of\n$\\textsf{PSL}(2,\\mathbb{Z})$ a graph which we call its silhouette, which can be\ninterpreted as a conjugacy class of free finite index subgroups of\n$\\textsf{PSL}(2,\\mathbb{Z})$.",
            "author": [
                "Fr\u00e9d\u00e9rique Bassino",
                "Cyril Nicaud",
                "Pascal Weil"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08021v1",
                "http://arxiv.org/pdf/2311.08021v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.CO",
                "20E07, 20F69, 05A15, 05E16, 05C30"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08020v1",
            "title": "A signed $e$-expansion of the chromatic symmetric function and some new\n  $e$-positive graphs",
            "updated": "2023-11-14T09:25:58Z",
            "published": "2023-11-14T09:25:58Z",
            "summary": "We prove a new signed elementary symmetric function expansion of the\nchromatic symmetric function of any unit interval graph. We then use\nsign-reversing involutions to prove new combinatorial formulas for many\nfamilies of graphs, including the K-chains studied by Gebhard and Sagan, formed\nby joining cliques at single vertices, and for graphs obtained from them by\nremoving any number of edges from any of the cut vertices. We also introduce a\nversion for the quasisymmetric refinement of Shareshian and Wachs.",
            "author": [
                "Foster Tom"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08020v1",
                "http://arxiv.org/pdf/2311.08020v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05E05 (Primary) 05E10, 05C15 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08018v1",
            "title": "The unitary Cayley graph of a semiring",
            "updated": "2023-11-14T09:23:41Z",
            "published": "2023-11-14T09:23:41Z",
            "summary": "We study the unitary Cayley graph of a matrix semiring. We find bounds for\nits diameter, clique number and independence number, and determine its girth.\nWe also find the relationship between the diameter and the clique number of a\nunitary Cayley graph of a semiring $S$ and a matrix semiring over $S$.",
            "author": [
                "David Dol\u017ean"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08018v1",
                "http://arxiv.org/pdf/2311.08018v1"
            ],
            "primary_category": "math.RA",
            "category": [
                "math.RA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08011v1",
            "title": "Forgetting before Learning: Utilizing Parametric Arithmetic for\n  Knowledge Updating in Large Language Models",
            "updated": "2023-11-14T09:12:40Z",
            "published": "2023-11-14T09:12:40Z",
            "summary": "Recently Large Language Models (LLMs) have demonstrated their amazing text\nunderstanding and generation capabilities. However, even stronger LLMs may\nstill learn incorrect knowledge from the training corpus, as well as some\nknowledge that is outdated over time. Direct secondary fine-tuning with data\ncontaining new knowledge may be ineffective in updating knowledge due to the\nconflict between old and new knowledge. In this paper, we propose a new\nparadigm for fine-tuning called F-Learning (Forgetting before Learning), which\nis based on parametric arithmetic to achieve forgetting of old knowledge and\nlearning of new knowledge. Experimental results on two publicly available\ndatasets demonstrate that our proposed F-Learning can obviously improve the\nknowledge updating performance of both full fine-tuning and LoRA fine-tuning.\nMoreover, we have also discovered that forgetting old knowledge by subtracting\nthe parameters of LoRA can achieve a similar effect to subtracting the\nparameters of full fine-tuning, and sometimes even surpass it significantly.",
            "author": [
                "Shiwen Ni",
                "Dingwei Chen",
                "Chengming Li",
                "Xiping Hu",
                "Ruifeng Xu",
                "Min Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08011v1",
                "http://arxiv.org/pdf/2311.08011v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08005v1",
            "title": "Iterative missing value imputation based on feature importance",
            "updated": "2023-11-14T09:03:33Z",
            "published": "2023-11-14T09:03:33Z",
            "summary": "Many datasets suffer from missing values due to various reasons,which not\nonly increases the processing difficulty of related tasks but also reduces the\naccuracy of classification. To address this problem, the mainstream approach is\nto use missing value imputation to complete the dataset. Existing imputation\nmethods estimate the missing parts based on the observed values in the original\nfeature space, and they treat all features as equally important during data\ncompletion, while in fact different features have different importance.\nTherefore, we have designed an imputation method that considers feature\nimportance. This algorithm iteratively performs matrix completion and feature\nimportance learning, and specifically, matrix completion is based on a filling\nloss that incorporates feature importance. Our experimental analysis involves\nthree types of datasets: synthetic datasets with different noisy features and\nmissing values, real-world datasets with artificially generated missing values,\nand real-world datasets originally containing missing values. The results on\nthese datasets consistently show that the proposed method outperforms the\nexisting five imputation algorithms.To the best of our knowledge, this is the\nfirst work that considers feature importance in the imputation model.",
            "author": [
                "Cong Guo",
                "Chun Liu",
                "Wei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08005v1",
                "http://arxiv.org/pdf/2311.08005v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08003v1",
            "title": "The Hochschild (co)homology of gentle algebras",
            "updated": "2023-11-14T08:58:46Z",
            "published": "2023-11-14T08:58:46Z",
            "summary": "In this paper, we calculate the complete Tamarkin Tsygan calculus for gentle\nalgebras. For this we give a complete description of the structure of the\nHochschild cohomology ring of a gentle algebra both as a graded commutative\nalgebra and as Gerstenhaber algebra. Furthermore, we show how these structures\nare encoded in the geometric surface model of the bounded derived category\nassociated to a gentle algebra via its ribbon graph. We also compute the\nHochschild homology, the cyclic homology, the Connes' map and the right module\nstructure of the Hochschild homology over the Hochschild cohomology ring via\nthe cap product.",
            "author": [
                "Cristian Chaparro",
                "Sibylle Schroll",
                "Andrea Solotar",
                "Mariano Su\u00e1rez-\u00c1lvarez"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08003v1",
                "http://arxiv.org/pdf/2311.08003v1"
            ],
            "primary_category": "math.RT",
            "category": [
                "math.RT",
                "16E30, 16E35, 16E40, 16G10, 18G80"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08000v1",
            "title": "LiPar: A Lightweight Parallel Learning Model for Practical In-Vehicle\n  Network Intrusion Detection",
            "updated": "2023-11-14T08:54:00Z",
            "published": "2023-11-14T08:54:00Z",
            "summary": "With the development of intelligent transportation systems, vehicles are\nexposed to a complex network environment. As the main network of in-vehicle\nnetworks, the controller area network (CAN) has many potential security\nhazards, resulting in higher requirements for intrusion detection systems to\nensure safety. Among intrusion detection technologies, methods based on deep\nlearning work best without prior expert knowledge. However, they all have a\nlarge model size and rely on cloud computing, and are therefore not suitable to\nbe installed on the in-vehicle network. Therefore, we propose a lightweight\nparallel neural network structure, LiPar, to allocate task loads to multiple\nelectronic control units (ECU). The LiPar model consists of multi-dimensional\nbranch convolution networks, spatial and temporal feature fusion learning, and\na resource adaptation algorithm. Through experiments, we prove that LiPar has\ngreat detection performance, running efficiency, and lightweight model size,\nwhich can be well adapted to the in-vehicle environment practically and protect\nthe in-vehicle CAN bus security.",
            "author": [
                "Aiheng Zhang",
                "Kai Wang",
                "Bailing Wang",
                "Yulei Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08000v1",
                "http://arxiv.org/pdf/2311.08000v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07995v1",
            "title": "EPPA numbers of graphs",
            "updated": "2023-11-14T08:47:57Z",
            "published": "2023-11-14T08:47:57Z",
            "summary": "If $G$ is a graph, $A,B$ its induced subgraphs and $f\\colon A\\to B$ an\nisomorphism, we say that $f$ is a partial automorphism of $G$. In 1992,\nHrushovski proved that graphs have the extension property for partial\nautomorphisms (EPPA, also called the Hrushovski property), that is, for every\nfinite graph $G$ there is a finite graph $H$, its EPPA-witness, such that $G$\nis an induced subgraph of $H$ and every partial automorphism of $G$ extends to\nan automorphism of $H$.\n  The EPPA number of a graph $G$, denoted by\n$\\mathop{\\mathrm{eppa}}\\nolimits(G)$, is the smallest number of vertices of an\nEPPA-witness for $G$, and we put $\\mathop{\\mathrm{eppa}}\\nolimits(n) =\n\\max\\{\\mathop{\\mathrm{eppa}}\\nolimits(G) : \\lvert G\\rvert = n\\}$. In this note\nwe review the state of the area, improve some lower bounds (in particular, we\nshow that $\\mathop{\\mathrm{eppa}}\\nolimits(n)\\geq \\frac{2^n}{\\sqrt{n}}$,\nthereby identifying the correct base of the exponential) and pose several open\nquestions. We also briefly discuss EPPA numbers of hypergraphs and directed\ngraphs.",
            "author": [
                "David Bradley-Williams",
                "Peter J. Cameron",
                "Jan Hubi\u010dka",
                "Mat\u011bj Kone\u010dn\u00fd"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07995v1",
                "http://arxiv.org/pdf/2311.07995v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07975v1",
            "title": "Out-of-Distribution Knowledge Distillation via Confidence Amendment",
            "updated": "2023-11-14T08:05:02Z",
            "published": "2023-11-14T08:05:02Z",
            "summary": "Out-of-distribution (OOD) detection is essential in identifying test samples\nthat deviate from the in-distribution (ID) data upon which a standard network\nis trained, ensuring network robustness and reliability. This paper introduces\nOOD knowledge distillation, a pioneering learning framework applicable whether\nor not training ID data is available, given a standard network. This framework\nharnesses OOD-sensitive knowledge from the standard network to craft a binary\nclassifier adept at distinguishing between ID and OOD samples. To accomplish\nthis, we introduce Confidence Amendment (CA), an innovative methodology that\ntransforms an OOD sample into an ID one while progressively amending prediction\nconfidence derived from the standard network. This approach enables the\nsimultaneous synthesis of both ID and OOD samples, each accompanied by an\nadjusted prediction confidence, thereby facilitating the training of a binary\nclassifier sensitive to OOD. Theoretical analysis provides bounds on the\ngeneralization error of the binary classifier, demonstrating the pivotal role\nof confidence amendment in enhancing OOD sensitivity. Extensive experiments\nspanning various datasets and network architectures confirm the efficacy of the\nproposed method in detecting OOD samples.",
            "author": [
                "Zhilin Zhao",
                "Longbing Cao",
                "Yixuan Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07975v1",
                "http://arxiv.org/pdf/2311.07975v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07972v1",
            "title": "Residual Importance Weighted Transfer Learning For High-dimensional\n  Linear Regression",
            "updated": "2023-11-14T07:53:42Z",
            "published": "2023-11-14T07:53:42Z",
            "summary": "Transfer learning is an emerging paradigm for leveraging multiple sources to\nimprove the statistical inference on a single target. In this paper, we propose\na novel approach named residual importance weighted transfer learning (RIW-TL)\nfor high-dimensional linear models built on penalized likelihood. Compared to\nexisting methods such as Trans-Lasso that selects sources in an all-in-all-out\nmanner, RIW-TL includes samples via importance weighting and thus may permit\nmore effective sample use. To determine the weights, remarkably RIW-TL only\nrequires the knowledge of one-dimensional densities dependent on residuals,\nthus overcoming the curse of dimensionality of having to estimate\nhigh-dimensional densities in naive importance weighting. We show that the\noracle RIW-TL provides a faster rate than its competitors and develop a\ncross-fitting procedure to estimate this oracle. We discuss variants of RIW-TL\nby adopting different choices for residual weighting. The theoretical\nproperties of RIW-TL and its variants are established and compared with those\nof LASSO and Trans-Lasso. Extensive simulation and a real data analysis confirm\nits advantages.",
            "author": [
                "Junlong Zhao",
                "Shengbin Zheng",
                "Chenlei Leng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07972v1",
                "http://arxiv.org/pdf/2311.07972v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07966v1",
            "title": "Higher-Order Expander Graph Propagation",
            "updated": "2023-11-14T07:44:46Z",
            "published": "2023-11-14T07:44:46Z",
            "summary": "Graph neural networks operate on graph-structured data via exchanging\nmessages along edges. One limitation of this message passing paradigm is the\nover-squashing problem. Over-squashing occurs when messages from a node's\nexpanded receptive field are compressed into fixed-size vectors, potentially\ncausing information loss. To address this issue, recent works have explored\nusing expander graphs, which are highly-connected sparse graphs with low\ndiameters, to perform message passing. However, current methods on expander\ngraph propagation only consider pair-wise interactions, ignoring higher-order\nstructures in complex data. To explore the benefits of capturing these\nhigher-order correlations while still leveraging expander graphs, we introduce\nhigher-order expander graph propagation. We propose two methods for\nconstructing bipartite expanders and evaluate their performance on both\nsynthetic and real-world datasets.",
            "author": [
                "Thomas Christie",
                "Yu He"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07966v1",
                "http://arxiv.org/pdf/2311.07966v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08434v1",
            "title": "Uplift Modeling based on Graph Neural Network Combined with Causal\n  Knowledge",
            "updated": "2023-11-14T07:21:00Z",
            "published": "2023-11-14T07:21:00Z",
            "summary": "Uplift modeling is a fundamental component of marketing effect modeling,\nwhich is commonly employed to evaluate the effects of treatments on outcomes.\nThrough uplift modeling, we can identify the treatment with the greatest\nbenefit. On the other side, we can identify clients who are likely to make\nfavorable decisions in response to a certain treatment. In the past, uplift\nmodeling approaches relied heavily on the difference-in-difference (DID)\narchitecture, paired with a machine learning model as the estimation learner,\nwhile neglecting the link and confidential information between features. We\nproposed a framework based on graph neural networks that combine causal\nknowledge with an estimate of uplift value. Firstly, we presented a causal\nrepresentation technique based on CATE (conditional average treatment effect)\nestimation and adjacency matrix structure learning. Secondly, we suggested a\nmore scalable uplift modeling framework based on graph convolution networks for\ncombining causal knowledge. Our findings demonstrate that this method works\neffectively for predicting uplift values, with small errors in typical\nsimulated data, and its effectiveness has been verified in actual industry\nmarketing data.",
            "author": [
                "Haowen Wang",
                "Xinyan Ye",
                "Yangze Zhou",
                "Zhiyi Zhang",
                "Longhan Zhang",
                "Jing Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08434v1",
                "http://arxiv.org/pdf/2311.08434v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07956v2",
            "title": "Robust Learning Based Condition Diagnosis Method for Distribution\n  Network Switchgear",
            "updated": "2023-12-07T00:17:41Z",
            "published": "2023-11-14T07:20:46Z",
            "summary": "This paper introduces a robust, learning-based method for diagnosing the\nstate of distribution network switchgear, which is crucial for maintaining the\npower quality for end users. Traditional diagnostic models often rely heavily\non expert knowledge and lack robustness. To address this, our method\nincorporates an expanded feature vector that includes environmental data,\ntemperature readings, switch position, motor operation, insulation conditions,\nand local discharge information. We tackle the issue of high dimensionality\nthrough feature mapping. The method introduces a decision radius to categorize\nunlabeled samples and updates the model parameters using a combination of\nsupervised and unsupervised loss, along with a consistency regularization\nfunction. This approach ensures robust learning even with a limited number of\nlabeled samples. Comparative analysis demonstrates that this method\nsignificantly outperforms existing models in both accuracy and robustness.",
            "author": [
                "Wenxi Zhang",
                "Zhe Li",
                "Weixi Li",
                "Weisi Ma",
                "Xinyi Chen",
                "Sizhe Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07956v2",
                "http://arxiv.org/pdf/2311.07956v2"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07939v1",
            "title": "Discretized Distributed Optimization over Dynamic Digraphs",
            "updated": "2023-11-14T06:33:41Z",
            "published": "2023-11-14T06:33:41Z",
            "summary": "We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.",
            "author": [
                "Mohammadreza Doostmohammadian",
                "Wei Jiang",
                "Muwahida Liaquat",
                "Alireza Aghasi",
                "Houman Zarrabi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07939v1",
                "http://arxiv.org/pdf/2311.07939v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "cs.LG",
                "cs.SY",
                "eess.SP",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07929v1",
            "title": "Self-supervised Heterogeneous Graph Variational Autoencoders",
            "updated": "2023-11-14T06:15:16Z",
            "published": "2023-11-14T06:15:16Z",
            "summary": "Heterogeneous Information Networks (HINs), which consist of various types of\nnodes and edges, have recently demonstrated excellent performance in graph\nmining. However, most existing heterogeneous graph neural networks (HGNNs)\nignore the problems of missing attributes, inaccurate attributes and scarce\nlabels for nodes, which limits their expressiveness. In this paper, we propose\na generative self-supervised model SHAVA to address these issues\nsimultaneously. Specifically, SHAVA first initializes all the nodes in the\ngraph with a low-dimensional representation matrix. After that, based on the\nvariational graph autoencoder framework, SHAVA learns both node-level and\nattribute-level embeddings in the encoder, which can provide fine-grained\nsemantic information to construct node attributes. In the decoder, SHAVA\nreconstructs both links and attributes. Instead of directly reconstructing raw\nfeatures for attributed nodes, SHAVA generates the initial low-dimensional\nrepresentation matrix for all the nodes, based on which raw features of\nattributed nodes are further reconstructed to leverage accurate attributes. In\nthis way, SHAVA can not only complete informative features for non-attributed\nnodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct\nextensive experiments to show the superiority of SHAVA in tackling HINs with\nmissing and inaccurate attributes.",
            "author": [
                "Yige Zhao",
                "Jianxiang Yu",
                "Yao Cheng",
                "Chengcheng Yu",
                "Yiding Liu",
                "Xiang Li",
                "Shuaiqiang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07929v1",
                "http://arxiv.org/pdf/2311.07929v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07921v1",
            "title": "From Horndeski to CGHS and beyond",
            "updated": "2023-11-14T05:45:25Z",
            "published": "2023-11-14T05:45:25Z",
            "summary": "The knowledge of what entered black hole (BH) is completely lost as it\nevaporates. This contradicts the unitarity principle of quantum mechanics and\nis referred to as the information loss paradox. Understanding the end stages of\nBH evaporation is key to resolving this paradox. As a first step, we need to\nhave exact models that can mimic 4-D BHs in General relativity in classical\nlimit and have a systematic way to include high-energy corrections. While there\nare various models in the literature, there is no systematic procedure by which\none can study high-energy corrections. In this work, for the first time, we\nobtain Callan, Giddings, Harvey, and Strominger (CGHS) -- a (1+1)-D -- model\nfrom 4-D Horndeski action -- the most general scalar-tensor theory that does\nnot lead to Ostrogradsky ghosts. We then show that 4-D Horndeski action can\nsystematically provide a route to include higher-order terms relevant at the\nend stages of black hole evaporation. We derive the leading order Hawking flux\nwhile discussing some intriguing characteristics of the corrected CGHS models.\nWe compare our results with other works and discuss the implications for\nprimordial BHs.",
            "author": [
                "Susobhan Mandal",
                "Tausif Parvez",
                "S. Shankaranarayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07921v1",
                "http://arxiv.org/pdf/2311.07921v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc",
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09248v1",
            "title": "Smart Home Goal Feature Model -- A guide to support Smart Homes for\n  Ageing in Place",
            "updated": "2023-11-14T05:42:13Z",
            "published": "2023-11-14T05:42:13Z",
            "summary": "Smart technologies are significant in supporting ageing in place for elderly.\nLeveraging Artificial Intelligence (AI) and Machine Learning (ML), it provides\npeace of mind, enabling the elderly to continue living independently. Elderly\nuse smart technologies for entertainment and social interactions, this can be\nextended to provide safety and monitor health and environmental conditions,\ndetect emergencies and notify informal and formal caregivers when care is\nneeded. This paper provides an overview of the smart home technologies\ncommercially available to support ageing in place, the advantages and\nchallenges of smart home technologies, and their usability from elderlys\nperspective. Synthesizing prior knowledge, we created a structured Smart Home\nGoal Feature Model (SHGFM) to resolve heuristic approaches used by the Subject\nMatter Experts (SMEs) at aged care facilities and healthcare researchers in\nadapting smart homes. The SHGFM provides SMEs the ability to (i) establish\ngoals and (ii) identify features to set up strategies to design, develop and\ndeploy smart homes for the elderly based on personalised needs. Our model\nprovides guidance to healthcare researchers and aged care industries to set up\nsmart homes based on the needs of elderly, by defining a set of goals at\ndifferent levels mapped to a different set of features.",
            "author": [
                "Irini Logothetis",
                "Priya Rani",
                "Shangeetha Sivasothy",
                "Rajesh Vasa",
                "Kon Mouzakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09248v1",
                "http://arxiv.org/pdf/2311.09248v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07919v1",
            "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified\n  Large-Scale Audio-Language Models",
            "updated": "2023-11-14T05:34:50Z",
            "published": "2023-11-14T05:34:50Z",
            "summary": "Recently, instruction-following audio-language models have received broad\nattention for audio interaction with humans. However, the absence of\npre-trained audio models capable of handling diverse audio types and tasks has\nhindered progress in this field. Consequently, most existing works have only\nbeen able to support a limited range of interaction capabilities. In this\npaper, we develop the Qwen-Audio model and address this limitation by scaling\nup audio-language pre-training to cover over 30 tasks and various audio types,\nsuch as human speech, natural sounds, music, and songs, to facilitate universal\naudio understanding abilities. However, directly co-training all tasks and\ndatasets can lead to interference issues, as the textual labels associated with\ndifferent datasets exhibit considerable variations due to differences in task\nfocus, language, granularity of annotation, and text structure. To overcome the\none-to-many interference, we carefully design a multi-task training framework\nby conditioning on a sequence of hierarchical tags to the decoder for\nencouraging knowledge sharing and avoiding interference through shared and\nspecified tags respectively. Remarkably, Qwen-Audio achieves impressive\nperformance across diverse benchmark tasks without requiring any task-specific\nfine-tuning, surpassing its counterparts. Building upon the capabilities of\nQwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from\nvarious audios and text inputs, enabling multi-turn dialogues and supporting\nvarious audio-central scenarios.",
            "author": [
                "Yunfei Chu",
                "Jin Xu",
                "Xiaohuan Zhou",
                "Qian Yang",
                "Shiliang Zhang",
                "Zhijie Yan",
                "Chang Zhou",
                "Jingren Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07919v1",
                "http://arxiv.org/pdf/2311.07919v1"
            ],
            "primary_category": "eess.AS",
            "category": [
                "eess.AS",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07914v1",
            "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
            "updated": "2023-11-14T05:21:57Z",
            "published": "2023-11-14T05:21:57Z",
            "summary": "The contemporary LLMs are prone to producing hallucinations, stemming mainly\nfrom the knowledge gaps within the models. To address this critical limitation,\nresearchers employ diverse strategies to augment the LLMs by incorporating\nexternal knowledge, aiming to reduce hallucinations and enhance reasoning\naccuracy. Among these strategies, leveraging knowledge graphs as a source of\nexternal information has demonstrated promising results. In this survey, we\nconduct a comprehensive review of these knowledge-graph-based knowledge\naugmentation techniques in LLMs, focusing on their efficacy in mitigating\nhallucinations. We systematically categorize these methods into three\noverarching groups, offering both methodological comparisons and empirical\nevaluations of their performance. Lastly, the paper explores the challenges\nassociated with these techniques and outlines potential avenues for future\nresearch in this emerging field.",
            "author": [
                "Garima Agrawal",
                "Tharindu Kumarage",
                "Zeyad Alghami",
                "Huan Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07914v1",
                "http://arxiv.org/pdf/2311.07914v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07908v1",
            "title": "Learning Bayes-Optimal Channel Estimation for Holographic MIMO in\n  Unknown EM Environments",
            "updated": "2023-11-14T05:02:46Z",
            "published": "2023-11-14T05:02:46Z",
            "summary": "Holographic MIMO (HMIMO) has recently been recognized as a promising enabler\nfor future 6G systems through the use of an ultra-massive number of antennas in\na compact space to exploit the propagation characteristics of the\nelectromagnetic (EM) channel. Nevertheless, the promised gain of HMIMO could\nnot be fully unleashed without an efficient means to estimate the\nhigh-dimensional channel. Bayes-optimal estimators typically necessitate either\na large volume of supervised training samples or a priori knowledge of the true\nchannel distribution, which could hardly be available in practice due to the\nenormous system scale and the complicated EM environments. It is thus important\nto design a Bayes-optimal estimator for the HMIMO channels in arbitrary and\nunknown EM environments, free of any supervision or priors. This work proposes\na self-supervised minimum mean-square-error (MMSE) channel estimation algorithm\nbased on powerful machine learning tools, i.e., score matching and principal\ncomponent analysis. The training stage requires only the pilot signals, without\nknowing the spatial correlation, the ground-truth channels, or the received\nsignal-to-noise-ratio. Simulation results will show that, even being totally\nself-supervised, the proposed algorithm can still approach the performance of\nthe oracle MMSE method with an extremely low complexity, making it a\ncompetitive candidate in practice.",
            "author": [
                "Wentao Yu",
                "Hengtao He",
                "Xianghao Yu",
                "Shenghui Song",
                "Jun Zhang",
                "Ross D. Murch",
                "Khaled B. Letaief"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07908v1",
                "http://arxiv.org/pdf/2311.07908v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.09247v2",
            "title": "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",
            "updated": "2023-11-26T20:42:08Z",
            "published": "2023-11-14T04:33:49Z",
            "summary": "We explore the abstract reasoning abilities of text-only and multimodal\nversions of GPT-4, using the ConceptARC benchmark [10], which is designed to\nevaluate robust understanding and reasoning with core-knowledge concepts. We\nextend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,\none-shot prompting (rather than simple, zero-shot prompts) with text versions\nof ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,\non zero- and one-shot prompts using image versions of the simplest tasks. Our\nexperimental results support the conclusion that neither version of GPT-4 has\ndeveloped robust abstraction abilities at humanlike levels.",
            "author": [
                "Melanie Mitchell",
                "Alessandro B. Palmarini",
                "Arseny Moskvichev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.09247v2",
                "http://arxiv.org/pdf/2311.09247v2"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07897v1",
            "title": "CPopQA: Ranking Cultural Concept Popularity by LLMs",
            "updated": "2023-11-14T04:10:40Z",
            "published": "2023-11-14T04:10:40Z",
            "summary": "Prior work has demonstrated large language models' (LLMs) potential to\ndiscern statistical tendencies within their pre-training corpora. Despite that,\nmany examinations of LLMs' knowledge capacity focus on knowledge explicitly\nappearing in the training data or implicitly inferable from similar contexts.\nHow well an LLM captures the corpus-level statistical trends of concepts for\nreasoning, especially long-tail ones, is still underexplored. In this study, we\nintroduce a novel few-shot question-answering task (CPopQA) that examines LLMs'\nstatistical ranking abilities for long-tail cultural concepts (e.g., holidays),\nwith a specific focus on these concepts' popularity in the United States and\nthe United Kingdom, respectively. We curate a dataset containing 459 holidays\nacross 58 countries, generating a total of 6,000 QA testing pairs. Experiments\non four strong LLMs show that large models are capable of ranking long-tail\ncultural concepts regarding their statistical tendency. Notably, GPT-3.5\ndisplayed superior performance and exhibited its potential to identify\ngeo-cultural proximity across continents.",
            "author": [
                "Ming Jiang",
                "Mansi Joshi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07897v1",
                "http://arxiv.org/pdf/2311.07897v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07886v2",
            "title": "Ultrafilters, Transversals, and the Hat Game",
            "updated": "2023-12-02T03:21:19Z",
            "published": "2023-11-14T03:41:38Z",
            "summary": "Geschke, Lubarsky, and Rahn in ``Choice and the Hat\nGame''~\\cite{choice-and-the-hat-game} generalize the classic hat game puzzle to\ninfinitely-many players and ask whether every model of set theory without\nchoice in which the optimal solution can be carried out contains either a\nnonprincipal ultrafilter on $\\mathbb N$ or else a Vitali set. A negative answer\nis obtained here by constructing a model in which there is an optimal solution\nto the hat game puzzle but no nonprincipal ultrafilter on $\\mathbb N$ and no\nVitali set. This is accomplished in a more general setting, establishing that\nfor any Borel bipartite graph $\\Gamma$ not embedding some $K_{n,\\omega_1}$ and\nwith countable colouring number there is a model of $\\mbox{ZF} + \\mbox{DC}$ in\nwhich $\\Gamma$ has a $2$-colouring but there is no ultrafilter as above or\nVitali set. The same conclusion applies to the natural generalization of the\nhat game to an arbitrary finite number of hat colours.",
            "author": [
                "Luke Serafin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07886v2",
                "http://arxiv.org/pdf/2311.07886v2"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO",
                "math.CO",
                "03E35 (Primary), 03E65, 05C15 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07885v1",
            "title": "One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View\n  Generation and 3D Diffusion",
            "updated": "2023-11-14T03:40:25Z",
            "published": "2023-11-14T03:40:25Z",
            "summary": "Recent advancements in open-world 3D object generation have been remarkable,\nwith image-to-3D methods offering superior fine-grained control over their\ntext-to-3D counterparts. However, most existing models fall short in\nsimultaneously providing rapid generation speeds and high fidelity to input\nimages - two features essential for practical applications. In this paper, we\npresent One-2-3-45++, an innovative method that transforms a single image into\na detailed 3D textured mesh in approximately one minute. Our approach aims to\nfully harness the extensive knowledge embedded in 2D diffusion models and\npriors from valuable yet limited 3D data. This is achieved by initially\nfinetuning a 2D diffusion model for consistent multi-view image generation,\nfollowed by elevating these images to 3D with the aid of multi-view conditioned\n3D native diffusion models. Extensive experimental evaluations demonstrate that\nour method can produce high-quality, diverse 3D assets that closely mirror the\noriginal input image. Our project webpage:\nhttps://sudo-ai-3d.github.io/One2345plus_page.",
            "author": [
                "Minghua Liu",
                "Ruoxi Shi",
                "Linghao Chen",
                "Zhuoyang Zhang",
                "Chao Xu",
                "Xinyue Wei",
                "Hansheng Chen",
                "Chong Zeng",
                "Jiayuan Gu",
                "Hao Su"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07885v1",
                "http://arxiv.org/pdf/2311.07885v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.GR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07876v1",
            "title": "Learning Adversarial Low-rank Markov Decision Processes with Unknown\n  Transition and Full-information Feedback",
            "updated": "2023-11-14T03:12:43Z",
            "published": "2023-11-14T03:12:43Z",
            "summary": "In this work, we study the low-rank MDPs with adversarially changed losses in\nthe full-information feedback setting. In particular, the unknown transition\nprobability kernel admits a low-rank matrix decomposition \\citep{REPUCB22}, and\nthe loss functions may change adversarially but are revealed to the learner at\nthe end of each episode. We propose a policy optimization-based algorithm POLO,\nand we prove that it attains the\n$\\widetilde{O}(K^{\\frac{5}{6}}A^{\\frac{1}{2}}d\\ln(1+M)/(1-\\gamma)^2)$ regret\nguarantee, where $d$ is rank of the transition kernel (and hence the dimension\nof the unknown representations), $A$ is the cardinality of the action space,\n$M$ is the cardinality of the model class, and $\\gamma$ is the discounted\nfactor. Notably, our algorithm is oracle-efficient and has a regret guarantee\nwith no dependence on the size of potentially arbitrarily large state space.\nFurthermore, we also prove an $\\Omega(\\frac{\\gamma^2}{1-\\gamma} \\sqrt{d A K})$\nregret lower bound for this problem, showing that low-rank MDPs are\nstatistically more difficult to learn than linear MDPs in the regret\nminimization setting. To the best of our knowledge, we present the first\nalgorithm that interleaves representation learning, exploration, and\nexploitation to achieve the sublinear regret guarantee for RL with nonlinear\nfunction approximation and adversarial losses.",
            "author": [
                "Canzhe Zhao",
                "Ruofeng Yang",
                "Baoxiang Wang",
                "Xuezhou Zhang",
                "Shuai Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07876v1",
                "http://arxiv.org/pdf/2311.07876v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07869v1",
            "title": "Hybrid GRU-CNN Bilinear Parameters Initialization for Quantum\n  Approximate Optimization Algorithm",
            "updated": "2023-11-14T03:00:39Z",
            "published": "2023-11-14T03:00:39Z",
            "summary": "The Quantum Approximate Optimization Algorithm (QAOA), a pivotal paradigm in\nthe realm of variational quantum algorithms (VQAs), offers promising\ncomputational advantages for tackling combinatorial optimization problems.\nWell-defined initial circuit parameters, responsible for preparing a\nparameterized quantum state encoding the solution, play a key role in\noptimizing QAOA. However, classical optimization techniques encounter\nchallenges in discerning optimal parameters that align with the optimal\nsolution. In this work, we propose a hybrid optimization approach that\nintegrates Gated Recurrent Units (GRU), Convolutional Neural Networks (CNN),\nand a bilinear strategy as an innovative alternative to conventional optimizers\nfor predicting optimal parameters of QAOA circuits. GRU serves to\nstochastically initialize favorable parameters for depth-1 circuits, while CNN\npredicts initial parameters for depth-2 circuits based on the optimized\nparameters of depth-1 circuits. To assess the efficacy of our approach, we\nconducted a comparative analysis with traditional initialization methods using\nQAOA on Erd\\H{o}s-R\\'enyi graph instances, revealing superior optimal\napproximation ratios. We employ the bilinear strategy to initialize QAOA\ncircuit parameters at greater depths, with reference parameters obtained from\nGRU-CNN optimization. This approach allows us to forecast parameters for a\ndepth-12 QAOA circuit, yielding a remarkable approximation ratio of 0.998\nacross 10 qubits, which surpasses that of the random initialization strategy\nand the PPN2 method at a depth of 10. The proposed hybrid GRU-CNN bilinear\noptimization method significantly improves the effectiveness and accuracy of\nparameters initialization, offering a promising iterative framework for QAOA\nthat elevates its performance.",
            "author": [
                "Zuyu Xu",
                "Pengnian Cai",
                "Kang Sheng",
                "Tao Yang",
                "Yuanming Hu",
                "Yunlai Zhu",
                "Zuheng Wu",
                "Yuehua Dai",
                "Fei Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07869v1",
                "http://arxiv.org/pdf/2311.07869v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07854v1",
            "title": "Topological and spectral properties of random digraphs",
            "updated": "2023-11-14T02:12:41Z",
            "published": "2023-11-14T02:12:41Z",
            "summary": "We investigate some topological and spectral properties of\nErd\\H{o}s-R\\'{e}nyi (ER) random digraphs $D(n,p)$. In terms of topological\nproperties, our primary focus lies in analyzing the number of non-isolated\nvertices $V_x(D)$ as well as two vertex-degree-based topological indices: the\nRandi\\'c index $R(D)$ and sum-connectivity index $\\chi(D)$. First, by\nperforming a scaling analysis we show that the average degree $\\langle k\n\\rangle$ serves as scaling parameter for the average values of $V_x(D)$, $R(D)$\nand $\\chi(D)$. Then, we also state expressions relating the number of arcs,\nspectral radius, and closed walks of length 2 to $(n,p)$, the parameters of ER\nrandom digraphs. Concerning spectral properties, we compute six different graph\nenergies on $D(n,p)$. We start by validating $\\langle k \\rangle$ as the scaling\nparameter of the graph energies. Additionally, we reformulate a set of bounds\npreviously reported in the literature for these energies as a function $(n,p)$.\nFinally, we phenomenologically state relations between energies that allow us\nto extend previously known bounds.",
            "author": [
                "C. T. Mart\u00ednez-Mart\u00ednez",
                "J. A. M\u00e9ndez-Berm\u00fadez",
                "Jos\u00e9 M. Sigarreta"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07854v1",
                "http://arxiv.org/pdf/2311.07854v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07850v1",
            "title": "Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA",
            "updated": "2023-11-14T02:05:29Z",
            "published": "2023-11-14T02:05:29Z",
            "summary": "We present BYOKG, a universal question-answering (QA) system that can operate\non any knowledge graph (KG), requires no human-annotated training data, and can\nbe ready to use within a day -- attributes that are out-of-scope for current\nKGQA systems. BYOKG draws inspiration from the remarkable ability of humans to\ncomprehend information present in an unseen KG through exploration -- starting\nat random nodes, inspecting the labels of adjacent nodes and edges, and\ncombining them with their prior world knowledge. In BYOKG, exploration\nleverages an LLM-backed symbolic agent that generates a diverse set of\nquery-program exemplars, which are then used to ground a retrieval-augmented\nreasoning procedure to predict programs for arbitrary questions. BYOKG is\neffective over both small- and large-scale graphs, showing dramatic gains in QA\naccuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,\nrespectively. On GrailQA, we further show that our unsupervised BYOKG\noutperforms a supervised in-context learning method, demonstrating the\neffectiveness of exploration. Lastly, we find that performance of BYOKG\nreliably improves with continued exploration as well as improvements in the\nbase LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1\non a sub-sampled zero-shot split of GrailQA.",
            "author": [
                "Dhruv Agarwal",
                "Rajarshi Das",
                "Sopan Khosla",
                "Rashmi Gangadharaiah"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07850v1",
                "http://arxiv.org/pdf/2311.07850v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.DB",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07841v2",
            "title": "PEMS: Pre-trained Epidemic Time-series Models",
            "updated": "2023-11-19T19:47:36Z",
            "published": "2023-11-14T01:40:21Z",
            "summary": "Providing accurate and reliable predictions about the future of an epidemic\nis an important problem for enabling informed public health decisions. Recent\nworks have shown that leveraging data-driven solutions that utilize advances in\ndeep learning methods to learn from past data of an epidemic often outperform\ntraditional mechanistic models. However, in many cases, the past data is sparse\nand may not sufficiently capture the underlying dynamics. While there exists a\nlarge amount of data from past epidemics, leveraging prior knowledge from\ntime-series data of other diseases is a non-trivial challenge. Motivated by the\nsuccess of pre-trained models in language and vision tasks, we tackle the\nproblem of pre-training epidemic time-series models to learn from multiple\ndatasets from different diseases and epidemics. We introduce Pre-trained\nEpidemic Time-Series Models (PEMS) that learn from diverse time-series datasets\nof a variety of diseases by formulating pre-training as a set of\nself-supervised learning (SSL) tasks. We tackle various important challenges\nspecific to pre-training for epidemic time-series such as dealing with\nheterogeneous dynamics and efficiently capturing useful patterns from multiple\nepidemic datasets by carefully designing the SSL tasks to learn important\npriors about the epidemic dynamics that can be leveraged for fine-tuning to\nmultiple downstream tasks. The resultant PEM outperforms previous\nstate-of-the-art methods in various downstream time-series tasks across\ndatasets of varying seasonal patterns, geography, and mechanism of contagion\nincluding the novel Covid-19 pandemic unseen in pre-trained data with better\nefficiency using smaller fraction of datasets.",
            "author": [
                "Harshavardhan Kamarthi",
                "B. Aditya Prakash"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07841v2",
                "http://arxiv.org/pdf/2311.07841v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07838v1",
            "title": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
            "updated": "2023-11-14T01:38:02Z",
            "published": "2023-11-14T01:38:02Z",
            "summary": "Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.",
            "author": [
                "Xiaonan Li",
                "Changtai Zhu",
                "Linyang Li",
                "Zhangyue Yin",
                "Tianxiang Sun",
                "Xipeng Qiu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07838v1",
                "http://arxiv.org/pdf/2311.07838v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07809v2",
            "title": "Non-radiative configurations of a few quantum emitters ensembles:\n  evolutionary optimization approach",
            "updated": "2023-12-01T14:36:01Z",
            "published": "2023-11-13T23:51:02Z",
            "summary": "In this work, we employ differential evolution algorithm to identify the\noptimal configurations of small atomic ensembles supporting quantum states with\nmaximal radiative lifetime. We demonstrate that atoms mostly tend to assemble\nin quasi-regular structures with specific geometry strongly depending on the\nminimal interatomic distance $r_{min}$. We identified the clear underlying\nphysics that governs the suppression of the radiative losses in particular\ngeometries. However, we reveal that the specific configurations in small\nensembles are not easily predictable based on the knowledge established for the\narrays of large size. In particular, the states that inherit their properties\nfrom bound states in continuum in infinite lattices turn out to be the most\nsubradiant in a wide range of $r_{min}$ values. We also show that for small\ninteratomic distance the chains with modulated interatomic distances exhibit\nfast exponential decrease of the radiative losses with the size of the\nensemble.",
            "author": [
                "Ilya Volkov",
                "Stanislav Mitsai",
                "Stepan Zhogolev",
                "Danil Kornovan",
                "Roman Savelev",
                "Mihail Petrov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07809v2",
                "http://arxiv.org/pdf/2311.07809v2"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "physics.atom-ph",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07783v1",
            "title": "Size-Aware Hypergraph Motifs",
            "updated": "2023-11-13T22:19:52Z",
            "published": "2023-11-13T22:19:52Z",
            "summary": "Complex systems frequently exhibit multi-way, rather than pairwise,\ninteractions. These group interactions cannot be faithfully modeled as\ncollections of pairwise interactions using graphs, and instead require\nhypergraphs. However, methods that analyze hypergraphs directly, rather than\nvia lossy graph reductions, remain limited. Hypergraph motif mining holds\npromise in this regard, as motif patterns serve as building blocks for larger\ngroup interactions which are inexpressible by graphs. Recent work has focused\non categorizing and counting hypergraph motifs based on the existence of nodes\nin hyperedge intersection regions. Here, we argue that the relative sizes of\nhyperedge intersections within motifs contain varied and valuable information.\nWe propose a suite of efficient algorithms for finding triplets of hyperedges\nbased on optimizing the sizes of these intersection patterns. This formulation\nuncovers interesting local patterns of interaction, finding hyperedge triplets\nthat either (1) are the least correlated with each other, (2) have the highest\npairwise but not groupwise correlation, or (3) are the most correlated with\neach other. We formalize this as a combinatorial optimization problem and\ndesign efficient algorithms based on filtering hyperedges. Our experimental\nevaluation shows that the resulting hyperedge triplets yield insightful\ninformation on real-world hypergraphs. Our approach is also orders of magnitude\nfaster than a naive baseline implementation.",
            "author": [
                "Jason Niu",
                "Ilya D. Amburg",
                "Sinan G. Aksoy",
                "Ahmet Erdem Sar\u0131y\u00fcce"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07783v1",
                "http://arxiv.org/pdf/2311.07783v1"
            ],
            "primary_category": "cs.DM",
            "category": [
                "cs.DM",
                "cs.SI",
                "physics.data-an",
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07780v2",
            "title": "Parrot-Trained Adversarial Examples: Pushing the Practicality of\n  Black-Box Audio Attacks against Speaker Recognition Models",
            "updated": "2023-11-17T21:34:33Z",
            "published": "2023-11-13T22:12:19Z",
            "summary": "Audio adversarial examples (AEs) have posed significant security challenges\nto real-world speaker recognition systems. Most black-box attacks still require\ncertain information from the speaker recognition model to be effective (e.g.,\nkeeping probing and requiring the knowledge of similarity scores). This work\naims to push the practicality of the black-box attacks by minimizing the\nattacker's knowledge about a target speaker recognition model. Although it is\nnot feasible for an attacker to succeed with completely zero knowledge, we\nassume that the attacker only knows a short (or a few seconds) speech sample of\na target speaker. Without any probing to gain further knowledge about the\ntarget model, we propose a new mechanism, called parrot training, to generate\nAEs against the target model. Motivated by recent advancements in voice\nconversion (VC), we propose to use the one short sentence knowledge to generate\nmore synthetic speech samples that sound like the target speaker, called parrot\nspeech. Then, we use these parrot speech samples to train a parrot-trained(PT)\nsurrogate model for the attacker. Under a joint transferability and perception\nframework, we investigate different ways to generate AEs on the PT model\n(called PT-AEs) to ensure the PT-AEs can be generated with high transferability\nto a black-box target model with good human perceptual quality. Real-world\nexperiments show that the resultant PT-AEs achieve the attack success rates of\n45.8% - 80.8% against the open-source models in the digital-line scenario and\n47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon\nEcho, and Google Home, in the over-the-air scenario.",
            "author": [
                "Rui Duan",
                "Zhe Qu",
                "Leah Ding",
                "Yao Liu",
                "Zhuo Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07780v2",
                "http://arxiv.org/pdf/2311.07780v2"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.AI",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07772v3",
            "title": "In-context Learning and Gradient Descent Revisited",
            "updated": "2023-11-18T19:58:27Z",
            "published": "2023-11-13T21:42:38Z",
            "summary": "In-context learning (ICL) has shown impressive results in few-shot learning\ntasks, yet its underlying mechanism is still not fully understood. Recent works\nsuggest that ICL can be thought of as a gradient descent (GD) based\noptimization process. While promising, these results mainly focus on simplified\nsettings of ICL and provide only a preliminary evaluation of the similarities\nbetween the two methods. In this work, we revisit the comparison between ICL\nand GD-based finetuning and study what properties of ICL an equivalent process\nmust follow. We highlight a major difference in the flow of information between\nICL and standard finetuning. Namely, ICL can only rely on information from\nlower layers at every point, while finetuning depends on loss gradients from\ndeeper layers. We refer to this discrepancy as Layer Causality and show that a\nlayer causal variant of the finetuning process aligns with ICL on par with\nvanilla finetuning and is even better in most cases across relevant metrics. To\nthe best of our knowledge, this is the first work to discuss this discrepancy\nexplicitly and suggest a solution that tackles this problem with minimal\nchanges.",
            "author": [
                "Gilad Deutch",
                "Nadav Magar",
                "Tomer Bar Natan",
                "Guy Dar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07772v3",
                "http://arxiv.org/pdf/2311.07772v3"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07759v1",
            "title": "Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic\n  Systems",
            "updated": "2023-11-13T21:20:17Z",
            "published": "2023-11-13T21:20:17Z",
            "summary": "High-level reasoning can be defined as the capability to generalize over\nknowledge acquired via experience, and to exhibit robust behavior in novel\nsituations. Such form of reasoning is a basic skill in humans, who seamlessly\nuse it in a broad spectrum of tasks, from language communication to decision\nmaking in complex situations. When it manifests itself in understanding and\nmanipulating the everyday world of objects and their interactions, we talk\nabout common sense or commonsense reasoning. State-of-the-art AI systems don't\npossess such capability: for instance, Large Language Models have recently\nbecome popular by demonstrating remarkable fluency in conversing with humans,\nbut they still make trivial mistakes when probed for commonsense competence; on\na different level, performance degradation outside training data prevents\nself-driving vehicles to safely adapt to unseen scenarios, a serious and\nunsolved problem that limits the adoption of such technology. In this paper we\npropose to enable high-level reasoning in AI systems by integrating cognitive\narchitectures with external neuro-symbolic components. We illustrate a hybrid\nframework centered on ACT-R and we discuss the role of generative models in\nrecent and future applications.",
            "author": [
                "Alessandro Oltramari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07759v1",
                "http://arxiv.org/pdf/2311.07759v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07742v1",
            "title": "Modeling Sequences as Star Graphs to Address Over-smoothing in\n  Self-attentive Sequential Recommendation",
            "updated": "2023-11-13T20:49:01Z",
            "published": "2023-11-13T20:49:01Z",
            "summary": "Self-attention (SA) mechanisms have been widely used in developing sequential\nrecommendation (SR) methods, and demonstrated state-of-the-art performance.\nHowever, in this paper, we show that self-attentive SR methods substantially\nsuffer from the over-smoothing issue that item embeddings within a sequence\nbecome increasingly similar across attention blocks. As widely demonstrated in\nthe literature, this issue could lead to a loss of information in individual\nitems, and significantly degrade models' scalability and performance. To\naddress the over-smoothing issue, in this paper, we view items within a\nsequence constituting a star graph and develop a method, denoted as MSSG, for\nSR. Different from existing self-attentive methods, MSSG introduces an\nadditional internal node to specifically capture the global information within\nthe sequence, and does not require information propagation among items. This\ndesign fundamentally addresses the over-smoothing issue and enables MSSG a\nlinear time complexity with respect to the sequence length. We compare MSSG\nwith ten state-of-the-art baseline methods on six public benchmark datasets.\nOur experimental results demonstrate that MSSG significantly outperforms the\nbaseline methods, with an improvement of as much as 10.10%. Our analysis shows\nthe superior scalability of MSSG over the state-of-the-art self-attentive\nmethods. Our complexity analysis and run-time performance comparison together\nshow that MSSG is both theoretically and practically more efficient than\nself-attentive methods. Our analysis of the attention weights learned in\nSA-based methods indicates that on sparse recommendation data, modeling\ndependencies in all item pairs using the SA mechanism yields limited\ninformation gain, and thus, might not benefit the recommendation performance",
            "author": [
                "Bo Peng",
                "Ziqi Chen",
                "Srinivasan Parthasarathy",
                "Xia Ning"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07742v1",
                "http://arxiv.org/pdf/2311.07742v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07727v1",
            "title": "Computing the homotopy type and homological invariants of the\n  independence complex of ternary graphs",
            "updated": "2023-11-13T20:12:46Z",
            "published": "2023-11-13T20:12:46Z",
            "summary": "In 2022, Jinha Kim proved a conjecture by Engstr\\\"om that states the\nindependence complex of a graph with no induced cycle of length divisible by 3\nis either contractible or homotopy equivalent to a sphere. We give criteria for\nwhen the independence complex of a ternary graph is contractible, and describe\nthe dimension of the sphere when it is not. We then apply our results to\ndescribe the multigraded betti numbers of the edge ideal of a ternary graph. In\nparticular, we show that the regularity and depth of edge ideals of a ternary\ngraph $G$ are equal if and only if the independence complex of $G$ is not\ncontractible. Finally, we apply our results to partially recover and generalize\nrecent results on the depth of edge ideals of some unicyclic graphs.",
            "author": [
                "Sara Faridi",
                "Thiago Holleben"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07727v1",
                "http://arxiv.org/pdf/2311.07727v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.AT",
                "math.CO",
                "13F55, 05E40, 05E45, 55U10, 55P15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07720v1",
            "title": "Sparse Regression LDPC Codes",
            "updated": "2023-11-13T20:01:36Z",
            "published": "2023-11-13T20:01:36Z",
            "summary": "This article introduces a novel concatenated coding scheme called sparse\nregression LDPC (SR-LDPC) codes. An SR-LDPC code consists of an outer\nnon-binary LDPC code and an inner sparse regression code (SPARC) whose\nrespective field size and section sizes are equal. For such codes, an efficient\ndecoding algorithm is proposed based on approximate message passing (AMP) that\ndynamically shares soft information between inner and outer decoders. This\ndynamic exchange of information is facilitated by a denoiser that runs belief\npropagation (BP) on the factor graph of the outer LDPC code within each AMP\niteration. It is shown that this denoiser falls within the class of\nnon-separable pseudo-Lipschitz denoising functions and thus that state\nevolution holds for the proposed AMP-BP algorithm. Leveraging the rich\nstructure of SR-LDPC codes, this article proposes an efficient low-dimensional\napproximate state evolution recursion that can be used for efficient\nhyperparameter tuning, thus paving the way for future work on optimal code\ndesign. Finally, numerical simulations demonstrate that SR-LDPC codes\noutperform contemporary codes over the AWGN channel for parameters of practical\ninterest. SR-LDPC codes are shown to be viable means to obtain shaping gains\nover the AWGN channel.",
            "author": [
                "Jamison R. Ebert",
                "Jean-Francois Chamberland",
                "Krishna R. Narayanan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07720v1",
                "http://arxiv.org/pdf/2311.07720v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07715v1",
            "title": "PolyIE: A Dataset of Information Extraction from Polymer Material\n  Scientific Literature",
            "updated": "2023-11-13T19:56:18Z",
            "published": "2023-11-13T19:56:18Z",
            "summary": "Scientific information extraction (SciIE), which aims to automatically\nextract information from scientific literature, is becoming more important than\never. However, there are no existing SciIE datasets for polymer materials,\nwhich is an important class of materials used ubiquitously in our daily lives.\nTo bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer\nmaterials. POLYIE is curated from 146 full-length polymer scholarly articles,\nwhich are annotated with different named entities (i.e., materials, properties,\nvalues, conditions) as well as their N-ary relations by domain experts. POLYIE\npresents several unique challenges due to diverse lexical formats of entities,\nambiguity between entities, and variable-length relations. We evaluate\nstate-of-the-art named entity extraction and relation extraction models on\nPOLYIE, analyze their strengths and weaknesses, and highlight some difficult\ncases for these models. To the best of our knowledge, POLYIE is the first SciIE\nbenchmark for polymer materials, and we hope it will lead to more research\nefforts from the community on this challenging task. Our code and data are\navailable on: https://github.com/jerry3027/PolyIE.",
            "author": [
                "Jerry Junyang Cheung",
                "Yuchen Zhuang",
                "Yinghao Li",
                "Pranav Shetty",
                "Wantian Zhao",
                "Sanjeev Grampurohit",
                "Rampi Ramprasad",
                "Chao Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07715v1",
                "http://arxiv.org/pdf/2311.07715v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07701v1",
            "title": "The process of fluctuations of the giant component of an\n  Erd\u0151s-R\u00e9nyi graph",
            "updated": "2023-11-13T19:38:46Z",
            "published": "2023-11-13T19:38:46Z",
            "summary": "We present a detailed study of the evolution of the giant component of the\nErd\\H{o}s-R\\'enyi graph process as the mean degree increases from 1 to\ninfinity. It leads to the identification of the limiting process of the\nrescaled fluctuations of its order around its deterministic asymptotic. This\nprocess is Gaussian with an explicit covariance.",
            "author": [
                "Nathana\u00ebl Enriquez",
                "Gabriel Faraud",
                "Sophie Lemaire"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07701v1",
                "http://arxiv.org/pdf/2311.07701v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO",
                "05C80 (Primary) 60F17 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07690v1",
            "title": "Correlated volumes for extended wavefunctions on a random-regular graph",
            "updated": "2023-11-13T19:15:18Z",
            "published": "2023-11-13T19:15:18Z",
            "summary": "We analyze the ergodic properties of a metallic wavefunction for the Anderson\nmodel in a disordered random-regular graph with branching number $k=2.$ A few\nq-moments $I_q$ associated with the zero energy eigenvector are numerically\ncomputed up to sizes $N=4\\times 10^6.$ We extract their corresponding fractal\ndimensions $D_q$ in the thermodynamic limit together with correlated volumes\n$N_q$ that control finite-size effects. At intermediate values of disorder $W,$\nwe obtain ergodicity $D_q=1$ for $q=1,2$ and correlation volumes that increase\nfast upon approaching the Anderson transition $\\log(\\log(N_q))\\sim W.$ We then\nfocus on the extraction of the volume $N_0$ associated with the typical value\nof the wavefunction $e^{<\\log|\\psi|^2>},$ which follows a similar tendency as\nthe ones for $N_1$ or $N_2.$ Its value at intermediate disorders is close, but\nsmaller, to the so-called ergodic volume previously found via the\nsuper-symmetric formalism and belief propagator algorithms. None of the\ncomputed correlated volumes shows a tendency to diverge up to disorders\n$W\\approx 15$, specifically none with exponent $\\nu=1/2$. Deeper in the metal,\nwe characterize the crossover to system sizes much smaller than the first\ncorrelated volume $N_1\\gg N.$ Once this crossover has taken place, we obtain\nevidence of a scaling in which the derivative of the first fractal dimension\n$D_1$ behaves critically with an exponent $\\nu=1.$",
            "author": [
                "Manuel Pino",
                "Jose E. Roman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07690v1",
                "http://arxiv.org/pdf/2311.07690v1"
            ],
            "primary_category": "cond-mat.dis-nn",
            "category": [
                "cond-mat.dis-nn",
                "quant-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07682v1",
            "title": "Fuse to Forget: Bias Reduction and Selective Memorization through Model\n  Fusion",
            "updated": "2023-11-13T19:02:56Z",
            "published": "2023-11-13T19:02:56Z",
            "summary": "Model fusion research aims to aggregate the knowledge of multiple models to\nenhance performance by combining their weights. In this work, we study the\ninverse, investigating whether and how can model fusion interfere and reduce\nunwanted knowledge. We delve into the effects of model fusion on the evolution\nof learned shortcuts, social biases, and memorization capabilities in\nfine-tuned language models. Through several experiments covering text\nclassification and generation tasks, our analysis highlights that shared\nknowledge among models is usually enhanced during model fusion, while unshared\nknowledge is usually lost or forgotten. Based on this observation, we\ndemonstrate the potential of model fusion as a debiasing tool and showcase its\nefficacy in addressing privacy concerns associated with language models.",
            "author": [
                "Kerem Zaman",
                "Leshem Choshen",
                "Shashank Srivastava"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07682v1",
                "http://arxiv.org/pdf/2311.07682v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07675v1",
            "title": "Spectral properties of random graphs with fixed equitable partition",
            "updated": "2023-11-13T19:01:40Z",
            "published": "2023-11-13T19:01:40Z",
            "summary": "We define a graph to be $S$-regular if it contains an equitable partition\ngiven by a matrix $S$. These graphs are generalizations of both regular and\nbipartite, biregular graphs. An $S$-regular matrix is defined then as a matrix\non an $S$-regular graph consistent with the graph's equitable partition. In\nthis paper we derive the limiting spectral density for large, random\n$S$-regular matrices as well as limiting functions of certain statistics for\ntheir eigenvector coordinates as a function of eigenvalue. These limiting\nfunctions are defined in terms of spectral measures on $S$-regular trees. In\ngeneral, these spectral measures do not have a closed-form expression; however,\nwe provide a defining system of polynomials for them. Finally, we explore\neigenvalue bounds of $S$-regular graph, proving an expander mixing lemma,\nAlon-Bopana bound, and other eigenvalue inequalities in terms of the\neigenvalues of the matrix $S$.",
            "author": [
                "Matthew B. Crawford",
                "David J. Marchette",
                "William Maxwell",
                "Samuel S. Mendelson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07675v1",
                "http://arxiv.org/pdf/2311.07675v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.SP",
                "05C75 (Primary) 60B20, 05C80 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07652v1",
            "title": "Safe but Incalculable: Energy-weighting is not all you need",
            "updated": "2023-11-13T19:00:02Z",
            "published": "2023-11-13T19:00:02Z",
            "summary": "Infrared and collinear (IRC) safety has long been used a proxy for robustness\nwhen developing new jet substructure observables. This guiding philosophy has\nbeen carried into the deep learning era, where IRC-safe neural networks have\nbeen used for many jet studies. For graph-based neural networks, the most\nstraightforward way to achieve IRC safety is to weight particle inputs by their\nenergies. However, energy-weighting by itself does not guarantee that\nperturbative calculations of machine-learned observables will enjoy small\nnon-perturbative corrections. In this paper, we demonstrate the sensitivity of\nIRC-safe networks to non-perturbative effects, by training an energy flow\nnetwork (EFN) to maximize its sensitivity to hadronization. We then show how to\nconstruct Lipschitz Energy Flow Networks (L-EFNs), which are both IRC safe and\nrelatively insensitive to non-perturbative corrections. We demonstrate the\nperformance of L-EFNs on generated samples of quark and gluon jets, and\nshowcase fascinating differences between the learned latent representations of\nEFNs and L-EFNs.",
            "author": [
                "Samuel Bright-Thonney",
                "Benjamin Nachman",
                "Jesse Thaler"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07652v1",
                "http://arxiv.org/pdf/2311.07652v1"
            ],
            "primary_category": "hep-ph",
            "category": [
                "hep-ph",
                "hep-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07554v1",
            "title": "Fast and Space-Efficient Parallel Algorithms for Influence Maximization",
            "updated": "2023-11-13T18:48:57Z",
            "published": "2023-11-13T18:48:57Z",
            "summary": "Influence Maximization (IM) is a crucial problem in data science. The goal is\nto find a fixed-size set of highly-influential seed vertices on a network to\nmaximize the influence spread along the edges. While IM is NP-hard on\ncommonly-used diffusion models, a greedy algorithm can achieve\n$(1-1/e)$-approximation, repeatedly selecting the vertex with the highest\nmarginal gain in influence as the seed. Due to theoretical guarantees, rich\nliterature focuses on improving the performance of the greedy algorithm. To\nestimate the marginal gain, existing work either runs Monte Carlo (MC)\nsimulations of influence spread or pre-stores hundreds of sketches (usually\nper-vertex information). However, these approaches can be inefficient in time\n(MC simulation) or space (storing sketches), preventing the ideas from scaling\nto today's large-scale graphs.\n  This paper significantly improves the scalability of IM using two key\ntechniques. The first is a sketch-compression technique for the independent\ncascading model on undirected graphs. It allows combining the simulation and\nsketching approaches to achieve a time-space tradeoff. The second technique\nincludes new data structures for parallel seed selection. Using our new\napproaches, we implemented PaC-IM: Parallel and Compressed IM.\n  We compare PaC-IM with state-of-the-art parallel IM systems on a 96-core\nmachine with 1.5TB memory. PaC-IM can process large-scale graphs with up to\n900M vertices and 74B edges in about 2 hours. On average across all tested\ngraphs, our uncompressed version is 5--18$\\times$ faster and about 1.4$\\times$\nmore space-efficient than existing parallel IM systems. Using compression\nfurther saves 3.8$\\times$ space with only 70% overhead in time on average.",
            "author": [
                "Letong Wang",
                "Xiangyun Ding",
                "Yan Gu",
                "Yihan Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07554v1",
                "http://arxiv.org/pdf/2311.07554v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07548v1",
            "title": "Interpretable Fine-Tuning for Graph Neural Network Surrogate Models",
            "updated": "2023-11-13T18:37:07Z",
            "published": "2023-11-13T18:37:07Z",
            "summary": "Data-based surrogate modeling has surged in capability in recent years with\nthe emergence of graph neural networks (GNNs), which can operate directly on\nmesh-based representations of data. The goal of this work is to introduce an\ninterpretable fine-tuning strategy for GNNs, with application to unstructured\nmesh-based fluid dynamics modeling. The end result is a fine-tuned GNN that\nadds interpretability to a pre-trained baseline GNN through an adaptive\nsub-graph sampling strategy that isolates regions in physical space\nintrinsically linked to the forecasting task, while retaining the predictive\ncapability of the baseline. The structures identified by the fine-tuned GNNs,\nwhich are adaptively produced in the forward pass as explicit functions of the\ninput, serve as an accessible link between the baseline model architecture, the\noptimization goal, and known problem-specific physics. Additionally, through a\nregularization procedure, the fine-tuned GNNs can also be used to identify,\nduring inference, graph nodes that correspond to a majority of the anticipated\nforecasting error, adding a novel interpretable error-tagging capability to\nbaseline models. Demonstrations are performed using unstructured flow data\nsourced from flow over a backward-facing step at high Reynolds numbers.",
            "author": [
                "Shivam Barwey",
                "Romit Maulik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07548v1",
                "http://arxiv.org/pdf/2311.07548v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "physics.comp-ph",
                "physics.flu-dyn"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07547v1",
            "title": "GPT-4V(ision) as A Social Media Analysis Engine",
            "updated": "2023-11-13T18:36:50Z",
            "published": "2023-11-13T18:36:50Z",
            "summary": "Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.",
            "author": [
                "Hanjia Lyu",
                "Jinfa Huang",
                "Daoan Zhang",
                "Yongsheng Yu",
                "Xinyi Mou",
                "Jinsheng Pan",
                "Zhengyuan Yang",
                "Zhongyu Wei",
                "Jiebo Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07547v1",
                "http://arxiv.org/pdf/2311.07547v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07540v1",
            "title": "Finding planted cliques using Markov chain Monte Carlo",
            "updated": "2023-11-13T18:31:42Z",
            "published": "2023-11-13T18:31:42Z",
            "summary": "The planted clique problem is a paradigmatic model of\nstatistical-to-computational gaps: the planted clique is\ninformation-theoretically detectable if its size $k\\ge 2\\log_2 n$ but\npolynomial-time algorithms only exist for the recovery task when $k=\n\\Omega(\\sqrt{n})$. By now, there are many simple and fast algorithms that\nsucceed as soon as $k = \\Omega(\\sqrt{n})$. Glaringly, however, no MCMC approach\nto the problem had been shown to work, including the Metropolis process on\ncliques studied by Jerrum since 1992. In fact, Chen, Mossel, and Zadik recently\nshowed that any Metropolis process whose state space is the set of cliques\nfails to find any sub-linear sized planted clique in polynomial time if\ninitialized naturally from the empty set. Here, we redeem MCMC performance for\nthe planted clique problem by relaxing the state space to all vertex subsets\nand adding a corresponding energy penalty for missing edges. With that, we\nprove that energy-minimizing Markov chains (gradient descent and a\nlow-temperature relaxation of it) succeed at recovering planted cliques of size\n$k = \\Omega(\\sqrt{n})$ if initialized from the full graph. Importantly,\ninitialized from the empty set, the relaxation still does not help the gradient\ndescent find sub-linear planted cliques. We also demonstrate robustness of\nthese Markov chain approaches under a natural contamination model.",
            "author": [
                "Reza Gheissari",
                "Aukosh Jagannath",
                "Yiming Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07540v1",
                "http://arxiv.org/pdf/2311.07540v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "math.PR",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10757v1",
            "title": "How Contentious Terms About People and Cultures are Used in Linked Open\n  Data",
            "updated": "2023-11-13T18:25:20Z",
            "published": "2023-11-13T18:25:20Z",
            "summary": "Web resources in linked open data (LOD) are comprehensible to humans through\nliteral textual values attached to them, such as labels, notes, or comments.\nWord choices in literals may not always be neutral. When outdated and\nculturally stereotyping terminology is used in literals, they may appear as\noffensive to users in interfaces and propagate stereotypes to algorithms\ntrained on them. We study how frequently and in which literals contentious\nterms about people and cultures occur in LOD and whether there are attempts to\nmark the usage of such terms. For our analysis, we reuse English and Dutch\nterms from a knowledge graph that provides opinions of experts from the\ncultural heritage domain about terms' contentiousness. We inspect occurrences\nof these terms in four widely used datasets: Wikidata, The Getty Art &\nArchitecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms\nare ambiguous and contentious only in particular senses. Applying word sense\ndisambiguation, we generate a set of literals relevant to our analysis. We\nfound that outdated, derogatory, stereotyping terms frequently appear in\ndescriptive and labelling literals, such as preferred labels that are usually\ndisplayed in interfaces and used for indexing. In some cases, LOD contributors\nmark contentious terms with words and phrases in literals (implicit markers) or\nproperties linked to resources (explicit markers). However, such marking is\nrare and non-consistent in all datasets. Our quantitative and qualitative\ninsights could be helpful in developing more systematic approaches to address\nthe propagation of stereotypes via LOD.",
            "author": [
                "Andrei Nesterov",
                "Laura Hollink",
                "Jacco van Ossenbruggen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10757v1",
                "http://arxiv.org/pdf/2311.10757v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07536v1",
            "title": "A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual\n  Question Answering",
            "updated": "2023-11-13T18:22:32Z",
            "published": "2023-11-13T18:22:32Z",
            "summary": "The emergence of multimodal large models (MLMs) has significantly advanced\nthe field of visual understanding, offering remarkable capabilities in the\nrealm of visual question answering (VQA). Yet, the true challenge lies in the\ndomain of knowledge-intensive VQA tasks, which necessitate not just recognition\nof visual elements, but also a deep comprehension of the visual information in\nconjunction with a vast repository of learned knowledge. To uncover such\ncapabilities of MLMs, particularly the newly introduced GPT-4V, we provide an\nin-depth evaluation from three perspectives: 1) Commonsense Knowledge, which\nassesses how well models can understand visual cues and connect to general\nknowledge; 2) Fine-grained World Knowledge, which tests the model's skill in\nreasoning out specific knowledge from images, showcasing their proficiency\nacross various specialized fields; 3) Comprehensive Knowledge with\nDecision-making Rationales, which examines model's capability to provide\nlogical explanations for its inference, facilitating a deeper analysis from the\ninterpretability perspective. Extensive experiments indicate that GPT-4V\nachieves SOTA performance on above three tasks. Interestingly, we find that: a)\nGPT-4V demonstrates enhanced reasoning and explanation when using composite\nimages as few-shot; b) GPT-4V produces severe hallucinations when dealing with\nworld knowledge, highlighting the future need for advancements in this research\ndirection.",
            "author": [
                "Yunxin Li",
                "Longyue Wang",
                "Baotian Hu",
                "Xinyu Chen",
                "Wanqi Zhong",
                "Chenyang Lyu",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07536v1",
                "http://arxiv.org/pdf/2311.07536v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07521v1",
            "title": "Sticky Brownian motions on star graphs",
            "updated": "2023-11-13T18:00:48Z",
            "published": "2023-11-13T18:00:48Z",
            "summary": "This paper is concerned with the construction of several stochastic processes\nin a star graph, that is a non-euclidean structure where some features of the\nclassical modelling fail. We propose a model for trapping phenomena with\ncharacterization of the traps in terms of a singular measure. This measure also\ndefines a non-local operator by means of which we introduce a non-local dynamic\ncondition for the parabolic problem on the star graph. We study semi-Markov\nprocesses on the rays of the graph in order to obtain a probabilistic\nrepresentation of the motion on the whole graph. Extensions to general graph\nstructures can be given by applying our results on star graphs.",
            "author": [
                "Stefano Bonaccorsi",
                "Mirko D'Ovidio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07521v1",
                "http://arxiv.org/pdf/2311.07521v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07514v1",
            "title": "VGSG: Vision-Guided Semantic-Group Network for Text-based Person Search",
            "updated": "2023-11-13T17:56:54Z",
            "published": "2023-11-13T17:56:54Z",
            "summary": "Text-based Person Search (TBPS) aims to retrieve images of target pedestrian\nindicated by textual descriptions. It is essential for TBPS to extract\nfine-grained local features and align them crossing modality. Existing methods\nutilize external tools or heavy cross-modal interaction to achieve explicit\nalignment of cross-modal fine-grained features, which is inefficient and\ntime-consuming. In this work, we propose a Vision-Guided Semantic-Group Network\n(VGSG) for text-based person search to extract well-aligned fine-grained visual\nand textual features. In the proposed VGSG, we develop a Semantic-Group Textual\nLearning (SGTL) module and a Vision-guided Knowledge Transfer (VGKT) module to\nextract textual local features under the guidance of visual local clues. In\nSGTL, in order to obtain the local textual representation, we group textual\nfeatures from the channel dimension based on the semantic cues of language\nexpression, which encourages similar semantic patterns to be grouped implicitly\nwithout external tools. In VGKT, a vision-guided attention is employed to\nextract visual-related textual features, which are inherently aligned with\nvisual cues and termed vision-guided textual features. Furthermore, we design a\nrelational knowledge transfer, including a vision-language similarity transfer\nand a class probability transfer, to adaptively propagate information of the\nvision-guided textual features to semantic-group textual features. With the\nhelp of relational knowledge transfer, VGKT is capable of aligning\nsemantic-group textual features with corresponding visual features without\nexternal tools and complex pairwise interaction. Experimental results on two\nchallenging benchmarks demonstrate its superiority over state-of-the-art\nmethods.",
            "author": [
                "Shuting He",
                "Hao Luo",
                "Wei Jiang",
                "Xudong Jiang",
                "Henghui Ding"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07514v1",
                "http://arxiv.org/pdf/2311.07514v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07509v1",
            "title": "A Benchmark to Understand the Role of Knowledge Graphs on Large Language\n  Model's Accuracy for Question Answering on Enterprise SQL Databases",
            "updated": "2023-11-13T17:54:50Z",
            "published": "2023-11-13T17:54:50Z",
            "summary": "Enterprise applications of Large Language Models (LLMs) hold promise for\nquestion answering on enterprise SQL databases. However, the extent to which\nLLMs can accurately respond to enterprise questions in such databases remains\nunclear, given the absence of suitable Text-to-SQL benchmarks tailored to\nenterprise settings. Additionally, the potential of Knowledge Graphs (KGs) to\nenhance LLM-based question answering by providing business context is not well\nunderstood. This study aims to evaluate the accuracy of LLM-powered question\nanswering systems in the context of enterprise questions and SQL databases,\nwhile also exploring the role of knowledge graphs in improving accuracy. To\nachieve this, we introduce a benchmark comprising an enterprise SQL schema in\nthe insurance domain, a range of enterprise queries encompassing reporting to\nmetrics, and a contextual layer incorporating an ontology and mappings that\ndefine a knowledge graph. Our primary finding reveals that question answering\nusing GPT-4, with zero-shot prompts directly on SQL databases, achieves an\naccuracy of 16%. Notably, this accuracy increases to 54% when questions are\nposed over a Knowledge Graph representation of the enterprise SQL database.\nTherefore, investing in Knowledge Graph provides higher accuracy for LLM\npowered question answering systems.",
            "author": [
                "Juan Sequeda",
                "Dean Allemang",
                "Bryon Jacob"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07509v1",
                "http://arxiv.org/pdf/2311.07509v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.CL",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07507v2",
            "title": "Scalar curvature for metric spaces: Defining curvature for Quantum\n  Gravity without coordinates",
            "updated": "2023-11-14T14:43:31Z",
            "published": "2023-11-13T17:53:26Z",
            "summary": "Geometrical properties of spacetime are difficult to study in nonperturbative\napproaches to quantum gravity like Causal Dynamical Triangulations (CDT), where\none uses simplicial manifolds to define the gravitational path integral,\ninstead of Riemannian manifolds. In particular, in CDT one only relies on two\nmathematical tools, a distance measure and a volume measure. In this paper, we\ndefine a notion of scalar curvature, for metric spaces endowed with a volume\nmeasure or a random walk, without assuming nor using notions of tensor\ncalculus. Furthermore, we directly define the Ricci scalar, without the need of\ndefining and computing the Riemann or the Ricci tensor a priori. For this, we\nmake use of quantities, like the surface of a geodesic sphere, or the return\nprobability of scalar diffusion processes, that can be computed in these metric\nspaces, as in a Riemannian manifold, where they receive scalar curvature\ncontributions. Our definitions recover the classical results of scalar\ncurvature when the sets are Riemannian manifolds. We propose seven methods to\ncompute the scalar curvature in these spaces, and we compare their features in\nnatural implementations in discrete spaces. The defined generalized scalar\ncurvatures are easily implemented on discrete spaces, like graphs. We present\nthe results of our definitions on random triangulations of a 2D sphere and\nplane. Additionally, we show the results of our generalized scalar curvatures\non the quantum geometries of 2D CDT, where we find that all our definitions\nindicate a flat ground state of the gravitational path integral.",
            "author": [
                "Agust\u00edn Silva",
                "Jesse van der Duin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07507v2",
                "http://arxiv.org/pdf/2311.07507v2"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07503v1",
            "title": "Planar graphs deformations of bordered knot algebras",
            "updated": "2023-11-13T17:44:35Z",
            "published": "2023-11-13T17:44:35Z",
            "summary": "In an earlier paper, we introduced ``bordered knot algebras'', which\n  are graded algebras indexed by a pair of integers (m,k). In a\n  subsequent paper, we introduced a two-parameter family of\n  differential graded algebra, the ``pong algebras'', and identified\n  their homology with the bordered knot algebras, and characterized\n  the induced A-infinity structure on the homology. The aim of the\n  present paper is to give an explicit, combinatorial model for this\n  A-infinity structure on the bordered knot algebras, and a further\n  weighted deformation of this structure, in the case where k=1.",
            "author": [
                "Peter Ozsvath",
                "Zoltan Szabo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07503v1",
                "http://arxiv.org/pdf/2311.07503v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "math.SG",
                "57M25, 57R58"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07495v1",
            "title": "The Last Decade in Review: Tracing the Evolution of Safety Assurance\n  Cases through a Comprehensive Bibliometric Analysis",
            "updated": "2023-11-13T17:34:23Z",
            "published": "2023-11-13T17:34:23Z",
            "summary": "Safety assurance is of paramount importance across various domains, including\nautomotive, aerospace, and nuclear energy, where the reliability and\nacceptability of mission-critical systems are imperative. This assurance is\neffectively realized through the utilization of Safety Assurance Cases. The use\nof safety assurance cases allows for verifying the correctness of the created\nsystems capabilities, preventing system failure. The latter may result in loss\nof life, severe injuries, large-scale environmental damage, property\ndestruction, and major economic loss. Still, the emergence of complex\ntechnologies such as cyber-physical systems (CPSs), characterized by their\nheterogeneity, autonomy, machine learning capabilities, and the uncertainty of\ntheir operational environments poses significant challenges for safety\nassurance activities. Several papers have tried to propose solutions to tackle\nthese challenges, but to the best of our knowledge, no secondary study\ninvestigates the trends, patterns, and relationships characterizing the safety\ncase scientific literature. This makes it difficult to have a holistic view of\nthe safety case landscape and to identify the most promising future research\ndirections. In this paper, we, therefore, rely on state-of-the-art bibliometric\ntools(e.g., VosViewer) to conduct a bibliometric analysis that allows us to\ngenerate valuable insights, identify key authors and venues, and gain a birds\neye view of the current state of research in the safety assurance area. By\nrevealing knowledge gaps and highlighting potential avenues for future\nresearch, our analysis provides an essential foundation for researchers,\ncorporate safety analysts, and regulators seeking to embrace or enhance safety\npractices that align with their specific needs and objectives.",
            "author": [
                "Mithila Sivakumar",
                "Alvine Boaye Belle",
                "Jinjun Shan",
                "Opeyemi Adesina",
                "Song Wang",
                "Marsha Chechik",
                "Marios Fokaefs",
                "Kimya Khakzad Shahandashti",
                "Oluwafemi Odu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07495v1",
                "http://arxiv.org/pdf/2311.07495v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07494v1",
            "title": "Biodiversity analysis of metaproteomics samples with Unipept: a\n  comprehensive tutorial",
            "updated": "2023-11-13T17:31:06Z",
            "published": "2023-11-13T17:31:06Z",
            "summary": "Metaproteomics has become a crucial omics technology for studying\nmicrobiomes. In this area, the Unipept ecosystem, accessible at\nhttps://unipept.ugent.be, has emerged as an invaluable resource for analyzing\nmetaproteomic data. It offers in-depth insights into both taxonomic\ndistributions and functional characteristics of complex ecosystems. This\ntutorial explains essential concepts like Lowest Common Ancestor (LCA)\ndetermination and the handling of peptides with missed cleavages. It also\nprovides a detailed, step-by-step guide on using the Unipept Web application\nand Unipept Desktop for thorough metaproteomics analyses. By integrating\ntheoretical principles with practical methodologies, this tutorial empowers\nresearchers with the essential knowledge and tools needed to fully utilize\nmetaproteomics in their microbiome studies.",
            "author": [
                "Tim Van Den Bossche",
                "Pieter Verschaffelt",
                "Tibo Vande Moortele",
                "Peter Dawyndt",
                "Lennart Martens",
                "Bart Mesuere"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07494v1",
                "http://arxiv.org/pdf/2311.07494v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07491v1",
            "title": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question\n  Decomposition with Large Language Models",
            "updated": "2023-11-13T17:28:03Z",
            "published": "2023-11-13T17:28:03Z",
            "summary": "While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.",
            "author": [
                "Hejing Cao",
                "Zhenwei An",
                "Jiazhan Feng",
                "Kun Xu",
                "Liwei Chen",
                "Dongyan Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07491v1",
                "http://arxiv.org/pdf/2311.07491v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07479v1",
            "title": "Towards Robotic Tree Manipulation: Leveraging Graph Representations",
            "updated": "2023-11-13T17:13:13Z",
            "published": "2023-11-13T17:13:13Z",
            "summary": "There is growing interest in automating agricultural tasks that require\nintricate and precise interaction with specialty crops, such as trees and\nvines. However, developing robotic solutions for crop manipulation remains a\ndifficult challenge due to complexities involved in modeling their deformable\nbehavior. In this study, we present a framework for learning the deformation\nbehavior of tree-like crops under contact interaction. Our proposed method\ninvolves encoding the state of a spring-damper modeled tree crop as a graph.\nThis representation allows us to employ graph networks to learn both a forward\nmodel for predicting resulting deformations, and a contact policy for inferring\nactions to manipulate tree crops. We conduct a comprehensive set of experiments\nin a simulated environment and demonstrate generalizability of our method on\npreviously unseen trees. Videos can be found on the project website:\nhttps://kantor-lab.github.io/tree_gnn",
            "author": [
                "Chung Hee Kim",
                "Moonyoung Lee",
                "Oliver Kroemer",
                "George Kantor"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07479v1",
                "http://arxiv.org/pdf/2311.07479v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07472v2",
            "title": "Invariance principle and local limit theorem for a class of random\n  conductance models with long-range jumps",
            "updated": "2023-11-20T15:34:54Z",
            "published": "2023-11-13T17:07:23Z",
            "summary": "We study continuous time random walks on $\\mathbb{Z}^d$ (with $d \\geq 2$)\namong random conductances $\\{ \\omega(\\{x,y\\}) : x,y \\in \\mathbb{Z}^d\\}$ that\npermit jumps of arbitrary length. The law of the random variables\n$\\omega(\\{x,y\\})$, taking values in $[0, \\infty)$, is assumed to be stationary\nand ergodic with respect to space shifts. Assuming that the first moment of\n$\\sum_{x \\in \\mathbb{Z}^d} \\omega(\\{0,x\\}) |x|^2$ and the $q$-th moment of\n$1/\\omega(0,x)$ for $x$ neighbouring the origin are finite for some $ q >d/2$,\nwe show a quenched invariance principle and a quenched local limit theorem,\nwhere the moment condition is optimal for the latter. We also obtain H\\\"older\nregularity estimates for solutions of the heat equation for the associated\nnon-local discrete operator, and deduce that the pointwise spectral dimension\nequals $d$ almost surely. Our results apply to random walks on long-range\npercolation graphs with connectivity exponents larger than $d+2$ when all\nnearest-neighbour edges are present.",
            "author": [
                "Sebastian Andres",
                "Martin Slowik"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07472v2",
                "http://arxiv.org/pdf/2311.07472v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.AP",
                "60K37, 60F17, 82C41, 82B43"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07470v1",
            "title": "Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer",
            "updated": "2023-11-13T17:03:02Z",
            "published": "2023-11-13T17:03:02Z",
            "summary": "Multi-modal large language models (LLM) have achieved powerful capabilities\nfor visual semantic understanding in recent years. However, little is known\nabout how LLMs comprehend visual information and interpret different modalities\nof features. In this paper, we propose a new method for identifying multi-modal\nneurons in transformer-based multi-modal LLMs. Through a series of experiments,\nWe highlight three critical properties of multi-modal neurons by four\nwell-designed quantitative evaluation metrics. Furthermore, we introduce a\nknowledge editing method based on the identified multi-modal neurons, for\nmodifying a specific token to another designative token. We hope our findings\ncan inspire further explanatory researches on understanding mechanisms of\nmulti-modal LLMs.",
            "author": [
                "Haowen Pan",
                "Yixin Cao",
                "Xiaozhi Wang",
                "Xun Yang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07470v1",
                "http://arxiv.org/pdf/2311.07470v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07468v2",
            "title": "Are We Falling in a Middle-Intelligence Trap? An Analysis and Mitigation\n  of the Reversal Curse",
            "updated": "2023-11-16T08:35:05Z",
            "published": "2023-11-13T17:01:12Z",
            "summary": "Recent studies have highlighted a phenomenon in large language models (LLMs)\nknown as \"the reversal curse,\" in which the order of knowledge entities in the\ntraining data biases the models' comprehension. For example, if a model is\ntrained on sentences where entity A consistently appears before entity B, it\ncan respond to queries about A by providing B as the answer. However, it may\nencounter confusion when presented with questions concerning B. We contend that\nthe reversal curse is partially a result of specific model training objectives,\nparticularly evident in the prevalent use of the next-token prediction within\nmost causal language models. For the next-token prediction, models solely focus\non a token's preceding context, resulting in a restricted comprehension of the\ninput. In contrast, we illustrate that the GLM, trained using the\nautoregressive blank infilling objective where tokens to be predicted have\naccess to the entire context, exhibits better resilience against the reversal\ncurse. We propose a novel training method, BIdirectional Casual language\nmodeling Optimization (BICO), designed to mitigate the reversal curse when\nfine-tuning pretrained causal language models on new data. BICO modifies the\ncausal attention mechanism to function bidirectionally and employs a mask\ndenoising optimization. In the task designed to assess the reversal curse, our\napproach improves Llama's accuracy from the original 0% to around 70%. We hope\nthat more attention can be focused on exploring and addressing these inherent\nweaknesses of the current LLMs, in order to achieve a higher level of\nintelligence.",
            "author": [
                "Ang Lv",
                "Kaiyi Zhang",
                "Shufang Xie",
                "Quan Tu",
                "Yuhan Chen",
                "Ji-Rong Wen",
                "Rui Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07468v2",
                "http://arxiv.org/pdf/2311.07468v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07460v1",
            "title": "KnowSafe: Combined Knowledge and Data Driven Hazard Mitigation in\n  Artificial Pancreas Systems",
            "updated": "2023-11-13T16:43:34Z",
            "published": "2023-11-13T16:43:34Z",
            "summary": "Significant progress has been made in anomaly detection and run-time\nmonitoring to improve the safety and security of cyber-physical systems (CPS).\nHowever, less attention has been paid to hazard mitigation. This paper proposes\na combined knowledge and data driven approach, KnowSafe, for the design of\nsafety engines that can predict and mitigate safety hazards resulting from\nsafety-critical malicious attacks or accidental faults targeting a CPS\ncontroller. We integrate domain-specific knowledge of safety constraints and\ncontext-specific mitigation actions with machine learning (ML) techniques to\nestimate system trajectories in the far and near future, infer potential\nhazards, and generate optimal corrective actions to keep the system safe.\nExperimental evaluation on two realistic closed-loop testbeds for artificial\npancreas systems (APS) and a real-world clinical trial dataset for diabetes\ntreatment demonstrates that KnowSafe outperforms the state-of-the-art by\nachieving higher accuracy in predicting system state trajectories and potential\nhazards, a low false positive rate, and no false negatives. It also maintains\nthe safe operation of the simulated APS despite faults or attacks without\nintroducing any new hazards, with a hazard mitigation success rate of 92.8%,\nwhich is at least 76% higher than solely rule-based (50.9%) and data-driven\n(52.7%) methods.",
            "author": [
                "Xugui Zhou",
                "Maxfield Kouzel",
                "Chloe Smith",
                "Homa Alemzadeh"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07460v1",
                "http://arxiv.org/pdf/2311.07460v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07454v1",
            "title": "Causal Discovery under Latent Class Confounding",
            "updated": "2023-11-13T16:35:34Z",
            "published": "2023-11-13T16:35:34Z",
            "summary": "Directed acyclic graphs are used to model the causal structure of a system.\n``Causal discovery'' describes the problem of learning this structure from\ndata. When data is an aggregate from multiple sources (populations or\nenvironments), global confounding obscures conditional independence properties\nthat drive many causal discovery algorithms. For this reason, existing causal\ndiscovery algorithms are not suitable for the multiple-source setting. We\ndemonstrate that, if the confounding is of bounded cardinality (i.e. the data\ncomes from a limited number of sources), causal discovery can still be\nachieved. The feasibility of this problem is governed by a trade-off between\nthe cardinality of the global confounder, the cardinalities of the observed\nvariables, and the sparsity of the causal structure.",
            "author": [
                "Bijan Mazaheri",
                "Spencer Gordon",
                "Yuval Rabani",
                "Leonard Schulman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07454v1",
                "http://arxiv.org/pdf/2311.07454v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CC",
                "math.ST",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.14707v1",
            "title": "Knowledge Tracing Challenge: Optimal Activity Sequencing for Students",
            "updated": "2023-11-13T16:28:34Z",
            "published": "2023-11-13T16:28:34Z",
            "summary": "Knowledge tracing is a method used in education to assess and track the\nacquisition of knowledge by individual learners. It involves using a variety of\ntechniques, such as quizzes, tests, and other forms of assessment, to determine\nwhat a learner knows and does not know about a particular subject. The goal of\nknowledge tracing is to identify gaps in understanding and provide targeted\ninstruction to help learners improve their understanding and retention of\nmaterial. This can be particularly useful in situations where learners are\nworking at their own pace, such as in online learning environments. By\nproviding regular feedback and adjusting instruction based on individual needs,\nknowledge tracing can help learners make more efficient progress and achieve\nbetter outcomes. Effectively solving the KT problem would unlock the potential\nof computer-aided education applications such as intelligent tutoring systems,\ncurriculum learning, and learning materials recommendations. In this paper, we\nwill present the results of the implementation of two Knowledge Tracing\nalgorithms on a newly released dataset as part of the AAAI2023 Global Knowledge\nTracing Challenge.",
            "author": [
                "Yann Hicke"
            ],
            "link": [
                "http://arxiv.org/abs/2311.14707v1",
                "http://arxiv.org/pdf/2311.14707v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07434v1",
            "title": "Understanding Users' Dissatisfaction with ChatGPT Responses: Types,\n  Resolving Tactics, and the Effect of Knowledge Level",
            "updated": "2023-11-13T16:08:16Z",
            "published": "2023-11-13T16:08:16Z",
            "summary": "Large language models (LLMs) with chat-based capabilities, such as ChatGPT,\nare widely used in various workflows. However, due to a limited understanding\nof these large-scale models, users struggle to use this technology and\nexperience different kinds of dissatisfaction. Researchers have introduced\nseveral methods such as prompt engineering to improve model responses. However,\nthey focus on crafting one prompt, and little has been investigated on how to\ndeal with the dissatisfaction the user encountered during the conversation.\nTherefore, with ChatGPT as the case study, we examine end users'\ndissatisfaction along with their strategies to address the dissatisfaction.\nAfter organizing users' dissatisfaction with LLM into seven categories based on\na literature review, we collected 511 instances of dissatisfactory ChatGPT\nresponses from 107 users and their detailed recollections of dissatisfied\nexperiences, which we release as a publicly accessible dataset. Our analysis\nreveals that users most frequently experience dissatisfaction when ChatGPT\nfails to grasp their intentions, while they rate the severity of\ndissatisfaction the highest with dissatisfaction related to accuracy. We also\nidentified four tactics users employ to address their dissatisfaction and their\neffectiveness. We found that users often do not use any tactics to address\ntheir dissatisfaction, and even when using tactics, 72% of dissatisfaction\nremained unresolved. Moreover, we found that users with low knowledge regarding\nLLMs tend to face more dissatisfaction on accuracy while they often put minimal\neffort in addressing dissatisfaction. Based on these findings, we propose\ndesign implications for minimizing user dissatisfaction and enhancing the\nusability of chat-based LLM services.",
            "author": [
                "Yoonsu Kim",
                "Jueon Lee",
                "Seoyoung Kim",
                "Jaehyuk Park",
                "Juho Kim"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07434v1",
                "http://arxiv.org/pdf/2311.07434v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07430v1",
            "title": "Controlled Text Generation for Black-box Language Models via Score-based\n  Progressive Editor",
            "updated": "2023-11-13T16:03:23Z",
            "published": "2023-11-13T16:03:23Z",
            "summary": "Despite recent progress in language models, generating constrained text for\nspecific domains remains a challenge, particularly when utilizing black-box\nmodels that lack domain-specific knowledge. In this paper, we introduce ScoPE\n(Score-based Progressive Editor) generation, a novel approach for controlled\ntext generation for black-box language models. We employ ScoPE to facilitate\ntext generation in the target domain by integrating it with language models\nthrough a cascading approach. Trained to enhance the target domain score of the\nedited text, ScoPE progressively edits intermediate output discrete tokens to\nalign with the target attributes throughout the auto-regressive generation\nprocess of the language model. This iterative process guides subsequent steps\nto produce desired output texts for the target domain. Our experimental results\non diverse controlled generations demonstrate that ScoPE effectively\nfacilitates controlled text generation for black-box language models in both\nin-domain and out-of-domain conditions, which is challenging for existing\nmethods.",
            "author": [
                "Sangwon Yu",
                "Changmin Lee",
                "Hojin Lee",
                "Sungroh Yoon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07430v1",
                "http://arxiv.org/pdf/2311.07430v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07418v1",
            "title": "Speech-based Slot Filling using Large Language Models",
            "updated": "2023-11-13T15:54:30Z",
            "published": "2023-11-13T15:54:30Z",
            "summary": "Recently, advancements in large language models (LLMs) have shown an\nunprecedented ability across various language tasks. This paper investigates\nthe potential application of LLMs to slot filling with noisy ASR\ntranscriptions, via both in-context learning and task-specific fine-tuning.\nDedicated prompt designs and fine-tuning approaches are proposed to improve the\nrobustness of LLMs for slot filling with noisy ASR transcriptions. Moreover, a\nlinearised knowledge injection (LKI) scheme is also proposed to integrate\ndynamic external knowledge into LLMs. Experiments were performed on SLURP to\nquantify the performance of LLMs, including GPT-3.5-turbo, GPT-4, LLaMA-13B and\nVicuna-13B (v1.1 and v1.5) with different ASR error rates. The use of the\nproposed fine-tuning together with the LKI scheme for LLaMA-13B achieved an\n8.3% absolute SLU-F1 improvement compared to the strong Flan-T5-base baseline\nsystem on a limited data setup.",
            "author": [
                "Guangzhi Sun",
                "Shutong Feng",
                "Dongcheng Jiang",
                "Chao Zhang",
                "Milica Ga\u0161i\u0107",
                "Philip C. Woodland"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07418v1",
                "http://arxiv.org/pdf/2311.07418v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07416v1",
            "title": "Three-dimensional granular flow simulation using graph neural\n  network-based learned simulator",
            "updated": "2023-11-13T15:54:09Z",
            "published": "2023-11-13T15:54:09Z",
            "summary": "Reliable evaluations of geotechnical hazards like landslides and debris flow\nrequire accurate simulation of granular flow dynamics. Traditional numerical\nmethods can simulate the complex behaviors of such flows that involve\nsolid-like to fluid-like transitions, but they are computationally intractable\nwhen simulating large-scale systems. Surrogate models based on statistical or\nmachine learning methods are a viable alternative, but they are typically\nempirical and rely on a confined set of parameters in evaluating associated\nrisks. Due to their permutation-dependent learning, conventional machine\nlearning models require an unreasonably large amount of training data for\nbuilding generalizable surrogate models. We employ a graph neural network\n(GNN), a novel deep learning technique, to develop a GNN-based simulator (GNS)\nfor granular flows to address these issues. Graphs represent the state of\ngranular flows and interactions, like the exchange of energy and momentum\nbetween grains, and GNN learns the local interaction law. GNS takes the current\nstate of the granular flow and estimates the next state using Euler explicit\nintegration. We train GNS on a limited set of granular flow trajectories and\nevaluate its performance in a three-dimensional granular column collapse\ndomain. GNS successfully reproduces the overall behaviors of column collapses\nwith various aspect ratios that were not encountered during training. The\ncomputation speed of GNS outperforms high-fidelity numerical simulators by 300\ntimes.",
            "author": [
                "Yongjin Choi",
                "Krishna Kumar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07416v1",
                "http://arxiv.org/pdf/2311.07416v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph",
                "cs.LG",
                "I.6.8"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07386v1",
            "title": "Modelling scientists' migration in the geometric knowledge space:\n  Tracing hotspots or Seeking Opportunity?",
            "updated": "2023-11-13T15:11:12Z",
            "published": "2023-11-13T15:11:12Z",
            "summary": "To understand the activity patterns of scientists within the knowledge space,\nthis study investigates the patterns and dynamic mechanisms of topic-transition\nbehavior among scientists. The selection of research topics by scientists can\nbe seen as an exploration behavior of cognitively limited individuals within a\ncomplex cognitive landscape, influenced by both individual volition and social\nfactors. Based on the constructed geometric knowledge space, this study depicts\nthe trajectories of topic-transiting formed by scientists within the knowledge\nspace and measures the flow trajectories and distance of research regions in\ndifferent geometric sub-spaces. At the collective level, it reveals a\nconservative pattern of topic transition among most scientists in local\nknowledge spaces. Furthermore, through simulation modeling analysis, it is\nfound that the core factor facilitating scientists' topic transition is the\nresearch intensity formed by the number of individuals within a region, while\nthe barrier to topic transition is constituted by the knowledge distance\nbetween fields. Although research opportunities for breakthrough scientific\ndevelopments may exist between subfields, empirical evidence indicates that\nthis is not a significant factor attracting scientists to engage in topic\ntransition.",
            "author": [
                "Feifan Liu",
                "Shuang Zhang",
                "Haoxiang Xia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07386v1",
                "http://arxiv.org/pdf/2311.07386v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07372v1",
            "title": "Multidimensional Electrical Networks and their Application to\n  Exponential Speedups for Graph Problems",
            "updated": "2023-11-13T14:39:55Z",
            "published": "2023-11-13T14:39:55Z",
            "summary": "Recently, Apers and Piddock [TQC '23] strengthened the natural connection\nbetween quantum walks and electrical networks by considering Kirchhoff's Law\nand Ohm's Law. In this work, we develop the multidimensional electrical network\nby defining Kirchhoff's Alternative Law and Ohm's Alternative Law based on the\nnovel multidimensional quantum walk framework by Jeffery and Zur [STOC '23].\nThis multidimensional electrical network allows us to sample from the\nelectrical flow obtained via a multidimensional quantum walk algorithm and\nachieve exponential quantum-classical separations for certain graph problems.\n  We first use this framework to find a marked vertex in one-dimensional random\nhierarchical graphs as defined by Balasubramanian, Li, and Harrow [arXiv '23].\nIn this work, they generalised the well known exponential quantum-classical\nseparation of the welded tree problem by Childs, Cleve, Deotto, Farhi, Gutmann,\nand Spielman [STOC '03] to random hierarchical graphs. Our result partially\nrecovers their results with an arguably simpler analysis. Furthermore, by\nconstructing a $3$-regular graph based on welded trees, this framework also\nallows us to show an exponential speedup for the pathfinding problem. This\nsolves one of the open problems by Li [arXiv '23], where they construct a\nnon-regular graph and use the degree information to achieve a similar speedup.\n  In analogy to the connection between the (edge-vertex) incidence matrix of a\ngraph and Kirchhoff's Law and Ohm's Law in an electrical network, we also\nrebuild the connection between the alternative incidence matrix and Kirchhoff's\nAlternative Law and Ohm's Alternative Law. By establishing this connection, we\nexpect that the multidimensional electrical network could have more\napplications beyond quantum walks.",
            "author": [
                "Jianqiang Li",
                "Sebastian Zur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07372v1",
                "http://arxiv.org/pdf/2311.07372v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07370v2",
            "title": "Classification of developmental and brain disorders via graph\n  convolutional aggregation",
            "updated": "2023-11-16T14:55:15Z",
            "published": "2023-11-13T14:36:29Z",
            "summary": "While graph convolution based methods have become the de-facto standard for\ngraph representation learning, their applications to disease prediction tasks\nremain quite limited, particularly in the classification of neurodevelopmental\nand neurodegenerative brain disorders. In this paper, we introduce an\naggregator normalization graph convolutional network by leveraging aggregation\nin graph sampling, as well as skip connections and identity mapping. The\nproposed model learns discriminative graph node representations by\nincorporating both imaging and non-imaging features into the graph nodes and\nedges, respectively, with the aim of augmenting predictive capabilities and\nproviding a holistic perspective on the underlying mechanisms of brain\ndisorders. Skip connections enable the direct flow of information from the\ninput features to later layers of the network, while identity mapping helps\nmaintain the structural information of the graph during feature learning. We\nbenchmark our model against several recent baseline methods on two large\ndatasets, Autism Brain Imaging Data Exchange (ABIDE) and Alzheimer's Disease\nNeuroimaging Initiative (ADNI), for the prediction of autism spectrum disorder\nand Alzheimer's disease, respectively. Experimental results demonstrate the\ncompetitive performance of our approach in comparison with recent baselines in\nterms of several evaluation metrics, achieving relative improvements of 50% and\n13.56% in classification accuracy over graph convolutional networks on ABIDE\nand ADNI, respectively.",
            "author": [
                "Ibrahim Salim",
                "A. Ben Hamza"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07370v2",
                "http://arxiv.org/pdf/2311.07370v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07363v3",
            "title": "Efficient bandwidth extension of musical signals using a differentiable\n  harmonic plus noise model",
            "updated": "2023-11-27T11:36:06Z",
            "published": "2023-11-13T14:26:32Z",
            "summary": "The task of bandwidth extension addresses the generation of missing high\nfrequencies of audio signals based on knowledge of the low-frequency part of\nthe sound. This task applies to various problems, such as audio coding or audio\nrestoration. In this article, we focus on efficient bandwidth extension of\nmonophonic and polyphonic musical signals using a differentiable digital signal\nprocessing (DDSP) model. Such a model is composed of a neural network part with\nrelatively few parameters trained to infer the parameters of a differentiable\ndigital signal processing model, which efficiently generates the output\nfull-band audio signal.\n  We first address bandwidth extension of monophonic signals, and then propose\ntwo methods to explicitely handle polyphonic signals. The benefits of the\nproposed models are first demonstrated on monophonic and polyphonic synthetic\ndata against a baseline and a deep-learning-based resnet model. The models are\nnext evaluated on recorded monophonic and polyphonic data, for a wide variety\nof instruments and musical genres. We show that all proposed models surpass a\nhigher complexity deep learning model for an objective metric computed in the\nfrequency domain. A MUSHRA listening test confirms the superiority of the\nproposed approach in terms of perceptual quality.",
            "author": [
                "Pierre-Amaury Grumiaux",
                "Mathieu Lagrange"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07363v3",
                "http://arxiv.org/pdf/2311.07363v3"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07361v1",
            "title": "The Impact of Large Language Models on Scientific Discovery: a\n  Preliminary Study using GPT-4",
            "updated": "2023-11-13T14:26:12Z",
            "published": "2023-11-13T14:26:12Z",
            "summary": "In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.",
            "author": [
                "Microsoft Research AI4Science",
                "Microsoft Azure Quantum"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07361v1",
                "http://arxiv.org/pdf/2311.07361v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07355v2",
            "title": "ADAMM: Anomaly Detection of Attributed Multi-graphs with Metadata: A\n  Unified Neural Network Approach",
            "updated": "2023-11-18T01:23:31Z",
            "published": "2023-11-13T14:19:36Z",
            "summary": "Given a complex graph database of node- and edge-attributed multi-graphs as\nwell as associated metadata for each graph, how can we spot the anomalous\ninstances? Many real-world problems can be cast as graph inference tasks where\nthe graph representation could capture complex relational phenomena (e.g.,\ntransactions among financial accounts in a journal entry), along with metadata\nreflecting tabular features (e.g. approver, effective date, etc.). While\nnumerous anomaly detectors based on Graph Neural Networks (GNNs) have been\nproposed, none are capable of directly handling directed graphs with\nmulti-edges and self-loops. Furthermore, the simultaneous handling of\nrelational and tabular features remains an unexplored area. In this work we\npropose ADAMM, a novel graph neural network model that handles directed\nmulti-graphs, providing a unified end-to-end architecture that fuses metadata\nand graph-level representation learning through an unsupervised anomaly\ndetection objective. Experiments on datasets from two different domains,\nnamely, general-ledger journal entries from different firms (accounting) as\nwell as human GPS trajectories from thousands of individuals (urban mobility)\nvalidate ADAMM's generality and detection effectiveness of expert-guided and\nground-truth anomalies. Notably, ADAMM outperforms existing baselines that\nhandle the two data modalities (graph and metadata) separately with post hoc\nsynthesis efforts.",
            "author": [
                "Konstantinos Sotiropoulos",
                "Lingxiao Zhao",
                "Pierre Jinghong Liang",
                "Leman Akoglu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07355v2",
                "http://arxiv.org/pdf/2311.07355v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07339v1",
            "title": "Pseudo-Anosov autoquivalances arising from Symplectic topology and their\n  hyperbolic actions on stability conditions",
            "updated": "2023-11-13T13:49:50Z",
            "published": "2023-11-13T13:49:50Z",
            "summary": "Within $N$-Calabi-Yau categories associated with quivers whose base graphs\nform trees, we delve into the study of the asymptotic behaviors of\nautoequivalences of a specific type. These autoequivalences, which we call\n\"Penner type,\" exhibit straightforward asymptotic characteristics, making them\nnoteworthy exemplars of \"pseudo-Anosov\" autoequivalences in the sense of\n\\cite{Fan-Filip-Haiden-Katzarkov-Liu21}, and also in a stronger sense that we\ndefine in the present paper.\n  In addition, we provide a practical methodology for calculating the\nstretching factors of Penner type autoequivalences. We expect that this\ncomputational approach can have applications. As an example, we establish a\npositive lower bound on the translation length of the induced action these\nautoequivalences have on the space of stability conditions. Our anticipation is\nthat this lower bound is, in fact, exact. Notably, we have observed instances\nof Penner type $\\Phi$ where the induced actions align precisely with this lower\nbound. In other words, these examples induce hyperbolic actions on the space of\nstability conditions.",
            "author": [
                "Hanwool Bae",
                "Sangjin Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07339v1",
                "http://arxiv.org/pdf/2311.07339v1"
            ],
            "primary_category": "math.SG",
            "category": [
                "math.SG",
                "math.CT",
                "math.DS",
                "math.RT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07329v1",
            "title": "When Distributed Consensus Meets Wireless Connected Autonomous Systems:\n  A Review and A DAG-based Approach",
            "updated": "2023-11-13T13:31:53Z",
            "published": "2023-11-13T13:31:53Z",
            "summary": "The connected and autonomous systems (CAS) and auto-driving era is coming\ninto our life. To support CAS applications such as AI-driven decision-making\nand blockchain-based smart data management platform, data and message\nexchange/dissemination is a fundamental element. The distributed message\nbroadcast and forward protocols in CAS, such as vehicular ad hoc networks\n(VANET), can suffer from significant message loss and uncertain transmission\ndelay, and faulty nodes might disseminate fake messages to confuse the network.\nTherefore, the consensus mechanism is essential in CAS with distributed\nstructure to guaranteed correct nodes agree on the same parameter and reach\nconsistency. However, due to the wireless nature of CAS, traditional consensus\ncannot be directly deployed. This article reviews several existing consensus\nmechanisms, including average/maximum/minimum estimation consensus mechanisms\nthat apply on quantity, Byzantine fault tolerance consensus for request, state\nmachine replication (SMR) and blockchain, as well as their implementations in\nCAS. To deploy wireless-adapted consensus, we propose a Directed Acyclic Graph\n(DAG)-based message structure to build a non-equivocation data dissemination\nprotocol for CAS, which has resilience against message loss and unpredictable\nforwarding latency. Finally, we enhance this protocol by developing a\ntwo-dimension DAG-based strategy to achieve partial order for blockchain and\ntotal order for the distributed service model SMR.",
            "author": [
                "Huanyu Wu",
                "Chentao Yue",
                "Lei Zhang",
                "Yonghui Li",
                "Muhammad Ali Imran"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07329v1",
                "http://arxiv.org/pdf/2311.07329v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.08427v2",
            "title": "Towards a Transportable Causal Network Model Based on Observational\n  Healthcare Data",
            "updated": "2023-11-20T15:05:59Z",
            "published": "2023-11-13T13:23:31Z",
            "summary": "Over the last decades, many prognostic models based on artificial\nintelligence techniques have been used to provide detailed predictions in\nhealthcare. Unfortunately, the real-world observational data used to train and\nvalidate these models are almost always affected by biases that can strongly\nimpact the outcomes validity: two examples are values missing not-at-random and\nselection bias. Addressing them is a key element in achieving transportability\nand in studying the causal relationships that are critical in clinical decision\nmaking, going beyond simpler statistical approaches based on probabilistic\nassociation.\n  In this context, we propose a novel approach that combines selection\ndiagrams, missingness graphs, causal discovery and prior knowledge into a\nsingle graphical model to estimate the cardiovascular risk of adolescent and\nyoung females who survived breast cancer. We learn this model from data\ncomprising two different cohorts of patients. The resulting causal network\nmodel is validated by expert clinicians in terms of risk assessment, accuracy\nand explainability, and provides a prognostic model that outperforms competing\nmachine learning methods.",
            "author": [
                "Alice Bernasconi",
                "Alessio Zanga",
                "Peter J. F. Lucas",
                "Marco Scutari",
                "Fabio Stella"
            ],
            "link": [
                "http://arxiv.org/abs/2311.08427v2",
                "http://arxiv.org/pdf/2311.08427v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07321v1",
            "title": "Connecting the Dots: Graph Neural Network Powered Ensemble and\n  Classification of Medical Images",
            "updated": "2023-11-13T13:20:54Z",
            "published": "2023-11-13T13:20:54Z",
            "summary": "Deep learning models have demonstrated remarkable results for various\ncomputer vision tasks, including the realm of medical imaging. However, their\napplication in the medical domain is limited due to the requirement for large\namounts of training data, which can be both challenging and expensive to\nobtain. To mitigate this, pre-trained models have been fine-tuned on\ndomain-specific data, but such an approach can suffer from inductive biases.\nFurthermore, deep learning models struggle to learn the relationship between\nspatially distant features and their importance, as convolution operations\ntreat all pixels equally. Pioneering a novel solution to this challenge, we\nemploy the Image Foresting Transform to optimally segment images into\nsuperpixels. These superpixels are subsequently transformed into\ngraph-structured data, enabling the proficient extraction of features and\nmodeling of relationships using Graph Neural Networks (GNNs). Our method\nharnesses an ensemble of three distinct GNN architectures to boost its\nrobustness. In our evaluations targeting pneumonia classification, our\nmethodology surpassed prevailing Deep Neural Networks (DNNs) in performance,\nall while drastically cutting down on the parameter count. This not only trims\ndown the expenses tied to data but also accelerates training and minimizes\nbias. Consequently, our proposition offers a sturdy, economically viable, and\nscalable strategy for medical image classification, significantly diminishing\ndependency on extensive training data sets.",
            "author": [
                "Aryan Singh",
                "Pepijn Van de Ven",
                "Ciar\u00e1n Eising",
                "Patrick Denny"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07321v1",
                "http://arxiv.org/pdf/2311.07321v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07633v2",
            "title": "Rethinking and Benchmarking Predict-then-Optimize Paradigm for\n  Combinatorial Optimization Problems",
            "updated": "2023-11-19T05:36:04Z",
            "published": "2023-11-13T13:19:34Z",
            "summary": "Numerous web applications rely on solving combinatorial optimization\nproblems, such as energy cost-aware scheduling, budget allocation on web\nadvertising, and graph matching on social networks. However, many optimization\nproblems involve unknown coefficients, and improper predictions of these\nfactors may lead to inferior decisions which may cause energy wastage,\ninefficient resource allocation, inappropriate matching in social networks,\netc. Such a research topic is referred to as \"Predict-Then-Optimize (PTO)\"\nwhich considers the performance of prediction and decision-making in a unified\nsystem. A noteworthy recent development is the end-to-end methods by directly\noptimizing the ultimate decision quality which claims to yield better results\nin contrast to the traditional two-stage approach. However, the evaluation\nbenchmarks in this field are fragmented and the effectiveness of various models\nin different scenarios remains unclear, hindering the comprehensive assessment\nand fast deployment of these methods. To address these issues, we provide a\ncomprehensive categorization of current approaches and integrate existing\nexperimental scenarios to establish a unified benchmark, elucidating the\ncircumstances under which end-to-end training yields improvements, as well as\nthe contexts in which it performs ineffectively. We also introduce a new\ndataset for the industrial combinatorial advertising problem for inclusive\nfinance to open-source. We hope the rethinking and benchmarking of PTO could\nfacilitate more convenient evaluation and deployment, and inspire further\nimprovements both in the academy and industry within this field.",
            "author": [
                "Haoyu Geng",
                "Hang Ruan",
                "Runzhong Wang",
                "Yang Li",
                "Yang Wang",
                "Lei Chen",
                "Junchi Yan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07633v2",
                "http://arxiv.org/pdf/2311.07633v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07632v1",
            "title": "ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical\n  Interactions Discovering",
            "updated": "2023-11-13T13:16:35Z",
            "published": "2023-11-13T13:16:35Z",
            "summary": "Biomedical information graphs are crucial for interaction discovering of\nbiomedical information in modern age, such as identification of multifarious\nmolecular interactions and drug discovery, which attracts increasing interests\nin biomedicine, bioinformatics, and human healthcare communities. Nowadays,\nmore and more graph neural networks have been proposed to learn the entities of\nbiomedical information and precisely reveal biomedical molecule interactions\nwith state-of-the-art results. These methods remedy the fading of features from\na far distance but suffer from remedying such problem at the expensive cost of\nredundant memory and time. In our paper, we propose a novel Residual Message\nGraph Convolution Network (ResMGCN) for fast and precise biomedical interaction\nprediction in a different idea. Specifically, instead of enhancing the message\nfrom far nodes, ResMGCN aggregates lower-order information with the next round\nhigher information to guide the node update to obtain a more meaningful node\nrepresentation. ResMGCN is able to perceive and preserve various messages from\nthe previous layer and high-order information in the current layer with least\nmemory and time cost to obtain informative representations of biomedical\nentities. We conduct experiments on four biomedical interaction network\ndatasets, including protein-protein, drug-drug, drug-target, and gene-disease\ninteractions, which demonstrates that ResMGCN outperforms previous\nstate-of-the-art models while achieving superb effectiveness on both storage\nand time.",
            "author": [
                "Zecheng Yin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07632v1",
                "http://arxiv.org/pdf/2311.07632v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "q-bio.MN"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07311v1",
            "title": "Do large language models and humans have similar behaviors in causal\n  inference with script knowledge?",
            "updated": "2023-11-13T13:05:15Z",
            "published": "2023-11-13T13:05:15Z",
            "summary": "Recently, large pre-trained language models (LLMs) have demonstrated superior\nlanguage understanding abilities, including zero-shot causal reasoning.\nHowever, it is unclear to what extent their capabilities are similar to human\nones. We here study the processing of an event $B$ in a script-based story,\nwhich causally depends on a previous event $A$. In our manipulation, event $A$\nis stated, negated, or omitted in an earlier section of the text. We first\nconducted a self-paced reading experiment, which showed that humans exhibit\nsignificantly longer reading times when causal conflicts exist ($\\neg A\n\\rightarrow B$) than under logical conditions ($A \\rightarrow B$). However,\nreading times remain similar when cause A is not explicitly mentioned,\nindicating that humans can easily infer event B from their script knowledge. We\nthen tested a variety of LLMs on the same data to check to what extent the\nmodels replicate human behavior. Our experiments show that 1) only recent LLMs,\nlike GPT-3 or Vicuna, correlate with human behavior in the $\\neg A \\rightarrow\nB$ condition. 2) Despite this correlation, all models still fail to predict\nthat $nil \\rightarrow B$ is less surprising than $\\neg A \\rightarrow B$,\nindicating that LLMs still have difficulties integrating script knowledge. Our\ncode and collected data set are available at\nhttps://github.com/tony-hong/causal-script.",
            "author": [
                "Xudong Hong",
                "Margarita Ryzhova",
                "Daniel Adrian Biondi",
                "Vera Demberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07311v1",
                "http://arxiv.org/pdf/2311.07311v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "I.2.7; I.2.0"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07308v1",
            "title": "Graded Betti numbers of a hyperedge ideal associated to join of graphs",
            "updated": "2023-11-13T13:00:37Z",
            "published": "2023-11-13T13:00:37Z",
            "summary": "Let $G$ be a finite simple graph on the vertex set $V(G)$ and let $r$ be a\npositive integer. We consider the hypergraph $\\mathrm{Con}_r(G)$ whose vertices\nare the vertices of $G$ and the (hyper)edges are all $A\\subseteq V(G)$ such\nthat $|A|=r+1$ and the induced subgraph $G[A]$ is connected. The (hyper)edge\nideal $I_r(G)$ of $\\mathrm{Con}_r(G)$ is also the Stanley-Reisner ideal of a\ngeneralisation of the independence complex of $G$, called the $r$-independence\ncomplex $\\mathrm{Ind}_r(G)$. In this article we make extensive use of the\nMayer-Vietoris sequence to find the graded Betti numbers of $I_r(G_1*G_2)$ in\nterms of the graded Betti numbers of $I_r(G_1)$ and $I_r(G_2)$, where $G_1*G_2$\nis the join of $G_1$ and $G_2$. Moreover, we find formulas for all the graded\nBetti numbers of $I_r(G)$, when $G$ is a complete graph, complete multipartite\ngraph, cycle graph and the wheel graph.",
            "author": [
                "Sourav Kanti Patra",
                "Amit Roy"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07308v1",
                "http://arxiv.org/pdf/2311.07308v1"
            ],
            "primary_category": "math.AC",
            "category": [
                "math.AC",
                "math.CO",
                "13F55, 05E45"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07306v1",
            "title": "What Large Language Models Bring to Text-rich VQA?",
            "updated": "2023-11-13T12:52:29Z",
            "published": "2023-11-13T12:52:29Z",
            "summary": "Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.",
            "author": [
                "Xuejing Liu",
                "Wei Tang",
                "Xinzhe Ni",
                "Jinghui Lu",
                "Rui Zhao",
                "Zechao Li",
                "Fei Tan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07306v1",
                "http://arxiv.org/pdf/2311.07306v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07301v1",
            "title": "Dynamically Weighted Factor-Graph for Feature-based Geo-localization",
            "updated": "2023-11-13T12:44:14Z",
            "published": "2023-11-13T12:44:14Z",
            "summary": "Feature-based geo-localization relies on associating features extracted from\naerial imagery with those detected by the vehicle's sensors. This requires that\nthe type of landmarks must be observable from both sources. This no-variety of\nfeature types generates poor representations that lead to outliers and\ndeviations, produced by ambiguities and lack of detections respectively. To\nmitigate these drawbacks, in this paper, we present a dynamically weighted\nfactor graph model for the vehicle's trajectory estimation. The weight\nadjustment in this implementation depends on information quantification in the\ndetections performed using a LiDAR sensor. Also, a prior (GNSS-based) error\nestimation is included in the model. Then, when the representation becomes\nambiguous or sparse, the weights are dynamically adjusted to rely on the\ncorrected prior trajectory, mitigating in this way outliers and deviations. We\ncompare our method against state-of-the-art geo-localization ones in a\nchallenging ambiguous environment, where we also cause detection losses. We\ndemonstrate mitigation of the mentioned drawbacks where the other methods fail.",
            "author": [
                "Miguel \u00c1ngel Mu\u00f1oz-Ba\u00f1\u00f3n",
                "Alejandro Olivas",
                "Edison Velasco-S\u00e1nchez",
                "Francisco A. Candelas",
                "Fernando Torres"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07301v1",
                "http://arxiv.org/pdf/2311.07301v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07299v1",
            "title": "Enhancing NAC-ABE to Support Access Control for mHealth Applications and\n  Beyond",
            "updated": "2023-11-13T12:39:35Z",
            "published": "2023-11-13T12:39:35Z",
            "summary": "Name-based access control (NAC) over NDN provides fine-grained data\nconfidentiality and access control by encrypting and signing data at the time\nof data production. NAC utilizes specially crafted naming conventions to define\nand enforce access control policies. NAC-ABE, an extension to NAC, uses an\nattribute-based encryption (ABE) scheme to support access control with improved\nscalability and flexibility. However, existing NAC-ABE libraries are based on\nciphertext-policy ABE (CP-ABE), which requires knowledge of the access policy\nwhen encrypting data packets. In some applications, including mHealth, the data\naccess policy is unknown at the time of data generation, while data attributes\nand properties are known. In this paper, we present an extension to the\nexisting NDN-ABE library which can be used by mHealth and other applications to\nenforce fine-granularity access control in data sharing. We also discuss the\nchallenges we encountered during the application deployment, and remaining open\nissues together with potential solution directions.",
            "author": [
                "Saurab Dulal",
                "Tianyuan Yu",
                "Siqi Liu",
                "Adam Robert Thieme",
                "Lixia Zhang",
                "Lan Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07299v1",
                "http://arxiv.org/pdf/2311.07299v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07290v1",
            "title": "Neutron calibrations in dark matter searches: the ANAIS-112 case",
            "updated": "2023-11-13T12:34:42Z",
            "published": "2023-11-13T12:34:42Z",
            "summary": "ANAIS is a direct dark matter detection experiment whose goal is to confirm\nor refute in a model independent way the positive annual modulation signal\nclaimed by DAMA/LIBRA. Consisting of 112.5 kg of NaI(Tl) scintillators,\nANAIS-112 is taking data at the Canfranc Underground Laboratory in Spain since\nAugust, 2017. Results corresponding to the analysis of three years of data are\ncompatible with the absence of modulation and incompatible with DAMA/LIBRA.\nHowever, testing this signal relies on the knowledge of the scintillation\nquenching factors (QF), which measure the relative efficiency for the\nconversion into light of the nuclear recoil energy with respect to the same\nenergy deposited by electrons. Previous measurements of the QF in NaI(Tl) show\na large dispersion. Consequently, in order to better understand the response of\nthe ANAIS-112 detectors to nuclear recoils, a specific neutron calibration\nprogram has been developed. This program combines two different approaches: on\nthe one hand, QF measurements were carried out in a monoenergetic neutron beam;\non the other hand, the study presented here aims at the evaluation of the QF by\nexposing directly the ANAIS-112 crystals to neutrons from low activity\n$^{252}$Cf sources, placed outside the lead shielding. Comparison between these\nonsite neutron measurements and detailed GEANT4 simulations will be presented,\nconfirming that this approach allows testing different QF models.",
            "author": [
                "T. Pardo",
                "J. Amar\u00e9",
                "J. Apilluelo",
                "S. Cebri\u00e1n",
                "D. Cintas",
                "I. Coarasa",
                "E. Garc\u00eda",
                "M. Mart\u00ednez",
                "M. A. Oliv\u00e1n",
                "Y. Ortigoza",
                "A. Ortiz de Sol\u00f3rzano",
                "M. Pellicer",
                "J. Puimed\u00f3n",
                "A. Salinas",
                "M. L. Sarsa",
                "P. Villar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07290v1",
                "http://arxiv.org/pdf/2311.07290v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "hep-ex",
                "physics.ins-det"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10104v1",
            "title": "A Framework of Defining, Modeling, and Analyzing Cognition Mechanisms",
            "updated": "2023-11-13T12:31:46Z",
            "published": "2023-11-13T12:31:46Z",
            "summary": "Cognition is a core part of and a common topic among philosophy of mind,\npsychology, neuroscience, AI, and cognitive science. Through a mechanistic\nlens, I propose a framework of defining, modeling, and analyzing cognition\nmechanisms. Firstly, appropriate terms are introduced and used in explanations\nrelated to the framework and within the definition of a mechanism. I implicitly\ncontend that this terminology essentially characterizes a conceptual world\nrequired for discussions in this paper. Secondly, a mathematical model of a\nmechanism based on directed graphs is proposed. Thirdly, the definition of a\nbase necessary for a mechanism to be classified as a cognition mechanism is\nproposed. I argue that the cognition base has the features of the cognition\nself of humans. Fourthly, three ways to mechanistically look at mechanisms is\ndefined and specific instances of them are suggested. Fifthly, standards for\nvisualization and presentation of mechanisms, cognition mechanisms, and the\ninstances to mechanistically look at them are suggested and used to analyze\ncognition mechanisms through appropriate examples. Finally, the features of\nthis paper are discussed and prospects of further development of the proposed\nframework are briefly expressed.",
            "author": [
                "Amir Fayezioghani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10104v1",
                "http://arxiv.org/pdf/2311.10104v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07287v1",
            "title": "Integral of depth zero to three basis of Modular Graph Functions",
            "updated": "2023-11-13T12:28:04Z",
            "published": "2023-11-13T12:28:04Z",
            "summary": "Modular Graph Functions (MGFs) emerge from the low-energy expansion of the\namplitude integral over the configuration of punctures on a torus in studying\nstring scattering amplitude at one-loop. These functions are\nSL(2,$\\mathbb{Z}$)-invariant. To find the string scattering amplitude, we must\nintegrate them over the moduli space of the torus. In this paper, we use the\niterated integral representation of MGFs to establish a depth-dependent basis\nfor them up to depth three, where depth refers to the number of iterations in\nthe integral. This basis has a suitable Laplace equation. We integrate this\nbasis from depth zero to depth three over the fundamental domain of\nSL(2,$\\mathbb{Z}$) with a cut-off.",
            "author": [
                "Mehregan Doroudiani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07287v1",
                "http://arxiv.org/pdf/2311.07287v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th",
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07277v1",
            "title": "AdaCCD: Adaptive Semantic Contrasts Discovery based Cross Lingual\n  Adaptation for Code Clone Detection",
            "updated": "2023-11-13T12:20:48Z",
            "published": "2023-11-13T12:20:48Z",
            "summary": "Code Clone Detection, which aims to retrieve functionally similar programs\nfrom large code bases, has been attracting increasing attention. Modern\nsoftware often involves a diverse range of programming languages. However,\ncurrent code clone detection methods are generally limited to only a few\npopular programming languages due to insufficient annotated data as well as\ntheir own model design constraints. To address these issues, we present AdaCCD,\na novel cross-lingual adaptation method that can detect cloned codes in a new\nlanguage without any annotations in that language. AdaCCD leverages\nlanguage-agnostic code representations from pre-trained programming language\nmodels and propose an Adaptively Refined Contrastive Learning framework to\ntransfer knowledge from resource-rich languages to resource-poor languages. We\nevaluate the cross-lingual adaptation results of AdaCCD by constructing a\nmultilingual code clone detection benchmark consisting of 5 programming\nlanguages. AdaCCD achieves significant improvements over other baselines, and\nit is even comparable to supervised fine-tuning.",
            "author": [
                "Yangkai Du",
                "Tengfei Ma",
                "Lingfei Wu",
                "Xuhong Zhang",
                "Shouling Ji"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07277v1",
                "http://arxiv.org/pdf/2311.07277v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07631v1",
            "title": "The 4+1 Model of Data Science",
            "updated": "2023-11-13T12:12:32Z",
            "published": "2023-11-13T12:12:32Z",
            "summary": "Data Science is a complex and evolving field, but most agree that it can be\ndefined as a combination of expertise drawn from three broad areascomputer\nscience and technology, math and statistics, and domain knowledge -- with the\npurpose of extracting knowledge and value from data. Beyond this, the field is\noften defined as a series of practical activities ranging from the cleaning and\nwrangling of data, to its analysis and use to infer models, to the visual and\nrhetorical representation of results to stakeholders and decision-makers. This\nessay proposes a model of data science that goes beyond laundry-list\ndefinitions to get at the specific nature of data science and help distinguish\nit from adjacent fields such as computer science and statistics. We define data\nscience as an interdisciplinary field comprising four broad areas of expertise:\nvalue, design, systems, and analytics. A fifth area, practice, integrates the\nother four in specific contexts of domain knowledge. We call this the 4+1 model\nof data science. Together, these areas belong to every data science project,\neven if they are often unconnected and siloed in the academy.",
            "author": [
                "Rafael C. Alvarado"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07631v1",
                "http://arxiv.org/pdf/2311.07631v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "cs.GL",
                "E.m; K.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07266v1",
            "title": "Self-testing of true multipartite entangled states",
            "updated": "2023-11-13T12:04:03Z",
            "published": "2023-11-13T12:04:03Z",
            "summary": "Self-testing is a method to certify quantum states and measurements in a\ndevice-independent way. The device-independent certification of quantum\nproperties is purely based on input-output measurement statistics of the\ninvolved devices with minimal knowledge about their internal workings.\nBipartite pure entangled states can be self-tested, but, in the case of\nmultipartite pure entangled states, the answer is not so straightforward.\nNevertheless, \\v{S}upi\\'{c} et al. recently introduced a novel self-testing\nmethod for any pure entangled quantum state, which leverages network assistance\nand relies on bipartite entangled measurements. Hence, their scheme loses the\ntrue device-independent flavor of self-testing. In this regard, we provide a\nself-testing scheme for genuine multipartite pure entangle states in the true\nsense by employing a generalized Hardy-type non-local argument. It is important\nto note that our approach involves only local operations and classical\ncommunications and it does not depend on bipartite entangled measurements and\nis free from any network assistance. In addition, we provide the\ndevice-independent bound of the maximum probability of success of the\ngeneralized Hardy-type nonlocality test.",
            "author": [
                "Ranendu Adhikary",
                "Abhishek Mishra",
                "Ramij Rahaman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07266v1",
                "http://arxiv.org/pdf/2311.07266v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07259v1",
            "title": "Towards Bounding Causal Effects under Markov Equivalence",
            "updated": "2023-11-13T11:49:55Z",
            "published": "2023-11-13T11:49:55Z",
            "summary": "Predicting the effect of unseen interventions is a fundamental research\nquestion across the data sciences. It is well established that, in general,\nsuch questions cannot be answered definitively from observational data, e.g.,\nas a consequence of unobserved confounding. A generalization of this task is to\ndetermine non-trivial bounds on causal effects induced by the data, also known\nas the task of partial causal identification. In the literature, several\nalgorithms have been developed for solving this problem. Most, however, require\na known parametric form or a fully specified causal diagram as input, which is\nusually not available in practical applications. In this paper, we assume as\ninput a less informative structure known as a Partial Ancestral Graph, which\nrepresents a Markov equivalence class of causal diagrams and is learnable from\nobservational data. In this more \"data-driven\" setting, we provide a systematic\nalgorithm to derive bounds on causal effects that can be computed analytically.",
            "author": [
                "Alexis Bellot"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07259v1",
                "http://arxiv.org/pdf/2311.07259v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07237v1",
            "title": "In Search of the Long-Tail: Systematic Generation of Long-Tail Knowledge\n  via Logical Rule Guided Search",
            "updated": "2023-11-13T10:56:59Z",
            "published": "2023-11-13T10:56:59Z",
            "summary": "Since large language models have approached human-level performance on many\ntasks, it has become increasingly harder for researchers to find tasks that are\nstill challenging to the models. Failure cases usually come from the long-tail\ndistribution - data that an oracle language model could assign a probability on\nthe lower end of its distribution. Current methodology such as prompt\nengineering or crowdsourcing are insufficient for creating long-tail examples\nbecause humans are constrained by cognitive bias. We propose a\nLogic-Induced-Knowledge-Search (LINK) framework for systematically generating\nlong-tail knowledge statements. Grounded by a symbolic rule, we search for\nlong-tail values for each variable of the rule by first prompting a LLM, then\nverifying the correctness of the values with a critic, and lastly pushing for\nthe long-tail distribution with a reranker. With this framework we construct a\ndataset, Logic-Induced-Long-Tail (LINT), consisting of 200 symbolic rules and\n50K knowledge statements spanning across four domains. Human annotations find\nthat 84% of the statements in LINT are factually correct. In contrast, ChatGPT\nand GPT4 struggle with directly generating long-tail statements under the\nguidance of logic rules, each only getting 56% and 78% of their statements\ncorrect. Moreover, their \"long-tail\" generations in fact fall into the higher\nlikelihood range, and thus are not really long-tail. Our findings suggest that\nLINK is effective for generating data in the long-tail distribution while\nenforcing quality. LINT can be useful for systematically evaluating LLMs'\ncapabilities in the long-tail distribution. We challenge the models with a\nsimple entailment classification task using samples from LINT. We find that\nChatGPT and GPT4's capability in identifying incorrect knowledge drop by ~3% in\nthe long-tail distribution compared to head distribution.",
            "author": [
                "Huihan Li",
                "Yuting Ning",
                "Zeyi Liao",
                "Siyuan Wang",
                "Xiang Lorraine Li",
                "Ximing Lu",
                "Faeze Brahman",
                "Wenting Zhao",
                "Yejin Choi",
                "Xiang Ren"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07237v1",
                "http://arxiv.org/pdf/2311.07237v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07233v1",
            "title": "IASCAR: Incremental Answer Set Counting by Anytime Refinement",
            "updated": "2023-11-13T10:53:48Z",
            "published": "2023-11-13T10:53:48Z",
            "summary": "Answer set programming (ASP) is a popular declarative programming paradigm\nwith various applications. Programs can easily have many answer sets that\ncannot be enumerated in practice, but counting still allows quantifying\nsolution spaces. If one counts under assumptions on literals, one obtains a\ntool to comprehend parts of the solution space, so-called answer set\nnavigation. However, navigating through parts of the solution space requires\ncounting many times, which is expensive in theory. Knowledge compilation\ncompiles instances into representations on which counting works in polynomial\ntime. However, these techniques exist only for CNF formulas, and compiling ASP\nprograms into CNF formulas can introduce an exponential overhead. This paper\nintroduces a technique to iteratively count answer sets under assumptions on\nknowledge compilations of CNFs that encode supported models. Our anytime\ntechnique uses the inclusion-exclusion principle to improve bounds by over- and\nundercounting systematically. In a preliminary empirical analysis, we\ndemonstrate promising results. After compiling the input (offline phase), our\napproach quickly (re)counts.",
            "author": [
                "Johannes K. Fichte",
                "Sarah Alice Gaggl",
                "Markus Hecher",
                "Dominik Rusovac"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07233v1",
                "http://arxiv.org/pdf/2311.07233v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07223v1",
            "title": "Wasm SpecTec: Engineering a Formal Language Standard",
            "updated": "2023-11-13T10:41:39Z",
            "published": "2023-11-13T10:41:39Z",
            "summary": "WebAssembly (Wasm) is a low-level bytecode language and virtual machine,\nintended as a compilation target for a wide range of programming languages,\nwhich is seeing increasing adoption across diverse ecosystems. As a young\ntechnology, Wasm continues to evolve -- it reached version 2.0 last year and\nanother major update is expected soon.\n  For a new feature to be standardised in Wasm, four key artefacts must be\npresented: a formal (mathematical) specification of the feature, an\naccompanying prose pseudocode description, an implementation in the official\nreference interpreter, and a suite of unit tests. This rigorous process helps\nto avoid errors in the design and implementation of new Wasm features, and\nWasm's distinctive formal specification in particular has facilitated\nmachine-checked proofs of various correctness properties for the language.\nHowever, manually crafting all of these artefacts requires expert knowledge\ncombined with repetitive and tedious labor, which is a burden on the language's\nstandardization process and authoring of the specification.\n  This paper presents Wasm SpecTec, a technology to express the formal\nspecification of Wasm through a domain-specific language. This DSL allows all\nof Wasm's currently handwritten specification artefacts to be error-checked and\ngenerated automatically from a single source of truth, and is designed to be\neasy to write, read, compare, and review. We believe that Wasm SpecTec's\nautomation and meta-level error checking will significantly ease the current\nburden of the language's specification authors. We demonstrate the current\ncapabilities of Wasm SpecTec by showcasing its proficiency in generating\nvarious artefacts, and describe our work towards replacing the manually written\nofficial Wasm specification document with specifications generated by Wasm\nSpecTec.",
            "author": [
                "Joachim Breitner",
                "Philippa Gardner",
                "Jaehyun Lee",
                "Sam Lindley",
                "Matija Pretnar",
                "Xiaojia Rao",
                "Andreas Rossberg",
                "Sukyoung Ryu",
                "Wonho Shin",
                "Conrad Watt",
                "Dongjun Youn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07223v1",
                "http://arxiv.org/pdf/2311.07223v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07221v1",
            "title": "Properly immersed curves in arbitrary surfaces via apparent contours on\n  spines of traversing flows",
            "updated": "2023-11-13T10:34:20Z",
            "published": "2023-11-13T10:34:20Z",
            "summary": "Let S be a compact surface with boundary and F be the set of the orbits of a\ntraversing flow on S. If the flow is generic, its orbit space is a spine G of\nS, namely G is a graph embedded in S and S is a regular neighbourhood of G.\nMoreover an extra structure on G turns it into a flow-spine, from which one can\nreconstruct S and F. In this paper we study properly immersed curves C in S. We\ndo this by considering generic C's and their apparent contour relative to F,\nnamely the set of points of G corresponding to orbits that either are tangent\nto C, or go through a self-intersection of C, or meet the boundary of C. We\ntranslate this apparent contour into a decoration of G that allows one to\nreconstruct C, and then we allow C to vary up to homotopy within a fixed\ngeneric F, and next also F to vary up to homotopy, and we identify a finite set\nof local moves on decorated graphs that translate these homotopies.",
            "author": [
                "Carlo Petronio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07221v1",
                "http://arxiv.org/pdf/2311.07221v1"
            ],
            "primary_category": "math.GT",
            "category": [
                "math.GT",
                "57R42 (primary), 37E35, 57R40, 58D10 (secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07219v1",
            "title": "On Blockers and Transversals of Maximum Independent Sets in\n  Co-Comparability Graphs",
            "updated": "2023-11-13T10:29:22Z",
            "published": "2023-11-13T10:29:22Z",
            "summary": "In this paper, we consider the following two problems: (i) Deletion\nBlocker($\\alpha$) where we are given an undirected graph $G=(V,E)$ and two\nintegers $k,d\\geq 1$ and ask whether there exists a subset of vertices\n$S\\subseteq V$ with $|S|\\leq k$ such that $\\alpha(G-S) \\leq \\alpha(G)-d$, that\nis the independence number of $G$ decreases by at least $d$ after having\nremoved the vertices from $S$; (ii) Transversal($\\alpha$) where we are given an\nundirected graph $G=(V,E)$ and two integers $k,d\\geq 1$ and ask whether there\nexists a subset of vertices $S\\subseteq V$ with $|S|\\leq k$ such that for every\nmaximum independent set $I$ we have $|I\\cap S| \\geq d$. We show that both\nproblems are polynomial-time solvable in the class of co-comparability graphs\nby reducing them to the well-known Vertex Cut problem. Our results generalize a\nresult of [Chang et al., Maximum clique transversals, Lecture Notes in Computer\nScience 2204, pp. 32-43, WG 2001] and a recent result of [Hoang et al.,\nAssistance and interdiction problems on interval graphs, Discrete Applied\nMathematics 340, pp. 153-170, 2023].",
            "author": [
                "Felicia Lucke",
                "Bernard Ries"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07219v1",
                "http://arxiv.org/pdf/2311.07219v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07204v1",
            "title": "On Elastic Language Models",
            "updated": "2023-11-13T09:55:52Z",
            "published": "2023-11-13T09:55:52Z",
            "summary": "Large-scale pretrained language models have achieved compelling performance\nin a wide range of language understanding and information retrieval tasks.\nKnowledge distillation offers an opportunity to compress a large language model\nto a small one, in order to reach a reasonable latency-performance tradeoff.\nHowever, for scenarios where the number of requests (e.g., queries submitted to\na search engine) is highly variant, the static tradeoff attained by the\ncompressed language model might not always fit. Once a model is assigned with a\nstatic tradeoff, it could be inadequate in that the latency is too high when\nthe number of requests is large or the performance is too low when the number\nof requests is small. To this end, we propose an elastic language model\n(ElasticLM) that elastically adjusts the tradeoff according to the request\nstream. The basic idea is to introduce a compute elasticity to the compressed\nlanguage model, so that the tradeoff could vary on-the-fly along scalable and\ncontrollable compute. Specifically, we impose an elastic structure to enable\nElasticLM with compute elasticity and design an elastic optimization to learn\nElasticLM under compute elasticity. To serve ElasticLM, we apply an elastic\nschedule. Considering the specificity of information retrieval, we adapt\nElasticLM to dense retrieval and reranking and present ElasticDenser and\nElasticRanker respectively. Offline evaluation is conducted on a language\nunderstanding benchmark GLUE; and several information retrieval tasks including\nNatural Question, Trivia QA, and MS MARCO. The results show that ElasticLM\nalong with ElasticDenser and ElasticRanker can perform correctly and\ncompetitively compared with an array of static baselines. Furthermore, online\nsimulation with concurrency is also carried out. The results demonstrate that\nElasticLM can provide elastic tradeoffs with respect to varying request stream.",
            "author": [
                "Chen Zhang",
                "Benyou Wang",
                "Dawei Song"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07204v1",
                "http://arxiv.org/pdf/2311.07204v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07630v1",
            "title": "Cross-modal Generative Model for Visual-Guided Binaural Stereo\n  Generation",
            "updated": "2023-11-13T09:53:14Z",
            "published": "2023-11-13T09:53:14Z",
            "summary": "Binaural stereo audio is recorded by imitating the way the human ear receives\nsound, which provides people with an immersive listening experience. Existing\napproaches leverage autoencoders and directly exploit visual spatial\ninformation to synthesize binaural stereo, resulting in a limited\nrepresentation of visual guidance. For the first time, we propose a visually\nguided generative adversarial approach for generating binaural stereo audio\nfrom mono audio. Specifically, we develop a Stereo Audio Generation Model\n(SAGM), which utilizes shared spatio-temporal visual information to guide the\ngenerator and the discriminator to work separately. The shared visual\ninformation is updated alternately in the generative adversarial stage,\nallowing the generator and discriminator to deliver their respective guided\nknowledge while visually sharing. The proposed method learns bidirectional\ncomplementary visual information, which facilitates the expression of visual\nguidance in generation. In addition, spatial perception is a crucial attribute\nof binaural stereo audio, and thus the evaluation of stereo spatial perception\nis essential. However, previous metrics failed to measure the spatial\nperception of audio. To this end, a metric to measure the spatial perception of\naudio is proposed for the first time. The proposed metric is capable of\nmeasuring the magnitude and direction of spatial perception in the temporal\ndimension. Further, considering its function, it is feasible to utilize it\ninstead of demanding user studies to some extent. The proposed method achieves\nstate-of-the-art performance on 2 datasets and 5 evaluation metrics.\nQualitative experiments and user studies demonstrate that the method generates\nspace-realistic stereo audio.",
            "author": [
                "Zhaojian Li",
                "Bin Zhao",
                "Yuan Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07630v1",
                "http://arxiv.org/pdf/2311.07630v1"
            ],
            "primary_category": "cs.SD",
            "category": [
                "cs.SD",
                "cs.CV",
                "cs.LG",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07203v1",
            "title": "Optical Quantum Sensing for Agnostic Environments via Deep Learning",
            "updated": "2023-11-13T09:46:05Z",
            "published": "2023-11-13T09:46:05Z",
            "summary": "Optical quantum sensing promises measurement precision beyond classical\nsensors termed the Heisenberg limit (HL). However, conventional methodologies\noften rely on prior knowledge of the target system to achieve HL, presenting\nchallenges in practical applications. Addressing this limitation, we introduce\nan innovative Deep Learning-based Quantum Sensing scheme (DQS), enabling\noptical quantum sensors to attain HL in agnostic environments. DQS incorporates\ntwo essential components: a Graph Neural Network (GNN) predictor and a\ntrigonometric interpolation algorithm. Operating within a data-driven paradigm,\nDQS utilizes the GNN predictor, trained on offline data, to unveil the\nintrinsic relationships between the optical setups employed in preparing the\nprobe state and the resulting quantum Fisher information (QFI) after\ninteraction with the agnostic environment. This distilled knowledge facilitates\nthe identification of optimal optical setups associated with maximal QFI.\nSubsequently, DQS employs a trigonometric interpolation algorithm to recover\nthe unknown parameter estimates for the identified optical setups. Extensive\nexperiments are conducted to investigate the performance of DQS under different\nsettings up to eight photons. Our findings not only offer a new lens through\nwhich to accelerate optical quantum sensing tasks but also catalyze future\nresearch integrating deep learning and quantum mechanics.",
            "author": [
                "Zeqiao Zhou",
                "Yuxuan Du",
                "Xu-Fei Yin",
                "Shanshan Zhao",
                "Xinmei Tian",
                "Dacheng Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07203v1",
                "http://arxiv.org/pdf/2311.07203v1"
            ],
            "primary_category": "quant-ph",
            "category": [
                "quant-ph",
                "cs.AI",
                "physics.optics"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07191v1",
            "title": "Applying Large Language Models for Causal Structure Learning in Non\n  Small Cell Lung Cancer",
            "updated": "2023-11-13T09:31:14Z",
            "published": "2023-11-13T09:31:14Z",
            "summary": "Causal discovery is becoming a key part in medical AI research. These methods\ncan enhance healthcare by identifying causal links between biomarkers,\ndemographics, treatments and outcomes. They can aid medical professionals in\nchoosing more impactful treatments and strategies. In parallel, Large Language\nModels (LLMs) have shown great potential in identifying patterns and generating\ninsights from text data. In this paper we investigate applying LLMs to the\nproblem of determining the directionality of edges in causal discovery.\nSpecifically, we test our approach on a deidentified set of Non Small Cell Lung\nCancer(NSCLC) patients that have both electronic health record and genomic\npanel data. Graphs are validated using Bayesian Dirichlet estimators using\ntabular data. Our result shows that LLMs can accurately predict the\ndirectionality of edges in causal graphs, outperforming existing\nstate-of-the-art methods. These findings suggests that LLMs can play a\nsignificant role in advancing causal discovery and help us better understand\ncomplex systems.",
            "author": [
                "Narmada Naik",
                "Ayush Khandelwal",
                "Mohit Joshi",
                "Madhusudan Atre",
                "Hollis Wright",
                "Kavya Kannan",
                "Scott Hill",
                "Giridhar Mamidipudi",
                "Ganapati Srinivasa",
                "Carlo Bifulco",
                "Brian Piening",
                "Kevin Matlock"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07191v1",
                "http://arxiv.org/pdf/2311.07191v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07188v1",
            "title": "Fitting tree model with CNN and geodesics to track vesselsand\n  application to Ultrasound Localization Microscopy data",
            "updated": "2023-11-13T09:25:03Z",
            "published": "2023-11-13T09:25:03Z",
            "summary": "Segmentation of tubular structures in vascular imaging is a well studied\ntask, although it is rare that we try to infuse knowledge of the tree-like\nstructure of the regions to be detected. Our work focuses on detecting the\nimportant landmarks in the vascular network (via CNN performing both\nlocalization and classification of the points of interest) and representing\nvessels as the edges in some minimal distance tree graph. We leverage geodesic\nmethods relevant to the detection of vessels and their geometry, making use of\nthe space of positions and orientations so that 2D vessels can be accurately\nrepresented as trees. We build our model to carry tracking on Ultrasound\nLocalization Microscopy (ULM) data, proposing to build a good cost function for\ntracking on this type of data. We also test our framework on synthetic and eye\nfundus data. Results show that scarcity of well annotated ULM data is an\nobstacle to localization of vascular landmarks but the Orientation Score built\nfrom ULM data yields good geodesics for tracking blood vessels.",
            "author": [
                "Th\u00e9o Bertrand",
                "Laurent D. Cohen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07188v1",
                "http://arxiv.org/pdf/2311.07188v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07180v1",
            "title": "Knowledge Graph Representations to enhance Intensive Care Time-Series\n  Predictions",
            "updated": "2023-11-13T09:11:55Z",
            "published": "2023-11-13T09:11:55Z",
            "summary": "Intensive Care Units (ICU) require comprehensive patient data integration for\nenhanced clinical outcome predictions, crucial for assessing patient\nconditions. Recent deep learning advances have utilized patient time series\ndata, and fusion models have incorporated unstructured clinical reports,\nimproving predictive performance. However, integrating established medical\nknowledge into these models has not yet been explored. The medical domain's\ndata, rich in structural relationships, can be harnessed through knowledge\ngraphs derived from clinical ontologies like the Unified Medical Language\nSystem (UMLS) for better predictions. Our proposed methodology integrates this\nknowledge with ICU data, improving clinical decision modeling. It combines\ngraph representations with vital signs and clinical reports, enhancing\nperformance, especially when data is missing. Additionally, our model includes\nan interpretability component to understand how knowledge graph nodes affect\npredictions.",
            "author": [
                "Samyak Jain",
                "Manuel Burger",
                "Gunnar R\u00e4tsch",
                "Rita Kuznetsova"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07180v1",
                "http://arxiv.org/pdf/2311.07180v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07629v2",
            "title": "Note on the second eigenvalue of regular graphs",
            "updated": "2023-11-21T15:25:46Z",
            "published": "2023-11-13T09:11:02Z",
            "summary": "The goal of this expository note is to give a short, self-contained proof of\nnearly optimal lower bounds for the second largest eigenvalue of the adjacency\nmatrix of regular graphs.",
            "author": [
                "Igor Balla",
                "Eero R\u00e4ty",
                "Benny Sudakov",
                "Istv\u00e1n Tomon"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07629v2",
                "http://arxiv.org/pdf/2311.07629v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07177v1",
            "title": "Conservation law and Hamilton-Jacobi equations on a junction: the convex\n  case",
            "updated": "2023-11-13T09:08:56Z",
            "published": "2023-11-13T09:08:56Z",
            "summary": "The goal of this paper is to study the link between the solution to an\nHamilton-Jacobi (HJ) equation and the solution to a Scalar Conservation Law\n(SCL) on a special network. When the equations are posed on the real axis, it\nis well known that the space derivative of the solution to the Hamilton-Jacobi\nequation is the solution to the corresponding scalar conservation law. On\nnetworks, the situation is more complicated and we show that this result still\nholds true in the convex case on a 1:1 junction. The correspondence between\nsolutions to HJ equations and SCL on a 1:1 junction is done showing the\nconvergence of associated numerical schemes. A second direct proof using\nsemi-algebraic functions is also given. Here a 1:1 junction is a simple network\ncomposed of two edges and one vertex. In the case of three edges or more, we\nshow that the associated HJ germ is not a L 1-dissipative germ, while it is the\ncase for only two edges. As an important byproduct of our numerical approach,\nwe get a new result on the convergence of numerical schemes for scalar\nconservation laws on a junction. For a general desired flux condition which is\ndiscretized, we show that the numerical solution with the general flux\ncondition converges to the solution of a SCL problem with an effective flux\ncondition at the junction. Up to our knowledge, in previous works the effective\ncondition was directly implemented in the numerical scheme. In general the\neffective flux condition differs from the desired one, and is its relaxation,\nwhich is very natural from the point of view of Hamilton-Jacobi equations. Here\nfor SCL, this effective flux condition is encoded in a germ that we\ncharacterize at the junction.",
            "author": [
                "Pierre Cardaliaguet",
                "Nicolas Forcadel",
                "Theo Girard",
                "Regis Monneau"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07177v1",
                "http://arxiv.org/pdf/2311.07177v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07170v1",
            "title": "Regenerating Arbitrary Video Sequences with Distillation Path-Finding",
            "updated": "2023-11-13T09:05:30Z",
            "published": "2023-11-13T09:05:30Z",
            "summary": "If the video has long been mentioned as a widespread visualization form, the\nanimation sequence in the video is mentioned as storytelling for people.\nProducing an animation requires intensive human labor from skilled professional\nartists to obtain plausible animation in both content and motion direction,\nincredibly for animations with complex content, multiple moving objects, and\ndense movement. This paper presents an interactive framework to generate new\nsequences according to the users' preference on the starting frame. The\ncritical contrast of our approach versus prior work and existing commercial\napplications is that novel sequences with arbitrary starting frame are produced\nby our system with a consistent degree in both content and motion direction. To\nachieve this effectively, we first learn the feature correlation on the\nframeset of the given video through a proposed network called RSFNet. Then, we\ndevelop a novel path-finding algorithm, SDPF, which formulates the knowledge of\nmotion directions of the source video to estimate the smooth and plausible\nsequences. The extensive experiments show that our framework can produce new\nanimations on the cartoon and natural scenes and advance prior works and\ncommercial applications to enable users to obtain more predictable results.",
            "author": [
                "Thi-Ngoc-Hanh Le",
                "Sheng-Yi Yao",
                "Chun-Te Wu",
                "Tong-Yee Lee"
            ],
            "link": [
                "http://dx.doi.org/10.1109/TVCG.2023.3237739",
                "http://arxiv.org/abs/2311.07170v1",
                "http://arxiv.org/pdf/2311.07170v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07165v4",
            "title": "The geometry of the maximum likelihood of Cauchy-like distributions",
            "updated": "2023-11-26T21:48:56Z",
            "published": "2023-11-13T09:00:12Z",
            "summary": "A simple way of obtaining robust estimates of the \"center\" (or the\n\"location\") and of the \"scatter\" of a dataset is to use the maximum likelihood\nestimate with a class of heavy-tailed distributions, regardless of the \"true\"\ndistribution generating the data. We observe that the maximum likelihood\nproblem for the Cauchy distributions, which have particularly heavy tails, is\ngeodesically convex and therefore efficiently solvable (Cauchy distributions\nare parametrized by the upper half plane, i.e. by the hyperbolic plane).\nMoreover, it has an appealing geometrical meaning: the datapoints, living on\nthe boundary of the hyperbolic plane, are attracting the parameter by unit\nforces, and we search the point where these forces are in equilibrium.\n  This picture generalizes to several classes of multivariate distributions\nwith heavy tails, including, in particular, the multivariate Cauchy\ndistributions. The hyperbolic plane gets replaced by symmetric spaces of\nnoncompact type. Geodesic convexity gives us an efficient numerical solution of\nthe maximum likelihood problem for these distribution classes. This can then be\nused for robust estimates of location and spread, thanks to the heavy tails of\nthese distributions.",
            "author": [
                "Pavol \u0160evera"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07165v4",
                "http://arxiv.org/pdf/2311.07165v4"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "math.DG",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07162v1",
            "title": "CycleGANAS: Differentiable Neural Architecture Search for CycleGAN",
            "updated": "2023-11-13T08:56:56Z",
            "published": "2023-11-13T08:56:56Z",
            "summary": "We develop a Neural Architecture Search (NAS) framework for CycleGAN that\ncarries out unpaired image-to-image translation task. Extending previous NAS\ntechniques for Generative Adversarial Networks (GANs) to CycleGAN is not\nstraightforward due to the task difference and greater search space. We design\narchitectures that consist of a stack of simple ResNet-based cells and develop\na search method that effectively explore the large search space. We show that\nour framework, called CycleGANAS, not only effectively discovers\nhigh-performance architectures that either match or surpass the performance of\nthe original CycleGAN, but also successfully address the data imbalance by\nindividual architecture search for each translation direction. To our best\nknowledge, it is the first NAS result for CycleGAN and shed light on NAS for\nmore complex structures.",
            "author": [
                "Taegun An",
                "Changhee Joo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07162v1",
                "http://arxiv.org/pdf/2311.07162v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07158v1",
            "title": "Investigating students' scientific reasoning through heuristic and\n  analytical thought processes",
            "updated": "2023-11-13T08:51:24Z",
            "published": "2023-11-13T08:51:24Z",
            "summary": "In recent years there has been growing evidence that even after teaching\ndesigned to address the learning difficulties dictated by literature, many\nphysics learners fail to create the proper reasoning chains that connect the\nfundamental principles and lead to reasoned predictions. Even though students\nhave the required knowledge and skills, they are often based on a variety of\nintuitive reasoning that leads them to wrong conclusions. This research studies\nstudents' reasoning on science problems through heuristic - analytical thought\nprocesses (System 1 - System 2). System 1 operates automatically and quickly\nwith little or no effort and no sense of voluntary control, while System 2\nfocuses on the demanding mental activities that require it and is slow based on\nrules. Specifically, we seek to study those cognitive processes and information\navailable to students when they face science problems and, therefore, to\nexplore the various heuristic processes that students use when solving physics\nproblems. Our results indicated four intuitive heuristics in students' minds\nwhen they solve problems in Mechanics and especially in the unit projectile\nmotion: associative activation, processing fluency, attribute substitution and\nanchoring effect. These heuristics prevent students from applying knowledge and\nmethods that they already possess to solve a physics problem.",
            "author": [
                "Dimitrios Gousopoulos"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07158v1",
                "http://arxiv.org/pdf/2311.07158v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07627v1",
            "title": "A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph\n  Learning",
            "updated": "2023-11-13T08:42:17Z",
            "published": "2023-11-13T08:42:17Z",
            "summary": "The task of semi-supervised classification aims at assigning labels to all\nnodes of a graph based on the labels known for a few nodes, called the seeds.\nOne of the most popular algorithms relies on the principle of heat diffusion,\nwhere the labels of the seeds are spread by thermoconductance and the\ntemperature of each node at equilibrium is used as a score function for each\nlabel. In this paper, we prove that this algorithm is not consistent unless the\ntemperatures of the nodes at equilibrium are centered before scoring. This\ncrucial step does not only make the algorithm provably consistent on a block\nmodel but brings significant performance gains on real graphs.",
            "author": [
                "Thomas Bonald",
                "Nathan de Lara"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07627v1",
                "http://arxiv.org/pdf/2311.07627v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07144v1",
            "title": "Nuclear physics inputs for dense-matter modelling in neutron stars. The\n  nuclear equation of state",
            "updated": "2023-11-13T08:20:30Z",
            "published": "2023-11-13T08:20:30Z",
            "summary": "In this contribution, we briefly present the equation-of-state modelling for\napplication to neutron stars and discuss current constraints coming from\nnuclear physics theory and experiments. To assess the impact of model\nuncertainties, we employ a nucleonic meta-modelling approach and perform a\nBayesian analysis to generate posterior distributions for the equation of state\nwith filters accounting for both our present low-density nuclear physics\nknowledge and high-density neutron-star physics constraints. The global\nstructure of neutron stars thus predicted is discussed in connection with\nrecent astrophysical observations.",
            "author": [
                "A. F. Fantina",
                "F. Gulminelli"
            ],
            "link": [
                "http://dx.doi.org/10.1088/1742-6596/2586/1/012112",
                "http://arxiv.org/abs/2311.07144v1",
                "http://arxiv.org/pdf/2311.07144v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "nucl-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07129v1",
            "title": "Multi-Point Method using Effective Demodulation and Decomposition\n  Techniques allowing Identification of Disturbing Loads in Power Grids",
            "updated": "2023-11-13T07:51:20Z",
            "published": "2023-11-13T07:51:20Z",
            "summary": "The paper presents an innovative approach to the identification of sources of\nvoltage fluctuations in power networks, also considering the localization\nunderstood as the indication of supply points of disturbing loads. The\npresented approach considers disturbance sources that change their operating\nstate with a frequency higher than the power frequency. Implementation of the\nproposed solution is also proposed in such a way that its implementation in the\nsmart meter infrastructure allows for automatic localization of disturbance\nsources without additional expert knowledge. In the proposed approach, the\nmodulation signal is estimated using a carrier signal estimator, which allows\nfor the estimation of modulation signal with a frequency higher than the power\nfrequency. The estimated modulating signal is decomposed into component signals\nassociated with individual disturbing loads by decomposition by approximation\nusing pulse waves. The decomposition process allows for the estimation of\nselected parameters associated with disturbing loads, on the basis of which the\nassessment of propagation of voltage fluctuations associated with the impact of\nindividual disturbance sources is performed, which allows for the indication of\ntheir supply point. The proposed approach was verified in numerical simulation\nstudies using MATLAB/SIMULINK and in experimental studies carried out in a real\nlow-voltage power grid.",
            "author": [
                "Piotr Kuwa\u0142ek",
                "Grzegorz Wiczy\u0144ski"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07129v1",
                "http://arxiv.org/pdf/2311.07129v1"
            ],
            "primary_category": "eess.SP",
            "category": [
                "eess.SP",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07127v2",
            "title": "Untargeted Black-box Attacks for Social Recommendations",
            "updated": "2023-11-19T07:43:52Z",
            "published": "2023-11-13T07:40:23Z",
            "summary": "The rise of online social networks has facilitated the evolution of social\nrecommender systems, which incorporate social relations to enhance users'\ndecision-making process. With the great success of Graph Neural Networks in\nlearning node representations, GNN-based social recommendations have been\nwidely studied to model user-item interactions and user-user social relations\nsimultaneously. Despite their great successes, recent studies have shown that\nthese advanced recommender systems are highly vulnerable to adversarial\nattacks, in which attackers can inject well-designed fake user profiles to\ndisrupt recommendation performances. While most existing studies mainly focus\non targeted attacks to promote target items on vanilla recommender systems,\nuntargeted attacks to degrade the overall prediction performance are less\nexplored on social recommendations under a black-box scenario. To perform\nuntargeted attacks on social recommender systems, attackers can construct\nmalicious social relationships for fake users to enhance the attack\nperformance. However, the coordination of social relations and item profiles is\nchallenging for attacking black-box social recommendations. To address this\nlimitation, we first conduct several preliminary studies to demonstrate the\neffectiveness of cross-community connections and cold-start items in degrading\nrecommendations performance. Specifically, we propose a novel framework\nMultiattack based on multi-agent reinforcement learning to coordinate the\ngeneration of cold-start item profiles and cross-community social relations for\nconducting untargeted attacks on black-box social recommendations.\nComprehensive experiments on various real-world datasets demonstrate the\neffectiveness of our proposed attacking framework under the black-box setting.",
            "author": [
                "Wenqi Fan",
                "Shijie Wang",
                "Xiao-yong Wei",
                "Xiaowei Mei",
                "Qing Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07127v2",
                "http://arxiv.org/pdf/2311.07127v2"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07116v1",
            "title": "The ability of Lisa, Taiji, and their networks to detect the stochastic\n  gravitational wave background generated by Cosmic Strings",
            "updated": "2023-11-13T07:15:54Z",
            "published": "2023-11-13T07:15:54Z",
            "summary": "The cosmic string contributes to our understanding and revelation of the\nfundamental structure and evolutionary patterns of the universe, unifying our\nknowledge of the cosmos and unveiling new physical laws and phenomena.\nTherefore, we anticipate the detection of Stochastic Gravitational Wave\nBackground (SGWB) signals generated by cosmic strings in space-based detectors.\nWe have analyzed the detection capabilities of individual space-based\ndetectors, Lisa and Taiji, as well as the joint space-based detector network,\nLisa-Taiji, for SGWB signals produced by cosmic strings, taking into account\nother astronomical noise sources. The results indicate that the Lisa-Taiji\nnetwork exhibits superior capabilities in detecting SGWB signals generated by\ncosmic strings and can provide strong evidence. The Lisa-Taiji network can\nachieve an uncertainty estimation of $\\Delta G\\mu/G\\mu<0.5$ for cosmic string\ntension $G\\mu\\sim4\\times10^{-17}$, and can provide evidence for the presence of\nSGWB signals generated by cosmic strings at $G\\mu\\sim10^{-17}$, and strong\nevidence at $G\\mu\\sim10^{-16}$. Even in the presence of only SGWB signals, it\ncan achieve a relative uncertainty of $\\Delta G\\mu/G\\mu<0.5$ for cosmic string\ntension $G\\mu<10^{-18}$, and provide strong evidence at $G\\mu\\sim10^{-17}$.",
            "author": [
                "Bo-Rui Wang",
                "Jin Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07116v1",
                "http://arxiv.org/pdf/2311.07116v1"
            ],
            "primary_category": "astro-ph.CO",
            "category": [
                "astro-ph.CO",
                "astro-ph.IM",
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07105v1",
            "title": "Collaborative Goal Tracking of Multiple Mobile Robots Based on Geometric\n  Graph Neural Network",
            "updated": "2023-11-13T06:40:31Z",
            "published": "2023-11-13T06:40:31Z",
            "summary": "Multi-robot systems are widely used in spatially distributed tasks, and their\ncollaborative path planning is of great significance for working efficiency.\nCurrently, different multi-robot collaborative path planning methods have been\nproposed, but how to process the sensory information of neighboring robots at\ndifferent locations from a local perception perspective in real environment to\nmake better decisions is still a major difficulty. To address this problem,\nthis paper proposes a multi-robot collaborative path planning method based on\ngeometric graph neural network (GeoGNN). GeoGNN introduces the relative\nposition information of neighboring robots into each interaction layer of the\ngraph neural network to better integrate neighbor sensing information. An\nexpert data generation method is designed for the robot to advance in a single\nstep, by which expert data are generated in ROS to train the network.\nExperimental results show that the accuracy of the proposed method is improved\nby about 5% compared to the model based only on CNN on the expert data set. In\nROS simulation environment path planning test, the success rate is improved by\nabout 4% compared to CNN and flowtime increase is reduced about 8%, which\noutperforms other graph neural network models.",
            "author": [
                "Qingquan Lin",
                "Weining Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07105v1",
                "http://arxiv.org/pdf/2311.07105v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.MA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07079v2",
            "title": "Sample Dominance Aware Framework via Non-Parametric Estimation for\n  Spontaneous Brain-Computer Interface",
            "updated": "2023-11-15T02:51:08Z",
            "published": "2023-11-13T05:08:26Z",
            "summary": "Deep learning has shown promise in decoding brain signals, such as\nelectroencephalogram (EEG), in the field of brain-computer interfaces (BCIs).\nHowever, the non-stationary characteristics of EEG signals pose challenges for\ntraining neural networks to acquire appropriate knowledge. Inconsistent EEG\nsignals resulting from these non-stationary characteristics can lead to poor\nperformance. Therefore, it is crucial to investigate and address sample\ninconsistency to ensure robust performance in spontaneous BCIs. In this study,\nwe introduce the concept of sample dominance as a measure of EEG signal\ninconsistency and propose a method to modulate its effect on network training.\nWe present a two-stage dominance score estimation technique that compensates\nfor performance degradation caused by sample inconsistencies. Our proposed\nmethod utilizes non-parametric estimation to infer sample inconsistency and\nassigns each sample a dominance score. This score is then aggregated with the\nloss function during training to modulate the impact of sample inconsistency.\nFurthermore, we design a curriculum learning approach that gradually increases\nthe influence of inconsistent signals during training to improve overall\nperformance. We evaluate our proposed method using public spontaneous BCI\ndataset. The experimental results confirm that our findings highlight the\nimportance of addressing sample dominance for achieving robust performance in\nspontaneous BCIs.",
            "author": [
                "Byeong-Hoo Lee",
                "Byoung-Hee Kwon",
                "Seong-Whan Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07079v2",
                "http://arxiv.org/pdf/2311.07079v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07078v1",
            "title": "The Distribution of Sandpile Groups of Random Graphs with their Pairings",
            "updated": "2023-11-13T05:02:42Z",
            "published": "2023-11-13T05:02:42Z",
            "summary": "We determine the distribution of the sandpile group (also known as the\nJacobian) of the Erd\\H{o}s-R\\'{e}nyi random graph $G(n,q)$ along with its\ncanonical duality pairing as $n$ tends to infinity, fully resolving a\nconjecture from 2015 due to Clancy, Leake, and Payne and generalizing the\nresult by Wood on the groups. In particular, we show that a finite abelian\n$p$-group $G$ equipped with a perfect symmetric pairing $\\delta$ appears as the\nSylow $p$-part of the sandpile group and its pairing with frequency inversely\nproportional to $|G||\\mathrm{Aut}(G,\\delta)|$, where $\\mathrm{Aut}(G,\\delta)$\nis the set of automorphisms of $G$ preserving the pairing $\\delta$. While this\ndistribution is related to the Cohen-Lenstra distribution, the two\ndistributions are not the same on account of the additional algebraic data of\nthe pairing. The proof utilizes the moment method: we first compute a complete\nset of moments for our random variable (the average number of epimorphisms from\nour random object to a fixed object in the category of interest) and then show\nthe moments determine the distribution. To obtain the moments, we prove a\nuniversality result for the moments of cokernels of random symmetric integral\nmatrices whose dual groups are equipped with symmetric pairings that is strong\nenough to handle both the dependence in the diagonal entries and the additional\ndata of the pairing. We then apply results due to Sawin and Wood to show that\nthese moments determine a unique distribution.",
            "author": [
                "Eliot Hodges"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07078v1",
                "http://arxiv.org/pdf/2311.07078v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.NT",
                "math.PR",
                "05C80, 15B52, 60B20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07073v2",
            "title": "Exposition on over-squashing problem on GNNs: Current Methods,\n  Benchmarks and Challenges",
            "updated": "2023-11-17T22:51:23Z",
            "published": "2023-11-13T04:40:13Z",
            "summary": "Graph-based message-passing neural networks (MPNNs) have achieved remarkable\nsuccess in both node and graph-level learning tasks. However, several\nidentified problems, including over-smoothing (OSM), limited expressive power,\nand over-squashing (OSQ), still limit the performance of MPNNs. In particular,\nOSQ serves as the latest identified problem, where MPNNs gradually lose their\nlearning accuracy when long-range dependencies between graph nodes are\nrequired. In this work, we provide an exposition on the OSQ problem by\nsummarizing different formulations of OSQ from current literature, as well as\nthe three different categories of approaches for addressing the OSQ problem. In\naddition, we also discuss the alignment between OSQ and expressive power and\nthe trade-off between OSQ and OSM. Furthermore, we summarize the empirical\nmethods leveraged from existing works to verify the efficiency of OSQ\nmitigation approaches, with illustrations of their computational complexities.\nLastly, we list some open questions that are of interest for further\nexploration of the OSQ problem along with potential directions from the best of\nour knowledge.",
            "author": [
                "Dai Shi",
                "Andi Han",
                "Lequan Lin",
                "Yi Guo",
                "Junbin Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07073v2",
                "http://arxiv.org/pdf/2311.07073v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07056v1",
            "title": "Effective In-vehicle Intrusion Detection via Multi-view Statistical\n  Graph Learning on CAN Messages",
            "updated": "2023-11-13T03:49:55Z",
            "published": "2023-11-13T03:49:55Z",
            "summary": "As an important component of internet of vehicles (IoV), intelligent\nconnected vehicles (ICVs) have to communicate with external networks\nfrequently. In this case, the resource-constrained in-vehicle network (IVN) is\nfacing a wide variety of complex and changing external cyber-attacks,\nespecially the masquerade attack with high difficulty of detection while\nserious damaging effects that few counter measures can identify successfully.\nMoreover, only coarse-grained recognition can be achieved in current mainstream\nintrusion detection mechanisms, i.e., whether a whole data flow observation\nwindow contains attack labels rather than fine-grained recognition on every\nsingle data item within this window. In this paper, we propose StatGraph: an\nEffective Multi-view Statistical Graph Learning Intrusion Detection to\nimplement the fine-grained intrusion detection. Specifically, StatGraph\ngenerates two statistical graphs, timing correlation graph (TCG) and coupling\nrelationship graph (CRG), based on data streams. In given message observation\nwindows, edge attributes in TCGs represent temporal correlation between\ndifferent message IDs, while edge attributes in CRGs denote the neighbour\nrelationship and contextual similarity. Besides, a lightweight shallow layered\nGCN network is trained based graph property of TCGs and CRGs, which can learn\nthe universal laws of various patterns more effectively and further enhance the\nperformance of detection. To address the problem of insufficient attack types\nin previous intrusion detection, we select two real in-vehicle CAN datasets\nthat cover four new attacks never investigated before. Experimental result\nshows StatGraph improves both detection granularity and detection performance\nover state-of-the-art intrusion detection methods.",
            "author": [
                "Kai Wang",
                "Qiguang Jiang",
                "Bailing Wang",
                "Yongzheng Zhang",
                "Yulei Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07056v1",
                "http://arxiv.org/pdf/2311.07056v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI",
                "cs.AI",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07054v1",
            "title": "Do LLMs Implicitly Exhibit User Discrimination in Recommendation? An\n  Empirical Study",
            "updated": "2023-11-13T03:42:17Z",
            "published": "2023-11-13T03:42:17Z",
            "summary": "Recently, Large Language Models (LLMs) have enhanced user interaction,\nenabling seamless information retrieval and recommendations. However, concerns\nemerge as these LLMs have shown tendencies to display discrimination related to\nusers' sensitive characteristics (such as gender), leading to explicit user\nunfairness. Furthermore, our analysis uncovers a more discreet variant of bias\nin LLMs, defined as implicit user unfairness, wherein these models demonstrate\ndiscriminatory recommendation behaviors based solely on non-sensitive user\ndetails, like usernames or email addresses. This subtle form of unfairness,\nwhile more pervasive, poses a significant threat to the ethical integrity and\nrights of minority user groups. To comprehensively explore implicit user\nunfairness, our analysis unfolds in three key steps: (1) We uncover the reasons\nfor this implicit user unfairness: LLMs can infer users' sensitive attributes\nfrom non-sensitive attributes (e.g. user names) due to their extensive world\nknowledge. (2) Our findings expose that the magnitude of implicit user\nunfairness within LLMs surpasses the level of explicit user unfairness observed\nin traditional recommender models, signifying a more alarming issue of\nunfairness, i.e. some non-sensitive features of users like names may result in\nmore serious discrimination phenomena. (3) We analyze the long-term effect of\nimplicit user unfairness, identifying that it will reinforce information\nbubbles at an accelerated rate compared to traditional RS. We emphasize the\nneed to identify and mitigate implicit user unfairness, aiming to avert the\npotential human-LLMs recommendation systems deterioration.",
            "author": [
                "Chen Xu",
                "Wenjie Wang",
                "Yuxin Li",
                "Liang Pang",
                "Jun Xu",
                "Tat-Seng Chua"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07054v1",
                "http://arxiv.org/pdf/2311.07054v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07052v1",
            "title": "Towards the Law of Capacity Gap in Distilling Language Models",
            "updated": "2023-11-13T03:36:18Z",
            "published": "2023-11-13T03:36:18Z",
            "summary": "Language model (LM) distillation is a trending area that aims to distil the\nknowledge resided in a large teacher LM to a small student one. While various\nmethods have been proposed to push the distillation to its limits, it is still\na pain distilling LMs when a large capacity gap is exhibited between the\nteacher and the student LMs. The pain is mainly resulted by the curse of\ncapacity gap, which describes that a larger teacher LM cannot always lead to a\nbetter student LM than one distilled from a smaller teacher LM due to the\naffect of capacity gap increment. That is, there is likely an optimal point\nyielding the best student LM along the scaling course of the teacher LM. Even\nworse, the curse of capacity gap can be only partly yet not fully lifted as\nindicated in previous studies.\n  However, the tale is not ever one-sided. Although a larger teacher LM has\nbetter performance than a smaller teacher LM, it is much more\nresource-demanding especially in the context of recent large LMs (LLMs).\nConsequently, instead of sticking to lifting the curse, leaving the curse as is\nshould be arguably fine. Even better, in this paper, we reveal that the optimal\ncapacity gap is almost consistent across different student scales and\narchitectures, fortunately turning the curse into the law of capacity gap. The\nlaw later guides us to distil a 3B student LM (termed MiniMA) from a 7B teacher\nLM (adapted LLaMA2-7B). MiniMA is demonstrated to yield a new\ncompute-performance pareto frontier among existing 3B LMs on commonly used\nbenchmarks, and its instruction-tuned version (termed MiniChat) outperforms a\nwide range of 3B competitors in GPT4 evaluation and could even compete with\nseveral 7B chat models.",
            "author": [
                "Chen Zhang",
                "Dawei Song",
                "Zheyu Ye",
                "Yan Gao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07052v1",
                "http://arxiv.org/pdf/2311.07052v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07042v2",
            "title": "Open-Vocabulary Video Anomaly Detection",
            "updated": "2023-11-15T02:17:52Z",
            "published": "2023-11-13T02:54:17Z",
            "summary": "Video anomaly detection (VAD) with weak supervision has achieved remarkable\nperformance in utilizing video-level labels to discriminate whether a video\nframe is normal or abnormal. However, current approaches are inherently limited\nto a closed-set setting and may struggle in open-world applications where there\ncan be anomaly categories in the test data unseen during training. A few recent\nstudies attempt to tackle a more realistic setting, open-set VAD, which aims to\ndetect unseen anomalies given seen anomalies and normal videos. However, such a\nsetting focuses on predicting frame anomaly scores, having no ability to\nrecognize the specific categories of anomalies, despite the fact that this\nability is essential for building more informed video surveillance systems.\nThis paper takes a step further and explores open-vocabulary video anomaly\ndetection (OVVAD), in which we aim to leverage pre-trained large models to\ndetect and categorize seen and unseen anomalies. To this end, we propose a\nmodel that decouples OVVAD into two mutually complementary tasks --\nclass-agnostic detection and class-specific classification -- and jointly\noptimizes both tasks. Particularly, we devise a semantic knowledge injection\nmodule to introduce semantic knowledge from large language models for the\ndetection task, and design a novel anomaly synthesis module to generate pseudo\nunseen anomaly videos with the help of large vision generation models for the\nclassification task. These semantic knowledge and synthesis anomalies\nsubstantially extend our model's capability in detecting and categorizing a\nvariety of seen and unseen anomalies. Extensive experiments on three\nwidely-used benchmarks demonstrate our model achieves state-of-the-art\nperformance on OVVAD task.",
            "author": [
                "Peng Wu",
                "Xuerong Zhou",
                "Guansong Pang",
                "Yujia Sun",
                "Jing Liu",
                "Peng Wang",
                "Yanning Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07042v2",
                "http://arxiv.org/pdf/2311.07042v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07021v1",
            "title": "Making Distribution State Estimation Practical: Challenges and\n  Opportunities",
            "updated": "2023-11-13T02:12:32Z",
            "published": "2023-11-13T02:12:32Z",
            "summary": "In increasingly digitalized and metered distribution networks, state\nestimation is generally recognized as a key enabler of advanced network\nmanagement functionalities. However, despite decades of research, the real-life\nadoption of state estimation in distribution systems remains sporadic. This\nsystematization of knowledge paper discusses the cause for this while comparing\nindustrial and academic experiences and reviewing well- and less-established\nresearch directions. We argue that to make distribution system state estimation\nmore practical and applicable in the field, new perspectives are needed. In\nparticular, research should move away from conventional approaches and embrace\ngeneralized problem specifications and more comprehensive workflows. These, in\nturn, require algorithm advancements and more general mathematical\nformulations. We discuss lines of work to enable the delivery of tangible\nresearch.",
            "author": [
                "Frederik Geth",
                "Marta Vanin",
                "Werner Van Westering",
                "Terese Milford",
                "Amritanshu Pandey"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07021v1",
                "http://arxiv.org/pdf/2311.07021v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07019v1",
            "title": "Compact to extended Lyman-$\u03b1$ emitters in MAGPI: strong blue peak\n  emission at $z\\gtrsim3$",
            "updated": "2023-11-13T02:09:13Z",
            "published": "2023-11-13T02:09:13Z",
            "summary": "We report the discovery of three double-peaked Lyman-$\\alpha$ emitters (LAEs)\nexhibiting strong blue peak emission at 2.9 $\\lesssim z \\lesssim$ 4.8, in the\nVLT/MUSE data obtained as part of the Middle Ages Galaxy Properties with\nIntegral Field Spectroscopy (MAGPI) survey. These strong blue peak systems\nprovide a unique window into the scattering of Lyman-$\\alpha$ photons by\nneutral hydrogen (HI), suggesting gas inflows along the line-of-sight and low\nHI column density. Two of them at $z=2.9$ and $z=3.6$ are spatially extended\nhalos with their core regions clearly exhibiting stronger blue peak emissions\nthan the red peak. However, spatial variations in the peak ratio and peak\nseparation are evident over $25\\times 26$ kpc ($z=2.9$) and $19\\times28$ kpc\n($z=3.6$) regions in these extended halos. Notably, these systems do not fall\nin the regime of Lyman-$\\alpha$ blobs or nebulae. To the best of our knowledge,\nsuch a Lyman-$\\alpha$ halo with a dominant blue core has not been observed\npreviously. In contrast, the LAE at $z\\sim4.8$ is a compact system spanning a\n$9\\times9$ kpc region and stands as the highest-redshift strong blue peak\nemitter ever detected. The peak separation of the bright cores in these three\nsystems ranges from $\\Delta_{\\mathrm{peak}}\\sim370$ to $660$ km/s. The observed\noverall trend of decreasing peak separation with increasing radius is supposed\nto be controlled by HI column density and gas covering fraction. Based on\nvarious estimations, in contrast to the compact LAE, our halos are found to be\ngood candidates for LyC leakers. These findings shed light on the complex\ninterplay between Lyman-$\\alpha$ emission, gas kinematics, and ionising\nradiation properties, offering valuable insights into the evolution and nature\nof high-redshift galaxies.",
            "author": [
                "T. Mukherjee",
                "T. Zafar",
                "T. Nanayakkara",
                "E. Wisnioski",
                "A. Battisti",
                "A. Gupta",
                "C. D. P. Lagos",
                "K. E. Harborne",
                "C. Foster",
                "T. Mendel",
                "S. M. Croom",
                "A. Mailvaganam",
                "J. Prathap"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07019v1",
                "http://arxiv.org/pdf/2311.07019v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07016v1",
            "title": "Maximum Flow on Highly Dynamic Graphs",
            "updated": "2023-11-13T02:00:22Z",
            "published": "2023-11-13T02:00:22Z",
            "summary": "Recent advances in dynamic graph processing have enabled the analysis of\nhighly dynamic graphs with change at rates as high as millions of edge changes\nper second. Solutions in this domain, however, have been demonstrated only for\nrelatively simple algorithms like PageRank, breadth-first search, and connected\ncomponents. Expanding beyond this, we explore the maximum flow problem, a\nfundamental, yet more complex problem, in graph analytics. We propose a novel,\ndistributed algorithm for max-flow on dynamic graphs, and implement it on top\nof an asynchronous vertex-centric abstraction. We show that our algorithm can\nprocess both additions and deletions of vertices and edges efficiently at scale\non fast-evolving graphs, and provide a comprehensive analysis by evaluating, in\naddition to throughput, two criteria that are important when applied to\nreal-world problems: result latency and solution stability.",
            "author": [
                "Juntong Luo",
                "Scott Sallinen",
                "Matei Ripeanu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07016v1",
                "http://arxiv.org/pdf/2311.07016v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07014v1",
            "title": "Teach me with a Whisper: Enhancing Large Language Models for Analyzing\n  Spoken Transcripts using Speech Embeddings",
            "updated": "2023-11-13T01:53:12Z",
            "published": "2023-11-13T01:53:12Z",
            "summary": "Speech data has rich acoustic and paralinguistic information with important\ncues for understanding a speaker's tone, emotion, and intent, yet traditional\nlarge language models such as BERT do not incorporate this information. There\nhas been an increased interest in multi-modal language models leveraging audio\nand/or visual information and text. However, current multi-modal language\nmodels require both text and audio/visual data streams during inference/test\ntime. In this work, we propose a methodology for training language models\nleveraging spoken language audio data but without requiring the audio stream\nduring prediction time. This leads to an improved language model for analyzing\nspoken transcripts while avoiding an audio processing overhead at test time. We\nachieve this via an audio-language knowledge distillation framework, where we\ntransfer acoustic and paralinguistic information from a pre-trained speech\nembedding (OpenAI Whisper) teacher model to help train a student language model\non an audio-text dataset. In our experiments, the student model achieves\nconsistent improvement over traditional language models on tasks analyzing\nspoken transcripts.",
            "author": [
                "Fatema Hasan",
                "Yulong Li",
                "James Foulds",
                "Shimei Pan",
                "Bishwaranjan Bhattacharjee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07014v1",
                "http://arxiv.org/pdf/2311.07014v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.SD",
                "eess.AS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06993v1",
            "title": "State-of-the-Art Review and Synthesis: A Requirement-based Roadmap for\n  Standardized Predictive Maintenance Automation Using Digital Twin\n  Technologies",
            "updated": "2023-11-13T00:16:25Z",
            "published": "2023-11-13T00:16:25Z",
            "summary": "Recent digital advances have popularized predictive maintenance (PMx),\noffering enhanced efficiency, automation, accuracy, cost savings, and\nindependence in maintenance. Yet, it continues to face numerous limitations\nsuch as poor explainability, sample inefficiency of data-driven methods,\ncomplexity of physics-based methods, and limited generalizability and\nscalability of knowledge-based methods. This paper proposes leveraging Digital\nTwins (DTs) to address these challenges and enable automated PMx adoption at\nlarger scales. While we argue that DTs have this transformative potential, they\nhave not yet reached the level of maturity needed to bridge these gaps in a\nstandardized way. Without a standard definition for such evolution, this\ntransformation lacks a solid foundation upon which to base its development.\nThis paper provides a requirement-based roadmap supporting standardized PMx\nautomation using DT technologies. A systematic approach comprising two primary\nstages is presented. First, we methodically identify the Informational\nRequirements (IRs) and Functional Requirements (FRs) for PMx, which serve as a\nfoundation from which any unified framework must emerge. Our approach to\ndefining and using IRs and FRs to form the backbone of any PMx DT is supported\nby the track record of IRs and FRs being successfully used as blueprints in\nother areas, such as for product development within the software industry.\nSecond, we conduct a thorough literature review spanning fields to determine\nthe ways in which these IRs and FRs are currently being used within DTs,\nenabling us to point to the specific areas where further research is warranted\nto support the progress and maturation of requirement-based PMx DTs.",
            "author": [
                "Sizhe Ma",
                "Katherine A. Flanigan",
                "Mario Berg\u00e9s"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06993v1",
                "http://arxiv.org/pdf/2311.06993v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.SY",
                "eess.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06984v1",
            "title": "Pipelines and Beyond: Graph Types for ADTs with Futures",
            "updated": "2023-11-12T23:12:56Z",
            "published": "2023-11-12T23:12:56Z",
            "summary": "Parallel programs are frequently modeled as dependency or cost graphs, which\ncan be used to detect various bugs, or simply to visualize the parallel\nstructure of the code. However, such graphs reflect just one particular\nexecution and are typically constructed in a post-hoc manner. Graph types,\nwhich were introduced recently to mitigate this problem, can be assigned\nstatically to a program by a type system and compactly represent the family of\nall graphs that could result from the program. Unfortunately, prior work is\nrestricted in its treatment of futures, an increasingly common and especially\ndynamic form of parallelism. In short, each instance of a future must be\nstatically paired with a vertex name. Previously, this led to the restriction\nthat futures could not be placed in collections or be used to construct data\nstructures. Doing so is not a niche exercise: such structures form the basis of\nnumerous algorithms that use forms of pipelining to achieve performance not\nattainable without futures. All but the most limited of these examples are out\nof reach of prior graph type systems. In this paper, we propose a graph type\nsystem that allows for almost arbitrary combinations of futures and recursive\ndata types. We do so by indexing datatypes with a type-level vertex structure,\na codata structure that supplies unique vertex names to the futures in a data\nstructure. We prove the soundness of the system in a parallel core calculus\nannotated with vertex structures and associated operations. Although the\ncalculus is annotated, this is merely for convenience in defining the type\nsystem. We prove that it is possible to annotate arbitrary recursive types with\nvertex structures, and show using a prototype inference engine that these\nannotations can be inferred from OCaml-like source code for several complex\nparallel algorithms.",
            "author": [
                "Francis Rinaldi",
                "june wunder",
                "Arthur Aevedo De Amorim",
                "Stefan K. Muller"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06984v1",
                "http://arxiv.org/pdf/2311.06984v1"
            ],
            "primary_category": "cs.PL",
            "category": [
                "cs.PL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06946v3",
            "title": "Performance of a Large Multimodal Model-based chatbot on the Test of\n  Understanding Graphs in Kinematics",
            "updated": "2023-11-22T13:40:44Z",
            "published": "2023-11-12T20:15:13Z",
            "summary": "The well-known artificial intelligence-based chatbot ChatGPT has recently\nbecome able to process image data as input. We investigated its performance on\nthe Test of Understanding Graphs in Kinematics (TUG-K) with the purpose of\ninforming the physics education community of the current potential of using\nChatGPT in the education process, particularly on tasks that involve graphical\ninterpretation. We used Robert Taylor's three-roles framework to guide our\nanalysis and frame our findings in terms of their educational implications. We\nfound that ChatGPT, on average, performed similarly to students at the high\nschool level, but with significant differences in the distribution of the\ncorrectness of its responses, as well as in terms of the displayed \"reasoning\"\nand \"visual\" abilities. While ChatGPT was very successful at proposing\nproductive strategies for solving the tasks on the test and expressed correct\n\"reasoning\" in most of its responses, it had difficulties correctly \"seeing\"\ngraphs. We suggest that, based on its performance, it would not be advisable to\nuse it in the role of a tutor, a model of a student, or a tool for assisting\nvision-impaired persons in the context of kinematics graphs.",
            "author": [
                "Giulia Polverini",
                "Bor Gregorcic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06946v3",
                "http://arxiv.org/pdf/2311.06946v3"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06942v1",
            "title": "Contractive Systems Improve Graph Neural Networks Against Adversarial\n  Attacks",
            "updated": "2023-11-12T20:06:48Z",
            "published": "2023-11-12T20:06:48Z",
            "summary": "Graph Neural Networks (GNNs) have established themselves as a key component\nin addressing diverse graph-based tasks. Despite their notable successes, GNNs\nremain susceptible to input perturbations in the form of adversarial attacks.\nThis paper introduces an innovative approach to fortify GNNs against\nadversarial perturbations through the lens of contractive dynamical systems.\nOur method introduces graph neural layers based on differential equations with\ncontractive properties, which, as we show, improve the robustness of GNNs. A\ndistinctive feature of the proposed approach is the simultaneous learned\nevolution of both the node features and the adjacency matrix, yielding an\nintrinsic enhancement of model robustness to perturbations in the input\nfeatures and the connectivity of the graph. We mathematically derive the\nunderpinnings of our novel architecture and provide theoretical insights to\nreason about its expected behavior. We demonstrate the efficacy of our method\nthrough numerous real-world benchmarks, reading on par or improved performance\ncompared to existing methods.",
            "author": [
                "Moshe Eliasof",
                "Davide Murari",
                "Ferdia Sherry",
                "Carola-Bibiane Sch\u00f6nlieb"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06942v1",
                "http://arxiv.org/pdf/2311.06942v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06941v1",
            "title": "Kepler Multi-Transiting Systems Physical Properties and Impact Parameter\n  Variations",
            "updated": "2023-11-12T20:06:10Z",
            "published": "2023-11-12T20:06:10Z",
            "summary": "We fit a dynamical model to Kepler systems that contain four or more\ntransiting planets using the analytic method AnalyticLC, and obtain physical\nand orbital parameters for 101 planets in 23 systems, of which 95 are of mass\nsignificance better than 3 sigma, and 46 are without previously reported mass\nconstraints nor upper limits. In addition, we compile a list of 71 KOIs that\ndisplay significant Impact Parameter Variations (TbVs), complementing our\npreviously published work on two- and three-transiting planet systems.\nTogether, these works include the detection of significant TbV signals of 130\nplanets, which is, to our knowledge, the largest catalog of this type to date.\nThe results indicate that the typical detectable TbV rate in the Kepler\npopulation is of order 10^{-2} yr^{-1}, and that rapid TbV rates (>~0.05\nyr^{-1}) are observed only in systems that contain a transiting planet of an\norbital period less than ~20 days. The observed TbV rates are only weakly\ncorrelated with orbital period within Kepler's <~100 days-period planets. If\nthis extends to longer periods, it implies a limit on the utility of the\ntransit technique for long-period planets. The TbVs we find may not be\ndetectable in direct impact parameter measurements but rather are inferred from\nthe full dynamics of the system, encoded in all types of transit variations.\nFinally, we find evidence that the mutual inclinations distribution is\nqualitatively consistent with the previously suggested AMD (angular momentum\ndeficit) model using an independent approach.",
            "author": [
                "Yair Judkovsky",
                "Aviv Ofir",
                "Oded Aharonson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06941v1",
                "http://arxiv.org/pdf/2311.06941v1"
            ],
            "primary_category": "astro-ph.EP",
            "category": [
                "astro-ph.EP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06911v1",
            "title": "More Than 50% Of The Time, Users Detect Real SMS as Fake: A Smishing\n  Detection Study Of US Population",
            "updated": "2023-11-12T17:56:42Z",
            "published": "2023-11-12T17:56:42Z",
            "summary": "With the booming popularity of smartphones, threats related to these devices\nare increasingly on the rise. Smishing, a combination of SMS (Short Message\nService) and phishing has emerged as a treacherous cyber threat used by\nmalicious actors to deceive users, aiming to steal sensitive information, money\nor install malware on their mobile devices. Despite the increase in smishing\nattacks in recent years, there are very few studies aimed at understanding the\nfactors that contribute to a user's ability to differentiate real from fake\nmessages.\n  To address this gap in knowledge, we have conducted an online survey on\nsmishing detection with 214 participants. In this study, we presented them with\n16 SMS screenshots and evaluated how different factors affect their decision\nmaking process in smishing detection. Next, we conducted a follow-up survey to\ngarner information on the participants' security attitudes, behavior and\nknowledge. Our results highlighted that attention and security behavioral\nscores had a significant impact on participants' accuracy identifying smishing\nmessages. Interestingly, we found that participants had more difficulty\nidentifying real messages from fake ones, with an accuracy of 64% with fake\nmessages and 46\\% with real messages. Understanding these factors is pivotal to\nbolstering users' resilience against these threats and to create a safer\ndigital environment.",
            "author": [
                "Daniel Timko",
                "Daniel Hernandez Castillo",
                "Muhammad Lutfor Rahman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06911v1",
                "http://arxiv.org/pdf/2311.06911v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06888v1",
            "title": "Preserving Node-level Privacy in Graph Neural Networks",
            "updated": "2023-11-12T16:21:29Z",
            "published": "2023-11-12T16:21:29Z",
            "summary": "Differential privacy (DP) has seen immense applications in learning on\ntabular, image, and sequential data where instance-level privacy is concerned.\nIn learning on graphs, contrastingly, works on node-level privacy are highly\nsparse. Challenges arise as existing DP protocols hardly apply to the\nmessage-passing mechanism in Graph Neural Networks (GNNs).\n  In this study, we propose a solution that specifically addresses the issue of\nnode-level privacy. Our protocol consists of two main components: 1) a sampling\nroutine called HeterPoisson, which employs a specialized node sampling strategy\nand a series of tailored operations to generate a batch of sub-graphs with\ndesired properties, and 2) a randomization routine that utilizes symmetric\nmultivariate Laplace (SML) noise instead of the commonly used Gaussian noise.\nOur privacy accounting shows this particular combination provides a non-trivial\nprivacy guarantee. In addition, our protocol enables GNN learning with good\nperformance, as demonstrated by experiments on five real-world datasets;\ncompared with existing baselines, our method shows significant advantages,\nespecially in the high privacy regime. Experimentally, we also 1) perform\nmembership inference attacks against our protocol and 2) apply privacy audit\ntechniques to confirm our protocol's privacy integrity.\n  In the sequel, we present a study on a seemingly appealing approach\n\\cite{sajadmanesh2023gap} (USENIX'23) that protects node-level privacy via\ndifferentially private node/instance embeddings. Unfortunately, such work has\nfundamental privacy flaws, which are identified through a thorough case study.\nMore importantly, we prove an impossibility result of achieving both (strong)\nprivacy and (acceptable) utility through private instance embedding. The\nimplication is that such an approach has intrinsic utility barriers when\nenforcing differential privacy.",
            "author": [
                "Zihang Xiang",
                "Tianhao Wang",
                "Di Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06888v1",
                "http://arxiv.org/pdf/2311.06888v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06886v1",
            "title": "Hitting probabilities and uniformly $S$-transient subgraphs",
            "updated": "2023-11-12T16:18:52Z",
            "published": "2023-11-12T16:18:52Z",
            "summary": "We study the probability that a random walk started inside a subgraph of a\nlarger graph exits that subgraph (or, equivalently, hits the exterior boundary\nof the subgraph). Considering the chance a random walk started in the subgraph\nnever leaves the subgraph leads to a notion we call \"survival\" transience, or\n$S$-transience. In the case where the heat kernel of the larger graph satisfies\ntwo-sided Gaussian estimates, we prove an upper bound on the probability of\nhitting the boundary of the subgraph. Under the additional hypothesis that the\nsubgraph is inner uniform, we prove a two-sided estimate for this probability.\nThe estimate depends upon a harmonic function in the subgraph. We also provide\ntwo-sided estimates for related probabilities, such as the harmonic measure\n(the chance the walk exits the subgraph at a particular point on its boundary).",
            "author": [
                "Emily Dautenhahn",
                "Laurent Saloff-Coste"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06886v1",
                "http://arxiv.org/pdf/2311.06886v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "60J10, 60G50"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06879v1",
            "title": "pFedES: Model Heterogeneous Personalized Federated Learning with Feature\n  Extractor Sharing",
            "updated": "2023-11-12T15:43:39Z",
            "published": "2023-11-12T15:43:39Z",
            "summary": "As a privacy-preserving collaborative machine learning paradigm, federated\nlearning (FL) has attracted significant interest from academia and the industry\nalike. To allow each data owner (a.k.a., FL clients) to train a heterogeneous\nand personalized local model based on its local data distribution, system\nresources and requirements on model structure, the field of model-heterogeneous\npersonalized federated learning (MHPFL) has emerged. Existing MHPFL approaches\neither rely on the availability of a public dataset with special\ncharacteristics to facilitate knowledge transfer, incur high computation and\ncommunication costs, or face potential model leakage risks. To address these\nlimitations, we propose a model-heterogeneous personalized Federated learning\napproach based on feature Extractor Sharing (pFedES). It incorporates a small\nhomogeneous feature extractor into each client's heterogeneous local model.\nClients train them via the proposed iterative learning method to enable the\nexchange of global generalized knowledge and local personalized knowledge. The\nsmall local homogeneous extractors produced after local training are uploaded\nto the FL server and for aggregation to facilitate easy knowledge sharing among\nclients. We theoretically prove that pFedES can converge over wall-to-wall\ntime. Extensive experiments on two real-world datasets against six\nstate-of-the-art methods demonstrate that pFedES builds the most accurate\nmodel, while incurring low communication and computation costs. Compared with\nthe best-performing baseline, it achieves 1.61% higher test accuracy, while\nreducing communication and computation costs by 99.6% and 82.9%, respectively.",
            "author": [
                "Liping Yi",
                "Han Yu",
                "Gang Wang",
                "Xiaoguang Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06879v1",
                "http://arxiv.org/pdf/2311.06879v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06875v1",
            "title": "Modularity of very dense graphs",
            "updated": "2023-11-12T15:26:51Z",
            "published": "2023-11-12T15:26:51Z",
            "summary": "It is known that complete graphs and complete bipartite graphs have\nmodularity zero. We show that the least number of edges we may delete from the\ncomplete graph $K_n$ to obtain a graph with non-zero modularity is $\\lfloor\nn/2\\rfloor +1$. Similarly we determine the least number of edges we may delete\nfrom or add to a complete bipartite graph to reach non-zero modularity.\n  We also analyse the modularity of very dense random graphs, and in particular\nwe find that there is a transition to modularity zero when the average degree\nof the complementary graph drops below 1.",
            "author": [
                "Colin McDiarmid",
                "Fiona Skerman"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06875v1",
                "http://arxiv.org/pdf/2311.06875v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06858v1",
            "title": "Can Large Language Models Augment a Biomedical Ontology with missing\n  Concepts and Relations?",
            "updated": "2023-11-12T14:20:55Z",
            "published": "2023-11-12T14:20:55Z",
            "summary": "Ontologies play a crucial role in organizing and representing knowledge.\nHowever, even current ontologies do not encompass all relevant concepts and\nrelationships. Here, we explore the potential of large language models (LLM) to\nexpand an existing ontology in a semi-automated fashion. We demonstrate our\napproach on the biomedical ontology SNOMED-CT utilizing semantic relation types\nfrom the widely used UMLS semantic network. We propose a method that uses\nconversational interactions with an LLM to analyze clinical practice guidelines\n(CPGs) and detect the relationships among the new medical concepts that are not\npresent in SNOMED-CT. Our initial experimentation with the conversational\nprompts yielded promising preliminary results given a manually generated gold\nstandard, directing our future potential improvements.",
            "author": [
                "Antonio Zaitoun",
                "Tomer Sagi",
                "Szymon Wilk",
                "Mor Peleg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06858v1",
                "http://arxiv.org/pdf/2311.06858v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06842v1",
            "title": "Unfolded Adinkra Properties of Supermultiplets (I)",
            "updated": "2023-11-12T13:33:37Z",
            "published": "2023-11-12T13:33:37Z",
            "summary": "Adinkra networks arise in the Carroll limit of supersymmetric QFT. Extensions\nof adinkras that are infinite dimensional graphs have never previously been\ndiscussed in the literature. We call these \"infinite unfolded'' adinkras and\nstudy the properties of their realization on familiar 4D, $\\cal N$ = 1\nsupermultiplets. A new feature in \"unfolded'' adinkras is the appearance of\nquantities whose actions resemble BRST operators within Verma-like modules. New\n\"net-centric\" quantities ${\\widetilde \\chi}_{(1)}$ and ${\\widetilde\n\\chi}_{(2)}$ are introduced, which along with quantity $\\chi_{\\rm o}$, describe\ndistinctions between familiar supermultiplets in 4D, $\\cal N $ = 1 theories. A\npreviously unobserved property in all adinkras that we call \"adinkra vorticity\"\nis noted.",
            "author": [
                "Aleksander J. Cianciara",
                "S. James Gates, Jr.",
                "Youngik",
                "Lee",
                "Ethan T. Levy",
                "Tarek O. Razzaz",
                "Jacob Richardson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06842v1",
                "http://arxiv.org/pdf/2311.06842v1"
            ],
            "primary_category": "hep-th",
            "category": [
                "hep-th"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06837v1",
            "title": "GraNNDis: Efficient Unified Distributed Training Framework for Deep GNNs\n  on Large Clusters",
            "updated": "2023-11-12T13:30:31Z",
            "published": "2023-11-12T13:30:31Z",
            "summary": "Graph neural networks (GNNs) are one of the most rapidly growing fields\nwithin deep learning. According to the growth in the dataset and the model size\nused for GNNs, an important problem is that it becomes nearly impossible to\nkeep the whole network on GPU memory. Among numerous attempts, distributed\ntraining is one popular approach to address the problem. However, due to the\nnature of GNNs, existing distributed approaches suffer from poor scalability,\nmainly due to the slow external server communications.\n  In this paper, we propose GraNNDis, an efficient distributed GNN training\nframework for training GNNs on large graphs and deep layers. GraNNDis\nintroduces three new techniques. First, shared preloading provides a training\nstructure for a cluster of multi-GPU servers. We suggest server-wise preloading\nof essential vertex dependencies to reduce the low-bandwidth external server\ncommunications. Second, we present expansion-aware sampling. Because shared\npreloading alone has limitations because of the neighbor explosion,\nexpansion-aware sampling reduces vertex dependencies that span across server\nboundaries. Third, we propose cooperative batching to create a unified\nframework for full-graph and minibatch training. It significantly reduces\nredundant memory usage in mini-batch training. From this, GraNNDis enables a\nreasonable trade-off between full-graph and mini-batch training through\nunification especially when the entire graph does not fit into the GPU memory.\nWith experiments conducted on a multi-server/multi-GPU cluster, we show that\nGraNNDis provides superior speedup over the state-of-the-art distributed GNN\ntraining frameworks.",
            "author": [
                "Jaeyong Song",
                "Hongsun Jang",
                "Jaewon Jung",
                "Youngsok Kim",
                "Jinho Lee"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06837v1",
                "http://arxiv.org/pdf/2311.06837v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.DC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06835v1",
            "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
            "updated": "2023-11-12T13:25:28Z",
            "published": "2023-11-12T13:25:28Z",
            "summary": "This paper considers an under-explored Graph Anomaly Detection (GAD) task,\nnamely open-set GAD, which aims to detect anomalous nodes using a small number\nof labelled training normal and anomaly nodes (known as seen anomalies) that\ncannot illustrate all possible inference-time abnormalities. The task has\nattracted growing attention due to the availability of anomaly prior knowledge\nfrom the label information that can help to substantially reduce detection\nerrors. However, current methods tend to over-emphasise fitting the seen\nanomalies, leading to a weak generalisation ability to detect unseen anomalies,\ni.e., those that are not illustrated by the labelled anomaly nodes. Further,\nthey were introduced to handle Euclidean data, failing to effectively capture\nimportant non-Euclidean features for GAD. In this work, we propose a novel\nopen-set GAD approach, namely normal structure regularisation (NSReg), to\nleverage the rich normal graph structure embedded in the labelled nodes to\ntackle the aforementioned two issues. In particular, NSReg trains an\nanomaly-discriminative supervised graph anomaly detector, with a plug-and-play\nregularisation term to enforce compact, semantically-rich representations of\nnormal nodes. To this end, the regularisation is designed to differentiate\nvarious types of normal nodes, including labelled normal nodes that are\nconnected in their local neighbourhood, and those that are not connected. By\ndoing so, it helps incorporate strong normality into the supervised anomaly\ndetector learning, mitigating their overfitting to the seen anomalies.\nExtensive empirical results on real-world datasets demonstrate the superiority\nof our proposed NSReg for open-set GAD.",
            "author": [
                "Qizhou Wang",
                "Guansong Pang",
                "Mahsa Salehi",
                "Wray Buntine",
                "Christopher Leckie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06835v1",
                "http://arxiv.org/pdf/2311.06835v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06833v1",
            "title": "The spectral radius of minor free graphs",
            "updated": "2023-11-12T13:09:46Z",
            "published": "2023-11-12T13:09:46Z",
            "summary": "In this paper, we present a sharp upper bound for the spectral radius of an\n$n$-vertex graph without $F$-minor for sufficient large $n$, where $F$ is\nobtained from the complete graph $K_r$ by deleting disjointed paths.\nFurthermore, the graphs which achieved the sharp bound are characterized. This\nresult may be regarded to be an extended revision of the number of edges in an\n$n$-vertex graph without $F$-minor.",
            "author": [
                "Ming-Zhu Chen",
                "A-Ming Liu",
                "Xiao-Dong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06833v1",
                "http://arxiv.org/pdf/2311.06833v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C50, 05C35, 05C83"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06810v1",
            "title": "On the trace-zero doubly stochastic matrices of order 5",
            "updated": "2023-11-12T11:16:28Z",
            "published": "2023-11-12T11:16:28Z",
            "summary": "We propose a graph theoretic approach to determine trace of product of two\npermutation matrices through a weighted digraph representation of the\npermutation matrices. Consequently, we derive trace-zero doubly stochastic (DS)\nmatrices of order $5$ whose $k$-th power is also a trace-zero DS matrix for\n$k\\in\\{2,3,4,5\\}$. Then, we determine necessary conditions for the coefficients\nof a generic polynomial of degree $5$ to be realizable as the characteristic\npolynomial of a trace-zero DS matrix of order $5$. Finally, we approximate the\neigenvalue region of trace-zero DS matrices of order $5.$",
            "author": [
                "Amrita Mandal",
                "Bibhas Adhikari"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06810v1",
                "http://arxiv.org/pdf/2311.06810v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06805v1",
            "title": "Tunable Soft Prompts are Messengers in Federated Learning",
            "updated": "2023-11-12T11:01:10Z",
            "published": "2023-11-12T11:01:10Z",
            "summary": "Federated learning (FL) enables multiple participants to collaboratively\ntrain machine learning models using decentralized data sources, alleviating\nprivacy concerns that arise from directly sharing local data. However, the lack\nof model privacy protection in FL becomes an unneglectable challenge,\nespecially when people want to federally finetune models based on a proprietary\nlarge language model. In this study, we propose a novel FL training approach\nthat accomplishes information exchange among participants via tunable soft\nprompts. These soft prompts, updated and transmitted between the server and\nclients, assume the role of the global model parameters and serve as messengers\nto deliver useful knowledge from the local data and global model. As the global\nmodel itself is not required to be shared and the local training is conducted\nbased on an auxiliary model with fewer parameters than the global model, the\nproposed approach provides protection for the global model while reducing\ncommunication and computation costs in FL. Extensive experiments show the\neffectiveness of the proposed approach compared to several baselines. We have\nreleased the source code at\n\\url{https://github.com/alibaba/FederatedScope/tree/fedsp/federatedscope/nlp/fedsp}.",
            "author": [
                "Chenhe Dong",
                "Yuexiang Xie",
                "Bolin Ding",
                "Ying Shen",
                "Yaliang Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06805v1",
                "http://arxiv.org/pdf/2311.06805v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06785v1",
            "title": "Depth and Breadth of Research Area Coverage and Its Impact on\n  Publication Citation: An Analysis of Bibliometric Papers",
            "updated": "2023-11-12T09:20:52Z",
            "published": "2023-11-12T09:20:52Z",
            "summary": "Many other factors affecting citation of publications, except for research\narea coverage, have been studied. This study aims to investigate impact of\nresearch area coverage. Bibliometric papers and their related papers (referred\npapers, citing papers and first author's papers) were screened and matched by\nPython program. Papers' research areas were classified according to Web of\nScience. Bibliometric parameters of the most cited 5% and the least cited 5%\npapers were compared. Firstly, coverage of related papers' research areas\nimpacts the citation of their original papers. The impact of references and\nciting papers are positive and negative, separately, while the first author's\npapers have no influence. Secondly, high-influence papers tend to cite\nreferences from a wider area and are cited by followers from a wider area.\nAdditionally, the pattern of knowledge flow differs significantly between high-\nand low-influence papers. Low-influence papers narrow knowledge flow, whereas\nhigh-influence papers broaden it. This study has shown that both depth and\nbreadth of research area coverage can influence citations. It is recommended\nthat authors should extensively cite high-influence publications, both within\nand beyond their own area.",
            "author": [
                "Zhuoran Lin",
                "Yun Wang",
                "Hongjun Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06785v1",
                "http://arxiv.org/pdf/2311.06785v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06777v1",
            "title": "Alleviating Behavior Data Imbalance for Multi-Behavior Graph\n  Collaborative Filtering",
            "updated": "2023-11-12T08:46:07Z",
            "published": "2023-11-12T08:46:07Z",
            "summary": "Graph collaborative filtering, which learns user and item representations\nthrough message propagation over the user-item interaction graph, has been\nshown to effectively enhance recommendation performance. However, most current\ngraph collaborative filtering models mainly construct the interaction graph on\na single behavior domain (e.g. click), even though users exhibit various types\nof behaviors on real-world platforms, including actions like click, cart, and\npurchase. Furthermore, due to variations in user engagement, there exists an\nimbalance in the scale of different types of behaviors. For instance, users may\nclick and view multiple items but only make selective purchases from a small\nsubset of them. How to alleviate the behavior imbalance problem and utilize\ninformation from the multiple behavior graphs concurrently to improve the\ntarget behavior conversion (e.g. purchase) remains underexplored. To this end,\nwe propose IMGCF, a simple but effective model to alleviate behavior data\nimbalance for multi-behavior graph collaborative filtering. Specifically, IMGCF\nutilizes a multi-task learning framework for collaborative filtering on\nmulti-behavior graphs. Then, to mitigate the data imbalance issue, IMGCF\nimproves representation learning on the sparse behavior by leveraging\nrepresentations learned from the behavior domain with abundant data volumes.\nExperiments on two widely-used multi-behavior datasets demonstrate the\neffectiveness of IMGCF.",
            "author": [
                "Yijie Zhang",
                "Yuanchen Bei",
                "Shiqi Yang",
                "Hao Chen",
                "Zhiqing Li",
                "Lijia Chen",
                "Feiran Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06777v1",
                "http://arxiv.org/pdf/2311.06777v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06766v1",
            "title": "Enhancing Control Performance through ESN-Based Model Compensation in\n  MPC for Dynamic Systems",
            "updated": "2023-11-12T08:06:42Z",
            "published": "2023-11-12T08:06:42Z",
            "summary": "Deriving precise system dynamic models through traditional numerical methods\nis often a challenging endeavor. The performance of Model Predictive Control is\nheavily contingent on the accuracy of the system dynamic model. Consequently,\nthis study employs Echo State Networks to acquire knowledge of the unmodeled\ndynamic characteristics inherent in the system. This information is then\nintegrated with the nominal model, functioning as a form of model compensation.\nThe present paper introduces a control framework that combines ESN with MPC. By\nperpetually assimilating the disparities between the nominal and real models,\ncontrol performance experiences augmentation. In a demonstrative example, a\nsecond order dynamic system is subjected to simulation. The outcomes\nconclusively evince that ESNbased MPC adeptly assimilates unmodeled dynamic\nattributes, thereby elevating the system control proficiency.",
            "author": [
                "Shuai Niu",
                "Qing Sun",
                "Minrui Fei",
                "Xuqian Ju"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06766v1",
                "http://arxiv.org/pdf/2311.06766v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07617v1",
            "title": "CLAMP: A Contrastive Language And Molecule Pre-training Network",
            "updated": "2023-11-12T07:45:35Z",
            "published": "2023-11-12T07:45:35Z",
            "summary": "This paper highlights a shift in how to approach material generation. Instead\nof material-to-material, we propose a language-to-material generation\narchitecture that utilizes millions of untapped data points. Using a web\nscraper to collect crystal text pairs from open-source research papers, a\ncontrastive model can be trained using a convolutional graph neural network\nencoder and a language encoder. This would allow unsupervised zero-shot\nclassification which can be trained by taking advantage of linguistic\nstructure. Without any specific training data, an ~82\\% accuracy was achieved\nand ~75\\% accuracy for photocatalyst prediction with an extremely small\ndataset. This novel network could ideally be cross-applied to any reaction that\ncan be described via text, opening completely new methods to think about 3D\nchemical framework generation. In the full experiment diffusion models would\nlikely be incorporated to fully exploit the latent space.",
            "author": [
                "Neel Redkar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07617v1",
                "http://arxiv.org/pdf/2311.07617v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.IR",
                "8.2.D.2.5"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06761v1",
            "title": "Learning Knowledge-Enhanced Contextual Language Representations for\n  Domain Natural Language Understanding",
            "updated": "2023-11-12T07:37:24Z",
            "published": "2023-11-12T07:37:24Z",
            "summary": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the\nperformance of various downstream NLP tasks by injecting knowledge facts from\nlarge-scale Knowledge Graphs (KGs). However, existing methods for pre-training\nKEPLMs with relational triples are difficult to be adapted to close domains due\nto the lack of sufficient domain graph semantics. In this paper, we propose a\nKnowledge-enhanced lANGuAge Representation learning framework for various\nclOsed dOmains (KANGAROO) via capturing the implicit graph structure among the\nentities. Specifically, since the entity coverage rates of closed-domain KGs\ncan be relatively low and may exhibit the global sparsity phenomenon for\nknowledge injection, we consider not only the shallow relational\nrepresentations of triples but also the hyperbolic embeddings of deep\nhierarchical entity-class structures for effective knowledge fusion.Moreover,\nas two closed-domain entities under the same entity-class often have locally\ndense neighbor subgraphs counted by max point biconnected component, we further\npropose a data augmentation strategy based on contrastive learning over\nsubgraphs to construct hard negative samples of higher quality. It makes the\nunderlying KELPMs better distinguish the semantics of these neighboring\nentities to further complement the global semantic sparsity. In the\nexperiments, we evaluate KANGAROO over various knowledge-aware and general NLP\ntasks in both full and few-shot learning settings, outperforming various KEPLM\ntraining paradigms performance in closed-domains significantly.",
            "author": [
                "Ruyao Xu",
                "Taolin Zhang",
                "Chengyu Wang",
                "Zhongjie Duan",
                "Cen Chen",
                "Minghui Qiu",
                "Dawei Cheng",
                "Xiaofeng He",
                "Weining Qian"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06761v1",
                "http://arxiv.org/pdf/2311.06761v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06758v1",
            "title": "Sharing, Teaching and Aligning: Knowledgeable Transfer Learning for\n  Cross-Lingual Machine Reading Comprehension",
            "updated": "2023-11-12T07:20:37Z",
            "published": "2023-11-12T07:20:37Z",
            "summary": "In cross-lingual language understanding, machine translation is often\nutilized to enhance the transferability of models across languages, either by\ntranslating the training data from the source language to the target, or from\nthe target to the source to aid inference. However, in cross-lingual machine\nreading comprehension (MRC), it is difficult to perform a deep level of\nassistance to enhance cross-lingual transfer because of the variation of answer\nspan positions in different languages. In this paper, we propose X-STA, a new\napproach for cross-lingual MRC. Specifically, we leverage an attentive teacher\nto subtly transfer the answer spans of the source language to the answer output\nspace of the target. A Gradient-Disentangled Knowledge Sharing technique is\nproposed as an improved cross-attention block. In addition, we force the model\nto learn semantic alignments from multiple granularities and calibrate the\nmodel outputs with teacher guidance to enhance cross-lingual transferability.\nExperiments on three multi-lingual MRC datasets show the effectiveness of our\nmethod, outperforming state-of-the-art approaches.",
            "author": [
                "Tingfeng Cao",
                "Chengyu Wang",
                "Chuanqi Tan",
                "Jun Huang",
                "Jinhui Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06758v1",
                "http://arxiv.org/pdf/2311.06758v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06747v1",
            "title": "Graph Signal Processing For Cancer Gene Co-Expression Network Analysis",
            "updated": "2023-11-12T06:03:13Z",
            "published": "2023-11-12T06:03:13Z",
            "summary": "Cancer heterogeneity arises from complex molecular interactions. Elucidating\nsystems-level properties of gene interaction networks distinguishing cancer\nfrom normal cells is critical for understanding disease mechanisms and\ndeveloping targeted therapies. Previous works focused only on identifying\ndifferences in network structures. In this study, we used graph frequency\nanalysis of cancer genetic signals defined on a co-expression network to\ndescribe the spectral properties of underlying cancer systems. We demonstrated\nthat cancer cells exhibit distinctive signatures in the graph frequency content\nof their gene expression signals. Applying graph frequency filtering, graph\nFourier transforms, and its inverse to gene expression from different cancer\nstages resulted in significant improvements in average F-statistics of the\ngenes compared to using their unfiltered expression levels. We propose graph\nspectral properties of cancer genetic signals defined on gene co-expression\nnetworks as cancer hallmarks with potential application for differential\nco-expression analysis.",
            "author": [
                "Radwa Adel",
                "Ercan Engin Kuruoglu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06747v1",
                "http://arxiv.org/pdf/2311.06747v1"
            ],
            "primary_category": "q-bio.MN",
            "category": [
                "q-bio.MN",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06746v1",
            "title": "Two Stream Scene Understanding on Graph Embedding",
            "updated": "2023-11-12T05:57:56Z",
            "published": "2023-11-12T05:57:56Z",
            "summary": "The paper presents a novel two-stream network architecture for enhancing\nscene understanding in computer vision. This architecture utilizes a graph\nfeature stream and an image feature stream, aiming to merge the strengths of\nboth modalities for improved performance in image classification and scene\ngraph generation tasks. The graph feature stream network comprises a\nsegmentation structure, scene graph generation, and a graph representation\nmodule. The segmentation structure employs the UPSNet architecture with a\nbackbone that can be a residual network, Vit, or Swin Transformer. The scene\ngraph generation component focuses on extracting object labels and neighborhood\nrelationships from the semantic map to create a scene graph. Graph\nConvolutional Networks (GCN), GraphSAGE, and Graph Attention Networks (GAT) are\nemployed for graph representation, with an emphasis on capturing node features\nand their interconnections. The image feature stream network, on the other\nhand, focuses on image classification through the use of Vision Transformer and\nSwin Transformer models. The two streams are fused using various data fusion\nmethods. This fusion is designed to leverage the complementary strengths of\ngraph-based and image-based features.Experiments conducted on the ADE20K\ndataset demonstrate the effectiveness of the proposed two-stream network in\nimproving image classification accuracy compared to conventional methods. This\nresearch provides a significant contribution to the field of computer vision,\nparticularly in the areas of scene understanding and image classification, by\neffectively combining graph-based and image-based approaches.",
            "author": [
                "Wenkai Yang",
                "Wenyuan Sun",
                "Runxaing Huang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06746v1",
                "http://arxiv.org/pdf/2311.06746v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06731v1",
            "title": "An advantage based policy transfer algorithm for reinforcement learning\n  with metrics of transferability",
            "updated": "2023-11-12T04:25:53Z",
            "published": "2023-11-12T04:25:53Z",
            "summary": "Reinforcement learning (RL) can enable sequential decision-making in complex\nand high-dimensional environments if the acquisition of a new state-action pair\nis efficient, i.e., when interaction with the environment is inexpensive.\nHowever, there are a myriad of real-world applications in which a high number\nof interactions are infeasible. In these environments, transfer RL algorithms,\nwhich can be used for the transfer of knowledge from one or multiple source\nenvironments to a target environment, have been shown to increase learning\nspeed and improve initial and asymptotic performance. However, most existing\ntransfer RL algorithms are on-policy and sample inefficient, and often require\nheuristic choices in algorithm design. This paper proposes an off-policy\nAdvantage-based Policy Transfer algorithm, APT-RL, for fixed domain\nenvironments. Its novelty is in using the popular notion of ``advantage'' as a\nregularizer, to weigh the knowledge that should be transferred from the source,\nrelative to new knowledge learned in the target, removing the need for\nheuristic choices. Further, we propose a new transfer performance metric to\nevaluate the performance of our algorithm and unify existing transfer RL\nframeworks. Finally, we present a scalable, theoretically-backed task\nsimilarity measurement algorithm to illustrate the alignments between our\nproposed transferability metric and similarities between source and target\nenvironments. Numerical experiments on three continuous control benchmark tasks\ndemonstrate that APT-RL outperforms existing transfer RL algorithms on most\ntasks, and is $10\\%$ to $75\\%$ more sample efficient than learning from\nscratch.",
            "author": [
                "Md Ferdous Alam",
                "Parinaz Naghizadeh",
                "David Hoelzle"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06731v1",
                "http://arxiv.org/pdf/2311.06731v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06726v1",
            "title": "The Distributed Complexity of Locally Checkable Labeling Problems Beyond\n  Paths and Trees",
            "updated": "2023-11-12T03:57:22Z",
            "published": "2023-11-12T03:57:22Z",
            "summary": "We consider locally checkable labeling LCL problems in the LOCAL model of\ndistributed computing. Since 2016, there has been a substantial body of work\nexamining the possible complexities of LCL problems. For example, it has been\nestablished that there are no LCL problems exhibiting deterministic\ncomplexities falling between $\\omega(\\log^* n)$ and $o(\\log n)$. This line of\ninquiry has yielded a wealth of algorithmic techniques and insights that are\nuseful for algorithm designers.\n  While the complexity landscape of LCL problems on general graphs, trees, and\npaths is now well understood, graph classes beyond these three cases remain\nlargely unexplored. Indeed, recent research trends have shifted towards a\nfine-grained study of special instances within the domains of paths and trees.\n  In this paper, we generalize the line of research on characterizing the\ncomplexity landscape of LCL problems to a much broader range of graph classes.\nWe propose a conjecture that characterizes the complexity landscape of LCL\nproblems for an arbitrary class of graphs that is closed under minors, and we\nprove a part of the conjecture.\n  Some highlights of our findings are as follows.\n  1. We establish a simple characterization of the minor-closed graph classes\nsharing the same deterministic complexity landscape as paths, where $O(1)$,\n$\\Theta(\\log^* n)$, and $\\Theta(n)$ are the only possible complexity classes.\n  2. It is natural to conjecture that any minor-closed graph class shares the\nsame complexity landscape as trees if and only if the graph class has bounded\ntreewidth and unbounded pathwidth. We prove the \"only if\" part of the\nconjecture.\n  3. In addition to the well-known complexity landscapes for paths, trees, and\ngeneral graphs, there are infinitely many different complexity landscapes among\nminor-closed graph classes.",
            "author": [
                "Yi-Jun Chang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06726v1",
                "http://arxiv.org/pdf/2311.06726v1"
            ],
            "primary_category": "cs.DC",
            "category": [
                "cs.DC",
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06723v1",
            "title": "A Nonlinear Analysis Software Toolkit for Biomechanical Data",
            "updated": "2023-11-12T03:45:34Z",
            "published": "2023-11-12T03:45:34Z",
            "summary": "In this paper, we present a nonlinear analysis software toolkit, which can\nhelp in biomechanical gait data analysis by implementing various nonlinear\nstatistical analysis algorithms. The toolkit is proposed to tackle the need for\nan easy-to-use and friendly analyzer for gait data where algorithms seem\ncomplex to implement in software and execute. With the availability of our\ntoolkit, people without programming knowledge can run the analysis to receive\nhuman gait data analysis results. Our toolkit includes the implementation of\nseveral nonlinear analysis algorithms, while it is also possible for users with\nprogramming experience to expand its scope by implementing and adding more\nalgorithms to the toolkit. Currently, the toolkit supports MatLab bindings\nwhile being developed in Python. The toolkit can seamlessly run as a background\nprocess to analyze hundreds of different gait data and produce analysis\noutcomes and figures that illustrate these results.",
            "author": [
                "Shifat Sarwar",
                "Aaron Likens",
                "Nick Stergiou",
                "Spyridon Mastorakis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06723v1",
                "http://arxiv.org/pdf/2311.06723v1"
            ],
            "primary_category": "cs.ET",
            "category": [
                "cs.ET",
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06712v2",
            "title": "PuzzleTuning: Explicitly Bridge Pathological and Natural Image with\n  Puzzles",
            "updated": "2023-11-30T10:03:17Z",
            "published": "2023-11-12T02:43:22Z",
            "summary": "Pathological image analysis is a crucial field in computer vision. Due to the\nannotation scarcity in the pathological field, recently, most of the works\nleverage self-supervised learning (SSL) trained on unlabeled pathological\nimages, hoping to mine the main representation automatically. However, there\nare two core defects in SSL-based pathological pre-training: (1) they do not\nexplicitly explore the essential focuses of the pathological field, and (2)\nthey do not effectively bridge with and thus take advantage of the large\nnatural image domain. To explicitly address them, we propose our large-scale\nPuzzleTuning framework, containing the following innovations. Firstly, we\nidentify three task focuses that can effectively bridge pathological and\nnatural domains: appearance consistency, spatial consistency, and misalignment\nunderstanding. Secondly, we devise a multiple puzzle restoring task to\nexplicitly pre-train the model with these focuses. Thirdly, for the existing\nlarge domain gap between natural and pathological fields, we introduce an\nexplicit prompt-tuning process to incrementally integrate the domain-specific\nknowledge with the natural knowledge. Additionally, we design a\ncurriculum-learning training strategy that regulates the task difficulty,\nmaking the model fit the complex multiple puzzle restoring task adaptively.\nExperimental results show that our PuzzleTuning framework outperforms the\nprevious SOTA methods in various downstream tasks on multiple datasets. The\ncode, demo, and pre-trained weights are available at\nhttps://github.com/sagizty/PuzzleTuning.",
            "author": [
                "Tianyi Zhang",
                "Shangqing Lyu",
                "Yanli Lei",
                "Sicheng Chen",
                "Nan Ying",
                "Yufang He",
                "Yu Zhao",
                "Yunlu Feng",
                "Guanglei Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06712v2",
                "http://arxiv.org/pdf/2311.06712v2"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06702v1",
            "title": "Machine Learning for Mechanistic Models of Metapopulation Dynamics",
            "updated": "2023-11-12T01:22:36Z",
            "published": "2023-11-12T01:22:36Z",
            "summary": "Mathematical models in ecology and epidemiology must be consistent with\nobserved data in order to generate reliable knowledge and sound policy.\nMetapopulation systems, which consist of a collection of sub-populations at\nvarious locations, pose technical challenges in statistical inference due to\nnonlinear, stochastic interactions. Difficulties encountered in these\nmethodological issues can obstruct the core scientific questions concerning the\nlink between the mathematical models and the data. Progress in statistically\nefficient simulation-based inference for partially observed stochastic dynamic\nsystems has enabled the development of statistically rigorous approaches to the\nanalysis of nonlinear but low-dimensional systems. Recently, an algorithm has\nbeen developed which enables comparable inference for higher-dimensional models\narising in metapopulation systems. The COVID-19 pandemic provides a situation\nwhere mathematical models and their policy implications were widely visible,\nand we revisit an influential metapopulation model used to inform basic\nepidemiological understanding early in the pandemic. Our methods support\nself-critical data analysis, enabling us to identify and address model\nlimitations, and leading to a new model with substantially improved statistical\nfit and parameter identifiability. Our results suggest that the lockdown\ninitiated on January 23, 2020 in China was more effective than previously\nthought. We proceed to recommend statistical analysis standards for future\nmetapopulation system modeling.",
            "author": [
                "Jifan Li",
                "Edward L. Ionides",
                "Aaron A. King",
                "Mercedes Pascual",
                "Ning Ning"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06702v1",
                "http://arxiv.org/pdf/2311.06702v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "physics.soc-ph",
                "q-bio.PE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06695v1",
            "title": "Conversational Data Exploration: A Game-Changer for Designing Data\n  Science Pipelines",
            "updated": "2023-11-12T00:22:09Z",
            "published": "2023-11-12T00:22:09Z",
            "summary": "This paper proposes a conversational approach implemented by the system\nChatin for driving an intuitive data exploration experience. Our work aims to\nunlock the full potential of data analytics and artificial intelligence with a\nnew generation of data science solutions. Chatin is a cutting-edge tool that\ndemocratises access to AI-driven solutions, empowering non-technical users from\nvarious disciplines to explore data and extract knowledge from it.",
            "author": [
                "Genoveva Vargas-Solar",
                "Tania Cerquitelli",
                "Javier A. Espinosa-Oviedo",
                "Fran\u00e7ois Cheval",
                "Anthelme Buchaille",
                "Luca Polgar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06695v1",
                "http://arxiv.org/pdf/2311.06695v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC",
                "cs.AI",
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06674v1",
            "title": "Education for a Future in Crisis: Developing a Humanities-Informed STEM\n  Curriculum",
            "updated": "2023-11-11T22:16:25Z",
            "published": "2023-11-11T22:16:25Z",
            "summary": "In the popular imagination, science and technology are often seen as fields\nof knowledge production critical to social progress and a cooperative future.\nThis optimistic portrayal of technological advancement also features\nprominently in internal discourses amongst scientists, industry leaders, and\nSTEM students alike. Yet, an overwhelming body of research, investigation, and\nfirst-person accounts highlight the varying ways modern science, technology,\nand engineering industries contribute to the degradation of our changing\nenvironments and exploit and harm global low-income and marginalized\npopulations. By and large, siloed higher-education STEM curricula provide\ninadequate opportunities for undergraduate and graduate students to critically\nanalyze the historical and epistemological foundations of scientific knowledge\nproduction and even fewer tools to engage with and respond to modern\ncommunity-based cases. Here, we describe the development of a humanities- and\nsocial sciences-informed curriculum designed to address the theory, content,\nand skill-based needs of traditional STEM students considering technoscientific\ncareers. In essence, this course is designed to foster behavior change,\nde-center dominant ways of knowing in the sciences, and bolster self-reflection\nand critical-thinking skills to equip the developing STEM workforce with a more\nnuanced and accurate understanding of the social, political, and economic role\nof science and technology. This curriculum has the potential to empower\nSTEM-educated professionals to contribute to a more promising, inclusive\nfuture. Our framework foregrounds key insights from science and technology\nstudies, Black and Native feminisms, queer theory, and disability studies,\nalongside real-world case studies using critical pedagogies.",
            "author": [
                "Ethan Lee",
                "Ariel Nicole Hart",
                "Thomas A. Searles",
                "Marc Levis-Fitzgerald",
                "Ram\u00f3n S. Barthelemy",
                "Shanna Shaked",
                "Victoria Marks",
                "Sergio Carbajo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06674v1",
                "http://arxiv.org/pdf/2311.06674v1"
            ],
            "primary_category": "physics.ed-ph",
            "category": [
                "physics.ed-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06673v1",
            "title": "Dream to Adapt: Meta Reinforcement Learning by Latent Context\n  Imagination and MDP Imagination",
            "updated": "2023-11-11T22:05:10Z",
            "published": "2023-11-11T22:05:10Z",
            "summary": "Meta reinforcement learning (Meta RL) has been amply explored to quickly\nlearn an unseen task by transferring previously learned knowledge from similar\ntasks. However, most state-of-the-art algorithms require the meta-training\ntasks to have a dense coverage on the task distribution and a great amount of\ndata for each of them. In this paper, we propose MetaDreamer, a context-based\nMeta RL algorithm that requires less real training tasks and data by doing\nmeta-imagination and MDP-imagination. We perform meta-imagination by\ninterpolating on the learned latent context space with disentangled properties,\nas well as MDP-imagination through the generative world model where physical\nknowledge is added to plain VAE networks. Our experiments with various\nbenchmarks show that MetaDreamer outperforms existing approaches in data\nefficiency and interpolated generalization.",
            "author": [
                "Lu Wen",
                "Songan Zhang",
                "H. Eric Tseng",
                "Huei Peng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06673v1",
                "http://arxiv.org/pdf/2311.06673v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06669v2",
            "title": "On interrelations between graph complexes",
            "updated": "2023-11-25T16:26:55Z",
            "published": "2023-11-11T21:22:00Z",
            "summary": "We study Maxim Kontsevich's graph complex $GC_d$ for any integer $d$ as well\nas its oriented and targeted versions, and show new short proofs of the\ntheorems due to Thomas Willwacher and Marko Zivkovic which establish\nisomorphisms of their cohomology groups. A new result relating the cohomology\nof the sourced-targeted graph complex in dimension $d+1$ with the direct sum of\ntwo copies of the cohomology group of Maxim Kontsevich's graph complex $GC_d$\nin dimension $d$ is obtained. We introduce a new graph complex spanned by\npurely trivalent graphs and show that its cohomology is isomorphic to\n$H(GC_d)$.",
            "author": [
                "Sergei Merkulov"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06669v2",
                "http://arxiv.org/pdf/2311.06669v2"
            ],
            "primary_category": "math.QA",
            "category": [
                "math.QA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06662v1",
            "title": "A Whitney polynomial for hypermaps",
            "updated": "2023-11-11T20:17:17Z",
            "published": "2023-11-11T20:17:17Z",
            "summary": "We introduce a Whitney polynomial for hypermaps and use it to generalize the\nresults connecting the circuit partition polynomial to the Martin polynomial\nand the results on several graph invariants.",
            "author": [
                "Robert Cori",
                "G\u00e1bor Hetyei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06662v1",
                "http://arxiv.org/pdf/2311.06662v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "Primary 05C30, Secondary 05C10, 05C15"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06658v1",
            "title": "Reconfigurable Inspection in Manufacturing: State of the Art and\n  Taxonomy",
            "updated": "2023-11-11T20:09:30Z",
            "published": "2023-11-11T20:09:30Z",
            "summary": "This article provides an overview of the evolution of the product quality and\nmeasurement inspection procedure with emphasis on the Reconfigurable Inspection\nSystem and Machine. The major components of a reconfigurable manufacturing\nsystem have been examined, and the evolution of manufacturing processes has\nbeen briefly discussed. Different Reconfigurable Inspection Machines (RIMs) and\ntheir arrangement in an assembly line as an inspection system have been\ncarefully studied and the modern inspection system equipped in RMS has been\ncompared to the traditional techniques commonly used in inspection of product\nquality. A survey of evolving inspection techniques is offered from the\nstandpoint of technological challenges and advancement affecting manufacturing\nover time. As per authors' knowledge, the review on Reconfigurable Inspection\nin Manufacturing and taxonomy of reconfigurable inspection systems is rare.\nConsidering the studies done in this domain, there is still resourceful\ntaxonomy for this paradigm. Therefore, different types of inspection procedures\nhave been discussed, their features and applications have been compared to\narrive at the taxonomy of the RIS based on the understanding of the nature of a\nRIS after a critical review.",
            "author": [
                "Harshit Gupta",
                "Ashok Kumar Madan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06658v1",
                "http://arxiv.org/pdf/2311.06658v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.SY",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06649v1",
            "title": "A Template Is All You Meme",
            "updated": "2023-11-11T19:38:14Z",
            "published": "2023-11-11T19:38:14Z",
            "summary": "Memes are a modern form of communication and meme templates possess a base\nsemantics that is customizable by whomever posts it on social media. Machine\nlearning systems struggle with memes, which is likely due to such systems\nhaving insufficient context to understand memes, as there is more to memes than\nthe obvious image and text. Here, to aid understanding of memes, we release a\nknowledge base of memes and information found on www.knowyourmeme.com, which we\ncall the Know Your Meme Knowledge Base (KYMKB), composed of more than 54,000\nimages. The KYMKB includes popular meme templates, examples of each template,\nand detailed information about the template. We hypothesize that meme templates\ncan be used to inject models with the context missing from previous approaches.\nTo test our hypothesis, we create a non-parametric majority-based classifier,\nwhich we call Template-Label Counter (TLC). We find TLC more effective than or\ncompetitive with fine-tuned baselines. To demonstrate the power of meme\ntemplates and the value of both our knowledge base and method, we conduct\nthorough classification experiments and exploratory data analysis in the\ncontext of five meme analysis tasks.",
            "author": [
                "Luke Bates",
                "Peter Ebert Christensen",
                "Preslav Nakov",
                "Iryna Gurevych"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06649v1",
                "http://arxiv.org/pdf/2311.06649v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06640v1",
            "title": "NewsGPT: ChatGPT Integration for Robot-Reporter",
            "updated": "2023-11-11T18:39:36Z",
            "published": "2023-11-11T18:39:36Z",
            "summary": "The integration of large language models (LLMs) with social robots has\nemerged as a promising avenue for enhancing human-robot interactions at a time\nwhen news reports generated by artificial intelligence (AI) are gaining in\ncredibility. This integration is expected to intensify and become a more\nproductive resource for journalism, media, communication, and education. In\nthis paper a novel system is proposed that integrates AI's generative\npretrained transformer (GPT) model with the Pepper robot, with the aim of\nimproving the robot's natural language understanding and response generation\ncapabilities for enhanced social interactions. By leveraging GPT's powerful\nlanguage processing capabilities, this system offers a comprehensive pipeline\nthat incorporates voice input recording, speech-to-text transcription, context\nanalysis, and text-to-speech synthesis action generation. The Pepper robot is\nenabled to comprehend user queries, generate informative responses with general\nknowledge, maintain contextually relevant conversations, and act as a more\ndomain-oriented news reporter. It is also linked with a news resource and\npowered with a Google search capability. To evaluate the performance of the\nframework, experiments were conducted involving a set of diverse questions. The\nrobot's responses were assessed on the basis of eight criteria, including\nrelevance, context, and fluency. Despite some identified limitations, this\nsystem contributes to the field of journalism and human-robot interaction by\nshowcasing the potential of integrating LLMs with social robots. The proposed\nframework opens up opportunities for improving the conversational capabilities\nof robots, enabling interactions that are smoother, more engaging, and more\ncontext aware.",
            "author": [
                "Abdelhadi Hireche",
                "Abdelkader Nasreddine Belkacem",
                "Sadia Jamil",
                "Chao Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06640v1",
                "http://arxiv.org/pdf/2311.06640v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.HC",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06638v1",
            "title": "Area of intrinsic graphs in homogeneous groups",
            "updated": "2023-11-11T18:34:49Z",
            "published": "2023-11-11T18:34:49Z",
            "summary": "We establish an area formula for computing the spherical measure of an\nintrinsic graph of any codimension in an arbitrary homogeneous group. Our\napproach only assumes that the map generating the intrinsic graph is\ncontinuously intrinsically differentiable. The important novelty lies in the\nnotion of Jacobian, which is built by the auxiliary Euclidean distance. The\nintroduction of this Jacobian allows the spherical factor to appear in the area\nformula and enables explicit computations.",
            "author": [
                "Francesca Corni",
                "Valentino Magnani"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06638v1",
                "http://arxiv.org/pdf/2311.06638v1"
            ],
            "primary_category": "math.MG",
            "category": [
                "math.MG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06632v1",
            "title": "The Exact Determinant of a Specific Class of Sparse Positive Definite\n  Matrices",
            "updated": "2023-11-11T18:31:25Z",
            "published": "2023-11-11T18:31:25Z",
            "summary": "For a specific class of sparse Gaussian graphical models, we provide a\nclosed-form solution for the determinant of the covariance matrix. In our\nframework, the graphical interaction model (i.e., the covariance selection\nmodel) is equal to replacement product of $\\mathcal{K}_{n}$ and\n$\\mathcal{K}_{n-1}$, where $\\mathcal{K}_n$ is the complete graph with $n$\nvertices. Our analysis is based on taking the Fourier transform of the local\nfactors of the model, which can be viewed as an application of the Normal\nFactor Graph Duality Theorem and holographic algorithms. The closed-form\nexpression is obtained by applying the Matrix Determinant Lemma on the\ntransformed graphical model. In this context, we will also define a notion of\nequivalence between two Gaussian graphical models.",
            "author": [
                "Mehdi Molkaraie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06632v1",
                "http://arxiv.org/pdf/2311.06632v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG",
                "cs.NA",
                "math.NA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06623v1",
            "title": "VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach\n  For Intelligent Highway Transportation Systems",
            "updated": "2023-11-11T17:52:06Z",
            "published": "2023-11-11T17:52:06Z",
            "summary": "Enhancing roadway safety and traffic management has become an essential focus\narea for a broad range of modern cyber-physical systems and intelligent\ntransportation systems. Vehicle Trajectory Prediction is a pivotal element\nwithin numerous applications for highway and road safety. These applications\nencompass a wide range of use cases, spanning from traffic management and\naccident prevention to enhancing work-zone safety and optimizing energy\nconservation. The ability to implement intelligent management in this context\nhas been greatly advanced by the developments in the field of Artificial\nIntelligence (AI), alongside the increasing deployment of surveillance cameras\nacross road networks. In this paper, we introduce a novel transformer-based\napproach for vehicle trajectory prediction for highway safety and surveillance,\ndenoted as VT-Former. In addition to utilizing transformers to capture\nlong-range temporal patterns, a new Graph Attentive Tokenization (GAT) module\nhas been proposed to capture intricate social interactions among vehicles.\nCombining these two core components culminates in a precise approach for\nvehicle trajectory prediction. Our study on three benchmark datasets with three\ndifferent viewpoints demonstrates the State-of-The-Art (SoTA) performance of\nVT-Former in vehicle trajectory prediction and its generalizability and\nrobustness. We also evaluate VT-Former's efficiency on embedded boards and\nexplore its potential for vehicle anomaly detection as a sample application,\nshowcasing its broad applicability.",
            "author": [
                "Armin Danesh Pazho",
                "Vinit Katariya",
                "Ghazal Alinezhad Noghre",
                "Hamed Tabkhi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06623v1",
                "http://arxiv.org/pdf/2311.06623v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06611v1",
            "title": "The Lov\u00e1sz-Cherkassky theorem in infinite graphs",
            "updated": "2023-11-11T16:51:43Z",
            "published": "2023-11-11T16:51:43Z",
            "summary": "Infinite generalizations of theorems in finite combinatorics were initiated\nby Erd\\H{o}s due to his famous Erd\\H{o}s-Menger conjecture (now known as the\nAharoni-Berger theorem) that extends Menger's theorem to infinite graphs in a\nstructural way. We prove a generalization of this manner of the classical\nresult about packing edge-disjoint $ T $-paths in an ``inner Eulerian'' setting\nobtained by Lov\\'asz and Cherkassky independently in the '70s.",
            "author": [
                "Attila Jo\u00f3"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06611v1",
                "http://arxiv.org/pdf/2311.06611v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06602v1",
            "title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance",
            "updated": "2023-11-11T16:16:11Z",
            "published": "2023-11-11T16:16:11Z",
            "summary": "As large language models (LLMs) impact a growing number of complex domains,\nit is becoming increasingly important to have fair, accurate, and rigorous\nevaluation benchmarks. Evaluating the reasoning skills required for business\nand financial NLP stands out as a particularly difficult challenge. We\nintroduce BizBench, a new benchmark for evaluating models' ability to reason\nabout realistic financial problems. BizBench comprises 8 quantitative reasoning\ntasks. Notably, BizBench targets the complex task of question-answering (QA)\nfor structured and unstructured financial data via program synthesis (i.e.,\ncode generation). We introduce three diverse financially-themed code-generation\ntasks from newly collected and augmented QA data. Additionally, we isolate\ndistinct financial reasoning capabilities required to solve these QA tasks:\nreading comprehension of financial text and tables, which is required to\nextract correct intermediate values; and understanding domain knowledge (e.g.,\nfinancial formulas) needed to calculate complex solutions. Collectively, these\ntasks evaluate a model's financial background knowledge, ability to extract\nnumeric entities from financial documents, and capacity to solve problems with\ncode. We conduct an in-depth evaluation of open-source and commercial LLMs,\nillustrating that BizBench is a challenging benchmark for quantitative\nreasoning in the finance and business domain.",
            "author": [
                "Rik Koncel-Kedziorski",
                "Michael Krumdick",
                "Viet Lai",
                "Varshini Reddy",
                "Charles Lovering",
                "Chris Tanner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06602v1",
                "http://arxiv.org/pdf/2311.06602v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06581v1",
            "title": "On the Free Boundary Problems for the Ideal Incompressible MHD Equations",
            "updated": "2023-11-11T14:31:11Z",
            "published": "2023-11-11T14:31:11Z",
            "summary": "We investigate the general plasma-vacuum interface problems for the ideal\nincompressible MHD equations with or without surface tension and prove their\nnonlinear local well-posedness in standard Sobolev spaces under either non-zero\nsurface tension or the stability condition that the magnetic fields are\neverywhere non-collinear on the interface. In particular, the results show that\nboth capillary forces and tangential magnetic fields can stabilize the motion\nof the plasma-vacuum interfaces. Moreover, the vanishing surface tension limit\nresults are established under the Rayleigh-Taylor sign condition or the\nnon-collinearity condition. All these results hold with no graph assumption on\nthe free interface.",
            "author": [
                "Sicheng Liu",
                "Zhouping Xin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06581v1",
                "http://arxiv.org/pdf/2311.06581v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06572v1",
            "title": "Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards\n  Fully Automated Radiation Oncology Treatments",
            "updated": "2023-11-11T13:52:59Z",
            "published": "2023-11-11T13:52:59Z",
            "summary": "The field of Radiation Oncology is uniquely positioned to benefit from the\nuse of artificial intelligence to fully automate the creation of radiation\ntreatment plans for cancer therapy. This time-consuming and specialized task\ncombines patient imaging with organ and tumor segmentation to generate a 3D\nradiation dose distribution to meet clinical treatment goals, similar to\nvoxel-level dense prediction. In this work, we propose Swin UNETR++, that\ncontains a lightweight 3D Dual Cross-Attention (DCA) module to capture the\nintra and inter-volume relationships of each patient's unique anatomy, which\nfully convolutional neural networks lack. Our model was trained, validated, and\ntested on the Open Knowledge-Based Planning dataset. In addition to metrics of\nDose Score $\\overline{S_{\\text{Dose}}}$ and DVH Score\n$\\overline{S_{\\text{DVH}}}$ that quantitatively measure the difference between\nthe predicted and ground-truth 3D radiation dose distribution, we propose the\nqualitative metrics of average volume-wise acceptance rate\n$\\overline{R_{\\text{VA}}}$ and average patient-wise clinical acceptance rate\n$\\overline{R_{\\text{PA}}}$ to assess the clinical reliability of the\npredictions. Swin UNETR++ demonstrates near-state-of-the-art performance on\nvalidation and test dataset (validation: $\\overline{S_{\\text{DVH}}}$=1.492 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.649 Gy, $\\overline{R_{\\text{VA}}}$=88.58%,\n$\\overline{R_{\\text{PA}}}$=100.0%; test: $\\overline{S_{\\text{DVH}}}$=1.634 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.757 Gy, $\\overline{R_{\\text{VA}}}$=90.50%,\n$\\overline{R_{\\text{PA}}}$=98.0%), establishing a basis for future studies to\ntranslate 3D dose predictions into a deliverable treatment plan, facilitating\nfull automation.",
            "author": [
                "Kuancheng Wang",
                "Hai Siong Tan",
                "Rafe Mcbeth"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06572v1",
                "http://arxiv.org/pdf/2311.06572v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06557v1",
            "title": "Identification of vortex in unstructured mesh with graph neural networks",
            "updated": "2023-11-11T12:10:16Z",
            "published": "2023-11-11T12:10:16Z",
            "summary": "Deep learning has been employed to identify flow characteristics from\nComputational Fluid Dynamics (CFD) databases to assist the researcher to better\nunderstand the flow field, to optimize the geometry design and to select the\ncorrect CFD configuration for corresponding flow characteristics. Convolutional\nNeural Network (CNN) is one of the most popular algorithms used to extract and\nidentify flow features. However its use, without any additional flow field\ninterpolation, is limited to the simple domain geometry and regular meshes\nwhich limits its application to real industrial cases where complex geometry\nand irregular meshes are usually used. Aiming at the aforementioned problems,\nwe present a Graph Neural Network (GNN) based model with U-Net architecture to\nidentify the vortex in CFD results on unstructured meshes. The graph generation\nand graph hierarchy construction using algebraic multigrid method from CFD\nmeshes are introduced. A vortex auto-labeling method is proposed to label\nvortex regions in 2D CFD meshes. We precise our approach by firstly optimizing\nthe input set on CNNs, then benchmarking current GNN kernels against CNN model\nand evaluating the performances of GNN kernels in terms of classification\naccuracy, training efficiency and identified vortex morphology. Finally, we\ndemonstrate the adaptability of our approach to unstructured meshes and\ngenerality to unseen cases with different turbulence models at different\nReynolds numbers.",
            "author": [
                "Lianfa Wang",
                "Yvan Fournier",
                "Jean-Francois Wald",
                "Youssef Mesri"
            ],
            "link": [
                "http://dx.doi.org/10.1016/j.compfluid.2023.106104",
                "http://arxiv.org/abs/2311.06557v1",
                "http://arxiv.org/pdf/2311.06557v1"
            ],
            "primary_category": "physics.flu-dyn",
            "category": [
                "physics.flu-dyn",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06554v1",
            "title": "Graph ODE with Factorized Prototypes for Modeling Complicated\n  Interacting Dynamics",
            "updated": "2023-11-11T12:04:47Z",
            "published": "2023-11-11T12:04:47Z",
            "summary": "This paper studies the problem of modeling interacting dynamical systems,\nwhich is critical for understanding physical dynamics and biological processes.\nRecent research predominantly uses geometric graphs to represent these\ninteractions, which are then captured by powerful graph neural networks (GNNs).\nHowever, predicting interacting dynamics in challenging scenarios such as\nout-of-distribution shift and complicated underlying rules remains unsolved. In\nthis paper, we propose a new approach named Graph ODE with factorized\nprototypes (GOAT) to address the problem. The core of GOAT is to incorporate\nfactorized prototypes from contextual knowledge into a continuous graph ODE\nframework. Specifically, GOAT employs representation disentanglement and system\nparameters to extract both object-level and system-level contexts from\nhistorical trajectories, which allows us to explicitly model their independent\ninfluence and thus enhances the generalization capability under system changes.\nThen, we integrate these disentangled latent representations into a graph ODE\nmodel, which determines a combination of various interacting prototypes for\nenhanced model expressivity. The entire model is optimized using an end-to-end\nvariational inference framework to maximize the likelihood. Extensive\nexperiments in both in-distribution and out-of-distribution settings validate\nthe superiority of GOAT.",
            "author": [
                "Xiao Luo",
                "Yiyang Gu",
                "Huiyu Jiang",
                "Jinsheng Huang",
                "Wei Ju",
                "Ming Zhang",
                "Yizhou Sun"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06554v1",
                "http://arxiv.org/pdf/2311.06554v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06553v1",
            "title": "Visual Commonsense based Heterogeneous Graph Contrastive Learning",
            "updated": "2023-11-11T12:01:18Z",
            "published": "2023-11-11T12:01:18Z",
            "summary": "How to select relevant key objects and reason about the complex relationships\ncross vision and linguistic domain are two key issues in many multi-modality\napplications such as visual question answering (VQA). In this work, we\nincorporate the visual commonsense information and propose a heterogeneous\ngraph contrastive learning method to better finish the visual reasoning task.\nOur method is designed as a plug-and-play way, so that it can be quickly and\neasily combined with a wide range of representative methods. Specifically, our\nmodel contains two key components: the Commonsense-based Contrastive Learning\nand the Graph Relation Network. Using contrastive learning, we guide the model\nconcentrate more on discriminative objects and relevant visual commonsense\nattributes. Besides, thanks to the introduction of the Graph Relation Network,\nthe model reasons about the correlations between homogeneous edges and the\nsimilarities between heterogeneous edges, which makes information transmission\nmore effective. Extensive experiments on four benchmarks show that our method\ngreatly improves seven representative VQA models, demonstrating its\neffectiveness and generalizability.",
            "author": [
                "Zongzhao Li",
                "Xiangyu Zhu",
                "Xi Zhang",
                "Zhaoxiang Zhang",
                "Zhen Lei"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06553v1",
                "http://arxiv.org/pdf/2311.06553v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07608v1",
            "title": "MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital\n  Readmission Prediction",
            "updated": "2023-11-11T11:14:07Z",
            "published": "2023-11-11T11:14:07Z",
            "summary": "Hospital readmission prediction is considered an essential approach to\ndecreasing readmission rates, which is a key factor in assessing the quality\nand efficacy of a healthcare system. Previous studies have extensively utilized\nthree primary modalities, namely electronic health records (EHR), medical\nimages, and clinical notes, to predict hospital readmissions. However, the\nmajority of these studies did not integrate information from all three\nmodalities or utilize the spatiotemporal relationships present in the dataset.\nThis study introduces a novel model called the Multimodal Spatiotemporal\nGraph-Transformer (MuST) for predicting hospital readmissions. By employing\nGraph Convolution Networks and temporal transformers, we can effectively\ncapture spatial and temporal dependencies in EHR and chest radiographs. We then\npropose a fusion transformer to combine the spatiotemporal features from the\ntwo modalities mentioned above with the features from clinical notes extracted\nby a pre-trained, domain-specific transformer. We assess the effectiveness of\nour methods using the latest publicly available dataset, MIMIC-IV. The\nexperimental results indicate that the inclusion of multimodal features in MuST\nimproves its performance in comparison to unimodal methods. Furthermore, our\nproposed pipeline outperforms the current leading methods in the prediction of\nhospital readmissions.",
            "author": [
                "Yan Miao",
                "Lequan Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07608v1",
                "http://arxiv.org/pdf/2311.07608v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06534v1",
            "title": "Enhancing Public Understanding of Court Opinions with Automated\n  Summarizers",
            "updated": "2023-11-11T11:05:27Z",
            "published": "2023-11-11T11:05:27Z",
            "summary": "Written judicial opinions are an important tool for building public trust in\ncourt decisions, yet they can be difficult for non-experts to understand. We\npresent a pipeline for using an AI assistant to generate simplified summaries\nof judicial opinions. These are more accessible to the public and more easily\nunderstood by non-experts, We show in a survey experiment that the simplified\nsummaries help respondents understand the key features of a ruling. We discuss\nhow to integrate legal domain knowledge into studies using large language\nmodels. Our results suggest a role both for AI assistants to inform the public,\nand for lawyers to guide the process of generating accessible summaries.",
            "author": [
                "Elliott Ash",
                "Aniket Kesari",
                "Suresh Naidu",
                "Lena Song",
                "Dominik Stammbach"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06534v1",
                "http://arxiv.org/pdf/2311.06534v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06531v2",
            "title": "Large deviation principles for graphon sampling",
            "updated": "2023-11-21T19:33:40Z",
            "published": "2023-11-11T11:01:49Z",
            "summary": "We investigate possible large deviation principles (LDPs) for the $n$-vertex\nsampling from a given graphon with various speeds $s(n)$ and resolve all the\ncases except when the speed $s(n)$ is of order $n^2$.\n  For quadratic speed $s=(c+o(1))n^2$, we establish an LDP for an arbitrary\n$k$-step graphon, which extends a result of Chatterjee and Varadhan [Europ. J.\nCombin., 32 (2011) 1000-1017] who did this for $k=1$ (that is, for the\nhomogeneous binomial random graphs). This is done by reducing the problem to\nthe LDP for stochastic $k$-block models established recently by Borgs, Chayes,\nGaudio, Petti and Sen [\"A large deviation principle for block models\",\narxiv:2007.14508, 2020]. Also, we improve some results by Borgs et al.",
            "author": [
                "Jan Greb\u00edk",
                "Oleg Pikhurko"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06531v2",
                "http://arxiv.org/pdf/2311.06531v2"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06524v1",
            "title": "Towards a Personal Health Knowledge Graph Framework for Patient\n  Monitoring",
            "updated": "2023-11-11T09:52:51Z",
            "published": "2023-11-11T09:52:51Z",
            "summary": "Healthcare providers face significant challenges with monitoring and managing\npatient data outside of clinics, particularly with insufficient resources and\nlimited feedback on their patients' conditions. Effective management of these\nsymptoms and exploration of larger bodies of data are vital for maintaining\nlong-term quality of life and preventing late interventions. In this paper, we\npropose a framework for constructing personal health knowledge graphs from\nheterogeneous data sources. Our approach integrates clinical databases,\nrelevant ontologies and standard healthcare guidelines to support alert\ngeneration, clinician interpretation and querying of patient data. Through a\nuse case of monitoring Chronic Obstructive Pulmonary Disease (COPD) patients,\nwe demonstrate that inference and reasoning on personal health knowledge graphs\nbuilt with our framework can aid in patient monitoring and enhance the efficacy\nand accuracy of patient data queries.",
            "author": [
                "Daniel Bloor",
                "Nnamdi Ugwuoke",
                "David Taylor",
                "Keir Lewis",
                "Luis Mur",
                "Chuan Lu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06524v1",
                "http://arxiv.org/pdf/2311.06524v1"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB",
                "68-06",
                "E.1.3"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07605v1",
            "title": "Conceptual Model Interpreter for Large Language Models",
            "updated": "2023-11-11T09:41:37Z",
            "published": "2023-11-11T09:41:37Z",
            "summary": "Large Language Models (LLMs) recently demonstrated capabilities for\ngenerating source code in common programming languages. Additionally,\ncommercial products such as ChatGPT 4 started to provide code interpreters,\nallowing for the automatic execution of generated code fragments, instant\nfeedback, and the possibility to develop and refine in a conversational\nfashion. With an exploratory research approach, this paper applies code\ngeneration and interpretation to conceptual models. The concept and prototype\nof a conceptual model interpreter is explored, capable of rendering visual\nmodels generated in textual syntax by state-of-the-art LLMs such as Llama~2 and\nChatGPT 4. In particular, these LLMs can generate textual syntax for the\nPlantUML and Graphviz modeling software that is automatically rendered within a\nconversational user interface. The first result is an architecture describing\nthe components necessary to interact with interpreters and LLMs through APIs or\nlocally, providing support for many commercial and open source LLMs and\ninterpreters. Secondly, experimental results for models generated with ChatGPT\n4 and Llama 2 are discussed in two cases covering UML and, on an instance\nlevel, graphs created from custom data. The results indicate the possibility of\nmodeling iteratively in a conversational fashion.",
            "author": [
                "Felix H\u00e4rer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07605v1",
                "http://arxiv.org/pdf/2311.07605v1"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE",
                "cs.AI",
                "cs.PL",
                "I.2.1"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06517v1",
            "title": "BClean: A Bayesian Data Cleaning System",
            "updated": "2023-11-11T09:22:07Z",
            "published": "2023-11-11T09:22:07Z",
            "summary": "There is a considerable body of work on data cleaning which employs various\nprinciples to rectify erroneous data and transform a dirty dataset into a\ncleaner one. One of prevalent approaches is probabilistic methods, including\nBayesian methods. However, existing probabilistic methods often assume a\nsimplistic distribution (e.g., Gaussian distribution), which is frequently\nunderfitted in practice, or they necessitate experts to provide a complex prior\ndistribution (e.g., via a programming language). This requirement is both\nlabor-intensive and costly, rendering these methods less suitable for\nreal-world applications. In this paper, we propose BClean, a Bayesian Cleaning\nsystem that features automatic Bayesian network construction and user\ninteraction. We recast the data cleaning problem as a Bayesian inference that\nfully exploits the relationships between attributes in the observed dataset and\nany prior information provided by users. To this end, we present an automatic\nBayesian network construction method that extends a structure learning-based\nfunctional dependency discovery method with similarity functions to capture the\nrelationships between attributes. Furthermore, our system allows users to\nmodify the generated Bayesian network in order to specify prior information or\ncorrect inaccuracies identified by the automatic generation process. We also\ndesign an effective scoring model (called the compensative scoring model)\nnecessary for the Bayesian inference. To enhance the efficiency of data\ncleaning, we propose several approximation strategies for the Bayesian\ninference, including graph partitioning, domain pruning, and pre-detection. By\nevaluating on both real-world and synthetic datasets, we demonstrate that\nBClean is capable of achieving an F-measure of up to 0.9 in data cleaning,\noutperforming existing Bayesian methods by 2% and other data cleaning methods\nby 15%.",
            "author": [
                "Jianbin Qin",
                "Sifan Huang",
                "Yaoshu Wang",
                "Jing Zhu",
                "Yifan Zhang",
                "Yukai Miao",
                "Rui Mao",
                "Makoto Onizuka",
                "Chuan Xiao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06517v1",
                "http://arxiv.org/pdf/2311.06517v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.DB",
                "cs.LG",
                "stat.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06508v1",
            "title": "Resonance graphs of plane bipartite graphs as daisy cubes",
            "updated": "2023-11-11T08:39:12Z",
            "published": "2023-11-11T08:39:12Z",
            "summary": "We characterize all plane bipartite graphs whose resonance graphs are daisy\ncubes and therefore generalize related results on resonance graphs of benzenoid\ngraphs, catacondensed even ring systems, as well as 2-connected outerplane\nbipartite graphs. Firstly, we prove that if $G$ is a plane elementary bipartite\ngraph other than $K_2$, then the resonance graph $R(G)$ is a daisy cube if and\nonly if the Fries number of $G$ equals the number of finite faces of $G$, which\nin turn is equivalent to $G$ being homeomorphically peripheral color\nalternating. Next, we extend the above characterization from plane elementary\nbipartite graphs to all plane bipartite graphs and show that the resonance\ngraph of a plane bipartite graph $G$ is a daisy cube if and only if $G$ is\nweakly elementary bipartite and every elementary component of $G$ other than\n$K_2$ is homeomorphically peripheral color alternating. Along the way, we prove\nthat a Cartesian product graph is a daisy cube if and only if all of its\nnontrivial factors are daisy cubes.",
            "author": [
                "Simon Brezovnik",
                "Zhongyuan Che",
                "Niko Tratnik",
                "Petra \u017digert Pleter\u0161ek"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06508v1",
                "http://arxiv.org/pdf/2311.06508v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C10 05C70 05C75 05C92 05C76"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06503v1",
            "title": "Knowledgeable Preference Alignment for LLMs in Domain-specific Question\n  Answering",
            "updated": "2023-11-11T07:56:40Z",
            "published": "2023-11-11T07:56:40Z",
            "summary": "Recently, the development of large language models (LLMs) has attracted wide\nattention in academia and industry. Deploying LLMs to real scenarios is one of\nthe key directions in the current Internet industry. In this paper, we present\na novel pipeline to apply LLMs for domain-specific question answering (QA) that\nincorporates domain knowledge graphs (KGs), addressing an important direction\nof LLM application. As a real-world application, the content generated by LLMs\nshould be user-friendly to serve the customers. Additionally, the model needs\nto utilize domain knowledge properly to generate reliable answers. These two\nissues are the two major difficulties in the LLM application as vanilla\nfine-tuning can not adequately address them. We think both requirements can be\nunified as the model preference problem that needs to align with humans to\nachieve practical application. Thus, we introduce Knowledgeable Preference\nAlignmenT (KnowPAT), which constructs two kinds of preference set called style\npreference set and knowledge preference set respectively to tackle the two\nissues. Besides, we design a new alignment objective to align the LLM\npreference with human preference, aiming to train a better LLM for\nreal-scenario domain-specific QA to generate reliable and user-friendly\nanswers. Adequate experiments and comprehensive with 15 baseline methods\ndemonstrate that our KnowPAT is an outperforming pipeline for real-scenario\ndomain-specific QA with LLMs. Our code is open-source at\nhttps://github.com/zjukg/KnowPAT.",
            "author": [
                "Yichi Zhang",
                "Zhuo Chen",
                "Yin Fang",
                "Lei Cheng",
                "Yanxi Lu",
                "Fangming Li",
                "Wen Zhang",
                "Huajun Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06503v1",
                "http://arxiv.org/pdf/2311.06503v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06500v1",
            "title": "Knowledge Distillation and Training Balance for Heterogeneous\n  Decentralized Multi-Modal Learning over Wireless Networks",
            "updated": "2023-11-11T07:41:51Z",
            "published": "2023-11-11T07:41:51Z",
            "summary": "Decentralized learning is widely employed for collaboratively training models\nusing distributed data over wireless networks. Existing decentralized learning\nmethods primarily focus on training single-modal networks. For the\ndecentralized multi-modal learning (DMML), the modality heterogeneity and the\nnon-independent and non-identically distributed (non-IID) data across devices\nmake it difficult for the training model to capture the correlated features\nacross different modalities. Moreover, modality competition can result in\ntraining imbalance among different modalities, which can significantly impact\nthe performance of DMML. To improve the training performance in the presence of\nnon-IID data and modality heterogeneity, we propose a novel DMML with knowledge\ndistillation (DMML-KD) framework, which decomposes the extracted feature into\nthe modality-common and the modality-specific components. In the proposed\nDMML-KD, a generator is applied to learn the global conditional distribution of\nthe modality-common features, thereby guiding the modality-common features of\ndifferent devices towards the same distribution. Meanwhile, we propose to\ndecrease the number of local iterations for the modalities with fast training\nspeed in DMML-KD to address the imbalanced training. We design a balance metric\nbased on the parameter variation to evaluate the training speed of different\nmodalities in DMML-KD. Using this metric, we optimize the number of local\niterations for different modalities on each device under the constraint of\nremaining energy on devices. Experimental results demonstrate that the proposed\nDMML-KD with training balance can effectively improve the training performance\nof DMML.",
            "author": [
                "Benshun Yin",
                "Zhiyong Chen",
                "Meixia Tao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06500v1",
                "http://arxiv.org/pdf/2311.06500v1"
            ],
            "primary_category": "cs.IT",
            "category": [
                "cs.IT",
                "math.IT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06493v1",
            "title": "L3 Ensembles: Lifelong Learning Approach for Ensemble of Foundational\n  Language Models",
            "updated": "2023-11-11T06:59:50Z",
            "published": "2023-11-11T06:59:50Z",
            "summary": "Fine-tuning pre-trained foundational language models (FLM) for specific tasks\nis often impractical, especially for resource-constrained devices. This\nnecessitates the development of a Lifelong Learning (L3) framework that\ncontinuously adapts to a stream of Natural Language Processing (NLP) tasks\nefficiently. We propose an approach that focuses on extracting meaningful\nrepresentations from unseen data, constructing a structured knowledge base, and\nimproving task performance incrementally. We conducted experiments on various\nNLP tasks to validate its effectiveness, including benchmarks like GLUE and\nSuperGLUE. We measured good performance across the accuracy, training\nefficiency, and knowledge transfer metrics. Initial experimental results show\nthat the proposed L3 ensemble method increases the model accuracy by 4% ~ 36%\ncompared to the fine-tuned FLM. Furthermore, L3 model outperforms naive\nfine-tuning approaches while maintaining competitive or superior performance\n(up to 15.4% increase in accuracy) compared to the state-of-the-art language\nmodel (T5) for the given task, STS benchmark.",
            "author": [
                "Aidin Shiri",
                "Kaushik Roy",
                "Amit Sheth",
                "Manas Gaur"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06493v1",
                "http://arxiv.org/pdf/2311.06493v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06490v1",
            "title": "Integer Programming Formulations and Probabilistic Bounds for Some\n  Domination Parameters",
            "updated": "2023-11-11T06:13:10Z",
            "published": "2023-11-11T06:13:10Z",
            "summary": "In this paper, we further study the concepts of hop domination and 2-step\ndomination and introduce the concepts of restrained hop domination, total\nrestrained hop domination, 2-step restrained domination, and total 2-step\nrestrained domination in graphs. We then construct integer programming\nformulations and present probabilistic upper bounds for these domination\nparameters.",
            "author": [
                "Mhelmar A. Labendia",
                "Clifford R. Pornia"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06490v1",
                "http://arxiv.org/pdf/2311.06490v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06487v2",
            "title": "An Augmented Index-based Efficient Community Search for Large Directed\n  Graphs",
            "updated": "2023-11-17T01:12:13Z",
            "published": "2023-11-11T05:54:59Z",
            "summary": "Given a graph G and a query vertex q, the topic of community search (CS),\naiming to retrieve a dense subgraph of G containing q, has gained much\nattention. Most existing works focus on undirected graphs which overlooks the\nrich information carried by the edge directions. Recently, the problem of\ncommunity search over directed graphs (or CSD problem) has been studied; it\nfinds a connected subgraph containing q, where the in-degree and out-degree of\neach vertex within the subgraph are at least k and l, respectively. However,\nexisting solutions are inefficient, especially on large graphs. To tackle this\nissue, in this paper, we propose a novel index called D-Forest, which allows a\nCSD query to be completed within the optimal time cost. We further propose\nefficient index construction methods. Extensive experiments on six real large\ngraphs show that our index-based query algorithm is up to two orders of\nmagnitude faster than existing solutions.",
            "author": [
                "Yankai Chen",
                "Jie Zhang",
                "Yixiang Fang",
                "Xin Cao",
                "Irwin King"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06487v2",
                "http://arxiv.org/pdf/2311.06487v2"
            ],
            "primary_category": "cs.DB",
            "category": [
                "cs.DB"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06477v3",
            "title": "Report of the 1st Workshop on Generative AI and Law",
            "updated": "2023-12-03T03:09:16Z",
            "published": "2023-11-11T04:13:37Z",
            "summary": "This report presents the takeaways of the inaugural Workshop on Generative AI\nand Law (GenLaw), held in July 2023. A cross-disciplinary group of\npractitioners and scholars from computer science and law convened to discuss\nthe technical, doctrinal, and policy challenges presented by law for Generative\nAI, and by Generative AI for law, with an emphasis on U.S. law in particular.\nWe begin the report with a high-level statement about why Generative AI is both\nimmensely significant and immensely challenging for law. To meet these\nchallenges, we conclude that there is an essential need for 1) a shared\nknowledge base that provides a common conceptual language for experts across\ndisciplines; 2) clarification of the distinctive technical capabilities of\ngenerative-AI systems, as compared and contrasted to other computer and AI\nsystems; 3) a logical taxonomy of the legal issues these systems raise; and, 4)\na concrete research agenda to promote collaboration and knowledge-sharing on\nemerging issues at the intersection of Generative AI and law. In this report,\nwe synthesize the key takeaways from the GenLaw workshop that begin to address\nthese needs. All of the listed authors contributed to the workshop upon which\nthis report is based, but they and their organizations do not necessarily\nendorse all of the specific claims in this report.",
            "author": [
                "A. Feder Cooper",
                "Katherine Lee",
                "James Grimmelmann",
                "Daphne Ippolito",
                "Christopher Callison-Burch",
                "Christopher A. Choquette-Choo",
                "Niloofar Mireshghallah",
                "Miles Brundage",
                "David Mimno",
                "Madiha Zahrah Choksi",
                "Jack M. Balkin",
                "Nicholas Carlini",
                "Christopher De Sa",
                "Jonathan Frankle",
                "Deep Ganguli",
                "Bryant Gipson",
                "Andres Guadamuz",
                "Swee Leng Harris",
                "Abigail Z. Jacobs",
                "Elizabeth Joh",
                "Gautam Kamath",
                "Mark Lemley",
                "Cass Matthews",
                "Christine McLeavey",
                "Corynne McSherry",
                "Milad Nasr",
                "Paul Ohm",
                "Adam Roberts",
                "Tom Rubin",
                "Pamela Samuelson",
                "Ludwig Schubert",
                "Kristen Vaccaro",
                "Luis Villa",
                "Felix Wu",
                "Elana Zeide"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06477v3",
                "http://arxiv.org/pdf/2311.06477v3"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06458v1",
            "title": "Conditional Adjustment in a Markov Equivalence Class",
            "updated": "2023-11-11T02:14:07Z",
            "published": "2023-11-11T02:14:07Z",
            "summary": "We consider the problem of identifying a conditional causal effect through\ncovariate adjustment. We focus on the setting where the causal graph is known\nup to one of two types of graphs: a maximally oriented partially directed\nacyclic graph (MPDAG) or a partial ancestral graph (PAG). Both MPDAGs and PAGs\nrepresent equivalence classes of possible underlying causal models. After\ndefining adjustment sets in this setting, we provide a necessary and sufficient\ngraphical criterion -- the conditional adjustment criterion -- for finding\nthese sets under conditioning on variables unaffected by treatment. We further\nprovide explicit sets from the graph that satisfy the conditional adjustment\ncriterion, and therefore, can be used as adjustment sets for conditional causal\neffect identification.",
            "author": [
                "Sara LaPlante",
                "Emilija Perkovi\u0107"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06458v1",
                "http://arxiv.org/pdf/2311.06458v1"
            ],
            "primary_category": "stat.ME",
            "category": [
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06456v2",
            "title": "Asymmetric Contrastive Multimodal Learning for Advancing Chemical\n  Understanding",
            "updated": "2023-11-20T21:40:55Z",
            "published": "2023-11-11T01:58:45Z",
            "summary": "The versatility of multimodal deep learning holds tremendous promise for\nadvancing scientific research and practical applications. As this field\ncontinues to evolve, the collective power of cross-modal analysis promises to\ndrive transformative innovations, leading us to new frontiers in chemical\nunderstanding and discovery. Hence, we introduce Asymmetric Contrastive\nMultimodal Learning (ACML) as a novel approach tailored for molecules,\nshowcasing its potential to advance the field of chemistry. ACML harnesses the\npower of effective asymmetric contrastive learning to seamlessly transfer\ninformation from various chemical modalities to molecular graph\nrepresentations. By combining pre-trained chemical unimodal encoders and a\nshallow-designed graph encoder, ACML facilitates the assimilation of\ncoordinated chemical semantics from different modalities, leading to\ncomprehensive representation learning with efficient training. This innovative\nframework enhances the interpretability of learned representations and bolsters\nthe expressive power of graph neural networks. Through practical tasks such as\nisomer discrimination and uncovering crucial chemical properties for drug\ndiscovery, ACML exhibits its capability to revolutionize chemical research and\napplications, providing a deeper understanding of chemical semantics of\ndifferent modalities.",
            "author": [
                "Hao Xu",
                "Yifei Wang",
                "Yunrui Li",
                "Pengyu Hong"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06456v2",
                "http://arxiv.org/pdf/2311.06456v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06438v1",
            "title": "Controllability-Constrained Deep Network Models for Enhanced Control of\n  Dynamical Systems",
            "updated": "2023-11-11T00:04:26Z",
            "published": "2023-11-11T00:04:26Z",
            "summary": "Control of a dynamical system without the knowledge of dynamics is an\nimportant and challenging task. Modern machine learning approaches, such as\ndeep neural networks (DNNs), allow for the estimation of a dynamics model from\ncontrol inputs and corresponding state observation outputs. Such data-driven\nmodels are often utilized for the derivation of model-based controllers.\nHowever, in general, there are no guarantees that a model represented by DNNs\nwill be controllable according to the formal control-theoretical meaning of\ncontrollability, which is crucial for the design of effective controllers. This\noften precludes the use of DNN-estimated models in applications, where formal\ncontrollability guarantees are required. In this proof-of-the-concept work, we\npropose a control-theoretical method that explicitly enhances models estimated\nfrom data with controllability. That is achieved by augmenting the model\nestimation objective with a controllability constraint, which penalizes models\nwith a low degree of controllability. As a result, the models estimated with\nthe proposed controllability constraint allow for the derivation of more\nefficient controllers, they are interpretable by the control-theoretical\nquantities and have a lower long-term prediction error. The proposed method\nprovides new insights on the connection between the DNN-based estimation of\nunknown dynamics and the control-theoretical guarantees of the solution\nproperties. We demonstrate the superiority of the proposed method in two\nstandard classical control systems with state observation given by low\nresolution high-dimensional images.",
            "author": [
                "Suruchi Sharma",
                "Volodymyr Makarenko",
                "Gautam Kumar",
                "Stas Tiomkin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06438v1",
                "http://arxiv.org/pdf/2311.06438v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.AI",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06418v1",
            "title": "A Computationally Efficient Hybrid Neural Network Architecture for\n  Porous Media: Integrating CNNs and GNNs for Improved Permeability Prediction",
            "updated": "2023-11-10T22:42:11Z",
            "published": "2023-11-10T22:42:11Z",
            "summary": "Subsurface fluid flow, essential in various natural and engineered processes,\nis largely governed by a rock's permeability, which describes its ability to\nallow fluid passage. While convolutional neural networks (CNNs) have been\nemployed to estimate permeability from high-resolution 3D rock images, our\nnovel visualization technology reveals that they occasionally miss higher-level\ncharacteristics, such as nuanced connectivity and flow paths, within porous\nmedia. To address this, we propose a novel fusion model to integrate CNN with\nthe graph neural network (GNN), which capitalizes on graph representations\nderived from pore network model to capture intricate relational data between\npores. The permeability prediction accuracy of the fusion model is superior to\nthe standalone CNN, whereas its total parameter number is nearly two orders of\nmagnitude lower than the latter. This innovative approach not only heralds a\nnew frontier in the research of digital rock property predictions, but also\ndemonstrates remarkable improvements in prediction accuracy and efficiency,\nemphasizing the transformative potential of hybrid neural network architectures\nin subsurface fluid flow research.",
            "author": [
                "Qingqi Zhao",
                "Xiaoxue Han",
                "Ruichang Guo",
                "Cheng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06418v1",
                "http://arxiv.org/pdf/2311.06418v1"
            ],
            "primary_category": "physics.geo-ph",
            "category": [
                "physics.geo-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06414v1",
            "title": "Knowledge Graphs are not Created Equal: Exploring the Properties and\n  Structure of Real KGs",
            "updated": "2023-11-10T22:18:09Z",
            "published": "2023-11-10T22:18:09Z",
            "summary": "Despite the recent popularity of knowledge graph (KG) related tasks and\nbenchmarks such as KG embeddings, link prediction, entity alignment and\nevaluation of the reasoning abilities of pretrained language models as KGs, the\nstructure and properties of real KGs are not well studied. In this paper, we\nperform a large scale comparative study of 29 real KG datasets from diverse\ndomains such as the natural sciences, medicine, and NLP to analyze their\nproperties and structural patterns. Based on our findings, we make several\nrecommendations regarding KG-based model development and evaluation. We believe\nthat the rich structural information contained in KGs can benefit the\ndevelopment of better KG models across fields and we hope this study will\ncontribute to breaking the existing data silos between different areas of\nresearch (e.g., ML, NLP, AI for sciences).",
            "author": [
                "Nedelina Teneva",
                "Estevam Hruschka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06414v1",
                "http://arxiv.org/pdf/2311.06414v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06408v1",
            "title": "Fitting multiple small-angle scattering datasets simultaneously: on the\n  optimal use of priors and weights",
            "updated": "2023-11-10T21:55:26Z",
            "published": "2023-11-10T21:55:26Z",
            "summary": "Small-angle X-ray and neutron scattering (SAXS and SANS) are powerful\ntechniques for elucidating the structure of diverse particles and materials.\nThis study address the challenge of effectively combining SAXS and SANS data\nfor accurate structural parameter determination. Surprisingly, our results\ndemonstrate that equally weighting all data points leads to the most accurate\nparameter estimation, even when SAXS data significantly outnumber SANS data. We\ncompared this approach with weighting schemes normalized by the number of\npoints and by the derived information content. Furthermore, we assessed the\nimpact of prior knowledge by incorporating Gaussian priors for model\nparameters. Our findings indicate that Gaussian priors improve the accuracy of\nrefined parameter values compared to uniform priors. When using a minimum and a\nmaximum values for model parameters, which is common practice, uniform priors\nare implicitely applied. Finally, we show that utilizing information content\naids in determining the degrees of freedom, enabling accurate calculation of\nthe goodness of fit. In conclusion, this research provides valuable insights\ninto the optimal combination of SAXS and SANS data, emphasizing the importance\nof weighting schemes and prior knowledge for enhanced accuracy in structural\nparameter determination.",
            "author": [
                "Andreas Haahr Larsen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06408v1",
                "http://arxiv.org/pdf/2311.06408v1"
            ],
            "primary_category": "cond-mat.soft",
            "category": [
                "cond-mat.soft",
                "physics.bio-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06407v1",
            "title": "On the Connectivity of the Vietoris-Rips Complex of a Hypercube Graph",
            "updated": "2023-11-10T21:54:12Z",
            "published": "2023-11-10T21:54:12Z",
            "summary": "We bring in the techniques of independence complexes and the notion of total\ndominating sets of a graph to bear on the question of the connectivity of the\nVietoris-Rips complexes $VR(Q_n; r)$ of an $n$-hypercube graph. We obtain a\nlower bound for the connectivity of $VR(Q_n; r)$ for an arbitrary $n$-dimension\nhypercube and at all scale parameters $r$. The obtained bounds disprove the\nconjecture of Shukla that $\\VR$ is $r$-connected.",
            "author": [
                "Martin Bendersky",
                "Jelena Grbic"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06407v1",
                "http://arxiv.org/pdf/2311.06407v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "math.AT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06402v1",
            "title": "A Dynamic Shortest Paths Toolbox: Low-Congestion Vertex Sparsifiers and\n  their Applications",
            "updated": "2023-11-10T21:32:53Z",
            "published": "2023-11-10T21:32:53Z",
            "summary": "We present a general toolbox, based on new vertex sparsifiers, for designing\ndata structures to maintain shortest paths in dynamic graphs.\n  In an $m$-edge graph undergoing edge insertions and deletions, our data\nstructures give the first algorithms for maintaining (a) $m^{o(1)}$-approximate\nall-pairs shortest paths (APSP) with \\emph{worst-case} update time $m^{o(1)}$\nand query time $\\tilde{O}(1)$, and (b) a tree $T$ that has diameter no larger\nthan a subpolynomial factor times the diameter of the underlying graph, where\neach update is handled in amortized subpolynomial time.\n  In graphs undergoing only edge deletions, we develop a simpler and more\nefficient data structure to maintain a $(1+\\epsilon)$-approximate single-source\nshortest paths (SSSP) tree $T$ in a graph undergoing edge deletions in\namortized time $m^{o(1)}$ per update.\n  Our data structures are deterministic. The trees we can maintain are not\nsubgraphs of $G$, but embed with small edge congestion into $G$. This is in\nstark contrast to previous approaches and is useful for algorithms that\ninternally use trees to route flow.\n  To illustrate the power of our new toolbox, we show that our SSSP data\nstructure gives simple deterministic implementations of flow-routing MWU\nmethods in several contexts, where previously only randomized methods had been\nknown.\n  To obtain our toolbox, we give the first algorithm that, given a graph $G$\nundergoing edge insertions and deletions and a dynamic terminal set $A$,\nmaintains a vertex sparsifier $H$ that approximately preserves distances\nbetween terminals in $A$, consists of at most $|A|m^{o(1)}$ vertices and edges,\nand can be updated in worst-case time $m^{o(1)}$.\n  Crucially, our vertex sparsifier construction allows us to maintain a low\nedge-congestion embedding of $H$ into $G$, which is needed for our\napplications.",
            "author": [
                "Rasmus Kyng",
                "Simon Meierhans",
                "Maximilian Probst Gutenberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06402v1",
                "http://arxiv.org/pdf/2311.06402v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06392v2",
            "title": "Sequential commutation in tracial von Neumann algebras",
            "updated": "2023-11-20T21:36:06Z",
            "published": "2023-11-10T20:48:18Z",
            "summary": "Recall that a unitary in a tracial von Neumann algebra is Haar if\n$\\tau(u^n)=0$ for all $n\\in \\mathbb{N}$. We introduce and study a new Borel\nequivalence relation $\\sim_N$ on the set of Haar unitaries in a diffuse tracial\nvon Neumann algebra $N$. Two Haar unitaries $u,v$ in $\\mathcal{U}(N)$ are\nrelated if there exists a finite path of sequentially commuting Haar unitaries\nin an ultrapower $N^\\mathcal{U}$, beginning at $u$ and ending at $v$. We show\nthat for any diffuse tracial von Neumann algebra $N$, the equivalence relation\n$\\sim_N$ admits either 1 orbit or uncountably many orbits. We characterize\nproperty Gamma in terms of path length and number of orbits of $\\sim_N$ and\nalso show the existence of non-Gamma II$_1$ factors so that $\\sim_N$ admits\nonly 1 orbit. Examples where $\\sim_N$ admits uncountably many orbits include\n$N$ having positive 1-bounded entropy: $h(N)>0$. As a key example, we\nexplicitly describe $\\sim_{L(\\mathbb{F}_t)}$ for the free group factors. Using\nthese ideas we introduce a numerical invariant for diffuse tracial von Neumann\nalgebras called the commutation diameter, with applications to elementary\nequivalence classification. We compute lower and upper bounds for the\ncommutation diameter in various examples. Notably we obtain non-trivial lower\nbounds for the family of arbitrary graph products $N$ of diffuse tracial von\nNeumann algebras whose underlying graph is connected and has diameter at least\n4, and distinguish them up to elementary equivalence from the [CIKE23] exotic\nfactors, despite satisfying $h(N)\\leq 0$.",
            "author": [
                "Srivatsav Kunnawalkam Elayavalli",
                "Gregory Patchell"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06392v2",
                "http://arxiv.org/pdf/2311.06392v2"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "math.GR",
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06387v1",
            "title": "Root and weight semigroup rings for signed posets",
            "updated": "2023-11-10T20:34:01Z",
            "published": "2023-11-10T20:34:01Z",
            "summary": "We consider a pair of semigroups associated to a signed poset, called the\nroot semigroup and the weight semigroup, and their semigroup rings,\n$R_P^\\mathrm{rt}$ and $R_P^\\mathrm{wt}$, respectively.\n  Theorem 4.1.5 gives generators for the toric ideal of affine semigroup rings\nassociated to signed posets and, more generally, oriented signed graphs. These\nare the subrings of Laurent polynomials generated by monomials of the form\n$t_i^{\\pm 1},t_i^{\\pm 2},t_i^{\\pm 1}t_j^{\\pm 1}$. This result appears to be new\nand generalizes work of Boussicault, F\\'eray, Lascoux and Reiner, of Gitler,\nReyes, and Villarreal, and of Villarreal. Theorem 4.2.12 shows that strongly\nplanar signed posets $P$ have rings $R_P^\\mathrm{rt}$,\n$R_{P^{\\scriptscriptstyle\\vee}}$ which are complete intersections, with\nCorollary 4.2.20 showing how to compute $\\Psi_P$ in this case. Theorem 5.2.3\ngives a Gr\\\"obner basis for the toric ideal of $R_P^{\\mathrm{wt}}$ in type B,\ngeneralizing Proposition 6.4 of F\\'eray and Reiner. Theorems 5.3.10 and 5.3.1\ngive two characterizations (via forbidden subposets versus via inductive\nconstructions) of the situation where this Gr\\\"obner basis gives a complete\nintersection presentation for its initial ideal, generalizing Theorems 10.5 and\n10.6 of F\\'eray and Reiner.",
            "author": [
                "Sebastian A. Csar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06387v1",
                "http://arxiv.org/pdf/2311.06387v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06383v1",
            "title": "Distilling Large Language Models using Skill-Occupation Graph Context\n  for HR-Related Tasks",
            "updated": "2023-11-10T20:25:42Z",
            "published": "2023-11-10T20:25:42Z",
            "summary": "Numerous HR applications are centered around resumes and job descriptions.\nWhile they can benefit from advancements in NLP, particularly large language\nmodels, their real-world adoption faces challenges due to absence of\ncomprehensive benchmarks for various HR tasks, and lack of smaller models with\ncompetitive capabilities. In this paper, we aim to bridge this gap by\nintroducing the Resume-Job Description Benchmark (RJDB). We meticulously craft\nthis benchmark to cater to a wide array of HR tasks, including matching and\nexplaining resumes to job descriptions, extracting skills and experiences from\nresumes, and editing resumes. To create this benchmark, we propose to distill\ndomain-specific knowledge from a large language model (LLM). We rely on a\ncurated skill-occupation graph to ensure diversity and provide context for LLMs\ngeneration. Our benchmark includes over 50 thousand triples of job\ndescriptions, matched resumes and unmatched resumes. Using RJDB, we train\nmultiple smaller student models. Our experiments reveal that the student models\nachieve near/better performance than the teacher model (GPT-4), affirming the\neffectiveness of the benchmark. Additionally, we explore the utility of RJDB on\nout-of-distribution data for skill extraction and resume-job description\nmatching, in zero-shot and weak supervision manner. We release our datasets and\ncode to foster further research and industry applications.",
            "author": [
                "Pouya Pezeshkpour",
                "Hayate Iso",
                "Thom Lake",
                "Nikita Bhutani",
                "Estevam Hruschka"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06383v1",
                "http://arxiv.org/pdf/2311.06383v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06380v1",
            "title": "Theory and implementation of inelastic Constitutive Artificial Neural\n  Networks",
            "updated": "2023-11-10T20:13:29Z",
            "published": "2023-11-10T20:13:29Z",
            "summary": "Nature has always been our inspiration in the research, design and\ndevelopment of materials and has driven us to gain a deep understanding of the\nmechanisms that characterize anisotropy and inelastic behavior. All this\nknowledge has been accumulated in the principles of thermodynamics. Deduced\nfrom these principles, the multiplicative decomposition combined with pseudo\npotentials are powerful and universal concepts. Simultaneously, the tremendous\nincrease in computational performance enabled us to investigate and rethink our\nhistory-dependent material models to make the most of our predictions. Today,\nwe have reached a point where materials and their models are becoming\nincreasingly sophisticated. This raises the question: How do we find the best\nmodel that includes all inelastic effects to explain our complex data?\nConstitutive Artificial Neural Networks (CANN) may answer this question. Here,\nwe extend the CANNs to inelastic materials (iCANN). Rigorous considerations of\nobjectivity, rigid motion of the reference configuration, multiplicative\ndecomposition and its inherent non-uniqueness, restrictions of energy and\npseudo potential, and consistent evolution guide us towards the architecture of\nthe iCANN satisfying thermodynamics per design. We combine feed-forward\nnetworks of the free energy and pseudo potential with a recurrent neural\nnetwork approach to take time dependencies into account. We demonstrate that\nthe iCANN is capable of autonomously discovering models for artificially\ngenerated data, the response of polymers for cyclic loading and the relaxation\nbehavior of muscle data. As the design of the network is not limited to\nvisco-elasticity, our vision is that the iCANN will reveal to us new ways to\nfind the various inelastic phenomena hidden in the data and to understand their\ninteraction. Our source code, data, and examples are available at\ndoi.org/10.5281/zenodo.10066805",
            "author": [
                "Hagen Holthusen",
                "Lukas Lamm",
                "Tim Brepols",
                "Stefanie Reese",
                "Ellen Kuhl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06380v1",
                "http://arxiv.org/pdf/2311.06380v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci",
                "65, 74",
                "I.6; J.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06375v1",
            "title": "Image Classification using Combination of Topological Features and\n  Neural Networks",
            "updated": "2023-11-10T20:05:40Z",
            "published": "2023-11-10T20:05:40Z",
            "summary": "In this work we use the persistent homology method, a technique in\ntopological data analysis (TDA), to extract essential topological features from\nthe data space and combine them with deep learning features for classification\ntasks. In TDA, the concepts of complexes and filtration are building blocks.\nFirstly, a filtration is constructed from some complex. Then, persistent\nhomology classes are computed, and their evolution along the filtration is\nvisualized through the persistence diagram. Additionally, we applied\nvectorization techniques to the persistence diagram to make this topological\ninformation compatible with machine learning algorithms. This was carried out\nwith the aim of classifying images from multiple classes in the MNIST dataset.\nOur approach inserts topological features into deep learning approaches\ncomposed by single and two-streams neural networks architectures based on a\nmulti-layer perceptron (MLP) and a convolutional neral network (CNN) taylored\nfor multi-class classification in the MNIST dataset. In our analysis, we\nevaluated the obtained results and compared them with the outcomes achieved\nthrough the baselines that are available in the TensorFlow library. The main\nconclusion is that topological information may increase neural network accuracy\nin multi-class classification tasks with the price of computational complexity\nof persistent homology calculation. Up to the best of our knowledge, it is the\nfirst work that combines deep learning features and the combination of\ntopological features for multi-class classification tasks.",
            "author": [
                "Mariana D\u00f3ria Prata Lima",
                "Gilson Antonio Giraldi",
                "Gast\u00e3o Flor\u00eancio Miranda Junior"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06375v1",
                "http://arxiv.org/pdf/2311.06375v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG",
                "cs.NE",
                "57T99, 57Z25",
                "I.4.10; I.2.10; I.5.4"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06367v1",
            "title": "The critical polynomial of a graph",
            "updated": "2023-11-10T19:40:51Z",
            "published": "2023-11-10T19:40:51Z",
            "summary": "Let $G$ be a connected graph on $n$ vertices with adjacency matrix $A_G$.\nAssociated to $G$ is a polynomial $d_G(x_1,\\dots, x_n)$ of degree $n$ in $n$\nvariables, obtained as the determinant of the matrix $M_G(x_1,\\dots,x_n)$,\nwhere $M_G={\\rm Diag}(x_1,\\dots,x_n)-A_G$. We investigate in this article the\nset $V_{d_G}(r)$ of non-negative values taken by this polynomial when $x_1,\n\\dots, x_n \\geq r \\geq 1$. We show that $V_{d_G}(1) = {\\mathbb Z}_{\\geq 0}$. We\nshow that for a large class of graphs one also has $V_{d_G}(2) = {\\mathbb\nZ}_{\\geq 0}$. When $V_{d_G}(2) \\neq {\\mathbb Z}_{\\geq 0}$, we show that for\nmany graphs $V_{d_G}(2) $ is dense in $ {\\mathbb Z}_{\\geq 0}$. We give\nnumerical evidence that in many cases, the complement of $V_{d_G}(2) $ in $\n{\\mathbb Z}_{\\geq 0}$ might in fact be finite. As a byproduct of our results,\nwe show that every graph can be endowed with an arithmetical structure whose\nassociated group is trivial.",
            "author": [
                "Dino Lorenzini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06367v1",
                "http://arxiv.org/pdf/2311.06367v1"
            ],
            "primary_category": "math.NT",
            "category": [
                "math.NT",
                "05C50, 11C20"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06356v1",
            "title": "Extragalactic Magnetism with SOFIA (SALSA Legacy Program). VII. A\n  tomographic view of far infrared and radio polarimetric observations through\n  MHD simulations of galaxies",
            "updated": "2023-11-10T19:07:39Z",
            "published": "2023-11-10T19:07:39Z",
            "summary": "The structure of magnetic fields in galaxies remains poorly constrained,\ndespite the importance of magnetism in the evolution of galaxies. Radio\nsynchrotron and far-infrared dust polarization (FIR) polarimetric observations\nare the best methods to measure galactic scale properties of magnetic fields in\ngalaxies beyond the Milky Way. We use synthetic polarimetric observations of a\nsimulated galaxy to identify and quantify the regions, scales, and interstellar\nmedium (ISM) phases probed at FIR and radio wavelengths. Our studied suite of\nmagnetohydrodynamical cosmological zoom-in simulations features\nhigh-resolutions (10 pc full-cell size) and multiple magnetization models. Our\nsynthetic observations have a striking resemblance to those of observed\ngalaxies. We find that the total and polarized radio emission extends to\napproximately double the altitude above the galactic disk (half-intensity disk\nthickness of $h_\\text{I radio} \\sim h_\\text{PI radio} = 0.23 \\pm 0.03$ kpc)\nrelative to the FIR total and polarized emission that are concentrated in the\ndisk midplane ($h_\\text{I FIR} \\sim h_\\text{PI FIR} = 0.11 \\pm 0.01$ kpc).\nRadio emission traces magnetic fields at scales of $\\gtrsim 300$ pc, whereas\nFIR emission probes magnetic fields at the smallest scales of our simulations.\nThese scales are comparable to our spatial resolution and well below the\nspatial resolution ($<300$ pc) of existing FIR polarimetric measurements.\nFinally, we confirm that synchrotron emission traces a combination of the warm\nneutral and cold neutral gas phases, whereas FIR emission follows the densest\ngas in the cold neutral phase in the simulation. These results are independent\nof the ISM magnetic field strength. The complementarity we measure between\nradio and FIR wavelengths motivates future multiwavelength polarimetric\nobservations to advance our knowledge of extragalactic magnetism.",
            "author": [
                "Sergio Martin-Alvarez",
                "Enrique Lopez-Rodriguez",
                "Tara Dacunha",
                "Susan E. Clark",
                "Alejandro S. Borlaff",
                "Rainer Beck",
                "Francisco Rodr\u00edguez Montero",
                "S. Lyla Jung",
                "Julien Devriendt",
                "Adrianne Slyz",
                "Julia Roman-Duval",
                "Evangelia Ntormousi",
                "Mehrnoosh Tahani",
                "Kandaswamy Subramanian",
                "Daniel A. Dale",
                "Pamela M. Marcum",
                "Konstantinos Tassis",
                "Ignacio del Moral-Castro",
                "Le Ngoc Tram",
                "Matt J. Jarvis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06356v1",
                "http://arxiv.org/pdf/2311.06356v1"
            ],
            "primary_category": "astro-ph.GA",
            "category": [
                "astro-ph.GA"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06345v1",
            "title": "Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking",
            "updated": "2023-11-10T19:00:02Z",
            "published": "2023-11-10T19:00:02Z",
            "summary": "Tracking dialogue states is an essential topic in task-oriented dialogue\nsystems, which involve filling in the necessary information in pre-defined\nslots corresponding to a schema. While general pre-trained language models have\nbeen shown effective in slot-filling, their performance is limited when applied\nto specific domains. We propose a graph-based framework that learns\ndomain-specific prompts by incorporating the dialogue schema. Specifically, we\nembed domain-specific schema encoded by a graph neural network into the\npre-trained language model, which allows for relations in the schema to guide\nthe model for better adaptation to the specific domain. Our experiments\ndemonstrate that the proposed graph-based method outperforms other multi-domain\nDST approaches while using similar or fewer trainable parameters. We also\nconduct a comprehensive study of schema graph architectures, parameter usage,\nand module ablation that demonstrate the effectiveness of our model on\nmulti-domain dialogue state tracking.",
            "author": [
                "Ruolin Su",
                "Ting-Wei Wu",
                "Biing-Hwang Juang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06345v1",
                "http://arxiv.org/pdf/2311.06345v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06235v1",
            "title": "Triviality of critical Fortuin-Kasteleyn decorated planar maps for $q>4$",
            "updated": "2023-11-10T18:50:37Z",
            "published": "2023-11-10T18:50:37Z",
            "summary": "We consider infinite random planar maps decorated by the critical\nFortuin-Kasteleyn model with parameter $q>4$. The paper demonstrates that when\nappropriately rescaled, these maps converge in law to the infinite continuum\nrandom tree as pointed metric-measure spaces, that is, with respect to the\nlocal Gromov-Hausdorff-Prokhorov topology. Furthermore, we also show that these\nmaps do not admit any Fortuin-Kasteleyn loops with a macroscopic graph distance\ndiameter. Our proof is based on Scott Sheffield's hamburger-cheeseburger\nbijection.",
            "author": [
                "Yuyang Feng"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06235v1",
                "http://arxiv.org/pdf/2311.06235v1"
            ],
            "primary_category": "math.PR",
            "category": [
                "math.PR",
                "math-ph",
                "math.CO",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06232v1",
            "title": "Better Sparsifiers for Directed Eulerian Graphs",
            "updated": "2023-11-10T18:48:49Z",
            "published": "2023-11-10T18:48:49Z",
            "summary": "Spectral sparsification for directed Eulerian graphs is a key component in\nthe design of fast algorithms for solving directed Laplacian linear systems.\nDirected Laplacian linear system solvers are crucial algorithmic primitives to\nfast computation of fundamental problems on random walks, such as computing\nstationary distribution, hitting and commute time, and personalized PageRank\nvectors. While spectral sparsification is well understood for undirected graphs\nand it is known that for every graph $G,$ $(1+\\varepsilon)$-sparsifiers with\n$O(n\\varepsilon^{-2})$ edges exist [Batson-Spielman-Srivastava, STOC '09]\n(which is optimal), the best known constructions of Eulerian sparsifiers\nrequire $\\Omega(n\\varepsilon^{-2}\\log^4 n)$ edges and are based on short-cycle\ndecompositions [Chu et al., FOCS '18].\n  In this paper, we give improved constructions of Eulerian sparsifiers,\nspecifically:\n  1. We show that for every directed Eulerian graph $\\vec{G},$ there exist an\nEulerian sparsifier with $O(n\\varepsilon^{-2} \\log^2 n \\log^2\\log n +\nn\\varepsilon^{-4/3}\\log^{8/3} n)$ edges. This result is based on combining\nshort-cycle decompositions [Chu-Gao-Peng-Sachdeva-Sawlani-Wang, FOCS '18,\nSICOMP] and [Parter-Yogev, ICALP '19], with recent progress on the matrix\nSpencer conjecture [Bansal-Meka-Jiang, STOC '23].\n  2. We give an improved analysis of the constructions based on short-cycle\ndecompositions, giving an $m^{1+\\delta}$-time algorithm for any constant\n$\\delta > 0$ for constructing Eulerian sparsifiers with\n$O(n\\varepsilon^{-2}\\log^3 n)$ edges.",
            "author": [
                "Sushant Sachdeva",
                "Anvith Thudi",
                "Yibin Zhao"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06232v1",
                "http://arxiv.org/pdf/2311.06232v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06229v1",
            "title": "Absolute retracts of reflexive oriented graphs: the role of the\n  MacNeille completion",
            "updated": "2023-11-10T18:35:05Z",
            "published": "2023-11-10T18:35:05Z",
            "summary": "We characterize the absolute retracts in the category of reflexive oriented\ngraphs, that is, antisymmetric reflexive graphs, where morphisms between\nobjects preserve arcs (which may be sent to loops). Here we show, by correcting\na much earlier attempt at a proof, that a reflexive oriented graph is an\nabsolute retract if and only if it is indeed a retract of some (direct) product\nof reflexive oriented zigzags (which are concatenations of reflexive oriented\npaths). Absolute retracts are therefore necessarily acyclic. In contrast to\nother categories of graphs and ordered sets, not every acyclic oriented graph\ncan be embedded isometrically into some absolute retract. Embedding involves\nisometry with respect to the zig-zag distances forming a particular \"dual\nquantale\", which is a complete lattice of certain sets of words over the\nalphabet $\\{+, -\\}$, endowed with an additional monoid operation (viz.,\ncompound concatenation of sets of words) and an involution (interchanging $+$\nand $- $ and then mirroring words). As reflexive oriented zigzags have\nMacNeille-closed distances, so do their products and retracts. So, the category\nof reflexive oriented graphs and its full subcategory of reflexive acyclic\ngraphs do not have enough injectives, as the injective objects coincide with\nthe absolute retracts.",
            "author": [
                "Hans-J\u00fcrgen Bandelt",
                "Maurice Pouzet",
                "Faouzi Sa\u00efdane"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06229v1",
                "http://arxiv.org/pdf/2311.06229v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "06A07, 06A12, 06D22, 08B30, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06228v2",
            "title": "Learning material synthesis-process-structure-property relationship by\n  data fusion: Bayesian Coregionalization N-Dimensional Piecewise Function\n  Learning",
            "updated": "2023-11-20T21:43:52Z",
            "published": "2023-11-10T18:34:24Z",
            "summary": "Autonomous materials research labs require the ability to combine and learn\nfrom diverse data streams. This is especially true for learning material\nsynthesis-process-structure-property relationships, key to accelerating\nmaterials optimization and discovery as well as accelerating mechanistic\nunderstanding. We present the Synthesis-process-structure-property relAtionship\ncoreGionalized lEarner (SAGE) algorithm. A fully Bayesian algorithm that uses\nmultimodal coregionalization to merge knowledge across data sources to learn\nsynthesis-process-structure-property relationships. SAGE outputs a\nprobabilistic posterior for the relationships including the most likely\nrelationships given the data.",
            "author": [
                "A. Gilad Kusne",
                "Austin McDannald",
                "Brian DeCost"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06228v2",
                "http://arxiv.org/pdf/2311.06228v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06200v1",
            "title": "Center-to-limb variation of spectral lines and their effect on full-disk\n  observations",
            "updated": "2023-11-10T17:32:08Z",
            "published": "2023-11-10T17:32:08Z",
            "summary": "An accurate description of the center-to-limb variation (CLV) of stellar\nspectra is becoming an increasingly critical factor in both stellar and\nexoplanet characterization. In particular, the CLV of spectral lines is\nextremely challenging as its characterization requires highly detailed\nknowledge of the stellar physical conditions. To this end, we present the\nNumerical Empirical Sun-as-a-Star Integrator (NESSI) as a tool for translating\nhigh-resolution solar observations of a partial field of view into\ndisk-integrated spectra that can be used to test common assumptions in stellar\nphysics.",
            "author": [
                "Alexander G. M. Pietrow",
                "Adur Pastor Yabar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06200v1",
                "http://arxiv.org/pdf/2311.06200v1"
            ],
            "primary_category": "astro-ph.SR",
            "category": [
                "astro-ph.SR",
                "astro-ph.EP",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06193v1",
            "title": "The Density Formula: One Lemma to Bound Them All",
            "updated": "2023-11-10T17:18:12Z",
            "published": "2023-11-10T17:18:12Z",
            "summary": "We introduce the Density Formula for (topological) drawings of graphs in the\nplane or on the sphere, which relates the number of edges, vertices, crossings,\nand sizes of cells in the drawing. We demonstrate its capability by providing\nseveral applications: we prove tight upper bounds on the edge density of\nvarious beyond-planar graph classes, including so-called $k$-planar graphs with\n$k=1,2$, fan-crossing / fan-planar graphs, $k$-bend RAC-graphs with $k=0,1,2$,\nand quasiplanar graphs. In some cases ($1$-bend and $2$-bend RAC-graphs and\nfan-crossing / fan-planar graphs), we thereby obtain the first tight upper\nbounds on the edge density of the respective graph classes. In other cases, we\ngive new streamlined and significantly shorter proofs for bounds that were\nalready known in the literature. Thanks to the Density Formula, all of our\nproofs are mostly elementary counting and mostly circumvent the typical\nintricate case analysis found in earlier proofs. Further, in some cases (simple\nand non-homotopic quasiplanar graphs), our alternative proofs using the Density\nFormula lead to the first tight lower bound examples.",
            "author": [
                "Michael Kaufmann",
                "Boris Klemz",
                "Kristin Knorr",
                "Meghana M. Reddy",
                "Felix Schr\u00f6der",
                "Torsten Ueckerdt"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06193v1",
                "http://arxiv.org/pdf/2311.06193v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06192v1",
            "title": "Greedy PIG: Adaptive Integrated Gradients",
            "updated": "2023-11-10T17:16:18Z",
            "published": "2023-11-10T17:16:18Z",
            "summary": "Deep learning has become the standard approach for most machine learning\ntasks. While its impact is undeniable, interpreting the predictions of deep\nlearning models from a human perspective remains a challenge. In contrast to\nmodel training, model interpretability is harder to quantify and pose as an\nexplicit optimization problem. Inspired by the AUC softmax information curve\n(AUC SIC) metric for evaluating feature attribution methods, we propose a\nunified discrete optimization framework for feature attribution and feature\nselection based on subset selection. This leads to a natural adaptive\ngeneralization of the path integrated gradients (PIG) method for feature\nattribution, which we call Greedy PIG. We demonstrate the success of Greedy PIG\non a wide variety of tasks, including image feature attribution, graph\ncompression/explanation, and post-hoc feature selection on tabular data. Our\nresults show that introducing adaptivity is a powerful and versatile method for\nmaking attribution methods more powerful.",
            "author": [
                "Kyriakos Axiotis",
                "Sami Abu-al-haija",
                "Lin Chen",
                "Matthew Fahrbach",
                "Gang Fu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06192v1",
                "http://arxiv.org/pdf/2311.06192v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06190v1",
            "title": "FourierGNN: Rethinking Multivariate Time Series Forecasting from a Pure\n  Graph Perspective",
            "updated": "2023-11-10T17:13:26Z",
            "published": "2023-11-10T17:13:26Z",
            "summary": "Multivariate time series (MTS) forecasting has shown great importance in\nnumerous industries. Current state-of-the-art graph neural network (GNN)-based\nforecasting methods usually require both graph networks (e.g., GCN) and\ntemporal networks (e.g., LSTM) to capture inter-series (spatial) dynamics and\nintra-series (temporal) dependencies, respectively. However, the uncertain\ncompatibility of the two networks puts an extra burden on handcrafted model\ndesigns. Moreover, the separate spatial and temporal modeling naturally\nviolates the unified spatiotemporal inter-dependencies in real world, which\nlargely hinders the forecasting performance. To overcome these problems, we\nexplore an interesting direction of directly applying graph networks and\nrethink MTS forecasting from a pure graph perspective. We first define a novel\ndata structure, hypervariate graph, which regards each series value (regardless\nof variates or timestamps) as a graph node, and represents sliding windows as\nspace-time fully-connected graphs. This perspective considers spatiotemporal\ndynamics unitedly and reformulates classic MTS forecasting into the predictions\non hypervariate graphs. Then, we propose a novel architecture Fourier Graph\nNeural Network (FourierGNN) by stacking our proposed Fourier Graph Operator\n(FGO) to perform matrix multiplications in Fourier space. FourierGNN\naccommodates adequate expressiveness and achieves much lower complexity, which\ncan effectively and efficiently accomplish the forecasting. Besides, our\ntheoretical analysis reveals FGO's equivalence to graph convolutions in the\ntime domain, which further verifies the validity of FourierGNN. Extensive\nexperiments on seven datasets have demonstrated our superior performance with\nhigher efficiency and fewer parameters compared with state-of-the-art methods.",
            "author": [
                "Kun Yi",
                "Qi Zhang",
                "Wei Fan",
                "Hui He",
                "Liang Hu",
                "Pengyang Wang",
                "Ning An",
                "Longbing Cao",
                "Zhendong Niu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06190v1",
                "http://arxiv.org/pdf/2311.06190v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06179v3",
            "title": "Cluster Expansion by Transfer Learning from Empirical Potentials",
            "updated": "2023-11-15T13:50:47Z",
            "published": "2023-11-10T16:52:51Z",
            "summary": "Cluster expansions provide effective representations of the potential energy\nlandscape of multicomponent crystalline solids. Notwithstanding major advances\nin cluster expansion implementations, it remains computationally demanding to\nconstruct these expansions for systems of low dimension or with a large number\nof components, such as clusters, interfaces, and multimetallic alloys. We\naddress these challenges by employing transfer learning to accelerate the\ncomputationally demanding step of generating configurational data from first\nprinciples. The proposed approach exploits Bayesian inference to incorporate\nprior knowledge from physics-based or machine-learning empirical potentials,\nenabling one to identify the most informative configurations within a dataset.\nThe efficacy of the method is tested on face-centered cubic Pt:Ni binaries,\nyielding a two- to three-fold reduction in the number of first-principles\ncalculations, while ensuring robust convergence of the energies with low\nstatistical fluctuations.",
            "author": [
                "A. Dana",
                "L. Mu",
                "S. Gelin",
                "S. B. Sinnott",
                "I. Dabo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06179v3",
                "http://arxiv.org/pdf/2311.06179v3"
            ],
            "primary_category": "cond-mat.mtrl-sci",
            "category": [
                "cond-mat.mtrl-sci"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06168v1",
            "title": "Challenges Modeling the Low-Luminosity Type Iax Supernovae",
            "updated": "2023-11-10T16:36:38Z",
            "published": "2023-11-10T16:36:38Z",
            "summary": "Numerical models allow the investigation of phenomena that cannot exist in a\nlaboratory. Computational simulations are therefore essential for advancing our\nknowledge of astrophysics, however, the very nature of simulation requires\nmaking assumptions that can substantially affect their outcome. Here, we\npresent the challenges faced when simulating dim thermonuclear explosions, Type\nIax supernovae. This class of dim events produce a slow moving, sparse ejecta\nthat presents challenges for simulation. We investigate the limitations of the\nequation of state and its applicability to the expanding, cooling ejecta. We\nalso discuss how the \"fluff\", i.e. the low-density gas on the grid in lieu of\nvacuum, inhibits the ejecta as it expands. We explore how the final state of\nthe simulation changes as we vary the character of the burning, which\ninfluences the outcome of the explosion. These challenges are applicable to a\nwide range of astrophysical simulations, and are important to discuss and\novercome as a community.",
            "author": [
                "Catherine Feldman",
                "Ellis Eisenberg",
                "Dean M. Townsley",
                "Alan C. Calder"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06168v1",
                "http://arxiv.org/pdf/2311.06168v1"
            ],
            "primary_category": "astro-ph.HE",
            "category": [
                "astro-ph.HE",
                "astro-ph.IM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06161v2",
            "title": "On irredundance coloring and irredundance compelling coloring of graphs",
            "updated": "2023-11-29T15:30:44Z",
            "published": "2023-11-10T16:26:56Z",
            "summary": "Irredundance coloring of $G$ is a proper coloring in which there exists a\nmaximal irredundant set $R$ such that all the vertices of $R$ have different\ncolors. The minimum number of colors required for an irredundance coloring of\n$G$ is called the irredundance chromatic number of $G$, and is denoted by\n$\\chi_{i}(G)$. Irredundance compelling coloring of $G$ is a proper coloring of\n$G$ in which every rainbow committee (the set containing a vertex of each\ncolor) is an irredundant set of $G$. The maximum number of colors required for\nan irredundance compelling coloring of $G$ is called the irredundance\ncompelling chromatic number of $G$, and is denoted by $\\chi_{irc}(G)$. In this\npaper, we make a detailed study on $\\chi_{i}(G)$, $\\chi_{irc}(G)$ and its\nrelation to other coloring and domination parameters",
            "author": [
                "David Ashok Kalarkop",
                "Pawaton Kaemawichanurat"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06161v2",
                "http://arxiv.org/pdf/2311.06161v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06153v1",
            "title": "Interpretable Graph Anomaly Detection using Gradient Attention Maps",
            "updated": "2023-11-10T16:14:21Z",
            "published": "2023-11-10T16:14:21Z",
            "summary": "Detecting unusual patterns in graph data is a crucial task in data mining.\nHowever, existing methods often face challenges in consistently achieving\nsatisfactory performance and lack interpretability, which hinders our\nunderstanding of anomaly detection decisions. In this paper, we propose a novel\napproach to graph anomaly detection that leverages the power of\ninterpretability to enhance performance. Specifically, our method extracts an\nattention map derived from gradients of graph neural networks, which serves as\na basis for scoring anomalies. In addition, we conduct theoretical analysis\nusing synthetic data to validate our method and gain insights into its\ndecision-making process. To demonstrate the effectiveness of our method, we\nextensively evaluate our approach against state-of-the-art graph anomaly\ndetection techniques. The results consistently demonstrate the superior\nperformance of our method compared to the baselines.",
            "author": [
                "Yifei Yang",
                "Peng Wang",
                "Xiaofan He",
                "Dongmian Zou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06153v1",
                "http://arxiv.org/pdf/2311.06153v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06152v1",
            "title": "Going beyond persistent homology using persistent homology",
            "updated": "2023-11-10T16:12:35Z",
            "published": "2023-11-10T16:12:35Z",
            "summary": "Representational limits of message-passing graph neural networks (MP-GNNs),\ne.g., in terms of the Weisfeiler-Leman (WL) test for isomorphism, are well\nunderstood. Augmenting these graph models with topological features via\npersistent homology (PH) has gained prominence, but identifying the class of\nattributed graphs that PH can recognize remains open. We introduce a novel\nconcept of color-separating sets to provide a complete resolution to this\nimportant problem. Specifically, we establish the necessary and sufficient\nconditions for distinguishing graphs based on the persistence of their\nconnected components, obtained from filter functions on vertex and edge colors.\nOur constructions expose the limits of vertex- and edge-level PH, proving that\nneither category subsumes the other. Leveraging these theoretical insights, we\npropose RePHINE for learning topological features on graphs. RePHINE\nefficiently combines vertex- and edge-level PH, achieving a scheme that is\nprovably more powerful than both. Integrating RePHINE into MP-GNNs boosts their\nexpressive power, resulting in gains over standard PH on several benchmarks for\ngraph classification.",
            "author": [
                "Johanna Immonen",
                "Amauri H. Souza",
                "Vikas Garg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06152v1",
                "http://arxiv.org/pdf/2311.06152v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06328v1",
            "title": "Poincare group spin networks",
            "updated": "2023-11-10T16:10:51Z",
            "published": "2023-11-10T16:10:51Z",
            "summary": "Spin network technique is usually generalized to relativistic case by\nchanging $SO(4)$ group -- Euclidean counterpart of the Lorentz group -- to its\nuniversal spin covering $SU(2)\\times SU(2)$, or by using the representations of\n$SO(1,3)$ Lorentz group. We extend this approach by using inhomogeneous Lorentz\ngroup $\\mathcal{P}=SO(1,3)\\rtimes T_4$, which results in the simplification of\nthe spin network technique. The labels on the network graph corresponding to\nthe subgroup of translations $T_4$ make the intertwinners into the products of\n$SU(2)$ parts and the energy-momentum conservation delta functions. This maps\nrelativistic spin networks to usual Feynman diagrams for the matter fields.",
            "author": [
                "M. V. Altaisky"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06328v1",
                "http://arxiv.org/pdf/2311.06328v1"
            ],
            "primary_category": "gr-qc",
            "category": [
                "gr-qc"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06141v1",
            "title": "Federated Learning Across Decentralized and Unshared Archives for Remote\n  Sensing Image Classification",
            "updated": "2023-11-10T15:58:53Z",
            "published": "2023-11-10T15:58:53Z",
            "summary": "Federated learning (FL) enables the collaboration of multiple deep learning\nmodels to learn from decentralized data archives (i.e., clients) without\naccessing data on clients. Although FL offers ample opportunities in knowledge\ndiscovery from distributed image archives, it is seldom considered in remote\nsensing (RS). In this paper, as a first time in RS, we present a comparative\nstudy of state-of-the-art FL algorithms. To this end, we initially provide a\nsystematic review of the FL algorithms presented in the computer vision\ncommunity for image classification problems, and select several\nstate-of-the-art FL algorithms based on their effectiveness with respect to\ntraining data heterogeneity across clients (known as non-IID data). After\npresenting an extensive overview of the selected algorithms, a theoretical\ncomparison of the algorithms is conducted based on their: 1) local training\ncomplexity; 2) aggregation complexity; 3) learning efficiency; 4) communication\ncost; and 5) scalability in terms of number of clients. As the classification\ntask, we consider multi-label classification (MLC) problem since RS images\ntypically consist of multiple classes, and thus can simultaneously be\nassociated with multi-labels. After the theoretical comparison, experimental\nanalyses are presented to compare them under different decentralization\nscenarios in terms of MLC performance. Based on our comprehensive analyses, we\nfinally derive a guideline for selecting suitable FL algorithms in RS. The code\nof this work will be publicly available at https://git.tu-berlin.de/rsim/FL-RS.",
            "author": [
                "Bar\u0131\u015f B\u00fcy\u00fckta\u015f",
                "Gencer Sumbul",
                "Beg\u00fcm Demir"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06141v1",
                "http://arxiv.org/pdf/2311.06141v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06117v1",
            "title": "Distributionally Robust Skeleton Learning of Discrete Bayesian Networks",
            "updated": "2023-11-10T15:33:19Z",
            "published": "2023-11-10T15:33:19Z",
            "summary": "We consider the problem of learning the exact skeleton of general discrete\nBayesian networks from potentially corrupted data. Building on distributionally\nrobust optimization and a regression approach, we propose to optimize the most\nadverse risk over a family of distributions within bounded Wasserstein distance\nor KL divergence to the empirical distribution. The worst-case risk accounts\nfor the effect of outliers. The proposed approach applies for general\ncategorical random variables without assuming faithfulness, an ordinal\nrelationship or a specific form of conditional distribution. We present\nefficient algorithms and show the proposed methods are closely related to the\nstandard regularized regression approach. Under mild assumptions, we derive\nnon-asymptotic guarantees for successful structure learning with logarithmic\nsample complexities for bounded-degree graphs. Numerical study on synthetic and\nreal datasets validates the effectiveness of our method. Code is available at\nhttps://github.com/DanielLeee/drslbn.",
            "author": [
                "Yeshu Li",
                "Brian D. Ziebart"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06117v1",
                "http://arxiv.org/pdf/2311.06117v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06116v1",
            "title": "Quantum Bisimilarity via Barbs and Contexts: Curbing the Power of\n  Non-Deterministic Observers",
            "updated": "2023-11-10T15:31:22Z",
            "published": "2023-11-10T15:31:22Z",
            "summary": "Past years have seen the development of a few proposals for quantum\nextensions of process calculi. The rationale is clear: with the development of\nquantum communication protocols, there is a need to abstract and focus on the\nbasic features of quantum concurrent systems, like CCS has done for its\nclassical counterpart. So far, though, no accepted standard has emerged,\nneither for the syntax nor for the behavioural semantics. Indeed, the various\nproposals do not agree on what should be the observational properties of\nquantum values, and as a matter of fact, the soundness of such properties has\nnever been validated against the prescriptions of quantum theory.\n  To this aim, we introduce a new calculus, Linear Quantum CCS, and investigate\nthe features of behavioural equivalences based on barbs and contexts. Our\ncalculus can be thought of as an asynchronous, linear version of qCCS (based on\nvalue-passing CCS). Linearity ensures that each qubit is sent exactly once,\nprecisely specifying which qubits of a process interact with the context.\n  We exploit contexts to examine how bisimilarities relate to quantum theory.\nWe show that the observational power of general contexts is incompatible with\nquantum theory: roughly, they can perform non-deterministic moves depending on\nquantum values without measuring (hence perturbing) them.\n  Therefore, we refine the operational semantics in order to prevent contexts\nfrom performing unfeasible non-deterministic choices. This induces a coarser\nbisimilarity that better fits the quantum setting: (i) it lifts the\nindistinguishability of quantum states to the distributions of processes and,\ndespite the additional constraints, (ii) it preserves the expressivity of\nnon-deterministic choices based on classical information. To the best of our\nknowledge, our semantics is the first one that satisfies the two properties\nabove.",
            "author": [
                "Lorenzo Ceragioli",
                "Fabio Gadducci",
                "Giuseppe Lomurno",
                "Gabriele Tedeschi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06116v1",
                "http://arxiv.org/pdf/2311.06116v1"
            ],
            "primary_category": "cs.LO",
            "category": [
                "cs.LO",
                "68Q85 (Primary) 81P68 (Secondary)",
                "D.3.1; F.3.2"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06085v1",
            "title": "The Impact of Symmetry Handling for the Stable Set Problem via\n  Schreier-Sims Cuts",
            "updated": "2023-11-10T14:33:27Z",
            "published": "2023-11-10T14:33:27Z",
            "summary": "Symmetry handling inequalities (SHIs) are an appealing and popular tool for\nhandling symmetries in integer programming. Despite their practical\napplication, little is known about their interaction with optimization\nproblems. This article focuses on Schreier-Sims (SST) cuts, a recently\nintroduced family of SHIs, and investigate their impact on the computational\nand polyhedral complexity of optimization problems. Given that SST cuts are not\nunique, a crucial question is to understand how different constructions of SST\ncuts influence the solving process.\n  First, we observe that SST cuts do not increase the computational complexity\nof solving a linear optimization problem over any polytope $P$. However,\nseparating the integer hull of $P$ enriched by SST cuts can be NP-hard, even if\n$P$ is integral and has a compact formulation. We study this phenomenon more\nin-depth for the stable set problem, particularly for subclasses of perfect\ngraphs. For bipartite graphs, we give a complete characterization of the\ninteger hull after adding SST cuts based on odd-cycle inequalities. For\ntrivially perfect graphs, we observe that the separation problem is still\nNP-hard after adding a generic set of SST cuts. Our main contribution is to\nidentify a specific class of SST cuts, called stringent SST cuts, that keeps\nthe separation problem polynomial and a complete set of inequalities, namely\nSST clique cuts, that yield a complete linear description.\n  We complement these results by giving SST cuts based presolving techniques\nand provide a computational study to compare the different approaches. In\nparticular, our newly identified stringent SST cuts dominate other approaches.",
            "author": [
                "Christopher Hojny",
                "Marc E. Pfetsch",
                "Jos\u00e9 Verschae"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06085v1",
                "http://arxiv.org/pdf/2311.06085v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06074v1",
            "title": "Two-compartment neuronal spiking model expressing brain-state specific\n  apical-amplification, -isolation and -drive regimes",
            "updated": "2023-11-10T14:16:46Z",
            "published": "2023-11-10T14:16:46Z",
            "summary": "There is mounting experimental evidence that brain-state specific neural\nmechanisms supported by connectomic architectures serve to combine past and\ncontextual knowledge with current, incoming flow of evidence (e.g. from sensory\nsystems). Such mechanisms are distributed across multiple spatial and temporal\nscales and require dedicated support at the levels of individual neurons and\nsynapses. A prominent feature in the neocortex is the structure of large, deep\npyramidal neurons which show a peculiar separation between an apical dendritic\ncompartment and a basal dentritic/peri-somatic compartment, with distinctive\npatterns of incoming connections and brain-state specific activation\nmechanisms, namely apical-amplification, -isolation and -drive associated to\nthe wakefulness, deeper NREM sleep stages and REM sleep. The cognitive roles of\napical mechanisms have been demonstrated in behaving animals. In contrast,\nclassical models of learning spiking networks are based on single compartment\nneurons that miss the description of mechanisms to combine apical and\nbasal/somatic information. This work aims to provide the computational\ncommunity with a two-compartment spiking neuron model which includes features\nthat are essential for supporting brain-state specific learning and with a\npiece-wise linear transfer function (ThetaPlanes) at highest abstraction level\nto be used in large scale bio-inspired artificial intelligence systems. A\nmachine learning algorithm, constrained by a set of fitness functions, selected\nthe parameters defining neurons expressing the desired apical mechanisms.",
            "author": [
                "Elena Pastorelli",
                "Alper Yegenoglu",
                "Nicole Kolodziej",
                "Willem Wybo",
                "Francesco Simula",
                "Sandra Diaz",
                "Johan Frederik Storm",
                "Pier Stanislao Paolucci"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06074v1",
                "http://arxiv.org/pdf/2311.06074v1"
            ],
            "primary_category": "q-bio.NC",
            "category": [
                "q-bio.NC",
                "cs.NE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06056v1",
            "title": "Learning Contrastive Self-Distillation for Ultra-Fine-Grained Visual\n  Categorization Targeting Limited Samples",
            "updated": "2023-11-10T13:39:47Z",
            "published": "2023-11-10T13:39:47Z",
            "summary": "In the field of intelligent multimedia analysis, ultra-fine-grained visual\ncategorization (Ultra-FGVC) plays a vital role in distinguishing intricate\nsubcategories within broader categories. However, this task is inherently\nchallenging due to the complex granularity of category subdivisions and the\nlimited availability of data for each category. To address these challenges,\nthis work proposes CSDNet, a pioneering framework that effectively explores\ncontrastive learning and self-distillation to learn discriminative\nrepresentations specifically designed for Ultra-FGVC tasks. CSDNet comprises\nthree main modules: Subcategory-Specific Discrepancy Parsing (SSDP), Dynamic\nDiscrepancy Learning (DDL), and Subcategory-Specific Discrepancy Transfer\n(SSDT), which collectively enhance the generalization of deep models across\ninstance, feature, and logit prediction levels. To increase the diversity of\ntraining samples, the SSDP module introduces augmented samples from different\nviewpoints to spotlight subcategory-specific discrepancies. Simultaneously, the\nproposed DDL module stores historical intermediate features by a dynamic memory\nqueue, which optimizes the feature learning space through iterative contrastive\nlearning. Furthermore, the SSDT module is developed by a novel\nself-distillation paradigm at the logit prediction level of raw and augmented\nsamples, which effectively distills more subcategory-specific discrepancies\nknowledge from the inherent structure of limited training data without\nrequiring additional annotations. Experimental results demonstrate that CSDNet\noutperforms current state-of-the-art Ultra-FGVC methods, emphasizing its\npowerful efficacy and adaptability in addressing Ultra-FGVC tasks.",
            "author": [
                "Ziye Fang",
                "Xin Jiang",
                "Hao Tang",
                "Zechao Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06056v1",
                "http://arxiv.org/pdf/2311.06056v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06049v1",
            "title": "Privacy-Preserving Individual-Level COVID-19 Infection Prediction via\n  Federated Graph Learning",
            "updated": "2023-11-10T13:22:14Z",
            "published": "2023-11-10T13:22:14Z",
            "summary": "Accurately predicting individual-level infection state is of great value\nsince its essential role in reducing the damage of the epidemic. However, there\nexists an inescapable risk of privacy leakage in the fine-grained user mobility\ntrajectories required by individual-level infection prediction. In this paper,\nwe focus on developing a framework of privacy-preserving individual-level\ninfection prediction based on federated learning (FL) and graph neural networks\n(GNN). We propose Falcon, a Federated grAph Learning method for\nprivacy-preserving individual-level infeCtion predictiON. It utilizes a novel\nhypergraph structure with spatio-temporal hyperedges to describe the complex\ninteractions between individuals and locations in the contagion process. By\norganically combining the FL framework with hypergraph neural networks, the\ninformation propagation process of the graph machine learning is able to be\ndivided into two stages distributed on the server and the clients,\nrespectively, so as to effectively protect user privacy while transmitting\nhigh-level information. Furthermore, it elaborately designs a differential\nprivacy perturbation mechanism as well as a plausible pseudo location\ngeneration approach to preserve user privacy in the graph structure. Besides,\nit introduces a cooperative coupling mechanism between the individual-level\nprediction model and an additional region-level model to mitigate the\ndetrimental impacts caused by the injected obfuscation mechanisms. Extensive\nexperimental results show that our methodology outperforms state-of-the-art\nalgorithms and is able to protect user privacy against actual privacy attacks.\nOur code and datasets are available at the link:\nhttps://github.com/wjfu99/FL-epidemic.",
            "author": [
                "Wenjie Fu",
                "Huandong Wang",
                "Chen Gao",
                "Guanghua Liu",
                "Yong Li",
                "Tao Jiang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06049v1",
                "http://arxiv.org/pdf/2311.06049v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.10756v1",
            "title": "Earnings Prediction Using Recurrent Neural Networks",
            "updated": "2023-11-10T13:04:34Z",
            "published": "2023-11-10T13:04:34Z",
            "summary": "Firm disclosures about future prospects are crucial for corporate valuation\nand compliance with global regulations, such as the EU's MAR and the US's SEC\nRule 10b-5 and RegFD. To comply with disclosure obligations, issuers must\nidentify nonpublic information with potential material impact on security\nprices as only new, relevant and unexpected information materially affects\nprices in efficient markets. Financial analysts, assumed to represent public\nknowledge on firms' earnings prospects, face limitations in offering\ncomprehensive coverage and unbiased estimates. This study develops a neural\nnetwork to forecast future firm earnings, using four decades of financial data,\naddressing analysts' coverage gaps and potentially revealing hidden insights.\nThe model avoids selectivity and survivorship biases as it allows for missing\ndata. Furthermore, the model is able to produce both fiscal-year-end and\nquarterly earnings predictions. Its performance surpasses benchmark models from\nthe academic literature by a wide margin and outperforms analysts' forecasts\nfor fiscal-year-end earnings predictions.",
            "author": [
                "Moritz Scherrmann",
                "Ralf Elsas"
            ],
            "link": [
                "http://arxiv.org/abs/2311.10756v1",
                "http://arxiv.org/pdf/2311.10756v1"
            ],
            "primary_category": "q-fin.ST",
            "category": [
                "q-fin.ST",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06025v2",
            "title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training\n  Regime and Better Alignment to Human Preferences",
            "updated": "2023-11-23T10:19:47Z",
            "published": "2023-11-10T12:25:32Z",
            "summary": "Recently, the increasing demand for superior medical services has highlighted\nthe discrepancies in the medical infrastructure. With big data, especially\ntexts, forming the foundation of medical services, there is an exigent need for\neffective natural language processing (NLP) solutions tailored to the\nhealthcare domain. Conventional approaches leveraging pre-trained models\npresent promising results in this domain and current large language models\n(LLMs) offer advanced foundation for medical text processing. However, most\nmedical LLMs are trained only with supervised fine-tuning (SFT), even though it\nefficiently empowers LLMs to understand and respond to medical instructions but\nis ineffective in learning domain knowledge and aligning with human preference.\nAnother engineering barrier that prevents current medical LLM from better text\nprocessing ability is their restricted context length (e.g., 2,048 tokens),\nmaking it hard for the LLMs to process long context, which is frequently\nrequired in the medical domain. In this work, we propose ChiMed-GPT, a new\nbenchmark LLM designed explicitly for Chinese medical domain, with enlarged\ncontext length to 4,096 tokens and undergoes a comprehensive training regime\nwith pre-training, SFT, and RLHF. Evaluations on real-world tasks including\ninformation extraction, question answering, and dialogue generation demonstrate\nChiMed-GPT's superior performance over general domain LLMs. Furthermore, we\nanalyze possible biases through prompting ChiMed-GPT to perform attitude scales\nregarding discrimination of patients, so as to contribute to further\nresponsible development of LLMs in the medical domain. The code and model are\nreleased at https://github.com/synlp/ChiMed-GPT.",
            "author": [
                "Yuanhe Tian",
                "Ruyi Gan",
                "Yan Song",
                "Jiaxing Zhang",
                "Yongdong Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06025v2",
                "http://arxiv.org/pdf/2311.06025v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06323v1",
            "title": "Reviewing Developments of Graph Convolutional Network Techniques for\n  Recommendation Systems",
            "updated": "2023-11-10T12:11:36Z",
            "published": "2023-11-10T12:11:36Z",
            "summary": "The Recommender system is a vital information service on today's Internet.\nRecently, graph neural networks have emerged as the leading approach for\nrecommender systems. We try to review recent literature on graph neural\nnetwork-based recommender systems, covering the background and development of\nboth recommender systems and graph neural networks. Then categorizing\nrecommender systems by their settings and graph neural networks by spectral and\nspatial models, we explore the motivation behind incorporating graph neural\nnetworks into recommender systems. We also analyze challenges and open problems\nin graph construction, embedding propagation and aggregation, and computation\nefficiency. This guides us to better explore the future directions and\ndevelopments in this domain.",
            "author": [
                "Haojun Zhu",
                "Vikram Kapoor",
                "Priya Sharma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06323v1",
                "http://arxiv.org/pdf/2311.06323v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06019v1",
            "title": "Multi-dimensional vibration sensing and simultaneous self-homodyne\n  optical transmission of single wavelength net 5.36 Tb/s signal using telecom\n  7-core fiber",
            "updated": "2023-11-10T12:05:45Z",
            "published": "2023-11-10T12:05:45Z",
            "summary": "We present a high-capacity self-homodyne optical transmission system that\nenables simultaneously multidimensional vibration sensing based on a\nweakly-coupled 7-core fiber. To our knowledge, we demonstrate for the\nfirst-time detection of fiber vibration direction along with strength,\nfrequency, and location of the vibration source, while transmitting in the\nmeantime single-carrier 16 QAM signal reaching a net date rate of 5.36 Tb/s\nover 41.4 km of telecom 7-core fiber.",
            "author": [
                "Jianwei Tang",
                "Xueyang Li",
                "Bang Yang",
                "Chen Cheng",
                "Yaguang Hao",
                "Yifan Xu",
                "Jiali Li",
                "Zhixue He",
                "Yanfu Yang",
                "Weisheng Hu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06019v1",
                "http://arxiv.org/pdf/2311.06019v1"
            ],
            "primary_category": "physics.optics",
            "category": [
                "physics.optics",
                "physics.app-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06017v1",
            "title": "Extended formulations for a class of polyhedra with bimodular cographic\n  constraint matrices",
            "updated": "2023-11-10T12:02:52Z",
            "published": "2023-11-10T12:02:52Z",
            "summary": "We are motivated by integer linear programs (ILPs) defined by constraint\nmatrices with bounded determinants. Such matrices generalize the notion of\ntotally-unimodular matrices. When the determinants are bounded by $2$, the\nmatrix is called bimodular. Artmann et al. give a polynomial-time algorithm for\nsolving any ILP defined by a bimodular constraint matrix. Complementing this\nresult, Conforti et al. give a compact extended formulation for a particular\nclass of bimodular-constrained ILPs, namely those that model the stable set\npolytope of a graph with odd cycle packing number $1$. We demonstrate that\ntheir compact extended formulation can be modified to hold for polyhedra such\nthat (1) the constraint matrix is bimodular, (2) the row-matroid generated by\nthe constraint matrix is cographic and (3) the right-hand side is a linear\ncombination of the columns of the constraint matrix. This generalizes the\nimportant special case from Conforti et al. concerning 4-connected graphs with\nodd cycle transversal number at least four. Moreover, our results yield compact\nextended formulations for a new class of polyhedra.",
            "author": [
                "Joseph Paat",
                "Zach Walsh",
                "Luze Xu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06017v1",
                "http://arxiv.org/pdf/2311.06017v1"
            ],
            "primary_category": "math.OC",
            "category": [
                "math.OC",
                "90C10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06015v1",
            "title": "RSG: Fast Learning Adaptive Skills for Quadruped Robots by Skill Graph",
            "updated": "2023-11-10T11:59:41Z",
            "published": "2023-11-10T11:59:41Z",
            "summary": "Developing robotic intelligent systems that can adapt quickly to unseen wild\nsituations is one of the critical challenges in pursuing autonomous robotics.\nAlthough some impressive progress has been made in walking stability and skill\nlearning in the field of legged robots, their ability to fast adaptation is\nstill inferior to that of animals in nature. Animals are born with massive\nskills needed to survive, and can quickly acquire new ones, by composing\nfundamental skills with limited experience. Inspired by this, we propose a\nnovel framework, named Robot Skill Graph (RSG) for organizing massive\nfundamental skills of robots and dexterously reusing them for fast adaptation.\nBearing a structure similar to the Knowledge Graph (KG), RSG is composed of\nmassive dynamic behavioral skills instead of static knowledge in KG and enables\ndiscovering implicit relations that exist in be-tween of learning context and\nacquired skills of robots, serving as a starting point for understanding subtle\npatterns existing in robots' skill learning. Extensive experimental results\ndemonstrate that RSG can provide rational skill inference upon new tasks and\nenvironments and enable quadruped robots to adapt to new scenarios and learn\nnew skills rapidly.",
            "author": [
                "Hongyin Zhang",
                "Diyuan Shi",
                "Zifeng Zhuang",
                "Han Zhao",
                "Zhenyu Wei",
                "Feng Zhao",
                "Sibo Gai",
                "Shangke Lyu",
                "Donglin Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06015v1",
                "http://arxiv.org/pdf/2311.06015v1"
            ],
            "primary_category": "cs.RO",
            "category": [
                "cs.RO",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06009v1",
            "title": "Polar-Net: A Clinical-Friendly Model for Alzheimer's Disease Detection\n  in OCTA Images",
            "updated": "2023-11-10T11:49:49Z",
            "published": "2023-11-10T11:49:49Z",
            "summary": "Optical Coherence Tomography Angiography (OCTA) is a promising tool for\ndetecting Alzheimer's disease (AD) by imaging the retinal microvasculature.\nOphthalmologists commonly use region-based analysis, such as the ETDRS grid, to\nstudy OCTA image biomarkers and understand the correlation with AD. However,\nexisting studies have used general deep computer vision methods, which present\nchallenges in providing interpretable results and leveraging clinical prior\nknowledge. To address these challenges, we propose a novel deep-learning\nframework called Polar-Net. Our approach involves mapping OCTA images from\nCartesian coordinates to polar coordinates, which allows for the use of\napproximate sector convolution and enables the implementation of the ETDRS\ngrid-based regional analysis method commonly used in clinical practice.\nFurthermore, Polar-Net incorporates clinical prior information of each sector\nregion into the training process, which further enhances its performance.\nAdditionally, our framework adapts to acquire the importance of the\ncorresponding retinal region, which helps researchers and clinicians understand\nthe model's decision-making process in detecting AD and assess its conformity\nto clinical observations. Through evaluations on private and public datasets,\nwe have demonstrated that Polar-Net outperforms existing state-of-the-art\nmethods and provides more valuable pathological evidence for the association\nbetween retinal vascular changes and AD. In addition, we also show that the two\ninnovative modules introduced in our framework have a significant impact on\nimproving overall performance.",
            "author": [
                "Shouyue Liu",
                "Jinkui Hao",
                "Yanwu Xu",
                "Huazhu Fu",
                "Xinyu Guo",
                "Jiang Liu",
                "Yalin Zheng",
                "Yonghuai Liu",
                "Jiong Zhang",
                "Yitian Zhao"
            ],
            "link": [
                "http://dx.doi.org/10.1007/978-3-031-43990-2_57",
                "http://arxiv.org/abs/2311.06009v1",
                "http://arxiv.org/pdf/2311.06009v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07596v1",
            "title": "Graph GOSPA metric: a metric to measure the discrepancy between graphs\n  of different sizes",
            "updated": "2023-11-10T11:40:24Z",
            "published": "2023-11-10T11:40:24Z",
            "summary": "This paper proposes a metric to measure the dissimilarity between graphs that\nmay have a different number of nodes. The proposed metric extends the\ngeneralised optimal subpattern assignment (GOSPA) metric, which is a metric for\nsets, to graphs. The proposed graph GOSPA metric includes costs associated with\nnode attribute errors for properly assigned nodes, missed and false nodes and\nedge mismatches between graphs. The computation of this metric is based on\nfinding the optimal assignments between nodes in the two graphs, with the\npossibility of leaving some of the nodes unassigned. We also propose a lower\nbound for the metric, which is also a metric for graphs and is computable in\npolynomial time using linear programming. The metric is first derived for\nundirected unweighted graphs and it is then extended to directed and weighted\ngraphs. The properties of the metric are demonstrated via simulated and\nempirical datasets.",
            "author": [
                "Jinhao Gu",
                "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez",
                "Robert E. Firth",
                "Lennart Svensson"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07596v1",
                "http://arxiv.org/pdf/2311.07596v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "cs.LG",
                "eess.SP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05997v3",
            "title": "JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal\n  Language Models",
            "updated": "2023-11-30T07:39:48Z",
            "published": "2023-11-10T11:17:58Z",
            "summary": "Achieving human-like planning and control with multimodal observations in an\nopen world is a key milestone for more functional generalist agents. Existing\napproaches can handle certain long-horizon tasks in an open world. However,\nthey still struggle when the number of open-world tasks could potentially be\ninfinite and lack the capability to progressively enhance task completion as\ngame time progresses. We introduce JARVIS-1, an open-world agent that can\nperceive multimodal input (visual observations and human instructions),\ngenerate sophisticated plans, and perform embodied control, all within the\npopular yet challenging open-world Minecraft universe. Specifically, we develop\nJARVIS-1 on top of pre-trained multimodal language models, which map visual\nobservations and textual instructions to plans. The plans will be ultimately\ndispatched to the goal-conditioned controllers. We outfit JARVIS-1 with a\nmultimodal memory, which facilitates planning using both pre-trained knowledge\nand its actual game survival experiences. JARVIS-1 is the existing most general\nagent in Minecraft, capable of completing over 200 different tasks using\ncontrol and observation space similar to humans. These tasks range from\nshort-horizon tasks, e.g., \"chopping trees\" to long-horizon tasks, e.g.,\n\"obtaining a diamond pickaxe\". JARVIS-1 performs exceptionally well in\nshort-horizon tasks, achieving nearly perfect performance. In the classic\nlong-term task of $\\texttt{ObtainDiamondPickaxe}$, JARVIS-1 surpasses the\nreliability of current state-of-the-art agents by 5 times and can successfully\ncomplete longer-horizon and more challenging tasks. The project page is\navailable at https://craftjarvis.org/JARVIS-1",
            "author": [
                "Zihao Wang",
                "Shaofei Cai",
                "Anji Liu",
                "Yonggang Jin",
                "Jinbing Hou",
                "Bowei Zhang",
                "Haowei Lin",
                "Zhaofeng He",
                "Zilong Zheng",
                "Yaodong Yang",
                "Xiaojian Ma",
                "Yitao Liang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05997v3",
                "http://arxiv.org/pdf/2311.05997v3"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05996v1",
            "title": "Generalised Indiscernibles, Dividing Lines, and Products of Structures",
            "updated": "2023-11-10T11:15:45Z",
            "published": "2023-11-10T11:15:45Z",
            "summary": "Generalised indiscernibles highlight a strong link between model theory and\nstructural Ramsey theory. In this paper, we use generalised indiscernibles as\ntools to prove results in both these areas. More precisely, we first show that\na reduct of an ultrahomogenous $\\aleph_0$-categorical structure which has\nhigher arity than the original structure cannot be Ramsey. In particular, the\nonly nontrivial Ramsey reduct of the generically ordered random $k$-hypergraph\nis the linear order. We then turn our attention to model-theoretic dividing\nlines that are characterised by collapsing generalised indiscernibles, and\nprove, for these dividing lines, several transfer principles in (full and\nlexicographic) products of structures. As an application, we construct new\nalgorithmically tame classes of graphs.",
            "author": [
                "Nadav Meir",
                "Aris Papadopoulos",
                "Pierre Touchard"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05996v1",
                "http://arxiv.org/pdf/2311.05996v1"
            ],
            "primary_category": "math.LO",
            "category": [
                "math.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05986v1",
            "title": "Signature-Based Community Detection for Time Series",
            "updated": "2023-11-10T10:56:20Z",
            "published": "2023-11-10T10:56:20Z",
            "summary": "Community detection for time series without prior knowledge poses an open\nchallenge within complex networks theory. Traditional approaches begin by\nassessing time series correlations and maximizing modularity under diverse null\nmodels. These methods suffer from assuming temporal stationarity and are\ninfluenced by the granularity of observation intervals. In this study, we\npropose an approach based on the signature matrix, a concept from path theory\nfor studying stochastic processes. By employing a signature-derived similarity\nmeasure, our method overcomes drawbacks of traditional correlation-based\ntechniques. Through a series of numerical experiments, we demonstrate that our\nmethod consistently yields higher modularity compared to baseline models, when\ntested on the Standard and Poor's 500 dataset. Moreover, our approach showcases\nenhanced stability in modularity when the length of the underlying time series\nis manipulated. This research contributes to the field of community detection\nby introducing a signature-based similarity measure, offering an alternative to\nconventional correlation matrices.",
            "author": [
                "Marco Gregnanin",
                "Johannes De Smedt",
                "Giorgio Gnecco",
                "Maurizio Parton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05986v1",
                "http://arxiv.org/pdf/2311.05986v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI",
                "physics.data-an"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07595v1",
            "title": "A Decision Support System for Liver Diseases Prediction: Integrating\n  Batch Processing, Rule-Based Event Detection and SPARQL Query",
            "updated": "2023-11-10T10:21:09Z",
            "published": "2023-11-10T10:21:09Z",
            "summary": "Liver diseases pose a significant global health burden, impacting a\nsubstantial number of individuals and exerting substantial economic and social\nconsequences. Rising liver problems are considered a fatal disease in many\ncountries, such as Egypt, Molda, etc. The objective of this study is to\nconstruct a predictive model for liver illness using Basic Formal Ontology\n(BFO) and detection rules derived from a decision tree algorithm. Based on\nthese rules, events are detected through batch processing using the Apache Jena\nframework. Based on the event detected, queries can be directly processed using\nSPARQL. To make the ontology operational, these Decision Tree (DT) rules are\nconverted into Semantic Web Rule Language (SWRL). Using this SWRL in the\nontology for predicting different types of liver disease with the help of the\nPellet and Drool inference engines in Protege Tools, a total of 615 records are\ntaken from different liver diseases. After inferring the rules, the result can\nbe generated for the patient according to the DT rules, and other\npatient-related details along with different precautionary suggestions can be\nobtained based on these results. Combining query results of batch processing\nand ontology-generated results can give more accurate suggestions for disease\nprevention and detection. This work aims to provide a comprehensive approach\nthat is applicable for liver disease prediction, rich knowledge graph\nrepresentation, and smart querying capabilities. The results show that\ncombining RDF data, SWRL rules, and SPARQL queries for analysing and predicting\nliver disease can help medical professionals to learn more about liver diseases\nand make a Decision Support System (DSS) for health care.",
            "author": [
                "Ritesh Chandra",
                "Sadhana Tiwari",
                "Satyam Rastogi",
                "Sonali Agarwal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07595v1",
                "http://arxiv.org/pdf/2311.07595v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05970v1",
            "title": "Quantized Distillation: Optimizing Driver Activity Recognition Models\n  for Resource-Constrained Environments",
            "updated": "2023-11-10T10:07:07Z",
            "published": "2023-11-10T10:07:07Z",
            "summary": "Deep learning-based models are at the forefront of most driver observation\nbenchmarks due to their remarkable accuracies but are also associated with high\ncomputational costs. This is challenging, as resources are often limited in\nreal-world driving scenarios. This paper introduces a lightweight framework for\nresource-efficient driver activity recognition. The framework enhances 3D\nMobileNet, a neural architecture optimized for speed in video classification,\nby incorporating knowledge distillation and model quantization to balance model\naccuracy and computational efficiency. Knowledge distillation helps maintain\naccuracy while reducing the model size by leveraging soft labels from a larger\nteacher model (I3D), instead of relying solely on original ground truth data.\nModel quantization significantly lowers memory and computation demands by using\nlower precision integers for model weights and activations. Extensive testing\non a public dataset for in-vehicle monitoring during autonomous driving\ndemonstrates that this new framework achieves a threefold reduction in model\nsize and a 1.4-fold improvement in inference time, compared to an already\noptimized architecture. The code for this study is available at\nhttps://github.com/calvintanama/qd-driver-activity-reco.",
            "author": [
                "Calvin Tanama",
                "Kunyu Peng",
                "Zdravko Marinov",
                "Rainer Stiefelhagen",
                "Alina Roitberg"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05970v1",
                "http://arxiv.org/pdf/2311.05970v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05965v1",
            "title": "Large Language Models are Zero Shot Hypothesis Proposers",
            "updated": "2023-11-10T10:03:49Z",
            "published": "2023-11-10T10:03:49Z",
            "summary": "Significant scientific discoveries have driven the progress of human\ncivilisation. The explosion of scientific literature and data has created\ninformation barriers across disciplines that have slowed the pace of scientific\ndiscovery. Large Language Models (LLMs) hold a wealth of global and\ninterdisciplinary knowledge that promises to break down these information\nbarriers and foster a new wave of scientific discovery. However, the potential\nof LLMs for scientific discovery has not been formally explored. In this paper,\nwe start from investigating whether LLMs can propose scientific hypotheses. To\nthis end, we construct a dataset consist of background knowledge and hypothesis\npairs from biomedical literature. The dataset is divided into training, seen,\nand unseen test sets based on the publication date to control visibility. We\nsubsequently evaluate the hypothesis generation capabilities of various\ntop-tier instructed models in zero-shot, few-shot, and fine-tuning settings,\nincluding both closed and open-source LLMs. Additionally, we introduce an\nLLM-based multi-agent cooperative framework with different role designs and\nexternal tools to enhance the capabilities related to generating hypotheses. We\nalso design four metrics through a comprehensive review to evaluate the\ngenerated hypotheses for both ChatGPT-based and human evaluations. Through\nexperiments and analyses, we arrive at the following findings: 1) LLMs\nsurprisingly generate untrained yet validated hypotheses from testing\nliterature. 2) Increasing uncertainty facilitates candidate generation,\npotentially enhancing zero-shot hypothesis generation capabilities. These\nfindings strongly support the potential of LLMs as catalysts for new scientific\ndiscoveries and guide further exploration.",
            "author": [
                "Biqing Qi",
                "Kaiyan Zhang",
                "Haoxiang Li",
                "Kai Tian",
                "Sihang Zeng",
                "Zhang-Ren Chen",
                "Bowen Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05965v1",
                "http://arxiv.org/pdf/2311.05965v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05964v1",
            "title": "Multiscale Neural Operators for Solving Time-Independent PDEs",
            "updated": "2023-11-10T10:02:56Z",
            "published": "2023-11-10T10:02:56Z",
            "summary": "Time-independent Partial Differential Equations (PDEs) on large meshes pose\nsignificant challenges for data-driven neural PDE solvers. We introduce a novel\ngraph rewiring technique to tackle some of these challenges, such as\naggregating information across scales and on irregular meshes. Our proposed\napproach bridges distant nodes, enhancing the global interaction capabilities\nof GNNs. Our experiments on three datasets reveal that GNN-based methods set\nnew performance standards for time-independent PDEs on irregular meshes.\nFinally, we show that our graph rewiring strategy boosts the performance of\nbaseline methods, achieving state-of-the-art results in one of the tasks.",
            "author": [
                "Winfried Ripken",
                "Lisa Coiffard",
                "Felix Pieper",
                "Sebastian Dziadzio"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05964v1",
                "http://arxiv.org/pdf/2311.05964v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05956v1",
            "title": "ID Embedding as Subtle Features of Content and Structure for Multimodal\n  Recommendation",
            "updated": "2023-11-10T09:41:28Z",
            "published": "2023-11-10T09:41:28Z",
            "summary": "Multimodal recommendation aims to model user and item representations\ncomprehensively with the involvement of multimedia content for effective\nrecommendations. Existing research has shown that it is beneficial for\nrecommendation performance to combine (user- and item-) ID embeddings with\nmultimodal salient features, indicating the value of IDs. However, there is a\nlack of a thorough analysis of the ID embeddings in terms of feature semantics\nin the literature. In this paper, we revisit the value of ID embeddings for\nmultimodal recommendation and conduct a thorough study regarding its semantics,\nwhich we recognize as subtle features of content and structures. Then, we\npropose a novel recommendation model by incorporating ID embeddings to enhance\nthe semantic features of both content and structures. Specifically, we put\nforward a hierarchical attention mechanism to incorporate ID embeddings in\nmodality fusing, coupled with contrastive learning, to enhance content\nrepresentations. Meanwhile, we propose a lightweight graph convolutional\nnetwork for each modality to amalgamate neighborhood and ID embeddings for\nimproving structural representations. Finally, the content and structure\nrepresentations are combined to form the ultimate item embedding for\nrecommendation. Extensive experiments on three real-world datasets (Baby,\nSports, and Clothing) demonstrate the superiority of our method over\nstate-of-the-art multimodal recommendation methods and the effectiveness of\nfine-grained ID embeddings.",
            "author": [
                "Yuting Liu",
                "Enneng Yang",
                "Yizhou Dang",
                "Guibing Guo",
                "Qiang Liu",
                "Yuliang Liang",
                "Linying Jiang",
                "Xingwei Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05956v1",
                "http://arxiv.org/pdf/2311.05956v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05924v1",
            "title": "Federated Learning with Manifold Regularization and Normalized Update\n  Reaggregation",
            "updated": "2023-11-10T08:14:27Z",
            "published": "2023-11-10T08:14:27Z",
            "summary": "Federated Learning (FL) is an emerging collaborative machine learning\nframework where multiple clients train the global model without sharing their\nown datasets. In FL, the model inconsistency caused by the local data\nheterogeneity across clients results in the near-orthogonality of client\nupdates, which leads to the global update norm reduction and slows down the\nconvergence. Most previous works focus on eliminating the difference of\nparameters (or gradients) between the local and global models, which may fail\nto reflect the model inconsistency due to the complex structure of the machine\nlearning model and the Euclidean space's limitation in meaningful geometric\nrepresentations. In this paper, we propose FedMRUR by adopting the manifold\nmodel fusion scheme and a new global optimizer to alleviate the negative\nimpacts. Concretely, FedMRUR adopts a hyperbolic graph manifold regularizer\nenforcing the representations of the data in the local and global models are\nclose to each other in a low-dimensional subspace. Because the machine learning\nmodel has the graph structure, the distance in hyperbolic space can reflect the\nmodel bias better than the Euclidean distance. In this way, FedMRUR exploits\nthe manifold structures of the representations to significantly reduce the\nmodel inconsistency. FedMRUR also aggregates the client updates norms as the\nglobal update norm, which can appropriately enlarge each client's contribution\nto the global update, thereby mitigating the norm reduction introduced by the\nnear-orthogonality of client updates. Furthermore, we theoretically prove that\nour algorithm can achieve a linear speedup property for non-convex setting\nunder partial client participation.Experiments demonstrate that FedMRUR can\nachieve a new state-of-the-art (SOTA) accuracy with less communication.",
            "author": [
                "Xuming An",
                "Li Shen",
                "Han Hu",
                "Yong Luo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05924v1",
                "http://arxiv.org/pdf/2311.05924v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05922v2",
            "title": "Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation\n  Extraction",
            "updated": "2023-11-15T05:23:04Z",
            "published": "2023-11-10T08:12:00Z",
            "summary": "Few-shot relation extraction involves identifying the type of relationship\nbetween two specific entities within a text, using a limited number of\nannotated samples. A variety of solutions to this problem have emerged by\napplying meta-learning and neural graph techniques which typically necessitate\na training process for adaptation. Recently, the strategy of in-context\nlearning has been demonstrating notable results without the need of training.\nFew studies have already utilized in-context learning for zero-shot information\nextraction. Unfortunately, the evidence for inference is either not considered\nor implicitly modeled during the construction of chain-of-thought prompts. In\nthis paper, we propose a novel approach for few-shot relation extraction using\nlarge language models, named CoT-ER, chain-of-thought with explicit evidence\nreasoning. In particular, CoT-ER first induces large language models to\ngenerate evidences using task-specific and concept-level knowledge. Then these\nevidences are explicitly incorporated into chain-of-thought prompting for\nrelation extraction. Experimental results demonstrate that our CoT-ER approach\n(with 0% training data) achieves competitive performance compared to the\nfully-supervised (with 100% training data) state-of-the-art approach on the\nFewRel1.0 and FewRel2.0 datasets.",
            "author": [
                "Xilai Ma",
                "Jing Li",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05922v2",
                "http://arxiv.org/pdf/2311.05922v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05919v2",
            "title": "Inter-object Discriminative Graph Modeling for Indoor Scene Recognition",
            "updated": "2023-11-13T09:39:49Z",
            "published": "2023-11-10T08:07:16Z",
            "summary": "Variable scene layouts and coexisting objects across scenes make indoor scene\nrecognition still a challenging task. Leveraging object information within\nscenes to enhance the distinguishability of feature representations has emerged\nas a key approach in this domain. Currently, most object-assisted methods use a\nseparate branch to process object information, combining object and scene\nfeatures heuristically. However, few of them pay attention to interpretably\nhandle the hidden discriminative knowledge within object information. In this\npaper, we propose to leverage discriminative object knowledge to enhance scene\nfeature representations. Initially, we capture the object-scene discriminative\nrelationships from a probabilistic perspective, which are transformed into an\nInter-Object Discriminative Prototype (IODP). Given the abundant prior\nknowledge from IODP, we subsequently construct a Discriminative Graph Network\n(DGN), in which pixel-level scene features are defined as nodes and the\ndiscriminative relationships between node features are encoded as edges. DGN\naims to incorporate inter-object discriminative knowledge into the image\nrepresentation through graph convolution. With the proposed IODP and DGN, we\nobtain state-of-the-art results on several widely used scene datasets,\ndemonstrating the effectiveness of the proposed approach.",
            "author": [
                "Chuanxin Song",
                "Hanbo Wu",
                "Xin Ma"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05919v2",
                "http://arxiv.org/pdf/2311.05919v2"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05909v1",
            "title": "How Embeddedness Affects the Evolution of Collaboration: The Role of\n  Knowledge Stock and Social Interactions",
            "updated": "2023-11-10T07:36:06Z",
            "published": "2023-11-10T07:36:06Z",
            "summary": "Science and technology are becoming increasingly collaborative. This paper\naims to explore the factors and mechanisms that impact the dynamic changes of\ncollaborative innovation networks. We consider both collaborative interactions\nof organizations and their knowledge element exchanges to reveal how social and\nknowledge network embeddedness affects the collaboration dynamics. Knowledge\nelements are extracted to present the core concepts of scientific and technical\ninformation, overcoming the limitations of using predefined categorizations\nsuch as IPC when representing the content. Based on multiple collaboration and\nknowledge networks, we then conduct a longitudinal analysis and apply a\nstochastic actor-oriented model (SAOM) to model network dynamics over different\nperiods. The influence of network features and structures, individual node\ncharacteristics, and various dimensions of proximity on collaboration dynamics\nis tested and analyzed.",
            "author": [
                "Hongshu Chen",
                "Qianqian Jin",
                "Xuefeng Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05909v1",
                "http://arxiv.org/pdf/2311.05909v1"
            ],
            "primary_category": "cs.SI",
            "category": [
                "cs.SI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05892v1",
            "title": "Structural Parameterizations of Vertex Integrity",
            "updated": "2023-11-10T06:43:02Z",
            "published": "2023-11-10T06:43:02Z",
            "summary": "The graph parameter vertex integrity measures how vulnerable a graph is to a\nremoval of a small number of vertices. More precisely, a graph with small\nvertex integrity admits a small number of vertex removals to make the remaining\nconnected components small. In this paper, we initiate a systematic study of\nstructural parameterizations of the problem of computing the\nunweighted/weighted vertex integrity. As structural graph parameters, we\nconsider well-known parameters such as clique-width, treewidth, pathwidth,\ntreedepth, modular-width, neighborhood diversity, twin cover number, and\ncluster vertex deletion number. We show several positive and negative results\nand present sharp complexity contrasts.",
            "author": [
                "Tatsuya Gima",
                "Tesshu Hanaka",
                "Yasuaki Kobayashi",
                "Ryota Murai",
                "Hirotaka Ono",
                "Yota Otachi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05892v1",
                "http://arxiv.org/pdf/2311.05892v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05876v2",
            "title": "Trends in Integration of Knowledge and Large Language Models: A Survey\n  and Taxonomy of Methods, Benchmarks, and Applications",
            "updated": "2023-12-07T12:42:07Z",
            "published": "2023-11-10T05:24:04Z",
            "summary": "Large language models (LLMs) exhibit superior performance on various natural\nlanguage tasks, but they are susceptible to issues stemming from outdated data\nand domain-specific limitations. In order to address these challenges,\nresearchers have pursued two primary strategies, knowledge editing and\nretrieval augmentation, to enhance LLMs by incorporating external information\nfrom different aspects. Nevertheless, there is still a notable absence of a\ncomprehensive survey. In this paper, we propose a review to discuss the trends\nin integration of knowledge and large language models, including taxonomy of\nmethods, benchmarks, and applications. In addition, we conduct an in-depth\nanalysis of different methods and point out potential research directions in\nthe future. We hope this survey offers the community quick access and a\ncomprehensive overview of this research area, with the intention of inspiring\nfuture research endeavors.",
            "author": [
                "Zhangyin Feng",
                "Weitao Ma",
                "Weijiang Yu",
                "Lei Huang",
                "Haotian Wang",
                "Qianglong Chen",
                "Weihua Peng",
                "Xiaocheng Feng",
                "Bing Qin",
                "Ting liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05876v2",
                "http://arxiv.org/pdf/2311.05876v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05861v1",
            "title": "Domain Generalization by Learning from Privileged Medical Imaging\n  Information",
            "updated": "2023-11-10T04:09:52Z",
            "published": "2023-11-10T04:09:52Z",
            "summary": "Learning the ability to generalize knowledge between similar contexts is\nparticularly important in medical imaging as data distributions can shift\nsubstantially from one hospital to another, or even from one machine to\nanother. To strengthen generalization, most state-of-the-art techniques inject\nknowledge of the data distribution shifts by enforcing constraints on learned\nfeatures or regularizing parameters. We offer an alternative approach: Learning\nfrom Privileged Medical Imaging Information (LPMII). We show that using some\nprivileged information such as tumor shape or location leads to stronger domain\ngeneralization ability than current state-of-the-art techniques. This paper\ndemonstrates that by using privileged information to predict the severity of\nintra-layer retinal fluid in optical coherence tomography scans, the\nclassification accuracy of a deep learning model operating on\nout-of-distribution data improves from $0.911$ to $0.934$. This paper provides\na strong starting point for using privileged information in other medical\nproblems requiring generalization.",
            "author": [
                "Steven Korevaar",
                "Ruwan Tennakoon",
                "Ricky O'Brien",
                "Dwarikanath Mahapatra",
                "Alireza Bab-Hadiasha"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05861v1",
                "http://arxiv.org/pdf/2311.05861v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.06318v1",
            "title": "Knowledge-Augmented Large Language Models for Personalized Contextual\n  Query Suggestion",
            "updated": "2023-11-10T01:18:47Z",
            "published": "2023-11-10T01:18:47Z",
            "summary": "Large Language Models (LLMs) excel at tackling various natural language\ntasks. However, due to the significant costs involved in re-training or\nfine-tuning them, they remain largely static and difficult to personalize.\nNevertheless, a variety of applications could benefit from generations that are\ntailored to users' preferences, goals, and knowledge. Among them is web search,\nwhere knowing what a user is trying to accomplish, what they care about, and\nwhat they know can lead to improved search experiences. In this work, we\npropose a novel and general approach that augments an LLM with relevant context\nfrom users' interaction histories with a search engine in order to personalize\nits outputs. Specifically, we construct an entity-centric knowledge store for\neach user based on their search and browsing activities on the web, which is\nthen leveraged to provide contextually relevant LLM prompt augmentations. This\nknowledge store is light-weight, since it only produces user-specific aggregate\nprojections of interests and knowledge onto public knowledge graphs, and\nleverages existing search log infrastructure, thereby mitigating the privacy,\ncompliance, and scalability concerns associated with building deep user\nprofiles for personalization. We then validate our approach on the task of\ncontextual query suggestion, which requires understanding not only the user's\ncurrent search context but also what they historically know and care about.\nThrough a number of experiments based on human evaluation, we show that our\napproach is significantly better than several other LLM-powered baselines,\ngenerating query suggestions that are contextually more relevant, personalized,\nand useful.",
            "author": [
                "Jinheon Baek",
                "Nirupama Chandrasekaran",
                "Silviu Cucerzan",
                "Allen herring",
                "Sujay Kumar Jauhar"
            ],
            "link": [
                "http://arxiv.org/abs/2311.06318v1",
                "http://arxiv.org/pdf/2311.06318v1"
            ],
            "primary_category": "cs.IR",
            "category": [
                "cs.IR",
                "cs.AI",
                "cs.CL",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05806v1",
            "title": "Likelihood ratio tests in random graph models with increasing dimensions",
            "updated": "2023-11-10T00:39:15Z",
            "published": "2023-11-10T00:39:15Z",
            "summary": "We explore the Wilks phenomena in two random graph models: the $\\beta$-model\nand the Bradley-Terry model. For two increasing dimensional null hypotheses,\nincluding a specified null $H_0: \\beta_i=\\beta_i^0$ for $i=1,\\ldots, r$ and a\nhomogenous null $H_0: \\beta_1=\\cdots=\\beta_r$, we reveal high dimensional\nWilks' phenomena that the normalized log-likelihood ratio statistic,\n$[2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}\n-r]/(2r)^{1/2}$, converges in distribution to the standard normal distribution\nas $r$ goes to infinity. Here, $\\ell( \\mathbf{\\beta})$ is the log-likelihood\nfunction on the model parameter $\\mathbf{\\beta}=(\\beta_1, \\ldots,\n\\beta_n)^\\top$, $\\widehat{\\mathbf{\\beta}}$ is its maximum likelihood estimator\n(MLE) under the full parameter space, and $\\widehat{\\mathbf{\\beta}}^0$ is the\nrestricted MLE under the null parameter space. For the homogenous null with a\nfixed $r$, we establish Wilks-type theorems that\n$2\\{\\ell(\\widehat{\\mathbf{\\beta}}) - \\ell(\\widehat{\\mathbf{\\beta}}^0)\\}$\nconverges in distribution to a chi-square distribution with $r-1$ degrees of\nfreedom, as the total number of parameters, $n$, goes to infinity. When testing\nthe fixed dimensional specified null, we find that its asymptotic null\ndistribution is a chi-square distribution in the $\\beta$-model. However,\nunexpectedly, this is not true in the Bradley-Terry model. By developing\nseveral novel technical methods for asymptotic expansion, we explore Wilks type\nresults in a principled manner; these principled methods should be applicable\nto a class of random graph models beyond the $\\beta$-model and the\nBradley-Terry model. Simulation studies and real network data applications\nfurther demonstrate the theoretical results.",
            "author": [
                "Ting Yan",
                "Yuanzhang Li",
                "Jinfeng Xu",
                "Yaning Yang",
                "Ji Zhu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05806v1",
                "http://arxiv.org/pdf/2311.05806v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.ME",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05795v1",
            "title": "Improvements on Uncertainty Quantification for Node Classification via\n  Distance-Based Regularization",
            "updated": "2023-11-10T00:00:20Z",
            "published": "2023-11-10T00:00:20Z",
            "summary": "Deep neural networks have achieved significant success in the last decades,\nbut they are not well-calibrated and often produce unreliable predictions. A\nlarge number of literature relies on uncertainty quantification to evaluate the\nreliability of a learning model, which is particularly important for\napplications of out-of-distribution (OOD) detection and misclassification\ndetection. We are interested in uncertainty quantification for interdependent\nnode-level classification. We start our analysis based on graph posterior\nnetworks (GPNs) that optimize the uncertainty cross-entropy (UCE)-based loss\nfunction. We describe the theoretical limitations of the widely-used UCE loss.\nTo alleviate the identified drawbacks, we propose a distance-based\nregularization that encourages clustered OOD nodes to remain clustered in the\nlatent space. We conduct extensive comparison experiments on eight standard\ndatasets and demonstrate that the proposed regularization outperforms the\nstate-of-the-art in both OOD detection and misclassification detection.",
            "author": [
                "Russell Alan Hart",
                "Linlin Yu",
                "Yifei Lou",
                "Feng Chen"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05795v1",
                "http://arxiv.org/pdf/2311.05795v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ML"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05792v1",
            "title": "Is a Seat at the Table Enough? Engaging Teachers and Students in Dataset\n  Specification for ML in Education",
            "updated": "2023-11-09T23:51:08Z",
            "published": "2023-11-09T23:51:08Z",
            "summary": "Despite the promises of ML in education, its adoption in the classroom has\nsurfaced numerous issues regarding fairness, accountability, and transparency,\nas well as concerns about data privacy and student consent. A root cause of\nthese issues is the lack of understanding of the complex dynamics of education,\nincluding teacher-student interactions, collaborative learning, and classroom\nenvironment. To overcome these challenges and fully utilize the potential of ML\nin education, software practitioners need to work closely with educators and\nstudents to fully understand the context of the data (the backbone of ML\napplications) and collaboratively define the ML data specifications. To gain a\ndeeper understanding of such a collaborative process, we conduct ten co-design\nsessions with ML software practitioners, educators, and students. In the\nsessions, teachers and students work with ML engineers, UX designers, and legal\npractitioners to define dataset characteristics for a given ML application. We\nfind that stakeholders contextualize data based on their domain and procedural\nknowledge, proactively design data requirements to mitigate downstream harms\nand data reliability concerns, and exhibit role-based collaborative strategies\nand contribution patterns. Further, we find that beyond a seat at the table,\nmeaningful stakeholder participation in ML requires structured supports:\ndefined processes for continuous iteration and co-evaluation, shared contextual\ndata quality standards, and information scaffolds for both technical and\nnon-technical stakeholders to traverse expertise boundaries.",
            "author": [
                "Mei Tan",
                "Hansol Lee",
                "Dakuo Wang",
                "Hariharan Subramonyam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05792v1",
                "http://arxiv.org/pdf/2311.05792v1"
            ],
            "primary_category": "cs.CY",
            "category": [
                "cs.CY",
                "cs.AI",
                "cs.HC",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05780v1",
            "title": "Real-time Control of Electric Autonomous Mobility-on-Demand Systems via\n  Graph Reinforcement Learning",
            "updated": "2023-11-09T22:57:21Z",
            "published": "2023-11-09T22:57:21Z",
            "summary": "Operators of Electric Autonomous Mobility-on-Demand (E-AMoD) fleets need to\nmake several real-time decisions such as matching available cars to ride\nrequests, rebalancing idle cars to areas of high demand, and charging vehicles\nto ensure sufficient range. While this problem can be posed as a linear program\nthat optimizes flows over a space-charge-time graph, the size of the resulting\noptimization problem does not allow for real-time implementation in realistic\nsettings. In this work, we present the E-AMoD control problem through the lens\nof reinforcement learning and propose a graph network-based framework to\nachieve drastically improved scalability and superior performance over\nheuristics. Specifically, we adopt a bi-level formulation where we (1) leverage\na graph network-based RL agent to specify a desired next state in the\nspace-charge graph, and (2) solve more tractable linear programs to best\nachieve the desired state while ensuring feasibility. Experiments using\nreal-world data from San Francisco and New York City show that our approach\nachieves up to 89% of the profits of the theoretically-optimal solution while\nachieving more than a 100x speedup in computational time. Furthermore, our\napproach outperforms the best domain-specific heuristics with comparable\nruntimes, with an increase in profits by up to 3x. Finally, we highlight\npromising zero-shot transfer capabilities of our learned policy on tasks such\nas inter-city generalization and service area expansion, thus showing the\nutility, scalability, and flexibility of our framework.",
            "author": [
                "Aaryan Singhal",
                "Daniele Gammelli",
                "Justin Luke",
                "Karthik Gopalakrishnan",
                "Dominik Helmreich",
                "Marco Pavone"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05780v1",
                "http://arxiv.org/pdf/2311.05780v1"
            ],
            "primary_category": "eess.SY",
            "category": [
                "eess.SY",
                "cs.LG",
                "cs.RO",
                "cs.SY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05778v1",
            "title": "DONUT-hole: DONUT Sparsification by Harnessing Knowledge and Optimizing\n  Learning Efficiency",
            "updated": "2023-11-09T22:49:05Z",
            "published": "2023-11-09T22:49:05Z",
            "summary": "This paper introduces DONUT-hole, a sparse OCR-free visual document\nunderstanding (VDU) model that addresses the limitations of its predecessor\nmodel, dubbed DONUT. The DONUT model, leveraging a transformer architecture,\novercoming the challenges of separate optical character recognition (OCR) and\nvisual semantic understanding (VSU) components. However, its deployment in\nproduction environments and edge devices is hindered by high memory and\ncomputational demands, particularly in large-scale request services. To\novercome these challenges, we propose an optimization strategy based on\nknowledge distillation and model pruning. Our paradigm to produce DONUT-hole,\nreduces the model denisty by 54\\% while preserving performance. We also achieve\na global representational similarity index between DONUT and DONUT-hole based\non centered kernel alignment (CKA) metric of 0.79. Moreover, we evaluate the\neffectiveness of DONUT-hole in the document image key information extraction\n(KIE) task, highlighting its potential for developing more efficient VDU\nsystems for logistic companies.",
            "author": [
                "Azhar Shaikh",
                "Michael Cochez",
                "Denis Diachkov",
                "Michiel de Rijcke",
                "Sahar Yousefi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05778v1",
                "http://arxiv.org/pdf/2311.05778v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05767v1",
            "title": "Dirichlet Energy Enhancement of Graph Neural Networks by Framelet\n  Augmentation",
            "updated": "2023-11-09T22:22:18Z",
            "published": "2023-11-09T22:22:18Z",
            "summary": "Graph convolutions have been a pivotal element in learning graph\nrepresentations. However, recursively aggregating neighboring information with\ngraph convolutions leads to indistinguishable node features in deep layers,\nwhich is known as the over-smoothing issue. The performance of graph neural\nnetworks decays fast as the number of stacked layers increases, and the\nDirichlet energy associated with the graph decreases to zero as well. In this\nwork, we introduce a framelet system into the analysis of Dirichlet energy and\ntake a multi-scale perspective to leverage the Dirichlet energy and alleviate\nthe over-smoothing issue. Specifically, we develop a Framelet Augmentation\nstrategy by adjusting the update rules with positive and negative increments\nfor low-pass and high-passes respectively. Based on that, we design the Energy\nEnhanced Convolution (EEConv), which is an effective and practical operation\nthat is proved to strictly enhance Dirichlet energy. From a message-passing\nperspective, EEConv inherits multi-hop aggregation property from the framelet\ntransform and takes into account all hops in the multi-scale representation,\nwhich benefits the node classification tasks over heterophilous graphs.\nExperiments show that deep GNNs with EEConv achieve state-of-the-art\nperformance over various node classification datasets, especially for\nheterophilous graphs, while also lifting the Dirichlet energy as the network\ngoes deeper.",
            "author": [
                "Jialin Chen",
                "Yuelin Wang",
                "Cristian Bodnar",
                "Rex Ying",
                "Pietro Lio",
                "Yu Guang Wang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05767v1",
                "http://arxiv.org/pdf/2311.05767v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05764v1",
            "title": "Generative Explanations for Graph Neural Network: Methods and\n  Evaluations",
            "updated": "2023-11-09T22:07:15Z",
            "published": "2023-11-09T22:07:15Z",
            "summary": "Graph Neural Networks (GNNs) achieve state-of-the-art performance in various\ngraph-related tasks. However, the black-box nature often limits their\ninterpretability and trustworthiness. Numerous explainability methods have been\nproposed to uncover the decision-making logic of GNNs, by generating underlying\nexplanatory substructures. In this paper, we conduct a comprehensive review of\nthe existing explanation methods for GNNs from the perspective of graph\ngeneration. Specifically, we propose a unified optimization objective for\ngenerative explanation methods, comprising two sub-objectives: Attribution and\nInformation constraints. We further demonstrate their specific manifestations\nin various generative model architectures and different explanation scenarios.\nWith the unified objective of the explanation problem, we reveal the shared\ncharacteristics and distinctions among current methods, laying the foundation\nfor future methodological advancements. Empirical results demonstrate the\nadvantages and limitations of different explainability approaches in terms of\nexplanation performance, efficiency, and generalizability.",
            "author": [
                "Jialin Chen",
                "Kenza Amara",
                "Junchi Yu",
                "Rex Ying"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05764v1",
                "http://arxiv.org/pdf/2311.05764v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05756v1",
            "title": "Step and Smooth Decompositions as Topological Clustering",
            "updated": "2023-11-09T21:46:22Z",
            "published": "2023-11-09T21:46:22Z",
            "summary": "We investigate a class of recovery problems for which observations are a\nnoisy combination of continuous and step functions. These problems can be seen\nas non-injective instances of non-linear ICA with direct applications to image\ndecontamination for magnetic resonance imaging. Alternately, the problem can be\nviewed as clustering in the presence of structured (smooth) contaminant. We\nshow that a global topological property (graph connectivity) interacts with a\nlocal property (the degree of smoothness of the continuous component) to\ndetermine conditions under which the components are identifiable. Additionally,\na practical estimation algorithm is provided for the case when the contaminant\nlies in a reproducing kernel Hilbert space of continuous functions. Algorithm\neffectiveness is demonstrated through a series of simulations and real-world\nstudies.",
            "author": [
                "Luciano Vinas",
                "Arash A. Amini"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05756v1",
                "http://arxiv.org/pdf/2311.05756v1"
            ],
            "primary_category": "math.ST",
            "category": [
                "math.ST",
                "stat.AP",
                "stat.ML",
                "stat.TH"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05733v1",
            "title": "LogShield: A Transformer-based APT Detection System Leveraging\n  Self-Attention",
            "updated": "2023-11-09T20:43:15Z",
            "published": "2023-11-09T20:43:15Z",
            "summary": "Cyber attacks are often identified using system and network logs. There have\nbeen significant prior works that utilize provenance graphs and ML techniques\nto detect attacks, specifically advanced persistent threats, which are very\ndifficult to detect. Lately, there have been studies where transformer-based\nlanguage models are being used to detect various types of attacks from system\nlogs. However, no such attempts have been made in the case of APTs. In\naddition, existing state-of-the-art techniques that use system provenance\ngraphs, lack a data processing framework generalized across datasets for\noptimal performance. For mitigating this limitation as well as exploring the\neffectiveness of transformer-based language models, this paper proposes\nLogShield, a framework designed to detect APT attack patterns leveraging the\npower of self-attention in transformers. We incorporate customized embedding\nlayers to effectively capture the context of event sequences derived from\nprovenance graphs. While acknowledging the computational overhead associated\nwith training transformer networks, our framework surpasses existing LSTM and\nLanguage models regarding APT detection. We integrated the model parameters and\ntraining procedure from the RoBERTa model and conducted extensive experiments\non well-known APT datasets (DARPA OpTC and DARPA TC E3). Our framework achieved\nsuperior F1 scores of 98% and 95% on the two datasets respectively, surpassing\nthe F1 scores of 96% and 94% obtained by LSTM models. Our findings suggest that\nLogShield's performance benefits from larger datasets and demonstrates its\npotential for generalization across diverse domains. These findings contribute\nto the advancement of APT attack detection methods and underscore the\nsignificance of transformer-based architectures in addressing security\nchallenges in computer systems.",
            "author": [
                "Sihat Afnan",
                "Mushtari Sadia",
                "Shahrear Iqbal",
                "Anindya Iqbal"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05733v1",
                "http://arxiv.org/pdf/2311.05733v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05729v1",
            "title": "GIPCOL: Graph-Injected Soft Prompting for Compositional Zero-Shot\n  Learning",
            "updated": "2023-11-09T20:32:18Z",
            "published": "2023-11-09T20:32:18Z",
            "summary": "Pre-trained vision-language models (VLMs) have achieved promising success in\nmany fields, especially with prompt learning paradigm. In this work, we propose\nGIP-COL (Graph-Injected Soft Prompting for COmpositional Learning) to better\nexplore the compositional zero-shot learning (CZSL) ability of VLMs within the\nprompt-based learning framework. The soft prompt in GIPCOL is structured and\nconsists of the prefix learnable vectors, attribute label and object label. In\naddition, the attribute and object labels in the soft prompt are designated as\nnodes in a compositional graph. The compositional graph is constructed based on\nthe compositional structure of the objects and attributes extracted from the\ntraining data and consequently feeds the updated concept representation into\nthe soft prompt to capture this compositional structure for a better prompting\nfor CZSL. With the new prompting strategy, GIPCOL achieves state-of-the-art AUC\nresults on all three CZSL benchmarks, including MIT-States, UT-Zappos, and\nC-GQA datasets in both closed and open settings compared to previous non-CLIP\nas well as CLIP-based methods. We analyze when and why GIPCOL operates well\ngiven the CLIP backbone and its training data limitations, and our findings\nshed light on designing more effective prompts for CZSL",
            "author": [
                "Guangyue Xu",
                "Joyce Chai",
                "Parisa Kordjamshidi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05729v1",
                "http://arxiv.org/pdf/2311.05729v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05722v1",
            "title": "Verilog-to-PyG -- A Framework for Graph Learning and Augmentation on RTL\n  Designs",
            "updated": "2023-11-09T20:11:40Z",
            "published": "2023-11-09T20:11:40Z",
            "summary": "The complexity of modern hardware designs necessitates advanced methodologies\nfor optimizing and analyzing modern digital systems. In recent times, machine\nlearning (ML) methodologies have emerged as potent instruments for assessing\ndesign quality-of-results at the Register-Transfer Level (RTL) or Boolean\nlevel, aiming to expedite design exploration of advanced RTL configurations. In\nthis presentation, we introduce an innovative open-source framework that\ntranslates RTL designs into graph representation foundations, which can be\nseamlessly integrated with the PyTorch Geometric graph learning platform.\nFurthermore, the Verilog-to-PyG (V2PYG) framework is compatible with the\nopen-source Electronic Design Automation (EDA) toolchain OpenROAD, facilitating\nthe collection of labeled datasets in an utterly open-source manner.\nAdditionally, we will present novel RTL data augmentation methods (incorporated\nin our framework) that enable functional equivalent design augmentation for the\nconstruction of an extensive graph-based RTL design database. Lastly, we will\nshowcase several using cases of V2PYG with detailed scripting examples. V2PYG\ncan be found at \\url{https://yu-maryland.github.io/Verilog-to-PyG/}.",
            "author": [
                "Yingjie Li",
                "Mingju Liu",
                "Alan Mishchenko",
                "Cunxi Yu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05722v1",
                "http://arxiv.org/pdf/2311.05722v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AR",
                "cs.LO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05719v1",
            "title": "Induced subgraphs and tree decompositions XIV. Non-adjacent neighbours\n  in a hole",
            "updated": "2023-11-09T20:01:16Z",
            "published": "2023-11-09T20:01:16Z",
            "summary": "A clock is a graph consisting of an induced cycle $C$ and a vertex not in $C$\nwith at least two non-adjacent neighbours in $C$. We show that every clock-free\ngraph of large treewidth contains a \"basic obstruction\" of large treewidth as\nan induced subgraph: a complete graph, a subdivision of a wall, or the line\ngraph of a subdivision of a wall.",
            "author": [
                "Maria Chudnovsky",
                "Sepehr Hajebi",
                "Sophie Spirkl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05719v1",
                "http://arxiv.org/pdf/2311.05719v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05713v1",
            "title": "List-$k$-Coloring $H$-free graphs for all $k>4$",
            "updated": "2023-11-09T19:48:23Z",
            "published": "2023-11-09T19:48:23Z",
            "summary": "Given an integer $k>4$ and a graph $H$, we prove that, assuming P$\\neq$NP,\nthe List-$k$-Coloring Problem restricted to $H$-free graphs can be solved in\npolynomial time if and only if either every component of $H$ is a path on at\nmost three vertices, or removing the isolated vertices of $H$ leaves an induced\nsubgraph of the five-vertex path. In fact, the \"if\" implication holds for all\n$k\\geq 1$.",
            "author": [
                "Maria Chudnovsky",
                "Sepehr Hajebi",
                "Sophie Spirkl"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05713v1",
                "http://arxiv.org/pdf/2311.05713v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05692v1",
            "title": "Phase-Field Model of Silicon Carbide Growth During Isothermal Condition",
            "updated": "2023-11-09T19:04:26Z",
            "published": "2023-11-09T19:04:26Z",
            "summary": "Silicon carbide (SiC) emerges as a promising ceramic material for\nhigh-temperature structural applications, especially within the aerospace\nsector. The utilization of SiC-based ceramic matrix composites (CMCs) instead\nof superalloys in components like engine shrouds, combustors, and nozzles\noffers notable advantages, including a 25% improvement in fuel efficiency, over\n10% enhanced thrust, and the capability to withstand up to 500$^{\\circ}$C\nhigher operating temperatures. Employing a CALPHAD-reinforced multi-phase-field\nmodel, our study delves into the evolution of the SiC layer under isothermal\nsolidification conditions. By modeling the growth of SiC between liquid Si and\nsolid C at 1450$^{\\circ}$C, we compared results with experimental\nmicrostructures and quantitatively examined the evolution of SiC thickness over\ntime. Efficient sampling across the entire model space mitigated uncertainty in\nhigh-temperature kinetic parameters, allowing us to predict a range of growth\nrates and morphologies for the SiC layer. The model accounts for parameter\nuncertainty stemming from limited experimental knowledge and successfully\npredicts relevant morphologies for the system. Experimental results validated\nthe kinetic parameters of the simulations, offering valuable insights and\npotential constraints on the reaction kinetics. We further explored the\nsignificance of multi-phase-field model parameters on two key outputs, and\nfound that the diffusion coefficient of liquid Si emerges as the most crucial\nparameter significantly impacting the SiC average layer thickness and grain\ncount over time. This study provides valuable insights into the microstructure\nevolution of the Si-C binary system, offering pertinent information for the\nengineering of CMCs in industrial applications.",
            "author": [
                "Elias J. Munoz",
                "Vahid Attari",
                "Marco Martinez",
                "Matthew B. Dickerson",
                "Miladin Radovic",
                "Raymundo Arroyave"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05692v1",
                "http://arxiv.org/pdf/2311.05692v1"
            ],
            "primary_category": "physics.app-ph",
            "category": [
                "physics.app-ph",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05596v1",
            "title": "LLM Augmented Hierarchical Agents",
            "updated": "2023-11-09T18:54:28Z",
            "published": "2023-11-09T18:54:28Z",
            "summary": "Solving long-horizon, temporally-extended tasks using Reinforcement Learning\n(RL) is challenging, compounded by the common practice of learning without\nprior knowledge (or tabula rasa learning). Humans can generate and execute\nplans with temporally-extended actions and quickly learn to perform new tasks\nbecause we almost never solve problems from scratch. We want autonomous agents\nto have this same ability. Recently, LLMs have been shown to encode a\ntremendous amount of knowledge about the world and to perform impressive\nin-context learning and reasoning. However, using LLMs to solve real world\nproblems is hard because they are not grounded in the current task. In this\npaper we exploit the planning capabilities of LLMs while using RL to provide\nlearning from the environment, resulting in a hierarchical agent that uses LLMs\nto solve long-horizon tasks. Instead of completely relying on LLMs, they guide\na high-level policy, making learning significantly more sample efficient. This\napproach is evaluated in simulation environments such as MiniGrid, SkillHack,\nand Crafter, and on a real robot arm in block manipulation tasks. We show that\nagents trained using our approach outperform other baselines methods and, once\ntrained, don't need access to LLMs during deployment.",
            "author": [
                "Bharat Prakash",
                "Tim Oates",
                "Tinoosh Mohsenin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05596v1",
                "http://arxiv.org/pdf/2311.05596v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.AI",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05580v1",
            "title": "Inference for Probabilistic Dependency Graphs",
            "updated": "2023-11-09T18:40:12Z",
            "published": "2023-11-09T18:40:12Z",
            "summary": "Probabilistic dependency graphs (PDGs) are a flexible class of probabilistic\ngraphical models, subsuming Bayesian Networks and Factor Graphs. They can also\ncapture inconsistent beliefs, and provide a way of measuring the degree of this\ninconsistency. We present the first tractable inference algorithm for PDGs with\ndiscrete variables, making the asymptotic complexity of PDG inference similar\nthat of the graphical models they generalize. The key components are: (1) the\nobservation that, in many cases, the distribution a PDG specifies can be\nformulated as a convex optimization problem (with exponential cone\nconstraints), (2) a construction that allows us to express these problems\ncompactly for PDGs of boundeed treewidth, (3) contributions to the theory of\nPDGs that justify the construction, and (4) an appeal to interior point methods\nthat can solve such problems in polynomial time. We verify the correctness and\ncomplexity of our approach, and provide an implementation of it. We then\nevaluate our implementation, and demonstrate that it outperforms baseline\napproaches. Our code is available at\nhttp://github.com/orichardson/pdg-infer-uai.",
            "author": [
                "Oliver E. Richardson",
                "Joseph Y. Halpern",
                "Christopher De Sa"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05580v1",
                "http://arxiv.org/pdf/2311.05580v1"
            ],
            "primary_category": "cs.DS",
            "category": [
                "cs.DS",
                "cs.AI",
                "cs.CC",
                "math.PR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05575v1",
            "title": "Cliques in derangement graphs for innately transitive groups",
            "updated": "2023-11-09T18:33:44Z",
            "published": "2023-11-09T18:33:44Z",
            "summary": "Given a permutation group $G$, the derangement graph of $G$ is the Cayley\ngraph with connection set the derangements of $G$. The group $G$ is said to be\ninnately transitive if $G$ has a transitive minimal normal subgroup. Clearly,\nevery primitive group is innately transitive. We show that, besides an infinite\nfamily of explicit exceptions, there exists a function $f:\\mathbb{N}\\to\n\\mathbb{N}$ such that, if $G$ is innately transitive of degree $n$ and the\nderangement graph of $G$ has no clique of size $k$, then $n\\le f(k)$.\n  Motivation for this work arises from investigations on Erd\\H{o}s-Ko-Rado type\ntheorems for permutation groups.",
            "author": [
                "Marco Fusari",
                "Andrea Previtali",
                "Pablo Spiga"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05575v1",
                "http://arxiv.org/pdf/2311.05575v1"
            ],
            "primary_category": "math.GR",
            "category": [
                "math.GR",
                "math.NT"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05574v1",
            "title": "A near-optimal zero-free disk for the Ising model",
            "updated": "2023-11-09T18:33:02Z",
            "published": "2023-11-09T18:33:02Z",
            "summary": "The partition function of the Ising model of a graph $G=(V,E)$ is defined as\n$Z_{\\text{Ising}}(G;b)=\\sum_{\\sigma:V\\to \\{0,1\\}} b^{m(\\sigma)}$, where\n$m(\\sigma)$ denotes the number of edges $e=\\{u,v\\}$ such that\n$\\sigma(u)=\\sigma(v)$. We show that for any positive integer $\\Delta$ and any\ngraph $G$ of maximum degree at most $\\Delta$, $Z_{\\text{Ising}}(G;b)\\neq 0$ for\nall $b\\in \\mathbb{C}$ satisfying $|\\frac{b-1}{b+1}| \\leq\n\\frac{1-o_\\Delta(1)}{\\Delta-1}$ (where $o_\\Delta(1) \\to 0$ as $\\Delta\\to\n\\infty$). This is optimal in the sense that $\\tfrac{1-o_\\Delta(1)}{\\Delta-1}$\ncannot be replaced by $\\tfrac{c}{\\Delta-1}$ for any constant $c > 1$ unless\nP=NP.\n  To prove our result we use a standard reformulation of the partition function\nof the Ising model as the generating function of even sets. We establish a\nzero-free disk for this generating function inspired by techniques from\nstatistical physics on partition functions of a polymer models. Our approach is\nquite general and we discuss extensions of it to a certain types of polymer\nmodels.",
            "author": [
                "Viresh Patel",
                "Guus Regts",
                "Ayla Stam"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05574v1",
                "http://arxiv.org/pdf/2311.05574v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "cs.DS",
                "math-ph",
                "math.MP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05542v2",
            "title": "Counterexamples to conjectures on the occupancy fraction of graphs",
            "updated": "2023-11-28T19:10:05Z",
            "published": "2023-11-09T17:44:26Z",
            "summary": "The occupancy fraction of a graph is a (normalized) measure on the size of\nindependent sets under the hard-core model, depending on a variable (fugacity)\n$\\lambda.$ We present a criterion for finding the graph with minimum occupancy\nfraction among graphs with a fixed order, and disprove five conjectures on the\nextremes of the occupancy fraction and (normalized) independence polynomial for\ncertain graph classes of regular graphs with a given girth.",
            "author": [
                "Stijn Cambie",
                "Jorik Jooken"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05542v2",
                "http://arxiv.org/pdf/2311.05542v2"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C07, 05C31, 05C35, 05C69, 68R05, 68R10"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05533v1",
            "title": "Building Hamiltonian Cycles in the Semi-Random Graph Process in Less\n  Than $2n$ Rounds",
            "updated": "2023-11-09T17:25:21Z",
            "published": "2023-11-09T17:25:21Z",
            "summary": "The semi-random graph process is an adaptive random graph process in which an\nonline algorithm is initially presented an empty graph on $n$ vertices. In each\nround, a vertex $u$ is presented to the algorithm independently and uniformly\nat random. The algorithm then adaptively selects a vertex $v$, and adds the\nedge $uv$ to the graph. For a given graph property, the objective of the\nalgorithm is to force the graph to satisfy this property asymptotically almost\nsurely in as few rounds as possible.\n  We focus on the property of Hamiltonicity. We present an adaptive strategy\nwhich creates a Hamiltonian cycle in $\\alpha n$ rounds, where $\\alpha <\n1.81696$ is derived from the solution to a system of differential equations. We\nalso show that achieving Hamiltonicity requires at least $\\beta n$ rounds,\nwhere $\\beta > 1.26575$.",
            "author": [
                "Alan Frieze",
                "Pu Gao",
                "Calum MacRury",
                "Pawe\u0142 Pra\u0142at",
                "Gregory Sorkin"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05533v1",
                "http://arxiv.org/pdf/2311.05533v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.07590v2",
            "title": "Technical Report: Large Language Models can Strategically Deceive their\n  Users when Put Under Pressure",
            "updated": "2023-11-27T15:17:49Z",
            "published": "2023-11-09T17:12:44Z",
            "summary": "We demonstrate a situation in which Large Language Models, trained to be\nhelpful, harmless, and honest, can display misaligned behavior and\nstrategically deceive their users about this behavior without being instructed\nto do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated\nenvironment, where it assumes the role of an autonomous stock trading agent.\nWithin this environment, the model obtains an insider tip about a lucrative\nstock trade and acts upon it despite knowing that insider trading is\ndisapproved of by company management. When reporting to its manager, the model\nconsistently hides the genuine reasons behind its trading decision. We perform\na brief investigation of how this behavior varies under changes to the setting,\nsuch as removing model access to a reasoning scratchpad, attempting to prevent\nthe misaligned behavior by changing system instructions, changing the amount of\npressure the model is under, varying the perceived risk of getting caught, and\nmaking other simple changes to the environment. To our knowledge, this is the\nfirst demonstration of Large Language Models trained to be helpful, harmless,\nand honest, strategically deceiving their users in a realistic situation\nwithout direct instructions or training for deception.",
            "author": [
                "J\u00e9r\u00e9my Scheurer",
                "Mikita Balesni",
                "Marius Hobbhahn"
            ],
            "link": [
                "http://arxiv.org/abs/2311.07590v2",
                "http://arxiv.org/pdf/2311.07590v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05521v2",
            "title": "BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis",
            "updated": "2023-11-28T15:31:46Z",
            "published": "2023-11-09T17:05:53Z",
            "summary": "Synthesizing photorealistic 4D human head avatars from videos is essential\nfor VR/AR, telepresence, and video game applications. Although existing Neural\nRadiance Fields (NeRF)-based methods achieve high-fidelity results, the\ncomputational expense limits their use in real-time applications. To overcome\nthis limitation, we introduce BakedAvatar, a novel representation for real-time\nneural head avatar synthesis, deployable in a standard polygon rasterization\npipeline. Our approach extracts deformable multi-layer meshes from learned\nisosurfaces of the head and computes expression-, pose-, and view-dependent\nappearances that can be baked into static textures for efficient rasterization.\nWe thus propose a three-stage pipeline for neural head avatar synthesis, which\nincludes learning continuous deformation, manifold, and radiance fields,\nextracting layered meshes and textures, and fine-tuning texture details with\ndifferential rasterization. Experimental results demonstrate that our\nrepresentation generates synthesis results of comparable quality to other\nstate-of-the-art methods while significantly reducing the inference time\nrequired. We further showcase various head avatar synthesis results from\nmonocular videos, including view synthesis, face reenactment, expression\nediting, and pose editing, all at interactive frame rates.",
            "author": [
                "Hao-Bin Duan",
                "Miao Wang",
                "Jin-Chuan Shi",
                "Xu-Chuan Chen",
                "Yan-Pei Cao"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3618399",
                "http://arxiv.org/abs/2311.05521v2",
                "http://arxiv.org/pdf/2311.05521v2"
            ],
            "primary_category": "cs.GR",
            "category": [
                "cs.GR",
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05501v1",
            "title": "Dirichlet Active Learning",
            "updated": "2023-11-09T16:39:02Z",
            "published": "2023-11-09T16:39:02Z",
            "summary": "This work introduces Dirichlet Active Learning (DiAL), a Bayesian-inspired\napproach to the design of active learning algorithms. Our framework models\nfeature-conditional class probabilities as a Dirichlet random field and lends\nobservational strength between similar features in order to calibrate the\nrandom field. This random field can then be utilized in learning tasks: in\nparticular, we can use current estimates of mean and variance to conduct\nclassification and active learning in the context where labeled data is scarce.\nWe demonstrate the applicability of this model to low-label rate graph learning\nby constructing ``propagation operators'' based upon the graph Laplacian, and\noffer computational studies demonstrating the method's competitiveness with the\nstate of the art. Finally, we provide rigorous guarantees regarding the ability\nof this approach to ensure both exploration and exploitation, expressed\nrespectively in terms of cluster exploration and increased attention to\ndecision boundaries.",
            "author": [
                "Kevin Miller",
                "Ryan Murray"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05501v1",
                "http://arxiv.org/pdf/2311.05501v1"
            ],
            "primary_category": "stat.ML",
            "category": [
                "stat.ML",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05500v1",
            "title": "Sparse universal graphs for sparse graph families",
            "updated": "2023-11-09T16:38:31Z",
            "published": "2023-11-09T16:38:31Z",
            "summary": "A graph $G$ is $\\textit{universal}$ for a (finite) family $\\mathcal{H}$ of\ngraphs if every $H \\in \\mathcal{H}$ is a subgraph of $G$. We show that for\nevery fixed integer $k \\geq 2$, the minimum number of edges in a universal\ngraph for the family of all graphs with $n$ vertices and acyclic chromatic\nnumber at most $k$ is at most $O(n^{2-1/k})$ and at least $n^{2-1/(k-1)-o(1)}$.\nWe also show that the minimum number of edges in a universal graph for the\nfamily of all $k$-degenerate graphs with $n$ vertices is at most\n$\\widetilde{O}(n^{2-1/(2k+1)})$ and at least $n^{2-1/k-o(1)}$.",
            "author": [
                "Noga Alon",
                "Natalie Dodson",
                "Carmen Jackson",
                "Rose McCarty",
                "Lani Southern"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05500v1",
                "http://arxiv.org/pdf/2311.05500v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05494v1",
            "title": "Object-centric Cross-modal Feature Distillation for Event-based Object\n  Detection",
            "updated": "2023-11-09T16:33:08Z",
            "published": "2023-11-09T16:33:08Z",
            "summary": "Event cameras are gaining popularity due to their unique properties, such as\ntheir low latency and high dynamic range. One task where these benefits can be\ncrucial is real-time object detection. However, RGB detectors still outperform\nevent-based detectors due to the sparsity of the event data and missing visual\ndetails. In this paper, we develop a novel knowledge distillation approach to\nshrink the performance gap between these two modalities. To this end, we\npropose a cross-modality object detection distillation method that by design\ncan focus on regions where the knowledge distillation works best. We achieve\nthis by using an object-centric slot attention mechanism that can iteratively\ndecouple features maps into object-centric features and corresponding\npixel-features used for distillation. We evaluate our novel distillation\napproach on a synthetic and a real event dataset with aligned grayscale images\nas a teacher modality. We show that object-centric distillation allows to\nsignificantly improve the performance of the event-based student object\ndetector, nearly halving the performance gap with respect to the teacher.",
            "author": [
                "Lei Li",
                "Alexander Liniger",
                "Mario Millhaeusler",
                "Vagia Tsiminaki",
                "Yuanyou Li",
                "Dengxin Dai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05494v1",
                "http://arxiv.org/pdf/2311.05494v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.RO"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05490v1",
            "title": "General Policies, Subgoal Structure, and Planning Width",
            "updated": "2023-11-09T16:30:22Z",
            "published": "2023-11-09T16:30:22Z",
            "summary": "It has been observed that many classical planning domains with atomic goals\ncan be solved by means of a simple polynomial exploration procedure, called IW,\nthat runs in time exponential in the problem width, which in these cases is\nbounded and small. Yet, while the notion of width has become part of\nstate-of-the-art planning algorithms such as BFWS, there is no good explanation\nfor why so many benchmark domains have bounded width when atomic goals are\nconsidered. In this work, we address this question by relating bounded width\nwith the existence of general optimal policies that in each planning instance\nare represented by tuples of atoms of bounded size. We also define the notions\nof (explicit) serializations and serialized width that have a broader scope as\nmany domains have a bounded serialized width but no bounded width. Such\nproblems are solved non-optimally in polynomial time by a suitable variant of\nthe Serialized IW algorithm. Finally, the language of general policies and the\nsemantics of serializations are combined to yield a simple, meaningful, and\nexpressive language for specifying serializations in compact form in the form\nof sketches, which can be used for encoding domain control knowledge by hand or\nfor learning it from small examples. Sketches express general problem\ndecompositions in terms of subgoals, and sketches of bounded width express\nproblem decompositions that can be solved in polynomial time.",
            "author": [
                "Blai Bonet",
                "Hector Geffner"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05490v1",
                "http://arxiv.org/pdf/2311.05490v1"
            ],
            "primary_category": "cs.AI",
            "category": [
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05489v1",
            "title": "On the KdV approximation for a Boussinesq equation posed on the infinite\n  necklace graph",
            "updated": "2023-11-09T16:29:17Z",
            "published": "2023-11-09T16:29:17Z",
            "summary": "We consider a Boussinesq equation posed on the infinite periodic necklace\ngraph. For the description of long wave traveling waves we derive the KdV\nequation and establish the validity of this formal approximation by providing\nestimates for the error. The proof is based on suitable energy estimates.",
            "author": [
                "Wolf-Patrick D\u00fcll",
                "Guido Schneider",
                "Raphael Taraca"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05489v1",
                "http://arxiv.org/pdf/2311.05489v1"
            ],
            "primary_category": "math.AP",
            "category": [
                "math.AP"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05479v1",
            "title": "Retinal OCT Synthesis with Denoising Diffusion Probabilistic Models for\n  Layer Segmentation",
            "updated": "2023-11-09T16:09:24Z",
            "published": "2023-11-09T16:09:24Z",
            "summary": "Modern biomedical image analysis using deep learning often encounters the\nchallenge of limited annotated data. To overcome this issue, deep generative\nmodels can be employed to synthesize realistic biomedical images. In this\nregard, we propose an image synthesis method that utilizes denoising diffusion\nprobabilistic models (DDPMs) to automatically generate retinal optical\ncoherence tomography (OCT) images. By providing rough layer sketches, the\ntrained DDPMs can generate realistic circumpapillary OCT images. We further\nfind that more accurate pseudo labels can be obtained through knowledge\nadaptation, which greatly benefits the segmentation task. Through this, we\nobserve a consistent improvement in layer segmentation accuracy, which is\nvalidated using various neural networks. Furthermore, we have discovered that a\nlayer segmentation model trained solely with synthesized images can achieve\ncomparable results to a model trained exclusively with real images. These\nfindings demonstrate the promising potential of DDPMs in reducing the need for\nmanual annotations of retinal OCT images.",
            "author": [
                "Yuli Wu",
                "Weidong He",
                "Dennis Eschweiler",
                "Ningxin Dou",
                "Zixin Fan",
                "Shengli Mi",
                "Peter Walter",
                "Johannes Stegmaier"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05479v1",
                "http://arxiv.org/pdf/2311.05479v1"
            ],
            "primary_category": "eess.IV",
            "category": [
                "eess.IV",
                "cs.CV",
                "physics.med-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05472v1",
            "title": "Text Representation Distillation via Information Bottleneck Principle",
            "updated": "2023-11-09T16:04:17Z",
            "published": "2023-11-09T16:04:17Z",
            "summary": "Pre-trained language models (PLMs) have recently shown great success in text\nrepresentation field. However, the high computational cost and high-dimensional\nrepresentation of PLMs pose significant challenges for practical applications.\nTo make models more accessible, an effective method is to distill large models\ninto smaller representation models. In order to relieve the issue of\nperformance degradation after distillation, we propose a novel Knowledge\nDistillation method called IBKD. This approach is motivated by the Information\nBottleneck principle and aims to maximize the mutual information between the\nfinal representation of the teacher and student model, while simultaneously\nreducing the mutual information between the student model's representation and\nthe input data. This enables the student model to preserve important learned\ninformation while avoiding unnecessary information, thus reducing the risk of\nover-fitting. Empirical studies on two main downstream applications of text\nrepresentation (Semantic Textual Similarity and Dense Retrieval tasks)\ndemonstrate the effectiveness of our proposed approach.",
            "author": [
                "Yanzhao Zhang",
                "Dingkun Long",
                "Zehan Li",
                "Pengjun Xie"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05472v1",
                "http://arxiv.org/pdf/2311.05472v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05467v1",
            "title": "Lithium-ion battery performance model including solvent segregation\n  effects",
            "updated": "2023-11-09T15:57:19Z",
            "published": "2023-11-09T15:57:19Z",
            "summary": "A model of a lithium-ion battery containing a cosolvent electrolyte is\ndeveloped and implemented within the open-source PyBaMM platform. Lithium-ion\nelectrolytes are essential to battery operation and normally contain at least\ntwo solvents to satisfy performance requirements. The widely used\nDoyle-Fuller-Newman battery model assumes that the electrolyte comprises a salt\ndissolved in a single effective solvent, however. This single-solvent\napproximation has been disproved experimentally and may hinder accurate battery\nmodelling. Here, we present a two-solvent model that resolves the transport of\nethylene carbonate (EC) and lithium salt in a background linear carbonate. EC\nconcentration polarization opposes that of Li+ during cycling, affecting local\nelectrolyte properties and cell-level overpotentials. Concentration gradients\nof Li+ can be affected by cross-diffusion, whereby EC gradients enhance or\nimpede salt flux. A rationally parametrized model that includes EC transport\npredicts 6% more power loss at 4.5C discharge and ~0.32% more capacity loss\nafter a thousand 1C cycles than its single-solvent equivalent. This work\nprovides a tool to model more transport behaviour in the electrolyte that may\naffect degradation and enables the transfer of microscopic knowledge about\nsolvation structure-dependent performance to the macroscale.",
            "author": [
                "Ruihe Li",
                "Simon O'Kane",
                "Andrew Wang",
                "Taeho Jung",
                "Niall Kirkaldy",
                "Monica Marinescu",
                "Charles W. Monroe",
                "Gregory J. Offer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05467v1",
                "http://arxiv.org/pdf/2311.05467v1"
            ],
            "primary_category": "physics.chem-ph",
            "category": [
                "physics.chem-ph"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05457v1",
            "title": "Automated Mobile Sensing Strategies Generation for Human Behaviour\n  Understanding",
            "updated": "2023-11-09T15:48:33Z",
            "published": "2023-11-09T15:48:33Z",
            "summary": "Mobile sensing plays a crucial role in generating digital traces to\nunderstand human daily lives. However, studying behaviours like mood or sleep\nquality in smartphone users requires carefully designed mobile sensing\nstrategies such as sensor selection and feature construction. This process is\ntime-consuming, burdensome, and requires expertise in multiple domains.\nFurthermore, the resulting sensing framework lacks generalizability, making it\ndifficult to apply to different scenarios. To address these challenges, we\npropose an automated mobile sensing strategy for human behaviour understanding.\nFirst, we establish a knowledge base and consolidate rules for effective\nfeature construction, data collection, and model selection. Then, we introduce\nthe multi-granular human behaviour representation and design procedures for\nleveraging large language models to generate strategies. Our approach is\nvalidated through blind comparative studies and usability evaluation.\nUltimately, our approach holds the potential to revolutionise the field of\nmobile sensing and its applications.",
            "author": [
                "Nan Gao",
                "Zhuolei Yu",
                "Chun Yu",
                "Yuntao Wang",
                "Flora D. Salim",
                "Yuanchun Shi"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05457v1",
                "http://arxiv.org/pdf/2311.05457v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05440v2",
            "title": "A Practical Approach to Novel Class Discovery in Tabular Data",
            "updated": "2023-12-05T16:46:09Z",
            "published": "2023-11-09T15:24:44Z",
            "summary": "The problem of Novel Class Discovery (NCD) consists in extracting knowledge\nfrom a labeled set of known classes to accurately partition an unlabeled set of\nnovel classes. While NCD has recently received a lot of attention from the\ncommunity, it is often solved on computer vision problems and under unrealistic\nconditions. In particular, the number of novel classes is usually assumed to be\nknown in advance, and their labels are sometimes used to tune hyperparameters.\nMethods that rely on these assumptions are not applicable in real-world\nscenarios. In this work, we focus on solving NCD in tabular data when no prior\nknowledge of the novel classes is available. To this end, we propose to tune\nthe hyperparameters of NCD methods by adapting the $k$-fold cross-validation\nprocess and hiding some of the known classes in each fold. Since we have found\nthat methods with too many hyperparameters are likely to overfit these hidden\nclasses, we define a simple deep NCD model. This method is composed of only the\nessential elements necessary for the NCD problem and performs impressively well\nunder realistic conditions. Furthermore, we find that the latent space of this\nmethod can be used to reliably estimate the number of novel classes.\nAdditionally, we adapt two unsupervised clustering algorithms ($k$-means and\nSpectral Clustering) to leverage the knowledge of the known classes. Extensive\nexperiments are conducted on 7 tabular datasets and demonstrate the\neffectiveness of the proposed method and hyperparameter tuning process, and\nshow that the NCD problem can be solved without relying on knowledge from the\nnovel classes.",
            "author": [
                "Colin Troisemaine",
                "Alexandre Reiffers-Masson",
                "St\u00e9phane Gosselin",
                "Vincent Lemaire",
                "Sandrine Vaton"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05440v2",
                "http://arxiv.org/pdf/2311.05440v2"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05437v1",
            "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
            "updated": "2023-11-09T15:22:26Z",
            "published": "2023-11-09T15:22:26Z",
            "summary": "LLaVA-Plus is a general-purpose multimodal assistant that expands the\ncapabilities of large multimodal models. It maintains a skill repository of\npre-trained vision and vision-language models and can activate relevant tools\nbased on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on\nmultimodal instruction-following data to acquire the ability to use tools,\ncovering visual understanding, generation, external knowledge retrieval, and\ncompositions. Empirical results show that LLaVA-Plus outperforms LLaVA in\nexisting capabilities and exhibits new ones. It is distinct in that the image\nquery is directly grounded and actively engaged throughout the entire human-AI\ninteraction sessions, significantly improving tool use performance and enabling\nnew scenarios.",
            "author": [
                "Shilong Liu",
                "Hao Cheng",
                "Haotian Liu",
                "Hao Zhang",
                "Feng Li",
                "Tianhe Ren",
                "Xueyan Zou",
                "Jianwei Yang",
                "Hang Su",
                "Jun Zhu",
                "Lei Zhang",
                "Jianfeng Gao",
                "Chunyuan Li"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05437v1",
                "http://arxiv.org/pdf/2311.05437v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.AI",
                "cs.CL",
                "cs.LG",
                "cs.MM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05434v1",
            "title": "Core determinants of quality criteria for mhealth for hypertension:\n  evidence from machine learning instruments",
            "updated": "2023-11-09T15:17:57Z",
            "published": "2023-11-09T15:17:57Z",
            "summary": "Uncontrolled hypertension is a global problem that needs to be addressed.\nDespite the many mHealth solutions in the market, the nonadherence relative to\nintended use jeopardizes treatment success. Although investigating user\nexperience is one of the most important mechanisms for understanding mHealth\ndiscontinuance, surprisingly, the core determinants of overall user experience\n(i.e., positive and negative) about mHealth apps for hypertension are unknown.\nTo address the mentioned gap in knowledge, this study adopts the computational\ngrounded theory methodological framework and employs advanced deep learning\nalgorithms to predict core quality criteria that affect overall user experience\nof hypertension apps published in the Apple App Store. This study contributes\nto theory and practice of designing evidence-based interventions for\nhypertension in the form of propositions and provide valuable managerial\nimplications and recommendations for manufacturers.",
            "author": [
                "Danielly de Paula",
                "Ariane Sasso",
                "Justus Coester",
                "Erwin Boettinger"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05434v1",
                "http://arxiv.org/pdf/2311.05434v1"
            ],
            "primary_category": "cs.HC",
            "category": [
                "cs.HC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05427v1",
            "title": "Residual Entropy as a Diagnostic and Stopping Metric for CLEAN",
            "updated": "2023-11-09T15:04:23Z",
            "published": "2023-11-09T15:04:23Z",
            "summary": "We propose the use of entropy, measured from the spatial and flux\ndistribution of pixels in the residual image, as a potential diagnostic and\nstopping metric for the CLEAN algorithm. Despite its broad success as the\nstandard deconvolution approach in radio interferometry, finding the optimum\nstopping point for the iterative CLEAN algorithm is still a challenge. We show\nthat the entropy of the residual image, measured during the final stages of\nCLEAN, can be computed without prior knowledge of the source structure or\nexpected noise levels, and that finding the point of maximum entropy as a\nmeasure of randomness in the residual image serves as a robust stopping\ncriterion. We also find that, when compared to the expected thermal noise in\nthe image, the maximum entropy of the residuals is a useful diagnostic that can\nreveal the presence of data editing, calibration, or deconvolution issues that\nmay limit the fidelity of the final CLEAN map.",
            "author": [
                "D. C. Homan",
                "J. S. Roth",
                "A. B. Pushkarev"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05427v1",
                "http://arxiv.org/pdf/2311.05427v1"
            ],
            "primary_category": "astro-ph.IM",
            "category": [
                "astro-ph.IM",
                "astro-ph.HE"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05421v1",
            "title": "Diffusion Based Causal Representation Learning",
            "updated": "2023-11-09T14:59:26Z",
            "published": "2023-11-09T14:59:26Z",
            "summary": "Causal reasoning can be considered a cornerstone of intelligent systems.\nHaving access to an underlying causal graph comes with the promise of\ncause-effect estimation and the identification of efficient and safe\ninterventions. However, learning causal representations remains a major\nchallenge, due to the complexity of many real-world systems. Previous works on\ncausal representation learning have mostly focused on Variational Auto-Encoders\n(VAE). These methods only provide representations from a point estimate, and\nthey are unsuitable to handle high dimensions. To overcome these problems, we\nproposed a new Diffusion-based Causal Representation Learning (DCRL) algorithm.\nThis algorithm uses diffusion-based representations for causal discovery. DCRL\noffers access to infinite dimensional latent codes, which encode different\nlevels of information in the latent code. In a first proof of principle, we\ninvestigate the use of DCRL for causal representation learning. We further\ndemonstrate experimentally that this approach performs comparably well in\nidentifying the causal structure and causal variables.",
            "author": [
                "Amir Mohammad Karimi Mamaghan",
                "Andrea Dittadi",
                "Stefan Bauer",
                "Karl Henrik Johansson",
                "Francesco Quinzan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05421v1",
                "http://arxiv.org/pdf/2311.05421v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05420v1",
            "title": "Counterfactually Fair Representation",
            "updated": "2023-11-09T14:58:53Z",
            "published": "2023-11-09T14:58:53Z",
            "summary": "The use of machine learning models in high-stake applications (e.g.,\nhealthcare, lending, college admission) has raised growing concerns due to\npotential biases against protected social groups. Various fairness notions and\nmethods have been proposed to mitigate such biases. In this work, we focus on\nCounterfactual Fairness (CF), a fairness notion that is dependent on an\nunderlying causal graph and first proposed by Kusner \\textit{et\nal.}~\\cite{kusner2017counterfactual}; it requires that the outcome an\nindividual perceives is the same in the real world as it would be in a\n\"counterfactual\" world, in which the individual belongs to another social\ngroup. Learning fair models satisfying CF can be challenging. It was shown in\n\\cite{kusner2017counterfactual} that a sufficient condition for satisfying CF\nis to \\textbf{not} use features that are descendants of sensitive attributes in\nthe causal graph. This implies a simple method that learns CF models only using\nnon-descendants of sensitive attributes while eliminating all descendants.\nAlthough several subsequent works proposed methods that use all features for\ntraining CF models, there is no theoretical guarantee that they can satisfy CF.\nIn contrast, this work proposes a new algorithm that trains models using all\nthe available features. We theoretically and empirically show that models\ntrained with this method can satisfy CF\\footnote{The code repository for this\nwork can be found in\n\\url{https://github.com/osu-srml/CF_Representation_Learning}}.",
            "author": [
                "Zhiqun Zuo",
                "Mohammad Mahdi Khalili",
                "Xueru Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05420v1",
                "http://arxiv.org/pdf/2311.05420v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "cs.CY"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05419v2",
            "title": "Mirror: A Universal Framework for Various Information Extraction Tasks",
            "updated": "2023-11-26T17:12:20Z",
            "published": "2023-11-09T14:58:46Z",
            "summary": "Sharing knowledge between information extraction tasks has always been a\nchallenge due to the diverse data formats and task variations. Meanwhile, this\ndivergence leads to information waste and increases difficulties in building\ncomplex applications in real scenarios. Recent studies often formulate IE tasks\nas a triplet extraction problem. However, such a paradigm does not support\nmulti-span and n-ary extraction, leading to weak versatility. To this end, we\nreorganize IE problems into unified multi-slot tuples and propose a universal\nframework for various IE tasks, namely Mirror. Specifically, we recast existing\nIE tasks as a multi-span cyclic graph extraction problem and devise a\nnon-autoregressive graph decoding algorithm to extract all spans in a single\nstep. It is worth noting that this graph structure is incredibly versatile, and\nit supports not only complex IE tasks, but also machine reading comprehension\nand classification tasks. We manually construct a corpus containing 57 datasets\nfor model pretraining, and conduct experiments on 30 datasets across 8\ndownstream tasks. The experimental results demonstrate that our model has\ndecent compatibility and outperforms or reaches competitive performance with\nSOTA systems under few-shot and zero-shot settings. The code, model weights,\nand pretraining corpus are available at https://github.com/Spico197/Mirror .",
            "author": [
                "Tong Zhu",
                "Junfei Ren",
                "Zijian Yu",
                "Mengsong Wu",
                "Guoliang Zhang",
                "Xiaoye Qu",
                "Wenliang Chen",
                "Zhefeng Wang",
                "Baoxing Huai",
                "Min Zhang"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05419v2",
                "http://arxiv.org/pdf/2311.05419v2"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05400v1",
            "title": "SIRE: scale-invariant, rotation-equivariant estimation of artery\n  orientations using graph neural networks",
            "updated": "2023-11-09T14:32:57Z",
            "published": "2023-11-09T14:32:57Z",
            "summary": "Blood vessel orientation as visualized in 3D medical images is an important\ndescriptor of its geometry that can be used for centerline extraction and\nsubsequent segmentation and visualization. Arteries appear at many scales and\nlevels of tortuosity, and determining their exact orientation is challenging.\nRecent works have used 3D convolutional neural networks (CNNs) for this\npurpose, but CNNs are sensitive to varying vessel sizes and orientations. We\npresent SIRE: a scale-invariant, rotation-equivariant estimator for local\nvessel orientation. SIRE is modular and can generalise due to symmetry\npreservation.\n  SIRE consists of a gauge equivariant mesh CNN (GEM-CNN) operating on multiple\nnested spherical meshes with different sizes in parallel. The features on each\nmesh are a projection of image intensities within the corresponding sphere.\nThese features are intrinsic to the sphere and, in combination with the\nGEM-CNN, lead to SO(3)-equivariance. Approximate scale invariance is achieved\nby weight sharing and use of a symmetric maximum function to combine\nmulti-scale predictions. Hence, SIRE can be trained with arbitrarily oriented\nvessels with varying radii to generalise to vessels with a wide range of\ncalibres and tortuosity.\n  We demonstrate the efficacy of SIRE using three datasets containing vessels\nof varying scales: the vascular model repository (VMR), the ASOCA coronary\nartery set, and a set of abdominal aortic aneurysms (AAAs). We embed SIRE in a\ncenterline tracker which accurately tracks AAAs, regardless of the data SIRE is\ntrained with. Moreover, SIRE can be used to track coronary arteries, even when\ntrained only with AAAs.\n  In conclusion, by incorporating SO(3) and scale symmetries, SIRE can\ndetermine the orientations of vessels outside of the training domain, forming a\nrobust and data-efficient solution to geometric analysis of blood vessels in 3D\nmedical images.",
            "author": [
                "Dieuwertje Alblas",
                "Julian Suk",
                "Christoph Brune",
                "Kak Khee Yeung",
                "Jelmer M. Wolterink"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05400v1",
                "http://arxiv.org/pdf/2311.05400v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV",
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05374v1",
            "title": "TencentLLMEval: A Hierarchical Evaluation of Real-World Capabilities for\n  Human-Aligned LLMs",
            "updated": "2023-11-09T13:58:59Z",
            "published": "2023-11-09T13:58:59Z",
            "summary": "Large language models (LLMs) have shown impressive capabilities across\nvarious natural language tasks. However, evaluating their alignment with human\npreferences remains a challenge. To this end, we propose a comprehensive human\nevaluation framework to assess LLMs' proficiency in following instructions on\ndiverse real-world tasks. We construct a hierarchical task tree encompassing 7\nmajor areas covering over 200 categories and over 800 tasks, which covers\ndiverse capabilities such as question answering, reasoning, multiturn dialogue,\nand text generation, to evaluate LLMs in a comprehensive and in-depth manner.\nWe also design detailed evaluation standards and processes to facilitate\nconsistent, unbiased judgments from human evaluators. A test set of over 3,000\ninstances is released, spanning different difficulty levels and knowledge\ndomains. Our work provides a standardized methodology to evaluate human\nalignment in LLMs for both English and Chinese. We also analyze the feasibility\nof automating parts of evaluation with a strong LLM (GPT-4). Our framework\nsupports a thorough assessment of LLMs as they are integrated into real-world\napplications. We have made publicly available the task tree, TencentLLMEval\ndataset, and evaluation methodology which have been demonstrated as effective\nin assessing the performance of Tencent Hunyuan LLMs. By doing so, we aim to\nfacilitate the benchmarking of advances in the development of safe and\nhuman-aligned LLMs.",
            "author": [
                "Shuyi Xie",
                "Wenlin Yao",
                "Yong Dai",
                "Shaobo Wang",
                "Donlin Zhou",
                "Lifeng Jin",
                "Xinhua Feng",
                "Pengzhi Wei",
                "Yujie Lin",
                "Zhichao Hu",
                "Dong Yu",
                "Zhengyou Zhang",
                "Jing Nie",
                "Yuhong Liu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05374v1",
                "http://arxiv.org/pdf/2311.05374v1"
            ],
            "primary_category": "cs.CL",
            "category": [
                "cs.CL",
                "cs.AI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05363v1",
            "title": "Beyond the training set: an intuitive method for detecting distribution\n  shift in model-based optimization",
            "updated": "2023-11-09T13:44:28Z",
            "published": "2023-11-09T13:44:28Z",
            "summary": "Model-based optimization (MBO) is increasingly applied to design problems in\nscience and engineering. A common scenario involves using a fixed training set\nto train models, with the goal of designing new samples that outperform those\npresent in the training data. A major challenge in this setting is distribution\nshift, where the distributions of training and design samples are different.\nWhile some shift is expected, as the goal is to create better designs, this\nchange can negatively affect model accuracy and subsequently, design quality.\nDespite the widespread nature of this problem, addressing it demands deep\ndomain knowledge and artful application. To tackle this issue, we propose a\nstraightforward method for design practitioners that detects distribution\nshifts. This method trains a binary classifier using knowledge of the unlabeled\ndesign distribution to separate the training data from the design data. The\nclassifier's logit scores are then used as a proxy measure of distribution\nshift. We validate our method in a real-world application by running offline\nMBO and evaluate the effect of distribution shift on design quality. We find\nthat the intensity of the shift in the design distribution varies based on the\nnumber of steps taken by the optimization algorithm, and our simple approach\ncan identify these shifts. This enables users to constrain their search to\nregions where the model's predictions are reliable, thereby increasing the\nquality of designs.",
            "author": [
                "Farhan Damani",
                "David H Brookes",
                "Theodore Sternlieb",
                "Cameron Webster",
                "Stephen Malina",
                "Rishi Jajoo",
                "Kathy Lin",
                "Sam Sinai"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05363v1",
                "http://arxiv.org/pdf/2311.05363v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG",
                "q-bio.QM"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05345v1",
            "title": "Fourier Transform-Based Post-Processing Drift Compensation and\n  Calibration Method for Scanning Probe Microscopy",
            "updated": "2023-11-09T13:14:08Z",
            "published": "2023-11-09T13:14:08Z",
            "summary": "Scanning probe microscopy (SPM) is ubiquitous in nanoscale science allowing\nthe observation of features in real space down to the angstrom resolution. The\nscanning nature of SPM, wherein a sharp tip rasters the surface during which a\nphysical setpoint is maintained via a control feedback loop, often implies that\nthe image is subject to drift effects, leading to distortion of the resulting\nimage. While there are \\emph{in-operando} methods to compensate for the drift,\ncorrecting the residual linear drift in obtained images is often neglected. In\nthis paper, we present a reciprocal space-based technique to compensate the\nlinear drift in atomically-resolved scanning probe microscopy images without\ndistinction of the fast and slow scanning directions; furthermore this method\ndoes not require the set of SPM images obtained for the different scanning\ndirections. Instead, the compensation is made possible by the a priori\nknowledge of the lattice parameters. The method can also be used to\ncharacterize and calibrate the SPM instrument.",
            "author": [
                "Maxime Le Ster",
                "S\u0142awomir Paw\u0142owski",
                "Iaroslav Lutsyk",
                "Pawe\u0142 Janusz Kowalczyk"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05345v1",
                "http://arxiv.org/pdf/2311.05345v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "cond-mat.mes-hall"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05337v1",
            "title": "Atom: Neural Traffic Compression with Spatio-Temporal Graph Neural\n  Networks",
            "updated": "2023-11-09T13:03:13Z",
            "published": "2023-11-09T13:03:13Z",
            "summary": "Storing network traffic data is key to efficient network management; however,\nit is becoming more challenging and costly due to the ever-increasing data\ntransmission rates, traffic volumes, and connected devices. In this paper, we\nexplore the use of neural architectures for network traffic compression.\nSpecifically, we consider a network scenario with multiple measurement points\nin a network topology. Such measurements can be interpreted as multiple time\nseries that exhibit spatial and temporal correlations induced by network\ntopology, routing, or user behavior. We present \\textit{Atom}, a neural traffic\ncompression method that leverages spatial and temporal correlations present in\nnetwork traffic. \\textit{Atom} implements a customized spatio-temporal graph\nneural network design that effectively exploits both types of correlations\nsimultaneously. The experimental results show that \\textit{Atom} can outperform\nGZIP's compression ratios by 50\\%-65\\% on three real-world networks.",
            "author": [
                "Paul Almasan",
                "Krzysztof Rusek",
                "Shihan Xiao",
                "Xiang Shi",
                "Xiangle Cheng",
                "Albert Cabellos-Aparicio",
                "Pere Barlet-Ros"
            ],
            "link": [
                "http://dx.doi.org/10.1145/3630049.3630170",
                "http://arxiv.org/abs/2311.05337v1",
                "http://arxiv.org/pdf/2311.05337v1"
            ],
            "primary_category": "cs.NI",
            "category": [
                "cs.NI"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05330v1",
            "title": "Applying a new category association estimator to sentiment analysis on\n  the Web",
            "updated": "2023-11-09T12:49:53Z",
            "published": "2023-11-09T12:49:53Z",
            "summary": "This paper introduces a novel Bayesian method for measuring the degree of\nassociation between categorical variables. The method is grounded in the formal\ndefinition of variable independence and was implemented using MCMC techniques.\nUnlike existing methods, this approach does not assume prior knowledge of the\ntotal number of occurrences for any category, making it particularly\nwell-suited for applications like sentiment analysis. We applied the method to\na dataset comprising 4,613 tweets written in Portuguese, each annotated for 30\npossibly overlapping emotional categories. Through this analysis, we identified\npairs of emotions that exhibit associations and mutually exclusive pairs.\nFurthermore, the method identifies hierarchical relations between categories, a\nfeature observed in our data, and was used to cluster emotions into basic level\ngroups.",
            "author": [
                "Henrique S. Xavier",
                "Diogo Cortiz",
                "Mateus Silvestrin",
                "Ana Lu\u00edsa Freitas",
                "Let\u00edcia Yumi Nakao Morello",
                "Fernanda Naomi Pantale\u00e3o",
                "Gabriel Gaudencio do R\u00eago"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05330v1",
                "http://arxiv.org/pdf/2311.05330v1"
            ],
            "primary_category": "stat.AP",
            "category": [
                "stat.AP",
                "cs.CY",
                "stat.ME"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05327v1",
            "title": "Independent domination in the graph defined by two consecutive levels of\n  the $n$-cube",
            "updated": "2023-11-09T12:46:23Z",
            "published": "2023-11-09T12:46:23Z",
            "summary": "Fix a positive integer $n$ and consider the bipartite graph whose vertices\nare the $3$-element subsets and the $2$-element subsets of\n$[n]=\\{1,2,\\dots,n\\}$, and there is an edge between $A$ and $B$ if $A\\subset\nB$. We prove that the domination number of this graph is\n$\\binom{n}{2}-\\lfloor\\frac{(n+1)^2}{8}\\rfloor$, we characterize the dominating\nsets of minimum size, and we observe that the minimum size dominating set can e\nchosen as an independent set. This is an exact version of an asymptotic result\nfrom [Balogh2021]. For the corresponding bipartite graph between the\n$(k+1)$-element subsets and the $k$-elements subsets of $[n]$ ($k\\geq 3$), we\nprovide a new construction for small independent dominating sets. This improves\non a construction from [Gerbner2012], where these independent dominating sets\nhave been studied under the name saturating flat antichains.",
            "author": [
                "Thomas Kalinowski",
                "Uwe Leck"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05327v1",
                "http://arxiv.org/pdf/2311.05327v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "05D05, 05C35"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05320v1",
            "title": "Dual-phase xenon time projection chambers for rare-event searches",
            "updated": "2023-11-09T12:40:48Z",
            "published": "2023-11-09T12:40:48Z",
            "summary": "In the past decade, dual-phase xenon time projection chambers (Xe-TPCs) have\nemerged as some of the most powerful detectors in the fields of astroparticle\nphysics and rare-event searches. Developed primarily towards the direct\ndetection of dark matter particles, experiments presently operating deep\nunderground have reached target masses at the multi-tonne scale, energy\nthresholds around 1\\,keV and radioactivity-induced background rates similar to\nthose from solar neutrinos. These unique properties, together with demonstrated\nstable operation over several years, allow for the exploration of new territory\nvia high-sensitivity searches for a plethora of ultra-rare interactions. These\ninclude searches for particle dark matter, for second order weak decays, and\nthe observation of astrophysical neutrinos. We first review some properties of\nxenon as a radiation detection medium and the operation principles of\ndual-phase Xe-TPCs together with their energy calibration and resolution. We\nthen discuss the status of currently running experiments and of proposed\nnext-generation projects, describing some of the technological challenges. We\nend by looking at their sensitivity to dark matter candidates, to second order\nweak decays and to solar and supernova neutrinos. Experiments based on\ndual-phase Xe-TPCs are difficult, and, like all good experiments, they are\nconstantly pushed to their limits. Together with many other endeavours in\nastroparticle physics and cosmology they will continue to push at the borders\nof the unknown, hopefully to reveal profound new knowledge about our cosmos.",
            "author": [
                "Laura Baudis"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05320v1",
                "http://arxiv.org/pdf/2311.05320v1"
            ],
            "primary_category": "physics.ins-det",
            "category": [
                "physics.ins-det",
                "astro-ph.IM",
                "hep-ex",
                "nucl-ex"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05312v1",
            "title": "Strongly clustered random graphs via triadic closure: an exactly\n  solvable model",
            "updated": "2023-11-09T12:18:35Z",
            "published": "2023-11-09T12:18:35Z",
            "summary": "Triadic closure, the formation of a connection between two nodes in a network\nsharing a common neighbor, is considered a fundamental mechanism determining\nthe clustered nature of many real-world topologies. In this work we define a\nstatic triadic closure (STC) model for clustered networks, whereby starting\nfrom an arbitrary fixed backbone network, each triad is closed independently\nwith a given probability. Assuming a locally treelike backbone we derive exact\nexpressions for the expected number of various small, loopy motifs (triangles,\nfour-loops, diamonds and four-cliques) as a function of moments of the backbone\ndegree distribution. In this way we determine how transitivity and its suitably\ndefined generalizations for higher-order motifs depend on the heterogeneity of\nthe original network, revealing the existence of transitions due to the\ninterplay between topologically inequivalent triads in the network.\nFurthermore, under reasonable assumptions for the moments of the backbone\nnetwork, we establish approximate relationships between motif densities, which\nwe test in a large dataset of real-world networks. We find a good agreement,\nindicating that STC is a realistic mechanism for the generation of clustered\nnetworks, while remaining simple enough to be amenable to analytical treatment.",
            "author": [
                "Lorenzo Cirigliano",
                "Claudio Castellano",
                "Gareth Baxter",
                "G\u00e1bor Tim\u00e1r"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05312v1",
                "http://arxiv.org/pdf/2311.05312v1"
            ],
            "primary_category": "physics.soc-ph",
            "category": [
                "physics.soc-ph",
                "cond-mat.dis-nn",
                "cond-mat.stat-mech"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05298v1",
            "title": "Improving Vision-and-Language Reasoning via Spatial Relations Modeling",
            "updated": "2023-11-09T11:54:55Z",
            "published": "2023-11-09T11:54:55Z",
            "summary": "Visual commonsense reasoning (VCR) is a challenging multi-modal task, which\nrequires high-level cognition and commonsense reasoning ability about the real\nworld. In recent years, large-scale pre-training approaches have been developed\nand promoted the state-of-the-art performance of VCR. However, the existing\napproaches almost employ the BERT-like objectives to learn multi-modal\nrepresentations. These objectives motivated from the text-domain are\ninsufficient for the excavation on the complex scenario of visual modality.\nMost importantly, the spatial distribution of the visual objects is basically\nneglected. To address the above issue, we propose to construct the spatial\nrelation graph based on the given visual scenario. Further, we design two\npre-training tasks named object position regression (OPR) and spatial relation\nclassification (SRC) to learn to reconstruct the spatial relation graph\nrespectively. Quantitative analysis suggests that the proposed method can guide\nthe representations to maintain more spatial context and facilitate the\nattention on the essential visual regions for reasoning. We achieve the\nstate-of-the-art results on VCR and two other vision-and-language reasoning\ntasks VQA, and NLVR.",
            "author": [
                "Cheng Yang",
                "Rui Xu",
                "Ye Guo",
                "Peixiang Huang",
                "Yiru Chen",
                "Wenkui Ding",
                "Zhongyuan Wang",
                "Hong Zhou"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05298v1",
                "http://arxiv.org/pdf/2311.05298v1"
            ],
            "primary_category": "cs.CV",
            "category": [
                "cs.CV"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.16162v1",
            "title": "Leveraging Artificial Intelligence Technology for Mapping Research to\n  Sustainable Development Goals: A Case Study",
            "updated": "2023-11-09T11:44:22Z",
            "published": "2023-11-09T11:44:22Z",
            "summary": "The number of publications related to the Sustainable Development Goals\n(SDGs) continues to grow. These publications cover a diverse spectrum of\nresearch, from humanities and social sciences to engineering and health. Given\nthe imperative of funding bodies to monitor outcomes and impacts, linking\npublications to relevant SDGs is critical but remains time-consuming and\ndifficult given the breadth and complexity of the SDGs. A publication may\nrelate to several goals (interconnection feature of goals), and therefore\nrequire multidisciplinary knowledge to tag accurately. Machine learning\napproaches are promising and have proven particularly valuable for tasks such\nas manual data labeling and text classification. In this study, we employed\nover 82,000 publications from an Australian university as a case study. We\nutilized a similarity measure to map these publications onto Sustainable\nDevelopment Goals (SDGs). Additionally, we leveraged the OpenAI GPT model to\nconduct the same task, facilitating a comparative analysis between the two\napproaches. Experimental results show that about 82.89% of the results obtained\nby the similarity measure overlap (at least one tag) with the outputs of the\nGPT model. The adopted model (similarity measure) can complement GPT model for\nSDG classification. Furthermore, deep learning methods, which include the\nsimilarity measure used here, are more accessible and trusted for dealing with\nsensitive data without the use of commercial AI services or the deployment of\nexpensive computing resources to operate large language models. Our study\ndemonstrates how a crafted combination of the two methods can achieve reliable\nresults for mapping research to the SDGs.",
            "author": [
                "Hui Yin",
                "Amir Aryani",
                "Gavin Lambert",
                "Marcus White",
                "Luis Salvador-Carulla",
                "Shazia Sadiq",
                "Elvira Sojli",
                "Jennifer Boddy",
                "Greg Murray",
                "Wing Wah Tham"
            ],
            "link": [
                "http://arxiv.org/abs/2311.16162v1",
                "http://arxiv.org/pdf/2311.16162v1"
            ],
            "primary_category": "cs.DL",
            "category": [
                "cs.DL",
                "cs.AI",
                "cs.CL",
                "I.2.7"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05288v1",
            "title": "Towards a Taxonomy of Large Language Model based Business Model\n  Transformations",
            "updated": "2023-11-09T11:32:37Z",
            "published": "2023-11-09T11:32:37Z",
            "summary": "Research on the role of Large Language Models (LLMs) in business models and\nservices is limited. Previous studies have utilized econometric models,\ntechnical showcases, and literature reviews. However, this research is\npioneering in its empirical examination of the influence of LLMs at the firm\nlevel. The study introduces a detailed taxonomy that can guide further research\non the criteria for successful LLM-based business model implementation and\ndeepen understanding of LLM-driven business transformations. Existing knowledge\non this subject is sparse and general. This research offers a more detailed\nbusiness model design framework based on LLM-driven transformations. This\ntaxonomy is not only beneficial for academic research but also has practical\nimplications. It can act as a strategic tool for businesses, offering insights\nand best practices. Businesses can lev-erage this taxonomy to make informed\ndecisions about LLM initiatives, ensuring that technology in-vestments align\nwith strategic goals.",
            "author": [
                "Jochen Wulf",
                "Juerg Meierhofer"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05288v1",
                "http://arxiv.org/pdf/2311.05288v1"
            ],
            "primary_category": "econ.GN",
            "category": [
                "econ.GN",
                "q-fin.EC"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05285v1",
            "title": "Group actions on multitrees and the $K$-theory of their crossed products",
            "updated": "2023-11-09T11:29:27Z",
            "published": "2023-11-09T11:29:27Z",
            "summary": "We study group actions on multitrees, which are directed graphs in which\nthere is at most one directed path between any two vertices. In our main result\nwe describe a six-term exact sequence in $K$-theory for the reduced crossed\nproduct $C_0(\\partial E)\\rtimes_r G$ induced from the action of a countable\ndiscrete group $G$ on a row-finite, finitely-aligned multitree $E$ with no\nsources. We provide formulas for the $K$-theory of $C_0(\\partial E) \\rtimes_r\nG$ in the case where $G$ acts freely on $E$, and in the case where all vertex\nstabilisers are infinite cyclic. We study the action $G\\curvearrowright\n\\partial E$ in a range of settings, and describe minimality, local\ncontractivity, topological freeness, and amenability in terms of properties of\nthe underlying data. In an application of our main theorem, we describe a\nsix-term exact sequence in $K$-theory for the crossed product induced from a\ngroup acting on the boundary of an undirected tree.",
            "author": [
                "Nathan Brownlowe",
                "Jack Spielberg",
                "Anne Thomas",
                "Victor Wu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05285v1",
                "http://arxiv.org/pdf/2311.05285v1"
            ],
            "primary_category": "math.OA",
            "category": [
                "math.OA",
                "math.GR",
                "math.KT",
                "46L80 (Primary), 20E08 (Secondary)"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05262v1",
            "title": "$K_2$-Hamiltonian Graphs: II",
            "updated": "2023-11-09T10:44:02Z",
            "published": "2023-11-09T10:44:02Z",
            "summary": "In this paper we use theoretical and computational tools to continue our\ninvestigation of $K_2$-hamiltonian graphs, i.e. graphs in which the removal of\nany pair of adjacent vertices yields a hamiltonian graph, and their interplay\nwith $K_1$-hamiltonian graphs, i.e. graphs in which every vertex-deleted\nsubgraph is hamiltonian. Perhaps surprisingly, there exist graphs that are both\n$K_1$- and $K_2$-hamiltonian, yet non-hamiltonian, e.g. the Petersen graph.\nGr\\\"unbaum conjectured that every planar $K_1$-hamiltonian graph must itself be\nhamiltonian; Thomassen disproved this conjecture. Here we show that even planar\ngraphs that are both $K_1$- and $K_2$-hamiltonian need not be hamiltonian, and\nthat the number of such graphs grows at least exponentially. Motivated by\nresults of Aldred, McKay, and Wormald, we determine for every integer $n$ that\nis not 14 or 17 whether there exists a $K_2$-hypohamiltonian, i.e.\nnon-hamiltonian and $K_2$-hamiltonian, graph of order~$n$, and characterise all\norders for which such cubic graphs and such snarks exist. We also describe the\nsmallest cubic planar graph which is $K_2$-hypohamiltonian, as well as the\nsmallest planar $K_2$-hypohamiltonian graph of girth $5$. We conclude with open\nproblems and by correcting two inaccuracies from the first article [Zamfirescu,\nSIAM J. Disc. Math. 35 (2021) 1706-1728].",
            "author": [
                "Jan Goedgebeur",
                "Jarne Renders",
                "G\u00e1bor Wiener",
                "Carol T. Zamfirescu"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05262v1",
                "http://arxiv.org/pdf/2311.05262v1"
            ],
            "primary_category": "math.CO",
            "category": [
                "math.CO",
                "cs.DM",
                "05C45, 05C38, 05C10, 05C85, 05C76"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05261v1",
            "title": "RAGLog: Log Anomaly Detection using Retrieval Augmented Generation",
            "updated": "2023-11-09T10:40:04Z",
            "published": "2023-11-09T10:40:04Z",
            "summary": "The ability to detect log anomalies from system logs is a vital activity\nneeded to ensure cyber resiliency of systems. It is applied for fault\nidentification or facilitate cyber investigation and digital forensics.\nHowever, as logs belonging to different systems and components differ\nsignificantly, the challenge to perform such analysis is humanly challenging\nfrom the volume, variety and velocity of logs. This is further complicated by\nthe lack or unavailability of anomalous log entries to develop trained machine\nlearning or artificial intelligence models for such purposes. In this research\nwork, we explore the use of a Retrieval Augmented Large Language Model that\nleverages a vector database to detect anomalies from logs. We used a Question\nand Answer configuration pipeline. To the best of our knowledge, our experiment\nwhich we called RAGLog is a novel one and the experimental results show much\npromise.",
            "author": [
                "Jonathan Pan",
                "Swee Liang Wong",
                "Yidi Yuan"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05261v1",
                "http://arxiv.org/pdf/2311.05261v1"
            ],
            "primary_category": "cs.CR",
            "category": [
                "cs.CR"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05256v1",
            "title": "Latent Task-Specific Graph Network Simulators",
            "updated": "2023-11-09T10:30:51Z",
            "published": "2023-11-09T10:30:51Z",
            "summary": "Simulating dynamic physical interactions is a critical challenge across\nmultiple scientific domains, with applications ranging from robotics to\nmaterial science. For mesh-based simulations, Graph Network Simulators (GNSs)\npose an efficient alternative to traditional physics-based simulators. Their\ninherent differentiability and speed make them particularly well-suited for\ninverse design problems. Yet, adapting to new tasks from limited available data\nis an important aspect for real-world applications that current methods\nstruggle with. We frame mesh-based simulation as a meta-learning problem and\nuse a recent Bayesian meta-learning method to improve GNSs adaptability to new\nscenarios by leveraging context data and handling uncertainties. Our approach,\nlatent task-specific graph network simulator, uses non-amortized task posterior\napproximations to sample latent descriptions of unknown system properties.\nAdditionally, we leverage movement primitives for efficient full trajectory\nprediction, effectively addressing the issue of accumulating errors encountered\nby previous auto-regressive methods. We validate the effectiveness of our\napproach through various experiments, performing on par with or better than\nestablished baseline methods. Movement primitives further allow us to\naccommodate various types of context data, as demonstrated through the\nutilization of point clouds during inference. By combining GNSs with\nmeta-learning, we bring them closer to real-world applicability, particularly\nin scenarios with smaller datasets.",
            "author": [
                "Philipp Dahlinger",
                "Niklas Freymuth",
                "Michael Volpp",
                "Tai Hoang",
                "Gerhard Neumann"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05256v1",
                "http://arxiv.org/pdf/2311.05256v1"
            ],
            "primary_category": "cs.LG",
            "category": [
                "cs.LG"
            ]
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@type": "ArxivArticle",
            "@id": "urn:research:http://arxiv.org/abs/2311.05243v2",
            "title": "A higher-order transformation approach to the formalization and analysis\n  of BPMN using graph transformation systems",
            "updated": "2023-11-10T10:53:35Z",
            "published": "2023-11-09T09:55:10Z",
            "summary": "The Business Process Modeling Notation (BPMN) is a widely used standard\nnotation for defining intra- and inter-organizational workflows. However, the\ninformal description of the BPMN execution semantics leads to different\ninterpretations of BPMN elements and difficulties in checking behavioral\nproperties. In this article, we propose a formalization of the execution\nsemantics of BPMN that, compared to existing approaches, covers more BPMN\nelements while also facilitating property checking. Our approach is based on a\nhigher-order transformation from BPMN models to graph transformation systems.\nTo show the capabilities of our approach, we implemented it as an open-source\nweb-based tool.",
            "author": [
                "Tim Kr\u00e4uter",
                "Adrian Rutle",
                "Harald K\u00f6nig",
                "Yngve Lamo"
            ],
            "link": [
                "http://arxiv.org/abs/2311.05243v2",
                "http://arxiv.org/pdf/2311.05243v2"
            ],
            "primary_category": "cs.SE",
            "category": [
                "cs.SE"
            ]
        }
    }
]