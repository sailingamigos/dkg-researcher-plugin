[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a35561e15d3270d63574905fca6b44b92ab5ace0",
            "@type": "ScholarlyArticle",
            "paperId": "a35561e15d3270d63574905fca6b44b92ab5ace0",
            "corpusId": 186689463,
            "url": "https://www.semanticscholar.org/paper/a35561e15d3270d63574905fca6b44b92ab5ace0",
            "title": "Lucas-Kanade 20 Years On: A Unifying Framework",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "29851256",
                "DBLP": "journals/ijcv/BakerM04",
                "DOI": "10.1023/B:VISI.0000011205.11775.fd",
                "CorpusId": 186689463
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 3246,
            "influentialCitationCount": 326,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.cmu.edu/afs/cs/academic/class/15385-s12/www/lec_slides/Baker%26Matthews.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2004-02-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "56"
            },
            "citationStyles": {
                "bibtex": "@Article{Baker2004LucasKanade2Y,\n author = {Simon Baker and Iain Matthews},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {221-255},\n title = {Lucas-Kanade 20 Years On: A Unifying Framework},\n volume = {56},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e36d8289d7474d407fbc2d47d249385f4c3da683",
            "@type": "ScholarlyArticle",
            "paperId": "e36d8289d7474d407fbc2d47d249385f4c3da683",
            "corpusId": 106678735,
            "url": "https://www.semanticscholar.org/paper/e36d8289d7474d407fbc2d47d249385f4c3da683",
            "title": "Robot Modeling and Control",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "1509235676",
                "DOI": "10.1108/ir.2006.33.5.403.1",
                "CorpusId": 106678735
            },
            "abstract": "Preface. 1. Introduction. 2. Rigid Motions and Homogeneous Transformations. 3. Forward and Inverse Kinematics. 4. Velocity Kinematics-The Jacobian. 5. Path and Trajectory Planning. 6. Independent Joint Control. 7. Dynamics. 8. Multivariable Control. 9. Force Control. 10. Geometric Nonlinear Control. 11. Computer Vision. 12. Vision-Based Control. Appendix A: Trigonometry. Appendix B: Linear Algebra. Appendix C: Dynamical Systems. Appendix D: Lyapunov Stability. Index.",
            "referenceCount": 68,
            "citationCount": 3267,
            "influentialCitationCount": 285,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.gbv.de/dms/ilmenau/toc/498926583.PDF",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2005-11-18",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Spong2005RobotMA,\n author = {M. Spong and S. Hutchinson and M. Vidyasagar},\n title = {Robot Modeling and Control},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d431f835c06afdea45dff6b24486bf301ebdef0",
            "@type": "ScholarlyArticle",
            "paperId": "6d431f835c06afdea45dff6b24486bf301ebdef0",
            "corpusId": 10175374,
            "url": "https://www.semanticscholar.org/paper/6d431f835c06afdea45dff6b24486bf301ebdef0",
            "title": "An Overview of Multi-Task Learning in Deep Neural Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Ruder17a",
                "ArXiv": "1706.05098",
                "MAG": "2624871570",
                "CorpusId": 10175374
            },
            "abstract": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
            "referenceCount": 58,
            "citationCount": 2312,
            "influentialCitationCount": 152,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-06-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.05098"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruder2017AnOO,\n author = {Sebastian Ruder},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Overview of Multi-Task Learning in Deep Neural Networks},\n volume = {abs/1706.05098},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "@type": "ScholarlyArticle",
            "paperId": "908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "corpusId": 1309931,
            "url": "https://www.semanticscholar.org/paper/908091b4a8757c3b2f7d9cfa2c4f616ee12c5157",
            "title": "SUN database: Large-scale scene recognition from abbey to zoo",
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2017814585",
                "DBLP": "conf/cvpr/XiaoHEOT10",
                "DOI": "10.1109/CVPR.2010.5539970",
                "CorpusId": 1309931
            },
            "abstract": "Scene categorization is a fundamental problem in computer vision. However, scene understanding research has been constrained by the limited scope of currently-used databases which do not capture the full variety of scene categories. Whereas standard databases for object categorization contain hundreds of different classes of objects, the largest available dataset of scene categories contains only 15 classes. In this paper we propose the extensive Scene UNderstanding (SUN) database that contains 899 categories and 130,519 images. We use 397 well-sampled categories to evaluate numerous state-of-the-art algorithms for scene recognition and establish new bounds of performance. We measure human scene classification performance on the SUN database and compare this with computational methods. Additionally, we study a finer-grained scene representation to detect scenes embedded inside of larger scenes.",
            "referenceCount": 33,
            "citationCount": 2936,
            "influentialCitationCount": 411,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/60690/1/Oliva_SUN%20database.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-13",
            "journal": {
                "name": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2010SUNDL,\n author = {Jianxiong Xiao and James Hays and Krista A. Ehinger and A. Oliva and A. Torralba},\n booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {3485-3492},\n title = {SUN database: Large-scale scene recognition from abbey to zoo},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:875c77b0daf65f2db77b48e784cb68fb312edea3",
            "@type": "ScholarlyArticle",
            "paperId": "875c77b0daf65f2db77b48e784cb68fb312edea3",
            "corpusId": 30176573,
            "url": "https://www.semanticscholar.org/paper/875c77b0daf65f2db77b48e784cb68fb312edea3",
            "title": "Sequential Monte Carlo Methods in Practice",
            "venue": "Statistics for Engineering and Information Science",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1483307070",
                "DBLP": "books/sp/DoucetFG01",
                "DOI": "10.1007/978-1-4757-3437-9",
                "CorpusId": 30176573
            },
            "abstract": null,
            "referenceCount": 324,
            "citationCount": 3582,
            "influentialCitationCount": 152,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Doucet2001SequentialMC,\n author = {A. Doucet and Nando de Freitas and N. Gordon},\n booktitle = {Statistics for Engineering and Information Science},\n pages = {1-582},\n title = {Sequential Monte Carlo Methods in Practice},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b1e1696564e5a3021ac3a501c9deeb6c0fbc637",
            "@type": "ScholarlyArticle",
            "paperId": "5b1e1696564e5a3021ac3a501c9deeb6c0fbc637",
            "corpusId": 8167136,
            "url": "https://www.semanticscholar.org/paper/5b1e1696564e5a3021ac3a501c9deeb6c0fbc637",
            "title": "Color indexing",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1991,
            "externalIds": {
                "MAG": "2614460925",
                "DBLP": "journals/ijcv/SwainB91",
                "DOI": "10.1007/BF00130487",
                "CorpusId": 8167136
            },
            "abstract": null,
            "referenceCount": 38,
            "citationCount": 5208,
            "influentialCitationCount": 225,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1991-11-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Swain1991ColorI,\n author = {M. Swain and D. Ballard},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {11-32},\n title = {Color indexing},\n volume = {7},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3dd70bc202db0ea2f2442589424dd85c37aab714",
            "@type": "ScholarlyArticle",
            "paperId": "3dd70bc202db0ea2f2442589424dd85c37aab714",
            "corpusId": 1354186,
            "url": "https://www.semanticscholar.org/paper/3dd70bc202db0ea2f2442589424dd85c37aab714",
            "title": "Bundle Adjustment - A Modern Synthesis",
            "venue": "Workshop on Vision Algorithms",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/iccvw/TriggsMHF99",
                "MAG": "2124313187",
                "DOI": "10.1007/3-540-44480-7_21",
                "CorpusId": 1354186
            },
            "abstract": null,
            "referenceCount": 122,
            "citationCount": 4251,
            "influentialCitationCount": 249,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.inria.fr/inria-00548290/file/Triggs-va99.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1999-09-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Triggs1999BundleA,\n author = {Bill Triggs and Philip F. McLauchlan and Richard I. Hartley and Andrew W. Fitzgibbon},\n booktitle = {Workshop on Vision Algorithms},\n pages = {298-372},\n title = {Bundle Adjustment - A Modern Synthesis},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4676d81d6a2477f6ea1de5723fc81e431fbaa96f",
            "@type": "ScholarlyArticle",
            "paperId": "4676d81d6a2477f6ea1de5723fc81e431fbaa96f",
            "corpusId": 1814423,
            "url": "https://www.semanticscholar.org/paper/4676d81d6a2477f6ea1de5723fc81e431fbaa96f",
            "title": "A tutorial on visual servo control",
            "venue": "IEEE Trans. Robotics Autom.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2167501464",
                "DBLP": "journals/trob/HutchinsonHC96",
                "DOI": "10.1109/70.538972",
                "CorpusId": 1814423
            },
            "abstract": "This article provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordinate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed in detail. Since any visual servo system must be capable of tracking image features in a sequence of images, we also include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control.",
            "referenceCount": 94,
            "citationCount": 3765,
            "influentialCitationCount": 236,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1996-12-01",
            "journal": {
                "name": "IEEE Trans. Robotics Autom.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Hutchinson1996ATO,\n author = {S. Hutchinson and Gregory Hager and Peter Corke},\n booktitle = {IEEE Trans. Robotics Autom.},\n journal = {IEEE Trans. Robotics Autom.},\n pages = {651-670},\n title = {A tutorial on visual servo control},\n volume = {12},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30c15f9be29524e72b9744f8dc14faf2a122d65f",
            "@type": "ScholarlyArticle",
            "paperId": "30c15f9be29524e72b9744f8dc14faf2a122d65f",
            "corpusId": 786967,
            "url": "https://www.semanticscholar.org/paper/30c15f9be29524e72b9744f8dc14faf2a122d65f",
            "title": "What energy functions can be minimized via graph cuts?",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2898704628",
                "DBLP": "journals/pami/KolmogorovZ04",
                "DOI": "10.1007/3-540-47977-5_5",
                "CorpusId": 786967,
                "PubMed": "15376891"
            },
            "abstract": null,
            "referenceCount": 50,
            "citationCount": 3637,
            "influentialCitationCount": 270,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ecommons.cornell.edu/bitstreams/41145b7f-ad3b-4489-a490-39e746e24d73/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Study",
                "JournalArticle"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "26"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolmogorov2002WhatEF,\n author = {V. Kolmogorov and R. Zabih},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {147-159},\n title = {What energy functions can be minimized via graph cuts?},\n volume = {26},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c495a9b835f7803746fce1f711dad0eeb411112",
            "@type": "ScholarlyArticle",
            "paperId": "9c495a9b835f7803746fce1f711dad0eeb411112",
            "corpusId": 13798326,
            "url": "https://www.semanticscholar.org/paper/9c495a9b835f7803746fce1f711dad0eeb411112",
            "title": "Transfer Feature Learning with Joint Distribution Adaptation",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/iccv/LongWDSY13",
                "MAG": "2096943734",
                "DOI": "10.1109/ICCV.2013.274",
                "CorpusId": 13798326
            },
            "abstract": "Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.",
            "referenceCount": 27,
            "citationCount": 1392,
            "influentialCitationCount": 337,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-01",
            "journal": {
                "name": "2013 IEEE International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Long2013TransferFL,\n author = {Mingsheng Long and Jianmin Wang and Guiguang Ding and Jiaguang Sun and Philip S. Yu},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2013 IEEE International Conference on Computer Vision},\n pages = {2200-2207},\n title = {Transfer Feature Learning with Joint Distribution Adaptation},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b942261c49553bba62c340b197cf6ef373fd5a4",
            "@type": "ScholarlyArticle",
            "paperId": "2b942261c49553bba62c340b197cf6ef373fd5a4",
            "corpusId": 608055,
            "url": "https://www.semanticscholar.org/paper/2b942261c49553bba62c340b197cf6ef373fd5a4",
            "title": "Supervised Descent Method and Its Applications to Face Alignment",
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2157285372",
                "DBLP": "conf/cvpr/XiongT13",
                "DOI": "10.1109/CVPR.2013.75",
                "CorpusId": 608055
            },
            "abstract": "Many computer vision problems (e.g., camera calibration, image alignment, structure from motion) are solved through a nonlinear optimization method. It is generally accepted that 2nd order descent methods are the most robust, fast and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, 2nd order descent methods have two main drawbacks: (1) The function might not be analytically differentiable and numerical approximations are impractical. (2) The Hessian might be large and not positive definite. To address these issues, this paper proposes a Supervised Descent Method (SDM) for minimizing a Non-linear Least Squares (NLS) function. During training, the SDM learns a sequence of descent directions that minimizes the mean of NLS functions sampled at different points. In testing, SDM minimizes the NLS objective using the learned descent directions without computing the Jacobian nor the Hessian. We illustrate the benefits of our approach in synthetic and real examples, and show how SDM achieves state-of-the-art performance in the problem of facial feature detection. The code is available at www.humansensing.cs. cmu.edu/intraface.",
            "referenceCount": 33,
            "citationCount": 2015,
            "influentialCitationCount": 374,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Supervised_Descent_Method_and_Its_Applications_to_Face_Alignment/6561023/1/files/12043331.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-23",
            "journal": {
                "name": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiong2013SupervisedDM,\n author = {Xuehan Xiong and F. D. L. Torre},\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {532-539},\n title = {Supervised Descent Method and Its Applications to Face Alignment},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:529403ab43b381b942b67751862b614cbd94341b",
            "@type": "ScholarlyArticle",
            "paperId": "529403ab43b381b942b67751862b614cbd94341b",
            "corpusId": 1726588,
            "url": "https://www.semanticscholar.org/paper/529403ab43b381b942b67751862b614cbd94341b",
            "title": "From learning models of natural image patches to whole image restoration",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2172275395",
                "DBLP": "conf/iccv/ZoranW11",
                "DOI": "10.1109/ICCV.2011.6126278",
                "CorpusId": 1726588
            },
            "abstract": "Learning good image priors is of utmost importance for the study of vision, computer vision and image processing applications. Learning priors and optimizing over whole images can lead to tremendous computational challenges. In contrast, when we work with small image patches, it is possible to learn priors and perform patch restoration very efficiently. This raises three questions - do priors that give high likelihood to the data also lead to good performance in restoration? Can we use such patch based priors to restore a full image? Can we learn better patch priors? In this work we answer these questions. We compare the likelihood of several patch models and show that priors that give high likelihood to data perform better in patch restoration. Motivated by this result, we propose a generic framework which allows for whole image restoration using any patch based prior for which a MAP (or approximate MAP) estimate can be calculated. We show how to derive an appropriate cost function, how to optimize it and how to use it to restore whole images. Finally, we present a generic, surprisingly simple Gaussian Mixture prior, learned from a set of natural images. When used with the proposed framework, this Gaussian Mixture Model outperforms all other generic prior methods for image denoising, deblurring and inpainting.",
            "referenceCount": 78,
            "citationCount": 1415,
            "influentialCitationCount": 288,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.huji.ac.il/%7Edaniez/EPLLICCVCameraReady.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-06",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zoran2011FromLM,\n author = {Daniel Zoran and Yair Weiss},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {479-486},\n title = {From learning models of natural image patches to whole image restoration},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05e9e85b5137016c93d042170e82f77bb551a108",
            "@type": "ScholarlyArticle",
            "paperId": "05e9e85b5137016c93d042170e82f77bb551a108",
            "corpusId": 1949934,
            "url": "https://www.semanticscholar.org/paper/05e9e85b5137016c93d042170e82f77bb551a108",
            "title": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/PerazziPMGGS16",
                "MAG": "2470139095",
                "DOI": "10.1109/CVPR.2016.85",
                "CorpusId": 1949934
            },
            "abstract": "Over the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many fields. At the same time, legacy datasets may impend the evolution of a field due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of fifty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.",
            "referenceCount": 54,
            "citationCount": 1497,
            "influentialCitationCount": 488,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-01",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Perazzi2016ABD,\n author = {Federico Perazzi and J. Pont-Tuset and B. McWilliams and L. Gool and M. Gross and A. Sorkine-Hornung},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {724-732},\n title = {A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:65b16da51891a6b98140d425804c8a0fd0299219",
            "@type": "ScholarlyArticle",
            "paperId": "65b16da51891a6b98140d425804c8a0fd0299219",
            "corpusId": 8671030,
            "url": "https://www.semanticscholar.org/paper/65b16da51891a6b98140d425804c8a0fd0299219",
            "title": "Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2560533888",
                "DBLP": "conf/cvpr/NahKL17",
                "ArXiv": "1612.02177",
                "DOI": "10.1109/CVPR.2017.35",
                "CorpusId": 8671030
            },
            "abstract": "Non-uniform blind deblurring for general dynamic scenes is a challenging computer vision problem as blurs arise not only from multiple object motions but also from camera shake, scene depth variation. To remove these complicated motion blurs, conventional energy optimization based methods rely on simple assumptions such that blur kernel is partially uniform or locally linear. Moreover, recent machine learning based methods also depend on synthetic blur datasets generated under these assumptions. This makes conventional deblurring methods fail to remove blurs where blur kernel is difficult to approximate or parameterize (e.g. object motion boundaries). In this work, we propose a multi-scale convolutional neural network that restores sharp images in an end-to-end manner where blur is caused by various sources. Together, we present multi-scale loss function that mimics conventional coarse-to-fine approaches. Furthermore, we propose a new large-scale dataset that provides pairs of realistic blurry image and the corresponding ground truth sharp image that are obtained by a high-speed camera. With the proposed model trained on this dataset, we demonstrate empirically that our method achieves the state-of-the-art performance in dynamic scene deblurring not only qualitatively, but also quantitatively.",
            "referenceCount": 34,
            "citationCount": 1440,
            "influentialCitationCount": 418,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1612.02177",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-07",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nah2016DeepMC,\n author = {Seungjun Nah and Tae Hyun Kim and Kyoung Mu Lee},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {257-265},\n title = {Deep Multi-scale Convolutional Neural Network for Dynamic Scene Deblurring},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24d1afc81644877f6fc34a5a15d7a41e03a4e522",
            "@type": "ScholarlyArticle",
            "paperId": "24d1afc81644877f6fc34a5a15d7a41e03a4e522",
            "corpusId": 206849534,
            "url": "https://www.semanticscholar.org/paper/24d1afc81644877f6fc34a5a15d7a41e03a4e522",
            "title": "G2o: A general framework for graph optimization",
            "venue": "IEEE International Conference on Robotics and Automation",
            "publicationVenue": {
                "id": "urn:research:3f2626a8-9d78-42ca-9e0d-4b853b59cdcc",
                "name": "IEEE International Conference on Robotics and Automation",
                "alternate_names": [
                    "International Conference on Robotics and Automation",
                    "Int Conf Robot Autom",
                    "ICRA",
                    "IEEE Int Conf Robot Autom"
                ],
                "issn": "2152-4092",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000639"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2153054365",
                "DBLP": "conf/icra/KummerleGSKB11",
                "DOI": "10.1109/ICRA.2011.5979949",
                "CorpusId": 206849534
            },
            "abstract": "Many popular problems in robotics and computer vision including various types of simultaneous localization and mapping (SLAM) or bundle adjustment (BA) can be phrased as least squares optimization of an error function that can be represented by a graph. This paper describes the general structure of such problems and presents g2o, an open-source C++ framework for optimizing graph-based nonlinear error functions. Our system has been designed to be easily extensible to a wide range of problems and a new problem typically can be specified in a few lines of code. The current implementation provides solutions to several variants of SLAM and BA. We provide evaluations on a wide range of real-world and simulated datasets. The results demonstrate that while being general g2o offers a performance comparable to implementations of state-of-the-art approaches for the specific problems.",
            "referenceCount": 30,
            "citationCount": 2087,
            "influentialCitationCount": 164,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-05-09",
            "journal": {
                "name": "2011 IEEE International Conference on Robotics and Automation",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{K\u00fcmmerle2011G2oAG,\n author = {R. K\u00fcmmerle and G. Grisetti and H. Strasdat and K. Konolige and Wolfram Burgard},\n booktitle = {IEEE International Conference on Robotics and Automation},\n journal = {2011 IEEE International Conference on Robotics and Automation},\n pages = {3607-3613},\n title = {G2o: A general framework for graph optimization},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d770060812fb646b3846a7d398a3066145b5e3c8",
            "@type": "ScholarlyArticle",
            "paperId": "d770060812fb646b3846a7d398a3066145b5e3c8",
            "corpusId": 11536917,
            "url": "https://www.semanticscholar.org/paper/d770060812fb646b3846a7d398a3066145b5e3c8",
            "title": "Do Deep Nets Really Need to be Deep?",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2134797427",
                "ArXiv": "1312.6184",
                "DBLP": "journals/corr/BaC13",
                "CorpusId": 11536917
            },
            "abstract": "Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.",
            "referenceCount": 23,
            "citationCount": 1902,
            "influentialCitationCount": 131,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ba2013DoDN,\n author = {Jimmy Ba and R. Caruana},\n booktitle = {Neural Information Processing Systems},\n pages = {2654-2662},\n title = {Do Deep Nets Really Need to be Deep?},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b771c57b8fc242ba589d449e04e982e0577b1c54",
            "@type": "ScholarlyArticle",
            "paperId": "b771c57b8fc242ba589d449e04e982e0577b1c54",
            "corpusId": 208310312,
            "url": "https://www.semanticscholar.org/paper/b771c57b8fc242ba589d449e04e982e0577b1c54",
            "title": "CSPNet: A New Backbone that can Enhance Learning Capability of CNN",
            "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/cvpr/WangLWCHY20",
                "MAG": "3042011474",
                "ArXiv": "1911.11929",
                "DOI": "10.1109/CVPRW50498.2020.00203",
                "CorpusId": 208310312
            },
            "abstract": "Neural networks have enabled state-of-the-art approaches to achieve incredible results on computer vision tasks such as object detection. However, such success greatly relies on costly computation resources, which hinders people with cheap devices from appreciating the advanced technology. In this paper, we propose Cross Stage Partial Network (CSPNet) to mitigate the problem that previous works require heavy inference computations from the network architecture perspective. We attribute the problem to the duplicate gradient information within network optimization. The proposed networks respect the variability of the gradients by integrating feature maps from the beginning and the end of a network stage, which, in our experiments, reduces computations by 20% with equivalent or even superior accuracy on the ImageNet dataset, and significantly outperforms state-of-the-art approaches in terms of AP50 on the MS COCO object detection dataset. The CSPNet is easy to implement and general enough to cope with architectures based on ResNet, ResNeXt, and DenseNet.",
            "referenceCount": 49,
            "citationCount": 1847,
            "influentialCitationCount": 194,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.11929",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-11-27",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019CSPNetAN,\n author = {Chien-Yao Wang and H. Liao and I-Hau Yeh and Yueh-Hua Wu and Ping-Yang Chen and J. Hsieh},\n booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {1571-1580},\n title = {CSPNet: A New Backbone that can Enhance Learning Capability of CNN},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35d81066cb1369acf4b6c5117fcbb862be2af350",
            "@type": "ScholarlyArticle",
            "paperId": "35d81066cb1369acf4b6c5117fcbb862be2af350",
            "corpusId": 7317448,
            "url": "https://www.semanticscholar.org/paper/35d81066cb1369acf4b6c5117fcbb862be2af350",
            "title": "Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration",
            "venue": "International Conference on Computer Vision Theory and Applications",
            "publicationVenue": {
                "id": "urn:research:1940eb2a-ba0e-4c54-a315-a1b1afdeb2bc",
                "name": "International Conference on Computer Vision Theory and Applications",
                "alternate_names": [
                    "VISAPP",
                    "Int Conf Comput Vis Theory Appl"
                ],
                "issn": null,
                "url": "http://visapp.visigrapp.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/visapp/MujaL09",
                "MAG": "1627400044",
                "DOI": "10.5220/0001787803310340",
                "CorpusId": 7317448
            },
            "abstract": "For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, \u201cWhat is the fastest approximate nearest-neighbor algorithm for my data?\u201d Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection.",
            "referenceCount": 18,
            "citationCount": 3052,
            "influentialCitationCount": 236,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.ubc.ca/~lowe/papers/09muja.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Muja2009FastAN,\n author = {Marius Muja and D. Lowe},\n booktitle = {International Conference on Computer Vision Theory and Applications},\n pages = {331-340},\n title = {Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52d7eb0fbc3522434c13cc247549f74bb9609c5d",
            "@type": "ScholarlyArticle",
            "paperId": "52d7eb0fbc3522434c13cc247549f74bb9609c5d",
            "corpusId": 12090268,
            "url": "https://www.semanticscholar.org/paper/52d7eb0fbc3522434c13cc247549f74bb9609c5d",
            "title": "WIDER FACE: A Face Detection Benchmark",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/YangLLT16",
                "ArXiv": "1511.06523",
                "MAG": "2176613063",
                "DOI": "10.1109/CVPR.2016.596",
                "CorpusId": 12090268
            },
            "abstract": "Face detection is one of the most studied topics in the computer vision community. Much of the progresses have been made by the availability of face detection benchmark datasets. We show that there is a gap between current face detection performance and the real world requirements. To facilitate future face detection research, we introduce the WIDER FACE dataset1, which is 10 times larger than existing datasets. The dataset contains rich annotations, including occlusions, poses, event categories, and face bounding boxes. Faces in the proposed dataset are extremely challenging due to large variations in scale, pose and occlusion, as shown in Fig. 1. Furthermore, we show that WIDER FACE dataset is an effective training source for face detection. We benchmark several representative detection systems, providing an overview of state-of-the-art performance and propose a solution to deal with large scale variation. Finally, we discuss common failure cases that worth to be further investigated.",
            "referenceCount": 40,
            "citationCount": 1325,
            "influentialCitationCount": 296,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1511.06523",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2015-11-20",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2015WIDERFA,\n author = {Shuo Yang and Ping Luo and Chen Change Loy and Xiaoou Tang},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5525-5533},\n title = {WIDER FACE: A Face Detection Benchmark},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
            "@type": "ScholarlyArticle",
            "paperId": "0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
            "corpusId": 1710722,
            "url": "https://www.semanticscholar.org/paper/0a28efacb92d16e6e0dd4d87b5aca91b28be8853",
            "title": "ActivityNet: A large-scale video benchmark for human activity understanding",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1927052826",
                "DBLP": "conf/cvpr/HeilbronEGN15",
                "DOI": "10.1109/CVPR.2015.7298698",
                "CorpusId": 1710722
            },
            "abstract": "In spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper we introduce ActivityNet, a new large-scale video benchmark for human activity understanding. Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: untrimmed video classification, trimmed activity classification and activity detection.",
            "referenceCount": 44,
            "citationCount": 1914,
            "influentialCitationCount": 314,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kaust.edu.sa/bitstream/10754/556141/1/ActivityNet_CVPR2015.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Heilbron2015ActivityNetAL,\n author = {Fabian Caba Heilbron and Victor Escorcia and Bernard Ghanem and Juan Carlos Niebles},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {961-970},\n title = {ActivityNet: A large-scale video benchmark for human activity understanding},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4603cb8e05258bb0572ae912ad20903b8f99f4b1",
            "@type": "ScholarlyArticle",
            "paperId": "4603cb8e05258bb0572ae912ad20903b8f99f4b1",
            "corpusId": 2908606,
            "url": "https://www.semanticscholar.org/paper/4603cb8e05258bb0572ae912ad20903b8f99f4b1",
            "title": "MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/eccv/GuoZHHG16",
                "ArXiv": "1607.08221",
                "MAG": "2952419167",
                "DOI": "10.1007/978-3-319-46487-9_6",
                "CorpusId": 2908606
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 1631,
            "influentialCitationCount": 298,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46487-9_6.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-07-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2016MSCeleb1MAD,\n author = {Yandong Guo and Lei Zhang and Yuxiao Hu and Xiaodong He and Jianfeng Gao},\n booktitle = {European Conference on Computer Vision},\n pages = {87-102},\n title = {MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c01e8108b4ca599eb2b1e6f26b2ecf1c42e323a9",
            "@type": "ScholarlyArticle",
            "paperId": "c01e8108b4ca599eb2b1e6f26b2ecf1c42e323a9",
            "corpusId": 215415900,
            "url": "https://www.semanticscholar.org/paper/c01e8108b4ca599eb2b1e6f26b2ecf1c42e323a9",
            "title": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1805.04687",
                "DBLP": "conf/cvpr/YuCWXCLMD20",
                "MAG": "3016101116",
                "DOI": "10.1109/cvpr42600.2020.00271",
                "CorpusId": 215415900
            },
            "abstract": "Datasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.",
            "referenceCount": 47,
            "citationCount": 1232,
            "influentialCitationCount": 245,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1805.04687",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-12",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2018BDD100KAD,\n author = {F. Yu and Haofeng Chen and Xin Wang and Wenqi Xian and Yingying Chen and Fangchen Liu and Vashisht Madhavan and Trevor Darrell},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2633-2642},\n title = {BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "@type": "ScholarlyArticle",
            "paperId": "759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "corpusId": 12035411,
            "url": "https://www.semanticscholar.org/paper/759d9a6c9206c366a8d94a06f4eb05659c2bb7f2",
            "title": "Toward Open Set Recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/pami/ScheirerRSB13",
                "MAG": "2119880843",
                "DOI": "10.1109/TPAMI.2012.256",
                "CorpusId": 12035411,
                "PubMed": "23682001"
            },
            "abstract": "To date, almost all experimental evaluations of machine learning-based recognition algorithms in computer vision have taken the form of \u201cclosed set\u201d recognition, whereby all testing classes are known at training time. A more realistic scenario for vision applications is \u201copen set\u201d recognition, where incomplete knowledge of the world is present at training time, and unknown classes can be submitted to an algorithm during testing. This paper explores the nature of open set recognition and formalizes its definition as a constrained minimization problem. The open set recognition problem is not well addressed by existing algorithms because it requires strong generalization. As a step toward a solution, we introduce a novel \u201c1-vs-set machine,\u201d which sculpts a decision space from the marginal distances of a 1-class or binary SVM with a linear kernel. This methodology applies to several different applications in computer vision where open set recognition is a challenging problem, including object recognition and face verification. We consider both in this work, with large scale cross-dataset experiments performed over the Caltech 256 and ImageNet sets, as well as face matching experiments performed over the Labeled Faces in the Wild set. The experiments highlight the effectiveness of machines adapted for open set evaluation compared to existing 1-class and binary SVMs for the same tasks.",
            "referenceCount": 55,
            "citationCount": 961,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-07-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Scheirer2013TowardOS,\n author = {W. Scheirer and A. Rocha and Archana Sapkota and T. Boult},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1757-1772},\n title = {Toward Open Set Recognition},\n volume = {35},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dce916351ef589afa7a63452648dd8acba931e92",
            "@type": "ScholarlyArticle",
            "paperId": "dce916351ef589afa7a63452648dd8acba931e92",
            "corpusId": 4853375,
            "url": "https://www.semanticscholar.org/paper/dce916351ef589afa7a63452648dd8acba931e92",
            "title": "Panoptic Segmentation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/cvpr/KirillovHGRD19",
                "MAG": "2999219213",
                "ArXiv": "1801.00868",
                "DOI": "10.1109/CVPR.2019.00963",
                "CorpusId": 4853375
            },
            "abstract": "We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified view of image segmentation. For more analysis and up-to-date results, please check the arXiv version of the paper: {\\small\\url{https://arxiv.org/abs/1801.00868}}.",
            "referenceCount": 54,
            "citationCount": 1007,
            "influentialCitationCount": 131,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-03",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kirillov2018PanopticS,\n author = {Alexander Kirillov and Kaiming He and Ross B. Girshick and C. Rother and Piotr Doll\u00e1r},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {9396-9405},\n title = {Panoptic Segmentation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b7d120c0e801ef318bc9c607a0789f175637c7f",
            "@type": "ScholarlyArticle",
            "paperId": "3b7d120c0e801ef318bc9c607a0789f175637c7f",
            "corpusId": 206652126,
            "url": "https://www.semanticscholar.org/paper/3b7d120c0e801ef318bc9c607a0789f175637c7f",
            "title": "OpenFace 2.0: Facial Behavior Analysis Toolkit",
            "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
            "publicationVenue": {
                "id": "urn:research:b0c05768-6345-45d7-b541-235edf6ead54",
                "name": "IEEE International Conference on Automatic Face & Gesture Recognition",
                "alternate_names": [
                    "IEEE Int Conf Autom Face Gesture Recognit",
                    "FG",
                    "IEEE International Conference on Automatic Face and Gesture Recognition",
                    "IEEE Int Conf Autom Face  Gesture Recognit",
                    "FGR",
                    "Form Gramm",
                    "Formal Grammar"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1029"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/fgr/BaltrusaitisZLM18",
                "MAG": "2807126412",
                "DOI": "10.1109/FG.2018.00019",
                "CorpusId": 206652126
            },
            "abstract": "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes.",
            "referenceCount": 87,
            "citationCount": 1124,
            "influentialCitationCount": 204,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-15",
            "journal": {
                "name": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baltru\u0161aitis2018OpenFace2F,\n author = {T. Baltru\u0161aitis and Amir Zadeh and Y. Lim and Louis-Philippe Morency},\n booktitle = {IEEE International Conference on Automatic Face & Gesture Recognition},\n journal = {2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)},\n pages = {59-66},\n title = {OpenFace 2.0: Facial Behavior Analysis Toolkit},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1365b4a286e607a4902ef11c84a1f309719d946c",
            "@type": "ScholarlyArticle",
            "paperId": "1365b4a286e607a4902ef11c84a1f309719d946c",
            "corpusId": 54216961,
            "url": "https://www.semanticscholar.org/paper/1365b4a286e607a4902ef11c84a1f309719d946c",
            "title": "ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1811.12833",
                "DBLP": "journals/corr/abs-1811-12833",
                "MAG": "2955959307",
                "DOI": "10.1109/CVPR.2019.00262",
                "CorpusId": 54216961
            },
            "abstract": "Semantic segmentation is a key problem for many computer vision tasks. While approaches based on convolutional neural networks constantly break new records on different benchmarks, generalizing well to diverse testing environments remains a major challenge. In numerous real-world applications, there is indeed a large gap between data distributions in train and test domains, which results in severe performance loss at run-time. In this work, we address the task of unsupervised domain adaptation in semantic segmentation with losses based on the entropy of the pixel-wise predictions. To this end, we propose two novel, complementary methods using (i) entropy loss and (ii) adversarial loss respectively. We demonstrate state-of-the-art performance in semantic segmentation on two challenging \u201csynthetic-2-real\u201d set-ups and show that the approach can also be used for detection.",
            "referenceCount": 52,
            "citationCount": 935,
            "influentialCitationCount": 204,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1811.12833",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-11-30",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vu2018ADVENTAE,\n author = {Tuan-Hung Vu and Himalaya Jain and Max Bucher and M. Cord and P. P\u00e9rez},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2512-2521},\n title = {ADVENT: Adversarial Entropy Minimization for Domain Adaptation in Semantic Segmentation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8337441971f941716a9e525a67f37088eb01fd13",
            "@type": "ScholarlyArticle",
            "paperId": "8337441971f941716a9e525a67f37088eb01fd13",
            "corpusId": 21435690,
            "url": "https://www.semanticscholar.org/paper/8337441971f941716a9e525a67f37088eb01fd13",
            "title": "Matterport3D: Learning from RGB-D Data in Indoor Environments",
            "venue": "International Conference on 3D Vision",
            "publicationVenue": {
                "id": "urn:research:4b02e809-1c26-4203-b9ba-311a418f664b",
                "name": "International Conference on 3D Vision",
                "alternate_names": [
                    "Int Conf 3D Vis",
                    "3DV"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964339842",
                "ArXiv": "1709.06158",
                "DBLP": "conf/3dim/ChangDFHNSSZZ17",
                "DOI": "10.1109/3DV.2017.00081",
                "CorpusId": 21435690
            },
            "abstract": "Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.",
            "referenceCount": 50,
            "citationCount": 1293,
            "influentialCitationCount": 291,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1709.06158",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-18",
            "journal": {
                "name": "2017 International Conference on 3D Vision (3DV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chang2017Matterport3DLF,\n author = {Angel X. Chang and Angela Dai and T. Funkhouser and Maciej Halber and M. Nie\u00dfner and M. Savva and Shuran Song and Andy Zeng and Yinda Zhang},\n booktitle = {International Conference on 3D Vision},\n journal = {2017 International Conference on 3D Vision (3DV)},\n pages = {667-676},\n title = {Matterport3D: Learning from RGB-D Data in Indoor Environments},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ecf9e6428edd33986e8c6a143870e57142fd5a1",
            "@type": "ScholarlyArticle",
            "paperId": "8ecf9e6428edd33986e8c6a143870e57142fd5a1",
            "corpusId": 32378562,
            "url": "https://www.semanticscholar.org/paper/8ecf9e6428edd33986e8c6a143870e57142fd5a1",
            "title": "DOTA: A Large-Scale Dataset for Object Detection in Aerial Images",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1711.10398",
                "MAG": "2769221444",
                "DBLP": "journals/corr/abs-1711-10398",
                "DOI": "10.1109/CVPR.2018.00418",
                "CorpusId": 32378562
            },
            "abstract": "Object detection is an important and challenging problem in computer vision. Although the past decade has witnessed major advances in object detection in natural scenes, such successes have been slow to aerial imagery, not only because of the huge variation in the scale, orientation and shape of the object instances on the earth's surface, but also due to the scarcity of well-annotated datasets of objects in aerial scenes. To advance object detection research in Earth Vision, also known as Earth Observation and Remote Sensing, we introduce a large-scale Dataset for Object deTection in Aerial images (DOTA). To this end, we collect 2806 aerial images from different sensors and platforms. Each image is of the size about 4000 \u00c3\u2014 4000 pixels and contains objects exhibiting a wide variety of scales, orientations, and shapes. These DOTA images are then annotated by experts in aerial image interpretation using 15 common object categories. The fully annotated DOTA images contains 188, 282 instances, each of which is labeled by an arbitrary (8 d.o.f.) quadrilateral. To build a baseline for object detection in Earth Vision, we evaluate state-of-the-art object detection algorithms on DOTA. Experiments demonstrate that DOTA well represents real Earth Vision applications and are quite challenging.",
            "referenceCount": 42,
            "citationCount": 1462,
            "influentialCitationCount": 389,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://elib.dlr.de/123453/1/CVPR2018_DOTA.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-28",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xia2017DOTAAL,\n author = {Gui-Song Xia and X. Bai and Jian Ding and Zhen Zhu and Serge J. Belongie and Jiebo Luo and M. Datcu and M. Pelillo and Liangpei Zhang},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {3974-3983},\n title = {DOTA: A Large-Scale Dataset for Object Detection in Aerial Images},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a62bdda9ae6f86fc06d7edf5d3b429eda3a6640e",
            "@type": "ScholarlyArticle",
            "paperId": "a62bdda9ae6f86fc06d7edf5d3b429eda3a6640e",
            "corpusId": 4918026,
            "url": "https://www.semanticscholar.org/paper/a62bdda9ae6f86fc06d7edf5d3b429eda3a6640e",
            "title": "SuperPoint: Self-Supervised Interest Point Detection and Description",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/cvpr/DeToneMR18",
                "ArXiv": "1712.07629",
                "MAG": "2775929773",
                "DOI": "10.1109/CVPRW.2018.00060",
                "CorpusId": 4918026
            },
            "abstract": "This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g., synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches when compared to LIFT, SIFT and ORB.",
            "referenceCount": 33,
            "citationCount": 1480,
            "influentialCitationCount": 407,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1712.07629",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-20",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{DeTone2017SuperPointSI,\n author = {Daniel DeTone and Tomasz Malisiewicz and Andrew Rabinovich},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {337-33712},\n title = {SuperPoint: Self-Supervised Interest Point Detection and Description},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:45557cc70cd6989ab6b03e5aeb787e34299099f7",
            "@type": "ScholarlyArticle",
            "paperId": "45557cc70cd6989ab6b03e5aeb787e34299099f7",
            "corpusId": 196831327,
            "url": "https://www.semanticscholar.org/paper/45557cc70cd6989ab6b03e5aeb787e34299099f7",
            "title": "Natural Adversarial Examples",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/cvpr/HendrycksZBSS21",
                "MAG": "2961301154",
                "ArXiv": "1907.07174",
                "DOI": "10.1109/CVPR46437.2021.01501",
                "CorpusId": 196831327
            },
            "abstract": "We introduce two challenging datasets that reliably cause machine learning model performance to substantially degrade. The datasets are collected with a simple adversarial filtration technique to create datasets with limited spurious cues. Our datasets\u2019 real-world, unmodified examples transfer to various unseen models reliably, demonstrating that computer vision models have shared weaknesses. The first dataset is called IMAGENET-A and is like the ImageNet test set, but it is far more challenging for existing models. We also curate an adversarial out-of-distribution detection dataset called IMAGENET-O, which is the first out-of-distribution detection dataset created for ImageNet models. On IMAGENET-A a DenseNet-121 obtains around 2% accuracy, an accuracy drop of approximately 90%, and its out-of-distribution detection performance on IMAGENET-O is near random chance levels. We find that existing data augmentation techniques hardly boost performance, and using other public training datasets provides improvements that are limited. However, we find that improvements to computer vision architectures provide a promising path towards robust models.",
            "referenceCount": 86,
            "citationCount": 846,
            "influentialCitationCount": 132,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.07174",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-16",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2019NaturalAE,\n author = {Dan Hendrycks and Kevin Zhao and Steven Basart and J. Steinhardt and D. Song},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {15257-15266},\n title = {Natural Adversarial Examples},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
            "@type": "ScholarlyArticle",
            "paperId": "f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
            "corpusId": 301319,
            "url": "https://www.semanticscholar.org/paper/f09f7888aa5aeaf88a2a44aea768d9a8747e97d2",
            "title": "Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2949755136",
                "DBLP": "conf/cvpr/MontiBMRSB17",
                "ArXiv": "1611.08402",
                "DOI": "10.1109/CVPR.2017.576",
                "CorpusId": 301319
            },
            "abstract": "Deep learning has achieved a remarkable performance breakthrough in several fields, most notably in speech recognition, natural language processing, and computer vision. In particular, convolutional neural network (CNN) architectures currently produce state-of-the-art performance on a variety of image analysis tasks such as object detection and recognition. Most of deep learning research has so far focused on dealing with 1D, 2D, or 3D Euclidean-structured data such as acoustic signals, images, or videos. Recently, there has been an increasing interest in geometric deep learning, attempting to generalize deep learning methods to non-Euclidean structured data such as graphs and manifolds, with a variety of applications from the domains of network analysis, computational social science, or computer graphics. In this paper, we propose a unified framework allowing to generalize CNN architectures to non-Euclidean domains (graphs and manifolds) and learn local, stationary, and compositional task-specific features. We show that various non-Euclidean CNN methods previously proposed in the literature can be considered as particular instances of our framework. We test the proposed method on standard tasks from the realms of image-, graph-and 3D shape analysis and show that it consistently outperforms previous approaches.",
            "referenceCount": 67,
            "citationCount": 1589,
            "influentialCitationCount": 152,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08402",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-25",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Monti2016GeometricDL,\n author = {Federico Monti and D. Boscaini and Jonathan Masci and E. Rodol\u00e0 and Jan Svoboda and M. Bronstein},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5425-5434},\n title = {Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d997919c30fa6711bc5c25cf8c8aea34fac27b91",
            "@type": "ScholarlyArticle",
            "paperId": "d997919c30fa6711bc5c25cf8c8aea34fac27b91",
            "corpusId": 1919851,
            "url": "https://www.semanticscholar.org/paper/d997919c30fa6711bc5c25cf8c8aea34fac27b91",
            "title": "OpenFace: An open source facial behavior analysis toolkit",
            "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:acd15a6d-3248-41fb-8439-9a40aabe5608",
                "name": "IEEE Workshop/Winter Conference on Applications of Computer Vision",
                "alternate_names": [
                    "Workshop on Applications of Computer Vision",
                    "WACV",
                    "IEEE Work Conf Appl Comput Vis",
                    "Workshop Appl Comput Vis"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=2993"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/wacv/Baltrusaitis0M16",
                "MAG": "2395639500",
                "DOI": "10.1109/WACV.2016.7477553",
                "CorpusId": 1919851
            },
            "abstract": "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.",
            "referenceCount": 75,
            "citationCount": 1226,
            "influentialCitationCount": 185,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.repository.cam.ac.uk/bitstreams/38d96efd-7698-4aea-825f-bda08c664807/download",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-07",
            "journal": {
                "name": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Baltru\u0161aitis2016OpenFaceAO,\n author = {T. Baltru\u0161aitis and P. Robinson and Louis-Philippe Morency},\n booktitle = {IEEE Workshop/Winter Conference on Applications of Computer Vision},\n journal = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},\n pages = {1-10},\n title = {OpenFace: An open source facial behavior analysis toolkit},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "@type": "ScholarlyArticle",
            "paperId": "bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "corpusId": 3120635,
            "url": "https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd",
            "title": "Image Captioning with Semantic Attention",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2302086703",
                "ArXiv": "1603.03925",
                "DBLP": "journals/corr/YouJWFL16",
                "DOI": "10.1109/CVPR.2016.503",
                "CorpusId": 3120635
            },
            "abstract": "Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.",
            "referenceCount": 39,
            "citationCount": 1499,
            "influentialCitationCount": 136,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1603.03925",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-12",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{You2016ImageCW,\n author = {Quanzeng You and Hailin Jin and Zhaowen Wang and Chen Fang and Jiebo Luo},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4651-4659},\n title = {Image Captioning with Semantic Attention},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:88512be44744615f4baa8e14f600f036db4c2433",
            "@type": "ScholarlyArticle",
            "paperId": "88512be44744615f4baa8e14f600f036db4c2433",
            "corpusId": 11371972,
            "url": "https://www.semanticscholar.org/paper/88512be44744615f4baa8e14f600f036db4c2433",
            "title": "Semantic Understanding of Scenes Through the ADE20K Dataset",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ZhouZPFBT16",
                "MAG": "2507296351",
                "ArXiv": "1608.05442",
                "DOI": "10.1007/s11263-018-1140-0",
                "CorpusId": 11371972
            },
            "abstract": null,
            "referenceCount": 45,
            "citationCount": 1197,
            "influentialCitationCount": 237,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11263-018-1140-0.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-08-18",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "127"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2016SemanticUO,\n author = {Bolei Zhou and Hang Zhao and Xavier Puig and S. Fidler and Adela Barriuso and A. Torralba},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {302 - 321},\n title = {Semantic Understanding of Scenes Through the ADE20K Dataset},\n volume = {127},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:adcf9bdac7f05ba2bc003256a6794974aa571e0c",
            "@type": "ScholarlyArticle",
            "paperId": "adcf9bdac7f05ba2bc003256a6794974aa571e0c",
            "corpusId": 215827033,
            "url": "https://www.semanticscholar.org/paper/adcf9bdac7f05ba2bc003256a6794974aa571e0c",
            "title": "Learning to compare image patches via convolutional neural networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1504.03641",
                "MAG": "2949213045",
                "DBLP": "journals/corr/ZagoruykoK15",
                "DOI": "10.1109/CVPR.2015.7299064",
                "CorpusId": 215827033
            },
            "abstract": "In this paper we show how to learn directly from image data (i.e., without resorting to manually-designed features) a general similarity function for comparing image patches, which is a task of fundamental importance for many computer vision problems. To encode such a function, we opt for a CNN-based model that is trained to account for a wide variety of changes in image appearance. To that end, we explore and study multiple neural network architectures, which are specifically adapted to this task. We show that such an approach can significantly outperform the state-of-the-art on several problems and benchmark datasets.",
            "referenceCount": 28,
            "citationCount": 1357,
            "influentialCitationCount": 161,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1504.03641",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-04-14",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zagoruyko2015LearningTC,\n author = {Sergey Zagoruyko and N. Komodakis},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4353-4361},\n title = {Learning to compare image patches via convolutional neural networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17fa1c2a24ba8f731c8b21f1244463bc4b465681",
            "@type": "ScholarlyArticle",
            "paperId": "17fa1c2a24ba8f731c8b21f1244463bc4b465681",
            "corpusId": 205514,
            "url": "https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
            "title": "Deep multi-scale video prediction beyond mean square error",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949900324",
                "DBLP": "journals/corr/MathieuCL15",
                "ArXiv": "1511.05440",
                "CorpusId": 205514
            },
            "abstract": "Learning to predict future images from a video sequence involves the construction of an internal representation that models the image evolution accurately, and therefore, to some degree, its content and dynamics. This is why pixel-space video prediction may be viewed as a promising avenue for unsupervised feature learning. In addition, while optical flow has been a very studied problem in computer vision for a long time, future frame prediction is rarely approached. Still, many vision applications could benefit from the knowledge of the next frames of videos, that does not require the complexity of tracking every pixel trajectories. In this work, we train a convolutional network to generate future frames given an input sequence. To deal with the inherently blurry predictions obtained from the standard Mean Squared Error (MSE) loss function, we propose three different and complementary feature learning strategies: a multi-scale architecture, an adversarial training method, and an image gradient difference loss function. We compare our predictions to different published results based on recurrent neural networks on the UCF101 dataset",
            "referenceCount": 35,
            "citationCount": 1740,
            "influentialCitationCount": 204,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-17",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.05440"
            },
            "citationStyles": {
                "bibtex": "@Article{Mathieu2015DeepMV,\n author = {Micha\u00ebl Mathieu and C. Couprie and Yann LeCun},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Deep multi-scale video prediction beyond mean square error},\n volume = {abs/1511.05440},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08d4b0960e90b390db7e51b7e6ba4bc40680dfd4",
            "@type": "ScholarlyArticle",
            "paperId": "08d4b0960e90b390db7e51b7e6ba4bc40680dfd4",
            "corpusId": 7495339,
            "url": "https://www.semanticscholar.org/paper/08d4b0960e90b390db7e51b7e6ba4bc40680dfd4",
            "title": "300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge",
            "venue": "2013 IEEE International Conference on Computer Vision Workshops",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/iccvw/SagonasTZP13",
                "MAG": "2058961190",
                "DOI": "10.1109/ICCVW.2013.59",
                "CorpusId": 7495339
            },
            "abstract": "Automatic facial point detection plays arguably the most important role in face analysis. Several methods have been proposed which reported their results on databases of both constrained and unconstrained conditions. Most of these databases provide annotations with different mark-ups and in some cases the are problems related to the accuracy of the fiducial points. The aforementioned issues as well as the lack of a evaluation protocol makes it difficult to compare performance between different systems. In this paper, we present the 300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge which is held in conjunction with the International Conference on Computer Vision 2013, Sydney, Australia. The main goal of this challenge is to compare the performance of different methods on a new-collected dataset using the same evaluation protocol and the same mark-up and hence to develop the first standardized benchmark for facial landmark localization.",
            "referenceCount": 20,
            "citationCount": 1001,
            "influentialCitationCount": 114,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://nottingham-repository.worktribe.com/preview/1000750/tzimiroICCVW13.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-02",
            "journal": {
                "name": "2013 IEEE International Conference on Computer Vision Workshops",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sagonas2013300FI,\n author = {Christos Sagonas and Georgios Tzimiropoulos and S. Zafeiriou and M. Pantic},\n booktitle = {2013 IEEE International Conference on Computer Vision Workshops},\n journal = {2013 IEEE International Conference on Computer Vision Workshops},\n pages = {397-403},\n title = {300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a8cfa45b4c0d071fbffa091c02670b19c94b693",
            "@type": "ScholarlyArticle",
            "paperId": "8a8cfa45b4c0d071fbffa091c02670b19c94b693",
            "corpusId": 43928547,
            "url": "https://www.semanticscholar.org/paper/8a8cfa45b4c0d071fbffa091c02670b19c94b693",
            "title": "Do Better ImageNet Models Transfer Better?",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1805.08974",
                "DBLP": "conf/cvpr/KornblithSL19",
                "MAG": "2963328637",
                "DOI": "10.1109/CVPR.2019.00277",
                "CorpusId": 43928547
            },
            "abstract": "Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy (r = 0.99 and 0.96, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.",
            "referenceCount": 98,
            "citationCount": 1036,
            "influentialCitationCount": 110,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1805.08974",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-23",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kornblith2018DoBI,\n author = {Simon Kornblith and Jonathon Shlens and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2656-2666},\n title = {Do Better ImageNet Models Transfer Better?},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d44d0acccb25fb0a3cb86bdba066039145df1a36",
            "@type": "ScholarlyArticle",
            "paperId": "d44d0acccb25fb0a3cb86bdba066039145df1a36",
            "corpusId": 61814016,
            "url": "https://www.semanticscholar.org/paper/d44d0acccb25fb0a3cb86bdba066039145df1a36",
            "title": "Improved adaptive Gaussian mixture model for background subtraction",
            "venue": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/icpr/Zivkovic04",
                "MAG": "2130293653",
                "DOI": "10.1109/ICPR.2004.1333992",
                "CorpusId": 61814016
            },
            "abstract": "Background subtraction is a common computer vision task. We analyze the usual pixel-level approach. We develop an efficient adaptive algorithm using Gaussian mixture probability density. Recursive equations are used to constantly update the parameters and but also to simultaneously select the appropriate number of components for each pixel.",
            "referenceCount": 14,
            "citationCount": 2434,
            "influentialCitationCount": 300,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-08-23",
            "journal": {
                "name": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Zivkovic2004ImprovedAG,\n author = {Z. Zivkovic},\n booktitle = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},\n journal = {Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.},\n pages = {28-31 Vol.2},\n title = {Improved adaptive Gaussian mixture model for background subtraction},\n volume = {2},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e083dc8aeb7983a5cdff146985363d38caf0886",
            "@type": "ScholarlyArticle",
            "paperId": "3e083dc8aeb7983a5cdff146985363d38caf0886",
            "corpusId": 2451341,
            "url": "https://www.semanticscholar.org/paper/3e083dc8aeb7983a5cdff146985363d38caf0886",
            "title": "Pedestrian detection: A benchmark",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2107775979",
                "DBLP": "conf/cvpr/DollarWSP09",
                "DOI": "10.1109/CVPR.2009.5206631",
                "CorpusId": 2451341
            },
            "abstract": "Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field.",
            "referenceCount": 47,
            "citationCount": 1361,
            "influentialCitationCount": 181,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://authors.library.caltech.edu/87172/1/05206631.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2009-06-20",
            "journal": {
                "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Doll\u00e1r2009PedestrianDA,\n author = {Piotr Doll\u00e1r and C. Wojek and B. Schiele and P. Perona},\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {304-311},\n title = {Pedestrian detection: A benchmark},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:017438e9f62834694a4ea85f87f48bfcbd0490d7",
            "@type": "ScholarlyArticle",
            "paperId": "017438e9f62834694a4ea85f87f48bfcbd0490d7",
            "corpusId": 212563318,
            "url": "https://www.semanticscholar.org/paper/017438e9f62834694a4ea85f87f48bfcbd0490d7",
            "title": "Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/nips/WrightGRPM09",
                "MAG": "2131628350",
                "CorpusId": 212563318
            },
            "abstract": "Principal component analysis is a fundamental operation in computational data analysis, with myriad applications ranging from web search to bioinformatics to computer vision and image analysis. However, its performance and applicability in real scenarios are limited by a lack of robustness to outlying or corrupted observations. This paper considers the idealized \"robust principal component analysis\" problem of recovering a low rank matrix A from corrupted observations D = A + E. Here, the corrupted entries E are unknown and the errors can be arbitrarily large (modeling grossly corrupted observations common in visual and bioinformatic data), but are assumed to be sparse. We prove that most matrices A can be efficiently and exactly recovered from most error sign-and-support patterns by solving a simple convex program, for which we give a fast and provably convergent algorithm. Our result holds even when the rank of A grows nearly proportionally (up to a logarithmic factor) to the dimensionality of the observation space and the number of errors E grows in proportion to the total number of entries in the matrix. A by-product of our analysis is the first proportional growth results for the related problem of completing a low-rank matrix from a small fraction of its entries. Simulations and real-data examples corroborate the theoretical results, and suggest potential applications in computer vision.",
            "referenceCount": 42,
            "citationCount": 1330,
            "influentialCitationCount": 150,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-12-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wright2009RobustPC,\n author = {John Wright and Arvind Ganesh and Shankar R. Rao and YiGang Peng and Yi Ma},\n booktitle = {Neural Information Processing Systems},\n pages = {2080-2088},\n title = {Robust Principal Component Analysis: Exact Recovery of Corrupted Low-Rank Matrices via Convex Optimization},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:21334d1aac5422da88780f8e24e181bfa15ef0e1",
            "@type": "ScholarlyArticle",
            "paperId": "21334d1aac5422da88780f8e24e181bfa15ef0e1",
            "corpusId": 18061547,
            "url": "https://www.semanticscholar.org/paper/21334d1aac5422da88780f8e24e181bfa15ef0e1",
            "title": "Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1604.01753",
                "DBLP": "journals/corr/SigurdssonVWFLG16",
                "MAG": "2337252826",
                "DOI": "10.1007/978-3-319-46448-0_31",
                "CorpusId": 18061547
            },
            "abstract": null,
            "referenceCount": 48,
            "citationCount": 975,
            "influentialCitationCount": 190,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46448-0_31.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sigurdsson2016HollywoodIH,\n author = {Gunnar A. Sigurdsson and G\u00fcl Varol and X. Wang and Ali Farhadi and I. Laptev and A. Gupta},\n booktitle = {European Conference on Computer Vision},\n pages = {510-526},\n title = {Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "@type": "ScholarlyArticle",
            "paperId": "7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "corpusId": 52177403,
            "url": "https://www.semanticscholar.org/paper/7536bce1007a765fd097a7cc8ea62208a8c89b85",
            "title": "Deep Learning for Generic Object Detection: A Survey",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2890715498",
                "DBLP": "journals/ijcv/LiuOWFCLP20",
                "ArXiv": "1809.02165",
                "DOI": "10.1007/s11263-019-01247-4",
                "CorpusId": 52177403
            },
            "abstract": null,
            "referenceCount": 374,
            "citationCount": 1888,
            "influentialCitationCount": 69,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11263-019-01247-4.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-09-06",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "128"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2018DeepLF,\n author = {Li Liu and Wanli Ouyang and Xiaogang Wang and P. Fieguth and Jie Chen and Xinwang Liu and M. Pietik\u00e4inen},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {261 - 318},\n title = {Deep Learning for Generic Object Detection: A Survey},\n volume = {128},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1deb84164223a3468a80b376befc7e0b7261a36b",
            "@type": "ScholarlyArticle",
            "paperId": "1deb84164223a3468a80b376befc7e0b7261a36b",
            "corpusId": 2700315,
            "url": "https://www.semanticscholar.org/paper/1deb84164223a3468a80b376befc7e0b7261a36b",
            "title": "Visibility in bad weather from a single image",
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/cvpr/Tan08",
                "MAG": "2114867966",
                "DOI": "10.1109/CVPR.2008.4587643",
                "CorpusId": 2700315
            },
            "abstract": "Bad weather, such as fog and haze, can significantly degrade the visibility of a scene. Optically, this is due to the substantial presence of particles in the atmosphere that absorb and scatter light. In computer vision, the absorption and scattering processes are commonly modeled by a linear combination of the direct attenuation and the airlight. Based on this model, a few methods have been proposed, and most of them require multiple input images of a scene, which have either different degrees of polarization or different atmospheric conditions. This requirement is the main drawback of these methods, since in many situations, it is difficult to be fulfilled. To resolve the problem, we introduce an automated method that only requires a single input image. This method is based on two basic observations: first, images with enhanced visibility (or clear-day images) have more contrast than images plagued by bad weather; second, airlight whose variation mainly depends on the distance of objects to the viewer, tends to be smooth. Relying on these two observations, we develop a cost function in the framework of Markov random fields, which can be efficiently optimized by various techniques, such as graph-cuts or belief propagation. The method does not require the geometrical information of the input image, and is applicable for both color and gray images.",
            "referenceCount": 14,
            "citationCount": 2106,
            "influentialCitationCount": 209,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-06-23",
            "journal": {
                "name": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tan2008VisibilityIB,\n author = {R. Tan},\n booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Visibility in bad weather from a single image},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e70bb805cb99904504f12b3fa32ddaae066323c",
            "@type": "ScholarlyArticle",
            "paperId": "5e70bb805cb99904504f12b3fa32ddaae066323c",
            "corpusId": 711203,
            "url": "https://www.semanticscholar.org/paper/5e70bb805cb99904504f12b3fa32ddaae066323c",
            "title": "Modeling the World from Internet Photo Collections",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2129201358",
                "DBLP": "journals/ijcv/SnavelySS08",
                "DOI": "10.1007/s11263-007-0107-3",
                "CorpusId": 711203
            },
            "abstract": null,
            "referenceCount": 89,
            "citationCount": 2212,
            "influentialCitationCount": 177,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.cornell.edu/~snavely/publications/papers/snavely_ijcv07.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geography",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-11-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "80"
            },
            "citationStyles": {
                "bibtex": "@Article{Snavely2008ModelingTW,\n author = {Noah Snavely and S. Seitz and R. Szeliski},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {189-210},\n title = {Modeling the World from Internet Photo Collections},\n volume = {80},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1cd44dd4f1b0b1c0499b1dc9637010c6d63d445c",
            "@type": "ScholarlyArticle",
            "paperId": "1cd44dd4f1b0b1c0499b1dc9637010c6d63d445c",
            "corpusId": 122725027,
            "url": "https://www.semanticscholar.org/paper/1cd44dd4f1b0b1c0499b1dc9637010c6d63d445c",
            "title": "A Tutorial on Particle Filtering and Smoothing: Fifteen years later",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2111787305",
                "CorpusId": 122725027
            },
            "abstract": "Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.",
            "referenceCount": 40,
            "citationCount": 2021,
            "influentialCitationCount": 189,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Economics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Doucet2008ATO,\n author = {A. Doucet and A. M. Johansen},\n title = {A Tutorial on Particle Filtering and Smoothing: Fifteen years later},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d7ce5665a72c0b607f484c1b448875f02ddfac3b",
            "@type": "ScholarlyArticle",
            "paperId": "d7ce5665a72c0b607f484c1b448875f02ddfac3b",
            "corpusId": 14521054,
            "url": "https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b",
            "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/JohnsonKL15",
                "MAG": "2254252455",
                "ArXiv": "1511.07571",
                "DOI": "10.1109/CVPR.2016.494",
                "CorpusId": 14521054
            },
            "abstract": "We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.",
            "referenceCount": 57,
            "citationCount": 1055,
            "influentialCitationCount": 114,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1511.07571",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-24",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Johnson2015DenseCapFC,\n author = {Justin Johnson and A. Karpathy and Li Fei-Fei},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4565-4574},\n title = {DenseCap: Fully Convolutional Localization Networks for Dense Captioning},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e944b414e9f601a6008076bd43b91d382090adbc",
            "@type": "ScholarlyArticle",
            "paperId": "e944b414e9f601a6008076bd43b91d382090adbc",
            "corpusId": 1203247,
            "url": "https://www.semanticscholar.org/paper/e944b414e9f601a6008076bd43b91d382090adbc",
            "title": "VirtualWorlds as Proxy for Multi-object Tracking Analysis",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/GaidonWCV16",
                "MAG": "2949907962",
                "ArXiv": "1605.06457",
                "DOI": "10.1109/CVPR.2016.470",
                "CorpusId": 1203247
            },
            "abstract": "Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called \"Virtual KITTI\", automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.",
            "referenceCount": 41,
            "citationCount": 886,
            "influentialCitationCount": 139,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://elib.dlr.de/105154/1/Gaidon_Virtual_Worlds_as_CVPR_2016_paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-20",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gaidon2016VirtualWorldsAP,\n author = {Adrien Gaidon and Qiao Wang and Yohann Cabon and E. Vig},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4340-4349},\n title = {VirtualWorlds as Proxy for Multi-object Tracking Analysis},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d6ae67e569f974360b107060c23cbb8a13b0687",
            "@type": "ScholarlyArticle",
            "paperId": "5d6ae67e569f974360b107060c23cbb8a13b0687",
            "corpusId": 206592873,
            "url": "https://www.semanticscholar.org/paper/5d6ae67e569f974360b107060c23cbb8a13b0687",
            "title": "Learning from massive noisy labeled data for image classification",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/XiaoXYHW15",
                "MAG": "1921293667",
                "DOI": "10.1109/CVPR.2015.7298885",
                "CorpusId": 206592873
            },
            "abstract": "Large-scale supervised datasets are crucial to train convolutional neural networks (CNNs) for various computer vision problems. However, obtaining a massive amount of well-labeled data is usually very expensive and time consuming. In this paper, we introduce a general framework to train CNNs with only a limited number of clean labels and millions of easily obtained noisy labels. We model the relationships between images, class labels and label noises with a probabilistic graphical model and further integrate it into an end-to-end deep learning system. To demonstrate the effectiveness of our approach, we collect a large-scale real-world clothing classification dataset with both noisy and clean labels. Experiments on this dataset indicate that our approach can better correct the noisy labels and improves the performance of trained CNNs.",
            "referenceCount": 32,
            "citationCount": 938,
            "influentialCitationCount": 204,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ee.cuhk.edu.hk/%7Exgwang/papers/xiaoXYHWcvpr15.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2015LearningFM,\n author = {Tong Xiao and Tian Xia and Yi Yang and Chang Huang and Xiaogang Wang},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2691-2699},\n title = {Learning from massive noisy labeled data for image classification},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d745cf8c51032996b5fee6b19e1b5321c14797eb",
            "@type": "ScholarlyArticle",
            "paperId": "d745cf8c51032996b5fee6b19e1b5321c14797eb",
            "corpusId": 3961724,
            "url": "https://www.semanticscholar.org/paper/d745cf8c51032996b5fee6b19e1b5321c14797eb",
            "title": "Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/eccv/GrayT08",
                "MAG": "1518138188",
                "DOI": "10.1007/978-3-540-88682-2_21",
                "CorpusId": 3961724
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 1482,
            "influentialCitationCount": 273,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-10-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gray2008ViewpointIP,\n author = {D. Gray and Hai Tao},\n booktitle = {European Conference on Computer Vision},\n pages = {262-275},\n title = {Viewpoint Invariant Pedestrian Recognition with an Ensemble of Localized Features},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "@type": "ScholarlyArticle",
            "paperId": "44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "corpusId": 205572964,
            "url": "https://www.semanticscholar.org/paper/44e1dd74f0446ec91221189ad3a65edb1a0208fe",
            "title": "A guide to deep learning in healthcare",
            "venue": "Nature Network Boston",
            "publicationVenue": {
                "id": "urn:research:9e995b6d-f30b-4ab4-a13b-3dc2cc992f47",
                "name": "Nature Network Boston",
                "alternate_names": [
                    "Nat Netw Boston",
                    "Nat Med",
                    "Nature Medicine"
                ],
                "issn": "1744-7933",
                "url": "https://www.nature.com/nature/articles?code=archive_news"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2905810301",
                "DOI": "10.1038/s41591-018-0316-z",
                "CorpusId": 205572964,
                "PubMed": "30617335"
            },
            "abstract": null,
            "referenceCount": 65,
            "citationCount": 1773,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Medicine",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Esteva2019AGT,\n author = {A. Esteva and Alexandre Robicquet and Bharath Ramsundar and Volodymyr Kuleshov and M. DePristo and Katherine Chou and Claire Cui and Greg S. Corrado and S. Thrun and J. Dean},\n booktitle = {Nature Network Boston},\n journal = {Nature Medicine},\n pages = {24 - 29},\n title = {A guide to deep learning in healthcare},\n volume = {25},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f797fd44b9ddd5845611eb7a705ca9464a8819d1",
            "@type": "ScholarlyArticle",
            "paperId": "f797fd44b9ddd5845611eb7a705ca9464a8819d1",
            "corpusId": 5079983,
            "url": "https://www.semanticscholar.org/paper/f797fd44b9ddd5845611eb7a705ca9464a8819d1",
            "title": "Very Deep Convolutional Networks for Text Classification",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:8de18c35-6785-4e54-99f2-21ee961302c6",
                "name": "Conference of the European Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "Conf Eur Chapter Assoc Comput Linguistics",
                    "EACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/eacl/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2964046515",
                "ACL": "E17-1104",
                "DBLP": "conf/eacl/SchwenkBCL17",
                "DOI": "10.18653/V1/E17-1104",
                "CorpusId": 5079983
            },
            "abstract": "The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.",
            "referenceCount": 28,
            "citationCount": 891,
            "influentialCitationCount": 100,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/E17-1104.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schwenk2016VeryDC,\n author = {Holger Schwenk and Lo\u00efc Barrault and Alexis Conneau and Yann LeCun},\n booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},\n pages = {1107-1116},\n title = {Very Deep Convolutional Networks for Text Classification},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7507603da9711c0e43e29f3094f33d9337e7dfbd",
            "@type": "ScholarlyArticle",
            "paperId": "7507603da9711c0e43e29f3094f33d9337e7dfbd",
            "corpusId": 232290593,
            "url": "https://www.semanticscholar.org/paper/7507603da9711c0e43e29f3094f33d9337e7dfbd",
            "title": "3D Human Pose Estimation with Spatial and Temporal Transformers",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2103.10455",
                "DBLP": "journals/corr/abs-2103-10455",
                "DOI": "10.1109/ICCV48922.2021.01145",
                "CorpusId": 232290593
            },
            "abstract": "Transformer architectures have become the model of choice in natural language processing and are now being introduced into computer vision tasks such as image classification, object detection, and semantic segmentation. However, in the field of human pose estimation, convolutional architectures still remain dominant. In this work, we present PoseFormer, a purely transformer-based approach for 3D human pose estimation in videos without convolutional architectures involved. Inspired by recent developments in vision transformers, we design a spatial-temporal transformer structure to comprehensively model the human joint relations within each frame as well as the temporal correlations across frames, then output an accurate 3D human pose of the center frame. We quantitatively and qualitatively evaluate our method on two popular and standard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments show that PoseFormer achieves state-of-the-art performance on both datasets. Code is available at https://github.com/zczcwh/PoseFormer",
            "referenceCount": 49,
            "citationCount": 229,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2103.10455",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-03-18",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zheng20213DHP,\n author = {Ce Zheng and Sijie Zhu and Mat'ias Mendieta and Taojiannan Yang and Chen Chen and Zhengming Ding},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {11636-11645},\n title = {3D Human Pose Estimation with Spatial and Temporal Transformers},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "@type": "ScholarlyArticle",
            "paperId": "3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "corpusId": 189857704,
            "url": "https://www.semanticscholar.org/paper/3aa681914a7da79f7d7293f51a058eefe61c8bb7",
            "title": "MVTec AD \u2014 A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2948982773",
                "DBLP": "conf/cvpr/BergmannFSS19",
                "DOI": "10.1109/CVPR.2019.00982",
                "CorpusId": 189857704
            },
            "abstract": "The detection of anomalous structures in natural image data is of utmost importance for numerous tasks in the \ufb01eld of computer vision. The development of methods for unsupervised anomaly detection requires data on which to train and evaluate new approaches and ideas. We introduce the MVTec Anomaly Detection (MVTec AD) dataset containing 5354 high-resolution color images of different object and texture categories. It contains normal, i.e., defect-free, images intended for training and images with anomalies intended for testing. The anomalies manifest themselves in the form of over 70 different types of defects such as scratches, dents, contaminations, and various structural changes. In addition, we provide pixel-precise ground truth regions for all anomalies. We also conduct a thorough evaluation of current state-of-the-art unsupervised anomaly detection methods based on deep architectures such as convolutional autoencoders, generative adversarial networks, and feature descriptors using pre-trained convolutional neural networks, as well as classical computer vision methods. This initial benchmark indicates that there is considerable room for improvement. To the best of our knowledge, this is the \ufb01rst comprehensive, multi-object, multi-defect dataset for anomaly detection that provides pixel-accurate ground truth regions and focuses on real-world applications.",
            "referenceCount": 29,
            "citationCount": 697,
            "influentialCitationCount": 217,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-01",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bergmann2019MVTecA,\n author = {Paul Bergmann and Michael Fauser and David Sattlegger and C. Steger},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {9584-9592},\n title = {MVTec AD \u2014 A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c8315ae713b3e27c6e9f291a158134d9c516166",
            "@type": "ScholarlyArticle",
            "paperId": "2c8315ae713b3e27c6e9f291a158134d9c516166",
            "corpusId": 118637813,
            "url": "https://www.semanticscholar.org/paper/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "title": "Learning Discriminative Model Prediction for Tracking",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3001584168",
                "ArXiv": "1904.07220",
                "DBLP": "journals/corr/abs-1904-07220",
                "DOI": "10.1109/ICCV.2019.00628",
                "CorpusId": 118637813
            },
            "abstract": "The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability. We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https://github.com/visionml/pytracking.",
            "referenceCount": 50,
            "citationCount": 720,
            "influentialCitationCount": 222,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.07220",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-15",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bhat2019LearningDM,\n author = {Goutam Bhat and Martin Danelljan and L. Gool and R. Timofte},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {6181-6190},\n title = {Learning Discriminative Model Prediction for Tracking},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:510a24375b5166842cae47f2e54052846704e8f4",
            "@type": "ScholarlyArticle",
            "paperId": "510a24375b5166842cae47f2e54052846704e8f4",
            "corpusId": 5334482,
            "url": "https://www.semanticscholar.org/paper/510a24375b5166842cae47f2e54052846704e8f4",
            "title": "Loss Functions for Image Restoration With Neural Networks",
            "venue": "IEEE Transactions on Computational Imaging",
            "publicationVenue": {
                "id": "urn:research:f2b47cba-3a35-4bd0-9e7e-fd2b23338309",
                "name": "IEEE Transactions on Computational Imaging",
                "alternate_names": [
                    "IEEE Trans Comput Imaging"
                ],
                "issn": "2333-9403",
                "url": "https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/tci/ZhaoGFK17",
                "MAG": "2562637781",
                "DOI": "10.1109/TCI.2016.2644865",
                "CorpusId": 5334482
            },
            "abstract": "Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is $\\ell _2$. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.",
            "referenceCount": 29,
            "citationCount": 1590,
            "influentialCitationCount": 103,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-01",
            "journal": {
                "name": "IEEE Transactions on Computational Imaging",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2017LossFF,\n author = {Hang Zhao and Orazio Gallo and I. Frosio and J. Kautz},\n booktitle = {IEEE Transactions on Computational Imaging},\n journal = {IEEE Transactions on Computational Imaging},\n pages = {47-57},\n title = {Loss Functions for Image Restoration With Neural Networks},\n volume = {3},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "@type": "ScholarlyArticle",
            "paperId": "0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "corpusId": 209501181,
            "url": "https://www.semanticscholar.org/paper/0311ace1d499cadd1cc0c515a625d1d045f60d25",
            "title": "Deep Learning for 3D Point Clouds: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2997320365",
                "DBLP": "journals/pami/GuoWHLLB21",
                "ArXiv": "1912.12033",
                "DOI": "10.1109/TPAMI.2020.3005434",
                "CorpusId": 209501181,
                "PubMed": "32750799"
            },
            "abstract": "Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.",
            "referenceCount": 279,
            "citationCount": 1047,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://jultika.oulu.fi/files/nbnfi-fe2022030121340.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-12-27",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2019DeepLF,\n author = {Yulan Guo and Hanyun Wang and Qingyong Hu and Hao Liu and Li Liu and Bennamoun},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {4338-4364},\n title = {Deep Learning for 3D Point Clouds: A Survey},\n volume = {43},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32b8f58a038df83138435b12a499c8bf0de13811",
            "@type": "ScholarlyArticle",
            "paperId": "32b8f58a038df83138435b12a499c8bf0de13811",
            "corpusId": 14136313,
            "url": "https://www.semanticscholar.org/paper/32b8f58a038df83138435b12a499c8bf0de13811",
            "title": "End-to-end scene text recognition",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1998042868",
                "DBLP": "conf/iccv/WangBB11",
                "DOI": "10.1109/ICCV.2011.6126402",
                "CorpusId": 14136313
            },
            "abstract": "This paper focuses on the problem of word detection and recognition in natural images. The problem is significantly more challenging than reading text in scanned documents, and has only recently gained attention from the computer vision community. Sub-components of the problem, such as text detection and cropped image word recognition, have been studied in isolation [7, 4, 20]. However, what is unclear is how these recent approaches contribute to solving the end-to-end problem of word recognition. We fill this gap by constructing and evaluating two systems. The first, representing the de facto state-of-the-art, is a two stage pipeline consisting of text detection followed by a leading OCR engine. The second is a system rooted in generic object recognition, an extension of our previous work in [20]. We show that the latter approach achieves superior performance. While scene text recognition has generally been treated with highly domain-specific methods, our results demonstrate the suitability of applying generic computer vision methods. Adopting this approach opens the door for real world scene text recognition to benefit from the rapid advances that have been taking place in object recognition.",
            "referenceCount": 20,
            "citationCount": 1083,
            "influentialCitationCount": 254,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-01",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2011EndtoendST,\n author = {Kai Wang and Boris Babenko and Serge J. Belongie},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {1457-1464},\n title = {End-to-end scene text recognition},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f908d2fb9cfcaff17030a912e4811fb02aaeec03",
            "@type": "ScholarlyArticle",
            "paperId": "f908d2fb9cfcaff17030a912e4811fb02aaeec03",
            "corpusId": 1680724,
            "url": "https://www.semanticscholar.org/paper/f908d2fb9cfcaff17030a912e4811fb02aaeec03",
            "title": "Fast cost-volume filtering for visual correspondence and beyond",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2148534289",
                "DBLP": "conf/cvpr/RhemannHBRG11",
                "DOI": "10.1109/CVPR.2011.5995372",
                "CorpusId": 1680724,
                "PubMed": "22848130"
            },
            "abstract": "Many computer vision tasks can be formulated as labeling problems. The desired solution is often a spatially smooth labeling where label transitions are aligned with color edges of the input image. We show that such solutions can be efficiently achieved by smoothing the label costs with a very fast edge preserving filter. In this paper we propose a generic and simple framework comprising three steps: (i) constructing a cost volume (ii) fast cost volume filtering and (iii) winner-take-all label selection. Our main contribution is to show that with such a simple framework state-of-the-art results can be achieved for several computer vision applications. In particular, we achieve (i) disparity maps in real-time, whose quality exceeds those of all other fast (local) approaches on the Middlebury stereo benchmark, and (ii) optical flow fields with very fine structures as well as large displacements. To demonstrate robustness, the few parameters of our framework are set to nearly identical values for both applications. Also, competitive results for interactive image segmentation are presented. With this work, we hope to inspire other researchers to leverage this framework to other application areas.",
            "referenceCount": 50,
            "citationCount": 1109,
            "influentialCitationCount": 140,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-20",
            "journal": {
                "name": "CVPR 2011",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rhemann2011FastCF,\n author = {Christoph Rhemann and A. Hosni and M. Bleyer and C. Rother and M. Gelautz},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {CVPR 2011},\n pages = {3017-3024},\n title = {Fast cost-volume filtering for visual correspondence and beyond},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:021fc345d40d3e6332cd2ef276e2eaa5e71102e4",
            "@type": "ScholarlyArticle",
            "paperId": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4",
            "corpusId": 17864746,
            "url": "https://www.semanticscholar.org/paper/021fc345d40d3e6332cd2ef276e2eaa5e71102e4",
            "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions",
            "venue": "British Machine Vision Conference",
            "publicationVenue": {
                "id": "urn:research:78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                "name": "British Machine Vision Conference",
                "alternate_names": [
                    "Br Mach Vis Conf",
                    "BMVC"
                ],
                "issn": null,
                "url": "http://www.bmva.org/bmvc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1996901117",
                "DBLP": "journals/corr/JaderbergVZ14",
                "ArXiv": "1405.3866",
                "DOI": "10.5244/C.28.88",
                "CorpusId": 17864746
            },
            "abstract": "The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5\u00d7 speedup with no loss in accuracy, and 4.5\u00d7 speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.",
            "referenceCount": 42,
            "citationCount": 1345,
            "influentialCitationCount": 84,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.bmva.org/bmvc/2014/files/abstract073.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-05-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1405.3866"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaderberg2014SpeedingUC,\n author = {Max Jaderberg and A. Vedaldi and Andrew Zisserman},\n booktitle = {British Machine Vision Conference},\n journal = {ArXiv},\n title = {Speeding up Convolutional Neural Networks with Low Rank Expansions},\n volume = {abs/1405.3866},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a737fbad8dd29730313c89ae1123efeab48786d",
            "@type": "ScholarlyArticle",
            "paperId": "2a737fbad8dd29730313c89ae1123efeab48786d",
            "corpusId": 2036193,
            "url": "https://www.semanticscholar.org/paper/2a737fbad8dd29730313c89ae1123efeab48786d",
            "title": "The lumigraph",
            "venue": "International Conference on Computer Graphics and Interactive Techniques",
            "publicationVenue": {
                "id": "urn:research:cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                "name": "International Conference on Computer Graphics and Interactive Techniques",
                "alternate_names": [
                    "Int Conf Comput Graph Interact Tech",
                    "SIGGRAPH"
                ],
                "issn": null,
                "url": "http://www.siggraph.org/"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2294985758",
                "DBLP": "conf/siggraph/GortlerGSC96",
                "DOI": "10.1145/237170.237200",
                "CorpusId": 2036193
            },
            "abstract": "This paper discusses a new method for capturing the complete appearance of both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subsetof the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation.",
            "referenceCount": 66,
            "citationCount": 2906,
            "influentialCitationCount": 126,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/237170.237200",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1996-08-01",
            "journal": {
                "name": "Proceedings of the 23rd annual conference on Computer graphics and interactive techniques",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Gortler1996TheL,\n author = {S. Gortler and R. Grzeszczuk and R. Szeliski and Michael F. Cohen},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques},\n title = {The lumigraph},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d995f65c858af09b5304dbb5303f8610bea32a8c",
            "@type": "ScholarlyArticle",
            "paperId": "d995f65c858af09b5304dbb5303f8610bea32a8c",
            "corpusId": 8192877,
            "url": "https://www.semanticscholar.org/paper/d995f65c858af09b5304dbb5303f8610bea32a8c",
            "title": "Marker tracking and HMD calibration for a video-based augmented reality conferencing system",
            "venue": "International Workshop on Automated Reasoning",
            "publicationVenue": {
                "id": "urn:research:4cdaca17-9ba3-48ea-a955-82a19519a121",
                "name": "International Workshop on Automated Reasoning",
                "alternate_names": [
                    "IWAR",
                    "Int Workshop Autom Reason"
                ],
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2127972053",
                "DBLP": "conf/ismar/KatoB99",
                "DOI": "10.1109/IWAR.1999.803809",
                "CorpusId": 8192877
            },
            "abstract": "We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on virtual monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and head mounted display (HMD) calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.",
            "referenceCount": 15,
            "citationCount": 2547,
            "influentialCitationCount": 197,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.vs.inf.ethz.ch/edu/SS2005/DS/papers/ar/kato-artoolkit.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-10-20",
            "journal": {
                "name": "Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kato1999MarkerTA,\n author = {H. Kato and M. Billinghurst},\n booktitle = {International Workshop on Automated Reasoning},\n journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},\n pages = {85-94},\n title = {Marker tracking and HMD calibration for a video-based augmented reality conferencing system},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b9701ad65e256bd8841c4f80ced09b4ca1d5e331",
            "@type": "ScholarlyArticle",
            "paperId": "b9701ad65e256bd8841c4f80ced09b4ca1d5e331",
            "corpusId": 12779752,
            "url": "https://www.semanticscholar.org/paper/b9701ad65e256bd8841c4f80ced09b4ca1d5e331",
            "title": "Markov Random Field Modeling in Image Analysis",
            "venue": "Computer Science Workbench",
            "publicationVenue": {
                "id": "urn:research:2e1919d9-9204-4018-b488-7a7bbcc1b84d",
                "name": "Computer Science Workbench",
                "alternate_names": [
                    "Comput Sci Work"
                ],
                "issn": "1431-1488",
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "series/acvpr/978-1-84800-278-4",
                "MAG": "1651266332",
                "DOI": "10.1007/978-4-431-67044-5",
                "CorpusId": 12779752
            },
            "abstract": null,
            "referenceCount": 341,
            "citationCount": 1908,
            "influentialCitationCount": 266,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-1-84800-279-1/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Computer Science Workbench",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2001MarkovRF,\n author = {S. Li},\n booktitle = {Computer Science Workbench},\n journal = {Computer Science Workbench},\n title = {Markov Random Field Modeling in Image Analysis},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8ec70f4d162cae2103c8dfcce3be4c10a2fcf10b",
            "@type": "ScholarlyArticle",
            "paperId": "8ec70f4d162cae2103c8dfcce3be4c10a2fcf10b",
            "corpusId": 206769306,
            "url": "https://www.semanticscholar.org/paper/8ec70f4d162cae2103c8dfcce3be4c10a2fcf10b",
            "title": "Flexible camera calibration by viewing a plane from unknown orientations",
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2095614026",
                "DBLP": "conf/iccv/Zhang99",
                "DOI": "10.1109/ICCV.1999.791289",
                "CorpusId": 206769306
            },
            "abstract": "Proposes a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment, such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real-world use. The corresponding software is available from the author's Web page ().",
            "referenceCount": 19,
            "citationCount": 2712,
            "influentialCitationCount": 176,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-09-01",
            "journal": {
                "name": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang1999FlexibleCC,\n author = {Zhengyou Zhang},\n booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n pages = {666-673 vol.1},\n title = {Flexible camera calibration by viewing a plane from unknown orientations},\n volume = {1},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7bbbbd2073503720f304d031cb4641eb45a3edee",
            "@type": "ScholarlyArticle",
            "paperId": "7bbbbd2073503720f304d031cb4641eb45a3edee",
            "corpusId": 14673939,
            "url": "https://www.semanticscholar.org/paper/7bbbbd2073503720f304d031cb4641eb45a3edee",
            "title": "Iterative point matching for registration of free-form curves and surfaces",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2118104180",
                "DBLP": "journals/ijcv/Zhang94",
                "DOI": "10.1007/BF01427149",
                "CorpusId": 14673939
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 2386,
            "influentialCitationCount": 187,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1994-10-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang1994IterativePM,\n author = {Zhengyou Zhang},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {119-152},\n title = {Iterative point matching for registration of free-form curves and surfaces},\n volume = {13},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4c1cfc0cce2140fb8cf5c175c0a34b467298ee9",
            "@type": "ScholarlyArticle",
            "paperId": "d4c1cfc0cce2140fb8cf5c175c0a34b467298ee9",
            "corpusId": 148573852,
            "url": "https://www.semanticscholar.org/paper/d4c1cfc0cce2140fb8cf5c175c0a34b467298ee9",
            "title": "Deep Closest Point: Learning Representations for Point Cloud Registration",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iccv/WangS19",
                "MAG": "2986382673",
                "ArXiv": "1905.03304",
                "DOI": "10.1109/ICCV.2019.00362",
                "CorpusId": 148573852
            },
            "abstract": "Point cloud registration is a key problem for computer vision applied to robotics, medical imaging, and other applications. This problem involves finding a rigid transformation from one point cloud into another so that they align. Iterative Closest Point (ICP) and its variants provide simple and easily-implemented iterative methods for this task, but these algorithms can converge to spurious local optima. To address local optima and other difficulties in the ICP pipeline, we propose a learning-based method, titled Deep Closest Point (DCP), inspired by recent techniques in computer vision and natural language processing. Our model consists of three parts: a point cloud embedding network, an attention-based module combined with a pointer generation layer to approximate combinatorial matching, and a differentiable singular value decomposition (SVD) layer to extract the final rigid transformation. We train our model end-to-end on the ModelNet40 dataset and show in several settings that it performs better than ICP, its variants (e.g., Go-ICP, FGR), and the recently-proposed learning-based method PointNetLK. Beyond providing a state-of-the-art registration technique, we evaluate the suitability of our learned features transferred to unseen objects. We also provide preliminary analysis of our learned model to help understand whether domain-specific and/or global features facilitate rigid registration.",
            "referenceCount": 65,
            "citationCount": 584,
            "influentialCitationCount": 143,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.03304",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-08",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2019DeepCP,\n author = {Yue Wang and J. Solomon},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {3522-3531},\n title = {Deep Closest Point: Learning Representations for Point Cloud Registration},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17555c227941654bc19d613742e2508f209c6d86",
            "@type": "ScholarlyArticle",
            "paperId": "17555c227941654bc19d613742e2508f209c6d86",
            "corpusId": 52298265,
            "url": "https://www.semanticscholar.org/paper/17555c227941654bc19d613742e2508f209c6d86",
            "title": "Albumentations: fast and flexible image augmentations",
            "venue": "Inf.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3099319035",
                "DBLP": "journals/information/BuslaevIKPDK20",
                "ArXiv": "1809.06839",
                "DOI": "10.3390/info11020125",
                "CorpusId": 52298265
            },
            "abstract": "Data augmentation is a commonly used technique for increasing both the size and the diversity of labeled training sets by leveraging input transformations that preserve corresponding output labels. In computer vision, image augmentations have become a common implicit regularization technique to combat overfitting in deep learning models and are ubiquitously used to improve performance. While most deep learning frameworks implement basic image transformations, the list is typically limited to some variations of flipping, rotating, scaling, and cropping. Moreover, image processing speed varies in existing image augmentation libraries. We present Albumentations, a fast and flexible open source library for image augmentation with many various image transform operations available that is also an easy-to-use wrapper around other augmentation libraries. We discuss the design principles that drove the implementation of Albumentations and give an overview of the key features and distinct capabilities. Finally, we provide examples of image augmentations for different computer vision tasks and demonstrate that Albumentations is faster than other commonly used image augmentation tools on most image transform operations.",
            "referenceCount": 81,
            "citationCount": 1231,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/2078-2489/11/2/125/pdf?version=1582551862",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-09-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1809.06839"
            },
            "citationStyles": {
                "bibtex": "@Article{Buslaev2018AlbumentationsFA,\n author = {A. Buslaev and Alex Parinov and Eugene Khvedchenya and V. Iglovikov and Alexandr A Kalinin},\n booktitle = {Inf.},\n journal = {ArXiv},\n title = {Albumentations: fast and flexible image augmentations},\n volume = {abs/1809.06839},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:52ca4ed04d1d9dba3e6ae30717898276735e0b79",
            "@type": "ScholarlyArticle",
            "paperId": "52ca4ed04d1d9dba3e6ae30717898276735e0b79",
            "corpusId": 5057778,
            "url": "https://www.semanticscholar.org/paper/52ca4ed04d1d9dba3e6ae30717898276735e0b79",
            "title": "Efficient Non-Maximum Suppression",
            "venue": "International Conference on Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:48782cc2-a3b4-44f3-95a4-2c4cb0f7245b",
                "name": "International Conference on Pattern Recognition",
                "alternate_names": [
                    "Pattern Recognit (ICPR Proc Int Conf",
                    "Int Conf Pattern Recognit",
                    "ICPR",
                    "International conference on pattern recognition",
                    "Int conf pattern recognit",
                    "Pattern Recognition (ICPR), Proceedings of the International Conference on"
                ],
                "issn": "1041-3278",
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000545/all-proceedings"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2144506857",
                "DBLP": "conf/icpr/NeubeckG06",
                "DOI": "10.1109/ICPR.2006.479",
                "CorpusId": 5057778
            },
            "abstract": "In this work we scrutinize a low level computer vision task - non-maximum suppression (NMS) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of NMS, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient",
            "referenceCount": 4,
            "citationCount": 1259,
            "influentialCitationCount": 163,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-08-20",
            "journal": {
                "name": "18th International Conference on Pattern Recognition (ICPR'06)",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Neubeck2006EfficientNS,\n author = {A. Neubeck and L. Gool},\n booktitle = {International Conference on Pattern Recognition},\n journal = {18th International Conference on Pattern Recognition (ICPR'06)},\n pages = {850-855},\n title = {Efficient Non-Maximum Suppression},\n volume = {3},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:93aeb19a54780c66662000dcfbef238fdcfcc10f",
            "@type": "ScholarlyArticle",
            "paperId": "93aeb19a54780c66662000dcfbef238fdcfcc10f",
            "corpusId": 38300792,
            "url": "https://www.semanticscholar.org/paper/93aeb19a54780c66662000dcfbef238fdcfcc10f",
            "title": "Mathematical problems in image processing - partial differential equations and the calculus of variations, 2nd Edition",
            "venue": "Applied Mathematical Sciences",
            "publicationVenue": {
                "id": "urn:research:369b7733-9b48-4432-85b6-1e3e41080d1b",
                "name": "Applied Mathematical Sciences",
                "alternate_names": [
                    "Applied mathematical sciences",
                    "Appl Math Sci",
                    "Appl math sci"
                ],
                "issn": "1312-885X",
                "url": "http://www.m-hikari.com/ams/ams-2013/ams-57-60-2013/sommerAMS57-60-2013.pdf"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "books/lib/AubertK02",
                "MAG": "1496099826",
                "CorpusId": 38300792
            },
            "abstract": "The updated 2nd edition of this book presents a variety of image analysis applications, reviews their precise mathematics and shows how to discretize them. For the mathematical community, the book shows the contribution of mathematics to this domain, and highlights unsolved theoretical questions. For the computer vision community, it presents a clear, self-contained and global overview of the mathematics involved in image procesing problems. The second edition offers a review of progress in image processing applications covered by the PDE framework, and updates the existing material. The book also provides programming tools for creating simulations with minimal effort.",
            "referenceCount": 0,
            "citationCount": 1380,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2010-11-19",
            "journal": {
                "name": null,
                "volume": "147"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Aubert2010MathematicalPI,\n author = {G. Aubert and Pierre Kornprobst},\n booktitle = {Applied Mathematical Sciences},\n pages = {I-XXXI, 1-377},\n title = {Mathematical problems in image processing - partial differential equations and the calculus of variations, 2nd Edition},\n volume = {147},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56ca1bcc0ee88770e86554ce54471130c9acf0e3",
            "@type": "ScholarlyArticle",
            "paperId": "56ca1bcc0ee88770e86554ce54471130c9acf0e3",
            "corpusId": 5388357,
            "url": "https://www.semanticscholar.org/paper/56ca1bcc0ee88770e86554ce54471130c9acf0e3",
            "title": "Human activity analysis",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1983705368",
                "DBLP": "journals/csur/AggarwalR11",
                "DOI": "10.1145/1922649.1922653",
                "CorpusId": 5388357
            },
            "abstract": "Human activity recognition is an important area of computer vision research. Its applications include surveillance systems, patient monitoring systems, and a variety of systems that involve interactions between persons and electronic devices such as human-computer interfaces. Most of these applications require an automated recognition of high-level activities, composed of multiple simple (or atomic) actions of persons. This article provides a detailed overview of various state-of-the-art research papers on human activity recognition. We discuss both the methodologies developed for simple human actions and those for high-level activities. An approach-based taxonomy is chosen that compares the advantages and limitations of each approach. Recognition methodologies for an analysis of the simple actions of a single person are first presented in the article. Space-time volume approaches and sequential approaches that represent and recognize activities directly from input images are discussed. Next, hierarchical recognition methodologies for high-level activities are presented and compared. Statistical approaches, syntactic approaches, and description-based approaches for hierarchical recognition are discussed in the article. In addition, we further discuss the papers on the recognition of human-object interactions and group activities. Public datasets designed for the evaluation of the recognition methodologies are illustrated in our article as well, comparing the methodologies' performances. This review will provide the impetus for future research in more productive areas.",
            "referenceCount": 109,
            "citationCount": 1431,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-04-01",
            "journal": {
                "name": "ACM Computing Surveys (CSUR)",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Aggarwal2011HumanAA,\n author = {J. Aggarwal and M. Ryoo},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys (CSUR)},\n pages = {1 - 43},\n title = {Human activity analysis},\n volume = {43},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9b686d76914befea66377ec79c1f9258d70ea7e3",
            "@type": "ScholarlyArticle",
            "paperId": "9b686d76914befea66377ec79c1f9258d70ea7e3",
            "corpusId": 2554264,
            "url": "https://www.semanticscholar.org/paper/9b686d76914befea66377ec79c1f9258d70ea7e3",
            "title": "ShapeNet: An Information-Rich 3D Model Repository",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2190691619",
                "ArXiv": "1512.03012",
                "DBLP": "journals/corr/ChangFGHHLSSSSX15",
                "CorpusId": 2554264
            },
            "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexed more than 3,000,000 models, 220,000 models out of which are classified into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.",
            "referenceCount": 41,
            "citationCount": 4084,
            "influentialCitationCount": 1070,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1512.03012"
            },
            "citationStyles": {
                "bibtex": "@Article{Chang2015ShapeNetAI,\n author = {Angel X. Chang and T. Funkhouser and L. Guibas and P. Hanrahan and Qi-Xing Huang and Zimo Li and S. Savarese and M. Savva and Shuran Song and Hao Su and Jianxiong Xiao and L. Yi and F. Yu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ShapeNet: An Information-Rich 3D Model Repository},\n volume = {abs/1512.03012},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd040c9f76d3b0b77e2065089b8d344c9b5d83d6",
            "@type": "ScholarlyArticle",
            "paperId": "bd040c9f76d3b0b77e2065089b8d344c9b5d83d6",
            "corpusId": 152282225,
            "url": "https://www.semanticscholar.org/paper/bd040c9f76d3b0b77e2065089b8d344c9b5d83d6",
            "title": "Object Detection in 20 Years: A Survey",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1905-05055",
                "MAG": "2944165510",
                "ArXiv": "1905.05055",
                "DOI": "10.1109/JPROC.2023.3238524",
                "CorpusId": 152282225
            },
            "abstract": "Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Over the past two decades, we have seen a rapid technological evolution of object detection and its profound impact on the entire computer vision field. If we consider today\u2019s object detection technique as a revolution driven by deep learning, then, back in the 1990s, we would see the ingenious thinking and long-term perspective design of early computer vision. This article extensively reviews this fast-moving research field in the light of technical evolution, spanning over a quarter-century\u2019s time (from the 1990s to 2022). A number of topics have been covered in this article, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speedup techniques, and recent state-of-the-art detection methods.",
            "referenceCount": 478,
            "citationCount": 1079,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.05055",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-05-13",
            "journal": {
                "name": "Proceedings of the IEEE",
                "volume": "111"
            },
            "citationStyles": {
                "bibtex": "@Article{Zou2019ObjectDI,\n author = {Zhengxia Zou and Zhenwei Shi and Yuhong Guo and Jieping Ye},\n booktitle = {Proceedings of the IEEE},\n journal = {Proceedings of the IEEE},\n pages = {257-276},\n title = {Object Detection in 20 Years: A Survey},\n volume = {111},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35812c1ee865369042ca1c35b44d9670c959075a",
            "@type": "ScholarlyArticle",
            "paperId": "35812c1ee865369042ca1c35b44d9670c959075a",
            "corpusId": 9562300,
            "url": "https://www.semanticscholar.org/paper/35812c1ee865369042ca1c35b44d9670c959075a",
            "title": "Visual servo control. I. Basic approaches",
            "venue": "IEEE robotics & automation magazine",
            "publicationVenue": {
                "id": "urn:research:bb803f8e-3f8e-4fd1-8192-391b7d4de1f1",
                "name": "IEEE robotics & automation magazine",
                "alternate_names": [
                    "IEEE robot  autom mag",
                    "IEEE Robotics & Automation Magazine",
                    "IEEE Robot  Autom Mag"
                ],
                "issn": "1070-9932",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=100"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2082991751",
                "DBLP": "journals/ram/ChaumetteH06",
                "DOI": "10.1109/MRA.2006.250573",
                "CorpusId": 9562300
            },
            "abstract": "This paper is the first of a two-part series on the topic of visual servo control using computer vision data in the servo loop to control the motion of a robot. In this paper, we describe the basic techniques that are by now well established in the field. We first give a general overview of the formulation of the visual servo control problem. We then describe the two archetypal visual servo control schemes: image-based and position-based visual servo control. Finally, we discuss performance and stability issues that pertain to these two schemes, motivating the second article in the series, in which we consider advanced techniques",
            "referenceCount": 23,
            "citationCount": 1914,
            "influentialCitationCount": 98,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2006-11-30",
            "journal": {
                "name": "IEEE Robotics & Automation Magazine",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Chaumette2006VisualSC,\n author = {F. Chaumette and S. Hutchinson},\n booktitle = {IEEE robotics & automation magazine},\n journal = {IEEE Robotics & Automation Magazine},\n pages = {82-90},\n title = {Visual servo control. I. Basic approaches},\n volume = {13},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f",
            "@type": "ScholarlyArticle",
            "paperId": "d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f",
            "corpusId": 232380284,
            "url": "https://www.semanticscholar.org/paper/d2a3bb6356d439146cd8d8e72dc728a1e3d93e7f",
            "title": "Understanding Robustness of Transformers for Image Classification",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2103-14586",
                "ArXiv": "2103.14586",
                "DOI": "10.1109/ICCV48922.2021.01007",
                "CorpusId": 232380284
            },
            "abstract": "Deep Convolutional Neural Networks (CNNs) have long been the architecture of choice for computer vision tasks. Recently, Transformer-based architectures like Vision Transformer (ViT) have matched or even surpassed ResNets for image classification. However, details of the Transformer architecture \u2013such as the use of non-overlapping patches\u2013 lead one to wonder whether these networks are as robust. In this paper, we perform an extensive study of a variety of different measures of robustness of ViT models and compare the findings to ResNet baselines. We investigate robustness to input perturbations as well as robustness to model perturbations. We find that when pre-trained with a sufficient amount of data, ViT models are at least as robust as the ResNet counterparts on a broad range of perturbations. We also find that Transformers are robust to the removal of almost any single layer, and that while activations from later layers are highly correlated with each other, they nevertheless play an important role in classification.",
            "referenceCount": 54,
            "citationCount": 250,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2103.14586",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-03-26",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bhojanapalli2021UnderstandingRO,\n author = {Srinadh Bhojanapalli and Ayan Chakrabarti and Daniel Glasner and Daliang Li and Thomas Unterthiner and Andreas Veit},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {10211-10221},\n title = {Understanding Robustness of Transformers for Image Classification},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628",
            "@type": "ScholarlyArticle",
            "paperId": "05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628",
            "corpusId": 29156801,
            "url": "https://www.semanticscholar.org/paper/05eb6eb4ea7d2b332295dfa5aeb64d5f47c1e628",
            "title": "The iNaturalist Species Classification and Detection Dataset",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/cvpr/HornASCSSAPB18",
                "MAG": "2950181225",
                "DOI": "10.1109/CVPR.2018.00914",
                "CorpusId": 29156801
            },
            "abstract": "Existing image classification datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier to photograph than others. To encourage further progress in challenging real world conditions we present the iNaturalist species classification and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been verified by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classification and detection models. Results show that current non-ensemble based methods achieve only 67% top one classification accuracy, illustrating the difficulty of the dataset. Specifically, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.",
            "referenceCount": 53,
            "citationCount": 1064,
            "influentialCitationCount": 75,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1707.06642",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-20",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Horn2017TheIS,\n author = {Grant Van Horn and Oisin Mac Aodha and Yang Song and Yin Cui and Chen Sun and Alexander Shepard and Hartwig Adam and P. Perona and Serge J. Belongie},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {8769-8778},\n title = {The iNaturalist Species Classification and Detection Dataset},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "@type": "ScholarlyArticle",
            "paperId": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "corpusId": 2845602,
            "url": "https://www.semanticscholar.org/paper/9008cdacbdcff8a218a6928e94fe7c6dfc237b24",
            "title": "Training support vector machines: an application to face detection",
            "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2124351082",
                "DBLP": "conf/cvpr/OsunaFG97",
                "DOI": "10.1109/CVPR.1997.609310",
                "CorpusId": 2845602
            },
            "abstract": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.",
            "referenceCount": 14,
            "citationCount": 2924,
            "influentialCitationCount": 112,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-06-17",
            "journal": {
                "name": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Osuna1997TrainingSV,\n author = {E. Osuna and R. Freund and F. Girosi},\n booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {130-136},\n title = {Training support vector machines: an application to face detection},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08c7127c98f27b2f5b9c9d0169f1bbb9517a4c28",
            "@type": "ScholarlyArticle",
            "paperId": "08c7127c98f27b2f5b9c9d0169f1bbb9517a4c28",
            "corpusId": 143380486,
            "url": "https://www.semanticscholar.org/paper/08c7127c98f27b2f5b9c9d0169f1bbb9517a4c28",
            "title": "Situated Cognition: On Human Knowledge and Computer Representations",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "1807695234",
                "CorpusId": 143380486
            },
            "abstract": "From the Publisher: \nThis book is about recent changes in the design of intelligent machines. New computer models of vision and navigation in animals suggest a different way to build machines. Cognition is viewed not just in terms of high-level \"expertise,\" but in terms of the ability to find one's way around the world, to learn new ways of seeing things, and to coordinate activity. This approach is called situated cognition. Situated Cognition differs from other purely philosophical treatises in that Clancey, who has built expert systems for twenty years, explores the limitations of existing computer programs and compares them to human memory and learning capabilities. He examines the implications of situated action from the perspective of artificial intelligence specialists interested in building robots and cognitive scientists seeking to relate descriptive models to neural and social views of knowledge.",
            "referenceCount": 0,
            "citationCount": 929,
            "influentialCitationCount": 67,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1997-08-28",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Clancey1997SituatedCO,\n author = {W. Clancey},\n title = {Situated Cognition: On Human Knowledge and Computer Representations},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:94c4ba7246f781632aa68ca5b1acff0fdbb2d92f",
            "@type": "ScholarlyArticle",
            "paperId": "94c4ba7246f781632aa68ca5b1acff0fdbb2d92f",
            "corpusId": 16970545,
            "url": "https://www.semanticscholar.org/paper/94c4ba7246f781632aa68ca5b1acff0fdbb2d92f",
            "title": "Using goal-driven deep learning models to understand sensory cortex",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2274405424",
                "DOI": "10.1038/nn.4244",
                "CorpusId": 16970545,
                "PubMed": "26906502"
            },
            "abstract": null,
            "referenceCount": 78,
            "citationCount": 1259,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2016-02-23",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Yamins2016UsingGD,\n author = {Daniel Yamins and J. DiCarlo},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {356-365},\n title = {Using goal-driven deep learning models to understand sensory cortex},\n volume = {19},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5298f435adfea49af3ef5b56bfea31a63c6f59af",
            "@type": "ScholarlyArticle",
            "paperId": "5298f435adfea49af3ef5b56bfea31a63c6f59af",
            "corpusId": 8629444,
            "url": "https://www.semanticscholar.org/paper/5298f435adfea49af3ef5b56bfea31a63c6f59af",
            "title": "Microsoft Kinect Sensor and Its Effect",
            "venue": "IEEE Multim.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/ieeemm/Zhang12",
                "MAG": "2056898157",
                "DOI": "10.1109/MMUL.2012.24",
                "CorpusId": 8629444
            },
            "abstract": "Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry.",
            "referenceCount": 11,
            "citationCount": 2267,
            "influentialCitationCount": 161,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-04-01",
            "journal": {
                "name": "IEEE Multim.",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2012MicrosoftKS,\n author = {Zhengyou Zhang},\n booktitle = {IEEE Multim.},\n journal = {IEEE Multim.},\n pages = {4-10},\n title = {Microsoft Kinect Sensor and Its Effect},\n volume = {19},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50e3e1b34d6d2ba4ad958e39e2805d13274d9f05",
            "@type": "ScholarlyArticle",
            "paperId": "50e3e1b34d6d2ba4ad958e39e2805d13274d9f05",
            "corpusId": 462228,
            "url": "https://www.semanticscholar.org/paper/50e3e1b34d6d2ba4ad958e39e2805d13274d9f05",
            "title": "Tangible bits: towards seamless interfaces between people, bits and atoms",
            "venue": "International Conference on Human Factors in Computing Systems",
            "publicationVenue": {
                "id": "urn:research:b55b50b1-aae7-47a7-b042-8aecc930073d",
                "name": "International Conference on Human Factors in Computing Systems",
                "alternate_names": [
                    "CHI",
                    "Int Conf Hum Factor Comput Syst",
                    "Human Factors in Computing Systems",
                    "Conference on Human Interface",
                    "Conf Hum Interface",
                    "Hum Factor Comput Syst"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigchi/"
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "conf/chi/IshiiU97",
                "MAG": "2149891956",
                "DOI": "10.1145/258549.258715",
                "CorpusId": 462228
            },
            "abstract": "This paper presents our vision of Human Computer Interaction (HCI): \"Tangible Bits.\" Tangible Bits allows users to \"grasp & manipulate\" bits in the center of users\u2019 attention by coupling the bits with everyday physical objects and architectural surfaces. Tangible Bits also enables users to be aware of background bits at the periphery of human perception using ambient display media such as light, sound, airflow, and water movement in an augmented space. The goal of Tangible Bits is to bridge the gaps between both cyberspace and the physical environment, as well as the foreground and background of human activities. This paper describes three key concepts of Tangible Bits: interactive surfaces; the coupling of bits with graspable physical objects; and ambient media for background awareness. We illustrate these concepts with three prototype systems \u2010 the metaDESK, transBOARD and ambientROOM \u2010 to identify underlying research issues.",
            "referenceCount": 27,
            "citationCount": 4326,
            "influentialCitationCount": 231,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "1997-03-27",
            "journal": {
                "name": "Proceedings of the ACM SIGCHI Conference on Human factors in computing systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ishii1997TangibleBT,\n author = {H. Ishii and Brygg Ullmer},\n booktitle = {International Conference on Human Factors in Computing Systems},\n journal = {Proceedings of the ACM SIGCHI Conference on Human factors in computing systems},\n title = {Tangible bits: towards seamless interfaces between people, bits and atoms},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7f065002afcb90240a41f05f138269c5675b9805",
            "@type": "ScholarlyArticle",
            "paperId": "7f065002afcb90240a41f05f138269c5675b9805",
            "corpusId": 645845,
            "url": "https://www.semanticscholar.org/paper/7f065002afcb90240a41f05f138269c5675b9805",
            "title": "On Human Motion Prediction Using Recurrent Neural Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951665584",
                "DBLP": "conf/cvpr/MartinezB017",
                "ArXiv": "1705.02445",
                "DOI": "10.1109/CVPR.2017.497",
                "CorpusId": 645845
            },
            "abstract": "Human motion modelling is a classical problem at the intersection of graphics and computer vision, with applications spanning human-computer interaction, motion synthesis, and motion prediction for virtual and augmented reality. Following the success of deep learning methods in several computer vision tasks, recent work has focused on using deep recurrent neural networks (RNNs) to model human motion, with the goal of learning time-dependent representations that perform tasks such as short-term motion prediction and long-term human motion synthesis. We examine recent work, with a focus on the evaluation methodologies commonly used in the literature, and show that, surprisingly, state of the art performance can be achieved by a simple baseline that does not attempt to model motion at all. We investigate this result, and analyze recent RNN methods by looking at the architectures, loss functions, and training procedures used in state-of-the-art approaches. We propose three changes to the standard RNN models typically used for human motion, which results in a simple and scalable RNN architecture that obtains state-of-the-art performance on human motion prediction.",
            "referenceCount": 53,
            "citationCount": 746,
            "influentialCitationCount": 184,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1705.02445",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-06",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Martinez2017OnHM,\n author = {Julieta Martinez and Michael J. Black and J. Romero},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4674-4683},\n title = {On Human Motion Prediction Using Recurrent Neural Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:20f0357688876fa4662f806f985779dce6e24f3c",
            "@type": "ScholarlyArticle",
            "paperId": "20f0357688876fa4662f806f985779dce6e24f3c",
            "corpusId": 6138085,
            "url": "https://www.semanticscholar.org/paper/20f0357688876fa4662f806f985779dce6e24f3c",
            "title": "Transforming Auto-Encoders",
            "venue": "International Conference on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                "name": "International Conference on Artificial Neural Networks",
                "alternate_names": [
                    "Int Conf Artif Neural Netw",
                    "ICANN"
                ],
                "issn": null,
                "url": "http://www.e-nns.org/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2966661",
                "DBLP": "conf/icann/HintonKW11",
                "DOI": "10.1007/978-3-642-21735-7_6",
                "CorpusId": 6138085
            },
            "abstract": null,
            "referenceCount": 18,
            "citationCount": 1052,
            "influentialCitationCount": 79,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.toronto.edu/%7Ehinton/absps/transauto6.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hinton2011TransformingA,\n author = {Geoffrey E. Hinton and A. Krizhevsky and Sida I. Wang},\n booktitle = {International Conference on Artificial Neural Networks},\n pages = {44-51},\n title = {Transforming Auto-Encoders},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15a148957469bc8b91bd7cc31aa1f0c6584a1571",
            "@type": "ScholarlyArticle",
            "paperId": "15a148957469bc8b91bd7cc31aa1f0c6584a1571",
            "corpusId": 21727516,
            "url": "https://www.semanticscholar.org/paper/15a148957469bc8b91bd7cc31aa1f0c6584a1571",
            "title": "DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1805.06561",
                "MAG": "2963482797",
                "DBLP": "journals/corr/abs-1805-06561",
                "DOI": "10.1109/CVPRW.2018.00031",
                "CorpusId": 21727516
            },
            "abstract": "We present the DeepGlobe 2018 Satellite Image Understanding Challenge, which includes three public competitions for segmentation, detection, and classification tasks on satellite images (Figure 1). Similar to other challenges in computer vision domain such as DAVIS[21] and COCO[33], DeepGlobe proposes three datasets and corresponding evaluation methodologies, coherently bundled in three competitions with a dedicated workshop co-located with CVPR 2018. We observed that satellite imagery is a rich and structured source of information, yet it is less investigated than everyday images by computer vision researchers. However, bridging modern computer vision with remote sensing data analysis could have critical impact to the way we understand our environment and lead to major breakthroughs in global urban planning or climate change research. Keeping such bridging objective in mind, DeepGlobe aims to bring together researchers from different domains to raise awareness of remote sensing in the computer vision community and vice-versa. We aim to improve and evaluate state-of-the-art satellite image understanding approaches, which can hopefully serve as reference benchmarks for future research in the same topic. In this paper, we analyze characteristics of each dataset, define the evaluation criteria of the competitions, and provide baselines for each task.",
            "referenceCount": 60,
            "citationCount": 593,
            "influentialCitationCount": 95,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/125668/2/Demir_DeepGlobe_2018_A_CVPR_2018_paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geography",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-05-17",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Demir2018DeepGlobe2A,\n author = {Ilke Demir and K. Koperski and David Lindenbaum and Guan Pang and Jing Huang and Saikat Basu and Forest Hughes and D. Tuia and R. Raskar},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {172-17209},\n title = {DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:727f736f07d9b0fd5ad95208079a09ee506e99e2",
            "@type": "ScholarlyArticle",
            "paperId": "727f736f07d9b0fd5ad95208079a09ee506e99e2",
            "corpusId": 2604670,
            "url": "https://www.semanticscholar.org/paper/727f736f07d9b0fd5ad95208079a09ee506e99e2",
            "title": "A Validity Measure for Fuzzy Clustering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1991,
            "externalIds": {
                "DBLP": "journals/pami/XieB91",
                "MAG": "1996747841",
                "DOI": "10.1109/34.85677",
                "CorpusId": 2604670
            },
            "abstract": "The authors present a fuzzy validity criterion based on a validity function which identifies compact and separate fuzzy c-partitions without assumptions as to the number of substructures inherent in the data. This function depends on the data set, geometric distance measure, distance between cluster centroids and more importantly on the fuzzy partition generated by any fuzzy algorithm used. The function is mathematically justified via its relationship to a well-defined hard clustering validity function, the separation index for which the condition of uniqueness has already been established. The performance of this validity function compares favorably to that of several others. The application of this validity function to color image segmentation in a computer color vision system for recognition of IC wafer defects which are otherwise impossible to detect using gray-scale image processing is discussed. >",
            "referenceCount": 20,
            "citationCount": 3353,
            "influentialCitationCount": 221,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1991-08-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Xie1991AVM,\n author = {X. Xie and G. Beni},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {841-847},\n title = {A Validity Measure for Fuzzy Clustering},\n volume = {13},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "@type": "ScholarlyArticle",
            "paperId": "62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "corpusId": 8289133,
            "url": "https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e",
            "title": "Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2463955103",
                "DBLP": "journals/corr/VinyalsTBE16",
                "ArXiv": "1609.06647",
                "DOI": "10.1109/TPAMI.2016.2587640",
                "CorpusId": 8289133,
                "PubMed": "28055847"
            },
            "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.",
            "referenceCount": 55,
            "citationCount": 766,
            "influentialCitationCount": 104,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1609.06647",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-09-21",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2016ShowAT,\n author = {Oriol Vinyals and Alexander Toshev and Samy Bengio and D. Erhan},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {652-663},\n title = {Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge},\n volume = {39},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "@type": "ScholarlyArticle",
            "paperId": "b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "corpusId": 7788290,
            "url": "https://www.semanticscholar.org/paper/b4e4b41b6010ac1e6c90791168f57bcd75b696ab",
            "title": "The Visual Analysis of Human Movement: A Survey",
            "venue": "Computer Vision and Image Understanding",
            "publicationVenue": {
                "id": "urn:research:5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                "name": "Computer Vision and Image Understanding",
                "alternate_names": [
                    "Comput Vis Image Underst"
                ],
                "issn": "1077-3142",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2121899951",
                "DBLP": "journals/cviu/Gavrila99",
                "DOI": "10.1006/cviu.1998.0716",
                "CorpusId": 7788290
            },
            "abstract": "The ability to recognize humans and their activities by vision is key for a machine to interact intelligently and effortlessly with a human-inhabited environment. Because of many potentially important applications, \u201clooking at people\u201d is currently one of the most active application domains in computer vision. This survey identifies a number of promising applications and provides an overview of recent developments in this domain. The scope of this survey is limited to work on whole-body or hand motion; it does not include work on human faces. The emphasis is on discussing the various methodologies; they are grouped in 2-D approaches with or without explicit shape models and 3-D approaches. Where appropriate, systems are reviewed. We conclude with some thoughts about future directions.",
            "referenceCount": 93,
            "citationCount": 2231,
            "influentialCitationCount": 68,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.jhu.edu/~hager/Public/teaching/CS600.641/human_motion_survey.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Comput. Vis. Image Underst.",
                "volume": "73"
            },
            "citationStyles": {
                "bibtex": "@Article{Gavrila1999TheVA,\n author = {D. Gavrila},\n booktitle = {Computer Vision and Image Understanding},\n journal = {Comput. Vis. Image Underst.},\n pages = {82-98},\n title = {The Visual Analysis of Human Movement: A Survey},\n volume = {73},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8c2a9f6c7721d3f39ed13be8ef1bb80670ed2282",
            "@type": "ScholarlyArticle",
            "paperId": "8c2a9f6c7721d3f39ed13be8ef1bb80670ed2282",
            "corpusId": 12507257,
            "url": "https://www.semanticscholar.org/paper/8c2a9f6c7721d3f39ed13be8ef1bb80670ed2282",
            "title": "EMNIST: an extension of MNIST to handwritten letters",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/CohenATS17",
                "ArXiv": "1702.05373",
                "MAG": "2590796488",
                "CorpusId": 12507257
            },
            "abstract": "The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.",
            "referenceCount": 20,
            "citationCount": 606,
            "influentialCitationCount": 142,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.05373"
            },
            "citationStyles": {
                "bibtex": "@Article{Cohen2017EMNISTAE,\n author = {Gregory Cohen and Saeed Afshar and J. Tapson and A. Schaik},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {ArXiv},\n title = {EMNIST: an extension of MNIST to handwritten letters},\n volume = {abs/1702.05373},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0f17f99c44807762f2a386ac6579c364330e082",
            "@type": "ScholarlyArticle",
            "paperId": "c0f17f99c44807762f2a386ac6579c364330e082",
            "corpusId": 8574504,
            "url": "https://www.semanticscholar.org/paper/c0f17f99c44807762f2a386ac6579c364330e082",
            "title": "A Review on Deep Learning Techniques Applied to Semantic Segmentation",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Garcia-GarciaOO17",
                "ArXiv": "1704.06857",
                "MAG": "2609077090",
                "CorpusId": 8574504
            },
            "abstract": "Image semantic segmentation is more and more being of interest for computer vision and machine learning researchers. Many applications on the rise need accurate and efficient segmentation mechanisms: autonomous driving, indoor navigation, and even virtual or augmented reality systems to name a few. This demand coincides with the rise of deep learning approaches in almost every field or application target related to computer vision, including semantic segmentation or scene understanding. This paper provides a review on deep learning methods for semantic segmentation applied to various application areas. Firstly, we describe the terminology of this field as well as mandatory background concepts. Next, the main datasets and challenges are exposed to help researchers decide which are the ones that best suit their needs and their targets. Then, existing methods are reviewed, highlighting their contributions and their significance in the field. Finally, quantitative results are given for the described methods and the datasets in which they were evaluated, following up with a discussion of the results. At last, we point out a set of promising future works and draw our own conclusions about the state of the art of semantic segmentation using deep learning techniques.",
            "referenceCount": 116,
            "citationCount": 1090,
            "influentialCitationCount": 49,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-04-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.06857"
            },
            "citationStyles": {
                "bibtex": "@Article{Garcia-Garcia2017ARO,\n author = {Alberto Garcia-Garcia and Sergio Orts and Sergiu Oprea and Victor Villena-Martinez and J. G. Rodr\u00edguez},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Review on Deep Learning Techniques Applied to Semantic Segmentation},\n volume = {abs/1704.06857},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f115fffa3a2af837cf869996e76b805e8f8cea4",
            "@type": "ScholarlyArticle",
            "paperId": "6f115fffa3a2af837cf869996e76b805e8f8cea4",
            "corpusId": 5544227,
            "url": "https://www.semanticscholar.org/paper/6f115fffa3a2af837cf869996e76b805e8f8cea4",
            "title": "Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2950025457",
                "ArXiv": "1505.05641",
                "DBLP": "conf/iccv/SuQLG15",
                "DOI": "10.1109/ICCV.2015.308",
                "CorpusId": 5544227
            },
            "abstract": "Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we propose a framework to address both issues by combining render-based image synthesis and CNNs (Convolutional Neural Networks). We believe that 3D models have the potential in generating a large number of images of high variation, which can be well exploited by deep CNN with a high learning capacity. Towards this goal, we propose a scalable and overfit-resistant image synthesis pipeline, together with a novel CNN specifically tailored for the viewpoint estimation task. Experimentally, we show that the viewpoint estimation from our pipeline can significantly outperform state-of-the-art methods on PASCAL 3D+ benchmark.",
            "referenceCount": 41,
            "citationCount": 697,
            "influentialCitationCount": 96,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1505.05641",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-21",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Su2015RenderFC,\n author = {Hao Su and C. Qi and Yangyan Li and L. Guibas},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2686-2694},\n title = {Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2adac8e7ad1818d6a1fbc19ecf858581d9a5df9f",
            "@type": "ScholarlyArticle",
            "paperId": "2adac8e7ad1818d6a1fbc19ecf858581d9a5df9f",
            "corpusId": 7230571,
            "url": "https://www.semanticscholar.org/paper/2adac8e7ad1818d6a1fbc19ecf858581d9a5df9f",
            "title": "Image Representation Using 2D Gabor Wavelets",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1996,
            "externalIds": {
                "DBLP": "journals/pami/Lee96a",
                "MAG": "2138584058",
                "DOI": "10.1109/34.541406",
                "CorpusId": 7230571
            },
            "abstract": "This paper extends to two dimensions the frame criterion developed by Daubechies for one-dimensional wavelets, and it computes the frame bounds for the particular case of 2D Gabor wavelets. Completeness criteria for 2D Gabor image representations are important because of their increasing role in many computer vision applications and also in modeling biological vision, since recent neurophysiological evidence from the visual cortex of mammalian brains suggests that the filter response profiles of the main class of linearly-responding cortical neurons (called simple cells) are best modeled as a family of self-similar 2D Gabor wavelets. We therefore derive the conditions under which a set of continuous 2D Gabor wavelets will provide a complete representation of any image, and we also find self-similar wavelet parametrization which allow stable reconstruction by summation as though the wavelets formed an orthonormal basis. Approximating a \"tight frame\" generates redundancy which allows low-resolution neural responses to represent high-resolution images.",
            "referenceCount": 44,
            "citationCount": 1821,
            "influentialCitationCount": 118,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cnbc.cmu.edu/~tai/papers/pami.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1996-10-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "18"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee1996ImageRU,\n author = {T. Lee},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {959-971},\n title = {Image Representation Using 2D Gabor Wavelets},\n volume = {18},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5cb6700d94c6118ee13f4f4fecac99f111189812",
            "@type": "ScholarlyArticle",
            "paperId": "5cb6700d94c6118ee13f4f4fecac99f111189812",
            "corpusId": 10116609,
            "url": "https://www.semanticscholar.org/paper/5cb6700d94c6118ee13f4f4fecac99f111189812",
            "title": "Baby talk: Understanding and generating simple image descriptions",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/pami/KulkarniPODLCBB13",
                "MAG": "1969616664",
                "DOI": "10.1109/CVPR.2011.5995466",
                "CorpusId": 10116609,
                "PubMed": "22848128"
            },
            "abstract": "We posit that visually descriptive language offers computer vision researchers both information about the world, and information about how people describe the world. The potential benefit from this source is made more significant due to the enormous amount of language data easily available today. We present a system to automatically generate natural language descriptions from images that exploits both statistics gleaned from parsing large quantities of text data and recognition algorithms from computer vision. The system is very effective at producing relevant sentences for images. It also generates descriptions that are notably more true to the specific image content than previous work.",
            "referenceCount": 59,
            "citationCount": 1078,
            "influentialCitationCount": 55,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://acberg.com/papers/baby_talk.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-20",
            "journal": {
                "name": "CVPR 2011",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kulkarni2011BabyTU,\n author = {Girish Kulkarni and Visruth Premraj and Sagnik Dhar and Siming Li and Yejin Choi and A. Berg and Tamara L. Berg},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {CVPR 2011},\n pages = {1601-1608},\n title = {Baby talk: Understanding and generating simple image descriptions},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4998462014c907c519ae801af39cf58a4c538bc9",
            "@type": "ScholarlyArticle",
            "paperId": "4998462014c907c519ae801af39cf58a4c538bc9",
            "corpusId": 3345006,
            "url": "https://www.semanticscholar.org/paper/4998462014c907c519ae801af39cf58a4c538bc9",
            "title": "Deformable Model Fitting by Regularized Landmark Mean-Shift",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/ijcv/SaragihLC11",
                "MAG": "2076017598",
                "DOI": "10.1007/s11263-010-0380-4",
                "CorpusId": 3345006
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 893,
            "influentialCitationCount": 123,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "91"
            },
            "citationStyles": {
                "bibtex": "@Article{Saragih2010DeformableMF,\n author = {Jason M. Saragih and S. Lucey and J. Cohn},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {200-215},\n title = {Deformable Model Fitting by Regularized Landmark Mean-Shift},\n volume = {91},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4543670c4b2d88a9b67525e0084044adef94ae76",
            "@type": "ScholarlyArticle",
            "paperId": "4543670c4b2d88a9b67525e0084044adef94ae76",
            "corpusId": 206592585,
            "url": "https://www.semanticscholar.org/paper/4543670c4b2d88a9b67525e0084044adef94ae76",
            "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/cvpr/NguyenYC15",
                "MAG": "2951220338",
                "ArXiv": "1412.1897",
                "DOI": "10.1109/CVPR.2015.7298640",
                "CorpusId": 206592585
            },
            "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call \u201cfooling images\u201d (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.",
            "referenceCount": 43,
            "citationCount": 2877,
            "influentialCitationCount": 109,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://yosinski.com/media/papers/Nguyen__2014__arXiv__Deep_Neural_Networks_are_Easily_Fooled.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-12-04",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nguyen2014DeepNN,\n author = {Anh M Nguyen and J. Yosinski and J. Clune},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {427-436},\n title = {Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15168665f4b8eb11466086e69780ed98e5280059",
            "@type": "ScholarlyArticle",
            "paperId": "15168665f4b8eb11466086e69780ed98e5280059",
            "corpusId": 4547917,
            "url": "https://www.semanticscholar.org/paper/15168665f4b8eb11466086e69780ed98e5280059",
            "title": "Generate to Adapt: Aligning Domains Using Generative Adversarial Networks",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Sankaranarayanan17a",
                "ArXiv": "1704.01705",
                "MAG": "2950634055",
                "DOI": "10.1109/CVPR.2018.00887",
                "CorpusId": 4547917
            },
            "abstract": "Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.",
            "referenceCount": 44,
            "citationCount": 602,
            "influentialCitationCount": 67,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1704.01705",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-06",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sankaranarayanan2017GenerateTA,\n author = {S. Sankaranarayanan and Y. Balaji and C. Castillo and R. Chellappa},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {8503-8512},\n title = {Generate to Adapt: Aligning Domains Using Generative Adversarial Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "@type": "ScholarlyArticle",
            "paperId": "7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "corpusId": 8081284,
            "url": "https://www.semanticscholar.org/paper/7e232313a59d735ef7c8a9f4cc7bc980a29deb5e",
            "title": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "3016211260",
                "DBLP": "conf/cvpr/GoyalKSBP17",
                "ArXiv": "1612.00837",
                "DOI": "10.1007/s11263-018-1116-0",
                "CorpusId": 8081284
            },
            "abstract": null,
            "referenceCount": 154,
            "citationCount": 1967,
            "influentialCitationCount": 439,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.00837",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-02",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "127"
            },
            "citationStyles": {
                "bibtex": "@Article{Goyal2016MakingTV,\n author = {Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {398 - 414},\n title = {Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},\n volume = {127},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e2da7c1c7dfc7960d1515b61f32fdc55359eea7",
            "@type": "ScholarlyArticle",
            "paperId": "3e2da7c1c7dfc7960d1515b61f32fdc55359eea7",
            "corpusId": 11897587,
            "url": "https://www.semanticscholar.org/paper/3e2da7c1c7dfc7960d1515b61f32fdc55359eea7",
            "title": "An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2339460098",
                "DBLP": "journals/corr/HughesS15",
                "ArXiv": "1511.08060",
                "CorpusId": 11897587
            },
            "abstract": "Human society needs to increase food production by an estimated 70% by 2050 to feed an expected population size that is predicted to be over 9 billion people. Currently infectious diseases reduce the potential yield by an average of 40% with many farmers in the developing world experiencing yield losses as high as 100%. Infectious diseases of crops are not new and historic examples such as the Irish Potato Famine of 1845-49 demonstrate this. But what is new is the widespread distribution of smartphones among crop growers around the world with an expected 5 billion smartphones by 2020. This offers the potential of turning the smartphone into a valuable tool for diverse communities growing food. One potential application is the development of mobile disease diagnostics through machine learning and crowdsourcing. Computer vision and machine learning have shown their potential to automatically classify images. To do this for plant diseases requires a training set that facilitates the development of the algorithms. Here we announce the release of >50,000 expertly curated images on healthy and infected leaves of crops plants through the existing platform www.PlantVillage.org. We describe both the data and the platform. These data are the beginning of an on-going, crowdsourcing effort to enable computer vision approaches to help solve the problem of yield losses in crop plants due to infectious diseases.",
            "referenceCount": 35,
            "citationCount": 711,
            "influentialCitationCount": 81,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Agricultural and Food Sciences",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-25",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.08060"
            },
            "citationStyles": {
                "bibtex": "@Article{Hughes2015AnOA,\n author = {David P. Hughes and M. Salath\u00e9},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An open access repository of images on plant health to enable the development of mobile disease diagnostics through machine learning and crowdsourcing},\n volume = {abs/1511.08060},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0596447b7d1c8d72e67ca22a2d5b859021ec509d",
            "@type": "ScholarlyArticle",
            "paperId": "0596447b7d1c8d72e67ca22a2d5b859021ec509d",
            "corpusId": 215753783,
            "url": "https://www.semanticscholar.org/paper/0596447b7d1c8d72e67ca22a2d5b859021ec509d",
            "title": "Local Binary Patterns and Its Application to Facial Image Analysis: A Survey",
            "venue": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
            "publicationVenue": {
                "id": "urn:research:ecb11fdd-9e59-482f-a3b6-0cb14372306c",
                "name": "IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)",
                "alternate_names": [
                    "IEEE Trans Syst Man Cybern Part C (applications Rev"
                ],
                "issn": "1094-6977",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=5326"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2096171208",
                "DBLP": "journals/tsmc/HuangSAWC11",
                "DOI": "10.1109/TSMCC.2011.2118750",
                "CorpusId": 215753783
            },
            "abstract": "Local binary pattern (LBP) is a nonparametric descriptor, which efficiently summarizes the local structures of images. In recent years, it has aroused increasing interest in many areas of image processing and computer vision and has shown its effectiveness in a number of applications, in particular for facial image analysis, including tasks as diverse as face detection, face recognition, facial expression analysis, and demographic classification. This paper presents a comprehensive survey of LBP methodology, including several more recent variations. As a typical application of the LBP approach, LBP-based facial image analysis is extensively reviewed, while its successful extensions, which deal with various tasks of facial image analysis, are also highlighted.",
            "referenceCount": 167,
            "citationCount": 926,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.archives-ouvertes.fr/hal-01354386/file/Liris-5004.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-11-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)",
                "volume": "41"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2011LocalBP,\n author = {Di Huang and C. Shan and M. Ardabilian and Yunhong Wang and Liming Chen},\n booktitle = {IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},\n pages = {765-781},\n title = {Local Binary Patterns and Its Application to Facial Image Analysis: A Survey},\n volume = {41},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8cfb50e6e02cdfb35b0c1571039b41eb6a6b64bf",
            "@type": "ScholarlyArticle",
            "paperId": "8cfb50e6e02cdfb35b0c1571039b41eb6a6b64bf",
            "corpusId": 229430,
            "url": "https://www.semanticscholar.org/paper/8cfb50e6e02cdfb35b0c1571039b41eb6a6b64bf",
            "title": "Shape, Illumination, and Reflectance from Shading",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/abs-2010-03592",
                "MAG": "3091778590",
                "ArXiv": "2010.03592",
                "DOI": "10.1109/TPAMI.2014.2377712",
                "CorpusId": 229430,
                "PubMed": "26353003"
            },
            "abstract": "A fundamental problem in computer vision is that of inferring the intrinsic, 3D structure of the world from flat, 2D images of that world. Traditional methods for recovering scene properties such as shape, reflectance, or illumination rely on multiple observations of the same scene to overconstrain the problem. Recovering these same properties from a single image seems almost impossible in comparison-there are an infinite number of shapes, paint, and lights that exactly reproduce a single image. However, certain explanations are more likely than others: surfaces tend to be smooth, paint tends to be uniform, and illumination tends to be natural. We therefore pose this problem as one of statistical inference, and define an optimization problem that searches for the most likely explanation of a single image. Our technique can be viewed as a superset of several classic computer vision problems (shape-from-shading, intrinsic images, color constancy, illumination estimation, etc) and outperforms all previous solutions to those constituent problems.",
            "referenceCount": 72,
            "citationCount": 647,
            "influentialCitationCount": 80,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2010.03592",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-08-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Barron2015ShapeIA,\n author = {Jonathan T. Barron and Jitendra Malik},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1670-1687},\n title = {Shape, Illumination, and Reflectance from Shading},\n volume = {37},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af0a2f6300b1888058b45fc96fe5bc4f4ca4302a",
            "@type": "ScholarlyArticle",
            "paperId": "af0a2f6300b1888058b45fc96fe5bc4f4ca4302a",
            "corpusId": 3027134,
            "url": "https://www.semanticscholar.org/paper/af0a2f6300b1888058b45fc96fe5bc4f4ca4302a",
            "title": "Graph Cuts and Efficient N-D Image Segmentation",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2119300483",
                "DBLP": "journals/ijcv/BoykovF06",
                "DOI": "10.1007/s11263-006-7934-5",
                "CorpusId": 3027134
            },
            "abstract": null,
            "referenceCount": 86,
            "citationCount": 2158,
            "influentialCitationCount": 178,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.csd.uwo.ca/~yuri/Papers/ijcv06.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-11-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "70"
            },
            "citationStyles": {
                "bibtex": "@Article{Boykov2006GraphCA,\n author = {Yuri Boykov and G. Funka-Lea},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {109-131},\n title = {Graph Cuts and Efficient N-D Image Segmentation},\n volume = {70},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2e2f7dbb9eb7b46e054e15158a79c47f2a97f725",
            "@type": "ScholarlyArticle",
            "paperId": "2e2f7dbb9eb7b46e054e15158a79c47f2a97f725",
            "corpusId": 60561998,
            "url": "https://www.semanticscholar.org/paper/2e2f7dbb9eb7b46e054e15158a79c47f2a97f725",
            "title": "Fuzzy Models and Algorithms for Pattern Recognition and Image Processing",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1508853606",
                "DOI": "10.1007/b106267",
                "CorpusId": 60561998
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 1431,
            "influentialCitationCount": 103,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-08-31",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bezdek1999FuzzyMA,\n author = {J. Bezdek and Mikhil R. Pal and J. Keller and R. Krisnapuram},\n title = {Fuzzy Models and Algorithms for Pattern Recognition and Image Processing},\n year = {1999}\n}\n"
            }
        }
    }
]