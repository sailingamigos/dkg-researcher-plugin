[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7daebb130b491b1ae8afff0e4e9b5f5302e2d996",
            "@type": "ScholarlyArticle",
            "paperId": "7daebb130b491b1ae8afff0e4e9b5f5302e2d996",
            "corpusId": 13443906,
            "url": "https://www.semanticscholar.org/paper/7daebb130b491b1ae8afff0e4e9b5f5302e2d996",
            "title": "Rolling Guidance Filter",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2231495490",
                "DBLP": "conf/eccv/ZhangSXJ14",
                "DOI": "10.1007/978-3-319-10578-9_53",
                "CorpusId": 13443906
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 529,
            "influentialCitationCount": 112,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_53.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2014RollingGF,\n author = {Qi Zhang and Xiaoyong Shen and Li Xu and Jiaya Jia},\n booktitle = {European Conference on Computer Vision},\n pages = {815-830},\n title = {Rolling Guidance Filter},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1e1a39534aed2cf4894896de75d0ba1fa75ab6a",
            "@type": "ScholarlyArticle",
            "paperId": "e1e1a39534aed2cf4894896de75d0ba1fa75ab6a",
            "corpusId": 3131710,
            "url": "https://www.semanticscholar.org/paper/e1e1a39534aed2cf4894896de75d0ba1fa75ab6a",
            "title": "Fast texture synthesis using tree-structured vector quantization",
            "venue": "International Conference on Computer Graphics and Interactive Techniques",
            "publicationVenue": {
                "id": "urn:research:cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                "name": "International Conference on Computer Graphics and Interactive Techniques",
                "alternate_names": [
                    "Int Conf Comput Graph Interact Tech",
                    "SIGGRAPH"
                ],
                "issn": null,
                "url": "http://www.siggraph.org/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2232702494",
                "DBLP": "conf/siggraph/WeiL00",
                "DOI": "10.1145/344779.345009",
                "CorpusId": 3131710
            },
            "abstract": "Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.",
            "referenceCount": 33,
            "citationCount": 1656,
            "influentialCitationCount": 126,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://graphics.stanford.edu/papers/texture-synthesis-sig00/texture_review.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2000-07-01",
            "journal": {
                "name": "Proceedings of the 27th annual conference on Computer graphics and interactive techniques",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wei2000FastTS,\n author = {Li-Yi Wei and M. Levoy},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {Proceedings of the 27th annual conference on Computer graphics and interactive techniques},\n title = {Fast texture synthesis using tree-structured vector quantization},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8d0165b52b1b4f5598776e8b4ff01ee98fb439bd",
            "@type": "ScholarlyArticle",
            "paperId": "8d0165b52b1b4f5598776e8b4ff01ee98fb439bd",
            "corpusId": 1522654,
            "url": "https://www.semanticscholar.org/paper/8d0165b52b1b4f5598776e8b4ff01ee98fb439bd",
            "title": "Algorithms for Finding Global Minimizers of Image Segmentation and Denoising Models",
            "venue": "SIAM Journal on Applied Mathematics",
            "publicationVenue": {
                "id": "urn:research:9b3052dd-9e29-41cb-927c-28df9e08a68b",
                "name": "SIAM Journal on Applied Mathematics",
                "alternate_names": [
                    "SIAM J Appl Math",
                    "Siam Journal on Applied Mathematics",
                    "Siam J Appl Math"
                ],
                "issn": "0036-1399",
                "url": "https://www.jstor.org/journal/siamjapplmath"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2038495454",
                "DBLP": "journals/siamam/NikolovaEC06",
                "DOI": "10.1137/040615286",
                "CorpusId": 1522654
            },
            "abstract": "We show how certain nonconvex optimization problems that arise in image processing and computer vision can be restated as convex minimization problems. This allows, in particular, the finding of global minimizers via standard convex minimization schemes.",
            "referenceCount": 28,
            "citationCount": 1155,
            "influentialCitationCount": 140,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "SIAM J. Appl. Math.",
                "volume": "66"
            },
            "citationStyles": {
                "bibtex": "@Article{Chan2006AlgorithmsFF,\n author = {T. Chan and S. Esedoglu and M. Nikolova},\n booktitle = {SIAM Journal on Applied Mathematics},\n journal = {SIAM J. Appl. Math.},\n pages = {1632-1648},\n title = {Algorithms for Finding Global Minimizers of Image Segmentation and Denoising Models},\n volume = {66},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:887567782cb859ecd339693589056903b0071353",
            "@type": "ScholarlyArticle",
            "paperId": "887567782cb859ecd339693589056903b0071353",
            "corpusId": 15724653,
            "url": "https://www.semanticscholar.org/paper/887567782cb859ecd339693589056903b0071353",
            "title": "Face Detection: A Survey",
            "venue": "Computer Vision and Image Understanding",
            "publicationVenue": {
                "id": "urn:research:5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                "name": "Computer Vision and Image Understanding",
                "alternate_names": [
                    "Comput Vis Image Underst"
                ],
                "issn": "1077-3142",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2163965432",
                "DBLP": "journals/cviu/HjelmasL01",
                "DOI": "10.1006/cviu.2001.0921",
                "CorpusId": 15724653
            },
            "abstract": "In this paper we present a comprehensive and critical survey of face detection algorithms. Face detection is a necessary first-step in face recognition systems, with the purpose of localizing and extracting the face region from the background. It also has several applications in areas such as content-based image retrieval, video coding, video conferencing, crowd surveillance, and intelligent human?computer interfaces. However, it was not until recently that the face detection problem received considerable attention among researchers. The human face is a dynamic object and has a high degree of variability in its apperance, which makes face detection a difficult problem in computer vision. A wide variety of techniques have been proposed, ranging from simple edge-based algorithms to composite high-level approaches utilizing advanced pattern recognition methods. The algorithms presented in this paper are classified as either feature-based or image-based and are discussed in terms of their technical approach and performance. Due to the lack of standardized tests, we do not provide a comprehensive comparative evaluation, but in cases where results are reported on common datasets, comparisons are presented. We also give a presentation of some proposed applications and possible application areas.",
            "referenceCount": 291,
            "citationCount": 1687,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2001-09-01",
            "journal": {
                "name": "Comput. Vis. Image Underst.",
                "volume": "83"
            },
            "citationStyles": {
                "bibtex": "@Article{Hjelm\u00e5s2001FaceDA,\n author = {Erik Hjelm\u00e5s and B. K. Low},\n booktitle = {Computer Vision and Image Understanding},\n journal = {Comput. Vis. Image Underst.},\n pages = {236-274},\n title = {Face Detection: A Survey},\n volume = {83},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f91ef3b3f71196def8bee7a7bacdc62cd07b64c",
            "@type": "ScholarlyArticle",
            "paperId": "2f91ef3b3f71196def8bee7a7bacdc62cd07b64c",
            "corpusId": 11157572,
            "url": "https://www.semanticscholar.org/paper/2f91ef3b3f71196def8bee7a7bacdc62cd07b64c",
            "title": "Face Detection without Bells and Whistles",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/eccv/MathiasBPG14",
                "MAG": "1849007038",
                "DOI": "10.1007/978-3-319-10593-2_47",
                "CorpusId": 11157572
            },
            "abstract": null,
            "referenceCount": 39,
            "citationCount": 576,
            "influentialCitationCount": 91,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mathias2014FaceDW,\n author = {Markus Mathias and Rodrigo Benenson and M. Pedersoli and L. Gool},\n booktitle = {European Conference on Computer Vision},\n pages = {720-735},\n title = {Face Detection without Bells and Whistles},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d517b13f2b152c913b81ce534a149493517dbdad",
            "@type": "ScholarlyArticle",
            "paperId": "d517b13f2b152c913b81ce534a149493517dbdad",
            "corpusId": 10158224,
            "url": "https://www.semanticscholar.org/paper/d517b13f2b152c913b81ce534a149493517dbdad",
            "title": "Big Data Deep Learning: Challenges and Perspectives",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/access/ChenL14",
                "MAG": "1984020445",
                "DOI": "10.1109/ACCESS.2014.2325029",
                "CorpusId": 10158224
            },
            "abstract": "Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.",
            "referenceCount": 115,
            "citationCount": 973,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-05-16",
            "journal": {
                "name": "IEEE Access",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2014BigDD,\n author = {Xue-wen Chen and Xiaotong Lin},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {514-525},\n title = {Big Data Deep Learning: Challenges and Perspectives},\n volume = {2},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:254be2055a84c4d80c4c8eb8e6090b3977cc6fb6",
            "@type": "ScholarlyArticle",
            "paperId": "254be2055a84c4d80c4c8eb8e6090b3977cc6fb6",
            "corpusId": 825395,
            "url": "https://www.semanticscholar.org/paper/254be2055a84c4d80c4c8eb8e6090b3977cc6fb6",
            "title": "Saliency, Scale and Image Description",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "1699734612",
                "DBLP": "journals/ijcv/KadirB01",
                "DOI": "10.1023/A:1012460413855",
                "CorpusId": 825395
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 1286,
            "influentialCitationCount": 67,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-11-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Kadir2001SaliencySA,\n author = {T. Kadir and M. Brady},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {83-105},\n title = {Saliency, Scale and Image Description},\n volume = {45},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:441151914fbfeb31c476b10500b76f9d9e822c4a",
            "@type": "ScholarlyArticle",
            "paperId": "441151914fbfeb31c476b10500b76f9d9e822c4a",
            "corpusId": 36225362,
            "url": "https://www.semanticscholar.org/paper/441151914fbfeb31c476b10500b76f9d9e822c4a",
            "title": "Visual Servo Control Part I: Basic Approaches",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2944123960",
                "CorpusId": 36225362
            },
            "abstract": "This article is the first of a two-part series on the topic of visual servo control: using computer vision data in the servo loop to control the motion of a robot. In the present article, we describe the basic techniques that are by now well established in the field. We first give a general overview of the formulation of the visual servo control problem. We then describe the two archetypal visual servo control schemes: image-based and position-based visual servo control. Finally, we discuss performance and stability issues that pertain to these two schemes, motivating the second article in the series, in which we consider advanced techniques.",
            "referenceCount": 20,
            "citationCount": 1133,
            "influentialCitationCount": 136,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Chaumette2006VisualSC,\n author = {F. Chaumette and S. Hutchinson},\n title = {Visual Servo Control Part I: Basic Approaches},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69319a6ffa037e99beeb25c1ebb0da59947104a0",
            "@type": "ScholarlyArticle",
            "paperId": "69319a6ffa037e99beeb25c1ebb0da59947104a0",
            "corpusId": 23021920,
            "url": "https://www.semanticscholar.org/paper/69319a6ffa037e99beeb25c1ebb0da59947104a0",
            "title": "Vergence-accommodation conflicts hinder visual performance and cause visual fatigue.",
            "venue": "Journal of Vision",
            "publicationVenue": {
                "id": "urn:research:c3faa921-3f7d-4435-906f-25cdb7d6a885",
                "name": "Journal of Vision",
                "alternate_names": [
                    "J Vis",
                    "Journal of Visualization"
                ],
                "issn": "1534-7362",
                "url": "http://www.journalofvision.org/4/6/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1991012411",
                "DOI": "10.1167/8.3.33",
                "CorpusId": 23021920,
                "PubMed": "18484839"
            },
            "abstract": "Three-dimensional (3D) displays have become important for many applications including vision research, operation of remote devices, medical imaging, surgical training, scientific visualization, virtual prototyping, and more. In many of these applications, it is important for the graphic image to create a faithful impression of the 3D structure of the portrayed object or scene. Unfortunately, 3D displays often yield distortions in perceived 3D structure compared with the percepts of the real scenes the displays depict. A likely cause of such distortions is the fact that computer displays present images on one surface. Thus, focus cues-accommodation and blur in the retinal image-specify the depth of the display rather than the depths in the depicted scene. Additionally, the uncoupling of vergence and accommodation required by 3D displays frequently reduces one's ability to fuse the binocular stimulus and causes discomfort and fatigue for the viewer. We have developed a novel 3D display that presents focus cues that are correct or nearly correct for the depicted scene. We used this display to evaluate the influence of focus cues on perceptual distortions, fusion failures, and fatigue. We show that when focus cues are correct or nearly correct, (1) the time required to identify a stereoscopic stimulus is reduced, (2) stereoacuity in a time-limited task is increased, (3) distortions in perceived depth are reduced, and (4) viewer fatigue and discomfort are reduced. We discuss the implications of this work for vision research and the design and use of displays.",
            "referenceCount": 110,
            "citationCount": 1450,
            "influentialCitationCount": 70,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2008-03-28",
            "journal": {
                "name": "Journal of vision",
                "volume": "8 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Hoffman2008VergenceaccommodationCH,\n author = {David M. Hoffman and A. Girshick and K. Akeley and M. Banks},\n booktitle = {Journal of Vision},\n journal = {Journal of vision},\n pages = {\n          33.1-30\n        },\n title = {Vergence-accommodation conflicts hinder visual performance and cause visual fatigue.},\n volume = {8 3},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:067944fa7e4f5ca08036fa1b98046ce41c133009",
            "@type": "ScholarlyArticle",
            "paperId": "067944fa7e4f5ca08036fa1b98046ce41c133009",
            "corpusId": 2505895,
            "url": "https://www.semanticscholar.org/paper/067944fa7e4f5ca08036fa1b98046ce41c133009",
            "title": "CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1604.02426",
                "DBLP": "conf/eccv/RadenovicTC16",
                "MAG": "2336302573",
                "DOI": "10.1007/978-3-319-46448-0_1",
                "CorpusId": 2505895
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 571,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1604.02426",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1604.02426"
            },
            "citationStyles": {
                "bibtex": "@Article{Radenovic2016CNNIR,\n author = {Filip Radenovic and Giorgos Tolias and Ond\u0159ej Chum},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples},\n volume = {abs/1604.02426},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e02edd5fc99c5900e71f9133ecb403b93cccdff0",
            "@type": "ScholarlyArticle",
            "paperId": "e02edd5fc99c5900e71f9133ecb403b93cccdff0",
            "corpusId": 6896735,
            "url": "https://www.semanticscholar.org/paper/e02edd5fc99c5900e71f9133ecb403b93cccdff0",
            "title": "Advances in Computational Stereo",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/pami/BrownBH03",
                "MAG": "1979846720",
                "DOI": "10.1109/TPAMI.2003.1217603",
                "CorpusId": 6896735
            },
            "abstract": "Extraction of three-dimensional structure of a scene from stereo images is a problem that has been studied by the computer vision community for decades. Early work focused on the fundamentals of image correspondence and stereo geometry. Stereo research has matured significantly throughout the years and many advances in computational stereo continue to be made, allowing stereo to be applied to new and more demanding problems. We review recent advances in computational stereo, focusing primarily on three important topics: correspondence methods, methods for occlusion, and real-time implementations. Throughout, we present tables that summarize and draw distinctions among key ideas and approaches. Where available, we provide comparative analyses and we make suggestions for analyses yet to be done.",
            "referenceCount": 97,
            "citationCount": 1287,
            "influentialCitationCount": 66,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2003-08-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Brown2003AdvancesIC,\n author = {M. Brown and Darius Burschka and Gregory Hager},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {993-1008},\n title = {Advances in Computational Stereo},\n volume = {25},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "@type": "ScholarlyArticle",
            "paperId": "b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "corpusId": 2061602,
            "url": "https://www.semanticscholar.org/paper/b48d90cfebb8fbff29d161f6704d31b6909eb7ad",
            "title": "IM2GPS: estimating geographic information from a single image",
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2103163130",
                "DBLP": "conf/cvpr/HaysE08",
                "DOI": "10.1109/CVPR.2008.4587784",
                "CorpusId": 2061602
            },
            "abstract": "Estimating geographic information from an image is an excellent, difficult high-level computer vision problem whose time has come. The emergence of vast amounts of geographically-calibrated image data is a great reason for computer vision to start looking globally - on the scale of the entire planet! In this paper, we propose a simple algorithm for estimating a distribution over geographic locations from a single image using a purely data-driven scene matching approach. For this task, we leverage a dataset of over 6 million GPS-tagged images from the Internet. We represent the estimated image location as a probability distribution over the Earthpsilas surface. We quantitatively evaluate our approach in several geolocation tasks and demonstrate encouraging performance (up to 30 times better than chance). We show that geolocation estimates can provide the basis for numerous other image understanding tasks such as population density estimation, land cover estimation or urban/rural classification.",
            "referenceCount": 22,
            "citationCount": 927,
            "influentialCitationCount": 123,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/IM2GPS_Estimating_Geographic_Information_From_a_Single_Image/6555080/1/files/12037271.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Geography"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Geography",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geography",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-06-23",
            "journal": {
                "name": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hays2008IM2GPSEG,\n author = {James Hays and Alexei A. Efros},\n booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {IM2GPS: estimating geographic information from a single image},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b68522f58b61e7235b852677337ef3725075fd9",
            "@type": "ScholarlyArticle",
            "paperId": "5b68522f58b61e7235b852677337ef3725075fd9",
            "corpusId": 233219797,
            "url": "https://www.semanticscholar.org/paper/5b68522f58b61e7235b852677337ef3725075fd9",
            "title": "Co-Scale Conv-Attentional Image Transformers",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/iccv/XuXCT21",
                "ArXiv": "2104.06399",
                "DOI": "10.1109/ICCV48922.2021.00983",
                "CorpusId": 233219797
            },
            "abstract": "In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers\u2019 encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT\u2019s backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.",
            "referenceCount": 57,
            "citationCount": 247,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2104.06399",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-04-13",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2021CoScaleCI,\n author = {Weijian Xu and Yifan Xu and Tyler A. Chang and Z. Tu},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {9961-9970},\n title = {Co-Scale Conv-Attentional Image Transformers},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4d954ec7f1091cb1d6b18b1b1e656d583e7a1353",
            "@type": "ScholarlyArticle",
            "paperId": "4d954ec7f1091cb1d6b18b1b1e656d583e7a1353",
            "corpusId": 14953529,
            "url": "https://www.semanticscholar.org/paper/4d954ec7f1091cb1d6b18b1b1e656d583e7a1353",
            "title": "Image Features from Phase Congruency",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1962010357",
                "CorpusId": 14953529
            },
            "abstract": "Videre: Journal of Computer Vision Research (ISSN 1089-2788) is a quarterly journal published electronically on the Internet by The MIT Press, Cambridge, Massachusetts, 02142. Subscriptions and address changes should be addressed to MIT Press Journals, Five Cambridge Center, Cambridge, MA 02142; phone: (617) 253-2889; fax: (617) 577-1545; e-mail: journals-orders@mit.edu. Subscription rates are: Individuals $30.00, Institutions $125.00. Canadians add additional 7% GST. Prices subject to change without notice.",
            "referenceCount": 39,
            "citationCount": 1335,
            "influentialCitationCount": 172,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kovesi1995ImageFF,\n author = {P. Kovesi},\n title = {Image Features from Phase Congruency},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c8cbeb547f3c982c820794ec9cf431b133aba57",
            "@type": "ScholarlyArticle",
            "paperId": "2c8cbeb547f3c982c820794ec9cf431b133aba57",
            "corpusId": 1482395,
            "url": "https://www.semanticscholar.org/paper/2c8cbeb547f3c982c820794ec9cf431b133aba57",
            "title": "A Fast Approximation of the Bilateral Filter Using a Signal Processing Approach",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "1528144695",
                "DBLP": "conf/eccv/ParisD06",
                "DOI": "10.1007/s11263-007-0110-8",
                "CorpusId": 1482395
            },
            "abstract": null,
            "referenceCount": 50,
            "citationCount": 1157,
            "influentialCitationCount": 80,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F11744085_44.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-05-07",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "81"
            },
            "citationStyles": {
                "bibtex": "@Article{Paris2006AFA,\n author = {Sylvain Paris and F. Durand},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {24-52},\n title = {A Fast Approximation of the Bilateral Filter Using a Signal Processing Approach},\n volume = {81},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fe7d0f0e7f4db152c85861d76c9e40a147e2fba",
            "@type": "ScholarlyArticle",
            "paperId": "7fe7d0f0e7f4db152c85861d76c9e40a147e2fba",
            "corpusId": 8433503,
            "url": "https://www.semanticscholar.org/paper/7fe7d0f0e7f4db152c85861d76c9e40a147e2fba",
            "title": "Bayesian modeling of uncertainty in low-level vision",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2099256125",
                "DBLP": "journals/ijcv/Szeliski90",
                "DOI": "10.1007/BF00126502",
                "CorpusId": 8433503
            },
            "abstract": null,
            "referenceCount": 110,
            "citationCount": 203,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-1-4613-1637-4/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-10-07",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Szeliski2011BayesianMO,\n author = {R. Szeliski},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {271-301},\n title = {Bayesian modeling of uncertainty in low-level vision},\n volume = {5},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79492593b0936714d5cc6db9339d5a486e6e77db",
            "@type": "ScholarlyArticle",
            "paperId": "79492593b0936714d5cc6db9339d5a486e6e77db",
            "corpusId": 474253,
            "url": "https://www.semanticscholar.org/paper/79492593b0936714d5cc6db9339d5a486e6e77db",
            "title": "SBA: A software package for generic sparse bundle adjustment",
            "venue": "TOMS",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/toms/LourakisA09",
                "MAG": "2133192850",
                "DOI": "10.1145/1486525.1486527",
                "CorpusId": 474253
            },
            "abstract": "Bundle adjustment constitutes a large, nonlinear least-squares problem that is often solved as the last step of feature-based structure and motion estimation computer vision algorithms to obtain optimal estimates. Due to the very large number of parameters involved, a general purpose least-squares algorithm incurs high computational and memory storage costs when applied to bundle adjustment. Fortunately, the lack of interaction among certain subgroups of parameters results in the corresponding Jacobian being sparse, a fact that can be exploited to achieve considerable computational savings. This article presents sba, a publicly available C/C++ software package for realizing generic bundle adjustment with high efficiency and flexibility regarding parameterization.",
            "referenceCount": 62,
            "citationCount": 915,
            "influentialCitationCount": 70,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-03-01",
            "journal": {
                "name": "ACM Trans. Math. Softw.",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Lourakis2009SBAAS,\n author = {Manolis I. A. Lourakis and Antonis A. Argyros},\n booktitle = {TOMS},\n journal = {ACM Trans. Math. Softw.},\n pages = {2:1-2:30},\n title = {SBA: A software package for generic sparse bundle adjustment},\n volume = {36},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18836d463407274ad8a0a8b0c9f6a15d506b61a6",
            "@type": "ScholarlyArticle",
            "paperId": "18836d463407274ad8a0a8b0c9f6a15d506b61a6",
            "corpusId": 17942780,
            "url": "https://www.semanticscholar.org/paper/18836d463407274ad8a0a8b0c9f6a15d506b61a6",
            "title": "Visual simultaneous localization and mapping: a survey",
            "venue": "Artificial Intelligence Review",
            "publicationVenue": {
                "id": "urn:research:ea8553fe-2467-4367-afee-c4deb3754820",
                "name": "Artificial Intelligence Review",
                "alternate_names": [
                    "Artif Intell Rev"
                ],
                "issn": "0269-2821",
                "url": "https://link.springer.com/journal/10462"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "1979266466",
                "DBLP": "journals/air/Fuentes-PachecoAR15",
                "DOI": "10.1007/s10462-012-9365-8",
                "CorpusId": 17942780
            },
            "abstract": null,
            "referenceCount": 183,
            "citationCount": 789,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2012-11-13",
            "journal": {
                "name": "Artificial Intelligence Review",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Fuentes-Pacheco2012VisualSL,\n author = {J. Fuentes-Pacheco and Jos\u00e9 Ru\u00edz Ascencio and J. M. Rendon-Mancha},\n booktitle = {Artificial Intelligence Review},\n journal = {Artificial Intelligence Review},\n pages = {55 - 81},\n title = {Visual simultaneous localization and mapping: a survey},\n volume = {43},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb",
            "@type": "ScholarlyArticle",
            "paperId": "a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb",
            "corpusId": 2102547,
            "url": "https://www.semanticscholar.org/paper/a573ecb0960d0d2c115c0ad3fc971aa6cdb578eb",
            "title": "Improving the Robustness of Deep Neural Networks via Stability Training",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1604.04326",
                "DBLP": "journals/corr/ZhengSLG16",
                "MAG": "2952477728",
                "DOI": "10.1109/CVPR.2016.485",
                "CorpusId": 2102547
            },
            "abstract": "In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state of-the-art Inception architecture [11] against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on largescale near-duplicate detection, similar-image ranking, and classification on noisy datasets.",
            "referenceCount": 13,
            "citationCount": 581,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1604.04326",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-15",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zheng2016ImprovingTR,\n author = {Stephan Zheng and Yang Song and Thomas Leung and I. Goodfellow},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4480-4488},\n title = {Improving the Robustness of Deep Neural Networks via Stability Training},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86dea10b5c7b831a24132db3e4b50a01f9f001b0",
            "@type": "ScholarlyArticle",
            "paperId": "86dea10b5c7b831a24132db3e4b50a01f9f001b0",
            "corpusId": 634889,
            "url": "https://www.semanticscholar.org/paper/86dea10b5c7b831a24132db3e4b50a01f9f001b0",
            "title": "An Unbiased Detector of Curvilinear Structures",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2156155065",
                "DBLP": "journals/pami/Steger98",
                "DOI": "10.1109/34.659930",
                "CorpusId": 634889
            },
            "abstract": "The extraction of curvilinear structures is an important low-level operation in computer vision that has many applications. Most existing operators use a simple model for the line that is to be extracted, i.e., they do not take into account the surroundings of a line. This leads to the undesired consequence that the line will be extracted in the wrong position whenever a line with different lateral contrast is extracted. In contrast, the algorithm proposed in this paper uses an explicit model for lines and their surroundings. By analyzing the scale-space behavior of a model line profile, it is shown how the bias that is induced by asymmetrical lines can be removed. Furthermore, the algorithm not only returns the precise subpixel line position, but also the width of the line for each line point, also with subpixel accuracy.",
            "referenceCount": 45,
            "citationCount": 1263,
            "influentialCitationCount": 99,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-02-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Steger1998AnUD,\n author = {C. Steger},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {113-125},\n title = {An Unbiased Detector of Curvilinear Structures},\n volume = {20},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "@type": "ScholarlyArticle",
            "paperId": "d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "corpusId": 57246310,
            "url": "https://www.semanticscholar.org/paper/d2c733e34d48784a37d717fe43d9e93277a8c53e",
            "title": "ImageNet: A large-scale hierarchical image database",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2108598243",
                "DBLP": "conf/cvpr/DengDSLL009",
                "DOI": "10.1109/CVPR.2009.5206848",
                "CorpusId": 57246310
            },
            "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called \u201cImageNet\u201d, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
            "referenceCount": 27,
            "citationCount": 48993,
            "influentialCitationCount": 9308,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.image-net.org/papers/imagenet_cvpr09.pdf",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-06-20",
            "journal": {
                "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2009ImageNetAL,\n author = {Jia Deng and Wei Dong and R. Socher and Li-Jia Li and K. Li and Li Fei-Fei},\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {248-255},\n title = {ImageNet: A large-scale hierarchical image database},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d033b30f38642e4b6dd146bb8b464bfb58aad96",
            "@type": "ScholarlyArticle",
            "paperId": "1d033b30f38642e4b6dd146bb8b464bfb58aad96",
            "corpusId": 263891125,
            "url": "https://www.semanticscholar.org/paper/1d033b30f38642e4b6dd146bb8b464bfb58aad96",
            "title": "Deep Clustering for Unsupervised Learning of Visual Features",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2883725317",
                "DBLP": "journals/corr/abs-1807-05520",
                "ArXiv": "1807.05520",
                "DOI": "10.1007/978-3-030-01264-9_9",
                "CorpusId": 263891125
            },
            "abstract": null,
            "referenceCount": 80,
            "citationCount": 515,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1807.05520",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-15",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Caron2018DeepCF,\n author = {Mathilde Caron and Piotr Bojanowski and Armand Joulin and Matthijs Douze},\n booktitle = {European Conference on Computer Vision},\n pages = {139-156},\n title = {Deep Clustering for Unsupervised Learning of Visual Features},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8d8829afd3fd402ff6750454859452395f7e13fd",
            "@type": "ScholarlyArticle",
            "paperId": "8d8829afd3fd402ff6750454859452395f7e13fd",
            "corpusId": 156668245,
            "url": "https://www.semanticscholar.org/paper/8d8829afd3fd402ff6750454859452395f7e13fd",
            "title": "The Social Life of Information",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2038231329",
                "DOI": "10.1108/WS.2000.07949DAE.002",
                "CorpusId": 156668245
            },
            "abstract": "From the Publisher: \nFor years pundits have predicted that information technology will obliterate the need for almost everything--from travel to supermarkets to business organizations to social life itself. Individual users, however, tend to be more skeptical. Beaten down by info-glut and exasperated by computer systems fraught with software crashes, viruses, and unintelligible error messages, they find it hard to get a fix on the true potential of the digital revolution. \nJohn Seely Brown and Paul Duguid help us to see through frenzied visions of the future to the real forces for change in society. They argue that the gap between digerati hype and enduser gloom is largely due to the \"tunnel vision\" that information-driven technologies breed. We've become so focused on where we think we ought to be--a place where technology empowers individuals and obliterates social organizations--that we often fail to see where we're really going and what's helping us get there. We need, they argue, to look beyond our obsession with information and individuals to include the critical social networks of which these are always a part.",
            "referenceCount": 0,
            "citationCount": 2925,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Economics",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Economics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2000-07-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Brown2000TheSL,\n author = {J. Brown and P. Duguid},\n title = {The Social Life of Information},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b282c5d7af473dd351e77833d9a2d06d1f33a1f7",
            "@type": "ScholarlyArticle",
            "paperId": "b282c5d7af473dd351e77833d9a2d06d1f33a1f7",
            "corpusId": 14455617,
            "url": "https://www.semanticscholar.org/paper/b282c5d7af473dd351e77833d9a2d06d1f33a1f7",
            "title": "Efficient Salient Region Detection with Soft Image Abstraction",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/iccv/ChengWLZVC13",
                "MAG": "2156777442",
                "DOI": "10.1109/ICCV.2013.193",
                "CorpusId": 14455617
            },
            "abstract": "Detecting visually salient regions in images is one of the fundamental problems in computer vision. We propose a novel method to decompose an image into large scale perceptually homogeneous elements for efficient salient region detection, using a soft image abstraction representation. By considering both appearance similarity and spatial distribution of image pixels, the proposed representation abstracts out unnecessary image details, allowing the assignment of comparable saliency values across similar regions, and producing perceptually accurate salient region detection. We evaluate our salient region detection approach on the largest publicly available dataset with pixel accurate annotations. The experimental results show that the proposed method outperforms 18 alternate methods, reducing the mean absolute error by 25.2% compared to the previous best result, while being computationally more efficient.",
            "referenceCount": 53,
            "citationCount": 563,
            "influentialCitationCount": 55,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-01",
            "journal": {
                "name": "2013 IEEE International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2013EfficientSR,\n author = {Ming-Ming Cheng and J. Warrell and Wen-Yan Lin and Shuai Zheng and Vibhav Vineet and Nigel Crook},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2013 IEEE International Conference on Computer Vision},\n pages = {1529-1536},\n title = {Efficient Salient Region Detection with Soft Image Abstraction},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "@type": "ScholarlyArticle",
            "paperId": "dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "corpusId": 1637703,
            "url": "https://www.semanticscholar.org/paper/dfbfaaec46d38392f61d683c340ee92a0a66e5d9",
            "title": "Learning to See by Moving",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2951590555",
                "DBLP": "journals/corr/AgrawalCM15",
                "ArXiv": "1505.01596",
                "DOI": "10.1109/ICCV.2015.13",
                "CorpusId": 1637703
            },
            "abstract": "The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn features for a diverse set of visual tasks using any other form of supervision? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigated if the awareness of egomotion(i.e. self motion) can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We found that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.",
            "referenceCount": 89,
            "citationCount": 529,
            "influentialCitationCount": 39,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-07",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Agrawal2015LearningTS,\n author = {Pulkit Agrawal and Jo\u00e3o Carreira and Jitendra Malik},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {37-45},\n title = {Learning to See by Moving},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38850b393d7132dc14141f7d643aca4cb9c321da",
            "@type": "ScholarlyArticle",
            "paperId": "38850b393d7132dc14141f7d643aca4cb9c321da",
            "corpusId": 47266338,
            "url": "https://www.semanticscholar.org/paper/38850b393d7132dc14141f7d643aca4cb9c321da",
            "title": "Texture Analysis: Representation and Matching",
            "venue": "International Conference on Image Analysis and Processing",
            "publicationVenue": {
                "id": "urn:research:c89c0957-b21e-40c3-9e6c-0ab46adf1e53",
                "name": "International Conference on Image Analysis and Processing",
                "alternate_names": [
                    "ICIAP",
                    "Int Conf Image Anal Process"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1380"
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1590543164",
                "DBLP": "conf/iciap/JainK95",
                "DOI": "10.1007/3-540-60298-4_229",
                "CorpusId": 47266338
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 1326,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F3-540-60298-4_229.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1995-09-13",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jain1995TextureAR,\n author = {Anil K. Jain and K. Karu},\n booktitle = {International Conference on Image Analysis and Processing},\n pages = {3-10},\n title = {Texture Analysis: Representation and Matching},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7928e47cca13d9d012952a311e405fa661e99071",
            "@type": "ScholarlyArticle",
            "paperId": "7928e47cca13d9d012952a311e405fa661e99071",
            "corpusId": 918513,
            "url": "https://www.semanticscholar.org/paper/7928e47cca13d9d012952a311e405fa661e99071",
            "title": "Visual Domain Adaptation: A survey of recent advances",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/spm/PatelGLC15",
                "MAG": "1982696459",
                "DOI": "10.1109/MSP.2014.2347059",
                "CorpusId": 918513
            },
            "abstract": "In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model have different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this article, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.",
            "referenceCount": 110,
            "citationCount": 773,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2015-04-02",
            "journal": {
                "name": "IEEE Signal Processing Magazine",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Patel2015VisualDA,\n author = {Vishal M. Patel and Raghuraman Gopalan and Ruonan Li and R. Chellappa},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {53-69},\n title = {Visual Domain Adaptation: A survey of recent advances},\n volume = {32},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6d7488328c0db4dd7deaf9eb3a110fdc6d903594",
            "@type": "ScholarlyArticle",
            "paperId": "6d7488328c0db4dd7deaf9eb3a110fdc6d903594",
            "corpusId": 3656020,
            "url": "https://www.semanticscholar.org/paper/6d7488328c0db4dd7deaf9eb3a110fdc6d903594",
            "title": "Games with a Purpose",
            "venue": "Computer",
            "publicationVenue": {
                "id": "urn:research:f6572f66-2623-4a5e-b0d9-4a5028dea98f",
                "name": "Computer",
                "alternate_names": [
                    "IEEE Computer",
                    "IEEE Comput"
                ],
                "issn": "0018-9162",
                "url": "http://www.computer.org/computer"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2107756083",
                "DBLP": "journals/computer/Ahn06",
                "DOI": "10.1109/MC.2006.196",
                "CorpusId": 3656020
            },
            "abstract": "Through online games, people can collectively solve large-scale computational problems. Such games constitute a general mechanism for using brain power to solve open problems. In fact, designing such a game is much like designing an algorithm - it must be proven correct, its efficiency can be analyzed, a more efficient version can supersede a less efficient one, and so on. \"Games with a purpose\" have a vast range of applications in areas as diverse as security, computer vision, Internet accessibility, adult content filtering, and Internet search. Any game designed to address these and other problems must ensure that game play results in a correct solution and, at the same time, is enjoyable. People will play such games to be entertained, not to solve a problem - no matter how laudable the objective",
            "referenceCount": 0,
            "citationCount": 1134,
            "influentialCitationCount": 64,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2006-06-01",
            "journal": {
                "name": "Computer",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Ahn2006GamesWA,\n author = {Luis von Ahn},\n booktitle = {Computer},\n journal = {Computer},\n pages = {92-94},\n title = {Games with a Purpose},\n volume = {39},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:943c372336ced4b28e15e02fe8db1f4b23bf6835",
            "@type": "ScholarlyArticle",
            "paperId": "943c372336ced4b28e15e02fe8db1f4b23bf6835",
            "corpusId": 4546146,
            "url": "https://www.semanticscholar.org/paper/943c372336ced4b28e15e02fe8db1f4b23bf6835",
            "title": "Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/spm/HanZCLX18",
                "MAG": "2783231089",
                "DOI": "10.1109/MSP.2017.2749125",
                "CorpusId": 4546146
            },
            "abstract": "Object detection, including objectness detection (OD), salient object detection (SOD), and category-specific object detection (COD), is one of the most fundamental yet challenging problems in the computer vision community. Over the last several decades, great efforts have been made by researchers to tackle this problem, due to its broad range of applications for other computer vision tasks such as activity or event recognition, content-based image retrieval and scene understanding, etc. While numerous methods have been presented in recent years, a comprehensive review for the proposed high-quality object detection techniques, especially for those based on advanced deep-learning techniques, is still lacking. To this end, this article delves into the recent progress in this research field, including 1) definitions, motivations, and tasks of each subdirection; 2) modern techniques and essential research trends; 3) benchmark data sets and evaluation metrics; and 4) comparisons and analysis of the experimental results. More importantly, we will reveal the underlying relationship among OD, SOD, and COD and discuss in detail some open questions as well as point out several unsolved challenges and promising future works.",
            "referenceCount": 97,
            "citationCount": 556,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-01-01",
            "journal": {
                "name": "IEEE Signal Processing Magazine",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Han2018AdvancedDT,\n author = {Junwei Han and Dingwen Zhang and Gong Cheng and Nian Liu and Dong Xu},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Processing Magazine},\n pages = {84-100},\n title = {Advanced Deep-Learning Techniques for Salient and Category-Specific Object Detection: A Survey},\n volume = {35},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f8b2c2a8c2369f4c40a9cf5e8599bf4ece665200",
            "@type": "ScholarlyArticle",
            "paperId": "f8b2c2a8c2369f4c40a9cf5e8599bf4ece665200",
            "corpusId": 940928,
            "url": "https://www.semanticscholar.org/paper/f8b2c2a8c2369f4c40a9cf5e8599bf4ece665200",
            "title": "Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades",
            "venue": "Frontiers in Neuroscience",
            "publicationVenue": {
                "id": "urn:research:2ca4279c-8ed7-4280-8022-09e577923a09",
                "name": "Frontiers in Neuroscience",
                "alternate_names": [
                    "Front Neurosci"
                ],
                "issn": "1662-453X",
                "url": "https://www.frontiersin.org/journals/neuroscience"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1507.07629",
                "PubMedCentral": "4644806",
                "DBLP": "journals/corr/OrchardJCT15",
                "MAG": "946256365",
                "DOI": "10.3389/fnins.2015.00437",
                "CorpusId": 940928,
                "PubMed": "26635513"
            },
            "abstract": "Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.",
            "referenceCount": 26,
            "citationCount": 494,
            "influentialCitationCount": 129,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.frontiersin.org/articles/10.3389/fnins.2015.00437/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-28",
            "journal": {
                "name": "Frontiers in Neuroscience",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Orchard2015ConvertingSI,\n author = {G. Orchard and Ajinkya Jayawant and Gregory Cohen and N. Thakor},\n booktitle = {Frontiers in Neuroscience},\n journal = {Frontiers in Neuroscience},\n title = {Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades},\n volume = {9},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4462c82748d81489f4f453f516754438b35a8cec",
            "@type": "ScholarlyArticle",
            "paperId": "4462c82748d81489f4f453f516754438b35a8cec",
            "corpusId": 1530384,
            "url": "https://www.semanticscholar.org/paper/4462c82748d81489f4f453f516754438b35a8cec",
            "title": "Object recognition by computer - the role of geometric constraints",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "MAG": "2103497972",
                "DBLP": "books/daglib/0066857",
                "DOI": "10.5860/choice.29-2747",
                "CorpusId": 1530384
            },
            "abstract": "With contributions from Tomas LozanoPerez and Daniel P. Huttenlocher.An intelligent system must know \"what \"the objects are and \"where \"they are in its environment. Examples of this ubiquitous problem in computer vision arise in tasks involving hand-eye coordination (such as assembling or sorting), inspection tasks, gauging operations, and in navigation and localization of mobile robots. This book describes an extended series of experiments into the role of geometry in the critical area of object recognition. It provides precise definitions of the recognition and localization problems, describes the methods used to address them, analyzes the solutions to these problems, and addresses the implications of this analysis.The solution to problems of object recognition are of fundamental importance in many real applications and versions of the techniques described here are already being used in industrial settings. Although a number of questions remain to be solved, the authors provide a valuable framework for understanding both the strengths and limitations of using object shape to guide recognition.W. Eric L. Grimson is Matsushita Associate Professor in the Department of Electrical Engineering and Computer Science at MIT.Contents: Introduction. Recognition as a Search Problem. Searching for Correspondences. Two-Dimensional Constraints. Three-Dimensional Constraints. Verifying Hypotheses. Controlling the Search Explosion. Selecting Subspaces of the Search Space. Empirical Testing. The Combinatorics of the Matching Process. The Combinatorics of Hough Transforms. The Combinatorics of Verification. The Combinatorics of Indexing. Evaluating the Methods. Recognition from Libraries. Parameterized Objects. The Role of Grouping. Sensing Strategies. Applications. The Next Steps.",
            "referenceCount": 0,
            "citationCount": 973,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1991-01-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Grimson1991ObjectRB,\n author = {W. Grimson},\n pages = {I-IX, 1-512},\n title = {Object recognition by computer - the role of geometric constraints},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b02f474196fb9bd61fa3d418a7ba8ac500e8d422",
            "@type": "ScholarlyArticle",
            "paperId": "b02f474196fb9bd61fa3d418a7ba8ac500e8d422",
            "corpusId": 723210,
            "url": "https://www.semanticscholar.org/paper/b02f474196fb9bd61fa3d418a7ba8ac500e8d422",
            "title": "Feature Detection with Automatic Scale Selection",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2109200236",
                "DBLP": "journals/ijcv/Lindeberg98",
                "DOI": "10.1023/A:1008045108935",
                "CorpusId": 723210
            },
            "abstract": null,
            "referenceCount": 66,
            "citationCount": 2963,
            "influentialCitationCount": 239,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://kth.diva-portal.org/smash/get/diva2:453064/FULLTEXT01",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-11-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Lindeberg1998FeatureDW,\n author = {T. Lindeberg},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {79-116},\n title = {Feature Detection with Automatic Scale Selection},\n volume = {30},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:616b246e332573af1f4859aa91440280774c183a",
            "@type": "ScholarlyArticle",
            "paperId": "616b246e332573af1f4859aa91440280774c183a",
            "corpusId": 207252270,
            "url": "https://www.semanticscholar.org/paper/616b246e332573af1f4859aa91440280774c183a",
            "title": "The Pascal Visual Object Classes Challenge: A Retrospective",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/ijcv/EveringhamEGWWZ15",
                "MAG": "2037227137",
                "DOI": "10.1007/s11263-014-0733-5",
                "CorpusId": 207252270
            },
            "abstract": null,
            "referenceCount": 62,
            "citationCount": 5053,
            "influentialCitationCount": 607,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/20017166/ijcv_voc14.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-06-25",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "111"
            },
            "citationStyles": {
                "bibtex": "@Article{Everingham2014ThePV,\n author = {M. Everingham and S. Eslami and L. Gool and Christopher K. I. Williams and J. Winn and Andrew Zisserman},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {98 - 136},\n title = {The Pascal Visual Object Classes Challenge: A Retrospective},\n volume = {111},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:21fffbcc74a9ea0129b75240051daffd7b20fc9d",
            "@type": "ScholarlyArticle",
            "paperId": "21fffbcc74a9ea0129b75240051daffd7b20fc9d",
            "corpusId": 261514006,
            "url": "https://www.semanticscholar.org/paper/21fffbcc74a9ea0129b75240051daffd7b20fc9d",
            "title": "Pattern recognition and neural networks",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2117812871",
                "CorpusId": 261514006
            },
            "abstract": "From the Publisher: \nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.",
            "referenceCount": 112,
            "citationCount": 1505,
            "influentialCitationCount": 33,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Ripley1996PatternRA,\n author = {B. D. Ripley and N. Hjort},\n title = {Pattern recognition and neural networks},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1242d79573397094c5670f55e58c8333cced0beb",
            "@type": "ScholarlyArticle",
            "paperId": "1242d79573397094c5670f55e58c8333cced0beb",
            "corpusId": 5005167,
            "url": "https://www.semanticscholar.org/paper/1242d79573397094c5670f55e58c8333cced0beb",
            "title": "Deep Learning: A Primer for Radiologists.",
            "venue": "Radiographics",
            "publicationVenue": {
                "id": "urn:research:6a469aed-f8e3-456a-92e6-53237e43d3da",
                "name": "Radiographics",
                "alternate_names": null,
                "issn": "0271-5333",
                "url": "https://pubs.rsna.org/loi/radiographics"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2767236661",
                "DOI": "10.1148/rg.2017170077",
                "CorpusId": 5005167,
                "PubMed": "29131760"
            },
            "abstract": "Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. \u00a9RSNA, 2017.",
            "referenceCount": 28,
            "citationCount": 716,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-11-13",
            "journal": {
                "name": "Radiographics : a review publication of the Radiological Society of North America, Inc",
                "volume": "37 7"
            },
            "citationStyles": {
                "bibtex": "@Article{Chartrand2017DeepLA,\n author = {G. Chartrand and P. Cheng and Eugene Vorontsov and M. Drozdzal and S. Turcotte and C. Pal and S. Kadoury and A. Tang},\n booktitle = {Radiographics},\n journal = {Radiographics : a review publication of the Radiological Society of North America, Inc},\n pages = {\n          2113-2131\n        },\n title = {Deep Learning: A Primer for Radiologists.},\n volume = {37 7},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16f4cfd4cbf504a151827b5a94df5deafd4930ac",
            "@type": "ScholarlyArticle",
            "paperId": "16f4cfd4cbf504a151827b5a94df5deafd4930ac",
            "corpusId": 16493916,
            "url": "https://www.semanticscholar.org/paper/16f4cfd4cbf504a151827b5a94df5deafd4930ac",
            "title": "Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform",
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/cvpr/GuoMZ08",
                "MAG": "2170869852",
                "DOI": "10.1109/CVPR.2008.4587715",
                "CorpusId": 16493916
            },
            "abstract": "Salient areas in natural scenes are generally regarded as the candidates of attention focus in human eyes, which is the key stage in object detection. In computer vision, many models have been proposed to simulate the behavior of eyes such as SaliencyToolBox (STB), neuromorphic vision toolkit (NVT) and etc., but they demand high computational cost and their remarkable results mostly rely on the choice of parameters. Recently a simple and fast approach based on Fourier transform called spectral residual (SR) was proposed, which used SR of the amplitude spectrum to obtain the saliency map. The results are good, but the reason is questionable.",
            "referenceCount": 14,
            "citationCount": 825,
            "influentialCitationCount": 110,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-06-23",
            "journal": {
                "name": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2008SpatiotemporalSD,\n author = {Chenlei Guo and Qi Ma and Liming Zhang},\n booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e3d5ee10d0489c768e943546038e3f53e7697349",
            "@type": "ScholarlyArticle",
            "paperId": "e3d5ee10d0489c768e943546038e3f53e7697349",
            "corpusId": 215416317,
            "url": "https://www.semanticscholar.org/paper/e3d5ee10d0489c768e943546038e3f53e7697349",
            "title": "State of the Art on Neural Rendering",
            "venue": "Computer graphics forum (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2004-03805",
                "ArXiv": "2004.03805",
                "MAG": "3016007010",
                "DOI": "10.1111/cgf.14022",
                "CorpusId": 215416317
            },
            "abstract": "Efficient rendering of photo\u2010realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo\u2010realistic images from hand\u2010crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo\u2010realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state\u2010of\u2010the\u2010art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state\u2010of\u2010the\u2010art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free\u2010viewpoint video, and the creation of photo\u2010realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",
            "referenceCount": 233,
            "citationCount": 355,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://rss.onlinelibrary.wiley.com/doi/am-pdf/10.1111/cgf.14022",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-08",
            "journal": {
                "name": "Computer Graphics Forum",
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Article{Tewari2020StateOT,\n author = {A. Tewari and Ohad Fried and Justus Thies and V. Sitzmann and Stephen Lombardi and Kalyan Sunkavalli and Ricardo Martin-Brualla and T. Simon and Jason M. Saragih and M. Nie\u00dfner and Rohit Pandey and S. Fanello and Gordon Wetzstein and Jun-Yan Zhu and C. Theobalt and Maneesh Agrawala and Eli Shechtman and Dan B. Goldman and Michael Zollhofer},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {State of the Art on Neural Rendering},\n volume = {39},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b5799d10df17de3232540e990da69553800d6376",
            "@type": "ScholarlyArticle",
            "paperId": "b5799d10df17de3232540e990da69553800d6376",
            "corpusId": 201124789,
            "url": "https://www.semanticscholar.org/paper/b5799d10df17de3232540e990da69553800d6376",
            "title": "PubLayNet: Largest Dataset Ever for Document Layout Analysis",
            "venue": "IEEE International Conference on Document Analysis and Recognition",
            "publicationVenue": {
                "id": "urn:research:991e8cbf-4a4a-4ac4-a273-63dd7a35c364",
                "name": "IEEE International Conference on Document Analysis and Recognition",
                "alternate_names": [
                    "IEEE Int Conf Doc Anal Recognit",
                    "International Conference on Document Analysis and Recognition",
                    "Int Conf Doc Anal Recognit",
                    "ICDAR"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1327"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1908-07836",
                "MAG": "3003711898",
                "ArXiv": "1908.07836",
                "DOI": "10.1109/ICDAR.2019.00166",
                "CorpusId": 201124789
            },
            "abstract": "Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.",
            "referenceCount": 22,
            "citationCount": 298,
            "influentialCitationCount": 83,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1908.07836",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-08-16",
            "journal": {
                "name": "2019 International Conference on Document Analysis and Recognition (ICDAR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhong2019PubLayNetLD,\n author = {Xu Zhong and Jianbin Tang and Antonio Jimeno-Yepes},\n booktitle = {IEEE International Conference on Document Analysis and Recognition},\n journal = {2019 International Conference on Document Analysis and Recognition (ICDAR)},\n pages = {1015-1022},\n title = {PubLayNet: Largest Dataset Ever for Document Layout Analysis},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0cae508c7df5b20c8009dde793f956af032fa76d",
            "@type": "ScholarlyArticle",
            "paperId": "0cae508c7df5b20c8009dde793f956af032fa76d",
            "corpusId": 5494812,
            "url": "https://www.semanticscholar.org/paper/0cae508c7df5b20c8009dde793f956af032fa76d",
            "title": "Vision in bad weather",
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2003709967",
                "DBLP": "conf/iccv/NayarN99",
                "DOI": "10.1109/ICCV.1999.790306",
                "CorpusId": 5494812
            },
            "abstract": "Current vision systems are designed to perform in clear weather. Needless to say, in any outdoor application, there is no escape from \"bad\" weather. Ultimately, computer vision systems must include mechanisms that enable them to function (even if somewhat less reliably) in the presence of haze, fog, rain, hail and snow. We begin by studying the visual manifestations of different weather conditions. For this, we draw on what is already known about atmospheric optics. Next, we identify effects caused by bad weather that can be turned to our advantage. Since the atmosphere modulates the information carried from a scene point to the observer it can be viewed as a mechanism of visual information coding. Based on this observation, we develop models and methods for recovering pertinent scene properties, such as three-dimensional structure, from images taken under poor weather conditions.",
            "referenceCount": 15,
            "citationCount": 796,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-09-20",
            "journal": {
                "name": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Nayar1999VisionIB,\n author = {S. Nayar and S. Narasimhan},\n booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n pages = {820-827 vol.2},\n title = {Vision in bad weather},\n volume = {2},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e2d3abc7008a269880918ee7d903a55d06acdd55",
            "@type": "ScholarlyArticle",
            "paperId": "e2d3abc7008a269880918ee7d903a55d06acdd55",
            "corpusId": 54444417,
            "url": "https://www.semanticscholar.org/paper/e2d3abc7008a269880918ee7d903a55d06acdd55",
            "title": "DeepVoxels: Learning Persistent 3D Feature Embeddings",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2903200123",
                "DBLP": "conf/cvpr/SitzmannTHNWZ19",
                "ArXiv": "1812.01024",
                "DOI": "10.1109/CVPR.2019.00254",
                "CorpusId": 54444417
            },
            "abstract": "In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D scene without having to explicitly model its geometry. At its core, our approach is based on a Cartesian 3D grid of persistent embedded features that learn to make use of the underlying 3D scene structure. Our approach combines insights from 3D geometric computer vision with recent advances in learning image-to-image mappings based on adversarial loss functions. DeepVoxels is supervised, without requiring a 3D reconstruction of the scene, using a 2D re-rendering loss and enforces perspective and multi-view geometry in a principled manner. We apply our persistent 3D scene representation to the problem of novel view synthesis demonstrating high-quality results for a variety of challenging scenes.",
            "referenceCount": 65,
            "citationCount": 507,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.01024",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-03",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sitzmann2018DeepVoxelsLP,\n author = {V. Sitzmann and Justus Thies and Felix Heide and M. Nie\u00dfner and Gordon Wetzstein and M. Zollh\u00f6fer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2432-2441},\n title = {DeepVoxels: Learning Persistent 3D Feature Embeddings},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1cd9ab95572772db415f35f223fbd914c14cb7c7",
            "@type": "ScholarlyArticle",
            "paperId": "1cd9ab95572772db415f35f223fbd914c14cb7c7",
            "corpusId": 210839618,
            "url": "https://www.semanticscholar.org/paper/1cd9ab95572772db415f35f223fbd914c14cb7c7",
            "title": "Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future",
            "venue": "Journal of Cognitive Neuroscience",
            "publicationVenue": {
                "id": "urn:research:b7f1bc59-118a-410a-afc3-aa5cbfb833ff",
                "name": "Journal of Cognitive Neuroscience",
                "alternate_names": [
                    "J Cogn Neurosci"
                ],
                "issn": "0898-929X",
                "url": "http://cognet.mit.edu/library/journals/journal?issn=0898929x"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2001.07092",
                "DBLP": "journals/corr/abs-2001-07092",
                "MAG": "3002446034",
                "DOI": "10.1162/jocn_a_01544",
                "CorpusId": 210839618,
                "PubMed": "32027584"
            },
            "abstract": "Abstract Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.",
            "referenceCount": 145,
            "citationCount": 259,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2001.07092",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Medicine",
                "Mathematics",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-01-20",
            "journal": {
                "name": "Journal of Cognitive Neuroscience",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Lindsay2020ConvolutionalNN,\n author = {Grace W. Lindsay},\n booktitle = {Journal of Cognitive Neuroscience},\n journal = {Journal of Cognitive Neuroscience},\n pages = {2017-2031},\n title = {Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},\n volume = {33},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9051b75d686cf52ae10edabd1ba322b8a683e025",
            "@type": "ScholarlyArticle",
            "paperId": "9051b75d686cf52ae10edabd1ba322b8a683e025",
            "corpusId": 2117401,
            "url": "https://www.semanticscholar.org/paper/9051b75d686cf52ae10edabd1ba322b8a683e025",
            "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence Coding , Analysis , Interpretation , and Recognition of Facial Expressions",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "CorpusId": 2117401
            },
            "abstract": "We describe a computer vision system for observing facial motion by using an optimal estimation optical flow method coupled with a geometric and a physical (muscle) model describing the facial structure. Our method produces a reliable parametric representation of the face\u2019s independent muscle action groups, as well as an accurate estimate of facial motion. Previous efforts at analysis of facial expression have been based on the Facial Action Coding System (FACS), a representation developed in order to allow human psychologists to code expression from static pictures. To avoid use of this heuristic coding scheme, we have used our computer vision system to probabilistically characterize facial motion and muscle activation in an experimental population, thus deriving a new, more accurate representation of human facial expressions that we call FACS+. We use this new representation for recognition in two different ways. The first method uses the physics-based model directly, by recognizing expressions through comparison of estimated muscle activations. The second method uses the physics-based model to generate spatio-temporal motionenergy templates of the whole face for each different expression. These simple, biologically-plausible motion energy \u201ctemplates\u201d are then used for recognition. Both methods show substantially greater accuracy at expression recognition than has been previously achieved. Categories: Facial Expressions, Expression Recognition, Face Processing, Facial Analysis, Motion and Pattern Analysis, Vision-based HCI.",
            "referenceCount": 41,
            "citationCount": 1059,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Essa2007IEEETO,\n author = {Irfan Essa and A. Pentland},\n title = {IEEE Transactions on Pattern Analysis and Machine Intelligence Coding , Analysis , Interpretation , and Recognition of Facial Expressions},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "@type": "ScholarlyArticle",
            "paperId": "a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "corpusId": 4560536,
            "url": "https://www.semanticscholar.org/paper/a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "title": "The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/eccv/DuQYYDLZHT18",
                "ArXiv": "1804.00518",
                "MAG": "2950636517",
                "DOI": "10.1007/978-3-030-01249-6_23",
                "CorpusId": 4560536
            },
            "abstract": null,
            "referenceCount": 53,
            "citationCount": 431,
            "influentialCitationCount": 111,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1804.00518",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Du2018TheUA,\n author = {Dawei Du and Yuankai Qi and Hongyang Yu and Yi-Fan Yang and Kaiwen Duan and Guorong Li and W. Zhang and Qingming Huang and Q. Tian},\n booktitle = {European Conference on Computer Vision},\n pages = {375-391},\n title = {The Unmanned Aerial Vehicle Benchmark: Object Detection and Tracking},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:836dd170045a02a1d20a22c13ed89ed02b203e70",
            "@type": "ScholarlyArticle",
            "paperId": "836dd170045a02a1d20a22c13ed89ed02b203e70",
            "corpusId": 10323459,
            "url": "https://www.semanticscholar.org/paper/836dd170045a02a1d20a22c13ed89ed02b203e70",
            "title": "A REVIEW OF VISION BASED HAND GESTURES RECOGNITION",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2485678069",
                "CorpusId": 10323459
            },
            "abstract": "With the ever-increasing diffusion of computers into the society, it is widely believed that present popular mode of interactio ns with computers (mouse and keyboard) will become a bottleneck in the effective utilization of information flow between the computers and the human. Vision based Gesture recognition has the potential to be a natural and powerful tool supporting efficient and intuitive interaction between the human and the computer. Visual interpretation of hand gestures can help in achieving the ease and naturalness desired for Human Computer Interaction (HCI). This has motivated many researchers in computer vision-based analysis and interpretation of hand gestures as a very active research area. We surveyed the literature on visual interpretation of hand gestures in the context of its role in HCI and various seminal works of researchers are emphasized. The purpose of this review is to introduce the field of gesture recognition as a mechanism for interaction with computers.",
            "referenceCount": 45,
            "citationCount": 353,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Murthy2009ARO,\n author = {G. Murthy and R. S. Jadon},\n title = {A REVIEW OF VISION BASED HAND GESTURES RECOGNITION},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:331a30863f04e91fc0ecc523a2686242972b43ab",
            "@type": "ScholarlyArticle",
            "paperId": "331a30863f04e91fc0ecc523a2686242972b43ab",
            "corpusId": 26221504,
            "url": "https://www.semanticscholar.org/paper/331a30863f04e91fc0ecc523a2686242972b43ab",
            "title": "A Dataset and Evaluation Methodology for Depth Estimation on 4D Light Fields",
            "venue": "Asian Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:a8f26d13-e373-4e48-b57b-ef89bf48f4db",
                "name": "Asian Conference on Computer Vision",
                "alternate_names": [
                    "Asian Conf Comput Vis",
                    "ACCV"
                ],
                "issn": null,
                "url": "http://www.cvl.iis.u-tokyo.ac.jp/afcv/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/accv/HonauerJKG16",
                "MAG": "2591697814",
                "DOI": "10.1007/978-3-319-54187-7_2",
                "CorpusId": 26221504
            },
            "abstract": null,
            "referenceCount": 29,
            "citationCount": 451,
            "influentialCitationCount": 100,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Honauer2016ADA,\n author = {Katrin Honauer and O. Johannsen and D. Kondermann and Bastian Goldl\u00fccke},\n booktitle = {Asian Conference on Computer Vision},\n pages = {19-34},\n title = {A Dataset and Evaluation Methodology for Depth Estimation on 4D Light Fields},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8582fd09981cb5fe9729b2014112674c2e991c1f",
            "@type": "ScholarlyArticle",
            "paperId": "8582fd09981cb5fe9729b2014112674c2e991c1f",
            "corpusId": 9202348,
            "url": "https://www.semanticscholar.org/paper/8582fd09981cb5fe9729b2014112674c2e991c1f",
            "title": "On the performance of ConvNet features for place recognition",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "301022506",
                "ArXiv": "1501.04158",
                "DBLP": "conf/iros/SunderhaufSDUM15",
                "DOI": "10.1109/IROS.2015.7353986",
                "CorpusId": 9202348
            },
            "abstract": "After the incredible success of deep learning in the computer vision domain, there has been much interest in applying Convolutional Network (ConvNet) features in robotic fields such as visual navigation and SLAM. Unfortunately, there are fundamental differences and challenges involved. Computer vision datasets are very different in character to robotic camera data, real-time performance is essential, and performance priorities can be different. This paper comprehensively evaluates and compares the utility of three state-of-the-art ConvNets on the problems of particular relevance to navigation for robots; viewpoint-invariance and condition-invariance, and for the first time enables real-time place recognition performance using ConvNets with large maps by integrating a variety of existing (locality-sensitive hashing) and novel (semantic search space partitioning) optimization techniques. We present extensive experiments on four real world datasets cultivated to evaluate each of the specific challenges in place recognition. The results demonstrate that speed-ups of two orders of magnitude can be achieved with minimal accuracy degradation, enabling real-time performance. We confirm that networks trained for semantic place categorization also perform better at (specific) place recognition when faced with severe appearance changes and provide a reference for which networks and layers are optimal for different aspects of the place recognition problem.",
            "referenceCount": 41,
            "citationCount": 492,
            "influentialCitationCount": 63,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1501.04158",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-01-16",
            "journal": {
                "name": "2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{S\u00fcnderhauf2015OnTP,\n author = {Niko S\u00fcnderhauf and Feras Dayoub and S. Shirazi and B. Upcroft and Michael Milford},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {4297-4304},\n title = {On the performance of ConvNet features for place recognition},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b813b5dcf1010adeb88c6c066a241dd319d36a3b",
            "@type": "ScholarlyArticle",
            "paperId": "b813b5dcf1010adeb88c6c066a241dd319d36a3b",
            "corpusId": 141002786,
            "url": "https://www.semanticscholar.org/paper/b813b5dcf1010adeb88c6c066a241dd319d36a3b",
            "title": "What is Cognitive Science",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "185656810",
                "DOI": "10.4324/9781315677101-32",
                "CorpusId": 141002786
            },
            "abstract": "Preface. Acknowledgments. 1. What's in Your Mind: Zenon W. Pylyshyn (Rutgers Center for Cognitive Science). 2. Explaining the Infant's Object Concept: Beyond the Perception/Cognition Dichotomy: Brian J. Scholl and Alan M. Leslie (Rutgers Center for Cognitive Science). 3. Rethinking Rationality: From Bleak Implications to Darwinian Modules: Richard Samuels, Stephen Stich, and Patrice D. Tremoulet. 4. New Foundations for Perception: Michael Leyton (Department of Psychology, Rutgers University). 5. Object Representation and Recognition: Sven J. Dickinson (Rutgers Center for Cognitive science and Department of Computer Science). 6. Does Vision Work? Towards a Semantics of Perception: Jacob Feldman (Rutgers Center for Cognitive Science). 7. The Brain as a Hypothesis-Constructing-and-Testing Agent: Thomas V. Papathomas (Laboratory of Vision Research and Department of Biomedical Engineering, Rutgers). 8. What Movements of the Eye Tell About the Mind: Eileen Kowler (Department of Psychology and center for Cognitive Science, Rutgers). 9. Visual Dilemmas, Competition Between Eyes and Between Precepts in Binocular Rivalry: Thomas V. Papathomas: (Laboratory of Vision Research, and Department of Biomedical Engineering, Rutgers), Ilona Kovacs (Laboratory of Vision Research and Department of Psychology, Rutgers), Akos Feher (Laboratory of Vision Research, Rutgers), and Bela Julesz (Laboratory of Vision Research and Department of Psychology, Rutgers). 10. Linguistic and Cognitive Explanation in Optimality Theory: Bruce Tesar, Jane Grimshaw, and Alan Prince. 11. Impossible Words?: Jerry Fodor and Ernie Lepore (Rutgers Center for Cognitive Science). 12. Bridging the Symbolic-Connectionist Gap in Language Comprehension: Suzanne Stevenson (Center for Cognitive Science and Department of Computer Science, Rutgers). 13. Language Acquisition: Karin Stromswold (Department of Psychology and Center for Cognitive science, Rutgers). 14. Connectionist Neuroscience: Representational and Learning Issues for Neuroscience: Stephen Jose Hanson, (Department of Psychology, Rutgers). Index.",
            "referenceCount": 0,
            "citationCount": 1591,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lepore1999WhatIC,\n author = {E. Lepore and Z. Pylyshyn},\n title = {What is Cognitive Science},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d77123b54dcc8014949584ab624e97298617bcad",
            "@type": "ScholarlyArticle",
            "paperId": "d77123b54dcc8014949584ab624e97298617bcad",
            "corpusId": 184487878,
            "url": "https://www.semanticscholar.org/paper/d77123b54dcc8014949584ab624e97298617bcad",
            "title": "Data-Free Quantization Through Weight Equalization and Bias Correction",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iccv/NagelBBW19",
                "ArXiv": "1906.04721",
                "MAG": "2981751377",
                "DOI": "10.1109/ICCV.2019.00141",
                "CorpusId": 184487878
            },
            "abstract": "We introduce a data-free quantization method for deep neural networks that does not require fine-tuning or hyperparameter selection. It achieves near-original model performance on common computer vision architectures and tasks. 8-bit fixed-point quantization is essential for efficient inference on modern deep learning hardware. However, quantizing models to run in 8-bit is a non-trivial task, frequently leading to either significant performance reduction or engineering time spent on training a network to be amenable to quantization. Our approach relies on equalizing the weight ranges in the network by making use of a scale-equivariance property of activation functions. In addition the method corrects biases in the error that are introduced during quantization. This improves quantization accuracy performance, and can be applied to many common computer vision architectures with a straight forward API call. For common architectures, such as the MobileNet family, we achieve state-of-the-art quantized model performance. We further show that the method also extends to other computer vision architectures and tasks such as semantic segmentation and object detection.",
            "referenceCount": 40,
            "citationCount": 351,
            "influentialCitationCount": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1906.04721",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-06-11",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nagel2019DataFreeQT,\n author = {Markus Nagel and Mart van Baalen and Tijmen Blankevoort and M. Welling},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {1325-1334},\n title = {Data-Free Quantization Through Weight Equalization and Bias Correction},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:12806c298e01083a79db77927530367d85939907",
            "@type": "ScholarlyArticle",
            "paperId": "12806c298e01083a79db77927530367d85939907",
            "corpusId": 11919941,
            "url": "https://www.semanticscholar.org/paper/12806c298e01083a79db77927530367d85939907",
            "title": "An Empirical Evaluation of Deep Learning on Highway Driving",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1585377561",
                "DBLP": "journals/corr/HuvalWTKSPARMCM15",
                "ArXiv": "1504.01716",
                "CorpusId": 11919941
            },
            "abstract": "Numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios. In this paper, we presented a number of empirical evaluations of recent deep learning advances. Computer vision, combined with deep learning, has the potential to bring about a relatively inexpensive, robust solution to autonomous driving. To prepare deep learning for industry uptake and practical applications, neural networks will require large data sets that represent all possible driving environments and scenarios. We collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection. We show how existing convolutional neural networks (CNNs) can be used to perform lane and vehicle detection while running at frame rates required for a real-time system. Our results lend credence to the hypothesis that deep learning holds promise for autonomous driving.",
            "referenceCount": 20,
            "citationCount": 557,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-04-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1504.01716"
            },
            "citationStyles": {
                "bibtex": "@Article{Huval2015AnEE,\n author = {Brody Huval and Tao Wang and S. Tandon and Jeff Kiske and W. Song and Joel Pazhayampallil and Mykhaylo Andriluka and Pranav Rajpurkar and Toki Migimatsu and Royce Cheng-Yue and Fernando A. Mujica and Adam Coates and A. Ng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {An Empirical Evaluation of Deep Learning on Highway Driving},\n volume = {abs/1504.01716},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cbf4a4bbfadf0c8321da579075f14b997b1354ad",
            "@type": "ScholarlyArticle",
            "paperId": "cbf4a4bbfadf0c8321da579075f14b997b1354ad",
            "corpusId": 17612122,
            "url": "https://www.semanticscholar.org/paper/cbf4a4bbfadf0c8321da579075f14b997b1354ad",
            "title": "EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/Benitez-QuirozS16",
                "MAG": "2436394355",
                "DOI": "10.1109/CVPR.2016.600",
                "CorpusId": 17612122
            },
            "abstract": "Research in face perception and emotion theory requires very large annotated databases of images of facial expressions of emotion. Annotations should include Action Units (AUs) and their intensities as well as emotion category. This goal cannot be readily achieved manually. Herein, we present a novel computer vision algorithm to annotate a large database of one million images of facial expressions of emotion in the wild (i.e., face images downloaded from the Internet). First, we show that this newly proposed algorithm can recognize AUs and their intensities reliably across databases. To our knowledge, this is the first published algorithm to achieve highly-accurate results in the recognition of AUs and their intensities across multiple databases. Our algorithm also runs in real-time (>30 images/second), allowing it to work with large numbers of images and video sequences. Second, we use WordNet to download 1,000,000 images of facial expressions with associated emotion keywords from the Internet. These images are then automatically annotated with AUs, AU intensities and emotion categories by our algorithm. The result is a highly useful database that can be readily queried using semantic descriptions for applications in computer vision, affective computing, social and cognitive psychology and neuroscience, e.g., \"show me all the images with happy faces\" or \"all images with AU 1 at intensity c\".",
            "referenceCount": 36,
            "citationCount": 457,
            "influentialCitationCount": 59,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-01",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Benitez-Quiroz2016EmotioNetAA,\n author = {C. F. Benitez-Quiroz and R. Srinivasan and Aleix M. Martinez},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5562-5570},\n title = {EmotioNet: An Accurate, Real-Time Algorithm for the Automatic Annotation of a Million Facial Expressions in the Wild},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e3ab08e93e5eb529692825cfedf6d3b6763bd76",
            "@type": "ScholarlyArticle",
            "paperId": "5e3ab08e93e5eb529692825cfedf6d3b6763bd76",
            "corpusId": 60519493,
            "url": "https://www.semanticscholar.org/paper/5e3ab08e93e5eb529692825cfedf6d3b6763bd76",
            "title": "Using color to separate reflection components",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1985,
            "externalIds": {
                "MAG": "3014734808",
                "DOI": "10.1002/COL.5080100409",
                "CorpusId": 60519493
            },
            "abstract": "Abstract : This paper presents an algorithm for analyzing a standard color image to determine intrinsic images of the amount of interface (specular) and body (diffuse) reflection at each pixel. The interface reflection represents the highlights from the original image, and the body reflection represents the original image with highlights removed. Such intrinsic images are of interest because the geometric properties of each type of reflection are simpler than the geometric properties of intensity in a black-and-white image. The algorithm is based upon a physical model of reflection which states that two distinct types of reflection--interface and body reflection--occur, and that each type can be decomposed into a relative spectral distribution and a geometric scale factor. This model is far more general than typical models used in computer vision and computer graphics, and includes most such models as special cases. In addition, the model does not assume a point light source or uniform illumination distribution over the scene. The properties of spectral projection into color space are used to derive a new model of pixel-value color distribution, and this model is exploited in an algorithm to derive the intrinsic images. Suggestions are provided for extending the model to deal with diffuse illumination and for analyzing the intrinsic images of reflection. Additional keywords: Dischromatic reflection model. (Author)",
            "referenceCount": 19,
            "citationCount": 1411,
            "influentialCitationCount": 111,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://urresearch.rochester.edu/fileDownloadForInstitutionalItem.action?itemId=2464&itemFileId=3408",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1985-12-01",
            "journal": {
                "name": "",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shafer1985UsingCT,\n author = {S. Shafer},\n pages = {43-51},\n title = {Using color to separate reflection components},\n volume = {10},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3971349aa161f410e7439af896382146edc4a865",
            "@type": "ScholarlyArticle",
            "paperId": "3971349aa161f410e7439af896382146edc4a865",
            "corpusId": 10747436,
            "url": "https://www.semanticscholar.org/paper/3971349aa161f410e7439af896382146edc4a865",
            "title": "TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2536305071",
                "DBLP": "conf/iccv/GuillauminMVS09",
                "DOI": "10.1109/ICCV.2009.5459266",
                "CorpusId": 10747436
            },
            "abstract": "Image auto-annotation is an important open problem in computer vision. For this task we propose TagProp, a discriminatively trained nearest neighbor model. Tags of test images are predicted using a weighted nearest-neighbor model to exploit labeled training images. Neighbor weights are based on neighbor rank or distance. TagProp allows the integration of metric learning by directly maximizing the log-likelihood of the tag predictions in the training set. In this manner, we can optimally combine a collection of image similarity metrics that cover different aspects of image content, such as local shape descriptors, or global color histograms. We also introduce a word specific sigmoidal modulation of the weighted neighbor tag predictions to boost the recall of rare words. We investigate the performance of different variants of our model and compare to existing work. We present experimental results for three challenging data sets. On all three, TagProp makes a marked improvement as compared to the current state-of-the-art.",
            "referenceCount": 28,
            "citationCount": 732,
            "influentialCitationCount": 137,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://lear.inrialpes.fr/pubs/2009/GMVS09/GMVS09.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-09-01",
            "journal": {
                "name": "2009 IEEE 12th International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Guillaumin2009TagPropDM,\n author = {M. Guillaumin and Thomas Mensink and J. Verbeek and C. Schmid},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2009 IEEE 12th International Conference on Computer Vision},\n pages = {309-316},\n title = {TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:932ac3707e1ed84ab67526692a1ef8f064f24ab5",
            "@type": "ScholarlyArticle",
            "paperId": "932ac3707e1ed84ab67526692a1ef8f064f24ab5",
            "corpusId": 10533233,
            "url": "https://www.semanticscholar.org/paper/932ac3707e1ed84ab67526692a1ef8f064f24ab5",
            "title": "Anticipating Visual Representations from Unlabeled Video",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2789198060",
                "DBLP": "conf/cvpr/VondrickPT16",
                "DOI": "10.1109/CVPR.2016.18",
                "CorpusId": 10533233
            },
            "abstract": "Anticipating actions and objects before they start or appear is a difficult problem in computer vision with several real-world applications. This task is challenging partly because it requires leveraging extensive knowledge of the world that is difficult to write down. We believe that a promising resource for efficiently learning this knowledge is through readily available unlabeled video. We present a framework that capitalizes on temporal structure in unlabeled video to learn to anticipate human actions and objects. The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions. We experimentally validate this idea on two datasets, anticipating actions one second in the future and objects five seconds in the future.",
            "referenceCount": 45,
            "citationCount": 473,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/113893/1/Torralba_Anticipating%20visual.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-04-29",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vondrick2015AnticipatingVR,\n author = {Carl Vondrick and H. Pirsiavash and A. Torralba},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {98-106},\n title = {Anticipating Visual Representations from Unlabeled Video},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8da267f7dec1cedc59f9c9aa7c4e1f88e3dcc62b",
            "@type": "ScholarlyArticle",
            "paperId": "8da267f7dec1cedc59f9c9aa7c4e1f88e3dcc62b",
            "corpusId": 9865213,
            "url": "https://www.semanticscholar.org/paper/8da267f7dec1cedc59f9c9aa7c4e1f88e3dcc62b",
            "title": "The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM",
            "venue": "Int. J. Robotics Res.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2567239141",
                "DBLP": "journals/corr/MuegglerRGDS16",
                "ArXiv": "1610.08336",
                "DOI": "10.1177/0278364917691115",
                "CorpusId": 9865213
            },
            "abstract": "New vision sensors, such as the dynamic and active-pixel vision sensor (DAVIS), incorporate a conventional global-shutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called \u201cevents\u201d) and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of ego-motion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e. rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.",
            "referenceCount": 25,
            "citationCount": 434,
            "influentialCitationCount": 81,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.sagepub.com/doi/pdf/10.1177/0278364917691115",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2016-10-26",
            "journal": {
                "name": "The International Journal of Robotics Research",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Mueggler2016TheED,\n author = {Elias Mueggler and Henri Rebecq and Guillermo Gallego and T. Delbr\u00fcck and D. Scaramuzza},\n booktitle = {Int. J. Robotics Res.},\n journal = {The International Journal of Robotics Research},\n pages = {142 - 149},\n title = {The event-camera dataset and simulator: Event-based data for pose estimation, visual odometry, and SLAM},\n volume = {36},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0df8575212ca58a746493607f3cf423ea2c35bef",
            "@type": "ScholarlyArticle",
            "paperId": "0df8575212ca58a746493607f3cf423ea2c35bef",
            "corpusId": 18502879,
            "url": "https://www.semanticscholar.org/paper/0df8575212ca58a746493607f3cf423ea2c35bef",
            "title": "Recognizing Objects in Range Data Using Regional Point Descriptors",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/eccv/FromeHKBM04",
                "MAG": "1564871316",
                "DOI": "10.1007/978-3-540-24672-5_18",
                "CorpusId": 18502879
            },
            "abstract": null,
            "referenceCount": 22,
            "citationCount": 914,
            "influentialCitationCount": 53,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-540-24672-5_18.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-05-11",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Frome2004RecognizingOI,\n author = {Andrea Frome and Daniel F. Huber and R. Kolluri and Thomas B\u00fclow and Jitendra Malik},\n booktitle = {European Conference on Computer Vision},\n pages = {224-237},\n title = {Recognizing Objects in Range Data Using Regional Point Descriptors},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:048bfc88b9f54512304433bb2eeb68a3172159a8",
            "@type": "ScholarlyArticle",
            "paperId": "048bfc88b9f54512304433bb2eeb68a3172159a8",
            "corpusId": 12793247,
            "url": "https://www.semanticscholar.org/paper/048bfc88b9f54512304433bb2eeb68a3172159a8",
            "title": "Multilinear Analysis of Image Ensembles: TensorFaces",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2113055885",
                "DBLP": "conf/eccv/VasilescuT02",
                "DOI": "10.1007/3-540-47969-4_30",
                "CorpusId": 12793247
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 935,
            "influentialCitationCount": 72,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vasilescu2002MultilinearAO,\n author = {M. Alex O. Vasilescu and Demetri Terzopoulos},\n booktitle = {European Conference on Computer Vision},\n pages = {447-460},\n title = {Multilinear Analysis of Image Ensembles: TensorFaces},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "@type": "ScholarlyArticle",
            "paperId": "2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "corpusId": 9680583,
            "url": "https://www.semanticscholar.org/paper/2323173a0bddac0dd2586b17a2f3ac33f401c45c",
            "title": "Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "202794677",
                "CorpusId": 9680583
            },
            "abstract": "Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.",
            "referenceCount": 31,
            "citationCount": 573,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2006-03-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Shakhnarovich2006NearestNeighborMI,\n author = {Gregory Shakhnarovich and Trevor Darrell and P. Indyk},\n title = {Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing)},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4875b6d6c6cc0ffc87d2452ac72eab8638b8921e",
            "@type": "ScholarlyArticle",
            "paperId": "4875b6d6c6cc0ffc87d2452ac72eab8638b8921e",
            "corpusId": 261364716,
            "url": "https://www.semanticscholar.org/paper/4875b6d6c6cc0ffc87d2452ac72eab8638b8921e",
            "title": "Is vision continuous with cognition? The case for cognitive impenetrability of visual perception.",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2105096388",
                "DOI": "10.1017/S0140525X99002022",
                "CorpusId": 261364716,
                "PubMed": "11301517"
            },
            "abstract": "Although the study of visual perception has made more progress in the past 40 years than any other area of cognitive science, there remain major disagreements as to how closely vision is tied to cognition. This target article sets out some of the arguments for both sides (arguments from computer vision, neuroscience, psychophysics, perceptual learning, and other areas of vision science) and defends the position that an important part of visual perception, corresponding to what some people have called early vision, is prohibited from accessing relevant expectations, knowledge, and utilities in determining the function it computes--in other words, it is cognitively impenetrable. That part of vision is complex and involves top-down interactions that are internal to the early vision system. Its function is to provide a structured representation of the 3-D surfaces of objects sufficient to serve as an index into memory, with somewhat different outputs being made available to other systems such as those dealing with motor control. The paper also addresses certain conceptual and methodological issues raised by this claim, such as whether signal detection theory and event-related potentials can be used to assess cognitive penetration of vision. A distinction is made among several stages in visual processing, including, in addition to the inflexible early-vision stage, a pre-perceptual attention-allocation stage and a post-perceptual evaluation, selection, and inference stage, which accesses long-term memory. These two stages provide the primary ways in which cognition can affect the outcome of visual perception. The paper discusses arguments from computer vision and psychology showing that vision is \"intelligent\" and involves elements of \"problem solving.\" The cases of apparently intelligent interpretation sometimes cited in support of this claim do not show cognitive penetration; rather, they show that certain natural constraints on interpretation, concerned primarily with optical and geometrical properties of the world, have been compiled into the visual system. The paper also examines a number of examples where instructions and \"hints\" are alleged to affect what is seen. In each case it is concluded that the evidence is more readily assimilated to the view that when cognitive effects are found, they have a locus outside early vision, in such processes as the allocation of focal attention and the identification of the stimulus.",
            "referenceCount": 490,
            "citationCount": 771,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "1999-06-01",
            "journal": {
                "name": "The Behavioral and brain sciences",
                "volume": "22 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Pylyshyn1999IsVC,\n author = {Z. Pylyshyn},\n booktitle = {Behavioral and Brain Sciences},\n journal = {The Behavioral and brain sciences},\n pages = {\n          341-65; discussion 366-423\n        },\n title = {Is vision continuous with cognition? The case for cognitive impenetrability of visual perception.},\n volume = {22 3},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:061d6d5f3df0db70b12f9e90bec327e19b7259c1",
            "@type": "ScholarlyArticle",
            "paperId": "061d6d5f3df0db70b12f9e90bec327e19b7259c1",
            "corpusId": 131776850,
            "url": "https://www.semanticscholar.org/paper/061d6d5f3df0db70b12f9e90bec327e19b7259c1",
            "title": "Local Relation Networks for Image Recognition",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1904-11491",
                "MAG": "2983446232",
                "ArXiv": "1904.11491",
                "DOI": "10.1109/ICCV.2019.00356",
                "CorpusId": 131776850
            },
            "abstract": "The convolution layer has been the dominant feature extractor in computer vision for years. However, the spatial aggregation in convolution is basically a pattern matching process that applies fixed filters which are inefficient at modeling visual elements with varying spatial distributions. This paper presents a new image feature extractor, called the local relation layer, that adaptively determines aggregation weights based on the compositional relationship of local pixel pairs. With this relational approach, it can composite visual elements into higher-level entities in a more efficient manner that benefits semantic inference. A network built with local relation layers, called the Local Relation Network (LR-Net), is found to provide greater modeling capacity than its counterpart built with regular convolution on large-scale recognition tasks such as ImageNet classification.",
            "referenceCount": 40,
            "citationCount": 398,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.11491",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-25",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2019LocalRN,\n author = {Han Hu and Zheng Zhang and Zhenda Xie and Stephen Lin},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {3463-3472},\n title = {Local Relation Networks for Image Recognition},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9e7e3b46475362c2e778fd9da2ee92ac82ef7146",
            "@type": "ScholarlyArticle",
            "paperId": "9e7e3b46475362c2e778fd9da2ee92ac82ef7146",
            "corpusId": 27499556,
            "url": "https://www.semanticscholar.org/paper/9e7e3b46475362c2e778fd9da2ee92ac82ef7146",
            "title": "Vision-based navigation in image-guided interventions.",
            "venue": "Annual Review of Biomedical Engineering",
            "publicationVenue": {
                "id": "urn:research:b1aef0b1-58ef-4dd9-aad5-2f40c42e19c4",
                "name": "Annual Review of Biomedical Engineering",
                "alternate_names": [
                    "Annu Rev Biomed Eng"
                ],
                "issn": "1523-9829",
                "url": "https://www.annualreviews.org/journal/bioeng"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2146082292",
                "DOI": "10.1146/annurev-bioeng-071910-124757",
                "CorpusId": 27499556,
                "PubMed": "21568713"
            },
            "abstract": "The trend toward minimally invasive surgical interventions has created new challenges for visualization during surgical procedures. However, at the same time, the introduction of high-definition digital endoscopy offers the opportunity to apply methods from computer vision to provide visualization enhancements such as anatomic reconstruction, surface registration, motion tracking, and augmented reality. This review provides a perspective on this rapidly evolving field. It first introduces the clinical and technical background necessary for developing vision-based algorithms for interventional applications. It then discusses several examples of clinical interventions where computer vision can be applied, including bronchoscopy, rhinoscopy, transnasal skull-base neurosurgery, upper airway interventions, laparoscopy, robotic-assisted surgery, and Natural Orifice Transluminal Endoscopic Surgery (NOTES). It concludes that the currently reported work is only the beginning. As the demand for minimally invasive procedures rises, computer vision in surgery will continue to advance through close interdisciplinary work between interventionists and engineers.",
            "referenceCount": 91,
            "citationCount": 114,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2011-07-14",
            "journal": {
                "name": "Annual review of biomedical engineering",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Mirota2011VisionbasedNI,\n author = {D. Mirota and M. Ishii and Gregory Hager},\n booktitle = {Annual Review of Biomedical Engineering},\n journal = {Annual review of biomedical engineering},\n pages = {\n          297-319\n        },\n title = {Vision-based navigation in image-guided interventions.},\n volume = {13},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41765f1f10bf2830c7749d8672eeabcb028381ae",
            "@type": "ScholarlyArticle",
            "paperId": "41765f1f10bf2830c7749d8672eeabcb028381ae",
            "corpusId": 8039756,
            "url": "https://www.semanticscholar.org/paper/41765f1f10bf2830c7749d8672eeabcb028381ae",
            "title": "The Generalized PatchMatch Correspondence Algorithm",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1763426478",
                "DBLP": "conf/eccv/BarnesSGF10",
                "DOI": "10.1007/978-3-642-15558-1_3",
                "CorpusId": 8039756
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 596,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Barnes2010TheGP,\n author = {Connelly Barnes and Eli Shechtman and Dan B. Goldman and Adam Finkelstein},\n booktitle = {European Conference on Computer Vision},\n pages = {29-43},\n title = {The Generalized PatchMatch Correspondence Algorithm},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b4df28b3206890ea364ea3dd2d420586d341447",
            "@type": "ScholarlyArticle",
            "paperId": "8b4df28b3206890ea364ea3dd2d420586d341447",
            "corpusId": 21656238,
            "url": "https://www.semanticscholar.org/paper/8b4df28b3206890ea364ea3dd2d420586d341447",
            "title": "A review of semantic segmentation using deep neural networks",
            "venue": "International Journal of Multimedia Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:d9c19003-0fdf-4501-b3cd-cf35b240bc6a",
                "name": "International Journal of Multimedia Information Retrieval",
                "alternate_names": [
                    "Int J Multimedia Inf Retr"
                ],
                "issn": "2192-662X",
                "url": "http://www.springer.com/computer/journal/13735"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2770233088",
                "DBLP": "journals/ijmir/GuoLGL18",
                "DOI": "10.1007/s13735-017-0141-z",
                "CorpusId": 21656238
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 465,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s13735-017-0141-z.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "International Journal of Multimedia Information Retrieval",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Guo2018ARO,\n author = {Yanming Guo and Y. Liu and Theodoros Georgiou and M. Lew},\n booktitle = {International Journal of Multimedia Information Retrieval},\n journal = {International Journal of Multimedia Information Retrieval},\n pages = {87-93},\n title = {A review of semantic segmentation using deep neural networks},\n volume = {7},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6dd6d184005388f75c411f27e224b2b6b87b7c60",
            "@type": "ScholarlyArticle",
            "paperId": "6dd6d184005388f75c411f27e224b2b6b87b7c60",
            "corpusId": 16922659,
            "url": "https://www.semanticscholar.org/paper/6dd6d184005388f75c411f27e224b2b6b87b7c60",
            "title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/HornBFHBIPB15",
                "MAG": "1954152232",
                "DOI": "10.1109/CVPR.2015.7298658",
                "CorpusId": 16922659
            },
            "abstract": "We introduce tools and methodologies to collect high quality, large scale fine-grained computer vision datasets using citizen scientists - crowd annotators who are passionate and knowledgeable about specific domains such as birds or airplanes. We worked with citizen scientists and domain experts to collect NABirds, a new high quality dataset containing 48,562 images of North American birds with 555 categories, part annotations and bounding boxes. We find that citizen scientists are significantly more accurate than Mechanical Turkers at zero cost. We worked with bird experts to measure the quality of popular datasets like CUB-200-2011 and ImageNet and found class label error rates of at least 4%. Nevertheless, we found that learning algorithms are surprisingly robust to annotation errors and this level of training data corruption can lead to an acceptably small increase in test error if the training set has sufficient size. At the same time, we found that an expert-curated high quality test set like NABirds is necessary to accurately measure the performance of fine-grained computer vision systems. We used NABirds to train a publicly available bird recognition service deployed on the web site of the Cornell Lab of Ornithology.",
            "referenceCount": 41,
            "citationCount": 407,
            "influentialCitationCount": 75,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Horn2015BuildingAB,\n author = {Grant Van Horn and Steve Branson and Ryan Farrell and Scott Haber and Jessie Barry and Panagiotis G. Ipeirotis and P. Perona and Serge J. Belongie},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {595-604},\n title = {Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a4230b89af8f05c593d3f0218add12fefd8a4bda",
            "@type": "ScholarlyArticle",
            "paperId": "a4230b89af8f05c593d3f0218add12fefd8a4bda",
            "corpusId": 207568300,
            "url": "https://www.semanticscholar.org/paper/a4230b89af8f05c593d3f0218add12fefd8a4bda",
            "title": "Representations for Rigid Solids: Theory, Methods, and Systems",
            "venue": "CSUR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1980,
            "externalIds": {
                "DBLP": "journals/csur/Requicha80",
                "MAG": "2100185791",
                "DOI": "10.1145/356827.356833",
                "CorpusId": 207568300
            },
            "abstract": "Computer-based systems for modehng the geometry of rigid solid objects are becoming increasingly important in mechanical and civil engineering, architecture, computer graphics, computer vision, and other fields that deal with spatial phenomena. At the heart of such systems are symbol structures (representations) designating \"abstract solids\" (subsets of Euclidean space) that model physical solids. Representations are the sources of data for procedures which compute useful properties of objects. The variety and uses of systems embodying representations of solids are growing rapidly, but so are the difficulties in assessing current designs, specifying the characteristics that future systems should exhibit, and designing systems t9 meet such specifications. This paper resolves many of these difficulties by providing a coherent view, based on sound theoretical principles, of what is presently known about the representation of solids. The paper is divided into three parts. The first introduces a simple mathematical framework for characterizing certain important aspects of representations, for example, their semantic (geometric) integrity. The second part uses the framework to describe and compare all of the major knownschemes fo~ representing solids. The third part briefly surveys extant geometric modeling systems and then applies the concepts developed in the paper to the high-level design of a multiple*representation geometric modeling system which exhibits a level of reliability and versatility supermr to that of systems currently used in industrial computer-aided design and manufacturing.",
            "referenceCount": 68,
            "citationCount": 1525,
            "influentialCitationCount": 53,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1980-12-01",
            "journal": {
                "name": "ACM Comput. Surv.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Requicha1980RepresentationsFR,\n author = {A. Requicha},\n booktitle = {CSUR},\n journal = {ACM Comput. Surv.},\n pages = {437-464},\n title = {Representations for Rigid Solids: Theory, Methods, and Systems},\n volume = {12},\n year = {1980}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1903563df0c85c8be6f9b0a76549ab62780f875a",
            "@type": "ScholarlyArticle",
            "paperId": "1903563df0c85c8be6f9b0a76549ab62780f875a",
            "corpusId": 117999851,
            "url": "https://www.semanticscholar.org/paper/1903563df0c85c8be6f9b0a76549ab62780f875a",
            "title": "Vision Science : Photons to Phenomenology",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1655654231",
                "CorpusId": 117999851
            },
            "abstract": "This book revolutionizes how vision can be taught to undergraduate and graduate students in cognitive science, psychology, and optometry. It is the first comprehensive textbook on vision to reflect the integrated computational approach of modern research scientists. This new interdisciplinary approach, called \"vision science,\" integrates psychological, computational, and neuroscientific perspectives. The book covers all major topics related to vision, from early neural processing of image structure in the retina to high-level visual attention, memory, imagery, and awareness. The presentation throughout is theoretically sophisticated yet requires minimal knowledge of mathematics. There is also an extensive glossary, as well as appendices on psychophysical methods, connectionist modeling, and color technology. The book will serve not only as a comprehensive textbook on vision, but also as a valuable reference for researchers in cognitive science, psychology, neuroscience, computer science, optometry, and philosophy.",
            "referenceCount": 0,
            "citationCount": 753,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-05-07",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Palmer1999VisionS,\n author = {S. Palmer},\n title = {Vision Science : Photons to Phenomenology},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e96c4626e4b3d09727bcbfbdb2672dd9b886743",
            "@type": "ScholarlyArticle",
            "paperId": "5e96c4626e4b3d09727bcbfbdb2672dd9b886743",
            "corpusId": 876150,
            "url": "https://www.semanticscholar.org/paper/5e96c4626e4b3d09727bcbfbdb2672dd9b886743",
            "title": "An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/WalkerDGH16",
                "ArXiv": "1606.07873",
                "MAG": "2952390294",
                "DOI": "10.1007/978-3-319-46478-7_51",
                "CorpusId": 876150
            },
            "abstract": null,
            "referenceCount": 35,
            "citationCount": 492,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1606.07873",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Walker2016AnUF,\n author = {Jacob Walker and Carl Doersch and A. Gupta and M. Hebert},\n booktitle = {European Conference on Computer Vision},\n pages = {835-851},\n title = {An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d391c46f4ad940ffb1e58b7af6215789aca27b94",
            "@type": "ScholarlyArticle",
            "paperId": "d391c46f4ad940ffb1e58b7af6215789aca27b94",
            "corpusId": 262234827,
            "url": "https://www.semanticscholar.org/paper/d391c46f4ad940ffb1e58b7af6215789aca27b94",
            "title": "Content-Based Image Retrieval at the End of the Early Years",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2000,
            "externalIds": {
                "DBLP": "journals/pami/SmeuldersWSGJ00",
                "MAG": "2620916795",
                "DOI": "10.1109/34.895972",
                "CorpusId": 262234827
            },
            "abstract": "Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.",
            "referenceCount": 216,
            "citationCount": 997,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2000-12-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Smeulders2000ContentBasedIR,\n author = {A. Smeulders and M. Worring and Simone Santini and Amarnath Gupta and Ramesh C. Jain},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {1349-1380},\n title = {Content-Based Image Retrieval at the End of the Early Years},\n volume = {22},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cc9b9277175fdbb8dd97def4fd2fd1e189352ce",
            "@type": "ScholarlyArticle",
            "paperId": "3cc9b9277175fdbb8dd97def4fd2fd1e189352ce",
            "corpusId": 17615337,
            "url": "https://www.semanticscholar.org/paper/3cc9b9277175fdbb8dd97def4fd2fd1e189352ce",
            "title": "Coherence-Enhancing Diffusion Filtering",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "journals/ijcv/Weickert99",
                "MAG": "127809959",
                "DOI": "10.1023/A:1008009714131",
                "CorpusId": 17615337
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 889,
            "influentialCitationCount": 125,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-04-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Weickert1999CoherenceEnhancingDF,\n author = {Joachim Weickert},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {111-127},\n title = {Coherence-Enhancing Diffusion Filtering},\n volume = {31},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8dae3252f478fccf818f38a5641ee24c42b69719",
            "@type": "ScholarlyArticle",
            "paperId": "8dae3252f478fccf818f38a5641ee24c42b69719",
            "corpusId": 13467224,
            "url": "https://www.semanticscholar.org/paper/8dae3252f478fccf818f38a5641ee24c42b69719",
            "title": "FPGA Design and Implementation of a Real-Time Stereo Vision System",
            "venue": "IEEE transactions on circuits and systems for video technology (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/tcsv/JinCPLPKJ10",
                "MAG": "2128369195",
                "DOI": "10.1109/TCSVT.2009.2026831",
                "CorpusId": 13467224
            },
            "abstract": "Stereo vision is a well-known ranging method because it resembles the basic mechanism of the human eye. However, the computational complexity and large amount of data access make real-time processing of stereo vision challenging because of the inherent instruction cycle delay within conventional computers. In order to solve this problem, the past 20 years of research have focused on the use of dedicated hardware architecture for stereo vision. This paper proposes a fully pipelined stereo vision system providing a dense disparity image with additional sub-pixel accuracy in real-time. The entire stereo vision process, such as rectification, stereo matching, and post-processing, is realized using a single field programmable gate array (FPGA) without the necessity of any external devices. The hardware implementation is more than 230 times faster when compared to a software program operating on a conventional computer, and shows stronger performance over previous hardware-related studies.",
            "referenceCount": 29,
            "citationCount": 234,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Circuits and Systems for Video Technology",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Jin2010FPGADA,\n author = {S. Jin and J. U. Cho and X. Pham and Kyoung Mu Lee and Sung-Kee Park and Munsang Kim and J. Jeon},\n booktitle = {IEEE transactions on circuits and systems for video technology (Print)},\n journal = {IEEE Transactions on Circuits and Systems for Video Technology},\n pages = {15-26},\n title = {FPGA Design and Implementation of a Real-Time Stereo Vision System},\n volume = {20},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9b208891d1287ebb5b84ac801b41c3313d7e3303",
            "@type": "ScholarlyArticle",
            "paperId": "9b208891d1287ebb5b84ac801b41c3313d7e3303",
            "corpusId": 3571438,
            "url": "https://www.semanticscholar.org/paper/9b208891d1287ebb5b84ac801b41c3313d7e3303",
            "title": "Wasserstein Barycenter and Its Application to Texture Mixing",
            "venue": "Scale Space and Variational Methods in Computer Vision",
            "publicationVenue": {
                "id": "urn:research:28223649-39bd-477b-90df-d2af723bd82a",
                "name": "Scale Space and Variational Methods in Computer Vision",
                "alternate_names": [
                    "Int Conf Scale Space Var Method comput vis",
                    "Scale Space Var Method Comput Vis",
                    "SSVM",
                    "International Conference on Scale Space and Variational Methods in computer vision"
                ],
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1639961155",
                "DBLP": "conf/scalespace/RabinPDB11",
                "DOI": "10.1007/978-3-642-24785-9_37",
                "CorpusId": 3571438
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 574,
            "influentialCitationCount": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://hal.archives-ouvertes.fr/hal-00476064/file/TexturesECCV10.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-05-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rabin2011WassersteinBA,\n author = {J. Rabin and G. Peyr\u00e9 and J. Delon and M. Bernot},\n booktitle = {Scale Space and Variational Methods in Computer Vision},\n pages = {435-446},\n title = {Wasserstein Barycenter and Its Application to Texture Mixing},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1de7dae4b03fb3ab1eeffc45d2fd761538962d5c",
            "@type": "ScholarlyArticle",
            "paperId": "1de7dae4b03fb3ab1eeffc45d2fd761538962d5c",
            "corpusId": 210714181,
            "url": "https://www.semanticscholar.org/paper/1de7dae4b03fb3ab1eeffc45d2fd761538962d5c",
            "title": "FedVision: An Online Visual Object Detection Platform Powered by Federated Learning",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2001.06202",
                "DBLP": "conf/aaai/LiuHLHLCFCYY20",
                "MAG": "3000514287",
                "DOI": "10.1609/aaai.v34i08.7021",
                "CorpusId": 210714181
            },
            "abstract": "Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks.",
            "referenceCount": 39,
            "citationCount": 201,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/7021/6875",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-01-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2001.06202"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2020FedVisionAO,\n author = {Yang Liu and Anbu Huang and Yu Luo and He Huang and Youzhi Liu and Yuanyuan Chen and Lican Feng and Tianjian Chen and Hang Yu and Qiang Yang},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {FedVision: An Online Visual Object Detection Platform Powered by Federated Learning},\n volume = {abs/2001.06202},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bed7834ae7d371171977a590872f60d137c2f951",
            "@type": "ScholarlyArticle",
            "paperId": "bed7834ae7d371171977a590872f60d137c2f951",
            "corpusId": 36417,
            "url": "https://www.semanticscholar.org/paper/bed7834ae7d371171977a590872f60d137c2f951",
            "title": "GuessWhat?! Visual Object Discovery through Multi-modal Dialogue",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.08481",
                "DBLP": "journals/corr/VriesSCPLC16",
                "MAG": "2558809543",
                "DOI": "10.1109/CVPR.2017.475",
                "CorpusId": 36417
            },
            "abstract": "We introduce GuessWhat?!, a two-player guessing game as a testbed for research on the interplay of computer vision and dialogue systems. The goal of the game is to locate an unknown object in a rich image scene by asking a sequence of questions. Higher-level image understanding, like spatial reasoning and language grounding, is required to solve the proposed task. Our key contribution is the collection of a large-scale dataset consisting of 150K human-played games with a total of 800K visual question-answer pairs on 66K images. We explain our design decisions in collecting the dataset and introduce the oracle and questioner tasks that are associated with the two players of the game. We prototyped deep learning models to establish initial baselines of the introduced tasks.",
            "referenceCount": 52,
            "citationCount": 381,
            "influentialCitationCount": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08481",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-23",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vries2016GuessWhatVO,\n author = {Harm de Vries and Florian Strub and A. Chandar and O. Pietquin and H. Larochelle and Aaron C. Courville},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4466-4475},\n title = {GuessWhat?! Visual Object Discovery through Multi-modal Dialogue},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8a043ca4a868a7f1099da1c8ef5928295969d338",
            "@type": "ScholarlyArticle",
            "paperId": "8a043ca4a868a7f1099da1c8ef5928295969d338",
            "corpusId": 7981750,
            "url": "https://www.semanticscholar.org/paper/8a043ca4a868a7f1099da1c8ef5928295969d338",
            "title": "Visual SLAM algorithms: a survey from 2010 to 2016",
            "venue": "IPSJ Transactions on Computer Vision and Applications",
            "publicationVenue": {
                "id": "urn:research:28739d82-07b8-4f7f-8605-d2a17ea2fabb",
                "name": "IPSJ Transactions on Computer Vision and Applications",
                "alternate_names": [
                    "Ipsj Transactions on Computer Vision and Applications",
                    "IPSJ Trans Comput Vis Appl",
                    "Ipsj Trans Comput Vis Appl"
                ],
                "issn": "1882-6695",
                "url": "http://ipsjcva.springeropen.com/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2621274416",
                "DBLP": "journals/ipsjtcva/TaketomiUI17",
                "DOI": "10.1186/s41074-017-0027-2",
                "CorpusId": 7981750
            },
            "abstract": null,
            "referenceCount": 89,
            "citationCount": 492,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ipsjcva.springeropen.com/track/pdf/10.1186/s41074-017-0027-2",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-06-02",
            "journal": {
                "name": "IPSJ Transactions on Computer Vision and Applications",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Taketomi2017VisualSA,\n author = {Takafumi Taketomi and Hideaki Uchiyama and Sei Ikeda},\n booktitle = {IPSJ Transactions on Computer Vision and Applications},\n journal = {IPSJ Transactions on Computer Vision and Applications},\n pages = {1-11},\n title = {Visual SLAM algorithms: a survey from 2010 to 2016},\n volume = {9},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4114c7cb6c8ebf7987c1ff536e1d2f81c3d04238",
            "@type": "ScholarlyArticle",
            "paperId": "4114c7cb6c8ebf7987c1ff536e1d2f81c3d04238",
            "corpusId": 980797,
            "url": "https://www.semanticscholar.org/paper/4114c7cb6c8ebf7987c1ff536e1d2f81c3d04238",
            "title": "Gradient vector flow: a new external force for snakes",
            "venue": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "conf/cvpr/XuP97",
                "MAG": "2106160885",
                "DOI": "10.1109/CVPR.1997.609299",
                "CorpusId": 980797
            },
            "abstract": "Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries. Problems associated with initialization and poor convergence to concave boundaries, however, have limited their utility. This paper develops a new external force for active contours, largely solving both problems. This external force, which we call gradient vector flow (GVF) is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image. The resultant field has a large capture range and forces active contours into concave regions. Examples on simulated images and one real image are presented.",
            "referenceCount": 18,
            "citationCount": 994,
            "influentialCitationCount": 110,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://iacl.ece.jhu.edu/%7Echenyang/research/pubs/p087c.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1997-06-17",
            "journal": {
                "name": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xu1997GradientVF,\n author = {Chenyang Xu and Jerry L Prince},\n booktitle = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {66-71},\n title = {Gradient vector flow: a new external force for snakes},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4975a215804aefac5765cb6e69cdaf9d9f3caec3",
            "@type": "ScholarlyArticle",
            "paperId": "4975a215804aefac5765cb6e69cdaf9d9f3caec3",
            "corpusId": 13746398,
            "url": "https://www.semanticscholar.org/paper/4975a215804aefac5765cb6e69cdaf9d9f3caec3",
            "title": "Hand Pose Estimation via Latent 2.5D Heatmap Regression",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2798581336",
                "DBLP": "journals/corr/abs-1804-09534",
                "ArXiv": "1804.09534",
                "DOI": "10.1007/978-3-030-01252-6_8",
                "CorpusId": 13746398
            },
            "abstract": null,
            "referenceCount": 68,
            "citationCount": 277,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1804.09534",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Iqbal2018HandPE,\n author = {Umar Iqbal and Pavlo Molchanov and T. Breuel and Juergen Gall and J. Kautz},\n booktitle = {European Conference on Computer Vision},\n pages = {125-143},\n title = {Hand Pose Estimation via Latent 2.5D Heatmap Regression},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5df0a0e9ceec70a9321b0555288222bf53216342",
            "@type": "ScholarlyArticle",
            "paperId": "5df0a0e9ceec70a9321b0555288222bf53216342",
            "corpusId": 62529370,
            "url": "https://www.semanticscholar.org/paper/5df0a0e9ceec70a9321b0555288222bf53216342",
            "title": "Neural Networks in Machine Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2096338573",
                "CorpusId": 62529370
            },
            "abstract": "Machine Learning is associated with the study and construction of systems that can learn on their own rather than following instructions. It is used in search engines, optical character recognition, computer vision etc. Neural networks are one of the several techniques used in machine learning. Here we are trying to discuss neural network approaches used in machine learning.",
            "referenceCount": 11,
            "citationCount": 414,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2014-11-18",
            "journal": {
                "name": "International Journal of Computer Applications",
                "volume": "105"
            },
            "citationStyles": {
                "bibtex": "@Article{Parashar2014NeuralNI,\n author = {Parul Parashar},\n journal = {International Journal of Computer Applications},\n pages = {1-3},\n title = {Neural Networks in Machine Learning},\n volume = {105},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf1b860d657db9e0874d08520bad2846a809faff",
            "@type": "ScholarlyArticle",
            "paperId": "bf1b860d657db9e0874d08520bad2846a809faff",
            "corpusId": 6173689,
            "url": "https://www.semanticscholar.org/paper/bf1b860d657db9e0874d08520bad2846a809faff",
            "title": "Computer Systems Based on Silicon Photonic Interconnects",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2135129294",
                "DBLP": "journals/pieee/KrishnamoorthyH09",
                "DOI": "10.1109/JPROC.2009.2020712",
                "CorpusId": 6173689
            },
            "abstract": "We present a computing microsystem that uniquely leverages the bandwidth, density, and latency advantages of silicon photonic interconnect to enable highly compact supercomputer-scale systems. We describe and justify single-node and multinode systems interconnected with wavelength-routed optical links, quantify their benefits vis-a-vis electrically connected systems, analyze the constituent optical component and system requirements, and provide an overview of the critical technologies needed to fulfill this system vision. This vision calls for more than a hundredfold reduction in energy to communicate an optical bit of information. We explore the power dissipation of a photonic link, suggest a roadmap to lower the energy-per-bit of silicon photonic interconnects, and identify the challenges that will be faced by device and circuit designers towards this goal.",
            "referenceCount": 64,
            "citationCount": 438,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-06-10",
            "journal": {
                "name": "Proceedings of the IEEE",
                "volume": "97"
            },
            "citationStyles": {
                "bibtex": "@Article{Krishnamoorthy2009ComputerSB,\n author = {A. Krishnamoorthy and R. Ho and Xuezhe Zheng and H. Schwetman and J. Lexau and P. Koka and Guoliang Li and I. Shubin and J. Cunningham},\n booktitle = {Proceedings of the IEEE},\n journal = {Proceedings of the IEEE},\n pages = {1337-1361},\n title = {Computer Systems Based on Silicon Photonic Interconnects},\n volume = {97},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0916866570527b609566404ab80ae066cbd3ca7c",
            "@type": "ScholarlyArticle",
            "paperId": "0916866570527b609566404ab80ae066cbd3ca7c",
            "corpusId": 17667834,
            "url": "https://www.semanticscholar.org/paper/0916866570527b609566404ab80ae066cbd3ca7c",
            "title": "A Survey of Recent Advances in Face Detection",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2125277152",
                "CorpusId": 17667834
            },
            "abstract": "Face detection has been one of the most studied topics in the computer vision literature. In this technical report, we survey the recent advances in face detection for the past decade. The seminal Viola-Jones face detector is first reviewed. We then survey the various techniques according to how they extract features and what learning algorithms are adopted. It is our hope that by reviewing the many existing algorithms, we will see even better algorithms developed to solve this fundamental computer vision problem. 1",
            "referenceCount": 119,
            "citationCount": 640,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2010-06-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zhang2010ASO,\n author = {Cha Zhang and Zhengyou Zhang},\n title = {A Survey of Recent Advances in Face Detection},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2071002b9f3edcbe46daedd1e3576cd1fb0be22f",
            "@type": "ScholarlyArticle",
            "paperId": "2071002b9f3edcbe46daedd1e3576cd1fb0be22f",
            "corpusId": 15199761,
            "url": "https://www.semanticscholar.org/paper/2071002b9f3edcbe46daedd1e3576cd1fb0be22f",
            "title": "Computational Color Constancy: Survey and Experiments",
            "venue": "IEEE Transactions on Image Processing",
            "publicationVenue": {
                "id": "urn:research:e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                "name": "IEEE Transactions on Image Processing",
                "alternate_names": [
                    "IEEE Trans Image Process"
                ],
                "issn": "1057-7149",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/tip/GijsenijGW11",
                "MAG": "2100001370",
                "DOI": "10.1109/TIP.2011.2118224",
                "CorpusId": 15199761,
                "PubMed": "21342844"
            },
            "abstract": "Computational color constancy is a fundamental prerequisite for many computer vision applications. This paper presents a survey of many recent developments and state-of-the-art methods. Several criteria are proposed that are used to assess the approaches. A taxonomy of existing algorithms is proposed and methods are separated in three groups: static methods, gamut-based methods, and learning-based methods. Further, the experimental setup is discussed including an overview of publicly available datasets. Finally, various freely available methods, of which some are considered to be state of the art, are evaluated on two datasets.",
            "referenceCount": 139,
            "citationCount": 536,
            "influentialCitationCount": 50,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2011-09-01",
            "journal": {
                "name": "IEEE Transactions on Image Processing",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Gijsenij2011ComputationalCC,\n author = {A. Gijsenij and T. Gevers and Joost van de Weijer},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE Transactions on Image Processing},\n pages = {2475-2489},\n title = {Computational Color Constancy: Survey and Experiments},\n volume = {20},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:49cc13128473fb5c056d577c46011cea45eb786d",
            "@type": "ScholarlyArticle",
            "paperId": "49cc13128473fb5c056d577c46011cea45eb786d",
            "corpusId": 218684764,
            "url": "https://www.semanticscholar.org/paper/49cc13128473fb5c056d577c46011cea45eb786d",
            "title": "CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-09544",
                "MAG": "3027552584",
                "ArXiv": "2005.09544",
                "DOI": "10.1109/CVPR42600.2020.00549",
                "CorpusId": 218684764
            },
            "abstract": "The unprecedented increase in the usage of computer vision technology in society goes hand in hand with an increased concern in data privacy. In many real-world scenarios like people tracking or action recognition, it is important to be able to process the data while taking careful consideration in protecting people's identity. We propose and develop CIAGAN, a model for image and video anonymization based on conditional generative adversarial networks. Our model is able to remove the identifying characteristics of faces and bodies while producing high-quality images and videos that can be used for any computer vision task, such as detection or tracking. Unlike previous methods, we have full control over the de-identification (anonymization) procedure, ensuring both anonymization as well as diversity. We compare our method to several baselines and achieve state-of-the-art results. To facilitate further research, we make available the code and the models at https://github.com/dvl-tum/ciagan.",
            "referenceCount": 45,
            "citationCount": 112,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2005.09544",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-05-19",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Maximov2020CIAGANCI,\n author = {Maxim Maximov and Ismail Elezi and L. Leal-Taix'e},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5446-5455},\n title = {CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7fdf31d5ebdd293b3027e6555e256a936ff5515a",
            "@type": "ScholarlyArticle",
            "paperId": "7fdf31d5ebdd293b3027e6555e256a936ff5515a",
            "corpusId": 14362511,
            "url": "https://www.semanticscholar.org/paper/7fdf31d5ebdd293b3027e6555e256a936ff5515a",
            "title": "Simultaneous image classification and annotation",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/cvpr/WangBL09",
                "MAG": "2161050705",
                "DOI": "10.1109/CVPR.2009.5206800",
                "CorpusId": 14362511
            },
            "abstract": "Image classification and annotation are important problems in computer vision, but rarely considered together. Intuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely annotated with words \u201croad,\u201d \u201ccar,\u201d and \u201ctraffic\u201d than words \u201cfish,\u201d \u201cboat,\u201d and \u201cscuba.\u201d In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation performance, and superior classification performance.",
            "referenceCount": 28,
            "citationCount": 612,
            "influentialCitationCount": 79,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-06-20",
            "journal": {
                "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2009SimultaneousIC,\n author = {Chong Wang and D. Blei and Li Fei-Fei},\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1903-1910},\n title = {Simultaneous image classification and annotation},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4b9f5cf22da4c0a12290eccc939f6cf20ab7edae",
            "@type": "ScholarlyArticle",
            "paperId": "4b9f5cf22da4c0a12290eccc939f6cf20ab7edae",
            "corpusId": 18600991,
            "url": "https://www.semanticscholar.org/paper/4b9f5cf22da4c0a12290eccc939f6cf20ab7edae",
            "title": "Yesterday\u2019s tomorrows: notes on ubiquitous computing\u2019s dominant vision",
            "venue": "Personal and Ubiquitous Computing",
            "publicationVenue": {
                "id": "urn:research:68fd8242-3be0-4b1e-8b5d-ad0a8a02db12",
                "name": "Personal and Ubiquitous Computing",
                "alternate_names": [
                    "Pers Ubiquitous Comput"
                ],
                "issn": "1617-4909",
                "url": "http://www.springer.com/computer/hci/journal/779"
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2004623491",
                "DBLP": "journals/puc/BellD07",
                "DOI": "10.1007/s00779-006-0071-x",
                "CorpusId": 18600991
            },
            "abstract": null,
            "referenceCount": 49,
            "citationCount": 472,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-01-24",
            "journal": {
                "name": "Personal and Ubiquitous Computing",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Bell2007YesterdaysTN,\n author = {Genevieve Bell and P. Dourish},\n booktitle = {Personal and Ubiquitous Computing},\n journal = {Personal and Ubiquitous Computing},\n pages = {133-143},\n title = {Yesterday\u2019s tomorrows: notes on ubiquitous computing\u2019s dominant vision},\n volume = {11},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50435ca83b4548a3abada283821ed5502361bcee",
            "@type": "ScholarlyArticle",
            "paperId": "50435ca83b4548a3abada283821ed5502361bcee",
            "corpusId": 8606013,
            "url": "https://www.semanticscholar.org/paper/50435ca83b4548a3abada283821ed5502361bcee",
            "title": "Small Vision Systems: Hardware and Implementation",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1602129629",
                "DOI": "10.1007/978-1-4471-1580-9_19",
                "CorpusId": 8606013
            },
            "abstract": null,
            "referenceCount": 16,
            "citationCount": 641,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Konolige1998SmallVS,\n author = {K. Konolige},\n pages = {203-212},\n title = {Small Vision Systems: Hardware and Implementation},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:025a44bf059aef8b9ee2e6ca598bebefc59a4a61",
            "@type": "ScholarlyArticle",
            "paperId": "025a44bf059aef8b9ee2e6ca598bebefc59a4a61",
            "corpusId": 49551723,
            "url": "https://www.semanticscholar.org/paper/025a44bf059aef8b9ee2e6ca598bebefc59a4a61",
            "title": "Human Action Recognition and Prediction: A Survey",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-11230",
                "MAG": "2810685774",
                "ArXiv": "1806.11230",
                "DOI": "10.1007/s11263-022-01594-9",
                "CorpusId": 49551723
            },
            "abstract": null,
            "referenceCount": 335,
            "citationCount": 340,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1806.11230",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-06-28",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "130"
            },
            "citationStyles": {
                "bibtex": "@Article{Kong2018HumanAR,\n author = {Yu Kong and Y. Fu},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {1366 - 1401},\n title = {Human Action Recognition and Prediction: A Survey},\n volume = {130},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce51aa425cccb509830f5e5882230ff976344cc5",
            "@type": "ScholarlyArticle",
            "paperId": "ce51aa425cccb509830f5e5882230ff976344cc5",
            "corpusId": 1219701,
            "url": "https://www.semanticscholar.org/paper/ce51aa425cccb509830f5e5882230ff976344cc5",
            "title": "Fast Approximate Energy Minimization with Label Costs",
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/cvpr/DelongOIB10",
                "MAG": "3112798525",
                "DOI": "10.1007/s11263-011-0437-z",
                "CorpusId": 1219701
            },
            "abstract": null,
            "referenceCount": 72,
            "citationCount": 576,
            "influentialCitationCount": 46,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-13",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "96"
            },
            "citationStyles": {
                "bibtex": "@Article{Delong2010FastAE,\n author = {Andrew Delong and A. Osokin and Hossam N. Isack and Yuri Boykov},\n booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {International Journal of Computer Vision},\n pages = {1-27},\n title = {Fast Approximate Energy Minimization with Label Costs},\n volume = {96},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b2639ef3220e13dee1a9b477572711a3f2d06e6",
            "@type": "ScholarlyArticle",
            "paperId": "2b2639ef3220e13dee1a9b477572711a3f2d06e6",
            "corpusId": 6397880,
            "url": "https://www.semanticscholar.org/paper/2b2639ef3220e13dee1a9b477572711a3f2d06e6",
            "title": "Multi-view Subspace Clustering",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/GaoNLH15",
                "MAG": "2199534117",
                "DOI": "10.1109/ICCV.2015.482",
                "CorpusId": 6397880
            },
            "abstract": "For many computer vision applications, the data sets distribute on certain low-dimensional subspaces. Subspace clustering is to find such underlying subspaces and cluster the data points correctly. In this paper, we propose a novel multi-view subspace clustering method. The proposed method performs clustering on the subspace representation of each view simultaneously. Meanwhile, we propose to use a common cluster structure to guarantee the consistence among different views. In addition, an efficient algorithm is proposed to solve the problem. Experiments on four benchmark data sets have been performed to validate our proposed method. The promising results demonstrate the effectiveness of our method.",
            "referenceCount": 27,
            "citationCount": 361,
            "influentialCitationCount": 41,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-07",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gao2015MultiviewSC,\n author = {Hongchang Gao and F. Nie and Xuelong Li and Heng Huang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {4238-4246},\n title = {Multi-view Subspace Clustering},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6a16d519a5ce9a27cefd11c4f96c714fd2413c50",
            "@type": "ScholarlyArticle",
            "paperId": "6a16d519a5ce9a27cefd11c4f96c714fd2413c50",
            "corpusId": 10294200,
            "url": "https://www.semanticscholar.org/paper/6a16d519a5ce9a27cefd11c4f96c714fd2413c50",
            "title": "The Fast Bilateral Solver",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1511.03296",
                "MAG": "2952075905",
                "DBLP": "conf/eccv/BarronP16",
                "DOI": "10.1007/978-3-319-46487-9_38",
                "CorpusId": 10294200
            },
            "abstract": null,
            "referenceCount": 64,
            "citationCount": 338,
            "influentialCitationCount": 42,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-10",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.03296"
            },
            "citationStyles": {
                "bibtex": "@Article{Barron2015TheFB,\n author = {J. Barron and Ben Poole},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {The Fast Bilateral Solver},\n volume = {abs/1511.03296},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6419cd260b314c22bad57053dfd48c09411490d3",
            "@type": "ScholarlyArticle",
            "paperId": "6419cd260b314c22bad57053dfd48c09411490d3",
            "corpusId": 53286887,
            "url": "https://www.semanticscholar.org/paper/6419cd260b314c22bad57053dfd48c09411490d3",
            "title": "Scene Text Detection and Recognition: The Deep Learning Era",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1811-04256",
                "MAG": "3082397598",
                "ArXiv": "1811.04256",
                "DOI": "10.1007/s11263-020-01369-0",
                "CorpusId": 53286887
            },
            "abstract": null,
            "referenceCount": 234,
            "citationCount": 303,
            "influentialCitationCount": 12,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1811.04256",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-11-10",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "129"
            },
            "citationStyles": {
                "bibtex": "@Article{Long2018SceneTD,\n author = {Shangbang Long and Xin He and C. Yao},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {161 - 184},\n title = {Scene Text Detection and Recognition: The Deep Learning Era},\n volume = {129},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:78dd9a3760c948ceb331f983653f3c6d77174e71",
            "@type": "ScholarlyArticle",
            "paperId": "78dd9a3760c948ceb331f983653f3c6d77174e71",
            "corpusId": 7510079,
            "url": "https://www.semanticscholar.org/paper/78dd9a3760c948ceb331f983653f3c6d77174e71",
            "title": "On the unification of line processes, outlier rejection, and robust statistics with applications in early vision",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2088616581",
                "DBLP": "journals/ijcv/BlackR96",
                "DOI": "10.1007/BF00131148",
                "CorpusId": 7510079
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 705,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.brown.edu/people/black/Papers/ijcv.19.1.1996.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1996-07-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Black1996OnTU,\n author = {Michael J. Black and Anand Rangarajan},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {57-91},\n title = {On the unification of line processes, outlier rejection, and robust statistics with applications in early vision},\n volume = {19},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98d4da29934552e5c0d8bdfd5fbcb5df965b5b9e",
            "@type": "ScholarlyArticle",
            "paperId": "98d4da29934552e5c0d8bdfd5fbcb5df965b5b9e",
            "corpusId": 46746239,
            "url": "https://www.semanticscholar.org/paper/98d4da29934552e5c0d8bdfd5fbcb5df965b5b9e",
            "title": "A review on deep convolutional neural networks",
            "venue": "International Conference on Cryptography, Security and Privacy",
            "publicationVenue": {
                "id": "urn:research:e251322f-1b8a-43c4-ae82-44aaa356cd4f",
                "name": "International Conference on Cryptography, Security and Privacy",
                "alternate_names": [
                    "Int Conf Cryptogr Secur Priv",
                    "International Conference Cryptography, Security and Privacy",
                    "ICCSP",
                    "International Conference on Communication and Signal Processing",
                    "Int Conf Commun Signal Process"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2786055572",
                "DOI": "10.1109/ICCSP.2017.8286426",
                "CorpusId": 46746239
            },
            "abstract": "The success of traditional methods for solving computer vision problems heavily depends on the feature extraction process. But Convolutional Neural Networks (CNN) have provided an alternative for automatically learning the domain specific features. Now every problem in the broader domain of computer vision is re-examined from the perspective of this new methodology. Therefore it is essential to figure-out the type of network specific to a problem. In this work, we have done a thorough literature survey of Convolutional Neural Networks which is the widely used framework of deep learning. With AlexNet as the base CNN model, we have reviewed all the variations emerged over time to suit various applications and a small discussion on the available frameworks for the implementation of the same. We hope this piece of article will really serve as a guide for any neophyte in the area.",
            "referenceCount": 28,
            "citationCount": 326,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference",
                "Review"
            ],
            "publicationDate": "2017-04-06",
            "journal": {
                "name": "2017 International Conference on Communication and Signal Processing (ICCSP)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Aloysius2017ARO,\n author = {Neena Aloysius and M. Geetha},\n booktitle = {International Conference on Cryptography, Security and Privacy},\n journal = {2017 International Conference on Communication and Signal Processing (ICCSP)},\n pages = {0588-0592},\n title = {A review on deep convolutional neural networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dcbf587642c39f495117552ca453a4f955ffa76a",
            "@type": "ScholarlyArticle",
            "paperId": "dcbf587642c39f495117552ca453a4f955ffa76a",
            "corpusId": 1811598,
            "url": "https://www.semanticscholar.org/paper/dcbf587642c39f495117552ca453a4f955ffa76a",
            "title": "Analyzing the Performance of Multilayer Neural Networks for Object Recognition",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/AgrawalGM14a",
                "ArXiv": "1407.1610",
                "MAG": "2949511860",
                "DOI": "10.1007/978-3-319-10584-0_22",
                "CorpusId": 1811598
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 417,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10584-0_22.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-07-07",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Agrawal2014AnalyzingTP,\n author = {Pulkit Agrawal and Ross B. Girshick and Jitendra Malik},\n booktitle = {European Conference on Computer Vision},\n pages = {329-344},\n title = {Analyzing the Performance of Multilayer Neural Networks for Object Recognition},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8701c031bd254012b355cbda5a3f5977c068fc07",
            "@type": "ScholarlyArticle",
            "paperId": "8701c031bd254012b355cbda5a3f5977c068fc07",
            "corpusId": 2377428,
            "url": "https://www.semanticscholar.org/paper/8701c031bd254012b355cbda5a3f5977c068fc07",
            "title": "Algorithms for Constraint-Satisfaction Problems: A Survey",
            "venue": "The AI Magazine",
            "publicationVenue": {
                "id": "urn:research:6fedff74-7525-4b7f-bbb4-4df4e23948e4",
                "name": "The AI Magazine",
                "alternate_names": [
                    "AI Mag",
                    "Ai Mag",
                    "Ai Magazine"
                ],
                "issn": "0738-4602",
                "url": "https://www.aaai.org/Library/Magazine/magazine-library.php"
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "journals/aim/Kumar92",
                "MAG": "2148546044",
                "DOI": "10.1609/aimag.v13i1.976",
                "CorpusId": 2377428
            },
            "abstract": "A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.",
            "referenceCount": 103,
            "citationCount": 1137,
            "influentialCitationCount": 77,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1992-04-01",
            "journal": {
                "name": "AI Mag.",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Kumar1992AlgorithmsFC,\n author = {Vipin Kumar},\n booktitle = {The AI Magazine},\n journal = {AI Mag.},\n pages = {32-44},\n title = {Algorithms for Constraint-Satisfaction Problems: A Survey},\n volume = {13},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7944d0b061610b1c67ad15efdf192681e60d0129",
            "@type": "ScholarlyArticle",
            "paperId": "7944d0b061610b1c67ad15efdf192681e60d0129",
            "corpusId": 10569278,
            "url": "https://www.semanticscholar.org/paper/7944d0b061610b1c67ad15efdf192681e60d0129",
            "title": "Spatially Adaptive Computation Time for Residual Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/FigurnovCZZHVS17",
                "ArXiv": "1612.02297",
                "MAG": "2562731582",
                "DOI": "10.1109/CVPR.2017.194",
                "CorpusId": 10569278
            },
            "abstract": "This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.",
            "referenceCount": 44,
            "citationCount": 306,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.02297",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-07",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Figurnov2016SpatiallyAC,\n author = {Michael Figurnov and Maxwell D. Collins and Yukun Zhu and Li Zhang and Jonathan Huang and D. Vetrov and R. Salakhutdinov},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1790-1799},\n title = {Spatially Adaptive Computation Time for Residual Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "@type": "ScholarlyArticle",
            "paperId": "e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "corpusId": 15872360,
            "url": "https://www.semanticscholar.org/paper/e9d7f589a3d368a3701832e28d90ca09ec9e5577",
            "title": "Segmentation using eigenvectors: a unifying view",
            "venue": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2160167256",
                "DBLP": "conf/iccv/Weiss99",
                "DOI": "10.1109/ICCV.1999.790354",
                "CorpusId": 15872360
            },
            "abstract": "Automatic grouping and segmentation of images remains a challenging problem in computer vision. Recently, a number of authors have demonstrated good performance on this task using methods that are based on eigenvectors of the affinity matrix. These approaches are extremely attractive in that they are based on simple eigendecomposition algorithms whose stability is well understood. Nevertheless, the use of eigendecompositions in the context of segmentation is far from well understood. In this paper we give a unified treatment of these algorithms, and show the close connections between them while highlighting their distinguishing features. We then prove results on eigenvectors of block matrices that allow us to analyze the performance of these algorithms in simple grouping settings. Finally, we use our analysis to motivate a variation on the existing methods that combines aspects from different eigenvector segmentation algorithms. We illustrate our analysis with results on real and synthetic images.",
            "referenceCount": 11,
            "citationCount": 863,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://people.csail.mit.edu/torralba/courses/6.869/lectures/lecture12/yair.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1999-09-20",
            "journal": {
                "name": "Proceedings of the Seventh IEEE International Conference on Computer Vision",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Weiss1999SegmentationUE,\n author = {Yair Weiss},\n booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},\n pages = {975-982 vol.2},\n title = {Segmentation using eigenvectors: a unifying view},\n volume = {2},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05de05f242f62bd597666c7821d99dc568d47b6c",
            "@type": "ScholarlyArticle",
            "paperId": "05de05f242f62bd597666c7821d99dc568d47b6c",
            "corpusId": 12000150,
            "url": "https://www.semanticscholar.org/paper/05de05f242f62bd597666c7821d99dc568d47b6c",
            "title": "The office of the future: a unified approach to image-based modeling and spatially immersive displays",
            "venue": "International Conference on Computer Graphics and Interactive Techniques",
            "publicationVenue": {
                "id": "urn:research:cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                "name": "International Conference on Computer Graphics and Interactive Techniques",
                "alternate_names": [
                    "Int Conf Comput Graph Interact Tech",
                    "SIGGRAPH"
                ],
                "issn": null,
                "url": "http://www.siggraph.org/"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/siggraph/RaskarWCLSF98",
                "MAG": "2237155658",
                "DOI": "10.1145/280814.280861",
                "CorpusId": 12000150
            },
            "abstract": "We introduce ideas, proposed technologies, and initial results for an office of the future that is based on a unified application of computer vision and computer graphics in a system that combines and builds upon the notions of the CAVE\u2122, tiled display systems, and image-based modeling . The basic idea is to use real-time computer vision techniques to dynamically extract per-pixel depth and reflectance information for the visible surfaces in the office including walls, furniture, objects, and people, and then to either project images on the surfaces, render images of the surfaces , or interpret changes in the surfaces. In the first case, one could designate every-day (potentially irregular) real surfaces in the office to be used as spatially immersive display surfaces, and then project high-resolution graphics and text onto those surfaces. In the second case, one could transmit the dynamic image-based models over a network for display at a remote site. Finally, one could interpret dynamic changes in the surfaces for the purposes of tracking, interaction, or augmented reality applications. To accomplish the simultaneous capture and display we envision an office of the future where the ceiling lights are replaced by computer controlled cameras and \u201csmart\u201d projectors that are used to capture dynamic image-based models with imperceptible structured light techniques, and to display high-resolution images on designated display surfaces. By doing both simultaneously on the designated display surfaces, one can dynamically adjust or autocalibrate for geometric, intensity, and resolution variations resulting from irregular or changing display surfaces, or overlapped projector images. Our current approach to dynamic image-based modeling is to use an optimized structured light scheme that can capture per-pixel depth and reflectance at interactive rates. Our system implementation is not yet imperceptible, but we can demonstrate the approach in the laboratory. Our approach to rendering on the designated (potentially irregular) display surfaces is to employ a two-pass projective texture scheme to generate images that when projected onto the surfaces appear correct to a moving headtracked observer. We present here an initial implementation of the overall vision, in an office-like setting, and preliminary demonstrations of our dynamic modeling and display techniques.",
            "referenceCount": 53,
            "citationCount": 972,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "Proceedings of the 25th annual conference on Computer graphics and interactive techniques",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Raskar1998TheOO,\n author = {R. Raskar and G. Welch and Matt Cutts and Adam T. Lake and Lev Stesin and H. Fuchs},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {Proceedings of the 25th annual conference on Computer graphics and interactive techniques},\n title = {The office of the future: a unified approach to image-based modeling and spatially immersive displays},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9800e3c3394c569be83379ee2ebe3424e09c2919",
            "@type": "ScholarlyArticle",
            "paperId": "9800e3c3394c569be83379ee2ebe3424e09c2919",
            "corpusId": 1274537,
            "url": "https://www.semanticscholar.org/paper/9800e3c3394c569be83379ee2ebe3424e09c2919",
            "title": "What Does Classifying More Than 10, 000 Image Categories Tell Us?",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1959000896",
                "DBLP": "conf/eccv/DengBLF10",
                "DOI": "10.1007/978-3-642-15555-0_6",
                "CorpusId": 1274537
            },
            "abstract": null,
            "referenceCount": 43,
            "citationCount": 558,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-642-15555-0_6.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2010WhatDC,\n author = {Jia Deng and A. Berg and K. Li and Li Fei-Fei},\n booktitle = {European Conference on Computer Vision},\n pages = {71-84},\n title = {What Does Classifying More Than 10, 000 Image Categories Tell Us?},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bca6248f5970f42018ff506b9e6681fc7f8f98a3",
            "@type": "ScholarlyArticle",
            "paperId": "bca6248f5970f42018ff506b9e6681fc7f8f98a3",
            "corpusId": 1877246,
            "url": "https://www.semanticscholar.org/paper/bca6248f5970f42018ff506b9e6681fc7f8f98a3",
            "title": "A Survey of Urban Reconstruction",
            "venue": "Computer graphics forum (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2127310338",
                "DBLP": "journals/cgf/MusialskiWAWGP13",
                "DOI": "10.1111/cgf.12077",
                "CorpusId": 1877246
            },
            "abstract": "This paper provides a comprehensive overview of urban reconstruction. While there exists a considerable body of literature, this topic is still under active research. The work reviewed in this survey stems from the following three research communities: computer graphics, computer vision and photogrammetry and remote sensing. Our goal is to provide a survey that will help researchers to better position their own work in the context of existing solutions, and to help newcomers and practitioners in computer graphics to quickly gain an overview of this vast field. Further, we would like to bring the mentioned research communities to even more interdisciplinary work, since the reconstruction problem itself is by far not solved.",
            "referenceCount": 308,
            "citationCount": 472,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kaust.edu.sa/bitstream/10754/565947/1/2013.CGF.Musialski.UrbanSurvey.HighRes.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Geography",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-09-01",
            "journal": {
                "name": "Computer Graphics Forum",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Musialski2013ASO,\n author = {Przemyslaw Musialski and Peter Wonka and Daniel G. Aliaga and M. Wimmer and L. Gool and W. Purgathofer},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {A Survey of Urban Reconstruction},\n volume = {32},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "@type": "ScholarlyArticle",
            "paperId": "8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "corpusId": 2751624,
            "url": "https://www.semanticscholar.org/paper/8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "title": "Tracking people with twists and exponential maps",
            "venue": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2118025528",
                "DBLP": "conf/cvpr/BreglerM98",
                "DOI": "10.1109/CVPR.1998.698581",
                "CorpusId": 2751624
            },
            "abstract": "This paper demonstrates a new visual motion estimation technique that is able to recover high degree-of-freedom articulated human body configurations in complex video sequences. We introduce the use of a novel mathematical technique, the product of exponential maps and twist motions, and its integration into a differential motion estimation. This results in solving simple linear systems, and enables us to recover robustly the kinematic degrees-of-freedom in noise and complex self occluded configurations. We demonstrate this on several image sequences of people doing articulated full body movements, and visualize the results in re-animating an artificial 3D human model. We are also able to recover and re-animate the famous movements of Eadweard Muybridge's motion studies from the last century. To the best of our knowledge, this is the first computer vision based system that is able to process such challenging footage and recover complex motions with such high accuracy.",
            "referenceCount": 31,
            "citationCount": 893,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.jhu.edu/~hager/Public/teaching/CS600.641/bregler_malik_cvpr98.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-06-23",
            "journal": {
                "name": "Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bregler1998TrackingPW,\n author = {C. Bregler and Jitendra Malik},\n booktitle = {Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)},\n journal = {Proceedings. 1998 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.98CB36231)},\n pages = {8-15},\n title = {Tracking people with twists and exponential maps},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cc1cc334897af613e9103de9afe23e1882e0c153",
            "@type": "ScholarlyArticle",
            "paperId": "cc1cc334897af613e9103de9afe23e1882e0c153",
            "corpusId": 102354614,
            "url": "https://www.semanticscholar.org/paper/cc1cc334897af613e9103de9afe23e1882e0c153",
            "title": "Hyperbolic Image Embeddings",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3035102141",
                "ArXiv": "1904.02239",
                "DBLP": "journals/corr/abs-1904-02239",
                "DOI": "10.1109/cvpr42600.2020.00645",
                "CorpusId": 102354614
            },
            "abstract": "Computer vision tasks such as image classification, image retrieval, and few-shot learning are currently dominated by Euclidean and spherical embeddings so that the final decisions about class belongings or the degree of similarity are made using linear hyperplanes, Euclidean distances, or spherical geodesic distances (cosine similarity). In this work, we demonstrate that in many practical scenarios, hyperbolic embeddings provide a better alternative.",
            "referenceCount": 67,
            "citationCount": 179,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.02239",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-03",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Khrulkov2019HyperbolicIE,\n author = {Valentin Khrulkov and L. Mirvakhabova and E. Ustinova and I. Oseledets and V. Lempitsky},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6417-6427},\n title = {Hyperbolic Image Embeddings},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed7038b052af85886bf96c3511910ad654e1811a",
            "@type": "ScholarlyArticle",
            "paperId": "ed7038b052af85886bf96c3511910ad654e1811a",
            "corpusId": 19645724,
            "url": "https://www.semanticscholar.org/paper/ed7038b052af85886bf96c3511910ad654e1811a",
            "title": "Light Field Image Processing: An Overview",
            "venue": "IEEE Journal on Selected Topics in Signal Processing",
            "publicationVenue": {
                "id": "urn:research:e93ebb7d-cfa6-4361-8051-3c6dff3eed1f",
                "name": "IEEE Journal on Selected Topics in Signal Processing",
                "alternate_names": [
                    "IEEE J Sel Top Signal Process",
                    "IEEE Journal of Selected Topics in Signal Processing"
                ],
                "issn": "1932-4553",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=4200690"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2753630964",
                "DBLP": "journals/jstsp/WuMJZWDCL17",
                "DOI": "10.1109/JSTSP.2017.2747126",
                "CorpusId": 19645724
            },
            "abstract": "Light field imaging has emerged as a technology allowing to capture richer visual information from our world. As opposed to traditional photography, which captures a 2D projection of the light in the scene integrating the angular domain, light fields collect radiance from rays in all directions, demultiplexing the angular information lost in conventional photography. On the one hand, this higher dimensional representation of visual data offers powerful capabilities for scene understanding, and substantially improves the performance of traditional computer vision problems such as depth sensing, post-capture refocusing, segmentation, video stabilization, material classification, etc. On the other hand, the high-dimensionality of light fields also brings up new challenges in terms of data capture, data compression, content editing, and display. Taking these two elements together, research in light field image processing has become increasingly popular in the computer vision, computer graphics, and signal processing communities. In this paper, we present a comprehensive overview and discussion of research in this field over the past 20 years. We focus on all aspects of light field image processing, including basic light field representation and theory, acquisition, super-resolution, depth estimation, compression, editing, processing algorithms for light field display, and computer vision applications of light field data.",
            "referenceCount": 274,
            "citationCount": 367,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://zaguan.unizar.es/record/74812/files/texto_completo.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2017-08-30",
            "journal": {
                "name": "IEEE Journal of Selected Topics in Signal Processing",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2017LightFI,\n author = {Gaochang Wu and B. Masi\u00e1 and A. Jarabo and Yuchen Zhang and Liangyong Wang and Qionghai Dai and Tianyou Chai and Yebin Liu},\n booktitle = {IEEE Journal on Selected Topics in Signal Processing},\n journal = {IEEE Journal of Selected Topics in Signal Processing},\n pages = {926-954},\n title = {Light Field Image Processing: An Overview},\n volume = {11},\n year = {2017}\n}\n"
            }
        }
    }
]