[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ecacee864757cc320fca7d7063a4cf969cb591c8",
            "@type": "ScholarlyArticle",
            "paperId": "ecacee864757cc320fca7d7063a4cf969cb591c8",
            "corpusId": 35272041,
            "url": "https://www.semanticscholar.org/paper/ecacee864757cc320fca7d7063a4cf969cb591c8",
            "title": "Biologically Motivated Computer Vision",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "164569299",
                "DBLP": "conf/bmcv/2002",
                "DOI": "10.1007/3-540-36181-2",
                "CorpusId": 35272041
            },
            "abstract": null,
            "referenceCount": 84,
            "citationCount": 167,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2002-11-01",
            "journal": {
                "name": null,
                "volume": "2525"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{B\u00fclthoff2002BiologicallyMC,\n author = {H. B\u00fclthoff and C. Wallraven and Seong-Whan Lee and T. Poggio},\n booktitle = {Lecture Notes in Computer Science},\n title = {Biologically Motivated Computer Vision},\n volume = {2525},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:914d36b8de3df6cabbc8235772e581179683501c",
            "@type": "ScholarlyArticle",
            "paperId": "914d36b8de3df6cabbc8235772e581179683501c",
            "corpusId": 10299609,
            "url": "https://www.semanticscholar.org/paper/914d36b8de3df6cabbc8235772e581179683501c",
            "title": "Discrete Representation of Spatial Objects in Computer Vision",
            "venue": "Computational Imaging and Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "series/civ/Latecki98",
                "MAG": "1481230418",
                "DOI": "10.1007/978-94-015-9002-0",
                "CorpusId": 10299609
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 65,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Latecki1998DiscreteRO,\n author = {Longin Jan Latecki},\n booktitle = {Computational Imaging and Vision},\n pages = {1-216},\n title = {Discrete Representation of Spatial Objects in Computer Vision},\n volume = {11},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5b13e6aa926f8cf82f48935905d7bc353c16a42b",
            "@type": "ScholarlyArticle",
            "paperId": "5b13e6aa926f8cf82f48935905d7bc353c16a42b",
            "corpusId": 33519317,
            "url": "https://www.semanticscholar.org/paper/5b13e6aa926f8cf82f48935905d7bc353c16a42b",
            "title": "Coping With Discontinuities In Computer Vision: Their Detection, Classification, And Measurement",
            "venue": "[1988 Proceedings] Second International Conference on Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1988,
            "externalIds": {
                "DBLP": "conf/iccv/Lee88a",
                "MAG": "1530056892",
                "DOI": "10.1109/CCV.1988.590035",
                "CorpusId": 33519317
            },
            "abstract": "The general principles of detection, classification, and measurement of discontinuities are studied. The following issues are discussed: detecting the location of discontinuities; classifying discontinuities by their degrees; measuring the size of discontinuities; and coping with the random noise and designing optimal discontinuity detectors. An algorithm is proposed for discontinuity detection from an input signal S. For degree k discontinuity detection and measurement, a detector (P, Phi ) is used, where P is the pattern and Phi is the corresponding filter. If there is a degree k discontinuity at location t/sub 0/, then in the filter response there is a scaled pattern alpha P at t/sub 0/, where alpha is the size of the discontinuity. This reduces the problem to searching for the scaled pattern in the filter response. A statistical method is proposed for the approximate pattern matching. To cope with the random noise, a study is made of optimal detectors, which minimize the effects of noise. >",
            "referenceCount": 27,
            "citationCount": 75,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1988-12-05",
            "journal": {
                "name": "[1988 Proceedings] Second International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lee1988CopingWD,\n author = {David Lee},\n booktitle = {[1988 Proceedings] Second International Conference on Computer Vision},\n journal = {[1988 Proceedings] Second International Conference on Computer Vision},\n pages = {546-557},\n title = {Coping With Discontinuities In Computer Vision: Their Detection, Classification, And Measurement},\n year = {1988}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ff99fff320f5e44dff043e855613783a555bfe4e",
            "@type": "ScholarlyArticle",
            "paperId": "ff99fff320f5e44dff043e855613783a555bfe4e",
            "corpusId": 62547266,
            "url": "https://www.semanticscholar.org/paper/ff99fff320f5e44dff043e855613783a555bfe4e",
            "title": "Parallel computer vision",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1987,
            "externalIds": {
                "MAG": "2093413210",
                "CorpusId": 62547266
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 104,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1987-09-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Uhr1987ParallelCV,\n author = {L. Uhr},\n title = {Parallel computer vision},\n year = {1987}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0492ce821fabd55bcd3385f06612a825a07c26cd",
            "@type": "ScholarlyArticle",
            "paperId": "0492ce821fabd55bcd3385f06612a825a07c26cd",
            "corpusId": 40680855,
            "url": "https://www.semanticscholar.org/paper/0492ce821fabd55bcd3385f06612a825a07c26cd",
            "title": "Probabilistic relaxation for matching problems in computer vision",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 1993,
            "externalIds": {
                "MAG": "2108726364",
                "DBLP": "conf/iccv/KittlerCP93",
                "DOI": "10.1109/ICCV.1993.378148",
                "CorpusId": 40680855
            },
            "abstract": "The authors present the theory of probabilistic relaxation for matching symbolic structures, derive as limiting cases the various heuristic formulas used by researchers in matching problems, and state the conditions under which they apply. They successfully apply the theory to the problem of matching and recognizing aerial road network images based on road network models and to the problem of edge matching in a stereo pair. For this purpose, each line network is represented by an attributed relational graph where each node is a straight line segment characterized by certain attributes and related with every other node via a set of binary relations.<<ETX>>",
            "referenceCount": 26,
            "citationCount": 65,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1993-05-11",
            "journal": {
                "name": "1993 (4th) International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kittler1993ProbabilisticRF,\n author = {J. Kittler and W. Christmas and M. Petrou},\n booktitle = {Vision},\n journal = {1993 (4th) International Conference on Computer Vision},\n pages = {666-673},\n title = {Probabilistic relaxation for matching problems in computer vision},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1b9c9601f19c415f4802f44195c21ccfd2705fbd",
            "@type": "ScholarlyArticle",
            "paperId": "1b9c9601f19c415f4802f44195c21ccfd2705fbd",
            "corpusId": 35615086,
            "url": "https://www.semanticscholar.org/paper/1b9c9601f19c415f4802f44195c21ccfd2705fbd",
            "title": "Scale Space Methods in Computer Vision",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/scalespace/2003",
                "MAG": "2481865072",
                "DOI": "10.1007/3-540-44935-3",
                "CorpusId": 35615086
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 115,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-540-44935-5/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "2695"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Griffin2003ScaleSM,\n author = {Lewis D. Griffin and M. Lillholm},\n booktitle = {Lecture Notes in Computer Science},\n title = {Scale Space Methods in Computer Vision},\n volume = {2695},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8dde86db9d0c98672c271288c9efe6549925f86",
            "@type": "ScholarlyArticle",
            "paperId": "c8dde86db9d0c98672c271288c9efe6549925f86",
            "corpusId": 27041556,
            "url": "https://www.semanticscholar.org/paper/c8dde86db9d0c98672c271288c9efe6549925f86",
            "title": "Computer vision theory: The lack thereof",
            "venue": "Computer Vision Graphics and Image Processing",
            "publicationVenue": {
                "id": "urn:research:451e9e22-42a8-43b7-ac3e-33b7d5de65e3",
                "name": "Computer Vision Graphics and Image Processing",
                "alternate_names": [
                    "Comput Vis Graph Image Process"
                ],
                "issn": "0734-189X",
                "url": "https://www.sciencedirect.com/journal/computer-vision-graphics-and-image-processing"
            },
            "year": 1986,
            "externalIds": {
                "MAG": "2009159061",
                "DBLP": "journals/cvgip/Haralick86",
                "DOI": "10.1016/0734-189X(86)90082-4",
                "CorpusId": 27041556
            },
            "abstract": null,
            "referenceCount": 5,
            "citationCount": 70,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1986-11-01",
            "journal": {
                "name": "Comput. Vis. Graph. Image Process.",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Haralick1986ComputerVT,\n author = {R. Haralick},\n booktitle = {Computer Vision Graphics and Image Processing},\n journal = {Comput. Vis. Graph. Image Process.},\n pages = {372-386},\n title = {Computer vision theory: The lack thereof},\n volume = {36},\n year = {1986}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18366169e2c8e058993621c15fdfa5655527efb7",
            "@type": "ScholarlyArticle",
            "paperId": "18366169e2c8e058993621c15fdfa5655527efb7",
            "corpusId": 117690408,
            "url": "https://www.semanticscholar.org/paper/18366169e2c8e058993621c15fdfa5655527efb7",
            "title": "Color in Computer Vision",
            "venue": "Handbook of Pattern Recognition and Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "31292679",
                "DBLP": "books/ws/93/Luong93",
                "DOI": "10.1142/9789814343138_0012",
                "CorpusId": 117690408
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 36,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1993-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Luong1993ColorIC,\n author = {Q. Luong},\n booktitle = {Handbook of Pattern Recognition and Computer Vision},\n pages = {311-368},\n title = {Color in Computer Vision},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a374c6fb55d7a2f356a51fc47a88cb8903214059",
            "@type": "ScholarlyArticle",
            "paperId": "a374c6fb55d7a2f356a51fc47a88cb8903214059",
            "corpusId": 19460790,
            "url": "https://www.semanticscholar.org/paper/a374c6fb55d7a2f356a51fc47a88cb8903214059",
            "title": "Computer Vision \u2014 ECCV '94",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2495756989",
                "DBLP": "conf/eccv/1994-1",
                "DOI": "10.1007/3-540-57956-7",
                "CorpusId": 19460790
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 153,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "800"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Eklundh1994ComputerV,\n author = {J. Eklundh},\n booktitle = {Lecture Notes in Computer Science},\n title = {Computer Vision \u2014 ECCV '94},\n volume = {800},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c346b70e838ad53cb44a1537447f0b3d3019db3",
            "@type": "ScholarlyArticle",
            "paperId": "3c346b70e838ad53cb44a1537447f0b3d3019db3",
            "corpusId": 108442672,
            "url": "https://www.semanticscholar.org/paper/3c346b70e838ad53cb44a1537447f0b3d3019db3",
            "title": "Sorting fish by computer vision",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2000176567",
                "DOI": "10.1016/S0168-1699(99)00030-7",
                "CorpusId": 108442672
            },
            "abstract": null,
            "referenceCount": 7,
            "citationCount": 89,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-09-01",
            "journal": {
                "name": "Computers and Electronics in Agriculture",
                "volume": "23"
            },
            "citationStyles": {
                "bibtex": "@Article{Zion1999SortingFB,\n author = {B. Zion and A. Shklyar and I. Karplus},\n journal = {Computers and Electronics in Agriculture},\n pages = {175-187},\n title = {Sorting fish by computer vision},\n volume = {23},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f636462eeff5553a5077e23c6349b5ef0513c9f",
            "@type": "ScholarlyArticle",
            "paperId": "2f636462eeff5553a5077e23c6349b5ef0513c9f",
            "corpusId": 29749335,
            "url": "https://www.semanticscholar.org/paper/2f636462eeff5553a5077e23c6349b5ef0513c9f",
            "title": "Computer Vision \u2014 ECCV'92",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1992,
            "externalIds": {
                "MAG": "2487251778",
                "DBLP": "conf/eccv/1992",
                "DOI": "10.1007/3-540-55426-2",
                "CorpusId": 29749335
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 124,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "588"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sandini1992ComputerV,\n author = {G. Sandini},\n booktitle = {Lecture Notes in Computer Science},\n title = {Computer Vision \u2014 ECCV'92},\n volume = {588},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:891c7da9be8899308d1558a130d24f1cae1ccc69",
            "@type": "ScholarlyArticle",
            "paperId": "891c7da9be8899308d1558a130d24f1cae1ccc69",
            "corpusId": 984229,
            "url": "https://www.semanticscholar.org/paper/891c7da9be8899308d1558a130d24f1cae1ccc69",
            "title": "Computer Vision \u2014 ECCV '96",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2504689794",
                "DBLP": "conf/eccv/1996-2",
                "DOI": "10.1007/3-540-61123-1",
                "CorpusId": 984229
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 200,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-540-49950-3/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "1065"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Buxton1996ComputerV,\n author = {B. Buxton and R. Cipolla},\n booktitle = {Lecture Notes in Computer Science},\n title = {Computer Vision \u2014 ECCV '96},\n volume = {1065},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef6c5e67b84912cfef8d0d343634d5d29f92145a",
            "@type": "ScholarlyArticle",
            "paperId": "ef6c5e67b84912cfef8d0d343634d5d29f92145a",
            "corpusId": 199689,
            "url": "https://www.semanticscholar.org/paper/ef6c5e67b84912cfef8d0d343634d5d29f92145a",
            "title": "Why aspect graphs are not (yet) practical for computer vision",
            "venue": "[1991 Proceedings] Workshop on Directions in Automated CAD-Based Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1991,
            "externalIds": {
                "DBLP": "journals/cviu/FaugerasMADPJIB92",
                "MAG": "2179779711",
                "DOI": "10.1109/CADVIS.1991.148762",
                "CorpusId": 199689
            },
            "abstract": "The aspect graph of an object is a graph structure in which each node represents a general view of the object as seen from some maximal, connected cell of viewpoint space; each arc represents an accidental view (or visual event) which occurs on the boundary between two cells of general viewpoint; there is a node for each possible general view of the object, and there is an arc for each possible visual event. The authors provide a critique of the aspect graph approach.<<ETX>>",
            "referenceCount": 0,
            "citationCount": 67,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://deepblue.lib.umich.edu/bitstream/2027.42/30175/1/0000560.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1991-06-02",
            "journal": {
                "name": "[1991 Proceedings] Workshop on Directions in Automated CAD-Based Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Faugeras1991WhyAG,\n author = {O. Faugeras and J. Mundy and N. Ahuja and C. Dyer and A. Pentland and R. Jain and K. Ikeuchi and K. Bowyer},\n booktitle = {[1991 Proceedings] Workshop on Directions in Automated CAD-Based Vision},\n journal = {[1991 Proceedings] Workshop on Directions in Automated CAD-Based Vision},\n pages = {97-104},\n title = {Why aspect graphs are not (yet) practical for computer vision},\n year = {1991}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:832d9b082ef30e6ba2b9d76da6982a256b211ffe",
            "@type": "ScholarlyArticle",
            "paperId": "832d9b082ef30e6ba2b9d76da6982a256b211ffe",
            "corpusId": 31440931,
            "url": "https://www.semanticscholar.org/paper/832d9b082ef30e6ba2b9d76da6982a256b211ffe",
            "title": "Object Representation in Computer Vision",
            "venue": "Lecture Notes in Computer Science",
            "publicationVenue": {
                "id": "urn:research:2f5d0e8a-faad-4f10-b323-2b2e3c439a78",
                "name": "Lecture Notes in Computer Science",
                "alternate_names": [
                    "LNCS",
                    "Transactions on Computational Systems Biology",
                    "Trans Comput Syst Biology",
                    "Lect Note Comput Sci"
                ],
                "issn": "0302-9743",
                "url": "http://www.springer.com/lncs"
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2506577142",
                "DBLP": "conf/nsf/1994",
                "DOI": "10.1007/3-540-60477-4",
                "CorpusId": 31440931
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 52,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "994"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hebert1994ObjectRI,\n author = {M. Hebert and J. Ponce and T. Boult and A. Gross},\n booktitle = {Lecture Notes in Computer Science},\n title = {Object Representation in Computer Vision},\n volume = {994},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24efb327f1154965f12915f0a6c483594d2b5e06",
            "@type": "ScholarlyArticle",
            "paperId": "24efb327f1154965f12915f0a6c483594d2b5e06",
            "corpusId": 15508056,
            "url": "https://www.semanticscholar.org/paper/24efb327f1154965f12915f0a6c483594d2b5e06",
            "title": "Vision-Based Offline-Online Perception Paradigm for Autonomous Driving",
            "venue": "2015 IEEE Winter Conference on Applications of Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/wacv/RosRGBVL15",
                "MAG": "2067107771",
                "DOI": "10.1109/WACV.2015.38",
                "CorpusId": 15508056
            },
            "abstract": "Autonomous driving is a key factor for future mobility. Properly perceiving the environment of the vehicles is essential for a safe driving, which requires computing accurate geometric and semantic information in real-time. In this paper, we challenge state-of-the-art computer vision algorithms for building a perception system for autonomous driving. An inherent drawback in the computation of visual semantics is the trade-off between accuracy and computational cost. We propose to circumvent this problem by following an offline-online strategy. During the offline stage dense 3D semantic maps are created. In the online stage the current driving area is recognized in the maps via a re-localization process, which allows to retrieve the pre-computed accurate semantics and 3D geometry in real-time. Then, detecting the dynamic obstacles we obtain a rich understanding of the current scene. We evaluate quantitatively our proposal in the KITTI dataset and discuss the related open challenges for the computer vision community.",
            "referenceCount": 35,
            "citationCount": 123,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-01-05",
            "journal": {
                "name": "2015 IEEE Winter Conference on Applications of Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ros2015VisionBasedOP,\n author = {G. Ros and Sebastian Ramos and Manuel Granados and Amir-Hossein Bakhtiary and David V\u00e1zquez and A. Pe\u00f1a},\n booktitle = {2015 IEEE Winter Conference on Applications of Computer Vision},\n journal = {2015 IEEE Winter Conference on Applications of Computer Vision},\n pages = {231-238},\n title = {Vision-Based Offline-Online Perception Paradigm for Autonomous Driving},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80de0bd883a20b42964a01004aad6716206b7fd8",
            "@type": "ScholarlyArticle",
            "paperId": "80de0bd883a20b42964a01004aad6716206b7fd8",
            "corpusId": 173186734,
            "url": "https://www.semanticscholar.org/paper/80de0bd883a20b42964a01004aad6716206b7fd8",
            "title": "Computer Vision",
            "venue": "Photonics West - Lasers and Applications in Science and Engineering",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1987,
            "externalIds": {
                "DOI": "10.1117/12.940005",
                "CorpusId": 173186734
            },
            "abstract": "The paper gives an hyperbrief review of computer vision, concen-trating on representative achievements in early vision. The account attempts to stress the underlying unity of its scientific foundations and intellectual achievements.",
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": "0755"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Poggio1987ComputerV,\n author = {T. Poggio},\n booktitle = {Photonics West - Lasers and Applications in Science and Engineering},\n title = {Computer Vision},\n volume = {0755},\n year = {1987}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28cd0d3fc6d18460c09b5ae7d9a646ceb2c5afdc",
            "@type": "ScholarlyArticle",
            "paperId": "28cd0d3fc6d18460c09b5ae7d9a646ceb2c5afdc",
            "corpusId": 19381082,
            "url": "https://www.semanticscholar.org/paper/28cd0d3fc6d18460c09b5ae7d9a646ceb2c5afdc",
            "title": "Robotics, Vision and Control - Fundamental Algorithms in MATLAB\u00ae",
            "venue": "Springer Tracts in Advanced Robotics",
            "publicationVenue": {
                "id": "urn:research:c19d488b-b6cc-4781-944c-772269efbd26",
                "name": "Springer Tracts in Advanced Robotics",
                "alternate_names": [
                    "Springer Tract Adv Robot"
                ],
                "issn": "1610-7438",
                "url": "https://www.springer.com/series/5208?changeHeader"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "3022621310",
                "DBLP": "series/star/Corke11",
                "DOI": "10.1007/978-3-642-20144-8",
                "CorpusId": 19381082
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 739,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://revistas.uptc.edu.co/revistas/index.php/ingenieria/article/download/4631/3802",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2011-09-05",
            "journal": {
                "name": null,
                "volume": "73"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Corke2011RoboticsVA,\n author = {Peter Corke},\n booktitle = {Springer Tracts in Advanced Robotics},\n pages = {1-495},\n title = {Robotics, Vision and Control - Fundamental Algorithms in MATLAB\u00ae},\n volume = {73},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:71f36c8e17a5c080fab31fce1ffea9551fc49e47",
            "@type": "ScholarlyArticle",
            "paperId": "71f36c8e17a5c080fab31fce1ffea9551fc49e47",
            "corpusId": 14563079,
            "url": "https://www.semanticscholar.org/paper/71f36c8e17a5c080fab31fce1ffea9551fc49e47",
            "title": "Predicting Failures of Vision Systems",
            "venue": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1991671938",
                "DBLP": "conf/cvpr/ZhangWFHP14",
                "DOI": "10.1109/CVPR.2014.456",
                "CorpusId": 14563079
            },
            "abstract": "Computer vision systems today fail frequently. They also fail abruptly without warning or explanation. Alleviating the former has been the primary focus of the community. In this work, we hope to draw the community's attention to the latter, which is arguably equally problematic for real applications. We promote two metrics to evaluate failure prediction. We show that a surprisingly straightforward and general approach, that we call ALERT, can predict the likely accuracy (or failure) of a variety of computer vision systems - semantic segmentation, vanishing point and camera parameter estimation, and image memorability prediction - on individual input images. We also explore attribute prediction, where classifiers are typically meant to generalize to new unseen categories. We show that ALERT can be useful in predicting failures of this transfer. Finally, we leverage ALERT to improve the performance of a downstream application of attribute prediction: zero-shot learning. We show that ALERT can outperform several strong baselines for zero-shot learning on four datasets.",
            "referenceCount": 64,
            "citationCount": 124,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ri.cmu.edu/pub_files/2014/3/predicting_failures_of_vision_systems_CVPR2014.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-01",
            "journal": {
                "name": "2014 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2014PredictingFO,\n author = {Peng Zhang and Jiuling Wang and Ali Farhadi and M. Hebert and Devi Parikh},\n booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {3566-3573},\n title = {Predicting Failures of Vision Systems},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:902bdc9d286759b925412da53b36bc2f810430a8",
            "@type": "ScholarlyArticle",
            "paperId": "902bdc9d286759b925412da53b36bc2f810430a8",
            "corpusId": 106537487,
            "url": "https://www.semanticscholar.org/paper/902bdc9d286759b925412da53b36bc2f810430a8",
            "title": "Real-Time Vision for Human-Computer Interaction",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1509934228",
                "DOI": "10.1007/0-387-27890-7",
                "CorpusId": 106537487
            },
            "abstract": null,
            "referenceCount": 1,
            "citationCount": 60,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2010-10-29",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Kisa\u010danin2010RealTimeVF,\n author = {B. Kisa\u010danin and V. Pavlovic and Thomas S. Huang},\n title = {Real-Time Vision for Human-Computer Interaction},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f7c7e4c8620dff3b763997900f615d84d93cb15",
            "@type": "ScholarlyArticle",
            "paperId": "6f7c7e4c8620dff3b763997900f615d84d93cb15",
            "corpusId": 13903524,
            "url": "https://www.semanticscholar.org/paper/6f7c7e4c8620dff3b763997900f615d84d93cb15",
            "title": "The Legion vision of a worldwide virtual computer",
            "venue": "Communications of the ACM",
            "publicationVenue": {
                "id": "urn:research:4d9ce1c4-dc84-46b9-903e-e3751c00c7dd",
                "name": "Communications of the ACM",
                "alternate_names": [
                    "Commun ACM",
                    "Communications of The ACM"
                ],
                "issn": "0001-0782",
                "url": "http://www.acm.org/pubs/cacm/"
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2076144894",
                "DBLP": "journals/cacm/GrimshawW97",
                "DOI": "10.1145/242857.242867",
                "CorpusId": 13903524
            },
            "abstract": "of a Worldwide Virtual Computer Long a vision of science fiction writers and distributed systems researchers, the notion of a worldwide computer, now taking shape through the Legion project, distributes computation like the World-Wide Web distributes multimedia, creating the illusion for users of a very, very powerful desktop computer. T ODAY\u2019S DRAMATIC INCREASE IN AVAILABLE NETWORK BANDWIDTH WILL qualitatively change how the world computes, communicates, and collaborates. The rapid expansion of the World-Wide Web and the changes it has wrought are just the beginning. As high-bandwidth connections become available, they shrink distances and change our modes of computation, storage, and interaction. Inevitably, users will operate in a wide-area environment transparently consisting of workstations, PCs, graphics-rendering engines, supercomputers, and nontraditional computing devices, such as televisions. The relative physical locations of users and their resources is increasingly irrelevant. Realization of such an environment, sometimes called a \u201cmetasystem,\u201d is not without problems. Today\u2019s experimental high-speed networks, such as the Very",
            "referenceCount": 8,
            "citationCount": 803,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/242857.242867",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-01-01",
            "journal": {
                "name": "Communications of the ACM",
                "volume": "40"
            },
            "citationStyles": {
                "bibtex": "@Article{Grimshaw1997TheLV,\n author = {A. Grimshaw and W. Wulf},\n booktitle = {Communications of the ACM},\n journal = {Communications of the ACM},\n pages = {39 - 45},\n title = {The Legion vision of a worldwide virtual computer},\n volume = {40},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:212dbfd09b4bc50e358be948e4219995c0a5d61e",
            "@type": "ScholarlyArticle",
            "paperId": "212dbfd09b4bc50e358be948e4219995c0a5d61e",
            "corpusId": 14099087,
            "url": "https://www.semanticscholar.org/paper/212dbfd09b4bc50e358be948e4219995c0a5d61e",
            "title": "The Ignorant Led by the Blind: A Hybrid Human\u2013Machine Vision System for Fine-Grained Categorization",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/ijcv/BransonHWPB14",
                "MAG": "1972939083",
                "DOI": "10.1007/s11263-014-0698-4",
                "CorpusId": 14099087
            },
            "abstract": null,
            "referenceCount": 76,
            "citationCount": 82,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-05-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "108"
            },
            "citationStyles": {
                "bibtex": "@Article{Branson2014TheIL,\n author = {Steve Branson and Grant Van Horn and C. Wah and P. Perona and Serge J. Belongie},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {3-29},\n title = {The Ignorant Led by the Blind: A Hybrid Human\u2013Machine Vision System for Fine-Grained Categorization},\n volume = {108},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:075bfb99ce2dbaa2005500dff90f893b7caa68c2",
            "@type": "ScholarlyArticle",
            "paperId": "075bfb99ce2dbaa2005500dff90f893b7caa68c2",
            "corpusId": 16135648,
            "url": "https://www.semanticscholar.org/paper/075bfb99ce2dbaa2005500dff90f893b7caa68c2",
            "title": "On-line Boosting and Vision",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/cvpr/GrabnerB06",
                "MAG": "2170865122",
                "DOI": "10.1109/CVPR.2006.215",
                "CorpusId": 16135648
            },
            "abstract": "Boosting has become very popular in computer vision, showing impressive performance in detection and recognition tasks. Mainly off-line training methods have been used, which implies that all training data has to be a priori given; training and usage of the classifier are separate steps. Training the classifier on-line and incrementally as new data becomes available has several advantages and opens new areas of application for boosting in computer vision. In this paper we propose a novel on-line AdaBoost feature selection method. In conjunction with efficient feature extraction methods the method is real time capable. We demonstrate the multifariousness of the method on such diverse tasks as learning complex background models, visual tracking and object detection. All approaches benefit significantly by the on-line training.",
            "referenceCount": 160,
            "citationCount": 1134,
            "influentialCitationCount": 123,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-17",
            "journal": {
                "name": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Grabner2006OnlineBA,\n author = {H. Grabner and H. Bischof},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\n pages = {260-267},\n title = {On-line Boosting and Vision},\n volume = {1},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:275f8eec5b70522336e77f4baa34e026879dbc7f",
            "@type": "ScholarlyArticle",
            "paperId": "275f8eec5b70522336e77f4baa34e026879dbc7f",
            "corpusId": 33789395,
            "url": "https://www.semanticscholar.org/paper/275f8eec5b70522336e77f4baa34e026879dbc7f",
            "title": "Impact of computer use on children's vision.",
            "venue": "Hippokratia",
            "publicationVenue": {
                "id": "urn:research:272bca73-1ce3-4f18-a83a-fcc9ac1ed026",
                "name": "Hippokratia",
                "alternate_names": null,
                "issn": "1108-4189",
                "url": "http://www.hippokratia.gr/index.php/hippo/index"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "8154336",
                "CorpusId": 33789395,
                "PubMed": "20011087"
            },
            "abstract": "Today, millions of children use computers on a daily basis. Extensive viewing of the computer screen can lead to eye discomfort, fatigue, blurred vision and headaches, dry eyes and other symptoms of eyestrain. These symptoms may be caused by poor lighting, glare, an improper work station set-up, vision problems of which the person was not previously aware, or a combination of these factors. Children can experience many of the same symptoms related to computer use as adults. However, some unique aspects of how children use computers may make them more susceptible than adults to the development of these problems. In this study, the most common eye symptoms related to computer use in childhood, the possible causes and ways to avoid them are reviewed.",
            "referenceCount": 3,
            "citationCount": 79,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-10-01",
            "journal": {
                "name": "Hippokratia",
                "volume": "13 4"
            },
            "citationStyles": {
                "bibtex": "@Article{Kozeis2009ImpactOC,\n author = {N. Kozeis},\n booktitle = {Hippokratia},\n journal = {Hippokratia},\n pages = {\n          230-1\n        },\n title = {Impact of computer use on children's vision.},\n volume = {13 4},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:177e957f5cd93229c9794ea652c646d2557b4a69",
            "@type": "ScholarlyArticle",
            "paperId": "177e957f5cd93229c9794ea652c646d2557b4a69",
            "corpusId": 245837420,
            "url": "https://www.semanticscholar.org/paper/177e957f5cd93229c9794ea652c646d2557b4a69",
            "title": "A ConvNet for the 2020s",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2201.03545",
                "DBLP": "journals/corr/abs-2201-03545",
                "DOI": "10.1109/CVPR52688.2022.01167",
                "CorpusId": 245837420
            },
            "abstract": "The \u201cRoaring 20s\u201d of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \u201cmodernize\u201d a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.",
            "referenceCount": 92,
            "citationCount": 2034,
            "influentialCitationCount": 463,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-01-10",
            "journal": {
                "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2022ACF,\n author = {Zhuang Liu and Hanzi Mao and Chaozheng Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11966-11976},\n title = {A ConvNet for the 2020s},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "@type": "ScholarlyArticle",
            "paperId": "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "corpusId": 231591445,
            "url": "https://www.semanticscholar.org/paper/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4",
            "title": "Learning Transferable Visual Models From Natural Language Supervision",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2103.00020",
                "DBLP": "conf/icml/RadfordKHRGASAM21",
                "CorpusId": 231591445
            },
            "abstract": "State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.",
            "referenceCount": 220,
            "citationCount": 9630,
            "influentialCitationCount": 3258,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-02-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Radford2021LearningTV,\n author = {Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {8748-8763},\n title = {Learning Transferable Visual Models From Natural Language Supervision},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:55fd2d967945624a5457f204b9cecbbc1ed54ad2",
            "@type": "ScholarlyArticle",
            "paperId": "55fd2d967945624a5457f204b9cecbbc1ed54ad2",
            "corpusId": 8574608,
            "url": "https://www.semanticscholar.org/paper/55fd2d967945624a5457f204b9cecbbc1ed54ad2",
            "title": "Combining brain computer interfaces with vision for object categorization",
            "venue": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/cvpr/KapoorST08",
                "MAG": "2114538779",
                "DOI": "10.1109/CVPR.2008.4587618",
                "CorpusId": 8574608
            },
            "abstract": "Human-aided computing proposes using information measured directly from the human brain in order to perform useful tasks. In this paper, we extend this idea by fusing computer vision-based processing and processing done by the human brain in order to build more effective object categorization systems. Specifically, we use an electroencephalograph (EEG) device to measure the subconscious cognitive processing that occurs in the brain as users see images, even when they are not trying to explicitly classify them. We present a novel framework that combines a discriminative visual category recognition system based on the pyramid match kernel (PMK) with information derived from EEG measurements as users view images. We propose a fast convex kernel alignment algorithm to effectively combine the two sources of information. Our approach is validated with experiments using real-world data, where we show significant gains in classification accuracy. We analyze the properties of this information fusion method by examining the relative contributions of the two modalities, the errors arising from each source, and the stability of the combination in repeated experiments.",
            "referenceCount": 40,
            "citationCount": 73,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-06-01",
            "journal": {
                "name": "2008 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kapoor2008CombiningBC,\n author = {Ashish Kapoor and P. Shenoy and Desney S. Tan},\n booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Combining brain computer interfaces with vision for object categorization},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dde2cfb524372ea709cc1dd645a339c32da6600b",
            "@type": "ScholarlyArticle",
            "paperId": "dde2cfb524372ea709cc1dd645a339c32da6600b",
            "corpusId": 35558834,
            "url": "https://www.semanticscholar.org/paper/dde2cfb524372ea709cc1dd645a339c32da6600b",
            "title": "Object Categorization: Computer and Human Vision Perspectives",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "CorpusId": 35558834
            },
            "abstract": null,
            "referenceCount": 88,
            "citationCount": 159,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Schiele2008ObjectCC,\n author = {B. Schiele and Michael Tarr},\n title = {Object Categorization: Computer and Human Vision Perspectives},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3aed4648f7857c1d5e9b1da4c3afaf97463138c3",
            "@type": "ScholarlyArticle",
            "paperId": "3aed4648f7857c1d5e9b1da4c3afaf97463138c3",
            "corpusId": 250311206,
            "url": "https://www.semanticscholar.org/paper/3aed4648f7857c1d5e9b1da4c3afaf97463138c3",
            "title": "YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2207.02696",
                "DBLP": "conf/cvpr/WangBL23",
                "DOI": "10.1109/CVPR52729.2023.00721",
                "CorpusId": 250311206
            },
            "abstract": "Real-time object detection is one of the most important research topics in computer vision. As new approaches regarding architecture optimization and training optimization are continually being developed, we have found two research topics that have spawned when dealing with these latest state-of-the-art methods. To address the topics, we propose a trainable bag-of-freebies oriented solution. We combine the flexible and efficient training tools with the proposed architecture and the compound scaling method. YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 120 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. Source code is released in https://github.com/WongKinYiu/yolov7.",
            "referenceCount": 99,
            "citationCount": 1682,
            "influentialCitationCount": 253,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2207.02696",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-07-06",
            "journal": {
                "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2022YOLOv7TB,\n author = {Chien-Yao Wang and Alexey Bochkovskiy and H. Liao},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {7464-7475},\n title = {YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed3b4b1cc2f5d10363e358f012628638f0f10a45",
            "@type": "ScholarlyArticle",
            "paperId": "ed3b4b1cc2f5d10363e358f012628638f0f10a45",
            "corpusId": 12951864,
            "url": "https://www.semanticscholar.org/paper/ed3b4b1cc2f5d10363e358f012628638f0f10a45",
            "title": "Hands-free vision-based interface for computer accessibility",
            "venue": "Journal of Network and Computer Applications",
            "publicationVenue": {
                "id": "urn:research:bc946b89-5188-4a8f-b9b6-61b71dd9fccc",
                "name": "Journal of Network and Computer Applications",
                "alternate_names": [
                    "J Netw Comput Appl",
                    "Journal of Network Computing and Applications"
                ],
                "issn": "1084-8045",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622893/description#description"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2125499583",
                "DBLP": "journals/jnca/VaronaML08",
                "DOI": "10.1016/j.jnca.2008.03.003",
                "CorpusId": 12951864
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 95,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-11-01",
            "journal": {
                "name": "J. Netw. Comput. Appl.",
                "volume": "31"
            },
            "citationStyles": {
                "bibtex": "@Article{Varona2008HandsfreeVI,\n author = {J. Varona and C. Manresa-Yee and Francisco J. Perales L\u00f3pez},\n booktitle = {Journal of Network and Computer Applications},\n journal = {J. Netw. Comput. Appl.},\n pages = {357-374},\n title = {Hands-free vision-based interface for computer accessibility},\n volume = {31},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:123f2eb08ee91217e691dbb63eeee7e546d41e22",
            "@type": "ScholarlyArticle",
            "paperId": "123f2eb08ee91217e691dbb63eeee7e546d41e22",
            "corpusId": 16929534,
            "url": "https://www.semanticscholar.org/paper/123f2eb08ee91217e691dbb63eeee7e546d41e22",
            "title": "\\the Protein Data Bank: a Computer-based Archival Le for Macromolecular Structures,\"",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": null,
            "externalIds": {
                "CorpusId": 16929534
            },
            "abstract": "do not seem to be sucient. Thus, multiple protein structure alignment should be studied further. In this paper, each protein structure is treated as a rigid body. That is, alignments are computed considering global positions only. Although such a treatment is adequate for comparing structures with strong similarities, it seems to be inadequate for comparing structures with weak similarities. Especially, in the case of classication of protein structures (or folding patterns) into the small number of families, more exible pattern matching methods should be employed. Holm et al. combined several algorithms for that purpose [6]. We also proposed another method [3]. But, these methods do not seem to be sucient. Thus, more exible pattern matching methods should be developed. [2] T. Akutsu, \\Ecient and robust three-dimensional pattern matching algorithms using hashing and dynamic programming techniques,\" Proc. \\Ecient detection of three-dimensional structural motifs in biological macromolecules by computer vision techniques,\" Proc.",
            "referenceCount": 8,
            "citationCount": 2380,
            "influentialCitationCount": 326,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Misc{None,\n author = {F. C. Bernstein and T. Koetzle and G. J. Williams and E. Meyer and M. Brice and J. Rodgers and T. P. Singh and T. Shimanouchi and M. Tasumi},\n title = {\\the Protein Data Bank: a Computer-based Archival Le for Macromolecular Structures,\"}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:157282748f001bc9876f9eaf1d53b98a6e579f19",
            "@type": "ScholarlyArticle",
            "paperId": "157282748f001bc9876f9eaf1d53b98a6e579f19",
            "corpusId": 35266275,
            "url": "https://www.semanticscholar.org/paper/157282748f001bc9876f9eaf1d53b98a6e579f19",
            "title": "Modeling Light Reflection for Computer Color Vision",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1990,
            "externalIds": {
                "MAG": "2176765001",
                "DBLP": "journals/pami/LeeBS90",
                "DOI": "10.1109/34.50626",
                "CorpusId": 35266275
            },
            "abstract": "In computer vision applications, analysis of shading information requires a proper model of light reflection from object surfaces. To overcome the shortcoming of the most often used model and to extend the reflection model for computer color vision, an examination is made of the light reflection problem using the bidirectional spectral-reflectance distribution function (BSRDF) to specify both incident- and reflected-beam geometries. It is shown that the product form can still be retained for a polychromatic light source under two lighting conditions: the light source is collimated; or the spectral factor and the geometric factor can be separated for both the light source and the BSRDF of the surface. The reflection model is then applied to the formulation of a neutral-interface-reflection model, which is tested experimentally. The results show the adequacy of this type of model for surfaces of some material compositions, e.g. plastics, plant leaves, painted surfaces, orange peels, and some glossy cloth, but not for others, e.g. colored paper and some ceramics. >",
            "referenceCount": 48,
            "citationCount": 201,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1990-04-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Lee1990ModelingLR,\n author = {Hsien-Che Lee and E. J. Breneman and C. P. Schulte},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {402-409},\n title = {Modeling Light Reflection for Computer Color Vision},\n volume = {12},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "@type": "ScholarlyArticle",
            "paperId": "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "corpusId": 225039882,
            "url": "https://www.semanticscholar.org/paper/268d347e8a55b5eb82fb5e7d2f800e33c75ab18a",
            "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3094502228",
                "DBLP": "journals/corr/abs-2010-11929",
                "ArXiv": "2010.11929",
                "CorpusId": 225039882
            },
            "abstract": "While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",
            "referenceCount": 0,
            "citationCount": 673,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.11929"
            },
            "citationStyles": {
                "bibtex": "@Article{None,\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},\n volume = {abs/2010.11929},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8388f1be26329fa45e5807e968a641ce170ea078",
            "@type": "ScholarlyArticle",
            "paperId": "8388f1be26329fa45e5807e968a641ce170ea078",
            "corpusId": 11758569,
            "url": "https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078",
            "title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2949811265",
                "ArXiv": "1511.06434",
                "DBLP": "journals/corr/RadfordMC15",
                "CorpusId": 11758569
            },
            "abstract": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
            "referenceCount": 46,
            "citationCount": 12395,
            "influentialCitationCount": 1830,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06434"
            },
            "citationStyles": {
                "bibtex": "@Article{Radford2015UnsupervisedRL,\n author = {Alec Radford and Luke Metz and Soumith Chintala},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},\n volume = {abs/1511.06434},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:27b35b8238270171147e099f862673821651a5c1",
            "@type": "ScholarlyArticle",
            "paperId": "27b35b8238270171147e099f862673821651a5c1",
            "corpusId": 8100642,
            "url": "https://www.semanticscholar.org/paper/27b35b8238270171147e099f862673821651a5c1",
            "title": "Real-time stereo vision on the PARTS reconfigurable computer",
            "venue": "Proceedings. The 5th Annual IEEE Symposium on Field-Programmable Custom Computing Machines Cat. No.97TB100186)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "DBLP": "conf/fccm/WoodfillH97",
                "MAG": "2158080433",
                "DOI": "10.1109/FPGA.1997.624620",
                "CorpusId": 8100642
            },
            "abstract": "The paper describes a powerful, scalable, reconfigurable computer called the PARTS engine. The PARTS engine consists of 16 Xilinx 4025 FPGAs, and 16 one-megabyte SRAMs. The FPGAs are connected in a partial torus-each associated with two adjacent SRAMs. The SRAMs are tightly coupled to the FPGAs so that all the SRAMs can be accessed concurrently. The PARTS engine fits on a standard PCI card in a personal computer or workstation. The first application implemented on the PARTS engine is a depth from stereo vision algorithm that computes 24 stereo disparities on 320 by 240 pixel images at 42 frames per second. Running at this speed, the engine is performing approximately 2.3 billion RISC-equivalent operations per second, accessing memory at a rate of 500 million bytes per second and attaining throughput of over 70 million point/spl times/disparity measurements per second.",
            "referenceCount": 12,
            "citationCount": 227,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1997-04-16",
            "journal": {
                "name": "Proceedings. The 5th Annual IEEE Symposium on Field-Programmable Custom Computing Machines Cat. No.97TB100186)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Woodfill1997RealtimeSV,\n author = {J. Woodfill and B. V. Herzen},\n booktitle = {Proceedings. The 5th Annual IEEE Symposium on Field-Programmable Custom Computing Machines Cat. No.97TB100186)},\n journal = {Proceedings. The 5th Annual IEEE Symposium on Field-Programmable Custom Computing Machines Cat. No.97TB100186)},\n pages = {201-210},\n title = {Real-time stereo vision on the PARTS reconfigurable computer},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:db4e821c2b09ff8774ee6f616e4dec202c9a419e",
            "@type": "ScholarlyArticle",
            "paperId": "db4e821c2b09ff8774ee6f616e4dec202c9a419e",
            "corpusId": 16357559,
            "url": "https://www.semanticscholar.org/paper/db4e821c2b09ff8774ee6f616e4dec202c9a419e",
            "title": "DigitEyes: vision-based hand tracking for human-computer interaction",
            "venue": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2144126588",
                "DOI": "10.1109/MNRAO.1994.346260",
                "CorpusId": 16357559
            },
            "abstract": "Computer sensing of hand and limb motion is an important problem for applications in human-computer interaction (HCI), virtual reality, and athletic performance measurement. Commercially available sensors are invasive, and require the user to wear gloves or targets. We have developed a noninvasive vision-based hand tracking system, called DigitEyes. Employing a kinematic hand model, the DigitEyes system has demonstrated tracking performance at speeds of up to 10 Hz, using line and point features extracted from gray scale images of unadorned, unmarked hands. We describe an application of our sensor to a 3D mouse user-interface problem.<<ETX>>",
            "referenceCount": 13,
            "citationCount": 227,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1994-11-11",
            "journal": {
                "name": "Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rehg1994DigitEyesVH,\n author = {James M. Rehg and T. Kanade},\n booktitle = {Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects},\n journal = {Proceedings of 1994 IEEE Workshop on Motion of Non-rigid and Articulated Objects},\n pages = {16-22},\n title = {DigitEyes: vision-based hand tracking for human-computer interaction},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c8a51d04522496c43db68f2582efd45eaf59fea",
            "@type": "ScholarlyArticle",
            "paperId": "7c8a51d04522496c43db68f2582efd45eaf59fea",
            "corpusId": 206592833,
            "url": "https://www.semanticscholar.org/paper/7c8a51d04522496c43db68f2582efd45eaf59fea",
            "title": "3D ShapeNets: A deep representation for volumetric shapes",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2951755740",
                "DBLP": "conf/cvpr/WuSKYZTX15",
                "DOI": "10.1109/CVPR.2015.7298801",
                "CorpusId": 206592833
            },
            "abstract": "3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from view-based 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representation automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet - a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.",
            "referenceCount": 43,
            "citationCount": 4412,
            "influentialCitationCount": 1162,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1406.5670",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-22",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu20143DSA,\n author = {Zhirong Wu and Shuran Song and A. Khosla and F. Yu and Linguang Zhang and Xiaoou Tang and Jianxiong Xiao},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {1912-1920},\n title = {3D ShapeNets: A deep representation for volumetric shapes},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "@type": "ScholarlyArticle",
            "paperId": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "corpusId": 2930547,
            "url": "https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "title": "ImageNet Large Scale Visual Recognition Challenge",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2014,
            "externalIds": {
                "ArXiv": "1409.0575",
                "DBLP": "journals/corr/RussakovskyDSKSMHKKBBF14",
                "MAG": "2546241758",
                "DOI": "10.1007/s11263-015-0816-y",
                "CorpusId": 2930547
            },
            "abstract": null,
            "referenceCount": 124,
            "citationCount": 33362,
            "influentialCitationCount": 5024,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/104944/1/11263_2015_Article_816.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-09-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "115"
            },
            "citationStyles": {
                "bibtex": "@Article{Russakovsky2014ImageNetLS,\n author = {Olga Russakovsky and Jia Deng and Hao Su and J. Krause and S. Satheesh and Sean Ma and Zhiheng Huang and A. Karpathy and A. Khosla and Michael S. Bernstein and A. Berg and Li Fei-Fei},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {211 - 252},\n title = {ImageNet Large Scale Visual Recognition Challenge},\n volume = {115},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb42cf88027de515750f230b23b1a057dc782108",
            "@type": "ScholarlyArticle",
            "paperId": "eb42cf88027de515750f230b23b1a057dc782108",
            "corpusId": 14124313,
            "url": "https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108",
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2949429431",
                "ArXiv": "1409.1556",
                "DBLP": "journals/corr/SimonyanZ14a",
                "CorpusId": 14124313
            },
            "abstract": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
            "referenceCount": 43,
            "citationCount": 84264,
            "influentialCitationCount": 16131,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-09-04",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1409.1556"
            },
            "citationStyles": {
                "bibtex": "@Article{Simonyan2014VeryDC,\n author = {K. Simonyan and Andrew Zisserman},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},\n volume = {abs/1409.1556},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eb926c671216af30e8e7ad2db8b60ffed05af30a",
            "@type": "ScholarlyArticle",
            "paperId": "eb926c671216af30e8e7ad2db8b60ffed05af30a",
            "corpusId": 59649283,
            "url": "https://www.semanticscholar.org/paper/eb926c671216af30e8e7ad2db8b60ffed05af30a",
            "title": "Computer and Robot Vision (Volume II)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "124706970",
                "CorpusId": 59649283
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 322,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2002-06-06",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Haralick2002ComputerAR,\n author = {R. Haralick and L. Shapiro},\n title = {Computer and Robot Vision (Volume II)},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "@type": "ScholarlyArticle",
            "paperId": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "corpusId": 5707386,
            "url": "https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2271840356",
                "ArXiv": "1603.04467",
                "DBLP": "journals/corr/AbadiABBCCCDDDG16",
                "CorpusId": 5707386
            },
            "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",
            "referenceCount": 60,
            "citationCount": 10166,
            "influentialCitationCount": 1042,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-03-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1603.04467"
            },
            "citationStyles": {
                "bibtex": "@Article{Abadi2016TensorFlowLM,\n author = {Mart\u00edn Abadi and Ashish Agarwal and P. Barham and E. Brevdo and Z. Chen and C. Citro and G. Corrado and Andy Davis and J. Dean and Matthieu Devin and S. Ghemawat and I. Goodfellow and A. Harp and G. Irving and M. Isard and Yangqing Jia and R. J\u00f3zefowicz and Lukasz Kaiser and M. Kudlur and J. Levenberg and Dandelion Man\u00e9 and R. Monga and Sherry Moore and D. Murray and C. Olah and M. Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and P. Tucker and Vincent Vanhoucke and Vijay Vasudevan and F. Vi\u00e9gas and Oriol Vinyals and P. Warden and M. Wattenberg and M. Wicke and Yuan Yu and Xiaoqiang Zheng},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems},\n volume = {abs/1603.04467},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "@type": "ScholarlyArticle",
            "paperId": "4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "corpusId": 1929844,
            "url": "https://www.semanticscholar.org/paper/4a6e74d4bf4fd0106891e5518692a77c7aa8811d",
            "title": "Outlier Detection in High Dimensional Data",
            "venue": "Regular Issue",
            "publicationVenue": {
                "id": "urn:research:b3c3187d-18c6-4d52-8596-9c4f882f647a",
                "name": "Regular Issue",
                "alternate_names": [
                    "Regular issue",
                    "Regul Issue",
                    "Regul issue"
                ],
                "issn": "2319-6378",
                "url": "https://www.ijese.org/"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3176919784",
                "DOI": "10.35940/ijeat.e2675.0610521",
                "CorpusId": 1929844
            },
            "abstract": "Artificial intelligence (AI) is the science that allows\ncomputers to replicate human intelligence in areas such as\ndecision-making, text processing, visual perception. Artificial\nIntelligence is the broader field that contains several subfields\nsuch as machine learning, robotics, and computer vision.\nMachine Learning is a branch of Artificial Intelligence that\nallows a machine to learn and improve at a task over time. Deep\nLearning is a subset of machine learning that makes use of deep\nartificial neural networks for training. The paper proposed on\noutlier detection for multivariate high dimensional data for\nAutoencoder unsupervised model.",
            "referenceCount": 6,
            "citationCount": 1057,
            "influentialCitationCount": 64,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2021-06-30",
            "journal": {
                "name": "Regular issue",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Aggarwal2021OutlierDI,\n author = {C. Aggarwal and Philip S. Yu},\n booktitle = {Regular Issue},\n journal = {Regular issue},\n title = {Outlier Detection in High Dimensional Data},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
            "@type": "ScholarlyArticle",
            "paperId": "16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
            "corpusId": 231718848,
            "url": "https://www.semanticscholar.org/paper/16f2d2f2b8103ed0c4a4e6f339a21247e58c5e78",
            "title": "Bottleneck Transformers for Visual Recognition",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2101-11605",
                "ArXiv": "2101.11605",
                "DOI": "10.1109/CVPR46437.2021.01625",
                "CorpusId": 231718848
            },
            "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt [67] evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in \"compute\"1 time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision.2",
            "referenceCount": 80,
            "citationCount": 657,
            "influentialCitationCount": 83,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2101.11605",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-01-27",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Srinivas2021BottleneckTF,\n author = {A. Srinivas and Tsung-Yi Lin and Niki Parmar and Jonathon Shlens and P. Abbeel and Ashish Vaswani},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {16514-16524},\n title = {Bottleneck Transformers for Visual Recognition},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8899094797e82c5c185a0893896320ef77f60e64",
            "@type": "ScholarlyArticle",
            "paperId": "8899094797e82c5c185a0893896320ef77f60e64",
            "corpusId": 4852647,
            "url": "https://www.semanticscholar.org/paper/8899094797e82c5c185a0893896320ef77f60e64",
            "title": "Non-local Neural Networks",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/cvpr/0004GGH18",
                "MAG": "2963091558",
                "ArXiv": "1711.07971",
                "DOI": "10.1109/CVPR.2018.00813",
                "CorpusId": 4852647
            },
            "abstract": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.",
            "referenceCount": 58,
            "citationCount": 7012,
            "influentialCitationCount": 868,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1711.07971",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-21",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2017NonlocalNN,\n author = {X. Wang and Ross B. Girshick and A. Gupta and Kaiming He},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {7794-7803},\n title = {Non-local Neural Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b3c785b99ec147049caa47f707f337b717705970",
            "@type": "ScholarlyArticle",
            "paperId": "b3c785b99ec147049caa47f707f337b717705970",
            "corpusId": 1806278,
            "url": "https://www.semanticscholar.org/paper/b3c785b99ec147049caa47f707f337b717705970",
            "title": "SLIC Superpixels Compared to State-of-the-Art Superpixel Methods",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2118246710",
                "DBLP": "journals/pami/AchantaSSLFS12",
                "DOI": "10.1109/TPAMI.2012.120",
                "CorpusId": 1806278,
                "PubMed": "22641706"
            },
            "abstract": "Computer vision applications have come to rely increasingly on superpixels in recent years, but it is not always clear what constitutes a good superpixel algorithm. In an effort to understand the benefits and drawbacks of existing methods, we empirically compare five state-of-the-art superpixel algorithms for their ability to adhere to image boundaries, speed, memory efficiency, and their impact on segmentation performance. We then introduce a new superpixel algorithm, simple linear iterative clustering (SLIC), which adapts a k-means clustering approach to efficiently generate superpixels. Despite its simplicity, SLIC adheres to boundaries as well as or better than previous methods. At the same time, it is faster and more memory efficient, improves segmentation performance, and is straightforward to extend to supervoxel generation.",
            "referenceCount": 32,
            "citationCount": 7696,
            "influentialCitationCount": 1220,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2012-11-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Achanta2012SLICSC,\n author = {R. Achanta and Appu Shaji and Kevin Smith and Aur\u00e9lien Lucchi and P. Fua and S. S\u00fcsstrunk},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2274-2282},\n title = {SLIC Superpixels Compared to State-of-the-Art Superpixel Methods},\n volume = {34},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "@type": "ScholarlyArticle",
            "paperId": "50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "corpusId": 206429151,
            "url": "https://www.semanticscholar.org/paper/50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "title": "V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation",
            "venue": "International Conference on 3D Vision",
            "publicationVenue": {
                "id": "urn:research:4b02e809-1c26-4203-b9ba-311a418f664b",
                "name": "International Conference on 3D Vision",
                "alternate_names": [
                    "Int Conf 3D Vis",
                    "3DV"
                ],
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950607311",
                "DBLP": "conf/3dim/MilletariNA16",
                "ArXiv": "1606.04797",
                "DOI": "10.1109/3DV.2016.79",
                "CorpusId": 206429151
            },
            "abstract": "Convolutional Neural Networks (CNNs) have been recently employed to solve problems from both the computer vision and medical image analysis fields. Despite their popularity, most approaches are only able to process 2D images while most medical data used in clinical practice consists of 3D volumes. In this work we propose an approach to 3D image segmentation based on a volumetric, fully convolutional, neural network. Our CNN is trained end-to-end on MRI volumes depicting prostate, and learns to predict segmentation for the whole volume at once. We introduce a novel objective function, that we optimise during training, based on Dice coefficient. In this way we can deal with situations where there is a strong imbalance between the number of foreground and background voxels. To cope with the limited number of annotated volumes available for training, we augment the data applying random non-linear transformations and histogram matching. We show in our experimental evaluation that our approach achieves good performances on challenging test data while requiring only a fraction of the processing time needed by other previous methods.",
            "referenceCount": 24,
            "citationCount": 6388,
            "influentialCitationCount": 702,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1606.04797",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-15",
            "journal": {
                "name": "2016 Fourth International Conference on 3D Vision (3DV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Milletar\u00ec2016VNetFC,\n author = {F. Milletar\u00ec and N. Navab and Seyed-Ahmad Ahmadi},\n booktitle = {International Conference on 3D Vision},\n journal = {2016 Fourth International Conference on 3D Vision (3DV)},\n pages = {565-571},\n title = {V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:95204425ae512cc77d46865c6b96f4fda656425a",
            "@type": "ScholarlyArticle",
            "paperId": "95204425ae512cc77d46865c6b96f4fda656425a",
            "corpusId": 670357,
            "url": "https://www.semanticscholar.org/paper/95204425ae512cc77d46865c6b96f4fda656425a",
            "title": "Color-defective vision and computer graphics displays",
            "venue": "IEEE Computer Graphics and Applications",
            "publicationVenue": {
                "id": "urn:research:0bf2c493-a93b-464a-b3d9-e18fbb4a7965",
                "name": "IEEE Computer Graphics and Applications",
                "alternate_names": [
                    "IEEE Comput Graph Appl"
                ],
                "issn": "0272-1716",
                "url": "http://www.computer.org/cga/"
            },
            "year": 1988,
            "externalIds": {
                "MAG": "2088437294",
                "DBLP": "journals/cga/MeyerG88",
                "DOI": "10.1109/38.7759",
                "CorpusId": 670357
            },
            "abstract": "A color space defined by the fundamental spectral sensitivity functions of the human visual system is used to assist in the design of computer graphics displays for color-deficient users. The functions are derived in terms of the CIE standard observer color-matching functions. The Farnsworth-Munsell 100-hue test, a widely used color vision test administered using physical color samples, is then implemented on a digitally controlled color television monitor. The flexibility of this computer graphics medium is then used to extend the Farnsworth-Munsell test in a way that improves the specificity of the diagnoses rendered by the test. The issue of how the world appears to color-deficient observers is addressed, and a full-color image is modified to represent a color-defective view of the scene. Specific guidelines are offered for the design of computer graphics displays that will accommodate almost all color-deficient users.<<ETX>>",
            "referenceCount": 40,
            "citationCount": 122,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1988-09-01",
            "journal": {
                "name": "IEEE Computer Graphics and Applications",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Meyer1988ColordefectiveVA,\n author = {G. Meyer and D. Greenberg},\n booktitle = {IEEE Computer Graphics and Applications},\n journal = {IEEE Computer Graphics and Applications},\n pages = {28-40},\n title = {Color-defective vision and computer graphics displays},\n volume = {8},\n year = {1988}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "@type": "ScholarlyArticle",
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "corpusId": 1169492,
            "url": "https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/VinyalsTBE14",
                "MAG": "2951912364",
                "ArXiv": "1411.4555",
                "DOI": "10.1109/CVPR.2015.7298935",
                "CorpusId": 1169492
            },
            "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
            "referenceCount": 35,
            "citationCount": 5393,
            "influentialCitationCount": 662,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.4555",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-11-17",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2014ShowAT,\n author = {Oriol Vinyals and Alexander Toshev and Samy Bengio and D. Erhan},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3156-3164},\n title = {Show and tell: A neural image caption generator},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8c278991dd8072be67b2bc284127d8a87170cabd",
            "@type": "ScholarlyArticle",
            "paperId": "8c278991dd8072be67b2bc284127d8a87170cabd",
            "corpusId": 57237750,
            "url": "https://www.semanticscholar.org/paper/8c278991dd8072be67b2bc284127d8a87170cabd",
            "title": "Understanding Computer Programming as a Literacy",
            "venue": "Logic in Computer Science",
            "publicationVenue": {
                "id": "urn:research:3673dec6-881d-437a-8c5d-710b0e94690c",
                "name": "Logic in Computer Science",
                "alternate_names": [
                    "LICS",
                    "Log Comput Sci"
                ],
                "issn": "1043-6871",
                "url": "http://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000420"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1482498656",
                "DOI": "10.21623/1.1.2.4",
                "CorpusId": 57237750
            },
            "abstract": "Since the 1960s, computer scientists and enthusiasts have paralleled computer programming to literacy, arguing it is a generalizable skill that should be more widely taught and held. Launching from that premise, this article leverages historical and social findings from literacy studies to frame computer programming as \u201ccomputational literacy.\u201d I argue that programming and writing have followed similar historical trajectories as material technologies and explain how they are intertwined in contemporary composition environments. A concept of \u201ccomputational literacy\u201d helps us to better understand the social, technical and cultural dynamics of programming, but it also enriches our vision of twenty-first century composition.",
            "referenceCount": 57,
            "citationCount": 115,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://licsjournal.org/index.php/LiCS/article/download/794/608",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2013-10-31",
            "journal": {
                "name": "",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Vee2013UnderstandingCP,\n author = {Annette Vee},\n booktitle = {Logic in Computer Science},\n pages = {42-64},\n title = {Understanding Computer Programming as a Literacy},\n volume = {1},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:25badc676197a70aaf9911865eb03469e402ba57",
            "@type": "ScholarlyArticle",
            "paperId": "25badc676197a70aaf9911865eb03469e402ba57",
            "corpusId": 17793133,
            "url": "https://www.semanticscholar.org/paper/25badc676197a70aaf9911865eb03469e402ba57",
            "title": "Machine learning - a probabilistic perspective",
            "venue": "Adaptive computation and machine learning series",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "books/lib/Murphy12",
                "MAG": "1503398984",
                "CorpusId": 17793133
            },
            "abstract": "Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.",
            "referenceCount": 1022,
            "citationCount": 8564,
            "influentialCitationCount": 1066,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-08-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Murphy2012MachineL,\n author = {Kevin P. Murphy},\n booktitle = {Adaptive computation and machine learning series},\n pages = {I-XXIX, 1-1067},\n title = {Machine learning - a probabilistic perspective},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "@type": "ScholarlyArticle",
            "paperId": "3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "corpusId": 195811894,
            "url": "https://www.semanticscholar.org/paper/3813b88a4ec3c63919df47e9694b577f4691f7e5",
            "title": "A survey on Image Data Augmentation for Deep Learning",
            "venue": "Journal of Big Data",
            "publicationVenue": {
                "id": "urn:research:d60da343-ab92-4310-b3d7-2c0860287a9d",
                "name": "Journal of Big Data",
                "alternate_names": [
                    "J Big Data",
                    "Journal on Big Data"
                ],
                "issn": "2196-1115",
                "url": "http://www.journalofbigdata.com/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jbd/ShortenK19",
                "MAG": "2954996726",
                "DOI": "10.1186/s40537-019-0197-0",
                "CorpusId": 195811894
            },
            "abstract": null,
            "referenceCount": 142,
            "citationCount": 5801,
            "influentialCitationCount": 160,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-06",
            "journal": {
                "name": "Journal of Big Data",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Shorten2019ASO,\n author = {Connor Shorten and T. Khoshgoftaar},\n booktitle = {Journal of Big Data},\n journal = {Journal of Big Data},\n pages = {1-48},\n title = {A survey on Image Data Augmentation for Deep Learning},\n volume = {6},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7aa07fa42e67d43aa58dd107da856b98235863c",
            "@type": "ScholarlyArticle",
            "paperId": "f7aa07fa42e67d43aa58dd107da856b98235863c",
            "corpusId": 10762887,
            "url": "https://www.semanticscholar.org/paper/f7aa07fa42e67d43aa58dd107da856b98235863c",
            "title": "A computer implementation of a theory of human stereo vision.",
            "venue": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences",
            "publicationVenue": {
                "id": "urn:research:6de51a3e-86d6-4b08-92f5-a2a4a3a1c47b",
                "name": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences",
                "alternate_names": [
                    "Philos trans R Soc Lond Ser B Biological sci"
                ],
                "issn": "0080-4622",
                "url": "http://www.jstor.org/action/showPublication?journalCode=philtranroyasoc2"
            },
            "year": 1981,
            "externalIds": {
                "MAG": "2167558105",
                "DOI": "10.1098/RSTB.1981.0031",
                "CorpusId": 10762887,
                "PubMed": "6115409"
            },
            "abstract": "Recently, Marr & Poggio (1979) presented a theory of human stereo vision. An implementation of that theory is presented, and consists of five steps. (i) The left and right images are each filtered with masks of four sizes that increase with eccentricity; the shape of these masks is given by delta 2G, the Laplacian of a Gaussian function. (ii) Zero crossings in the filtered images are found along horizontal scan lines. (iii) For each mask size, matching takes place between zero crossings of the same sign and roughly the same orientation in the two images, for a range of disparities up to about the width of the mask's central region. Within this disparity range, it can be shown that false targets pose only a simple problem. (iv) The output of the wide masks can control vergence movements, thus causing small masks to come into correspondence. In this way, the matching process gradually moves from dealing with large disparities at a low resolution to dealing with small disparities at a high resolution. (v) When a correspondence is achieved, it is stored in a dynamic buffer, called the 2 1/2-dimensional sketch. To support the adequacy of the Marr-Poggio model of human stereo vision, the implementation was tested on a wide range of stereograms from the human stereopsis literature. The performance of the implementation is illustrated and compared with human perception. Also statistical assumptions made by Marr & Poggio are supported by comparison with statistics found in practice. Finally, the process of implementing the theory has led to the clarification and refinement of a number of details within the theory; these are discussed in detail.",
            "referenceCount": 25,
            "citationCount": 310,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/5716/2/AIM-565.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1981-05-12",
            "journal": {
                "name": "Philosophical transactions of the Royal Society of London. Series B, Biological sciences",
                "volume": "292 1058"
            },
            "citationStyles": {
                "bibtex": "@Article{Grimson1981ACI,\n author = {W. Grimson},\n booktitle = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},\n journal = {Philosophical transactions of the Royal Society of London. Series B, Biological sciences},\n pages = {\n          217-53\n        },\n title = {A computer implementation of a theory of human stereo vision.},\n volume = {292 1058},\n year = {1981}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb7621c9bf7acdca786ea4619dd0f296994196c9",
            "@type": "ScholarlyArticle",
            "paperId": "fb7621c9bf7acdca786ea4619dd0f296994196c9",
            "corpusId": 49333383,
            "url": "https://www.semanticscholar.org/paper/fb7621c9bf7acdca786ea4619dd0f296994196c9",
            "title": "Single Image Haze Removal Using Dark Channel Prior",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2341055911",
                "DOI": "10.23977/csic.2018.0921",
                "CorpusId": 49333383
            },
            "abstract": "Haze brings troubles to many computer vision/graphics applications. It reduces the visibility of the scenes and lowers the reliability of outdoor surveillance systems; it reduces the clarity of the satellite images; it also changes the colors and decreases the contrast of daily photos, which is an annoying problem to photographers. Therefore, removing haze from images is an important and widely demanded topic in computer vision and computer graphics areas. The main challenge lies in the ambiguity of the problem. Haze attenuates the light reflected from the scenes, and further blends it with some additive light in the atmosphere. The target of haze removal is to recover the reflected light (i.e., the scene colors) from the blended light. This problem is mathematically ambiguous: there are an infinite number of solutions given the blended light. How can we know which solution is true? We need to answer this question in haze removal. Ambiguity is a common challenge for many computer vision problems. In terms of mathematics, ambiguity is because the number of equations is smaller than the number of unknowns. The methods in computer vision to solve the ambiguity can roughly categorized into two strategies. The first one is to acquire more known variables, e.g., some haze removal algorithms capture multiple images of the same scene under different settings (like polarizers).But it is not easy to obtain extra images in practice. The second strategy is to impose extra constraints using some knowledge or assumptions .All the images in this thesis are best viewed in the electronic version. This way is more practical since it requires as few as only one image. To this end, we focus on single image haze removal in this thesis. The key is to find a suitable prior. Priors are important in many computer vision topics. A prior tells the algorithm \"what can we know about the fact beforehand\" when the fact is not directly available. In general, a prior can be some statistical/physical properties, rules, or heuristic assumptions. The performance of the algorithms is often determined by the extent to which the prior is valid. Some widely used priors in computer vision are the smoothness prior, sparsity prior, and symmetry prior. In this thesis, we develop an effective but very simple prior, called the dark channel prior, to remove haze from a single image. The dark channel prior is a statistical property of outdoor haze-free images: most patches in these images should contain pixels which are dark in at least one color channel. These dark pixels can be due to shadows, colorfulness, geometry, or other factors. This prior provides a constraint for each pixel, and thus solves the ambiguity of the problem. Combining this prior with a physical haze imaging model, we can easily recover high quality haze-free images.",
            "referenceCount": 45,
            "citationCount": 3687,
            "influentialCitationCount": 1004,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Geography"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Geography",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Malhotra2016SingleIH,\n author = {Mohinder Malhotra},\n title = {Single Image Haze Removal Using Dark Channel Prior},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:258986132bf17755fe8263e42429fe73218c1534",
            "@type": "ScholarlyArticle",
            "paperId": "258986132bf17755fe8263e42429fe73218c1534",
            "corpusId": 9026666,
            "url": "https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534",
            "title": "CIDEr: Consensus-based image description evaluation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/VedantamZP14a",
                "MAG": "2952574180",
                "ArXiv": "1411.5726",
                "DOI": "10.1109/CVPR.2015.7299087",
                "CorpusId": 9026666
            },
            "abstract": "Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.",
            "referenceCount": 57,
            "citationCount": 3280,
            "influentialCitationCount": 934,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.5726.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-11-20",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vedantam2014CIDErCI,\n author = {Ramakrishna Vedantam and C. L. Zitnick and Devi Parikh},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4566-4575},\n title = {CIDEr: Consensus-based image description evaluation},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3596cc61267bdae97bb1df9b01a8e9a1b8dca450",
            "@type": "ScholarlyArticle",
            "paperId": "3596cc61267bdae97bb1df9b01a8e9a1b8dca450",
            "corpusId": 362991,
            "url": "https://www.semanticscholar.org/paper/3596cc61267bdae97bb1df9b01a8e9a1b8dca450",
            "title": "Natural Image Statistics - A Probabilistic Approach to Early Computational Vision",
            "venue": "Computational Imaging and Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "series/civ/HyvarinenHH09",
                "MAG": "104847522",
                "DOI": "10.1007/978-1-84882-491-1",
                "CorpusId": 362991
            },
            "abstract": null,
            "referenceCount": 4,
            "citationCount": 645,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-04-21",
            "journal": {
                "name": null,
                "volume": "39"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hyv\u00e4rinen2009NaturalIS,\n author = {Aapo Hyv\u00e4rinen and J. Hurri and P. Hoyer},\n booktitle = {Computational Imaging and Vision},\n pages = {1-451},\n title = {Natural Image Statistics - A Probabilistic Approach to Early Computational Vision},\n volume = {39},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
            "@type": "ScholarlyArticle",
            "paperId": "41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
            "corpusId": 208175544,
            "url": "https://www.semanticscholar.org/paper/41c67d04be2d1632c0d3b0880c21c9fe797cdab8",
            "title": "EfficientDet: Scalable and Efficient Object Detection",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2990268359",
                "ArXiv": "1911.09070",
                "DBLP": "journals/corr/abs-1911-09070",
                "DOI": "10.1109/cvpr42600.2020.01079",
                "CorpusId": 208175544
            },
            "abstract": "Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDetD7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detector.",
            "referenceCount": 46,
            "citationCount": 3132,
            "influentialCitationCount": 523,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.09070",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-11-20",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tan2019EfficientDetSA,\n author = {Mingxing Tan and Ruoming Pang and Quoc V. Le},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {10778-10787},\n title = {EfficientDet: Scalable and Efficient Object Detection},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9e475a514f54665478aac6038c262e5a6bac5e64",
            "@type": "ScholarlyArticle",
            "paperId": "9e475a514f54665478aac6038c262e5a6bac5e64",
            "corpusId": 85517967,
            "url": "https://www.semanticscholar.org/paper/9e475a514f54665478aac6038c262e5a6bac5e64",
            "title": "nuScenes: A Multimodal Dataset for Autonomous Driving",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1903-11027",
                "ArXiv": "1903.11027",
                "MAG": "3035574168",
                "DOI": "10.1109/cvpr42600.2020.01164",
                "CorpusId": 85517967
            },
            "abstract": "Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.",
            "referenceCount": 102,
            "citationCount": 3094,
            "influentialCitationCount": 584,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1903.11027",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-26",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Caesar2019nuScenesAM,\n author = {Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and Venice Erin Liong and Qiang Xu and Anush Krishnan and Yuxin Pan and G. Baldan and Oscar Beijbom},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11618-11628},\n title = {nuScenes: A Multimodal Dataset for Autonomous Driving},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:02227c94dd41fe0b439e050d377b0beb5d427cda",
            "@type": "ScholarlyArticle",
            "paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda",
            "corpusId": 16852518,
            "url": "https://www.semanticscholar.org/paper/02227c94dd41fe0b439e050d377b0beb5d427cda",
            "title": "Reading Digits in Natural Images with Unsupervised Feature Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2335728318",
                "CorpusId": 16852518
            },
            "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.",
            "referenceCount": 28,
            "citationCount": 5525,
            "influentialCitationCount": 1917,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Netzer2011ReadingDI,\n author = {Yuval Netzer and Tao Wang and Adam Coates and A. Bissacco and Bo Wu and A. Ng},\n title = {Reading Digits in Natural Images with Unsupervised Feature Learning},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:976e29fe6c9baeb39732bca0e35f66f84d5bdd90",
            "@type": "ScholarlyArticle",
            "paperId": "976e29fe6c9baeb39732bca0e35f66f84d5bdd90",
            "corpusId": 206769866,
            "url": "https://www.semanticscholar.org/paper/976e29fe6c9baeb39732bca0e35f66f84d5bdd90",
            "title": "ORB: An efficient alternative to SIFT or SURF",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/iccv/RubleeRKB11",
                "MAG": "2117228865",
                "DOI": "10.1109/ICCV.2011.6126544",
                "CorpusId": 206769866
            },
            "abstract": "Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on BRIEF, called ORB, which is rotation invariant and resistant to noise. We demonstrate through experiments how ORB is at two orders of magnitude faster than SIFT, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.",
            "referenceCount": 32,
            "citationCount": 8690,
            "influentialCitationCount": 1363,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-06",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rublee2011ORBAE,\n author = {Ethan Rublee and V. Rabaud and K. Konolige and G. Bradski},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {2564-2571},\n title = {ORB: An efficient alternative to SIFT or SURF},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c2fb5b39428818d7ec8cc78e152e19c21b7db568",
            "@type": "ScholarlyArticle",
            "paperId": "c2fb5b39428818d7ec8cc78e152e19c21b7db568",
            "corpusId": 12552176,
            "url": "https://www.semanticscholar.org/paper/c2fb5b39428818d7ec8cc78e152e19c21b7db568",
            "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/DosovitskiyFIHH15",
                "MAG": "2951309005",
                "ArXiv": "1504.06852",
                "DOI": "10.1109/ICCV.2015.316",
                "CorpusId": 12552176
            },
            "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks CNNs succeeded at. In this paper we construct CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a large synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.",
            "referenceCount": 37,
            "citationCount": 3582,
            "influentialCitationCount": 564,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1504.06852",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-04-26",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dosovitskiy2015FlowNetLO,\n author = {A. Dosovitskiy and P. Fischer and Eddy Ilg and Philip H\u00e4usser and C. Hazirbas and Vladimir Golkov and Patrick van der Smagt and D. Cremers and T. Brox},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2758-2766},\n title = {FlowNet: Learning Optical Flow with Convolutional Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
            "@type": "ScholarlyArticle",
            "paperId": "0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
            "corpusId": 206764694,
            "url": "https://www.semanticscholar.org/paper/0e5a262bf59b68ba8a7a1103d16fa33a9f5ffc28",
            "title": "Contour Detection and Hierarchical Image Segmentation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/pami/ArbelaezMFM11",
                "MAG": "2110158442",
                "DOI": "10.1109/TPAMI.2010.161",
                "CorpusId": 206764694,
                "PubMed": "20733228"
            },
            "abstract": "This paper investigates two fundamental problems in computer vision: contour detection and image segmentation. We present state-of-the-art algorithms for both of these tasks. Our contour detector combines multiple local cues into a globalization framework based on spectral clustering. Our segmentation algorithm consists of generic machinery for transforming the output of any contour detector into a hierarchical region tree. In this manner, we reduce the problem of image segmentation to that of contour detection. Extensive experimental evaluation demonstrates that both our contour detection and segmentation methods significantly outperform competing algorithms. The automatically generated hierarchical segmentations can be interactively refined by user-specified annotations. Computation at multiple image resolutions provides a means of coupling our system to recognition applications.",
            "referenceCount": 89,
            "citationCount": 4910,
            "influentialCitationCount": 824,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.berkeley.edu/%7Emalik/papers/arbelaezMFM-pami2010.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-05-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "33"
            },
            "citationStyles": {
                "bibtex": "@Article{Arbel\u00e1ez2011ContourDA,\n author = {Pablo Arbel\u00e1ez and M. Maire and Charless C. Fowlkes and Jitendra Malik},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {898-916},\n title = {Contour Detection and Hierarchical Image Segmentation},\n volume = {33},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8b3b8848a311c501e704c45c6d50430ab7068956",
            "@type": "ScholarlyArticle",
            "paperId": "8b3b8848a311c501e704c45c6d50430ab7068956",
            "corpusId": 206769852,
            "url": "https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956",
            "title": "HMDB: A large video database for human motion recognition",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2126579184",
                "DBLP": "conf/iccv/KuehneJGPS11",
                "DOI": "10.1109/ICCV.2011.6126543",
                "CorpusId": 206769852
            },
            "abstract": "With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.",
            "referenceCount": 35,
            "citationCount": 3245,
            "influentialCitationCount": 965,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/69981/1/Poggio-HMDB.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-06",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kuehne2011HMDBAL,\n author = {Hilde Kuehne and Hueihan Jhuang and Est\u00edbaliz Garrote and T. Poggio and Thomas Serre},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {2556-2563},\n title = {HMDB: A large video database for human motion recognition},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "@type": "ScholarlyArticle",
            "paperId": "bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "corpusId": 1660289,
            "url": "https://www.semanticscholar.org/paper/bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "title": "Online Object Tracking: A Benchmark",
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2089961441",
                "DBLP": "conf/cvpr/WuLY13",
                "DOI": "10.1109/CVPR.2013.312",
                "CorpusId": 1660289
            },
            "abstract": "Object tracking is one of the most important components in numerous applications of computer vision. While much progress has been made in recent years with efforts on sharing code and datasets, it is of great importance to develop a library and benchmark to gauge the state of the art. After briefly reviewing recent advances of online object tracking, we carry out large scale experiments with various evaluation criteria to understand how these algorithms perform. The test image sequences are annotated with different attributes for performance evaluation and analysis. By analyzing quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.",
            "referenceCount": 66,
            "citationCount": 3653,
            "influentialCitationCount": 1002,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://faculty.ucmerced.edu/mhyang/papers/cvpr13_benchmark.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2013-06-23",
            "journal": {
                "name": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2013OnlineOT,\n author = {Yi Wu and Jongwoo Lim and Ming-Hsuan Yang},\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {2411-2418},\n title = {Online Object Tracking: A Benchmark},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1b7fc36dc56a400f7a7f4c75e53550bbb6a0a4d8",
            "@type": "ScholarlyArticle",
            "paperId": "1b7fc36dc56a400f7a7f4c75e53550bbb6a0a4d8",
            "corpusId": 1264129,
            "url": "https://www.semanticscholar.org/paper/1b7fc36dc56a400f7a7f4c75e53550bbb6a0a4d8",
            "title": "Guided Image Filtering",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2160956336",
                "DBLP": "conf/eccv/HeST10",
                "DOI": "10.1007/978-3-642-15549-9_1",
                "CorpusId": 1264129,
                "PubMed": "23599054"
            },
            "abstract": null,
            "referenceCount": 61,
            "citationCount": 5685,
            "influentialCitationCount": 647,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15549-9_1.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{He2010GuidedIF,\n author = {Kaiming He and Jian Sun and Xiaoou Tang},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1397-1409},\n title = {Guided Image Filtering},\n volume = {35},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:87e43e9eba01a4eb03436c9946bf6aa031a5d5af",
            "@type": "ScholarlyArticle",
            "paperId": "87e43e9eba01a4eb03436c9946bf6aa031a5d5af",
            "corpusId": 16074195,
            "url": "https://www.semanticscholar.org/paper/87e43e9eba01a4eb03436c9946bf6aa031a5d5af",
            "title": "Tensor Decompositions and Applications",
            "venue": "SIAM Review",
            "publicationVenue": {
                "id": "urn:research:8f59dd66-e4cd-4341-8ea9-9a03d965a009",
                "name": "SIAM Review",
                "alternate_names": [
                    "SIAM Rev",
                    "Siam Rev",
                    "Siam Review"
                ],
                "issn": "0036-1445",
                "url": "https://www.jstor.org/journal/siamreview"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/siamrev/KoldaB09",
                "MAG": "2024165284",
                "DOI": "10.1137/07070111X",
                "CorpusId": 16074195
            },
            "abstract": "This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or $N$-way array. Decompositions of higher-order tensors (i.e., $N$-way arrays with $N \\geq 3$) have applications in psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.",
            "referenceCount": 268,
            "citationCount": 8576,
            "influentialCitationCount": 1251,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2009-08-01",
            "journal": {
                "name": "SIAM Rev.",
                "volume": "51"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolda2009TensorDA,\n author = {T. Kolda and Brett W. Bader},\n booktitle = {SIAM Review},\n journal = {SIAM Rev.},\n pages = {455-500},\n title = {Tensor Decompositions and Applications},\n volume = {51},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:689c97982f0ef6d8b0df3ec33a3abe29b8f97c1f",
            "@type": "ScholarlyArticle",
            "paperId": "689c97982f0ef6d8b0df3ec33a3abe29b8f97c1f",
            "corpusId": 2329405,
            "url": "https://www.semanticscholar.org/paper/689c97982f0ef6d8b0df3ec33a3abe29b8f97c1f",
            "title": "Global contrast based salient region detection",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2037954058",
                "DBLP": "conf/cvpr/ChengZMHH11",
                "DOI": "10.1109/CVPR.2011.5995344",
                "CorpusId": 2329405
            },
            "abstract": "Reliable estimation of visual saliency allows appropriate processing of images without prior knowledge of their contents, and thus remains an important step in many computer vision tasks including image segmentation, object recognition, and adaptive compression. We propose a regional contrast based saliency extraction algorithm, which simultaneously evaluates global contrast differences and spatial coherence. The proposed algorithm is simple, efficient, and yields full resolution saliency maps. Our algorithm consistently outperformed existing saliency detection methods, yielding higher precision and better recall rates, when evaluated using one of the largest publicly available data sets. We also demonstrate how the extracted saliency map can be used to create high quality segmentation masks for subsequent image processing.",
            "referenceCount": 96,
            "citationCount": 3604,
            "influentialCitationCount": 535,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kaust.edu.sa/bitstream/10754/622089/1/05995344.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-20",
            "journal": {
                "name": "CVPR 2011",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2011GlobalCB,\n author = {Ming-Ming Cheng and Guo-Xin Zhang and N. Mitra and Xiaolei Huang and Shimin Hu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {CVPR 2011},\n pages = {409-416},\n title = {Global contrast based salient region detection},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86d27422aac2398cfe132ae8e312a6f2d190f754",
            "@type": "ScholarlyArticle",
            "paperId": "86d27422aac2398cfe132ae8e312a6f2d190f754",
            "corpusId": 1211102,
            "url": "https://www.semanticscholar.org/paper/86d27422aac2398cfe132ae8e312a6f2d190f754",
            "title": "BRISK: Binary Robust invariant scalable keypoints",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/iccv/LeuteneggerCS11",
                "MAG": "2141584146",
                "DOI": "10.1109/ICCV.2011.6126542",
                "CorpusId": 1211102
            },
            "abstract": "Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the SIFT and SURF algorithms which exhibit great performance under a variety of image transformations, with SURF in particular considered as the most computationally efficient amongst the high-performance methods to date. In this paper we propose BRISK1, a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals BRISK's adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than SURF in cases). The key to speed lies in the application of a novel scale-space FAST-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.",
            "referenceCount": 15,
            "citationCount": 3389,
            "influentialCitationCount": 516,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://margaritachli.com/papers/ICCV2011paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-06",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Leutenegger2011BRISKBR,\n author = {Stefan Leutenegger and M. Chli and R. Siegwart},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {2548-2555},\n title = {BRISK: Binary Robust invariant scalable keypoints},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:82082c783bfbab54681976473bb9e95a95c0b0a4",
            "@type": "ScholarlyArticle",
            "paperId": "82082c783bfbab54681976473bb9e95a95c0b0a4",
            "corpusId": 33421355,
            "url": "https://www.semanticscholar.org/paper/82082c783bfbab54681976473bb9e95a95c0b0a4",
            "title": "The detection of intensity changes by computer and biological vision systems",
            "venue": "Computer Vision Graphics and Image Processing",
            "publicationVenue": {
                "id": "urn:research:451e9e22-42a8-43b7-ac3e-33b7d5de65e3",
                "name": "Computer Vision Graphics and Image Processing",
                "alternate_names": [
                    "Comput Vis Graph Image Process"
                ],
                "issn": "0734-189X",
                "url": "https://www.sciencedirect.com/journal/computer-vision-graphics-and-image-processing"
            },
            "year": 1983,
            "externalIds": {
                "DBLP": "journals/cvgip/Hildreth83",
                "MAG": "2050600304",
                "DOI": "10.1016/0734-189X(83)90093-2",
                "CorpusId": 33421355
            },
            "abstract": null,
            "referenceCount": 42,
            "citationCount": 206,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1983-04-01",
            "journal": {
                "name": "Comput. Vis. Graph. Image Process.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Hildreth1983TheDO,\n author = {E. Hildreth},\n booktitle = {Computer Vision Graphics and Image Processing},\n journal = {Comput. Vis. Graph. Image Process.},\n pages = {1-27},\n title = {The detection of intensity changes by computer and biological vision systems},\n volume = {22},\n year = {1983}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1061dea79f8c5e55bf11f7873b9de109c51cbc67",
            "@type": "ScholarlyArticle",
            "paperId": "1061dea79f8c5e55bf11f7873b9de109c51cbc67",
            "corpusId": 53624641,
            "url": "https://www.semanticscholar.org/paper/1061dea79f8c5e55bf11f7873b9de109c51cbc67",
            "title": "Fundamentals of Digital Image Processing",
            "venue": "Control of Color Imaging Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2100115174",
                "DOI": "10.1201/9781315221236-5",
                "CorpusId": 53624641
            },
            "abstract": "Introduction. 1. Two Dimensional Systems and Mathematical Preliminaries. 2. Image Perception. 3. Image Sampling and Quantization. 4. Image Transforms. 5. Image Representation by Stochastic Models. 6. Image Enhancement. 7. Image Filtering and Restoration. 8. Image Analysis and Computer Vision. 9. Image Reconstruction From Projections. 10. Image Data Compression.",
            "referenceCount": 94,
            "citationCount": 3759,
            "influentialCitationCount": 192,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www3.ub.tu-berlin.de/ihv/000048660.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-10-03",
            "journal": {
                "name": "Control of Color Imaging Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jain2018FundamentalsOD,\n author = {Anil K. Jain},\n booktitle = {Control of Color Imaging Systems},\n journal = {Control of Color Imaging Systems},\n title = {Fundamentals of Digital Image Processing},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "@type": "ScholarlyArticle",
            "paperId": "1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "corpusId": 40100965,
            "url": "https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a",
            "title": "Universal Language Model Fine-tuning for Text Classification",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2798812533",
                "DBLP": "conf/acl/RuderH18",
                "ACL": "P18-1031",
                "DOI": "10.18653/v1/P18-1031",
                "CorpusId": 40100965
            },
            "abstract": "Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",
            "referenceCount": 57,
            "citationCount": 3088,
            "influentialCitationCount": 296,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P18-1031.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-01-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Howard2018UniversalLM,\n author = {Jeremy Howard and Sebastian Ruder},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {328-339},\n title = {Universal Language Model Fine-tuning for Text Classification},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:60104351ac65115503c9e92e856bcab6a13b0ce8",
            "@type": "ScholarlyArticle",
            "paperId": "60104351ac65115503c9e92e856bcab6a13b0ce8",
            "corpusId": 4883312,
            "url": "https://www.semanticscholar.org/paper/60104351ac65115503c9e92e856bcab6a13b0ce8",
            "title": "Multimodal Unsupervised Image-to-Image Translation",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963890275",
                "DBLP": "journals/corr/abs-1804-04732",
                "ArXiv": "1804.04732",
                "DOI": "10.1007/978-3-030-01219-9_11",
                "CorpusId": 4883312
            },
            "abstract": null,
            "referenceCount": 87,
            "citationCount": 2120,
            "influentialCitationCount": 460,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1804.04732",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2018MultimodalUI,\n author = {Xun Huang and Ming-Yu Liu and Serge J. Belongie and J. Kautz},\n booktitle = {European Conference on Computer Vision},\n pages = {179-196},\n title = {Multimodal Unsupervised Image-to-Image Translation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a0dc3135c40e150f0271002a96b7c9680b6cac40",
            "@type": "ScholarlyArticle",
            "paperId": "a0dc3135c40e150f0271002a96b7c9680b6cac40",
            "corpusId": 219791950,
            "url": "https://www.semanticscholar.org/paper/a0dc3135c40e150f0271002a96b7c9680b6cac40",
            "title": "Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/nips/TancikSMFRSRBN20",
                "ArXiv": "2006.10739",
                "MAG": "3036843665",
                "CorpusId": 219791950
            },
            "abstract": "We show that passing input points through a simple Fourier feature mapping enables a multilayer perceptron (MLP) to learn high-frequency functions in low-dimensional problem domains. These results shed light on recent advances in computer vision and graphics that achieve state-of-the-art results by using MLPs to represent complex 3D objects and scenes. Using tools from the neural tangent kernel (NTK) literature, we show that a standard MLP fails to learn high frequencies both in theory and in practice. To overcome this spectral bias, we use a Fourier feature mapping to transform the effective NTK into a stationary kernel with a tunable bandwidth. We suggest an approach for selecting problem-specific Fourier features that greatly improves the performance of MLPs for low-dimensional regression tasks relevant to the computer vision and graphics communities.",
            "referenceCount": 48,
            "citationCount": 1312,
            "influentialCitationCount": 162,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2006.10739"
            },
            "citationStyles": {
                "bibtex": "@Article{Tancik2020FourierFL,\n author = {Matthew Tancik and Pratul P. Srinivasan and B. Mildenhall and Sara Fridovich-Keil and N. Raghavan and Utkarsh Singhal and R. Ramamoorthi and J. Barron and Ren Ng},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},\n volume = {abs/2006.10739},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dd81523b9accdf1c13cd37f76b22ab27d84b7a42",
            "@type": "ScholarlyArticle",
            "paperId": "dd81523b9accdf1c13cd37f76b22ab27d84b7a42",
            "corpusId": 58007025,
            "url": "https://www.semanticscholar.org/paper/dd81523b9accdf1c13cd37f76b22ab27d84b7a42",
            "title": "DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2951382975",
                "DBLP": "journals/corr/abs-1901-05103",
                "ArXiv": "1901.05103",
                "DOI": "10.1109/CVPR.2019.00025",
                "CorpusId": 58007025
            },
            "abstract": "Computer graphics, 3D computer vision and robotics communities have produced multiple approaches to representing 3D geometry for rendering and reconstruction. These provide trade-offs across fidelity, efficiency and compression capabilities. In this work, we introduce DeepSDF, a learned continuous Signed Distance Function (SDF) representation of a class of shapes that enables high quality shape representation, interpolation and completion from partial and noisy 3D input data. DeepSDF, like its classical counterpart, represents a shape's surface by a continuous volumetric field: the magnitude of a point in the field represents the distance to the surface boundary and the sign indicates whether the region is inside (-) or outside (+) of the shape, hence our representation implicitly encodes a shape's boundary as the zero-level-set of the learned function while explicitly representing the classification of space as being part of the shapes interior or not. While classical SDF's both in analytical or discretized voxel form typically represent the surface of a single shape, DeepSDF can represent an entire class of shapes. Furthermore, we show state-of-the-art performance for learned 3D shape representation and completion while reducing the model size by an order of magnitude compared with previous work.",
            "referenceCount": 68,
            "citationCount": 2399,
            "influentialCitationCount": 376,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.05103",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-01-16",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Park2019DeepSDFLC,\n author = {Jeong Joon Park and Peter R. Florence and Julian Straub and Richard A. Newcombe and S. Lovegrove},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {165-174},\n title = {DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "@type": "ScholarlyArticle",
            "paperId": "04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "corpusId": 661123,
            "url": "https://www.semanticscholar.org/paper/04b23f577c20d1a0e2a67aadda555f58e6d23d6e",
            "title": "Support vector machines",
            "venue": "Data Mining and Knowledge Discovery Handbook",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1516735418",
                "DBLP": "books/sp/datamining2005/Shmilovici05",
                "DOI": "10.1002/wics.49",
                "CorpusId": 661123
            },
            "abstract": "Support vector machines (SVMs) are a family of machine learning methods, originally introduced for the problem of classification and later generalized to various other situations. They are based on principles of statistical learning theory and convex optimization, and are currently used in various domains of application, including bioinformatics, text categorization, and computer vision. Copyright \u00a9 2009 John Wiley & Sons, Inc.",
            "referenceCount": 240,
            "citationCount": 6180,
            "influentialCitationCount": 880,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2008-08-12",
            "journal": {
                "name": "Wiley Interdisciplinary Reviews: Computational Statistics",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Steinwart2008SupportVM,\n author = {Ingo Steinwart and A. Christmann},\n booktitle = {Data Mining and Knowledge Discovery Handbook},\n journal = {Wiley Interdisciplinary Reviews: Computational Statistics},\n title = {Support vector machines},\n volume = {1},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6ff2a434578ff2746b9283e45abf296887f48a2d",
            "@type": "ScholarlyArticle",
            "paperId": "6ff2a434578ff2746b9283e45abf296887f48a2d",
            "corpusId": 13046179,
            "url": "https://www.semanticscholar.org/paper/6ff2a434578ff2746b9283e45abf296887f48a2d",
            "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/HendrycksG16c",
                "ArXiv": "1610.02136",
                "MAG": "2952053192",
                "CorpusId": 13046179
            },
            "abstract": "We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",
            "referenceCount": 47,
            "citationCount": 2363,
            "influentialCitationCount": 548,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.02136"
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2016ABF,\n author = {Dan Hendrycks and Kevin Gimpel},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},\n volume = {abs/1610.02136},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:43cb4886a8056d5005702edbc51be327542b2124",
            "@type": "ScholarlyArticle",
            "paperId": "43cb4886a8056d5005702edbc51be327542b2124",
            "corpusId": 227239228,
            "url": "https://www.semanticscholar.org/paper/43cb4886a8056d5005702edbc51be327542b2124",
            "title": "Pre-Trained Image Processing Transformer",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/cvpr/Chen000DLMX0021",
                "ArXiv": "2012.00364",
                "MAG": "3109319753",
                "DOI": "10.1109/CVPR46437.2021.01212",
                "CorpusId": 227239228
            },
            "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",
            "referenceCount": 97,
            "citationCount": 985,
            "influentialCitationCount": 102,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2012.00364",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-12-01",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2020PreTrainedIP,\n author = {Hanting Chen and Yunhe Wang and Tianyu Guo and Chang Xu and Yiping Deng and Zhenhua Liu and Siwei Ma and Chunjing Xu and Chao Xu and Wen Gao},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {12294-12305},\n title = {Pre-Trained Image Processing Transformer},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3efd851140aa28e95221b55fcc5659eea97b172d",
            "@type": "ScholarlyArticle",
            "paperId": "3efd851140aa28e95221b55fcc5659eea97b172d",
            "corpusId": 206756462,
            "url": "https://www.semanticscholar.org/paper/3efd851140aa28e95221b55fcc5659eea97b172d",
            "title": "The Graph Neural Network Model",
            "venue": "IEEE Transactions on Neural Networks",
            "publicationVenue": {
                "id": "urn:research:2ac50919-507e-41c7-93a8-721c4b804757",
                "name": "IEEE Transactions on Neural Networks",
                "alternate_names": [
                    "IEEE Trans Neural Netw"
                ],
                "issn": "1045-9227",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=72"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/tnn/ScarselliGTHM09",
                "MAG": "2116341502",
                "DOI": "10.1109/TNN.2008.2005605",
                "CorpusId": 206756462,
                "PubMed": "19068426"
            },
            "abstract": "Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",
            "referenceCount": 112,
            "citationCount": 5320,
            "influentialCitationCount": 455,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ro.uow.edu.au/cgi/viewcontent.cgi?article=10501&context=infopapers",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Neural Networks",
                "volume": "20"
            },
            "citationStyles": {
                "bibtex": "@Article{Scarselli2009TheGN,\n author = {F. Scarselli and M. Gori and A. Tsoi and M. Hagenbuchner and G. Monfardini},\n booktitle = {IEEE Transactions on Neural Networks},\n journal = {IEEE Transactions on Neural Networks},\n pages = {61-80},\n title = {The Graph Neural Network Model},\n volume = {20},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "@type": "ScholarlyArticle",
            "paperId": "6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "corpusId": 1694378,
            "url": "https://www.semanticscholar.org/paper/6818668fb895d95861a2eb9673ddc3a41e27b3b3",
            "title": "A Combined Corner and Edge Detector",
            "venue": "Alvey Vision Conference",
            "publicationVenue": {
                "id": "urn:research:67ceef4f-3b48-4a6c-ac54-c29c57589d33",
                "name": "Alvey Vision Conference",
                "alternate_names": [
                    "AVC",
                    "Alvey Vis Conf"
                ],
                "issn": null,
                "url": null
            },
            "year": 1988,
            "externalIds": {
                "DBLP": "conf/bmvc/HarrisS88",
                "MAG": "2111308925",
                "DOI": "10.5244/C.2.23",
                "CorpusId": 1694378
            },
            "abstract": "The problem we are addressing in Alvey Project MMI149 is that of using computer vision to understand the unconstrained 3D world, in which the viewed scenes will in general contain too wide a diversity of objects for topdown recognition techniques to work. For example, we desire to obtain an understanding of natural scenes, containing roads, buildings, trees, bushes, etc., as typified by the two frames from a sequence illustrated in Figure 1. The solution to this problem that we are pursuing is to use a computer vision system based upon motion analysis of a monocular image sequence from a mobile camera. By extraction and tracking of image features, representations of the 3D analogues of these features can be constructed.",
            "referenceCount": 10,
            "citationCount": 14769,
            "influentialCitationCount": 893,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.bmva.org/bmvc/1988/avc-88-023.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Harris1988ACC,\n author = {C. G. Harris and M. Stephens},\n booktitle = {Alvey Vision Conference},\n pages = {1-6},\n title = {A Combined Corner and Edge Detector},\n year = {1988}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "@type": "ScholarlyArticle",
            "paperId": "d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "corpusId": 195859047,
            "url": "https://www.semanticscholar.org/paper/d2f78c2b2b325d72f359d4c797c9aab6a8e60942",
            "title": "A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms",
            "venue": "Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2104974755",
                "DBLP": "journals/ijcv/ScharsteinS02",
                "DOI": "10.1023/A:1014573219977",
                "CorpusId": 195859047
            },
            "abstract": null,
            "referenceCount": 127,
            "citationCount": 7488,
            "influentialCitationCount": 785,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2001-12-09",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "47"
            },
            "citationStyles": {
                "bibtex": "@Article{Scharstein2001ATA,\n author = {D. Scharstein and R. Szeliski},\n booktitle = {Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)},\n journal = {International Journal of Computer Vision},\n pages = {7-42},\n title = {A Taxonomy and Evaluation of Dense Two-Frame Stereo Correspondence Algorithms},\n volume = {47},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bdacaf992b0394cc73ff94fcbf6b31483406286",
            "@type": "ScholarlyArticle",
            "paperId": "6bdacaf992b0394cc73ff94fcbf6b31483406286",
            "corpusId": 1150626,
            "url": "https://www.semanticscholar.org/paper/6bdacaf992b0394cc73ff94fcbf6b31483406286",
            "title": "A Flexible New Technique for Camera Calibration",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2167667767",
                "DBLP": "journals/pami/Zhang00",
                "DOI": "10.1109/34.888718",
                "CorpusId": 1150626
            },
            "abstract": "We propose a flexible technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use.",
            "referenceCount": 28,
            "citationCount": 13119,
            "influentialCitationCount": 960,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-11-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2000AFN,\n author = {Zhengyou Zhang},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {1330-1334},\n title = {A Flexible New Technique for Camera Calibration},\n volume = {22},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d",
            "@type": "ScholarlyArticle",
            "paperId": "2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d",
            "corpusId": 5636055,
            "url": "https://www.semanticscholar.org/paper/2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d",
            "title": "Scene Parsing through ADE20K Dataset",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2737258237",
                "DBLP": "conf/cvpr/ZhouZPFB017",
                "DOI": "10.1109/CVPR.2017.544",
                "CorpusId": 5636055
            },
            "abstract": "Scene parsing, or recognizing and segmenting objects and stuff in an image, is one of the key problems in computer vision. Despite the communitys efforts in data collection, there are still few image datasets covering a wide range of scenes and object categories with dense and detailed annotations for scene parsing. In this paper, we introduce and analyze the ADE20K dataset, spanning diverse annotations of scenes, objects, parts of objects, and in some cases even parts of parts. A scene parsing benchmark is built upon the ADE20K with 150 object and stuff classes included. Several segmentation baseline models are evaluated on the benchmark. A novel network design called Cascade Segmentation Module is proposed to parse a scene into stuff, objects, and object parts in a cascade and improve over the baselines. We further show that the trained scene parsing networks can lead to applications such as image content removal and scene synthesis1.",
            "referenceCount": 35,
            "citationCount": 2151,
            "influentialCitationCount": 500,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/124982/2/scene-parse-camera-ready.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-21",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2017ScenePT,\n author = {Bolei Zhou and Hang Zhao and Xavier Puig and S. Fidler and Adela Barriuso and A. Torralba},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5122-5130},\n title = {Scene Parsing through ADE20K Dataset},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f58ea68448a584b8c8540c86bb9965d746b767a9",
            "@type": "ScholarlyArticle",
            "paperId": "f58ea68448a584b8c8540c86bb9965d746b767a9",
            "corpusId": 53304118,
            "url": "https://www.semanticscholar.org/paper/f58ea68448a584b8c8540c86bb9965d746b767a9",
            "title": "Deep Learning: Methods and Applications",
            "venue": "Foundations and Trends\u00ae in Signal Processing",
            "publicationVenue": {
                "id": "urn:research:a30697a9-2a04-4a33-ae2f-03ea5fbb71f8",
                "name": "Foundations and Trends\u00ae in Signal Processing",
                "alternate_names": [
                    "Found Trends Signal Process",
                    "Found Trends\u00ae Signal Process",
                    "Foundations and Trends in Signal Processing"
                ],
                "issn": "1932-8346",
                "url": "https://www.nowpublishers.com/sig"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2150341604",
                "DBLP": "journals/ftsig/DengY14",
                "DOI": "10.1561/2000000039",
                "CorpusId": 53304118
            },
            "abstract": "This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and computer vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.",
            "referenceCount": 454,
            "citationCount": 3183,
            "influentialCitationCount": 143,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ped-perinatology.ru/jour/article/download/586/570",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2014-06-12",
            "journal": {
                "name": "Found. Trends Signal Process.",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Deng2014DeepLM,\n author = {L. Deng and Dong Yu},\n booktitle = {Foundations and Trends\u00ae in Signal Processing},\n journal = {Found. Trends Signal Process.},\n pages = {197-387},\n title = {Deep Learning: Methods and Applications},\n volume = {7},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ba8376f416e90fe434977ae9f300997667498d2",
            "@type": "ScholarlyArticle",
            "paperId": "1ba8376f416e90fe434977ae9f300997667498d2",
            "corpusId": 2407217,
            "url": "https://www.semanticscholar.org/paper/1ba8376f416e90fe434977ae9f300997667498d2",
            "title": "Multi-view Convolutional Neural Networks for 3D Shape Recognition",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/SuMKL15",
                "ArXiv": "1505.00880",
                "MAG": "2952789225",
                "DOI": "10.1109/ICCV.2015.114",
                "CorpusId": 2407217
            },
            "abstract": "A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.",
            "referenceCount": 39,
            "citationCount": 2711,
            "influentialCitationCount": 394,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://people.cs.umass.edu/%7Eelm/papers/HangSu_3D_arXiv.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-05",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Su2015MultiviewCN,\n author = {Hang Su and Subhransu Maji and E. Kalogerakis and E. Learned-Miller},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {945-953},\n title = {Multi-view Convolutional Neural Networks for 3D Shape Recognition},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "@type": "ScholarlyArticle",
            "paperId": "de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "corpusId": 125617073,
            "url": "https://www.semanticscholar.org/paper/de5e7320729f5d3cbb6709eb6329ec41ace8c95d",
            "title": "Gaussian Error Linear Units (GELUs)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.08415",
                "MAG": "2899663614",
                "CorpusId": 125617073
            },
            "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs ($x\\mathbf{1}_{x>0}$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.",
            "referenceCount": 30,
            "citationCount": 2819,
            "influentialCitationCount": 345,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2016-06-27",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Hendrycks2016GaussianEL,\n author = {Dan Hendrycks and Kevin Gimpel},\n journal = {arXiv: Learning},\n title = {Gaussian Error Linear Units (GELUs)},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1728cb805a9573b59330890ba9723e73d6c3c974",
            "@type": "ScholarlyArticle",
            "paperId": "1728cb805a9573b59330890ba9723e73d6c3c974",
            "corpusId": 219559263,
            "url": "https://www.semanticscholar.org/paper/1728cb805a9573b59330890ba9723e73d6c3c974",
            "title": "Knowledge Distillation: A Survey",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2006-05525",
                "MAG": "3034368386",
                "ArXiv": "2006.05525",
                "DOI": "10.1007/s11263-021-01453-z",
                "CorpusId": 219559263
            },
            "abstract": null,
            "referenceCount": 371,
            "citationCount": 1302,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://eprints.bbk.ac.uk/id/eprint/44038/1/KD_Survey-arxiv.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-06-09",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "129"
            },
            "citationStyles": {
                "bibtex": "@Article{Gou2020KnowledgeDA,\n author = {Jianping Gou and B. Yu and S. Maybank and D. Tao},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {1789 - 1819},\n title = {Knowledge Distillation: A Survey},\n volume = {129},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:398c296d0cc7f9d180f84969f8937e6d3a413796",
            "@type": "ScholarlyArticle",
            "paperId": "398c296d0cc7f9d180f84969f8937e6d3a413796",
            "corpusId": 2161592,
            "url": "https://www.semanticscholar.org/paper/398c296d0cc7f9d180f84969f8937e6d3a413796",
            "title": "Multi-column deep neural networks for image classification",
            "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2141125852",
                "DBLP": "conf/cvpr/CiresanMS12",
                "ArXiv": "1202.2745",
                "DOI": "10.1109/CVPR.2012.6248110",
                "CorpusId": 2161592
            },
            "abstract": "Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.",
            "referenceCount": 44,
            "citationCount": 3777,
            "influentialCitationCount": 177,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1202.2745",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-02-13",
            "journal": {
                "name": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ciresan2012MulticolumnDN,\n author = {D. Ciresan and U. Meier and J. Schmidhuber},\n booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {3642-3649},\n title = {Multi-column deep neural networks for image classification},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:427e2415d0d8f846cdabba34842162f7bab6af02",
            "@type": "ScholarlyArticle",
            "paperId": "427e2415d0d8f846cdabba34842162f7bab6af02",
            "corpusId": 47302,
            "url": "https://www.semanticscholar.org/paper/427e2415d0d8f846cdabba34842162f7bab6af02",
            "title": "Principal Warps: Thin-Plate Splines and the Decomposition of Deformations",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 1989,
            "externalIds": {
                "MAG": "2128409098",
                "DBLP": "journals/pami/Bookstein89",
                "DOI": "10.1109/34.24792",
                "CorpusId": 47302
            },
            "abstract": "The decomposition of deformations by principal warps is demonstrated. The method is extended to deal with curving edges between landmarks. This formulation is related to other applications of splines current in computer vision. How they might aid in the extraction of features for analysis, comparison, and diagnosis of biological and medical images in indicated. >",
            "referenceCount": 25,
            "citationCount": 5081,
            "influentialCitationCount": 439,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1989-06-01",
            "journal": {
                "name": "IEEE Trans. Pattern Anal. Mach. Intell.",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Bookstein1989PrincipalWT,\n author = {Fred L. Bookstein},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Trans. Pattern Anal. Mach. Intell.},\n pages = {567-585},\n title = {Principal Warps: Thin-Plate Splines and the Decomposition of Deformations},\n volume = {11},\n year = {1989}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a1067e837c7cbbefb30f8324c1eacb1afde39fc6",
            "@type": "ScholarlyArticle",
            "paperId": "a1067e837c7cbbefb30f8324c1eacb1afde39fc6",
            "corpusId": 222243846,
            "url": "https://www.semanticscholar.org/paper/a1067e837c7cbbefb30f8324c1eacb1afde39fc6",
            "title": "Optimal approximations by piecewise smooth functions and associated variational problems",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1989,
            "externalIds": {
                "MAG": "2114487471",
                "DOI": "10.1002/CPA.3160420503",
                "CorpusId": 222243846
            },
            "abstract": "Abstract : This reprint will introduce and study the most basic properties of three new variational problems which are suggested by applications to computer vision. In computer vision, a fundamental problem is to appropriately decompose the domain R of a function g (x,y) of two variables. This problem starts by describing the physical situation which produces images: assume that a three-dimensional world is observed by an eye or camera from some point P and that g1(rho) represents the intensity of the light in this world approaching the point sub 1 from a direction rho. If one has a lens at P focusing this light on a retina or a film-in both cases a plane domain R in which we may introduce coordinates x, y then let g(x,y) be the strength of the light signal striking R at a point with coordinates (x,y); g(x,y) is essentially the same as sub 1 (rho) -possibly after a simple transformation given by the geometry of the imaging syste. The function g(x,y) defined on the plane domain R will be called an image. What sort of function is g? The light reflected off the surfaces Si of various solid objects O sub i visible from P will strike the domain R in various open subsets R sub i. When one object O1 is partially in front of another object O2 as seen from P, but some of object O2 appears as the background to the sides of O1, then the open sets R1 and R2 will have a common boundary (the 'edge' of object O1 in the image defined on R) and one usually expects the image g(x,y) to be discontinuous along this boundary. (JHD)",
            "referenceCount": 13,
            "citationCount": 5630,
            "influentialCitationCount": 466,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dash.harvard.edu/bitstream/1/3637121/1/Mumford_OptimalApproxPiece.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1989-07-01",
            "journal": {
                "name": "Communications on Pure and Applied Mathematics",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Mumford1989OptimalAB,\n author = {D. Mumford and J. Shah},\n journal = {Communications on Pure and Applied Mathematics},\n pages = {577-685},\n title = {Optimal approximations by piecewise smooth functions and associated variational problems},\n volume = {42},\n year = {1989}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c896b6c0e8e6be8ba28142e096d705241ce94b9a",
            "@type": "ScholarlyArticle",
            "paperId": "c896b6c0e8e6be8ba28142e096d705241ce94b9a",
            "corpusId": 27576942,
            "url": "https://www.semanticscholar.org/paper/c896b6c0e8e6be8ba28142e096d705241ce94b9a",
            "title": "Level set methods and dynamic implicit surfaces",
            "venue": "Applied Mathematical Sciences",
            "publicationVenue": {
                "id": "urn:research:369b7733-9b48-4432-85b6-1e3e41080d1b",
                "name": "Applied Mathematical Sciences",
                "alternate_names": [
                    "Applied mathematical sciences",
                    "Appl Math Sci",
                    "Appl math sci"
                ],
                "issn": "1312-885X",
                "url": "http://www.m-hikari.com/ams/ams-2013/ams-57-60-2013/sommerAMS57-60-2013.pdf"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2093834886",
                "DBLP": "books/lib/OsherF03",
                "DOI": "10.1115/1.1760520",
                "CorpusId": 27576942
            },
            "abstract": "This book is an introduction to level set methods and dynamic implicit surfaces. These are powerful techniques for analyzing and computing moving fronts in a variety of different settings. While it gives many examples of the utility of the methods to a diverse set of applications, it also gives complete numerical analysis and recipes, which will enable users to quickly apply the techniques to real problems. The book begins with a description of implicit surfaces and their basic properties, then devises the level set geometry and calculus toolbox, including the construction of signed distance functions. Part II adds dynamics to this static calculus. Topics include the level set equation itself, Hamilton-Jacobi equations, motion of a surface normal to itself, re-initialization to a signed distance function, extrapolation in the normal direction, the particle level set method and the motion of co-dimension two (and higher) objects. Part III is concerned with topics taken from the fields of Image Processing and Computer Vision. These include the restoration of images degraded by noise and blur, image segmentation with active contours (snakes), and reconstruction of surfaces from unorganized data points. Part IV is dedicated to Computational Physics. It begins with one phase compressible fluid dynamics, then two-phase compressible flow involving possibly different equations of state, detonation and deflagration waves, and solid/fluid structure interaction. Next it discusses incompressible fluid dynamics, including a computer graphics simulation of smoke, free surface flows, including a computer graphics simulation of water, and fully two-phase incompressible flow. Additional related topics include incompressible flames with applications to computer graphics and coupling a compressible and incompressible fluid. Finally, heat flow and Stefan problems are discussed. A student or researcher working in mathematics, computer graphics, science, or engineering interested in any dynamic moving front, which might change its topology or develop singularities, will find this book interesting and useful.",
            "referenceCount": 0,
            "citationCount": 5983,
            "influentialCitationCount": 482,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-0-387-22746-7/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2002-10-31",
            "journal": {
                "name": null,
                "volume": "153"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Osher2002LevelSM,\n author = {S. Osher and Ronald Fedkiw},\n booktitle = {Applied Mathematical Sciences},\n pages = {I-XIII, 1-273},\n title = {Level set methods and dynamic implicit surfaces},\n volume = {153},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:712e32e2da67428ba6c6add1605410e1c3792883",
            "@type": "ScholarlyArticle",
            "paperId": "712e32e2da67428ba6c6add1605410e1c3792883",
            "corpusId": 210702798,
            "url": "https://www.semanticscholar.org/paper/712e32e2da67428ba6c6add1605410e1c3792883",
            "title": "Image Segmentation Using Deep Learning: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2001.05566",
                "DBLP": "journals/corr/abs-2001-05566",
                "MAG": "2999607073",
                "DOI": "10.1109/TPAMI.2021.3059968",
                "CorpusId": 210702798,
                "PubMed": "33596172"
            },
            "abstract": "Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of deep learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.",
            "referenceCount": 205,
            "citationCount": 1620,
            "influentialCitationCount": 43,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2001.05566",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-01-15",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Minaee2020ImageSU,\n author = {Shervin Minaee and Yuri Boykov and F. Porikli and A. Plaza and N. Kehtarnavaz and Demetri Terzopoulos},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {3523-3542},\n title = {Image Segmentation Using Deep Learning: A Survey},\n volume = {44},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7580c72df96488927c59a5f31a4b10784adfc749",
            "@type": "ScholarlyArticle",
            "paperId": "7580c72df96488927c59a5f31a4b10784adfc749",
            "corpusId": 49389393,
            "url": "https://www.semanticscholar.org/paper/7580c72df96488927c59a5f31a4b10784adfc749",
            "title": "Convolutional neural networks: an overview and application in radiology",
            "venue": "Insights into Imaging",
            "publicationVenue": {
                "id": "urn:research:4569c7ff-7138-4cef-b44d-21049e1faa98",
                "name": "Insights into Imaging",
                "alternate_names": [
                    "Insight Imaging",
                    "Insights Into Imaging"
                ],
                "issn": "1869-4101",
                "url": "http://www.springer.com/13244"
            },
            "year": 2018,
            "externalIds": {
                "PubMedCentral": "6108980",
                "MAG": "2809254203",
                "DOI": "10.1007/s13244-018-0639-9",
                "CorpusId": 49389393,
                "PubMed": "29934920"
            },
            "abstract": null,
            "referenceCount": 66,
            "citationCount": 2242,
            "influentialCitationCount": 95,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://insightsimaging.springeropen.com/track/pdf/10.1007/s13244-018-0639-9",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-06-22",
            "journal": {
                "name": "Insights into Imaging",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Yamashita2018ConvolutionalNN,\n author = {R. Yamashita and M. Nishio and R. Do and K. Togashi},\n booktitle = {Insights into Imaging},\n journal = {Insights into Imaging},\n pages = {611 - 629},\n title = {Convolutional neural networks: an overview and application in radiology},\n volume = {9},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a42758b4943c7599925e8c8415ee9b8078ff57ad",
            "@type": "ScholarlyArticle",
            "paperId": "a42758b4943c7599925e8c8415ee9b8078ff57ad",
            "corpusId": 1396647,
            "url": "https://www.semanticscholar.org/paper/a42758b4943c7599925e8c8415ee9b8078ff57ad",
            "title": "In Defense of the Triplet Loss for Person Re-Identification",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/HermansBL17",
                "ArXiv": "1703.07737",
                "MAG": "2598634450",
                "CorpusId": 1396647
            },
            "abstract": "In the past few years, the field of computer vision has gone through a revolution fueled mainly by the advent of large datasets and the adoption of deep convolutional neural networks for end-to-end learning. The person re-identification subfield is no exception to this. Unfortunately, a prevailing belief in the community seems to be that the triplet loss is inferior to using surrogate losses (classification, verification) followed by a separate metric learning step. We show that, for models trained from scratch as well as pretrained ones, using a variant of the triplet loss to perform end-to-end deep metric learning outperforms most other published methods by a large margin.",
            "referenceCount": 55,
            "citationCount": 2719,
            "influentialCitationCount": 321,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.07737"
            },
            "citationStyles": {
                "bibtex": "@Article{Hermans2017InDO,\n author = {Alexander Hermans and Lucas Beyer and B. Leibe},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {In Defense of the Triplet Loss for Person Re-Identification},\n volume = {abs/1703.07737},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
            "@type": "ScholarlyArticle",
            "paperId": "ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
            "corpusId": 1277217,
            "url": "https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921",
            "title": "StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/ZhangXLZHWM16",
                "MAG": "2949526397",
                "ArXiv": "1612.03242",
                "DOI": "10.1109/ICCV.2017.629",
                "CorpusId": 1277217
            },
            "abstract": "Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.",
            "referenceCount": 46,
            "citationCount": 2383,
            "influentialCitationCount": 268,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.03242",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-10",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2016StackGANTT,\n author = {Han Zhang and Tao Xu and Hongsheng Li and Shaoting Zhang and Xiaogang Wang and Xiaolei Huang and Dimitris N. Metaxas},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {5908-5916},\n title = {StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "@type": "ScholarlyArticle",
            "paperId": "0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "corpusId": 15195762,
            "url": "https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c",
            "title": "Geometric Deep Learning: Going beyond Euclidean data",
            "venue": "IEEE Signal Processing Magazine",
            "publicationVenue": {
                "id": "urn:research:f62e5eab-173a-4e0a-a963-ed8de9835d22",
                "name": "IEEE Signal Processing Magazine",
                "alternate_names": [
                    "IEEE Signal Process Mag"
                ],
                "issn": "1053-5888",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=79"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/spm/BronsteinBLSV17",
                "MAG": "3102013575",
                "ArXiv": "1611.08097",
                "DOI": "10.1109/MSP.2017.2693418",
                "CorpusId": 15195762
            },
            "abstract": "Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",
            "referenceCount": 125,
            "citationCount": 2754,
            "influentialCitationCount": 155,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.08097",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-24",
            "journal": {
                "name": "IEEE Signal Process. Mag.",
                "volume": "34"
            },
            "citationStyles": {
                "bibtex": "@Article{Bronstein2016GeometricDL,\n author = {M. Bronstein and Joan Bruna and Yann LeCun and Arthur Szlam and P. Vandergheynst},\n booktitle = {IEEE Signal Processing Magazine},\n journal = {IEEE Signal Process. Mag.},\n pages = {18-42},\n title = {Geometric Deep Learning: Going beyond Euclidean data},\n volume = {34},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5d9a3036181676e187c9c0ff995d8bed1db3557d",
            "@type": "ScholarlyArticle",
            "paperId": "5d9a3036181676e187c9c0ff995d8bed1db3557d",
            "corpusId": 7534823,
            "url": "https://www.semanticscholar.org/paper/5d9a3036181676e187c9c0ff995d8bed1db3557d",
            "title": "Adapting Visual Category Models to New Domains",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1722318740",
                "DBLP": "conf/eccv/SaenkoKFD10",
                "DOI": "10.1007/978-3-642-15561-1_16",
                "CorpusId": 7534823
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 2526,
            "influentialCitationCount": 453,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Saenko2010AdaptingVC,\n author = {Kate Saenko and B. Kulis and Mario Fritz and Trevor Darrell},\n booktitle = {European Conference on Computer Vision},\n pages = {213-226},\n title = {Adapting Visual Category Models to New Domains},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e30d9b8ce108d982169621b88a5e3fb69fec70e1",
            "@type": "ScholarlyArticle",
            "paperId": "e30d9b8ce108d982169621b88a5e3fb69fec70e1",
            "corpusId": 2528492,
            "url": "https://www.semanticscholar.org/paper/e30d9b8ce108d982169621b88a5e3fb69fec70e1",
            "title": "Using Deep Learning for Image-Based Plant Disease Detection",
            "venue": "Frontiers in Plant Science",
            "publicationVenue": {
                "id": "urn:research:e110cc75-cd00-4b7f-968c-fd70b464a553",
                "name": "Frontiers in Plant Science",
                "alternate_names": [
                    "Front Plant Sci"
                ],
                "issn": "1664-462X",
                "url": "http://www.frontiersin.org/about/journalseries"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/MohantyHS16",
                "MAG": "2473156356",
                "ArXiv": "1604.03169",
                "PubMedCentral": "5032846",
                "DOI": "10.3389/fpls.2016.01419",
                "CorpusId": 2528492,
                "PubMed": "27713752"
            },
            "abstract": "Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35% on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path toward smartphone-assisted crop disease diagnosis on a massive global scale.",
            "referenceCount": 39,
            "citationCount": 2163,
            "influentialCitationCount": 107,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.frontiersin.org/articles/10.3389/fpls.2016.01419/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Agricultural and Food Sciences",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-04-11",
            "journal": {
                "name": "Frontiers in Plant Science",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Mohanty2016UsingDL,\n author = {S. Mohanty and David P. Hughes and M. Salath\u00e9},\n booktitle = {Frontiers in Plant Science},\n journal = {Frontiers in Plant Science},\n title = {Using Deep Learning for Image-Based Plant Disease Detection},\n volume = {7},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29b3f21ea6b842a920831106e97a2f19d0e9a915",
            "@type": "ScholarlyArticle",
            "paperId": "29b3f21ea6b842a920831106e97a2f19d0e9a915",
            "corpusId": 60778534,
            "url": "https://www.semanticscholar.org/paper/29b3f21ea6b842a920831106e97a2f19d0e9a915",
            "title": "Image Correlation for Shape, Motion and Deformation Measurements: Basic Concepts,Theory and Applications",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "1580750110",
                "DOI": "10.1007/978-0-387-78747-3",
                "CorpusId": 60778534
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 3876,
            "influentialCitationCount": 263,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-0-387-78747-3/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2009-03-26",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Sutton2009ImageCF,\n author = {M. Sutton and J. Orteu and H. Schreier},\n title = {Image Correlation for Shape, Motion and Deformation Measurements: Basic Concepts,Theory and Applications},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ab99aa04e3a8340a7552355fb547374a5604b24",
            "@type": "ScholarlyArticle",
            "paperId": "0ab99aa04e3a8340a7552355fb547374a5604b24",
            "corpusId": 5501470,
            "url": "https://www.semanticscholar.org/paper/0ab99aa04e3a8340a7552355fb547374a5604b24",
            "title": "Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "CorpusId": 5501470
            },
            "abstract": "D EEP learning is a growing trend in general data analysis and has been termed one of the 10 breakthrough technologies of 2013 [1]. Deep learning is an improvement of artificial neural networks, consisting of more layers that permit higher levels of abstraction and improved predictions from data [2]. To date, it is emerging as the leading machine-learning tool in the general imaging and computer vision domains. In particular, convolutional neural networks (CNNs) have proven to be powerful tools for a broad range of computer vision tasks. Deep CNNs automatically learn mid-level and high-level abstractions obtained from raw data (e.g., images). Recent results indicate that the generic descriptors extracted from CNNs are extremely effective in object recognition and localization in natural images. Medical image analysis groups across the world are quickly entering the field and applying CNNs and other deep learning methodologies to a wide variety of applications. Promising results are emerging. In medical imaging, the accurate diagnosis and/or assessment of a disease depends on both image acquisition and image interpretation. Image acquisition has improved substantially over recent years, with devices acquiring data at faster rates and increased resolution. The image interpretation process, however, has only recently begun to benefit from computer technology. Most interpretations of medical images are performed by physicians; however, image interpretation by humans is limited due to its subjectivity, large variations across interpreters, and fatigue. Many diagnostic tasks require an initial search process to detect abnormalities, and to quantify measurements and changes over time. Computerized tools, specifically image analysis and machine learning, are the key enablers to improve diagnosis, by facilitating identification of the findings that require treatment and to support the expert\u2019s workflow. Among these tools, deep learning is rapidly proving to be the state-of-the-art foundation, leading to improved accuracy. It has also opened up new frontiers in data analysis with rates of progress not before experienced.",
            "referenceCount": 36,
            "citationCount": 1223,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{None,\n title = {Deep Learning in Medical Imaging: Overview and Future Promise of an Exciting New Technique},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67711d42b77a13a04822ae00620660cef3abf8c4",
            "@type": "ScholarlyArticle",
            "paperId": "67711d42b77a13a04822ae00620660cef3abf8c4",
            "corpusId": 102496818,
            "url": "https://www.semanticscholar.org/paper/67711d42b77a13a04822ae00620660cef3abf8c4",
            "title": "Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "1905829557",
                "DBLP": "conf/iccv/EigenF15",
                "ArXiv": "1411.4734",
                "DOI": "10.1109/ICCV.2015.304",
                "CorpusId": 102496818
            },
            "abstract": "In this paper we address three different computer vision tasks using a single basic architecture: depth prediction, surface normal estimation, and semantic labeling. We use a multiscale convolutional network that is able to adapt easily to each task using only small modifications, regressing from the input image to the output map directly. Our method progressively refines predictions using a sequence of scales, and captures many image details without any superpixels or low-level segmentation. We achieve state-of-the-art performance on benchmarks for all three tasks.",
            "referenceCount": 54,
            "citationCount": 2420,
            "influentialCitationCount": 277,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.4734",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-11-17",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Eigen2014PredictingDS,\n author = {D. Eigen and R. Fergus},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2650-2658},\n title = {Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a35561e15d3270d63574905fca6b44b92ab5ace0",
            "@type": "ScholarlyArticle",
            "paperId": "a35561e15d3270d63574905fca6b44b92ab5ace0",
            "corpusId": 186689463,
            "url": "https://www.semanticscholar.org/paper/a35561e15d3270d63574905fca6b44b92ab5ace0",
            "title": "Lucas-Kanade 20 Years On: A Unifying Framework",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "29851256",
                "DBLP": "journals/ijcv/BakerM04",
                "DOI": "10.1023/B:VISI.0000011205.11775.fd",
                "CorpusId": 186689463
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 3246,
            "influentialCitationCount": 326,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.cmu.edu/afs/cs/academic/class/15385-s12/www/lec_slides/Baker%26Matthews.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2004-02-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "56"
            },
            "citationStyles": {
                "bibtex": "@Article{Baker2004LucasKanade2Y,\n author = {Simon Baker and Iain Matthews},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {221-255},\n title = {Lucas-Kanade 20 Years On: A Unifying Framework},\n volume = {56},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28ec1fe81dfc6eebe359898ff79960e24876032e",
            "@type": "ScholarlyArticle",
            "paperId": "28ec1fe81dfc6eebe359898ff79960e24876032e",
            "corpusId": 6175091,
            "url": "https://www.semanticscholar.org/paper/28ec1fe81dfc6eebe359898ff79960e24876032e",
            "title": "Snakes, shapes, and gradient vector flow",
            "venue": "IEEE Transactions on Image Processing",
            "publicationVenue": {
                "id": "urn:research:e117fa7f-05b7-4dd6-b64b-0a0a7c0393d8",
                "name": "IEEE Transactions on Image Processing",
                "alternate_names": [
                    "IEEE Trans Image Process"
                ],
                "issn": "1057-7149",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=83"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2145803225",
                "DBLP": "journals/tip/XuP98",
                "DOI": "10.1109/83.661186",
                "CorpusId": 6175091,
                "PubMed": "18276256"
            },
            "abstract": "Snakes, or active contours, are used extensively in computer vision and image processing applications, particularly to locate object boundaries. Problems associated with initialization and poor convergence to boundary concavities, however, have limited their utility. This paper presents a new external force for active contours, largely solving both problems. This external force, which we call gradient vector flow (GVF), is computed as a diffusion of the gradient vectors of a gray-level or binary edge map derived from the image. It differs fundamentally from traditional snake external forces in that it cannot be written as the negative gradient of a potential function, and the corresponding snake is formulated directly from a force balance condition rather than a variational formulation. Using several two-dimensional (2-D) examples and one three-dimensional (3-D) example, we show that GVF has a large capture range and is able to move snakes into boundary concavities.",
            "referenceCount": 22,
            "citationCount": 3591,
            "influentialCitationCount": 278,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://iacl.ece.jhu.edu/~chenyang/research/pubs/tip98.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-03-01",
            "journal": {
                "name": "IEEE transactions on image processing : a publication of the IEEE Signal Processing Society",
                "volume": "7 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu1998SnakesSA,\n author = {Chenyang Xu and Jerry L Prince},\n booktitle = {IEEE Transactions on Image Processing},\n journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},\n pages = {\n          359-69\n        },\n title = {Snakes, shapes, and gradient vector flow},\n volume = {7 3},\n year = {1998}\n}\n"
            }
        }
    }
]