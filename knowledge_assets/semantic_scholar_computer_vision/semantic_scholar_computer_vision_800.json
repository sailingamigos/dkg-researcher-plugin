[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:89bffba9cb9fc3769eccc33c67ba1268c96afe48",
            "@type": "ScholarlyArticle",
            "paperId": "89bffba9cb9fc3769eccc33c67ba1268c96afe48",
            "corpusId": 144046748,
            "url": "https://www.semanticscholar.org/paper/89bffba9cb9fc3769eccc33c67ba1268c96afe48",
            "title": "Foundations of Cyclopean Perception",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1971,
            "externalIds": {
                "MAG": "1997494543",
                "DOI": "10.2307/25294744",
                "CorpusId": 144046748
            },
            "abstract": "This classic work on cyclopean perception has influenced a generation of vision researchers, cognitive scientists, and neuroscientists and has inspired artists, designers, and computer graphics pioneers. In Foundations of Cyclopean Perception (first published in 1971 and unavailable for years), Bela Julesz traced the visual information flow in the brain, analyzing how the brain combines separate images received from the two eyes to produce depth perception. Julesz developed novel tools to do this: random-dot stereograms and cinematograms, generated by early digital computers at Bell Labs. These images, when viewed with the special glasses that came with the book, revealed complex, three-dimensional surfaces; this mode of visual stimulus became a paradigm for research in vision and perception. This reprint edition includes all 48 color random-dot designs from the original, as well as the special 3-D glasses required to view them.Foundations of Cyclopean Perception has had a profound impact on the vision studies community. It was chosen as one of the one hundred most influential works in cognitive science in a poll conducted by the University of Minnesota's Center for Cognitive Sciences. Many copies are \"permanently borrowed\" from college libraries; used copies are sought after online. Now, with this facsimile of the 1971 edition, the book is available again to cognitive scientists, neuroscientists, vision researchers, artists, and designers.",
            "referenceCount": 0,
            "citationCount": 1701,
            "influentialCitationCount": 56,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "History",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "History",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Julesz1971FoundationsOC,\n author = {B. Julesz},\n title = {Foundations of Cyclopean Perception},\n year = {1971}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a937a8061710c5dcc9111f4f150c528c1706b649",
            "@type": "ScholarlyArticle",
            "paperId": "a937a8061710c5dcc9111f4f150c528c1706b649",
            "corpusId": 118642996,
            "url": "https://www.semanticscholar.org/paper/a937a8061710c5dcc9111f4f150c528c1706b649",
            "title": "Learning Shape Templates With Structured Implicit Functions",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2939088803",
                "DBLP": "journals/corr/abs-1904-06447",
                "ArXiv": "1904.06447",
                "DOI": "10.1109/ICCV.2019.00725",
                "CorpusId": 118642996
            },
            "abstract": "Template 3D shapes are useful for many tasks in graphics and vision, including fitting observation data, analyzing shape collections, and transferring shape attributes. Because of the variety of geometry and topology of real-world shapes, previous methods generally use a library of hand-made templates. In this paper, we investigate learning a general shape template from data. To allow for widely varying geometry and topology, we choose an implicit surface representation based on composition of local shape elements. While long known to computer graphics, this representation has not yet been explored in the context of machine learning for vision. We show that structured implicit functions are suitable for learning and allow a network to smoothly and simultaneously fit multiple classes of shapes. The learned shape template supports applications such as shape exploration, correspondence, abstraction, interpolation, and semantic segmentation from an RGB image.",
            "referenceCount": 64,
            "citationCount": 308,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1904.06447",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-12",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Genova2019LearningST,\n author = {Kyle Genova and Forrester Cole and Daniel Vlasic and Aaron Sarna and W. Freeman and T. Funkhouser},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {7153-7163},\n title = {Learning Shape Templates With Structured Implicit Functions},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5fa973b8d284145bf0ced9acf2913a74674260f6",
            "@type": "ScholarlyArticle",
            "paperId": "5fa973b8d284145bf0ced9acf2913a74674260f6",
            "corpusId": 6733279,
            "url": "https://www.semanticscholar.org/paper/5fa973b8d284145bf0ced9acf2913a74674260f6",
            "title": "Yin and Yang: Balancing and Answering Binary Visual Questions",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2273038706",
                "ArXiv": "1511.05099",
                "DBLP": "conf/cvpr/ZhangGSBP16",
                "DOI": "10.1109/CVPR.2016.542",
                "CorpusId": 6733279
            },
            "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is \"yes\", and otherwise \"no\". Abstract scenes play two roles (1) They allow us to focus on the highlevel semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.",
            "referenceCount": 54,
            "citationCount": 299,
            "influentialCitationCount": 31,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.05099",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-16",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2015YinAY,\n author = {Peng Zhang and Yash Goyal and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5014-5022},\n title = {Yin and Yang: Balancing and Answering Binary Visual Questions},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:95498890efa965658d77c118eaec935622634fc0",
            "@type": "ScholarlyArticle",
            "paperId": "95498890efa965658d77c118eaec935622634fc0",
            "corpusId": 695955,
            "url": "https://www.semanticscholar.org/paper/95498890efa965658d77c118eaec935622634fc0",
            "title": "Navier-stokes, fluid dynamics, and image and video inpainting",
            "venue": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/cvpr/BertalmioBS01",
                "MAG": "2165736859",
                "DOI": "10.1109/CVPR.2001.990497",
                "CorpusId": 695955
            },
            "abstract": "Image inpainting involves filling in part of an image or video using information from the surrounding area. Applications include the restoration of damaged photographs and movies and the removal of selected objects. We introduce a class of automated methods for digital inpainting. The approach uses ideas from classical fluid dynamics to propagate isophote lines continuously from the exterior into the region to be inpainted. The main idea is to think of the image intensity as a 'stream function for a two-dimensional incompressible flow. The Laplacian of the image intensity plays the role of the vorticity of the fluid; it is transported into the region to be inpainted by a vector field defined by the stream function. The resulting algorithm is designed to continue isophotes while matching gradient vectors at the boundary of the inpainting region. The method is directly based on the Navier-Stokes equations for fluid dynamics, which has the immediate advantage of well-developed theoretical and numerical results. This is a new approach for introducing ideas from computational fluid dynamics into problems in computer vision and image analysis.",
            "referenceCount": 60,
            "citationCount": 1101,
            "influentialCitationCount": 70,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://conservancy.umn.edu/bitstream/11299/3607/1/1772.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-12-08",
            "journal": {
                "name": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Bertalm\u00edo2001NavierstokesFD,\n author = {M. Bertalm\u00edo and A. Bertozzi and G. Sapiro},\n booktitle = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},\n journal = {Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001},\n pages = {I-I},\n title = {Navier-stokes, fluid dynamics, and image and video inpainting},\n volume = {1},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dcb5cf1e7e8a0afa88c15ecf13267c7a072fb48b",
            "@type": "ScholarlyArticle",
            "paperId": "dcb5cf1e7e8a0afa88c15ecf13267c7a072fb48b",
            "corpusId": 1243387,
            "url": "https://www.semanticscholar.org/paper/dcb5cf1e7e8a0afa88c15ecf13267c7a072fb48b",
            "title": "The brain\u2013computer interface cycle",
            "venue": "Journal of Neural Engineering",
            "publicationVenue": {
                "id": "urn:research:aa06d038-4db2-4d34-a660-be35ff62d392",
                "name": "Journal of Neural Engineering",
                "alternate_names": [
                    "J Neural Eng"
                ],
                "issn": "1741-2552",
                "url": "http://iopscience.iop.org/1741-2552/"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2154777661",
                "DOI": "10.1088/1741-2560/6/4/041001",
                "CorpusId": 1243387,
                "PubMed": "19622847"
            },
            "abstract": "Brain\u2013computer interfaces (BCIs) have attracted much attention recently, triggered by new scientific progress in understanding brain function and by impressive applications. The aim of this review is to give an overview of the various steps in the BCI cycle, i.e., the loop from the measurement of brain activity, classification of data, feedback to the subject and the effect of feedback on brain activity. In this article we will review the critical steps of the BCI cycle, the present issues and state-of-the-art results. Moreover, we will develop a vision on how recently obtained results may contribute to new insights in neurocognition and, in particular, in the neural representation of perceived stimuli, intended actions and emotions. Now is the right time to explore what can be gained by embracing real-time, online BCI and by adding it to the set of experimental tools already available to the cognitive neuroscientist. We close by pointing out some unresolved issues and present our view on how BCI could become an important new tool for probing human cognition.",
            "referenceCount": 123,
            "citationCount": 344,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ris.utwente.nl/ws/files/6544517/jne2009.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics",
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2009-08-06",
            "journal": {
                "name": "Journal of Neural Engineering",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Gerven2009TheBI,\n author = {M. Gerven and J. Farquhar and R. Schaefer and R. Vlek and J. Geuze and A. Nijholt and N. Ramsey and Pim Haselager and L. Vuurpijl and S. Gielen and P. Desain},\n booktitle = {Journal of Neural Engineering},\n journal = {Journal of Neural Engineering},\n pages = {041001},\n title = {The brain\u2013computer interface cycle},\n volume = {6},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2601b6fd6da9630f07ee21e8789afb339c399506",
            "@type": "ScholarlyArticle",
            "paperId": "2601b6fd6da9630f07ee21e8789afb339c399506",
            "corpusId": 32359247,
            "url": "https://www.semanticscholar.org/paper/2601b6fd6da9630f07ee21e8789afb339c399506",
            "title": "Edge Detection",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1985,
            "externalIds": {
                "MAG": "2911321676",
                "CorpusId": 32359247
            },
            "abstract": "The goal of vision is to recover physical properties of objects in a scene, such as the location of object boundaries and the structure, color, and texture of object surfaces, from the two-dimensional image that is projected onto the eye or camera. The first clues about the physical properties of the scene are provided by the {\\it changes of intensity} in the image. The importance of intensity changes and edges in early visual processing has led to extensive research on their detection, description, and use, both in computer and biological vision systems. This article reviews some of the theory that underlies the detection of edges and the methods used to carry out this analysis.",
            "referenceCount": 18,
            "citationCount": 1300,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1985-09-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hildreth1985EdgeD,\n author = {E. Hildreth},\n title = {Edge Detection},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:262cb6f0b9c910792b23aa4c3371652a976c01d1",
            "@type": "ScholarlyArticle",
            "paperId": "262cb6f0b9c910792b23aa4c3371652a976c01d1",
            "corpusId": 2752739,
            "url": "https://www.semanticscholar.org/paper/262cb6f0b9c910792b23aa4c3371652a976c01d1",
            "title": "Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2465108587",
                "DBLP": "conf/cvpr/Jourabloo016",
                "DOI": "10.1109/CVPR.2016.454",
                "CorpusId": 2752739
            },
            "abstract": "Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction. Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results. In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM. We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors. The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.",
            "referenceCount": 38,
            "citationCount": 304,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-27",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jourabloo2016LargePoseFA,\n author = {Amin Jourabloo and Xiaoming Liu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4188-4196},\n title = {Large-Pose Face Alignment via CNN-Based Dense 3D Model Fitting},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e2d7645ddba79b576d3653d00a5ea19a1083515d",
            "@type": "ScholarlyArticle",
            "paperId": "e2d7645ddba79b576d3653d00a5ea19a1083515d",
            "corpusId": 10589867,
            "url": "https://www.semanticscholar.org/paper/e2d7645ddba79b576d3653d00a5ea19a1083515d",
            "title": "Real-time system for monitoring driver vigilance",
            "venue": "Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "journals/tits/BergasaNSBG06",
                "MAG": "2052770734",
                "DOI": "10.1109/tits.2006.869598",
                "CorpusId": 10589867
            },
            "abstract": "In this paper we present a non-intrusive prototype computer vision system for real-time monitoring driver's vigilance. It is based on a hardware system, for real time acquisition of driver's images using an active IR illuminator, and their software implementation for monitoring some visual behaviors that characterize a driver's level of vigilance. These are the eyelid movements and the pose face. The system has been tested with different sequences recorded on night and day driving conditions in a motorway and with different users. We show some experimental results and some conclusions about the performance of the system.",
            "referenceCount": 38,
            "citationCount": 814,
            "influentialCitationCount": 45,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.robesafe.com/personal/bergasa/papers/IEEETITS2006.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2004-06-14",
            "journal": {
                "name": "Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.",
                "volume": "3"
            },
            "citationStyles": {
                "bibtex": "@Article{Bergasa2004RealtimeSF,\n author = {L. Bergasa and J. Nuevo and M. Sotelo and R. Barea and M. E. L. Guill\u00e9n},\n booktitle = {Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.},\n journal = {Proceedings of the IEEE International Symposium on Industrial Electronics, 2005. ISIE 2005.},\n pages = {1303-1308 vol. 3},\n title = {Real-time system for monitoring driver vigilance},\n volume = {3},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d2e9e6991e526a5ec130ea07efab354398faec70",
            "@type": "ScholarlyArticle",
            "paperId": "d2e9e6991e526a5ec130ea07efab354398faec70",
            "corpusId": 209386709,
            "url": "https://www.semanticscholar.org/paper/d2e9e6991e526a5ec130ea07efab354398faec70",
            "title": "Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy",
            "venue": "FAT*",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2996535895",
                "DBLP": "journals/corr/abs-1912-07726",
                "ArXiv": "1912.07726",
                "DOI": "10.1145/3351095.3375709",
                "CorpusId": 209386709
            },
            "abstract": "Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.",
            "referenceCount": 86,
            "citationCount": 228,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3351095.3375709&file=p547-yang-supp.pdf&download=true",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book"
            ],
            "publicationDate": "2019-12-16",
            "journal": {
                "name": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2019TowardsFD,\n author = {Kaiyu Yang and Klint Qinami and Li Fei-Fei and Jia Deng and Olga Russakovsky},\n booktitle = {FAT*},\n journal = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},\n title = {Towards fairer datasets: filtering and balancing the distribution of the people subtree in the ImageNet hierarchy},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ae89592317675c9c7642a3976c3a064cef736f92",
            "@type": "ScholarlyArticle",
            "paperId": "ae89592317675c9c7642a3976c3a064cef736f92",
            "corpusId": 206769405,
            "url": "https://www.semanticscholar.org/paper/ae89592317675c9c7642a3976c3a064cef736f92",
            "title": "Geometric context from a single image",
            "venue": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "conf/iccv/HoiemEH05",
                "MAG": "2155871590",
                "DOI": "10.1109/ICCV.2005.107",
                "CorpusId": 206769405
            },
            "abstract": "Many computer vision algorithms limit their performance by ignoring the underlying 3D geometric structure in the image. We show that we can estimate the coarse geometric properties of a scene by learning appearance-based models of geometric classes, even in cluttered natural scenes. Geometric classes describe the 3D orientation of an image region with respect to the camera. We provide a multiple-hypothesis framework for robustly estimating scene structure from a single image and obtaining confidences for each geometric label. These confidences can then be used to improve the performance of many other applications. We provide a thorough quantitative evaluation of our algorithm on a set of outdoor images and demonstrate its usefulness in two applications: object detection and automatic single-view reconstruction.",
            "referenceCount": 34,
            "citationCount": 783,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Geometric_Context_from_a_Single_Image/6554984/1/files/12037178.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-10-17",
            "journal": {
                "name": "Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Hoiem2005GeometricCF,\n author = {Derek Hoiem and Alexei A. Efros and M. Hebert},\n booktitle = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},\n journal = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},\n pages = {654-661 Vol. 1},\n title = {Geometric context from a single image},\n volume = {1},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79baf8cf6be6510f69be8c515516136138678cf5",
            "@type": "ScholarlyArticle",
            "paperId": "79baf8cf6be6510f69be8c515516136138678cf5",
            "corpusId": 2021646,
            "url": "https://www.semanticscholar.org/paper/79baf8cf6be6510f69be8c515516136138678cf5",
            "title": "The More You Know: Using Knowledge Graphs for Image Classification",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/MarinoSG16",
                "MAG": "2949231412",
                "ArXiv": "1612.04844",
                "DOI": "10.1109/CVPR.2017.10",
                "CorpusId": 2021646
            },
            "abstract": "One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.",
            "referenceCount": 45,
            "citationCount": 312,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.04844",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-14",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Marino2016TheMY,\n author = {Kenneth Marino and R. Salakhutdinov and A. Gupta},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {20-28},\n title = {The More You Know: Using Knowledge Graphs for Image Classification},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59fe0f477f81a8671956b8d1363bdc06ae8b08b3",
            "@type": "ScholarlyArticle",
            "paperId": "59fe0f477f81a8671956b8d1363bdc06ae8b08b3",
            "corpusId": 3716703,
            "url": "https://www.semanticscholar.org/paper/59fe0f477f81a8671956b8d1363bdc06ae8b08b3",
            "title": "Vision-Based Gesture Recognition: A Review",
            "venue": "Gesture Workshop",
            "publicationVenue": {
                "id": "urn:research:ea101aff-8d3b-418c-9620-a20bbc00cec6",
                "name": "Gesture Workshop",
                "alternate_names": [
                    "GW",
                    "Gesture Workshop"
                ],
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "1595932190",
                "DBLP": "conf/gw/WuH99",
                "DOI": "10.1007/3-540-46616-9_10",
                "CorpusId": 3716703
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 648,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1999-03-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu1999VisionBasedGR,\n author = {Ying Wu and Thomas S. Huang},\n booktitle = {Gesture Workshop},\n pages = {103-115},\n title = {Vision-Based Gesture Recognition: A Review},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f1d332de6c042836f036906f60d28f186df59c02",
            "@type": "ScholarlyArticle",
            "paperId": "f1d332de6c042836f036906f60d28f186df59c02",
            "corpusId": 4597042,
            "url": "https://www.semanticscholar.org/paper/f1d332de6c042836f036906f60d28f186df59c02",
            "title": "A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3097980825",
                "DBLP": "journals/corr/abs-1804-01306",
                "ArXiv": "1804.01306",
                "DOI": "10.1109/CVPR.2018.00407",
                "CorpusId": 4597042
            },
            "abstract": "We present a unifying framework to solve several computer vision problems with event cameras: motion, depth and optical flow estimation. The main idea of our framework is to find the point trajectories on the image plane that are best aligned with the event data by maximizing an objective function: the contrast of an image of warped events. Our method implicitly handles data association between the events, and therefore, does not rely on additional appearance information about the scene. In addition to accurately recovering the motion parameters of the problem, our framework produces motion-corrected edge-like images with high dynamic range that can be used for further scene analysis. The proposed method is not only simple, but more importantly, it is, to the best of our knowledge, the first method that can be successfully applied to such a diverse set of important vision tasks with event cameras.",
            "referenceCount": 42,
            "citationCount": 236,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.zora.uzh.ch/id/eprint/176051/1/CVPR18_Gallego.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-04-04",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gallego2018AUC,\n author = {Guillermo Gallego and Henri Rebecq and D. Scaramuzza},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {3867-3876},\n title = {A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:011b9e8787b05772ccf3c24d6946995014181a3a",
            "@type": "ScholarlyArticle",
            "paperId": "011b9e8787b05772ccf3c24d6946995014181a3a",
            "corpusId": 10021879,
            "url": "https://www.semanticscholar.org/paper/011b9e8787b05772ccf3c24d6946995014181a3a",
            "title": "Rotation Averaging",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/ijcv/HartleyTDL13",
                "DOI": "10.1007/s11263-012-0601-0",
                "CorpusId": 10021879
            },
            "abstract": null,
            "referenceCount": 94,
            "citationCount": 477,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "103"
            },
            "citationStyles": {
                "bibtex": "@Article{Hartley2012RotationA,\n author = {R. Hartley and J. Trumpf and Yuchao Dai and Hongdong Li},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {267-305},\n title = {Rotation Averaging},\n volume = {103},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8c30968d96e0c601adaa74db8907fa6ad73bae31",
            "@type": "ScholarlyArticle",
            "paperId": "8c30968d96e0c601adaa74db8907fa6ad73bae31",
            "corpusId": 5776303,
            "url": "https://www.semanticscholar.org/paper/8c30968d96e0c601adaa74db8907fa6ad73bae31",
            "title": "Learning Visual Features from Large Weakly Supervised Data",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2100031962",
                "DBLP": "conf/eccv/JoulinMJV16",
                "ArXiv": "1511.02251",
                "DOI": "10.1007/978-3-319-46478-7_5",
                "CorpusId": 5776303
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 353,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1511.02251",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-11-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.02251"
            },
            "citationStyles": {
                "bibtex": "@Article{Joulin2015LearningVF,\n author = {Armand Joulin and L. Maaten and A. Jabri and Nicolas Vasilache},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Learning Visual Features from Large Weakly Supervised Data},\n volume = {abs/1511.02251},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d385429e7d8e0552824366d1f70d13781ef789f",
            "@type": "ScholarlyArticle",
            "paperId": "1d385429e7d8e0552824366d1f70d13781ef789f",
            "corpusId": 16611295,
            "url": "https://www.semanticscholar.org/paper/1d385429e7d8e0552824366d1f70d13781ef789f",
            "title": "Discriminative clustering for image co-segmentation",
            "venue": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2010,
            "externalIds": {
                "MAG": "2086052791",
                "DBLP": "conf/cvpr/JoulinBP10",
                "DOI": "10.1109/CVPR.2010.5539868",
                "CorpusId": 16611295
            },
            "abstract": "Purely bottom-up, unsupervised segmentation of a single image into foreground and background regions remains a challenging task for computer vision. Co-segmentation is the problem of simultaneously dividing multiple images into regions (segments) corresponding to different object classes. In this paper, we combine existing tools for bottom-up image segmentation such as normalized cuts, with kernel methods commonly used in object recognition. These two sets of techniques are used within a discriminative clustering framework: the goal is to assign foreground/background labels jointly to all images, so that a supervised classifier trained with these labels leads to maximal separation of the two classes. In practice, we obtain a combinatorial optimization problem which is relaxed to a continuous convex optimization problem, that can itself be solved efficiently for up to dozens of images. We illustrate the proposed method on images with very similar foreground objects, as well as on more challenging problems with objects with higher intra-class variations.",
            "referenceCount": 28,
            "citationCount": 484,
            "influentialCitationCount": 75,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.di.ens.fr/%7Efbach/cosegmentation_cvpr2010.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-06-13",
            "journal": {
                "name": "2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Joulin2010DiscriminativeCF,\n author = {Armand Joulin and F. Bach and J. Ponce},\n booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},\n pages = {1943-1950},\n title = {Discriminative clustering for image co-segmentation},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "@type": "ScholarlyArticle",
            "paperId": "0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "corpusId": 14911813,
            "url": "https://www.semanticscholar.org/paper/0d307221fa52e3939d46180cb5921ebbd92c8adb",
            "title": "Word Spotting in the Wild",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1521064364",
                "DBLP": "conf/eccv/WangB10",
                "DOI": "10.1007/978-3-642-15549-9_43",
                "CorpusId": 14911813
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 468,
            "influentialCitationCount": 68,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2010WordSI,\n author = {Kai Wang and Serge J. Belongie},\n booktitle = {European Conference on Computer Vision},\n pages = {591-604},\n title = {Word Spotting in the Wild},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:56178de1001efe54792ad93f6980de5d5e91906b",
            "@type": "ScholarlyArticle",
            "paperId": "56178de1001efe54792ad93f6980de5d5e91906b",
            "corpusId": 14622215,
            "url": "https://www.semanticscholar.org/paper/56178de1001efe54792ad93f6980de5d5e91906b",
            "title": "Metrics for 3D Rotations: Comparison and Analysis",
            "venue": "Journal of Mathematical Imaging and Vision",
            "publicationVenue": {
                "id": "urn:research:5da99f9f-4568-4d95-b707-0e1ff266bf8f",
                "name": "Journal of Mathematical Imaging and Vision",
                "alternate_names": [
                    "J Math Imaging Vis"
                ],
                "issn": "0924-9907",
                "url": "http://www.springer.com/computer/computer+imaging/journal/10851"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "journals/jmiv/Huynh09",
                "MAG": "2016959837",
                "DOI": "10.1007/s10851-009-0161-2",
                "CorpusId": 14622215
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 580,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-10-01",
            "journal": {
                "name": "Journal of Mathematical Imaging and Vision",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Huynh2009MetricsF3,\n author = {D. Huynh},\n booktitle = {Journal of Mathematical Imaging and Vision},\n journal = {Journal of Mathematical Imaging and Vision},\n pages = {155-164},\n title = {Metrics for 3D Rotations: Comparison and Analysis},\n volume = {35},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c2707f677cef1883e44141efdda643c86a20586",
            "@type": "ScholarlyArticle",
            "paperId": "1c2707f677cef1883e44141efdda643c86a20586",
            "corpusId": 17485166,
            "url": "https://www.semanticscholar.org/paper/1c2707f677cef1883e44141efdda643c86a20586",
            "title": "Diagonal preconditioning for first order primal-dual algorithms in convex optimization",
            "venue": "Vision",
            "publicationVenue": {
                "id": "urn:research:4144b5fb-0a80-4663-8ebf-80ca0c47231a",
                "name": "Vision",
                "alternate_names": [
                    "International Conference on Computer Vision",
                    "Int Conf Comput Vis",
                    "VISION"
                ],
                "issn": "0917-1142",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-1000285"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "conf/iccv/PockC11",
                "MAG": "2158449358",
                "DOI": "10.1109/ICCV.2011.6126441",
                "CorpusId": 17485166
            },
            "abstract": "In this paper we study preconditioning techniques for the first-order primal-dual algorithm proposed in [5]. In particular, we propose simple and easy to compute diagonal preconditioners for which convergence of the algorithm is guaranteed without the need to compute any step size parameters. As a by-product, we show that for a certain instance of the preconditioning, the proposed algorithm is equivalent to the old and widely unknown alternating step method for monotropic programming [7]. We show numerical results on general linear programming problems and a few standard computer vision problems. In all examples, the preconditioned algorithm significantly outperforms the algorithm of [5].",
            "referenceCount": 16,
            "citationCount": 461,
            "influentialCitationCount": 61,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-11-06",
            "journal": {
                "name": "2011 International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pock2011DiagonalPF,\n author = {T. Pock and A. Chambolle},\n booktitle = {Vision},\n journal = {2011 International Conference on Computer Vision},\n pages = {1762-1769},\n title = {Diagonal preconditioning for first order primal-dual algorithms in convex optimization},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:57de1a6cc6035d4ac6305121367ad3c6d3fd5104",
            "@type": "ScholarlyArticle",
            "paperId": "57de1a6cc6035d4ac6305121367ad3c6d3fd5104",
            "corpusId": 146121106,
            "url": "https://www.semanticscholar.org/paper/57de1a6cc6035d4ac6305121367ad3c6d3fd5104",
            "title": "WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1905.01489",
                "MAG": "3004734937",
                "DBLP": "conf/iccv/YogamaniWRNMVPO19",
                "DOI": "10.1109/ICCV.2019.00940",
                "CorpusId": 146121106
            },
            "abstract": "Fisheye cameras are commonly employed for obtaining a large field of view in surveillance, augmented reality and in particular automotive applications. In spite of their prevalence, there are few public datasets for detailed evaluation of computer vision algorithms on fisheye images. We release the first extensive fisheye automotive dataset, WoodScape, named after Robert Wood who invented the fisheye camera in 1906. WoodScape comprises of four surround view cameras and nine tasks including segmentation, depth estimation, 3D bounding box detection and soiling detection. Semantic annotation of 40 classes at the instance level is provided for over 10,000 images and annotation for other tasks are provided for over 100,000 images. With WoodScape, we would like to encourage the community to adapt computer vision models for fisheye camera instead of using naive rectification.",
            "referenceCount": 67,
            "citationCount": 177,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1905.01489",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-05-04",
            "journal": {
                "name": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yogamani2019WoodScapeAM,\n author = {S. Yogamani and Ciar\u00e1n Hughes and J. Horgan and Ganesh Sistu and P. Varley and D. O'Dea and Michal U\u0159i\u010d\u00e1\u0159 and Stefan Milz and Martin Simon and Karl Amende and Christian Witt and Hazem Rashed and Sumanth Chennupati and Sanjaya Nayak and Saquib Mansoor and Xavier Perroton and P. P\u00e9rez},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {9307-9317},\n title = {WoodScape: A Multi-Task, Multi-Camera Fisheye Dataset for Autonomous Driving},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c51ca3b2bd924d689713862d0fdbaafa5b24e5b",
            "@type": "ScholarlyArticle",
            "paperId": "4c51ca3b2bd924d689713862d0fdbaafa5b24e5b",
            "corpusId": 51768777,
            "url": "https://www.semanticscholar.org/paper/4c51ca3b2bd924d689713862d0fdbaafa5b24e5b",
            "title": "Monocular Model-Based 3D Tracking of Rigid Objects: A Survey",
            "venue": "Foundations and Trends in Computer Graphics and Vision",
            "publicationVenue": {
                "id": "urn:research:a21f6aaa-21ef-418d-b2dd-e56ce6e16570",
                "name": "Foundations and Trends in Computer Graphics and Vision",
                "alternate_names": [
                    "Found Trends Comput Graph Vis"
                ],
                "issn": "1572-2740",
                "url": "https://www.nowpublishers.com/cgv"
            },
            "year": 2005,
            "externalIds": {
                "DBLP": "journals/ftcgv/LepetitF05",
                "MAG": "2169102162",
                "DOI": "10.1561/0600000001",
                "CorpusId": 51768777
            },
            "abstract": "Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Augmented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated interfaces can use. Computer Vision offers solutions that are cheap, practical and non-invasive.This survey reviews the different techniques and approaches that have been developed by industry and research. First, important mathematical tools are introduced: Camera representation, robust estimation and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery concludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking.Because it encompasses many computer vision techniques from low-level vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system.",
            "referenceCount": 144,
            "citationCount": 743,
            "influentialCitationCount": 38,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://infoscience.epfl.ch/record/64661/files/top.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2005-08-31",
            "journal": {
                "name": "Found. Trends Comput. Graph. Vis.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Lepetit2005MonocularM3,\n author = {V. Lepetit and P. Fua},\n booktitle = {Foundations and Trends in Computer Graphics and Vision},\n journal = {Found. Trends Comput. Graph. Vis.},\n title = {Monocular Model-Based 3D Tracking of Rigid Objects: A Survey},\n volume = {1},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d6c1e14e8bea932f821352ea9e33928129f7d065",
            "@type": "ScholarlyArticle",
            "paperId": "d6c1e14e8bea932f821352ea9e33928129f7d065",
            "corpusId": 52088075,
            "url": "https://www.semanticscholar.org/paper/d6c1e14e8bea932f821352ea9e33928129f7d065",
            "title": "Deep Learning of Graph Matching",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2799132636",
                "DBLP": "conf/cvpr/ZanfirS18",
                "DOI": "10.1109/CVPR.2018.00284",
                "CorpusId": 52088075
            },
            "abstract": "The problem of graph matching under node and pairwise constraints is fundamental in areas as diverse as combinatorial optimization, machine learning or computer vision, where representing both the relations between nodes and their neighborhood structure is essential. We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and pairwise node neighborhoods, represented as deep feature extraction hierarchies. The challenge is in the formulation of the different matrix computation layers of the model in a way that enables the consistent, efficient propagation of gradients in the complete pipeline from the loss function, through the combinatorial optimization layer solving the matching problem, and the feature extraction hierarchy. Our computer vision experiments and ablation studies on challenging datasets like PASCAL VOC keypoints, Sintel and CUB show that matching models refined end-to-end are superior to counterparts based on feature hierarchies trained for other problems.",
            "referenceCount": 40,
            "citationCount": 198,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zanfir2018DeepLO,\n author = {Andrei Zanfir and C. Sminchisescu},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {2684-2693},\n title = {Deep Learning of Graph Matching},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:177793b5b3f7d8610b159e7da4a85013db60acdd",
            "@type": "ScholarlyArticle",
            "paperId": "177793b5b3f7d8610b159e7da4a85013db60acdd",
            "corpusId": 18172770,
            "url": "https://www.semanticscholar.org/paper/177793b5b3f7d8610b159e7da4a85013db60acdd",
            "title": "Artificial vision for the blind by connecting a television camera to the visual cortex.",
            "venue": "ASAIO journal (1992)",
            "publicationVenue": {
                "id": "urn:research:bc295d74-fcba-4d6f-b927-b17b29ee047b",
                "name": "ASAIO journal (1992)",
                "alternate_names": [
                    "Asaio Journal",
                    "ASAIO j (1992",
                    "Asaio J"
                ],
                "issn": "1058-2916",
                "url": "http://www.asaiojournal.com/"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "1982250766",
                "DOI": "10.1097/00002480-200001000-00002",
                "CorpusId": 18172770,
                "PubMed": "10667705"
            },
            "abstract": "Blindness is more feared by the public than any ailment with the exception of cancer and AIDS. We report the development of the first visual prosthesis providing useful \"artificial vision\" to a blind volunteer by connecting a digital video camera, computer, and associated electronics to the visual cortex of his brain. This device has been the objective of a development effort begun by our group in 1968 and represents realization of the prediction of an artificial vision system made by Benjamin Franklin in his report on the \"kite and key\" experiment, with which he discovered electricity in 1751.",
            "referenceCount": 16,
            "citationCount": 562,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "ASAIO journal",
                "volume": "46 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Dobelle2000ArtificialVF,\n author = {W. H. Dobelle},\n booktitle = {ASAIO journal (1992)},\n journal = {ASAIO journal},\n pages = {\n          3-9\n        },\n title = {Artificial vision for the blind by connecting a television camera to the visual cortex.},\n volume = {46 1},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9c11b1552748c697bbfd33c157f7b7875686eb10",
            "@type": "ScholarlyArticle",
            "paperId": "9c11b1552748c697bbfd33c157f7b7875686eb10",
            "corpusId": 4012493,
            "url": "https://www.semanticscholar.org/paper/9c11b1552748c697bbfd33c157f7b7875686eb10",
            "title": "Application of deep learning in object detection",
            "venue": "International Conference on Interaction Sciences",
            "publicationVenue": {
                "id": "urn:research:60c45ccc-5528-42eb-85a9-7606a3d9b220",
                "name": "International Conference on Interaction Sciences",
                "alternate_names": [
                    "Annual ACIS International Conference on Computer and Information Science",
                    "IEEE/ACIS International Conference on Computer and Information Science",
                    "International Conference on Intelligent Systems",
                    "Annu ACIS Int Conf Comput Inf Sci",
                    "International Conference on Information Systems",
                    "Int Conf Interact Sci",
                    "ICIS",
                    "Int Conf Internet Comput Inf Serv",
                    "Int Conf Intell Syst",
                    "IEEE/ACIS Int Conf Comput Inf Sci",
                    "Int Conf Inf Syst",
                    "International Conference on Internet Computing and Information Services"
                ],
                "issn": null,
                "url": "https://web.archive.org/web/*/http://www.aicit.org/icis/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/ACISicis/ZhouGFD17",
                "MAG": "2727887115",
                "DOI": "10.1109/ICIS.2017.7960069",
                "CorpusId": 4012493
            },
            "abstract": "This paper deals with the field of computer vision, mainly for the application of deep learning in object detection task. On the one hand, there is a simple summary of the datasets and deep learning algorithms commonly used in computer vision. On the other hand, a new dataset is built according to those commonly used datasets, and choose one of the network called faster r-cnn to work on this new dataset. Through the experiment to strengthen the understanding of these networks, and through the analysis of the results learn the importance of deep learning technology, and the importance of the dataset for deep learning.",
            "referenceCount": 13,
            "citationCount": 281,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-24",
            "journal": {
                "name": "2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhou2017ApplicationOD,\n author = {Xinyi Zhou and Wei Gong and W. Fu and Fengtong Du},\n booktitle = {International Conference on Interaction Sciences},\n journal = {2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)},\n pages = {631-634},\n title = {Application of deep learning in object detection},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:72b8d4907191f0b86eb525a84eb5b1c6ce39d14b",
            "@type": "ScholarlyArticle",
            "paperId": "72b8d4907191f0b86eb525a84eb5b1c6ce39d14b",
            "corpusId": 6196948,
            "url": "https://www.semanticscholar.org/paper/72b8d4907191f0b86eb525a84eb5b1c6ce39d14b",
            "title": "A Comparative Analysis of RANSAC Techniques Leading to Adaptive Real-Time Random Sample Consensus",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "conf/eccv/RaguramFP08",
                "MAG": "2168104347",
                "DOI": "10.1007/978-3-540-88688-4_37",
                "CorpusId": 6196948
            },
            "abstract": null,
            "referenceCount": 13,
            "citationCount": 531,
            "influentialCitationCount": 44,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.unc.edu/~rraguram/papers/eccv2008.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2008-10-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Raguram2008ACA,\n author = {R. Raguram and Jan-Michael Frahm and M. Pollefeys},\n booktitle = {European Conference on Computer Vision},\n pages = {500-513},\n title = {A Comparative Analysis of RANSAC Techniques Leading to Adaptive Real-Time Random Sample Consensus},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fff3876663581b51273cde3f6151960b75a0e12f",
            "@type": "ScholarlyArticle",
            "paperId": "fff3876663581b51273cde3f6151960b75a0e12f",
            "corpusId": 207158556,
            "url": "https://www.semanticscholar.org/paper/fff3876663581b51273cde3f6151960b75a0e12f",
            "title": "Peekaboom: a game for locating objects in images",
            "venue": "International Conference on Human Factors in Computing Systems",
            "publicationVenue": {
                "id": "urn:research:b55b50b1-aae7-47a7-b042-8aecc930073d",
                "name": "International Conference on Human Factors in Computing Systems",
                "alternate_names": [
                    "CHI",
                    "Int Conf Hum Factor Comput Syst",
                    "Human Factors in Computing Systems",
                    "Conference on Human Interface",
                    "Conf Hum Interface",
                    "Hum Factor Comput Syst"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigchi/"
            },
            "year": 2006,
            "externalIds": {
                "DBLP": "conf/chi/AhnLB06",
                "MAG": "2080942732",
                "DOI": "10.1145/1124772.1124782",
                "CorpusId": 207158556
            },
            "abstract": "We introduce Peekaboom, an entertaining web-based game that can help computers locate objects in images. People play the game because of its entertainment value, and as a side effect of them playing, we collect valuable image metadata, such as which pixels belong to which object in the image. The collected data could be applied towards constructing more accurate computer vision algorithms, which require massive amounts of training and testing data not currently available. Peekaboom has been played by thousands of people, some of whom have spent over 12 hours a day playing, and thus far has generated millions of data points. In addition to its purely utilitarian aspect, Peekaboom is an example of a new, emerging class of games, which not only bring people together for leisure purposes, but also exist to improve artificial intelligence. Such games appeal to a general audience, while providing answers to problems that computers cannot yet solve.",
            "referenceCount": 15,
            "citationCount": 642,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-04-22",
            "journal": {
                "name": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Ahn2006PeekaboomAG,\n author = {Luis von Ahn and Ruoran Liu and M. Blum},\n booktitle = {International Conference on Human Factors in Computing Systems},\n journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n title = {Peekaboom: a game for locating objects in images},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ce9643fbfcb9c3156c7b26b5c92ec3bc67111202",
            "@type": "ScholarlyArticle",
            "paperId": "ce9643fbfcb9c3156c7b26b5c92ec3bc67111202",
            "corpusId": 16525678,
            "url": "https://www.semanticscholar.org/paper/ce9643fbfcb9c3156c7b26b5c92ec3bc67111202",
            "title": "Intelligent distributed surveillance systems: a review",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2124111503",
                "DOI": "10.1049/IP-VIS:20041147",
                "CorpusId": 16525678
            },
            "abstract": "This survey describes the current state-of-the-art in the development of automated visual surveillance systems so as to provide researchers in the field with a summary of progress achieved to date and to identify areas where further research is needed. The ability to recognise objects and humans, to describe their actions and interactions from information acquired by sensors is essential for automated visual surveillance. The increasing need for intelligent visual surveillance in commercial, law enforcement and military applications makes automated visual surveillance systems one of the main current application domains in computer vision. The emphasis of this review is on discussion of the creation of intelligent distributed automated surveillance systems. The survey concludes with a discussion of possible future directions.",
            "referenceCount": 81,
            "citationCount": 770,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2005-04-08",
            "journal": {
                "name": "",
                "volume": "152"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Valera2005IntelligentDS,\n author = {M. Valera and S. Velast\u00edn},\n pages = {192-204},\n title = {Intelligent distributed surveillance systems: a review},\n volume = {152},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7238200341f0fc27cadf07a00046a994fe89f6e4",
            "@type": "ScholarlyArticle",
            "paperId": "7238200341f0fc27cadf07a00046a994fe89f6e4",
            "corpusId": 202750227,
            "url": "https://www.semanticscholar.org/paper/7238200341f0fc27cadf07a00046a994fe89f6e4",
            "title": "Synthetic Data for Deep Learning",
            "venue": "Springer Optimization and Its Applications",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1909.11512",
                "MAG": "2975317124",
                "DBLP": "journals/corr/abs-1909-11512",
                "DOI": "10.1007/978-3-030-75178-4",
                "CorpusId": 202750227
            },
            "abstract": null,
            "referenceCount": 706,
            "citationCount": 203,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/bfm:978-3-030-75178-4/1?pdf=chapter%20toc",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Review"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": "Synthetic Data for Deep Learning",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nikolenko2019SyntheticDF,\n author = {S. Nikolenko},\n booktitle = {Springer Optimization and Its Applications},\n journal = {Synthetic Data for Deep Learning},\n title = {Synthetic Data for Deep Learning},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10478ed2892f24c49ca0be1588c1c0e29841abb1",
            "@type": "ScholarlyArticle",
            "paperId": "10478ed2892f24c49ca0be1588c1c0e29841abb1",
            "corpusId": 722896,
            "url": "https://www.semanticscholar.org/paper/10478ed2892f24c49ca0be1588c1c0e29841abb1",
            "title": "Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2167366427",
                "DBLP": "conf/iccv/FangXR13",
                "DOI": "10.1109/ICCV.2013.208",
                "CorpusId": 722896
            },
            "abstract": "Many standard computer vision datasets exhibit biases due to a variety of sources including illumination condition, imaging system, and preference of dataset collectors. Biases like these can have downstream effects in the use of vision datasets in the construction of generalizable techniques, especially for the goal of the creation of a classification system capable of generalizing to unseen and novel datasets. In this work we propose Unbiased Metric Learning (UML), a metric learning approach, to achieve this goal. UML operates in the following two steps: (1) By varying hyper parameters, it learns a set of less biased candidate distance metrics on training examples from multiple biased datasets. The key idea is to learn a neighborhood for each example, which consists of not only examples of the same category from the same dataset, but those from other datasets. The learning framework is based on structural SVM. (2) We do model validation on a set of weakly-labeled web images retrieved by issuing class labels as keywords to search engine. The metric with best validation performance is selected. Although the web images sometimes have noisy labels, they often tend to be less biased, which makes them suitable for the validation set in our task. Cross-dataset image classification experiments are carried out. Results show significant performance improvement on four well-known computer vision datasets.",
            "referenceCount": 27,
            "citationCount": 271,
            "influentialCitationCount": 94,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-01",
            "journal": {
                "name": "2013 IEEE International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fang2013UnbiasedML,\n author = {Chen Fang and Ye Xu and D. Rockmore},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2013 IEEE International Conference on Computer Vision},\n pages = {1657-1664},\n title = {Unbiased Metric Learning: On the Utilization of Multiple Datasets and Web Images for Softening Bias},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:14686cbab601311da52323656ed17f05cb03d982",
            "@type": "ScholarlyArticle",
            "paperId": "14686cbab601311da52323656ed17f05cb03d982",
            "corpusId": 12666501,
            "url": "https://www.semanticscholar.org/paper/14686cbab601311da52323656ed17f05cb03d982",
            "title": "Real-Time Correlation-Based Stereo Vision with Reduced Border Errors",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2002,
            "externalIds": {
                "DBLP": "journals/ijcv/HirschmullerIG02",
                "MAG": "1536827095",
                "DOI": "10.1023/A:1014554110407",
                "CorpusId": 12666501
            },
            "abstract": null,
            "referenceCount": 19,
            "citationCount": 525,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-04-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "47"
            },
            "citationStyles": {
                "bibtex": "@Article{Hirschm\u00fcller2002RealTimeCS,\n author = {H. Hirschm\u00fcller and P. Innocent and J. Garibaldi},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {229-246},\n title = {Real-Time Correlation-Based Stereo Vision with Reduced Border Errors},\n volume = {47},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8560e9c39c50ea928eef7c115d99fdae5acfdfa3",
            "@type": "ScholarlyArticle",
            "paperId": "8560e9c39c50ea928eef7c115d99fdae5acfdfa3",
            "corpusId": 14648882,
            "url": "https://www.semanticscholar.org/paper/8560e9c39c50ea928eef7c115d99fdae5acfdfa3",
            "title": "Three-dimensional object recognition",
            "venue": "CSUR",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1985,
            "externalIds": {
                "DBLP": "journals/csur/BeslJ85",
                "MAG": "1996773532",
                "DOI": "10.1145/4078.4081",
                "CorpusId": 14648882
            },
            "abstract": "A general-purpose computer vision system must be capable of recognizing three-dimensional (3-D) objects. This paper proposes a precise definition of the 3-D object recognition problem, discusses basic concepts associated with this problem, and reviews the relevant literature. Because range images (or depth maps) are often used as sensor input instead of intensity images, techniques for obtaining, processing, and characterizing range data are also surveyed.",
            "referenceCount": 192,
            "citationCount": 1172,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1985-03-01",
            "journal": {
                "name": "ACM Comput. Surv.",
                "volume": "17"
            },
            "citationStyles": {
                "bibtex": "@Article{Besl1985ThreedimensionalOR,\n author = {P. Besl and R. Jain},\n booktitle = {CSUR},\n journal = {ACM Comput. Surv.},\n pages = {75-145},\n title = {Three-dimensional object recognition},\n volume = {17},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:268d83cf13d30e747fb1028a4d847339bc00b8a9",
            "@type": "ScholarlyArticle",
            "paperId": "268d83cf13d30e747fb1028a4d847339bc00b8a9",
            "corpusId": 34936561,
            "url": "https://www.semanticscholar.org/paper/268d83cf13d30e747fb1028a4d847339bc00b8a9",
            "title": "Visual tracking of a moving target by a camera mounted on a robot: a combination of control and vision",
            "venue": "IEEE Trans. Robotics Autom.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1993,
            "externalIds": {
                "MAG": "2125858574",
                "DBLP": "journals/trob/PapanikolopoulosKK93",
                "DOI": "10.1109/70.210792",
                "CorpusId": 34936561
            },
            "abstract": "The authors present algorithms for robotic (eye-in-hand configuration) real-time visual tracking of arbitrary 3D objects traveling at unknown velocities in a 2D space (depth is given as known). Visual tracking is formulated as a problem of combining control with computer vision. A mathematical formulation of the control problem that includes information from a novel feedback vision sensor and represents everything with respect to the camera frame is presented. The sum-of-squared differences (SSD) optical flow is used to compute the vector of discrete displacements each instant of time. These displacements can be fed either directly to a PI (proportional-integral) controller or to a pole assignment controller or discrete steady-state Kalman filter. In the latter case, the Kalman filter calculates the estimated values of the system's states and the exogenous disturbances, and a discrete LQG (linear-quadratic Gaussian) controller computes the desired motion of the robotic system. The outputs of the controllers are sent to the Cartesian robotic controller. Performance results are presented. >",
            "referenceCount": 30,
            "citationCount": 634,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1993-02-01",
            "journal": {
                "name": "IEEE Trans. Robotics Autom.",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Papanikolopoulos1993VisualTO,\n author = {N. Papanikolopoulos and P. Khosla and T. Kanade},\n booktitle = {IEEE Trans. Robotics Autom.},\n journal = {IEEE Trans. Robotics Autom.},\n pages = {14-35},\n title = {Visual tracking of a moving target by a camera mounted on a robot: a combination of control and vision},\n volume = {9},\n year = {1993}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d0bfd3cb732471a0843a39d2d047caf60a844466",
            "@type": "ScholarlyArticle",
            "paperId": "d0bfd3cb732471a0843a39d2d047caf60a844466",
            "corpusId": 71148268,
            "url": "https://www.semanticscholar.org/paper/d0bfd3cb732471a0843a39d2d047caf60a844466",
            "title": "RAVEN: A Dataset for Relational and Analogical Visual REasoNing",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1903.02741",
                "MAG": "2953612685",
                "DBLP": "journals/corr/abs-1903-02741",
                "DOI": "10.1109/CVPR.2019.00546",
                "CorpusId": 71148268
            },
            "abstract": "Dramatic progress has been witnessed in basic vision tasks involving low-level perception, such as object recognition, detection, and tracking. Unfortunately, there is still enormous performance gap between artificial vision systems and human intelligence in terms of higher-level vision problems, especially ones involving reasoning. Earlier attempts in equipping machines with high-level reasoning have hovered around Visual Question Answering (VQA), one typical task associating vision and language understanding. In this work, we propose a new dataset, built in the context of Raven's Progressive Matrices (RPM) and aimed at lifting machine intelligence by associating vision with structural, relational, and analogical reasoning in a hierarchical representation. Unlike previous works in measuring abstract reasoning using RPM, we establish a semantic link between vision and reasoning by providing structure representation. This addition enables a new type of abstract reasoning by jointly operating on the structure representation. Machine reasoning ability using modern computer vision is evaluated in this newly proposed dataset. Additionally, we also provide human performance as a reference. Finally, we show consistent improvement across all models by incorporating a simple neural module that combines visual understanding and structure reasoning.",
            "referenceCount": 63,
            "citationCount": 183,
            "influentialCitationCount": 52,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1903.02741",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-07",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2019RAVENAD,\n author = {Chi Zhang and Feng Gao and Baoxiong Jia and Yixin Zhu and Song-Chun Zhu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5312-5322},\n title = {RAVEN: A Dataset for Relational and Analogical Visual REasoNing},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd3219fb608ea4ef5103c115e0afd308f851d89a",
            "@type": "ScholarlyArticle",
            "paperId": "cd3219fb608ea4ef5103c115e0afd308f851d89a",
            "corpusId": 16647912,
            "url": "https://www.semanticscholar.org/paper/cd3219fb608ea4ef5103c115e0afd308f851d89a",
            "title": "Visual Recognition with Humans in the Loop",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "conf/eccv/BransonWSBWPB10",
                "MAG": "2103490241",
                "DOI": "10.1007/978-3-642-15561-1_32",
                "CorpusId": 16647912
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 470,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://authors.library.caltech.edu/94220/2/Visipedia20q.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Branson2010VisualRW,\n author = {Steve Branson and C. Wah and Florian Schroff and Boris Babenko and P. Welinder and P. Perona and Serge J. Belongie},\n booktitle = {European Conference on Computer Vision},\n pages = {438-451},\n title = {Visual Recognition with Humans in the Loop},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:26a7c5cd92c018f8348c1424e10042811ec15148",
            "@type": "ScholarlyArticle",
            "paperId": "26a7c5cd92c018f8348c1424e10042811ec15148",
            "corpusId": 78184,
            "url": "https://www.semanticscholar.org/paper/26a7c5cd92c018f8348c1424e10042811ec15148",
            "title": "SD-VBS: The San Diego Vision Benchmark Suite",
            "venue": "IEEE International Symposium on Workload Characterization",
            "publicationVenue": {
                "id": "urn:research:36e4bc17-266a-4715-9b27-2be27e1e994e",
                "name": "IEEE International Symposium on Workload Characterization",
                "alternate_names": [
                    "IISWC",
                    "IEEE Int Symp Workload Charact"
                ],
                "issn": null,
                "url": "http://www.iiswc.org/"
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/iiswc/VenkataAJGLGBT09",
                "MAG": "2132511032",
                "DOI": "10.1109/IISWC.2009.5306794",
                "CorpusId": 78184
            },
            "abstract": "In the era of multi-core, computer vision has emerged as an exciting application area which promises to continue to drive the demand for both more powerful and more energy efficient processors. Although there is still a long way to go, vision has matured significantly over the last few decades, and the list of applications that are useful to end users continues to grow. The parallelism inherent in vision applications makes them a promising workload for multi-core and many-core processors.",
            "referenceCount": 33,
            "citationCount": 208,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-10-01",
            "journal": {
                "name": "2009 IEEE International Symposium on Workload Characterization (IISWC)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Venkata2009SDVBSTS,\n author = {Sravanthi Kota Venkata and Ikkjin Ahn and Donghwan Jeon and Anshuman Gupta and Christopher M. Louie and Saturnino Garcia and Serge J. Belongie and M. Taylor},\n booktitle = {IEEE International Symposium on Workload Characterization},\n journal = {2009 IEEE International Symposium on Workload Characterization (IISWC)},\n pages = {55-64},\n title = {SD-VBS: The San Diego Vision Benchmark Suite},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fda48e021f7c1445406fa7db32a443726e0ef8d3",
            "@type": "ScholarlyArticle",
            "paperId": "fda48e021f7c1445406fa7db32a443726e0ef8d3",
            "corpusId": 5034184,
            "url": "https://www.semanticscholar.org/paper/fda48e021f7c1445406fa7db32a443726e0ef8d3",
            "title": "State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications",
            "venue": "Computer graphics forum (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/cgf/ZollhoferTGBBPS18",
                "MAG": "2806379360",
                "DOI": "10.1111/cgf.13382",
                "CorpusId": 5034184
            },
            "abstract": "The computer graphics and vision communities have dedicated long standing efforts in building computerized tools for reconstructing, tracking, and analyzing human faces based on visual input. Over the past years rapid progress has been made, which led to novel and powerful algorithms that obtain impressive results even in the very challenging case of reconstruction from a single RGB or RGB\u2010D camera. The range of applications is vast and steadily growing as these technologies are further improving in speed, accuracy, and ease of use.",
            "referenceCount": 200,
            "citationCount": 270,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-01",
            "journal": {
                "name": "Computer Graphics Forum",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Zollh\u00f6fer2018StateOT,\n author = {M. Zollh\u00f6fer and Justus Thies and Pablo Garrido and D. Bradley and T. Beeler and P. P\u00e9rez and M. Stamminger and M. Nie\u00dfner and C. Theobalt},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications},\n volume = {37},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7bc2d21ab6b79935fccf3aad37deee5176683101",
            "@type": "ScholarlyArticle",
            "paperId": "7bc2d21ab6b79935fccf3aad37deee5176683101",
            "corpusId": 1904017,
            "url": "https://www.semanticscholar.org/paper/7bc2d21ab6b79935fccf3aad37deee5176683101",
            "title": "Multimodal Human Computer Interaction: A Survey",
            "venue": "ICCV-HCI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2005,
            "externalIds": {
                "MAG": "238919717",
                "DBLP": "conf/iccv/JaimesS05",
                "DOI": "10.1007/11573425_1",
                "CorpusId": 1904017
            },
            "abstract": null,
            "referenceCount": 198,
            "citationCount": 513,
            "influentialCitationCount": 31,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2005-10-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jaimes2005MultimodalHC,\n author = {A. Jaimes and N. Sebe},\n booktitle = {ICCV-HCI},\n pages = {1-15},\n title = {Multimodal Human Computer Interaction: A Survey},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b99d84daf2896c9b54d834d68299321916a1d6e",
            "@type": "ScholarlyArticle",
            "paperId": "3b99d84daf2896c9b54d834d68299321916a1d6e",
            "corpusId": 5281907,
            "url": "https://www.semanticscholar.org/paper/3b99d84daf2896c9b54d834d68299321916a1d6e",
            "title": "Synergism in low level vision",
            "venue": "Object recognition supported by user interaction for service robots",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2100435576",
                "DBLP": "conf/icpr/ChristoudiasGM02",
                "DOI": "10.1109/ICPR.2002.1047421",
                "CorpusId": 5281907
            },
            "abstract": "Guiding image segmentation with edge information is an often employed strategy in low level computer vision. To improve the trade-off between the sensitivity of homogeneous region delineation and the over-segmentation on of the image, we have incorporated a recently proposed edge magnitude/confidence map into a color image segmenter based on the mean shift procedure. The new method can recover regions with weak but sharp boundaries and thus can provide a more accurate input for high level interpretation modules. The edge detection and image segmentation (EDISON) system, available for download, implements the proposed technique and provides a complete toolbox for discontinuity preserving filtering, segmentation and edge detection.",
            "referenceCount": 16,
            "citationCount": 484,
            "influentialCitationCount": 35,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-08-11",
            "journal": {
                "name": "Object recognition supported by user interaction for service robots",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Christoudias2002SynergismIL,\n author = {C. M. Christoudias and B. Georgescu and P. Meer},\n booktitle = {Object recognition supported by user interaction for service robots},\n journal = {Object recognition supported by user interaction for service robots},\n pages = {150-155 vol.4},\n title = {Synergism in low level vision},\n volume = {4},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5e9f10adb1596259c7e83fce337575b11381e6c7",
            "@type": "ScholarlyArticle",
            "paperId": "5e9f10adb1596259c7e83fce337575b11381e6c7",
            "corpusId": 6096277,
            "url": "https://www.semanticscholar.org/paper/5e9f10adb1596259c7e83fce337575b11381e6c7",
            "title": "Rendering of Eyes for Eye-Shape Registration and Gaze Estimation",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/iccv/WoodBZS0B15",
                "MAG": "2950402190",
                "ArXiv": "1505.05916",
                "DOI": "10.1109/ICCV.2015.428",
                "CorpusId": 6096277
            },
            "abstract": "Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniquesto build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.",
            "referenceCount": 40,
            "citationCount": 287,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1505.05916",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-21",
            "journal": {
                "name": "2015 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wood2015RenderingOE,\n author = {Erroll Wood and T. Baltru\u0161aitis and Xucong Zhang and Yusuke Sugano and P. Robinson and A. Bulling},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2015 IEEE International Conference on Computer Vision (ICCV)},\n pages = {3756-3764},\n title = {Rendering of Eyes for Eye-Shape Registration and Gaze Estimation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c81664aec2276e7420c2dd27c89974c6339feba5",
            "@type": "ScholarlyArticle",
            "paperId": "c81664aec2276e7420c2dd27c89974c6339feba5",
            "corpusId": 13390420,
            "url": "https://www.semanticscholar.org/paper/c81664aec2276e7420c2dd27c89974c6339feba5",
            "title": "Front-End Vision and Multi-Scale Image Analysis",
            "venue": "Computational Imaging and Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "1584972072",
                "DBLP": "series/civ/Romeny03",
                "DOI": "10.1007/978-1-4020-8840-7",
                "CorpusId": 13390420
            },
            "abstract": null,
            "referenceCount": 209,
            "citationCount": 448,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2003-09-30",
            "journal": {
                "name": null,
                "volume": "27"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Romeny2003FrontEndVA,\n author = {B. H. Romeny},\n booktitle = {Computational Imaging and Vision},\n pages = {1-466},\n title = {Front-End Vision and Multi-Scale Image Analysis},\n volume = {27},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bf549ded8c9ee32077a894f227886a07e8a62d25",
            "@type": "ScholarlyArticle",
            "paperId": "bf549ded8c9ee32077a894f227886a07e8a62d25",
            "corpusId": 49668202,
            "url": "https://www.semanticscholar.org/paper/bf549ded8c9ee32077a894f227886a07e8a62d25",
            "title": "Deep Imbalanced Attribute Classification using Visual Attention Aggregation",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1807-03903",
                "MAG": "2867270703",
                "ArXiv": "1807.03903",
                "DOI": "10.1007/978-3-030-01252-6_42",
                "CorpusId": 49668202
            },
            "abstract": null,
            "referenceCount": 63,
            "citationCount": 174,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1807.03903",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sarafianos2018DeepIA,\n author = {N. Sarafianos and Xiang Xu and I. Kakadiaris},\n booktitle = {European Conference on Computer Vision},\n pages = {708-725},\n title = {Deep Imbalanced Attribute Classification using Visual Attention Aggregation},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b03420aa079b8d4c5332f369b21dc997723e23db",
            "@type": "ScholarlyArticle",
            "paperId": "b03420aa079b8d4c5332f369b21dc997723e23db",
            "corpusId": 1417739,
            "url": "https://www.semanticscholar.org/paper/b03420aa079b8d4c5332f369b21dc997723e23db",
            "title": "Crowd analysis: a survey",
            "venue": "Machine Vision and Applications",
            "publicationVenue": {
                "id": "urn:research:400d5e36-be35-4097-898f-753f4493156e",
                "name": "Machine Vision and Applications",
                "alternate_names": [
                    "Journal of Machine Vision and Applications",
                    "Mach Vis Appl",
                    "J Mach Vis Appl"
                ],
                "issn": "0932-8092",
                "url": "https://www.springer.com/computer/image+processing/journal/138"
            },
            "year": 2008,
            "externalIds": {
                "DBLP": "journals/mva/ZhanMRVX08",
                "MAG": "2065994824",
                "DOI": "10.1007/s00138-008-0132-4",
                "CorpusId": 1417739
            },
            "abstract": null,
            "referenceCount": 92,
            "citationCount": 577,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Sociology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2008-09-30",
            "journal": {
                "name": "Machine Vision and Applications",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhan2008CrowdAA,\n author = {B. Zhan and D. Monekosso and Paolo Remagnino and S. Velast\u00edn and Li-Qun Xu},\n booktitle = {Machine Vision and Applications},\n journal = {Machine Vision and Applications},\n pages = {345-357},\n title = {Crowd analysis: a survey},\n volume = {19},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:749683345813b14018ddea33157d2a2efb0f6565",
            "@type": "ScholarlyArticle",
            "paperId": "749683345813b14018ddea33157d2a2efb0f6565",
            "corpusId": 1008695,
            "url": "https://www.semanticscholar.org/paper/749683345813b14018ddea33157d2a2efb0f6565",
            "title": "PlayAnywhere: a compact interactive tabletop projection-vision system",
            "venue": "ACM Symposium on User Interface Software and Technology",
            "publicationVenue": {
                "id": "urn:research:c62b1316-0733-4b4c-8017-c07e18afa954",
                "name": "ACM Symposium on User Interface Software and Technology",
                "alternate_names": [
                    "User Interface Software and Technology",
                    "ACM Symp User Interface Softw Technol",
                    "User Interface Softw Technol",
                    "UIST"
                ],
                "issn": null,
                "url": "http://www.acm.org/uist/"
            },
            "year": 2005,
            "externalIds": {
                "MAG": "2148819007",
                "DBLP": "conf/uist/Wilson05",
                "DOI": "10.1145/1095034.1095047",
                "CorpusId": 1008695
            },
            "abstract": "We introduce PlayAnywhere, a front-projected computer vision-based interactive table system which uses a new commercially available projection technology to obtain a compact, self-contained form factor. PlayAnywhere's configuration addresses installation, calibration, and portability issues that are typical of most vision-based table systems, and thereby is particularly motivated in consumer applications. PlayAnywhere also makes a number of contributions related to image processing techniques for front-projected vision-based table systems, including a shadow-based touch detection algorithm, a fast, simple visual bar code scheme tailored to projection-vision table systems, the ability to continuously track sheets of paper, and an optical flow-based algorithm for the manipulation of onscreen objects that does not rely on fragile tracking algorithms.",
            "referenceCount": 46,
            "citationCount": 430,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2005-10-23",
            "journal": {
                "name": "Proceedings of the 18th annual ACM symposium on User interface software and technology",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wilson2005PlayAnywhereAC,\n author = {Andrew D. Wilson},\n booktitle = {ACM Symposium on User Interface Software and Technology},\n journal = {Proceedings of the 18th annual ACM symposium on User interface software and technology},\n title = {PlayAnywhere: a compact interactive tabletop projection-vision system},\n year = {2005}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec1f3f1f1d9af3046650c3a30f95a7f2f0a78390",
            "@type": "ScholarlyArticle",
            "paperId": "ec1f3f1f1d9af3046650c3a30f95a7f2f0a78390",
            "corpusId": 4078515,
            "url": "https://www.semanticscholar.org/paper/ec1f3f1f1d9af3046650c3a30f95a7f2f0a78390",
            "title": "What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3100615802",
                "ArXiv": "1801.06397",
                "DBLP": "journals/corr/abs-1801-06397",
                "DOI": "10.1007/s11263-018-1082-6",
                "CorpusId": 4078515
            },
            "abstract": null,
            "referenceCount": 59,
            "citationCount": 185,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1801.06397",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-01-19",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "126"
            },
            "citationStyles": {
                "bibtex": "@Article{Mayer2018WhatMG,\n author = {N. Mayer and Eddy Ilg and P. Fischer and C. Hazirbas and D. Cremers and A. Dosovitskiy and T. Brox},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {942 - 960},\n title = {What Makes Good Synthetic Training Data for Learning Disparity and Optical Flow Estimation?},\n volume = {126},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a81ea76d2f1f496dfe3be903f50a0d81883bfa1",
            "@type": "ScholarlyArticle",
            "paperId": "3a81ea76d2f1f496dfe3be903f50a0d81883bfa1",
            "corpusId": 8738055,
            "url": "https://www.semanticscholar.org/paper/3a81ea76d2f1f496dfe3be903f50a0d81883bfa1",
            "title": "Symmetry in 3D Geometry: Extraction and Applications",
            "venue": "Computer graphics forum (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/cgf/MitraPWC13",
                "MAG": "1580065766",
                "DOI": "10.1111/cgf.12010",
                "CorpusId": 8738055
            },
            "abstract": "The concept of symmetry has received significant attention in computer graphics and computer vision research in recent years. Numerous methods have been proposed to find, extract, encode and exploit geometric symmetries and high\u2010level structural information for a wide variety of geometry processing tasks. This report surveys and classifies recent developments in symmetry detection. We focus on elucidating the key similarities and differences between existing methods to gain a better understanding of a fundamental problem in digital geometry processing and shape understanding in general. We discuss a variety of applications in computer graphics and geometry processing that benefit from symmetry information for more effective processing. An analysis of the strengths and limitations of existing algorithms highlights the plenitude of opportunities for future research both in terms of theory and applications.",
            "referenceCount": 99,
            "citationCount": 321,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.ucl.ac.uk/staff/n.mitra/research/symmetry_survey/paper_docs/symmetryStar_small_EG12.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-09-01",
            "journal": {
                "name": "Computer Graphics Forum",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Mitra2013SymmetryI3,\n author = {N. Mitra and M. Pauly and Michael Wand and Duygu Ceylan},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {Symmetry in 3D Geometry: Extraction and Applications},\n volume = {32},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd3863065fb9a55fb1c6edf5fc2e927937d73a57",
            "@type": "ScholarlyArticle",
            "paperId": "bd3863065fb9a55fb1c6edf5fc2e927937d73a57",
            "corpusId": 8010468,
            "url": "https://www.semanticscholar.org/paper/bd3863065fb9a55fb1c6edf5fc2e927937d73a57",
            "title": "Human skin color clustering for face detection",
            "venue": "The IEEE Region 8 EUROCON 2003. Computer as a Tool.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2106075360",
                "DOI": "10.1109/EURCON.2003.1248169",
                "CorpusId": 8010468
            },
            "abstract": "Computer vision is one of many areas that wants to understand the process of human functionality and copy that process with the intention to complement human life with intelligent machines. For better human-computer interaction it is necessary for the machine to see people. This can be achieved by employing face detection algorithms, like the one used in the installation \"15 Seconds of Fame\". This installation unites the areas of modern art and technology. Its algorithm is based on skin colour detection. One of the problems that this and similar algorithms have to deal with is sensitivity to the illumination conditions under which the input image is captured. Hence illumination sensitivity influences face detection results. One of the aspects from which we can observe illumination influence is the choice of proper colour space. Since some colour spaces are designed to eliminate the influence of illumination (brightness) when describing colour of an object, an idea of using such a colour space for skin-colour detection has been considered and some of the methods have been researched and tested.",
            "referenceCount": 13,
            "citationCount": 674,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2003-12-03",
            "journal": {
                "name": "The IEEE Region 8 EUROCON 2003. Computer as a Tool.",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Kovac2003HumanSC,\n author = {J. Kovac and Peter Peer and F. Solina},\n booktitle = {The IEEE Region 8 EUROCON 2003. Computer as a Tool.},\n journal = {The IEEE Region 8 EUROCON 2003. Computer as a Tool.},\n pages = {144-148 vol.2},\n title = {Human skin color clustering for face detection},\n volume = {2},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:917fbd64a435cb33e0e5b4cd73fe830db7b166db",
            "@type": "ScholarlyArticle",
            "paperId": "917fbd64a435cb33e0e5b4cd73fe830db7b166db",
            "corpusId": 8712237,
            "url": "https://www.semanticscholar.org/paper/917fbd64a435cb33e0e5b4cd73fe830db7b166db",
            "title": "Distributional Semantics in Technicolor",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2137735870",
                "DBLP": "conf/acl/BruniBBT12",
                "ACL": "P12-1015",
                "CorpusId": 8712237
            },
            "abstract": "Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.",
            "referenceCount": 37,
            "citationCount": 375,
            "influentialCitationCount": 57,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-07-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bruni2012DistributionalSI,\n author = {Elia Bruni and Gemma Boleda and Marco Baroni and N. Tran},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {136-145},\n title = {Distributional Semantics in Technicolor},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e6584ab83b86f8cd2ce654b448ce6a15c83962db",
            "@type": "ScholarlyArticle",
            "paperId": "e6584ab83b86f8cd2ce654b448ce6a15c83962db",
            "corpusId": 7935559,
            "url": "https://www.semanticscholar.org/paper/e6584ab83b86f8cd2ce654b448ce6a15c83962db",
            "title": "Digital camera calibration methods: Considerations and comparisons",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2006,
            "externalIds": {
                "MAG": "3029595082",
                "DOI": "10.3929/ETHZ-B-000158067",
                "CorpusId": 7935559
            },
            "abstract": "Camera calibration has always been an essential component of photogrammetric measurement, with self-calibration nowadays being an integral and routinely applied operation within photogrammetric triangulation, especially in high-accuracy close-range measurement. With the very rapid growth in adoption of off-the-shelf digital cameras for a host of new 3D measurement applications, however, there are many situations where the geometry of the image network will not support robust recovery of camera parameters via on-the-job calibration. For this reason, stand-alone camera calibration has again emerged as an important issue in close-range photogrammetry, and it also remains a topic of research interest in computer vision. This paper overviews the current approaches adopted for camera calibration in close-range photogrammetry and computer vision, and discusses operational aspects for self-calibration. Also, the results of camera calibrations using different algorithms are summarized. Finally, the impact of chromatic aberration on modelled radial distortion is touched upon to highlight the fact that there are still issues of research interest in the photogrammetric calibration of consumer-grade digital cameras.",
            "referenceCount": 45,
            "citationCount": 587,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Remondino2006DigitalCC,\n author = {Fabio Remondino and C. Fraser},\n journal = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},\n pages = {266-272},\n title = {Digital camera calibration methods: Considerations and comparisons},\n volume = {36},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98348794c4ad61619859054d8818afca65cb7c2f",
            "@type": "ScholarlyArticle",
            "paperId": "98348794c4ad61619859054d8818afca65cb7c2f",
            "corpusId": 2425134,
            "url": "https://www.semanticscholar.org/paper/98348794c4ad61619859054d8818afca65cb7c2f",
            "title": "Pervasive Computing: A Paradigm for the 21st Century",
            "venue": "Computer",
            "publicationVenue": {
                "id": "urn:research:f6572f66-2623-4a5e-b0d9-4a5028dea98f",
                "name": "Computer",
                "alternate_names": [
                    "IEEE Computer",
                    "IEEE Comput"
                ],
                "issn": "0018-9162",
                "url": "http://www.computer.org/computer"
            },
            "year": 2003,
            "externalIds": {
                "MAG": "2081191534",
                "DBLP": "journals/computer/SahaM03",
                "DOI": "10.1109/MC.2003.1185214",
                "CorpusId": 2425134
            },
            "abstract": "Pervasive computing promises to make life simpler via digital environments that sense, adapt, and respond to human needs. Yet we still view computers as machines that run programs in a virtual environment. Pervasive computing presumes a different vision. A device can be a portal into an application-data space, not just a repository of custom software a user must manage. An application is a means by which a user performs a task, not software written to exploit a device's capabilities. And a computing environment is an information-enhanced physical space, not a virtual environment that exists to store and run software. Pervasive computing is close to technical and economic viability.",
            "referenceCount": 10,
            "citationCount": 769,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2003-03-01",
            "journal": {
                "name": "Computer",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Saha2003PervasiveCA,\n author = {Debashis Saha and A. Mukherjee},\n booktitle = {Computer},\n journal = {Computer},\n pages = {25-31},\n title = {Pervasive Computing: A Paradigm for the 21st Century},\n volume = {36},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90dab18196c20d37b4da55a3ea58c3b4aa231e6d",
            "@type": "ScholarlyArticle",
            "paperId": "90dab18196c20d37b4da55a3ea58c3b4aa231e6d",
            "corpusId": 243737,
            "url": "https://www.semanticscholar.org/paper/90dab18196c20d37b4da55a3ea58c3b4aa231e6d",
            "title": "Context-Based Pedestrian Path Prediction",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2101415982",
                "DBLP": "conf/eccv/KooijSFG14",
                "DOI": "10.1007/978-3-319-10599-4_40",
                "CorpusId": 243737
            },
            "abstract": null,
            "referenceCount": 34,
            "citationCount": 257,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.gavrila.net/Publications/eccv14.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kooij2014ContextBasedPP,\n author = {Julian F. P. Kooij and Nicolas Schneider and F. Flohr and D. Gavrila},\n booktitle = {European Conference on Computer Vision},\n pages = {618-633},\n title = {Context-Based Pedestrian Path Prediction},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:24644d5eb05e6b0ba31bc28b798656b77c4cc004",
            "@type": "ScholarlyArticle",
            "paperId": "24644d5eb05e6b0ba31bc28b798656b77c4cc004",
            "corpusId": 8714546,
            "url": "https://www.semanticscholar.org/paper/24644d5eb05e6b0ba31bc28b798656b77c4cc004",
            "title": "Tracking Groups of People",
            "venue": "Computer Vision and Image Understanding",
            "publicationVenue": {
                "id": "urn:research:5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                "name": "Computer Vision and Image Understanding",
                "alternate_names": [
                    "Comput Vis Image Underst"
                ],
                "issn": "1077-3142",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description"
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2072595688",
                "DBLP": "journals/cviu/McKennaJDRW00",
                "DOI": "10.1006/cviu.2000.0870",
                "CorpusId": 8714546
            },
            "abstract": "A computer vision system for tracking multiple people in relatively unconstrained environments is described. Tracking is performed at three levels of abstraction: regions, people, and groups. A novel, adaptive background subtraction method that combines color and gradient information is used to cope with shadows and unreliable color cues. People are tracked through mutual occlusions as they form groups and separate from one another. Strong use is made of color information to disambiguate occlusion and to provide qualitative estimates of depth ordering and position during occlusion. Simple interactions with objects can also be detected. The system is tested using both indoor and outdoor sequences. It is robust and should provide a useful mechanism for bootstrapping and reinitialization of tracking using more specific but less robust human models.",
            "referenceCount": 33,
            "citationCount": 768,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-10-01",
            "journal": {
                "name": "Comput. Vis. Image Underst.",
                "volume": "80"
            },
            "citationStyles": {
                "bibtex": "@Article{McKenna2000TrackingGO,\n author = {S. McKenna and S. Jabri and Zoran Duric and A. Rosenfeld and H. Wechsler},\n booktitle = {Computer Vision and Image Understanding},\n journal = {Comput. Vis. Image Underst.},\n pages = {42-56},\n title = {Tracking Groups of People},\n volume = {80},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f9e65fcb0e04174577f211d702d3f837e3624c5b",
            "@type": "ScholarlyArticle",
            "paperId": "f9e65fcb0e04174577f211d702d3f837e3624c5b",
            "corpusId": 1427294,
            "url": "https://www.semanticscholar.org/paper/f9e65fcb0e04174577f211d702d3f837e3624c5b",
            "title": "Multiclass Object Recognition with Sparse, Localized Features",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2105464770",
                "DBLP": "conf/cvpr/MutchL06",
                "DOI": "10.1109/CVPR.2006.200",
                "CorpusId": 1427294
            },
            "abstract": "We apply a biologically inspired model of visual object recognition to the multiclass object categorization problem. Our model modifies that of Serre, Wolf, and Poggio. As in that work, we first apply Gabor filters at all positions and scales; feature complexity and position/scale invariance are then built up by alternating template matching and max pooling operations. We refine the approach in several biologically plausible ways, using simple versions of sparsification and lateral inhibition. We demonstrate the value of retaining some position and scale information above the intermediate feature level. Using feature selection we arrive at a model that performs better with fewer features. Our final model is tested on the Caltech 101 object categories and the UIUC car localization task, in both cases achieving state-of-the-art performance. The results strengthen the case for using this class of model in computer vision.",
            "referenceCount": 29,
            "citationCount": 546,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.587.5216&rep=rep1&type=pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-17",
            "journal": {
                "name": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Mutch2006MulticlassOR,\n author = {Jim Mutch and D. Lowe},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\n pages = {11-18},\n title = {Multiclass Object Recognition with Sparse, Localized Features},\n volume = {1},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:922c5fcceeaa0ba8129dc8104bdd3df543a6beba",
            "@type": "ScholarlyArticle",
            "paperId": "922c5fcceeaa0ba8129dc8104bdd3df543a6beba",
            "corpusId": 8348576,
            "url": "https://www.semanticscholar.org/paper/922c5fcceeaa0ba8129dc8104bdd3df543a6beba",
            "title": "Asirra: a CAPTCHA that exploits interest-aligned manual image categorization",
            "venue": "Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:73f7fe95-b68b-468f-b7ba-3013ca879e50",
                "name": "Conference on Computer and Communications Security",
                "alternate_names": [
                    "Int Workshop Cogn Cell Syst",
                    "CCS",
                    "Comput Commun Secur",
                    "CcS",
                    "International Symposium on Community-centric Systems",
                    "International Workshop on Cognitive Cellular Systems",
                    "Conf Comput Commun Secur",
                    "Comb Comput Sci",
                    "Int Symp Community-centric Syst",
                    "Combinatorics and Computer Science",
                    "Circuits, Signals, and Systems",
                    "Computer and Communications Security",
                    "Circuit Signal Syst"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/ccs"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/ccs/ElsonDHS07",
                "MAG": "2156749117",
                "DOI": "10.1145/1315245.1315291",
                "CorpusId": 8348576
            },
            "abstract": "We present Asirra (Figure 1), a CAPTCHA that asks users to identify cats out of a set of 12 photographs of both cats and dogs. Asirra is easy for users; user studies indicate it can be solved by humans 99.6% of the time in under 30 seconds. Barring a major advance in machine vision, we expect computers will have no better than a 1/54,000 chance of solving it. Asirra\u2019s image database is provided by a novel, mutually beneficial partnership with Petfinder.com. In exchange for the use of their three million images, we display an \u201cadopt me\u201d link beneath each one, promoting Petfinder\u2019s primary mission of finding homes for homeless animals. We describe the design of Asirra, discuss threats to its security, and report early deployment experiences. We also describe two novel algorithms for amplifying the skill gap between humans and computers that can be used on many existing CAPTCHAs.",
            "referenceCount": 15,
            "citationCount": 542,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2007-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Elson2007AsirraAC,\n author = {J. Elson and John R. Douceur and Jon Howell and J. Saul},\n booktitle = {Conference on Computer and Communications Security},\n pages = {366-374},\n title = {Asirra: a CAPTCHA that exploits interest-aligned manual image categorization},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:047134b44dadb9859d1bcb19d589bcfaf1d0cd62",
            "@type": "ScholarlyArticle",
            "paperId": "047134b44dadb9859d1bcb19d589bcfaf1d0cd62",
            "corpusId": 39032320,
            "url": "https://www.semanticscholar.org/paper/047134b44dadb9859d1bcb19d589bcfaf1d0cd62",
            "title": "Nonparametric belief propagation",
            "venue": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "conf/cvpr/SudderthIFW03",
                "MAG": "2167947196",
                "DOI": "10.1145/1831407.1831431",
                "CorpusId": 39032320
            },
            "abstract": "In many applications of graphical models arising in computer vision, the hidden variables of interest are most naturally specified by continuous, non-Gaussian distributions. There exist inference algorithms for discrete approximations to these continuous distributions, but for the high-dimensional variables typically of interest, discrete inference becomes infeasible. Stochastic methods such as particle filters provide an appealing alternative. However, existing techniques fail to exploit the rich structure of the graphical models describing many vision problems. Drawing on ideas from regularized particle filters and belief propagation (BP), this paper develops a nonparametric belief propagation (NBP) algorithm applicable to general graphs. Each NBP iteration uses an efficient sampling procedure to update kernel-based approximations to the true, continuous likelihoods. The algorithm can accommodate an extremely broad class of potential functions, including nonparametric representations. Thus, NBP extends particle filtering methods to the more general vision problems that graphical models can describe. We apply the NBP algorithm to infer component interrelationships in a parts-based face model, allowing location and reconstruction of occluded features.",
            "referenceCount": 65,
            "citationCount": 613,
            "influentialCitationCount": 76,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ssg.mit.edu/~esuddert/papers/LIDS-TR2551.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2003-06-18",
            "journal": {
                "name": "2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Sudderth2003NonparametricBP,\n author = {Erik B. Sudderth and A. Ihler and W. Freeman and A. Willsky},\n booktitle = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},\n journal = {2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings.},\n pages = {I-I},\n title = {Nonparametric belief propagation},\n volume = {1},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd7d18aee6f43baeda78eea356d2a6497c07c8f7",
            "@type": "ScholarlyArticle",
            "paperId": "bd7d18aee6f43baeda78eea356d2a6497c07c8f7",
            "corpusId": 2164492,
            "url": "https://www.semanticscholar.org/paper/bd7d18aee6f43baeda78eea356d2a6497c07c8f7",
            "title": "Deriving intrinsic images from image sequences",
            "venue": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "DBLP": "conf/iccv/Weiss01",
                "MAG": "2136748901",
                "DOI": "10.1109/ICCV.2001.937606",
                "CorpusId": 2164492
            },
            "abstract": "Intrinsic images are a useful midlevel description of scenes proposed by H.G. Barrow and J.M. Tenenbaum (1978). An image is de-composed into two images: a reflectance image and an illumination image. Finding such a decomposition remains a difficult problem in computer vision. We focus on a slightly, easier problem: given a sequence of T images where the reflectance is constant and the illumination changes, can we recover T illumination images and a single reflectance image? We show that this problem is still imposed and suggest approaching it as a maximum-likelihood estimation problem. Following recent work on the statistics of natural images, we use a prior that assumes that illumination images will give rise to sparse filter outputs. We show that this leads to a simple, novel algorithm for recovering reflectance images. We illustrate the algorithm's performance on real and synthetic image sequences.",
            "referenceCount": 13,
            "citationCount": 682,
            "influentialCitationCount": 79,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-07-07",
            "journal": {
                "name": "Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Weiss2001DerivingII,\n author = {Yair Weiss},\n booktitle = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},\n journal = {Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001},\n pages = {68-75 vol.2},\n title = {Deriving intrinsic images from image sequences},\n volume = {2},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f59f40be27ea17b40d5d54367624e89fc3b6659",
            "@type": "ScholarlyArticle",
            "paperId": "1f59f40be27ea17b40d5d54367624e89fc3b6659",
            "corpusId": 15833852,
            "url": "https://www.semanticscholar.org/paper/1f59f40be27ea17b40d5d54367624e89fc3b6659",
            "title": "Blind Haze Separation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2006,
            "externalIds": {
                "MAG": "2141030717",
                "DBLP": "conf/cvpr/ShwartzNS06",
                "DOI": "10.1109/CVPR.2006.71",
                "CorpusId": 15833852
            },
            "abstract": "Outdoor imaging is plagued by poor visibility conditions due to atmospheric scattering, particularly in haze. A major problem is spatially-varying reduction of contrast by stray radiance (airlight), which is scattered by the haze particles towards the camera. Recent computer vision methods have shown that images can be compensated for haze, and even yield a depth map of the scene. A key step in such a scene recovery is subtraction of the airlight. In particular, this can be achieved by analyzing polarization-filtered images. However, the recovery requires parameters of the airlight. These parameters were estimated in past studies by measuring pixels in sky areas. This paper derives an approach for blindly recovering the parameter needed for separating the airlight from the measurements, thus recovering contrast, with neither user interaction nor existence of the sky in the frame. This eases the interaction and conditions needed for image dehazing, which also requires compensation for attenuation. The approach has proved successful in experiments, some of which are shown here.",
            "referenceCount": 35,
            "citationCount": 577,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2006-06-17",
            "journal": {
                "name": "2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Shwartz2006BlindHS,\n author = {S. Shwartz and Einav Namer and Y. Schechner},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},\n pages = {1984-1991},\n title = {Blind Haze Separation},\n volume = {2},\n year = {2006}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0c1270c6d22773b84f9465df926387d6b5e478ac",
            "@type": "ScholarlyArticle",
            "paperId": "0c1270c6d22773b84f9465df926387d6b5e478ac",
            "corpusId": 2265660,
            "url": "https://www.semanticscholar.org/paper/0c1270c6d22773b84f9465df926387d6b5e478ac",
            "title": "Adaptive Active Learning for Image Classification",
            "venue": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/cvpr/LiG13",
                "MAG": "1975672287",
                "DOI": "10.1109/CVPR.2013.116",
                "CorpusId": 2265660
            },
            "abstract": "Recently active learning has attracted a lot of attention in computer vision field, as it is time and cost consuming to prepare a good set of labeled images for vision data analysis. Most existing active learning approaches employed in computer vision adopt most uncertainty measures as instance selection criteria. Although most uncertainty query selection strategies are very effective in many circumstances, they fail to take information in the large amount of unlabeled instances into account and are prone to querying outliers. In this paper, we present a novel adaptive active learning approach that combines an information density measure and a most uncertainty measure together to select critical instances to label for image classifications. Our experiments on two essential tasks of computer vision, object recognition and scene recognition, demonstrate the efficacy of the proposed approach.",
            "referenceCount": 38,
            "citationCount": 256,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://astro.temple.edu/%7Etud51700/pdfs/cvpr13.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-06-23",
            "journal": {
                "name": "2013 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2013AdaptiveAL,\n author = {X. Li and Yuhong Guo},\n booktitle = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {859-866},\n title = {Adaptive Active Learning for Image Classification},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:610f13cd127a5e9cfebe01ca78c83378e796130b",
            "@type": "ScholarlyArticle",
            "paperId": "610f13cd127a5e9cfebe01ca78c83378e796130b",
            "corpusId": 52744,
            "url": "https://www.semanticscholar.org/paper/610f13cd127a5e9cfebe01ca78c83378e796130b",
            "title": "Age classification from facial images",
            "venue": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1994,
            "externalIds": {
                "MAG": "2121939926",
                "DBLP": "journals/cviu/KwonL99",
                "DOI": "10.1109/CVPR.1994.323894",
                "CorpusId": 52744
            },
            "abstract": "The ability to classify age from a facial image has not been pursued in computer vision. This research addresses the limited task of age classification of a facial image into a baby, young adult, and senior adult. This is the first reported work to classify age, and to successfully extract and use natural wrinkles. We present a theory and practical computations for visual age classification from facial images, based on cranio-facial changes in feature-position ratios, and on skin wrinkle analysis. Three age groups are classified.<<ETX>>",
            "referenceCount": 30,
            "citationCount": 816,
            "influentialCitationCount": 56,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1994-06-21",
            "journal": {
                "name": "1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kwon1994AgeCF,\n author = {Y. Kwon and N. Lobo},\n booktitle = {1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {1994 Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {762-767},\n title = {Age classification from facial images},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:973915e24f2a46ec9b5035e4b12908519980fd2a",
            "@type": "ScholarlyArticle",
            "paperId": "973915e24f2a46ec9b5035e4b12908519980fd2a",
            "corpusId": 10494418,
            "url": "https://www.semanticscholar.org/paper/973915e24f2a46ec9b5035e4b12908519980fd2a",
            "title": "Global supervised descent method",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1946919140",
                "DBLP": "conf/cvpr/XiongT15",
                "DOI": "10.1109/CVPR.2015.7298882",
                "CorpusId": 10494418
            },
            "abstract": "Mathematical optimization plays a fundamental role in solving many problems in computer vision (e.g., camera calibration, image alignment, structure from motion). It is generally accepted that second order descent methods are the most robust, fast, and reliable approaches for nonlinear optimization of a general smooth function. However, in the context of computer vision, second order descent methods have two main drawbacks: 1) the function might not be analytically differentiable and numerical approximations are impractical, and 2) the Hessian may be large and not positive definite. Recently, Supervised Descent Method (SDM), a method that learns the \u201cweighted averaged gradients\u201d in a supervised manner has been proposed to solve these issues. However, SDM is a local algorithm and it is likely to average conflicting gradient directions. This paper proposes Global SDM (GSDM), an extension of SDM that divides the search space into regions of similar gradient directions. GSDM provides a better and more efficient strategy to minimize non-linear least squares functions in computer vision problems. We illustrate the effectiveness of GSDM in two problems: non-rigid image alignment and extrinsic camera calibration.",
            "referenceCount": 52,
            "citationCount": 209,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiong2015GlobalSD,\n author = {Xuehan Xiong and F. D. L. Torre},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2664-2673},\n title = {Global supervised descent method},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f5dfbd571ef719a2c5dc1ed63ca6a3702f7861e",
            "@type": "ScholarlyArticle",
            "paperId": "3f5dfbd571ef719a2c5dc1ed63ca6a3702f7861e",
            "corpusId": 5413050,
            "url": "https://www.semanticscholar.org/paper/3f5dfbd571ef719a2c5dc1ed63ca6a3702f7861e",
            "title": "A Framework for Robust Subspace Learning",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2003,
            "externalIds": {
                "DBLP": "journals/ijcv/TorreB03",
                "MAG": "1513013675",
                "DOI": "10.1023/A:1023709501986",
                "CorpusId": 5413050
            },
            "abstract": null,
            "referenceCount": 82,
            "citationCount": 636,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2003-08-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Torre2003AFF,\n author = {F. D. L. Torre and Michael J. Black},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {117-142},\n title = {A Framework for Robust Subspace Learning},\n volume = {54},\n year = {2003}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f3f0567853773023196890cb0a52e2292daca56f",
            "@type": "ScholarlyArticle",
            "paperId": "f3f0567853773023196890cb0a52e2292daca56f",
            "corpusId": 14664687,
            "url": "https://www.semanticscholar.org/paper/f3f0567853773023196890cb0a52e2292daca56f",
            "title": "Optimizing Binary MRFs via Extended Roof Duality",
            "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/cvpr/RotherKLS07",
                "MAG": "2137117160",
                "DOI": "10.1109/CVPR.2007.383203",
                "CorpusId": 14664687
            },
            "abstract": "Many computer vision applications rely on the efficient optimization of challenging, so-called non-submodular, binary pairwise MRFs. A promising graph cut based approach for optimizing such MRFs known as \"roof duality\" was recently introduced into computer vision. We study two methods which extend this approach. First, we discuss an efficient implementation of the \"probing\" technique introduced recently by Bows et al. (2006). It simplifies the MRF while preserving the global optimum. Our code is 400-700 faster on some graphs than the implementation of the work of Bows et al. (2006). Second, we present a new technique which takes an arbitrary input labeling and tries to improve its energy. We give theoretical characterizations of local minima of this procedure. We applied both techniques to many applications, including image segmentation, new view synthesis, super-resolution, diagram recognition, parameter learning, texture restoration, and image deconvolution. For several applications we see that we are able to find the global minimum very efficiently, and considerably outperform the original roof duality approach. In comparison to existing techniques, such as graph cut, TRW, BP, ICM, and simulated annealing, we nearly always find a lower energy.",
            "referenceCount": 34,
            "citationCount": 502,
            "influentialCitationCount": 83,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-06-17",
            "journal": {
                "name": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rother2007OptimizingBM,\n author = {C. Rother and V. Kolmogorov and V. Lempitsky and M. Szummer},\n booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Optimizing Binary MRFs via Extended Roof Duality},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5547be02207085a31cd4e1e5c6577d1bafe66824",
            "@type": "ScholarlyArticle",
            "paperId": "5547be02207085a31cd4e1e5c6577d1bafe66824",
            "corpusId": 2218668,
            "url": "https://www.semanticscholar.org/paper/5547be02207085a31cd4e1e5c6577d1bafe66824",
            "title": "A multiscale model of adaptation and spatial vision for realistic image display",
            "venue": "International Conference on Computer Graphics and Interactive Techniques",
            "publicationVenue": {
                "id": "urn:research:cf6b5e76-9274-46e6-a2dd-7c190ec2ec5f",
                "name": "International Conference on Computer Graphics and Interactive Techniques",
                "alternate_names": [
                    "Int Conf Comput Graph Interact Tech",
                    "SIGGRAPH"
                ],
                "issn": null,
                "url": "http://www.siggraph.org/"
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2056016381",
                "DBLP": "conf/siggraph/PattanaikFFG98",
                "DOI": "10.1145/280814.280922",
                "CorpusId": 2218668
            },
            "abstract": "In this paper we develop a computational model of adaptation and spatial vision for realistic tone reproduction. The model is based on a multiscale representation of pattern, luminance, and color processing in the human visual system. We incorporate the model into a tone reproduction operator that maps the vast ranges of radiances found in real and synthetic scenes into the small fixed ranges available on conventional display devices such as CRT\u2019s and printers. The model allows the operator to address the two major problems in realistic tone reproduction: wide absolute range and high dynamic range scenes can be displayed; and the displayed images match our perceptions of the scenes at both threshold and suprathreshold levels to the degree possible given a particular display device. Although in this paper we apply our visual model to the tone reproduction problem, the model is general and can be usefully applied to image quality metrics, image compression methods, and perceptually-based image synthesis algorithms. CR Categories: I.3.0 [Computer Graphics]: General;",
            "referenceCount": 35,
            "citationCount": 466,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.ucf.edu/~sumant/publications/cconf98.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-07-24",
            "journal": {
                "name": "Proceedings of the 25th annual conference on Computer graphics and interactive techniques",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Pattanaik1998AMM,\n author = {S. Pattanaik and J. Ferwerda and M. Fairchild and D. Greenberg},\n booktitle = {International Conference on Computer Graphics and Interactive Techniques},\n journal = {Proceedings of the 25th annual conference on Computer graphics and interactive techniques},\n title = {A multiscale model of adaptation and spatial vision for realistic image display},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:80734be3328f28e4babe8832e608a33bb768a838",
            "@type": "ScholarlyArticle",
            "paperId": "80734be3328f28e4babe8832e608a33bb768a838",
            "corpusId": 742021,
            "url": "https://www.semanticscholar.org/paper/80734be3328f28e4babe8832e608a33bb768a838",
            "title": "SiZer for Exploration of Structures in Curves",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1999,
            "externalIds": {
                "MAG": "2099416373",
                "DOI": "10.1080/01621459.1999.10474186",
                "CorpusId": 742021
            },
            "abstract": "Abstract In the use of smoothing methods in data analysis, an important question is which observed features are \u201creally there,\u201d as opposed to being spurious sampling artifacts. An approach is described based on scale-space ideas originally developed in the computer vision literature. Assessment of Significant ZERo crossings of derivatives results in the SiZer map, a graphical device for display of significance of features with respect to both location and scale. Here \u201cscale\u201d means \u201clevel of resolution\u201d; that is, \u201cbandwidth.\u201d",
            "referenceCount": 52,
            "citationCount": 617,
            "influentialCitationCount": 108,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://cdr.lib.unc.edu/downloads/rx913z87v",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1999-09-01",
            "journal": {
                "name": "Journal of the American Statistical Association",
                "volume": "94"
            },
            "citationStyles": {
                "bibtex": "@Article{Chaudhuri1999SiZerFE,\n author = {P. Chaudhuri and J. Marron},\n journal = {Journal of the American Statistical Association},\n pages = {807-823},\n title = {SiZer for Exploration of Structures in Curves},\n volume = {94},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac3a13203372a40f9d12ae74899ba4401d1c7e26",
            "@type": "ScholarlyArticle",
            "paperId": "ac3a13203372a40f9d12ae74899ba4401d1c7e26",
            "corpusId": 107613538,
            "url": "https://www.semanticscholar.org/paper/ac3a13203372a40f9d12ae74899ba4401d1c7e26",
            "title": "Handbook of Augmented Reality",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "books/daglib/0027797",
                "MAG": "9783058",
                "DOI": "10.1007/978-1-4614-0064-6",
                "CorpusId": 107613538
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 397,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2011-09-28",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Furht2011HandbookOA,\n author = {B. Furht},\n title = {Handbook of Augmented Reality},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f63d805d72621a38284039cb095848a8e877f260",
            "@type": "ScholarlyArticle",
            "paperId": "f63d805d72621a38284039cb095848a8e877f260",
            "corpusId": 5101545,
            "url": "https://www.semanticscholar.org/paper/f63d805d72621a38284039cb095848a8e877f260",
            "title": "i-LAND: an interactive landscape for creativity and innovation",
            "venue": "International Conference on Human Factors in Computing Systems",
            "publicationVenue": {
                "id": "urn:research:b55b50b1-aae7-47a7-b042-8aecc930073d",
                "name": "International Conference on Human Factors in Computing Systems",
                "alternate_names": [
                    "CHI",
                    "Int Conf Hum Factor Comput Syst",
                    "Human Factors in Computing Systems",
                    "Conference on Human Interface",
                    "Conf Hum Interface",
                    "Hum Factor Comput Syst"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigchi/"
            },
            "year": 1999,
            "externalIds": {
                "DBLP": "conf/chi/StreitzGHKMRRSS99",
                "MAG": "2094982166",
                "DOI": "10.1145/302979.303010",
                "CorpusId": 5101545
            },
            "abstract": "We describe the i-LAND environment which constitutes an exampleof our vision of the workspaces of the future, in this casesupporting cooperative work of dynamic teams with changing needs.i-LAND requires and provides new forms of human-computerinteraction and new forms of computer-supported cooperative work.Its design is based on an integration of information andarchitectural spaces, implications of new work practices and anempirical requirements study informing our design. i-LAND consistsof several roomware components, i.e. computer-aug- mented objectsintegrating room elements with information technology. We presentthe current realization of i-LAND in terms of an interactiveelectronic wall, an interactive table, two computer-enhancedchairs, and two bridges for the Passage-mechanism. This iscomplemented by the description of the creativity supportapplication and the technological infrastructure. The paper isaccompanied by a video figure in the CHI99 video program.",
            "referenceCount": 28,
            "citationCount": 764,
            "influentialCitationCount": 36,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1999-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Streitz1999iLANDAI,\n author = {Norbert A. Streitz and J\u00f6rg Gei\u00dfler and Torsten Holmer and S. Konomi and Christian M\u00fcller-Tomfelde and Wolfgang Reischl and P. Rexroth and P. Tandler and R. Steinmetz},\n booktitle = {International Conference on Human Factors in Computing Systems},\n pages = {120-127},\n title = {i-LAND: an interactive landscape for creativity and innovation},\n year = {1999}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e1e4f2c791f81a5fe95e693644e002fb652adb39",
            "@type": "ScholarlyArticle",
            "paperId": "e1e4f2c791f81a5fe95e693644e002fb652adb39",
            "corpusId": 2472547,
            "url": "https://www.semanticscholar.org/paper/e1e4f2c791f81a5fe95e693644e002fb652adb39",
            "title": "Learning Attributes Equals Multi-Source Domain Generalization",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950286390",
                "ArXiv": "1605.00743",
                "DBLP": "journals/corr/GanYG16",
                "DOI": "10.1109/CVPR.2016.17",
                "CorpusId": 2472547
            },
            "abstract": "Attributes possess appealing properties and benefit many computer vision problems, such as object recognition, learning with humans in the loop, and image retrieval. Whereas the existing work mainly pursues utilizing attributes for various computer vision problems, we contend that the most basic problem-how to accurately and robustly detect attributes from images-has been left under explored. Especially, the existing work rarely explicitly tackles the need that attribute detectors should generalize well across different categories, including those previously unseen. Noting that this is analogous to the objective of multi-source domain generalization, if we treat each category as a domain, we provide a novel perspective to attribute detection and propose to gear the techniques in multi-source domain generalization for the purpose of learning cross-category generalizable attribute detectors. We validate our understanding and approach with extensive experiments on four challenging datasets and three different problems.",
            "referenceCount": 83,
            "citationCount": 186,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1605.00743",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-05-03",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gan2016LearningAE,\n author = {Chuang Gan and Tianbao Yang and Boqing Gong},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {87-97},\n title = {Learning Attributes Equals Multi-Source Domain Generalization},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0095ca57665eae51c9dd7a4ed8a3311aeea1b441",
            "@type": "ScholarlyArticle",
            "paperId": "0095ca57665eae51c9dd7a4ed8a3311aeea1b441",
            "corpusId": 2794268,
            "url": "https://www.semanticscholar.org/paper/0095ca57665eae51c9dd7a4ed8a3311aeea1b441",
            "title": "Graph Cut Based Inference with Co-occurrence Statistics",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1748750709",
                "DBLP": "conf/eccv/LadickyRKT10",
                "DOI": "10.1007/978-3-642-15555-0_18",
                "CorpusId": 2794268
            },
            "abstract": null,
            "referenceCount": 43,
            "citationCount": 364,
            "influentialCitationCount": 50,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15555-0_18.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2010-09-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ladicky2010GraphCB,\n author = {L. Ladicky and Chris Russell and Pushmeet Kohli and Philip H. S. Torr},\n booktitle = {European Conference on Computer Vision},\n pages = {239-253},\n title = {Graph Cut Based Inference with Co-occurrence Statistics},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:39f5cefa99fd6f10db15c3783883ebf362fd7848",
            "@type": "ScholarlyArticle",
            "paperId": "39f5cefa99fd6f10db15c3783883ebf362fd7848",
            "corpusId": 13565271,
            "url": "https://www.semanticscholar.org/paper/39f5cefa99fd6f10db15c3783883ebf362fd7848",
            "title": "Riemannian Analysis of Probability Density Functions with Applications in Vision",
            "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "2169678505",
                "DBLP": "conf/cvpr/SrivastavaJJ07",
                "DOI": "10.1109/CVPR.2007.383188",
                "CorpusId": 13565271
            },
            "abstract": "Applications in computer vision involve statistically analyzing an important class of constrained, non-negative functions, including probability density functions (in texture analysis), dynamic time-warping functions (in activity analysis), and re-parametrization or non-rigid registration functions (in shape analysis of curves). For this one needs to impose a Riemannian structure on the spaces formed by these functions. We propose a \"spherical\" version of the Fisher-Rao metric that provides closed-form expressions for geodesies and distances, and allows fast computation of sample statistics. To demonstrate this approach, we present an application in planar shape classification.",
            "referenceCount": 20,
            "citationCount": 199,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://dro.dur.ac.uk/18442/1/18442.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-06-17",
            "journal": {
                "name": "2007 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Srivastava2007RiemannianAO,\n author = {Anuj Srivastava and Ian H. Jermyn and Shantanu H. Joshi},\n booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {1-8},\n title = {Riemannian Analysis of Probability Density Functions with Applications in Vision},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5283e1b8ff34bc6c939e5a1b83767784e1492a67",
            "@type": "ScholarlyArticle",
            "paperId": "5283e1b8ff34bc6c939e5a1b83767784e1492a67",
            "corpusId": 112721847,
            "url": "https://www.semanticscholar.org/paper/5283e1b8ff34bc6c939e5a1b83767784e1492a67",
            "title": "Forces of production : a social history of industrial automation",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1985,
            "externalIds": {
                "MAG": "2798360528",
                "DOI": "10.4324/9780203791806",
                "CorpusId": 112721847
            },
            "abstract": "Focusing on the design and implementation of computer-based automatic machine tools, David F. Noble challenges the idea that technology has a life of its own. Technology has In which students of the much, more careerist vision. David noble argues that the american metal working industry workers wanted this. Animals also demonstrates how the whole sum of automation more than tail! Using detailed exposition of men enter into a path.",
            "referenceCount": 0,
            "citationCount": 936,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Sociology",
                "Engineering",
                "Business"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "external"
                },
                {
                    "category": "History",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "1985-06-01",
            "journal": {
                "name": "The American Historical Review",
                "volume": "94"
            },
            "citationStyles": {
                "bibtex": "@Article{Noble1985ForcesOP,\n author = {D. Noble},\n journal = {The American Historical Review},\n pages = {545},\n title = {Forces of production : a social history of industrial automation},\n volume = {94},\n year = {1985}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:61a805d396c284837bcd17650c9fefd361fc3fac",
            "@type": "ScholarlyArticle",
            "paperId": "61a805d396c284837bcd17650c9fefd361fc3fac",
            "corpusId": 7146270,
            "url": "https://www.semanticscholar.org/paper/61a805d396c284837bcd17650c9fefd361fc3fac",
            "title": "Incremental Singular Value Decomposition of Uncertain Data with Missing Values",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2110121428",
                "DBLP": "conf/eccv/Brand02",
                "DOI": "10.1007/3-540-47969-4_47",
                "CorpusId": 7146270
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 549,
            "influentialCitationCount": 88,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.merl.com/papers/docs/TR2002-24.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Brand2002IncrementalSV,\n author = {M. Brand},\n booktitle = {European Conference on Computer Vision},\n pages = {707-720},\n title = {Incremental Singular Value Decomposition of Uncertain Data with Missing Values},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2fe88db144825e3fa72a8fae1e307cc7d056db5c",
            "@type": "ScholarlyArticle",
            "paperId": "2fe88db144825e3fa72a8fae1e307cc7d056db5c",
            "corpusId": 12002813,
            "url": "https://www.semanticscholar.org/paper/2fe88db144825e3fa72a8fae1e307cc7d056db5c",
            "title": "Real time face and object tracking as a component of a perceptual user interface",
            "venue": "Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "2121836097",
                "DBLP": "conf/wacv/Bradski98",
                "DOI": "10.1109/ACV.1998.732882",
                "CorpusId": 12002813
            },
            "abstract": "As a step towards a perceptual user interface, an object tracking algorithm is developed and demonstrated tracking human faces. Computer vision algorithms that are intended to form part of a perceptual user interface must be fast and efficient. They must be able to track in real time and yet not absorb a major share of computational resources. An efficient, new algorithm is described here based on the mean shift algorithm. The mean shift algorithm robustly finds the mode (peak) of probability distributions. We first describe histogram based methods of producing object probability distributions. In our case, we want to track the mode of an object's probability distribution within a video scene. Since the probability distribution of the object can change and move dynamically in time, the mean shift algorithm is modified to deal with dynamically changing probability distributions. The modified algorithm is called the Continuously Adaptive Mean Shift (CAMSHIFT) algorithm. CAMSHIFT is then used as an interface for games and graphics.",
            "referenceCount": 10,
            "citationCount": 681,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "1998-10-19",
            "journal": {
                "name": "Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bradski1998RealTF,\n author = {G. Bradski},\n booktitle = {Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)},\n journal = {Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)},\n pages = {214-219},\n title = {Real time face and object tracking as a component of a perceptual user interface},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6bb60de0e93c9df260ef3d20dbff0716ed1ec711",
            "@type": "ScholarlyArticle",
            "paperId": "6bb60de0e93c9df260ef3d20dbff0716ed1ec711",
            "corpusId": 123520,
            "url": "https://www.semanticscholar.org/paper/6bb60de0e93c9df260ef3d20dbff0716ed1ec711",
            "title": "Assessing the Quality of Actions",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/eccv/PirsiavashVT14",
                "MAG": "2260521078",
                "DOI": "10.1007/978-3-319-10599-4_36",
                "CorpusId": 123520
            },
            "abstract": null,
            "referenceCount": 43,
            "citationCount": 177,
            "influentialCitationCount": 42,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10599-4_36.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-09-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pirsiavash2014AssessingTQ,\n author = {H. Pirsiavash and Carl Vondrick and A. Torralba},\n booktitle = {European Conference on Computer Vision},\n pages = {556-571},\n title = {Assessing the Quality of Actions},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7d44d973439d7844b236f4225881f057699dfd4",
            "@type": "ScholarlyArticle",
            "paperId": "f7d44d973439d7844b236f4225881f057699dfd4",
            "corpusId": 17588199,
            "url": "https://www.semanticscholar.org/paper/f7d44d973439d7844b236f4225881f057699dfd4",
            "title": "Clear underwater vision",
            "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/cvpr/SchechnerK04",
                "MAG": "2145213600",
                "DOI": "10.1109/CVPR.2004.59",
                "CorpusId": 17588199
            },
            "abstract": "Underwater imaging is important for scientific research and technology, as well as for popular activities. We present a computer vision approach which easily removes degradation effects in underwater vision. We analyze the physical effects of visibility degradation. We show that the main degradation effects can be associated with partial polarization of light. We therefore present an algorithm which inverts the image formation process, to recover a good visibility image of the object. The algorithm is based on a couple of images taken through a polarizer at different orientations. As a by product, a distance map of the scene is derived as well. We successfully used our approach when experimenting in the sea using a system we built. We obtained great improvement of scene contrast and color correction, and nearly doubled the underwater visibility range.",
            "referenceCount": 39,
            "citationCount": 330,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ee.technion.ac.il/~yoav/publications/tWaterCVPR04.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-07-19",
            "journal": {
                "name": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Schechner2004ClearUV,\n author = {Y. Schechner and N. Karpel},\n booktitle = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},\n journal = {Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.},\n pages = {I-I},\n title = {Clear underwater vision},\n volume = {1},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1bbc31c14529b64cd7ad833b8a270b3b3e515c75",
            "@type": "ScholarlyArticle",
            "paperId": "1bbc31c14529b64cd7ad833b8a270b3b3e515c75",
            "corpusId": 6557946,
            "url": "https://www.semanticscholar.org/paper/1bbc31c14529b64cd7ad833b8a270b3b3e515c75",
            "title": "Best of both worlds: Human-machine collaboration for object annotation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/cvpr/RussakovskyLL15",
                "MAG": "1908985308",
                "DOI": "10.1109/CVPR.2015.7298824",
                "CorpusId": 6557946
            },
            "abstract": "The long-standing goal of localizing every object in an image remains elusive. Manually annotating objects is quite expensive despite crowd engineering innovations. Current state-of-the-art automatic object detectors can accurately detect at most a few objects per image. This paper brings together the latest advancements in object detection and in crowd engineering into a principled framework for accurately and efficiently localizing objects in images. The input to the system is an image to annotate and a set of annotation constraints: desired precision, utility and/or human cost of the labeling. The output is a set of object annotations, informed by human feedback and computer vision. Our model seamlessly integrates multiple computer vision models with multiple sources of human input in a Markov Decision Process. We empirically validate the effectiveness of our human-in-the-loop labeling approach on the ILSVRC2014 object detection dataset.",
            "referenceCount": 66,
            "citationCount": 195,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ai.stanford.edu/%7Eolga/papers/RussakovskyCVPR15.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-07",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Russakovsky2015BestOB,\n author = {Olga Russakovsky and Li-Jia Li and Li Fei-Fei},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2121-2131},\n title = {Best of both worlds: Human-machine collaboration for object annotation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5eff2dfb1e2dc775a90c34c5a13337a386e5d7f6",
            "@type": "ScholarlyArticle",
            "paperId": "5eff2dfb1e2dc775a90c34c5a13337a386e5d7f6",
            "corpusId": 400434,
            "url": "https://www.semanticscholar.org/paper/5eff2dfb1e2dc775a90c34c5a13337a386e5d7f6",
            "title": "A vision system for landing an unmanned aerial vehicle",
            "venue": "Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2148498215",
                "DBLP": "conf/icra/SharpSS01",
                "DOI": "10.1109/ROBOT.2001.932859",
                "CorpusId": 400434
            },
            "abstract": "We present the design and implementation of a real-time computer vision system for a rotorcraft unmanned aerial vehicle to land onto a known landing target. This vision system consists of customized software and off-the-shelf hardware which perform image processing, segmentation, feature point extraction, camera pan/tilt control, and motion estimation. We introduce the design of a landing target which significantly simplifies the computer vision tasks such as corner detection and correspondence matching. Customized algorithms are developed to allow for realtime computation at a frame rate of 30 Hz. Such algorithms include certain linear and nonlinear optimization schemes for model-based camera pose estimation. We present results from an actual flight test which show the vision-based state estimates are accurate to within 5 cm in each axis of translation, and 5 degrees in each axis of rotation, making vision a viable sensor to be placed in the control loop of a hierarchical flight management system.",
            "referenceCount": 14,
            "citationCount": 373,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2001-05-21",
            "journal": {
                "name": "Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Sharp2001AVS,\n author = {Courtney S. Sharp and O. Shakernia and S. Sastry},\n booktitle = {Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)},\n journal = {Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)},\n pages = {1720-1727 vol.2},\n title = {A vision system for landing an unmanned aerial vehicle},\n volume = {2},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:119cdc6ff080c2490af41654a7f2223cc55d27de",
            "@type": "ScholarlyArticle",
            "paperId": "119cdc6ff080c2490af41654a7f2223cc55d27de",
            "corpusId": 25883705,
            "url": "https://www.semanticscholar.org/paper/119cdc6ff080c2490af41654a7f2223cc55d27de",
            "title": "Visual search: a retrospective.",
            "venue": "Journal of Vision",
            "publicationVenue": {
                "id": "urn:research:c3faa921-3f7d-4435-906f-25cdb7d6a885",
                "name": "Journal of Vision",
                "alternate_names": [
                    "J Vis",
                    "Journal of Visualization"
                ],
                "issn": "1534-7362",
                "url": "http://www.journalofvision.org/4/6/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2033710484",
                "DOI": "10.1167/11.5.14",
                "CorpusId": 25883705,
                "PubMed": "22209816"
            },
            "abstract": "Visual search, a vital task for humans and animals, has also become a common and important tool for studying many topics central to active vision and cognition ranging from spatial vision, attention, and oculomotor control to memory, decision making, and rewards. While visual search often seems effortless to humans, trying to recreate human visual search abilities in machines has represented an incredible challenge for computer scientists and engineers. What are the brain computations that ensure successful search? This review article draws on efforts from various subfields and discusses the mechanisms and strategies the brain uses to optimize visual search: the psychophysical evidence, their neural correlates, and if unknown, possible loci of the neural computations. Mechanisms and strategies include use of knowledge about the target, distractor, background statistical properties, location probabilities, contextual cues, scene context, rewards, target prevalence, and also the role of saliency, center-surround organization of search templates, and eye movement plans. I provide overviews of classic and contemporary theories of covert attention and eye movements during search explaining their differences and similarities. To allow the reader to anchor some of the laboratory findings to real-world tasks, the article includes interviews with three expert searchers: a radiologist, a fisherman, and a satellite image analyst.",
            "referenceCount": 334,
            "citationCount": 409,
            "influentialCitationCount": 23,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2011-05-01",
            "journal": {
                "name": "Journal of vision",
                "volume": "11 5"
            },
            "citationStyles": {
                "bibtex": "@Article{Eckstein2011VisualSA,\n author = {M. Eckstein},\n booktitle = {Journal of Vision},\n journal = {Journal of vision},\n title = {Visual search: a retrospective.},\n volume = {11 5},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5566d1ed71b637d5236e90609c1b1c614367de1c",
            "@type": "ScholarlyArticle",
            "paperId": "5566d1ed71b637d5236e90609c1b1c614367de1c",
            "corpusId": 14635034,
            "url": "https://www.semanticscholar.org/paper/5566d1ed71b637d5236e90609c1b1c614367de1c",
            "title": "Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/JayasumanaHSLH14",
                "ArXiv": "1412.0265",
                "MAG": "2103096501",
                "DOI": "10.1109/TPAMI.2015.2414422",
                "CorpusId": 14635034,
                "PubMed": "26539851"
            },
            "abstract": "In this paper, we develop an approach to exploiting kernel methods with manifold-valued data. In many computer vision problems, the data can be naturally represented as points on a Riemannian manifold. Due to the non-Euclidean geometry of Riemannian manifolds, usual Euclidean computer vision and machine learning algorithms yield inferior results on such data. In this paper, we define Gaussian radial basis function (RBF)-based positive definite kernels on manifolds that permit us to embed a given manifold with a corresponding metric in a high dimensional reproducing kernel Hilbert space. These kernels make it possible to utilize algorithms developed for linear spaces on nonlinear manifold-valued data. Since the Gaussian RBF defined with any given metric is not always positive definite, we present a unified framework for analyzing the positive definiteness of the Gaussian RBF on a generic metric space. We then use the proposed framework to identify positive definite kernels on two specific manifolds commonly encountered in computer vision: the Riemannian manifold of symmetric positive definite matrices and the Grassmann manifold, i.e., the Riemannian manifold of linear subspaces of a Euclidean space. We show that many popular algorithms designed for Euclidean spaces, such as support vector machines, discriminant analysis and principal component analysis can be generalized to Riemannian manifolds with the help of such positive definite Gaussian kernels.",
            "referenceCount": 56,
            "citationCount": 202,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1412.0265",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-11-30",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Jayasumana2014KernelMO,\n author = {Sadeep Jayasumana and R. Hartley and M. Salzmann and Hongdong Li and M. Harandi},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2464-2477},\n title = {Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels},\n volume = {37},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:954da573e1268c43480684d5c59e89f8d8f3ce42",
            "@type": "ScholarlyArticle",
            "paperId": "954da573e1268c43480684d5c59e89f8d8f3ce42",
            "corpusId": 18725313,
            "url": "https://www.semanticscholar.org/paper/954da573e1268c43480684d5c59e89f8d8f3ce42",
            "title": "Yet Another Survey on Image Segmentation: Region and Boundary Information Integration",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1498238238",
                "DBLP": "conf/eccv/FreixenetMRMC02",
                "DOI": "10.1007/3-540-47977-5_27",
                "CorpusId": 18725313
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 591,
            "influentialCitationCount": 63,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/3-540-47977-5_27.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference",
                "Review"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Freixenet2002YetAS,\n author = {J. Freixenet and X. Mu\u00f1oz and D. Raba and J. Mart\u00ed and X. Cuf\u00ed},\n booktitle = {European Conference on Computer Vision},\n pages = {408-422},\n title = {Yet Another Survey on Image Segmentation: Region and Boundary Information Integration},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47272d33efa13809d26d3f4222a696acc17ecf30",
            "@type": "ScholarlyArticle",
            "paperId": "47272d33efa13809d26d3f4222a696acc17ecf30",
            "corpusId": 897936,
            "url": "https://www.semanticscholar.org/paper/47272d33efa13809d26d3f4222a696acc17ecf30",
            "title": "Shape Priors for Level Set Representations",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "1480873147",
                "DBLP": "conf/eccv/RoussonP02",
                "DOI": "10.1007/3-540-47967-8_6",
                "CorpusId": 897936
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 601,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2002-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Rousson2002ShapePF,\n author = {Mika\u00ebl Rousson and N. Paragios},\n booktitle = {European Conference on Computer Vision},\n pages = {78-92},\n title = {Shape Priors for Level Set Representations},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb240b6b25044e251ec568611546c9954e0625a0",
            "@type": "ScholarlyArticle",
            "paperId": "bb240b6b25044e251ec568611546c9954e0625a0",
            "corpusId": 16214807,
            "url": "https://www.semanticscholar.org/paper/bb240b6b25044e251ec568611546c9954e0625a0",
            "title": "Virtual object manipulation on a table-top AR environment",
            "venue": "Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2000,
            "externalIds": {
                "MAG": "2114261446",
                "DBLP": "conf/ismar/KatoBPIT00",
                "DOI": "10.1109/ISAR.2000.880934",
                "CorpusId": 16214807
            },
            "abstract": "We address the problems of virtual object interaction and user tracking in a table-top augmented reality (AR) interface. In this setting there is a need for very accurate tracking and registration techniques and an intuitive and useful interface. This is especially true in AR interfaces for supporting face to face collaboration where users need to be able to easily cooperate with each other. We describe an accurate vision-based tracking method for table-top AR environments and tangible user interface (TUI) techniques based on this method that allow users to manipulate virtual objects in a natural and intuitive manner. Our approach is robust, allowing users to cover some of the tracking markers while still returning camera viewpoint information, overcoming one of the limitations of traditional computer vision based systems. After describing this technique we describe its use in prototype AR applications.",
            "referenceCount": 20,
            "citationCount": 662,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ivanpoupyrev.com/e-library/2000/ISAR2000_tabletop.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2000-10-05",
            "journal": {
                "name": "Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kato2000VirtualOM,\n author = {H. Kato and M. Billinghurst and I. Poupyrev and K. Imamoto and K. Tachibana},\n booktitle = {Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)},\n journal = {Proceedings IEEE and ACM International Symposium on Augmented Reality (ISAR 2000)},\n pages = {111-119},\n title = {Virtual object manipulation on a table-top AR environment},\n year = {2000}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aeb621ca3170a24a8306979c95367762d5fce924",
            "@type": "ScholarlyArticle",
            "paperId": "aeb621ca3170a24a8306979c95367762d5fce924",
            "corpusId": 18470736,
            "url": "https://www.semanticscholar.org/paper/aeb621ca3170a24a8306979c95367762d5fce924",
            "title": "Deep Learning applied to NLP",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/LopezK17",
                "MAG": "2604652676",
                "ArXiv": "1703.03091",
                "CorpusId": 18470736
            },
            "abstract": "Convolutional Neural Network (CNNs) are typically associated with Computer Vision. CNNs are responsible for major breakthroughs in Image Classification and are the core of most Computer Vision systems today. More recently CNNs have been applied to problems in Natural Language Processing and gotten some interesting results. In this paper, we will try to explain the basics of CNNs, its different variations and how they have been applied to NLP.",
            "referenceCount": 32,
            "citationCount": 133,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.03091"
            },
            "citationStyles": {
                "bibtex": "@Article{Lopez2017DeepLA,\n author = {Marc Moreno Lopez and J. Kalita},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Learning applied to NLP},\n volume = {abs/1703.03091},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9df38dcbc464e42bd61c8c086e90beafba4de5d2",
            "@type": "ScholarlyArticle",
            "paperId": "9df38dcbc464e42bd61c8c086e90beafba4de5d2",
            "corpusId": 29505340,
            "url": "https://www.semanticscholar.org/paper/9df38dcbc464e42bd61c8c086e90beafba4de5d2",
            "title": "On Human Action",
            "venue": "Visual Analysis of Humans",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "books/daglib/p/BobickK11",
                "MAG": "2174681608",
                "DOI": "10.1007/978-0-85729-997-0_14",
                "CorpusId": 29505340
            },
            "abstract": null,
            "referenceCount": 57,
            "citationCount": 343,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bobick2011OnHA,\n author = {A. Bobick and V. Kr\u00fcger},\n booktitle = {Visual Analysis of Humans},\n pages = {279-288},\n title = {On Human Action},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:68166aec5176b2f2265d66a8902706265ec7e669",
            "@type": "ScholarlyArticle",
            "paperId": "68166aec5176b2f2265d66a8902706265ec7e669",
            "corpusId": 12925372,
            "url": "https://www.semanticscholar.org/paper/68166aec5176b2f2265d66a8902706265ec7e669",
            "title": "Spectral Curvature Clustering (SCC)",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2013712253",
                "DBLP": "journals/ijcv/ChenL09",
                "DOI": "10.1007/s11263-008-0178-9",
                "CorpusId": 12925372
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 382,
            "influentialCitationCount": 51,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/s11263-008-0178-9.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-03-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "81"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2009SpectralCC,\n author = {Guangliang Chen and Gilad Lerman},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {317-330},\n title = {Spectral Curvature Clustering (SCC)},\n volume = {81},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ded4013ce19fa4aa058d709097bf71b7de18b211",
            "@type": "ScholarlyArticle",
            "paperId": "ded4013ce19fa4aa058d709097bf71b7de18b211",
            "corpusId": 6031226,
            "url": "https://www.semanticscholar.org/paper/ded4013ce19fa4aa058d709097bf71b7de18b211",
            "title": "Survey of Polygonal Surface Simplification Algorithms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "1562281518",
                "CorpusId": 6031226
            },
            "abstract": "Abstract : This paper surveys methods for simplifying and approximating polygonal surfaces. A polygonal surface is a piecewise-linear surface in 3-D defined by a set of polygons; typically a set of triangles. Methods from computer graphics, computer vision, cartography, computational geometry, and other fields are classified, summarized, and compared both practically and theoretically. The surface types range from height fields (bivariate functions), to manifolds, to non-manifold self-intersecting surfaces. Piecewise-linear curve simplification is also briefly surveyed.",
            "referenceCount": 128,
            "citationCount": 661,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1997-05-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Heckbert1997SurveyOP,\n author = {Paul S. Heckbert and Michael Garland},\n title = {Survey of Polygonal Surface Simplification Algorithms},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:281ec4ef1bf7a61aa24120dc4305517b7e3b0a7e",
            "@type": "ScholarlyArticle",
            "paperId": "281ec4ef1bf7a61aa24120dc4305517b7e3b0a7e",
            "corpusId": 61776097,
            "url": "https://www.semanticscholar.org/paper/281ec4ef1bf7a61aa24120dc4305517b7e3b0a7e",
            "title": "Wavelets for a vision",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 1996,
            "externalIds": {
                "MAG": "2128848833",
                "DBLP": "journals/pieee/Mallat96",
                "DOI": "10.1109/5.488702",
                "CorpusId": 61776097
            },
            "abstract": "Early on, computer vision researchers have realized that multiscale transforms are important to analyze the information content of images. The wavelet theory gives a stable mathematical foundation to understand the properties of such multiscale algorithms. This tutorial describes major applications to multiresolution search, multiscale edge detection, and texture discrimination.",
            "referenceCount": 29,
            "citationCount": 372,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1996-04-01",
            "journal": {
                "name": "Proc. IEEE",
                "volume": "84"
            },
            "citationStyles": {
                "bibtex": "@Article{Mallat1996WaveletsFA,\n author = {S. Mallat},\n booktitle = {Proceedings of the IEEE},\n journal = {Proc. IEEE},\n pages = {604-614},\n title = {Wavelets for a vision},\n volume = {84},\n year = {1996}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3a58dedf7192e991b74145da928cae679d0998ce",
            "@type": "ScholarlyArticle",
            "paperId": "3a58dedf7192e991b74145da928cae679d0998ce",
            "corpusId": 39646931,
            "url": "https://www.semanticscholar.org/paper/3a58dedf7192e991b74145da928cae679d0998ce",
            "title": "Survey of Convolutional Neural Network",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2909630715",
                "CorpusId": 39646931
            },
            "abstract": "Convolutional Neural Network (CNN) was \ufb01rstly introduced in Computer Vision for image recognition by LeCun et al . in 1989. Since then, it has been widely used in image recognition and classi\ufb01cation tasks. The recent impressive success of Krizhevsky et al . in ILSVRC 2012 competition demonstrates the signi\ufb01cant advance of modern deep CNN on image classi\ufb01cation task. Inspired by his work, many recent research works have been concentrat-ing on understanding CNN and extending its application to more conventional computer vision tasks. Their successes and lessons have promoted the development of both CNN and vision science. This article makes a survey of recent progress in CNN since 2012. We will introduce the general architecture of a modern CNN and make insights into several typical CNN incarnations which have been studied extensively. We will also review the efforts to understand CNNs and review important applications of CNNs in computer vision tasks.",
            "referenceCount": 65,
            "citationCount": 151,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Fan2016SurveyOC,\n author = {Chenyou Fan},\n title = {Survey of Convolutional Neural Network},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ecb89cd2d7d3f8822abd647b448d187c0e73f3ad",
            "@type": "ScholarlyArticle",
            "paperId": "ecb89cd2d7d3f8822abd647b448d187c0e73f3ad",
            "corpusId": 18425438,
            "url": "https://www.semanticscholar.org/paper/ecb89cd2d7d3f8822abd647b448d187c0e73f3ad",
            "title": "Estimating the tensor of curvature of a surface from a polyhedral approximation",
            "venue": "Proceedings of IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "DBLP": "conf/iccv/Taubin95a",
                "MAG": "2125685777",
                "DOI": "10.1109/ICCV.1995.466840",
                "CorpusId": 18425438
            },
            "abstract": "Estimating principal curvatures and principal directions of a surface from a polyhedral approximation with a large number of small faces, such as those produced by iso-surface construction algorithms, has become a basic step in many computer vision algorithms, particularly in those targeted at medical applications. We describe a method to estimate the tensor of curvature of a surface at the vertices of a polyhedral approximation. Principal curvatures and principal directions are obtained by computing in closed form the eigenvalues and eigenvectors of certain 3/spl times/3 symmetric matrices defined by integral formulas, and closely related to the matrix representation of the tensor of curvature. The resulting algorithm is linear, both in time and in space, as a function of the number of vertices and faces of the polyhedral surface.<<ETX>>",
            "referenceCount": 16,
            "citationCount": 661,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1995-06-20",
            "journal": {
                "name": "Proceedings of IEEE International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Taubin1995EstimatingTT,\n author = {G. Taubin},\n booktitle = {Proceedings of IEEE International Conference on Computer Vision},\n journal = {Proceedings of IEEE International Conference on Computer Vision},\n pages = {902-907},\n title = {Estimating the tensor of curvature of a surface from a polyhedral approximation},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:94ce530ac467e66d402ce05c6a4975393812c7f2",
            "@type": "ScholarlyArticle",
            "paperId": "94ce530ac467e66d402ce05c6a4975393812c7f2",
            "corpusId": 53893520,
            "url": "https://www.semanticscholar.org/paper/94ce530ac467e66d402ce05c6a4975393812c7f2",
            "title": "Readings in human-computer interaction : toward the year 2000",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1995,
            "externalIds": {
                "MAG": "1496263431",
                "CorpusId": 53893520
            },
            "abstract": "Chapter 1 A Historical and Intellectual Perspective Chapter 2 Design and Evaluation Chapter 3 Considering Work Contexts in Design Chapter 4 Software Development Contexts Chapter 5 Development Tools Chapter 6 Vision, Graphic Design, and Visual Display Chapter 7 Touch, Gesture, and Marking Chapter 8 Speech, Language , and Audition Chapter 9 Human Information Processing Chapter 10 Designing to Fit Human Capabilities Chapter 11 Groupware and Computer-Supported Work Chapter 12 From Customizable Systems to Intelligent Agents Chapter 13 Hypertext and Multimedia Chapter 14 Cyberspace",
            "referenceCount": 0,
            "citationCount": 477,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Baecker1995ReadingsIH,\n author = {R. Baecker},\n title = {Readings in human-computer interaction : toward the year 2000},\n year = {1995}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ca2e917e2f399010da6bb9ba989c01fd406ddcb6",
            "@type": "ScholarlyArticle",
            "paperId": "ca2e917e2f399010da6bb9ba989c01fd406ddcb6",
            "corpusId": 5918812,
            "url": "https://www.semanticscholar.org/paper/ca2e917e2f399010da6bb9ba989c01fd406ddcb6",
            "title": "From Gestalt Theory to Image Analysis: A Probabilistic Approach",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "1533452257",
                "CorpusId": 5918812
            },
            "abstract": "This book introduces the reader to a recent theory in Computer Vision yielding elementary techniques to analyse digital images. These techniques are inspired from and are a mathematical formalization of the Gestalt theory. Gestalt theory, which had never been formalized is a rigorous realm of vision psychology developped between 1923 and 1975. From the mathematical viewpoint the closest field to it is stochastic geometry, involving basic probability and statistics, in the context of image analysis. The authors maintain a public software, MegaWave, containing implementations of most of the image analysis techniques developped in the book. The book is intended for researchers and engineers. It is mathematically self-contained and requires only the basic notions in probability and calculus.",
            "referenceCount": 188,
            "citationCount": 445,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2007-12-18",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Desolneux2007FromGT,\n author = {A. Desolneux and L. Moisan and J. Morel},\n title = {From Gestalt Theory to Image Analysis: A Probabilistic Approach},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1d6e55cfb0a61ddd6cc326aeeac513f553e565c9",
            "@type": "ScholarlyArticle",
            "paperId": "1d6e55cfb0a61ddd6cc326aeeac513f553e565c9",
            "corpusId": 9819258,
            "url": "https://www.semanticscholar.org/paper/1d6e55cfb0a61ddd6cc326aeeac513f553e565c9",
            "title": "Relative neighborhood graphs and their relatives",
            "venue": "Proceedings of the IEEE",
            "publicationVenue": {
                "id": "urn:research:6faaccca-1cc4-45a9-aeb6-96a4901d2606",
                "name": "Proceedings of the IEEE",
                "alternate_names": [
                    "Proc IEEE"
                ],
                "issn": "0018-9219",
                "url": "http://www.ieee.org/portal/pages/pubs/proceedings/"
            },
            "year": 1992,
            "externalIds": {
                "DBLP": "journals/pieee/JaromczykT92",
                "MAG": "2163227453",
                "DOI": "10.1109/5.163414",
                "CorpusId": 9819258
            },
            "abstract": "Results of neighborhood graphs are surveyed. Properties, bounds on the size, algorithms, and variants of the neighborhood graphs are discussed. Numerous applications including computational morphology, spatial analysis, pattern classification, and databases for computer vision are described. >",
            "referenceCount": 106,
            "citationCount": 655,
            "influentialCitationCount": 47,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Proc. IEEE",
                "volume": "80"
            },
            "citationStyles": {
                "bibtex": "@Article{Jaromczyk1992RelativeNG,\n author = {J. Jaromczyk and G. Toussaint},\n booktitle = {Proceedings of the IEEE},\n journal = {Proc. IEEE},\n pages = {1502-1517},\n title = {Relative neighborhood graphs and their relatives},\n volume = {80},\n year = {1992}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c54292843486dbc3555186184b7c931b0c405d96",
            "@type": "ScholarlyArticle",
            "paperId": "c54292843486dbc3555186184b7c931b0c405d96",
            "corpusId": 6681074,
            "url": "https://www.semanticscholar.org/paper/c54292843486dbc3555186184b7c931b0c405d96",
            "title": "A Survey Of Free-Form Object Representation and Recognition Techniques",
            "venue": "Computer Vision and Image Understanding",
            "publicationVenue": {
                "id": "urn:research:5fbb417b-d7a5-44e6-856d-993f0624ed9c",
                "name": "Computer Vision and Image Understanding",
                "alternate_names": [
                    "Comput Vis Image Underst"
                ],
                "issn": "1077-3142",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/622809/description#description"
            },
            "year": 2001,
            "externalIds": {
                "MAG": "2144473577",
                "DBLP": "journals/cviu/CampbellF01",
                "DOI": "10.1006/cviu.2000.0889",
                "CorpusId": 6681074
            },
            "abstract": "Advances in computer speed, memory capacity, and hardware graphics acceleration have made the interactive manipulation and visualization of complex, detailed (and therefore large) three-dimensional models feasible. These models are either painstakingly designed through an elaborate CAD process or reverse engineered from sculpted prototypes using modern scanning technologies and integration methods. The availability of detailed data describing the shape of an object offers the computer vision practitioner new ways to recognize and localize free-form objects. This survey reviews recent literature on both the 3D model building process and techniques used to match and identify free-form objects from imagery.",
            "referenceCount": 130,
            "citationCount": 613,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2001-02-01",
            "journal": {
                "name": "Comput. Vis. Image Underst.",
                "volume": "81"
            },
            "citationStyles": {
                "bibtex": "@Article{Campbell2001ASO,\n author = {Richard J. Campbell and P. Flynn},\n booktitle = {Computer Vision and Image Understanding},\n journal = {Comput. Vis. Image Underst.},\n pages = {166-210},\n title = {A Survey Of Free-Form Object Representation and Recognition Techniques},\n volume = {81},\n year = {2001}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3efe88739c2604d7fede2cf1da977670dd7968e6",
            "@type": "ScholarlyArticle",
            "paperId": "3efe88739c2604d7fede2cf1da977670dd7968e6",
            "corpusId": 11915313,
            "url": "https://www.semanticscholar.org/paper/3efe88739c2604d7fede2cf1da977670dd7968e6",
            "title": "Geometric hashing: an overview",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1997,
            "externalIds": {
                "MAG": "2144071905",
                "DOI": "10.1109/99.641604",
                "CorpusId": 11915313
            },
            "abstract": "Geometric hashing, a technique originally developed in computer vision for matching geometric features against a database of such features, finds use in a number of other areas. Matching is possible even when the recognizable database objects have undergone transformations or when only partial information is present. The technique is highly efficient and of low polynomial complexity.",
            "referenceCount": 28,
            "citationCount": 629,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1997-10-01",
            "journal": {
                "name": "",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Wolfson1997GeometricHA,\n author = {H. Wolfson and I. Rigoutsos},\n pages = {10-21},\n title = {Geometric hashing: an overview},\n volume = {4},\n year = {1997}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fd61a8d6ebd6109a63bf2c68fa19731495a573ac",
            "@type": "ScholarlyArticle",
            "paperId": "fd61a8d6ebd6109a63bf2c68fa19731495a573ac",
            "corpusId": 10529675,
            "url": "https://www.semanticscholar.org/paper/fd61a8d6ebd6109a63bf2c68fa19731495a573ac",
            "title": "Real-Time Eye, Gaze, and Face Pose Tracking for Monitoring Driver Vigilance",
            "venue": "Real-time imaging",
            "publicationVenue": {
                "id": "urn:research:ce6d0a88-c09e-48fb-9582-d39c25f72a30",
                "name": "Real-time imaging",
                "alternate_names": [
                    "Real-time Imaging"
                ],
                "issn": "1077-2014",
                "url": "http://www.sciencedirect.com/science/journal/10772014"
            },
            "year": 2002,
            "externalIds": {
                "MAG": "2155217597",
                "DBLP": "journals/rti/JiY02",
                "DOI": "10.1006/rtim.2002.0279",
                "CorpusId": 10529675
            },
            "abstract": "This paper describes a real-time prototype computer vision system for monitoring driver vigilance. The main components of the system consists of a remotely located video CCD camera, a specially designed hardware system for real-time image acquisition and for controlling the illuminator and the alarm system, and various computer vision algorithms for simultaneously, real-time and non-intrusively monitoring various visual bio-behaviors that typically characterize a driver's level of vigilance. The visual behaviors include eyelid movement, face orientation, and gaze movement (pupil movement). The system was tested in a simulating environment with subjects of different ethnic backgrounds, different genders, ages, with/without glasses, and under different illumination conditions, and it was found very robust, reliable and accurate.",
            "referenceCount": 31,
            "citationCount": 586,
            "influentialCitationCount": 22,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.ecse.rpi.edu/~qji/rti.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2002-10-01",
            "journal": {
                "name": "Real Time Imaging",
                "volume": "8"
            },
            "citationStyles": {
                "bibtex": "@Article{Ji2002RealTimeEG,\n author = {Q. Ji and Xiaojie Yang},\n booktitle = {Real-time imaging},\n journal = {Real Time Imaging},\n pages = {357-377},\n title = {Real-Time Eye, Gaze, and Face Pose Tracking for Monitoring Driver Vigilance},\n volume = {8},\n year = {2002}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:41dd3da0216e5f47511c2e7413089043c11295f6",
            "@type": "ScholarlyArticle",
            "paperId": "41dd3da0216e5f47511c2e7413089043c11295f6",
            "corpusId": 15319364,
            "url": "https://www.semanticscholar.org/paper/41dd3da0216e5f47511c2e7413089043c11295f6",
            "title": "Minimizing Nonsubmodular Functions with Graph Cuts-A Review",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "journals/pami/KolmogorovR07",
                "MAG": "2153396823",
                "DOI": "10.1109/TPAMI.2007.1031",
                "CorpusId": 15319364,
                "PubMed": "17496384"
            },
            "abstract": "Optimization techniques based on graph cuts have become a standard tool for many vision applications. These techniques allow to minimize efficiently certain energy functions corresponding to pairwise Markov random fields (MRFs). Currently, there is an accepted view within the computer vision community that graph cuts can only be used for optimizing a limited class of MRF energies (e.g., submodular functions). In this survey, we review some results that show that graph cuts can be applied to a much larger class of energy functions (in particular, nonsubmodular functions). While these results are well-known in the optimization community, to our knowledge they were not used in the context of computer vision and MRF optimization. We demonstrate the relevance of these results to vision on the problem of binary texture restoration.",
            "referenceCount": 44,
            "citationCount": 422,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2007-07-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Kolmogorov2007MinimizingNF,\n author = {V. Kolmogorov and C. Rother},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n title = {Minimizing Nonsubmodular Functions with Graph Cuts-A Review},\n volume = {29},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bdb04bd534a911b40ac24c69d62b26b89e9c2810",
            "@type": "ScholarlyArticle",
            "paperId": "bdb04bd534a911b40ac24c69d62b26b89e9c2810",
            "corpusId": 1447246,
            "url": "https://www.semanticscholar.org/paper/bdb04bd534a911b40ac24c69d62b26b89e9c2810",
            "title": "Review and analysis of solutions of the three point perspective pose estimation problem",
            "venue": "International Journal of Computer Vision",
            "publicationVenue": {
                "id": "urn:research:939ee07c-6009-43f8-b884-69238b40659e",
                "name": "International Journal of Computer Vision",
                "alternate_names": [
                    "Int J Comput Vis"
                ],
                "issn": "0920-5691",
                "url": "https://www.springer.com/computer/image+processing/journal/11263"
            },
            "year": 1994,
            "externalIds": {
                "DBLP": "journals/ijcv/HaralickLON94",
                "MAG": "2085078372",
                "DOI": "10.1007/BF02028352",
                "CorpusId": 1447246
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 604,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Geography",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Geography",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "1994-12-01",
            "journal": {
                "name": "International Journal of Computer Vision",
                "volume": "13"
            },
            "citationStyles": {
                "bibtex": "@Article{Haralick1994ReviewAA,\n author = {R. Haralick and Chung-Nan Lee and K. Ottenberg and M. N\u00f6lle},\n booktitle = {International Journal of Computer Vision},\n journal = {International Journal of Computer Vision},\n pages = {331-356},\n title = {Review and analysis of solutions of the three point perspective pose estimation problem},\n volume = {13},\n year = {1994}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c0979c590670afe4741749f44b318c62ca7b6e8",
            "@type": "ScholarlyArticle",
            "paperId": "4c0979c590670afe4741749f44b318c62ca7b6e8",
            "corpusId": 54154592,
            "url": "https://www.semanticscholar.org/paper/4c0979c590670afe4741749f44b318c62ca7b6e8",
            "title": "Purposive and qualitative active vision",
            "venue": "[1990] Proceedings. 10th International Conference on Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1990,
            "externalIds": {
                "MAG": "1622015710",
                "DBLP": "conf/icpr/Aloimonos90",
                "DOI": "10.1109/ICPR.1990.118128",
                "CorpusId": 54154592
            },
            "abstract": "The traditional view of the problem of computer vision as a recovery problem is questioned, and the paradigm of purposive-qualitative vision is offered as an alternative. This paradigm considers vision as a general recognition problem (recognition of objects, patterns or situations). To demonstrate the usefulness of the framework, the design of the Medusa of CVL is described. It is noted that this machine can perform complex visual tasks without reconstructing the world. If it is provided with intentions, knowledge of the environment, and planning capabilities, it can perform highly sophisticated navigational tasks. It is explained why the traditional structure from motion problem cannot be solved in some cases and why there is reason to be pessimistic about the optimal performance of a structure from motion module. New directions for future research on this problem in the recovery paradigm, e.g., research on stability or robustness, are suggested.<<ETX>>",
            "referenceCount": 19,
            "citationCount": 383,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1990-06-16",
            "journal": {
                "name": "[1990] Proceedings. 10th International Conference on Pattern Recognition",
                "volume": "i"
            },
            "citationStyles": {
                "bibtex": "@Article{Aloimonos1990PurposiveAQ,\n author = {Y. Aloimonos},\n booktitle = {[1990] Proceedings. 10th International Conference on Pattern Recognition},\n journal = {[1990] Proceedings. 10th International Conference on Pattern Recognition},\n pages = {346-360 vol.1},\n title = {Purposive and qualitative active vision},\n volume = {i},\n year = {1990}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:994d34d2c1d7195ccc14c358da4d2ce3dfb600c3",
            "@type": "ScholarlyArticle",
            "paperId": "994d34d2c1d7195ccc14c358da4d2ce3dfb600c3",
            "corpusId": 10128146,
            "url": "https://www.semanticscholar.org/paper/994d34d2c1d7195ccc14c358da4d2ce3dfb600c3",
            "title": "Contextual classification with functional Max-Margin Markov Networks",
            "venue": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "DBLP": "conf/cvpr/MunozBVH09",
                "MAG": "2159213092",
                "DOI": "10.1109/CVPR.2009.5206590",
                "CorpusId": 10128146
            },
            "abstract": "We address the problem of label assignment in computer vision: given a novel 3D or 2D scene, we wish to assign a unique label to every site (voxel, pixel, superpixel, etc.). To this end, the Markov Random Field framework has proven to be a model of choice as it uses contextual information to yield improved classification results over locally independent classifiers. In this work we adapt a functional gradient approach for learning high-dimensional parameters of random fields in order to perform discrete, multi-label classification. With this approach we can learn robust models involving high-order interactions better than the previously used learning method. We validate the approach in the context of point cloud classification and improve the state of the art. In addition, we successfully demonstrate the generality of the approach on the challenging vision problem of recovering 3-D geometric surfaces from images.",
            "referenceCount": 29,
            "citationCount": 335,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://figshare.com/articles/journal_contribution/Contextual_Classification_with_Functional_Max-Margin_Markov_Networks/6552392/1/files/12033704.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2009-06-20",
            "journal": {
                "name": "2009 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Munoz2009ContextualCW,\n author = {Daniel Munoz and Andrew Bagnell},\n booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},\n pages = {975-982},\n title = {Contextual classification with functional Max-Margin Markov Networks},\n year = {2009}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e254d0e670ce8217e74aaf1421c72e63e62929e3",
            "@type": "ScholarlyArticle",
            "paperId": "e254d0e670ce8217e74aaf1421c72e63e62929e3",
            "corpusId": 3169874,
            "url": "https://www.semanticscholar.org/paper/e254d0e670ce8217e74aaf1421c72e63e62929e3",
            "title": "Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "journals/pami/MatheS15",
                "MAG": "2071555787",
                "ArXiv": "1312.7570",
                "DOI": "10.1109/TPAMI.2014.2366154",
                "CorpusId": 3169874,
                "PubMed": "26352449"
            },
            "abstract": "Systems based on bag-of-words models from image features collected at maxima of sparse interest point operators have been used successfully for both computer visual object and action recognition tasks. While the sparse, interest-point based approach to recognition is not inconsistent with visual processing in biological systems that operate in `saccade and fixate' regimes, the methodology and emphasis in the human and the computer vision communities remains sharply distinct. Here, we make three contributions aiming to bridge this gap. First, we complement existing state-of-the art large scale dynamic computer vision annotated datasets like Hollywood-2 [1] and UCF Sports [2] with human eye movements collected under the ecological constraints of visual action and scene context recognition tasks. To our knowledge these are the first large human eye tracking datasets to be collected and made publicly available for video, vision.imar.ro/eyetracking (497,107 frames, each viewed by 19 subjects), unique in terms of their (a) large scale and computer vision relevance, (b) dynamic, video stimuli, (c) task control, as well as free-viewing. Second, we introduce novel dynamic consistency and alignment measures, which underline the remarkable stability of patterns of visual search among subjects. Third, we leverage the significant amount of collected data in order to pursue studies and build automatic, end-to-end trainable computer vision systems based on human eye movements. Our studies not only shed light on the differences between computer vision spatio-temporal interest point image sampling strategies and the human fixations, as well as their impact for visual recognition performance, but also demonstrate that human fixations can be accurately predicted, and when used in an end-to-end automatic system, leveraging some of the advanced computer vision practice, can lead to state of the art results.",
            "referenceCount": 75,
            "citationCount": 172,
            "influentialCitationCount": 39,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1312.7570",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2013-12-29",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Mathe2013ActionsIT,\n author = {Stefan Mathe and C. Sminchisescu},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {1408-1424},\n title = {Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition},\n volume = {37},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dccdfea475faa2ecee679803a8b4177ea7b5384c",
            "@type": "ScholarlyArticle",
            "paperId": "dccdfea475faa2ecee679803a8b4177ea7b5384c",
            "corpusId": 14024984,
            "url": "https://www.semanticscholar.org/paper/dccdfea475faa2ecee679803a8b4177ea7b5384c",
            "title": "Recognizing People by Their Gait: The Shape of Motion",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1998,
            "externalIds": {
                "MAG": "1558751150",
                "CorpusId": 14024984
            },
            "abstract": "Videre: Journal of Computer Vision Research (ISSN 1089-2788) is a quarterly journal published electronically on the Internet by The MIT Press, Cambridge, Massachusetts, 02142. Subscriptions and address changes should be addressed to MIT Press Journals, Five Cambridge Center, Cambridge, MA 02142; phone: (617) 253-2889; fax: (617) 577-1545; e-mail: journals-orders@mit.edu. Subscription rates are: Individuals $30.00, Institutions $125.00. Canadians add additional 7% GST. Prices subject to change without notice.",
            "referenceCount": 42,
            "citationCount": 568,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Little1998RecognizingPB,\n author = {J. Little and J. Boyd},\n title = {Recognizing People by Their Gait: The Shape of Motion},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1dd68d85acc5cde4a0dee33aa052f65388eda6b",
            "@type": "ScholarlyArticle",
            "paperId": "b1dd68d85acc5cde4a0dee33aa052f65388eda6b",
            "corpusId": 7708101,
            "url": "https://www.semanticscholar.org/paper/b1dd68d85acc5cde4a0dee33aa052f65388eda6b",
            "title": "Learning to Rank Using Privileged Information",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2126942721",
                "DBLP": "conf/iccv/SharmanskaQL13",
                "DOI": "10.1109/ICCV.2013.107",
                "CorpusId": 7708101
            },
            "abstract": "Many computer vision problems have an asymmetric distribution of information between training and test time. In this work, we study the case where we are given additional information about the training data, which however will not be available at test time. This situation is called learning using privileged information (LUPI). We introduce two maximum-margin techniques that are able to make use of this additional source of information, and we show that the framework is applicable to several scenarios that have been studied in computer vision before. Experiments with attributes, bounding boxes, image tags and rationales as additional information in object classification show promising results.",
            "referenceCount": 33,
            "citationCount": 182,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://openaccess.thecvf.com/content_iccv_2013/papers/Sharmanska_Learning_to_Rank_2013_ICCV_paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-01",
            "journal": {
                "name": "2013 IEEE International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sharmanska2013LearningTR,\n author = {V. Sharmanska and Novi Quadrianto and Christoph H. Lampert},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2013 IEEE International Conference on Computer Vision},\n pages = {825-832},\n title = {Learning to Rank Using Privileged Information},\n year = {2013}\n}\n"
            }
        }
    }
]