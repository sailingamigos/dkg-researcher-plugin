[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
            "@type": "ScholarlyArticle",
            "paperId": "17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
            "corpusId": 6126582,
            "url": "https://www.semanticscholar.org/paper/17f5c7411eeeeedf25b0db99a9130aa353aee4ba",
            "title": "Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/aaai/SerbanSBCP16",
                "MAG": "2951834849",
                "ArXiv": "1507.04808",
                "DOI": "10.1609/aaai.v30i1.9883",
                "CorpusId": 6126582
            },
            "abstract": "\n \n We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.\n \n",
            "referenceCount": 54,
            "citationCount": 1659,
            "influentialCitationCount": 214,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/9883/9742",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-07-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Serban2015BuildingED,\n author = {Iulian Serban and Alessandro Sordoni and Yoshua Bengio and Aaron C. Courville and Joelle Pineau},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3776-3784},\n title = {Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b86b08ccc78b8c19fa462dff789433daf088c50a",
            "@type": "ScholarlyArticle",
            "paperId": "b86b08ccc78b8c19fa462dff789433daf088c50a",
            "corpusId": 232134814,
            "url": "https://www.semanticscholar.org/paper/b86b08ccc78b8c19fa462dff789433daf088c50a",
            "title": "Efficient generative modeling of protein sequences using simple autoregressive models",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2021,
            "externalIds": {
                "PubMedCentral": "8490405",
                "ArXiv": "2103.03292",
                "DOI": "10.1038/s41467-021-25756-4",
                "CorpusId": 232134814,
                "PubMed": "34608136"
            },
            "abstract": null,
            "referenceCount": 101,
            "citationCount": 42,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.nature.com/articles/s41467-021-25756-4.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Biology",
                "Physics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-04",
            "journal": {
                "name": "Nature Communications",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Trinquier2021EfficientGM,\n author = {J. Trinquier and G. Uguzzoni and A. Pagnani and F. Zamponi and M. Weigt},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Efficient generative modeling of protein sequences using simple autoregressive models},\n volume = {12},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3f5ed1e7e552e17275ac89b09b8fa203cfb8b51d",
            "@type": "ScholarlyArticle",
            "paperId": "3f5ed1e7e552e17275ac89b09b8fa203cfb8b51d",
            "corpusId": 244060009,
            "url": "https://www.semanticscholar.org/paper/3f5ed1e7e552e17275ac89b09b8fa203cfb8b51d",
            "title": "Generative Models",
            "venue": "Introduction to Deep Learning for Healthcare",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2021,
            "externalIds": {
                "DOI": "10.1201/9781420010800.ch8",
                "CorpusId": 244060009
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": "Introduction to Deep Learning for Healthcare",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{None,\n booktitle = {Introduction to Deep Learning for Healthcare},\n journal = {Introduction to Deep Learning for Healthcare},\n title = {Generative Models},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e83291498a3bc6b0efe8f9571e9c9ca1811707bd",
            "@type": "ScholarlyArticle",
            "paperId": "e83291498a3bc6b0efe8f9571e9c9ca1811707bd",
            "corpusId": 3313632,
            "url": "https://www.semanticscholar.org/paper/e83291498a3bc6b0efe8f9571e9c9ca1811707bd",
            "title": "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2765233338",
                "ArXiv": "1710.10766",
                "DBLP": "conf/iclr/SongKNEK18",
                "CorpusId": 3313632
            },
            "abstract": "Adversarial perturbations of normal images are usually imperceptible to humans, but they can seriously confuse state-of-the-art machine learning models. What makes them so special in the eyes of image classifiers? In this paper, we show empirically that adversarial examples mainly lie in the low probability regions of the training distribution, regardless of attack types and targeted models. Using statistical hypothesis testing, we find that modern neural density models are surprisingly good at detecting imperceptible image perturbations. Based on this discovery, we devised PixelDefend, a new approach that purifies a maliciously perturbed image by moving it back towards the distribution seen in the training data. The purified image is then run through an unmodified classifier, making our method agnostic to both the classifier and the attacking method. As a result, PixelDefend can be used to protect already deployed models and be combined with other model-specific defenses. Experiments show that our method greatly improves resilience across a wide variety of state-of-the-art attacking methods, increasing accuracy on the strongest attack from 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.",
            "referenceCount": 48,
            "citationCount": 674,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.10766"
            },
            "citationStyles": {
                "bibtex": "@Article{Song2017PixelDefendLG,\n author = {Yang Song and Taesup Kim and Sebastian Nowozin and Stefano Ermon and Nate Kushman},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples},\n volume = {abs/1710.10766},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8653fb8c5f723b61c9ca54129a55d0d878ed5f18",
            "@type": "ScholarlyArticle",
            "paperId": "8653fb8c5f723b61c9ca54129a55d0d878ed5f18",
            "corpusId": 208206203,
            "url": "https://www.semanticscholar.org/paper/8653fb8c5f723b61c9ca54129a55d0d878ed5f18",
            "title": "Randomized SMILES strings improve the quality of molecular generative models",
            "venue": "Journal of Cheminformatics",
            "publicationVenue": {
                "id": "urn:research:fd4675fe-4136-446c-aefd-3658aae698ac",
                "name": "Journal of Cheminformatics",
                "alternate_names": [
                    "J Cheminformatics"
                ],
                "issn": "1758-2946",
                "url": "https://jcheminf.biomedcentral.com/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2989615256",
                "DBLP": "journals/jcheminf/Arus-PousJPBTRC19",
                "PubMedCentral": "6873550",
                "DOI": "10.1186/s13321-019-0393-0",
                "CorpusId": 208206203,
                "PubMed": "33430971"
            },
            "abstract": null,
            "referenceCount": 57,
            "citationCount": 181,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jcheminf.biomedcentral.com/track/pdf/10.1186/s13321-019-0393-0.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-05",
            "journal": {
                "name": "Journal of Cheminformatics",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Ar\u00fas\u2010Pous2019RandomizedSS,\n author = {Josep Ar\u00fas\u2010Pous and Simon Johansson and Oleksii Prykhodko and E. Bjerrum and C. Tyrchan and J. Reymond and Hongming Chen and O. Engkvist},\n booktitle = {Journal of Cheminformatics},\n journal = {Journal of Cheminformatics},\n title = {Randomized SMILES strings improve the quality of molecular generative models},\n volume = {11},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:547711207f25deda86f5a079167c0916e6809ad5",
            "@type": "ScholarlyArticle",
            "paperId": "547711207f25deda86f5a079167c0916e6809ad5",
            "corpusId": 25027985,
            "url": "https://www.semanticscholar.org/paper/547711207f25deda86f5a079167c0916e6809ad5",
            "title": "Analyzing the Training Processes of Deep Generative Models",
            "venue": "IEEE Transactions on Visualization and Computer Graphics",
            "publicationVenue": {
                "id": "urn:research:5e1f6444-5d03-48c7-b202-7f47d492aeae",
                "name": "IEEE Transactions on Visualization and Computer Graphics",
                "alternate_names": [
                    "IEEE Trans Vis Comput Graph"
                ],
                "issn": "1077-2626",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2752332392",
                "DBLP": "journals/tvcg/LiuSCZL18",
                "DOI": "10.1109/TVCG.2017.2744938",
                "CorpusId": 25027985,
                "PubMed": "28866564"
            },
            "abstract": "Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.",
            "referenceCount": 49,
            "citationCount": 137,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "IEEE Transactions on Visualization and Computer Graphics",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2018AnalyzingTT,\n author = {Mengchen Liu and Jiaxin Shi and Kelei Cao and Jun Zhu and Shixia Liu},\n booktitle = {IEEE Transactions on Visualization and Computer Graphics},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {77-87},\n title = {Analyzing the Training Processes of Deep Generative Models},\n volume = {24},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d05d1e83e5f2edf5e9e28486a540ae7e039ee344",
            "@type": "ScholarlyArticle",
            "paperId": "d05d1e83e5f2edf5e9e28486a540ae7e039ee344",
            "corpusId": 195791659,
            "url": "https://www.semanticscholar.org/paper/d05d1e83e5f2edf5e9e28486a540ae7e039ee344",
            "title": "Generative Models for Automatic Chemical Design",
            "venue": "Machine Learning Meets Quantum Physics",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1907-01632",
                "ArXiv": "1907.01632",
                "MAG": "2953641781",
                "DOI": "10.1007/978-3-030-40245-7_21",
                "CorpusId": 195791659
            },
            "abstract": null,
            "referenceCount": 235,
            "citationCount": 73,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1907.01632",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-07-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.01632"
            },
            "citationStyles": {
                "bibtex": "@Article{Schwalbe-Koda2019GenerativeMF,\n author = {Daniel Schwalbe-Koda and Rafael G\u00f3mez-Bombarelli},\n booktitle = {Machine Learning Meets Quantum Physics},\n journal = {ArXiv},\n title = {Generative Models for Automatic Chemical Design},\n volume = {abs/1907.01632},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8",
            "@type": "ScholarlyArticle",
            "paperId": "4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8",
            "corpusId": 86562557,
            "url": "https://www.semanticscholar.org/paper/4c53cd4c5a1536f1cddcd541fec3e1c59822b6b8",
            "title": "Causal deconvolution by algorithmic generative models",
            "venue": "Nature Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:6457124b-39bf-4d02-bff4-73752ff21562",
                "name": "Nature Machine Intelligence",
                "alternate_names": [
                    "Nat Mach Intell"
                ],
                "issn": "2522-5839",
                "url": "https://www.nature.com/natmachintell/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/natmi/ZenilKZT19",
                "MAG": "2904276544",
                "DOI": "10.1038/S42256-018-0005-0",
                "CorpusId": 86562557
            },
            "abstract": null,
            "referenceCount": 50,
            "citationCount": 69,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://repository.kaust.edu.sa/bitstream/10754/630919/1/1802.09904.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "Nature Machine Intelligence",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Zenil2019CausalDB,\n author = {H. Zenil and N. Kiani and Allan A. Zea and J. Tegn\u00e9r},\n booktitle = {Nature Machine Intelligence},\n journal = {Nature Machine Intelligence},\n pages = {58-66},\n title = {Causal deconvolution by algorithmic generative models},\n volume = {1},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c002ceaeee6fb74bebbdc2012a877ba78f1fd4de",
            "@type": "ScholarlyArticle",
            "paperId": "c002ceaeee6fb74bebbdc2012a877ba78f1fd4de",
            "corpusId": 170079056,
            "url": "https://www.semanticscholar.org/paper/c002ceaeee6fb74bebbdc2012a877ba78f1fd4de",
            "title": "Emergence of Object Segmentation in Perturbed Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2947847748",
                "DBLP": "conf/nips/BielskiF19",
                "ArXiv": "1905.12663",
                "CorpusId": 170079056
            },
            "abstract": "We introduce a novel framework to build a model that can learn how to segment objects from a collection of images without any human annotation. Our method builds on the observation that the location of object segments can be perturbed locally relative to a given background without affecting the realism of a scene. Our approach is to first train a generative model of a layered scene. The layered representation consists of a background image, a foreground image and the mask of the foreground. A composite image is then obtained by overlaying the masked foreground image onto the background. The generative model is trained in an adversarial fashion against a discriminator, which forces the generative model to produce realistic composite images. To force the generator to learn a representation where the foreground layer corresponds to an object, we perturb the output of the generative model by introducing a random shift of both the foreground image and mask relative to the background. Because the generator is unaware of the shift before computing its output, it must produce layered representations that are realistic for any such random perturbation. Finally, we learn to segment an image by defining an autoencoder consisting of an encoder, which we train, and the pre-trained generator as the decoder, which we freeze. The encoder maps an image to a feature vector, which is fed as input to the generator to give a composite image matching the original input image. Because the generator outputs an explicit layered representation of the scene, the encoder learns to detect and segment objects. We demonstrate this framework on real images of several object categories.",
            "referenceCount": 35,
            "citationCount": 84,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1905.12663"
            },
            "citationStyles": {
                "bibtex": "@Article{Bielski2019EmergenceOO,\n author = {Adam Bielski and P. Favaro},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Emergence of Object Segmentation in Perturbed Generative Models},\n volume = {abs/1905.12663},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3c4bb963e2dd70eafa8fd9276655d090f7b2b570",
            "@type": "ScholarlyArticle",
            "paperId": "3c4bb963e2dd70eafa8fd9276655d090f7b2b570",
            "corpusId": 56657834,
            "url": "https://www.semanticscholar.org/paper/3c4bb963e2dd70eafa8fd9276655d090f7b2b570",
            "title": "Generative Models from the perspective of Continual Learning",
            "venue": "IEEE International Joint Conference on Neural Network",
            "publicationVenue": {
                "id": "urn:research:f80ba4a3-7aed-4021-b4d8-e4f50668847a",
                "name": "IEEE International Joint Conference on Neural Network",
                "alternate_names": [
                    "IJCNN",
                    "IEEE Int Jt Conf Neural Netw",
                    "Int Jt Conf Neural Netw",
                    "International Joint Conference on Neural Network"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=1573"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2906272760",
                "DBLP": "journals/corr/abs-1812-09111",
                "ArXiv": "1812.09111",
                "DOI": "10.1109/IJCNN.2019.8851986",
                "CorpusId": 56657834
            },
            "abstract": "Which generative model is the most suitable for Continual Learning? This paper aims at evaluating and comparing generative models on disjoint sequential image generation tasks. We investigate how several models learn and forget, considering various strategies: rehearsal, regularization, generative replay and fine-tuning. We used two quantitative metrics to estimate the generation quality and memory ability. We experiment with sequential tasks on three commonly used benchmarks for Continual Learning (MNIST, Fashion MNIST and CIFAR10). We found that among all models, the original GAN performs best and among Continual Learning strategies, generative replay outperforms all other methods. Even if we found satisfactory combinations on MNIST and Fashion MNIST, training generative models sequentially on CIFAR10 is particularly instable, and remains a challenge. Our code is available online1.",
            "referenceCount": 37,
            "citationCount": 131,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://openaccess.city.ac.uk/id/eprint/22452/1/1812.09111.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-21",
            "journal": {
                "name": "2019 International Joint Conference on Neural Networks (IJCNN)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lesort2018GenerativeMF,\n author = {Timoth\u00e9e Lesort and Hugo Caselles-Dupr\u00e9 and M. G. Ortiz and A. Stoian and David Filliat},\n booktitle = {IEEE International Joint Conference on Neural Network},\n journal = {2019 International Joint Conference on Neural Networks (IJCNN)},\n pages = {1-8},\n title = {Generative Models from the perspective of Continual Learning},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df81de654e048f710088fce45ee9846f3abd6b79",
            "@type": "ScholarlyArticle",
            "paperId": "df81de654e048f710088fce45ee9846f3abd6b79",
            "corpusId": 54434742,
            "url": "https://www.semanticscholar.org/paper/df81de654e048f710088fce45ee9846f3abd6b79",
            "title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/cvpr/Ritchie0L19",
                "MAG": "2964334375",
                "ArXiv": "1811.12463",
                "DOI": "10.1109/CVPR.2019.00634",
                "CorpusId": 54434742
            },
            "abstract": "We present a new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models. Our method operates on a top-down image-based representation, and inserts objects iteratively into the scene by predict their category, location, orientation and size with separate neural network modules. Our pipeline naturally supports automatic completion of partial scenes, as well as synthesis of complete scenes, without any modifications. Our method is significantly faster than the previous image-based method, and generates results that outperforms it and other state-of-the-art deep generative scene models in terms of faithfulness to training data and perceived visual quality.",
            "referenceCount": 35,
            "citationCount": 103,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1811.12463",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-11-29",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ritchie2018FastAF,\n author = {Daniel Ritchie and Kai Wang and Yu-An Lin},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6175-6183},\n title = {Fast and Flexible Indoor Scene Synthesis via Deep Convolutional Generative Models},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f18d245627d6089cb8a0e4a7757f45c13b96bdaf",
            "@type": "ScholarlyArticle",
            "paperId": "f18d245627d6089cb8a0e4a7757f45c13b96bdaf",
            "corpusId": 3631904,
            "url": "https://www.semanticscholar.org/paper/f18d245627d6089cb8a0e4a7757f45c13b96bdaf",
            "title": "Learning and Querying Fast Generative Models for Reinforcement Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-03006",
                "ArXiv": "1802.03006",
                "MAG": "2786019934",
                "CorpusId": 3631904
            },
            "abstract": "A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.",
            "referenceCount": 36,
            "citationCount": 118,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.03006"
            },
            "citationStyles": {
                "bibtex": "@Article{Buesing2018LearningAQ,\n author = {Lars Buesing and T. Weber and S. Racani\u00e8re and S. Eslami and Danilo Jimenez Rezende and David P. Reichert and Fabio Viola and F. Besse and Karol Gregor and D. Hassabis and Daan Wierstra},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning and Querying Fast Generative Models for Reinforcement Learning},\n volume = {abs/1802.03006},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:98689eb943f20e58296cb4f4c15f702ce7041121",
            "@type": "ScholarlyArticle",
            "paperId": "98689eb943f20e58296cb4f4c15f702ce7041121",
            "corpusId": 52055207,
            "url": "https://www.semanticscholar.org/paper/98689eb943f20e58296cb4f4c15f702ce7041121",
            "title": "GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation",
            "venue": "IEEE Transactions on Visualization and Computer Graphics",
            "publicationVenue": {
                "id": "urn:research:5e1f6444-5d03-48c7-b202-7f47d492aeae",
                "name": "IEEE Transactions on Visualization and Computer Graphics",
                "alternate_names": [
                    "IEEE Trans Vis Comput Graph"
                ],
                "issn": "1077-2626",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=2945"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2888723145",
                "DBLP": "journals/tvcg/KahngTCVW19",
                "ArXiv": "1809.01587",
                "DOI": "10.1109/TVCG.2018.2864500",
                "CorpusId": 52055207,
                "PubMed": "30130198"
            },
            "abstract": "Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.",
            "referenceCount": 52,
            "citationCount": 131,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/2945/8547224/08440049.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-09-05",
            "journal": {
                "name": "IEEE Transactions on Visualization and Computer Graphics",
                "volume": "25"
            },
            "citationStyles": {
                "bibtex": "@Article{Kahng2018GANLU,\n author = {Minsuk Kahng and Nikhil Thorat and Duen Horng Chau and F. Vi\u00e9gas and M. Wattenberg},\n booktitle = {IEEE Transactions on Visualization and Computer Graphics},\n journal = {IEEE Transactions on Visualization and Computer Graphics},\n pages = {310-320},\n title = {GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation},\n volume = {25},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb79a0cc3641d7d9c2a6a4e6fa2ac9c23d958e78",
            "@type": "ScholarlyArticle",
            "paperId": "fb79a0cc3641d7d9c2a6a4e6fa2ac9c23d958e78",
            "corpusId": 202588705,
            "url": "https://www.semanticscholar.org/paper/fb79a0cc3641d7d9c2a6a4e6fa2ac9c23d958e78",
            "title": ": Membership Inference Attacks Against Generative Models",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "CorpusId": 202588705
            },
            "abstract": "Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality.",
            "referenceCount": 67,
            "citationCount": 103,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Hayes2018MI,\n author = {Jamie Hayes and Luca Melis and G. Danezis and Emiliano De Cristofaro},\n title = {: Membership Inference Attacks Against Generative Models},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fa2d7908c5eac437b8f3f1000640632a283b7da2",
            "@type": "ScholarlyArticle",
            "paperId": "fa2d7908c5eac437b8f3f1000640632a283b7da2",
            "corpusId": 71148500,
            "url": "https://www.semanticscholar.org/paper/fa2d7908c5eac437b8f3f1000640632a283b7da2",
            "title": "Theoretical guarantees for sampling and inference in generative models with latent diffusions",
            "venue": "Annual Conference Computational Learning Theory",
            "publicationVenue": {
                "id": "urn:research:24b0721b-0592-414a-ac79-7271515aaab0",
                "name": "Annual Conference Computational Learning Theory",
                "alternate_names": [
                    "Conf Learn Theory",
                    "COLT",
                    "Conference on Learning Theory",
                    "Annu Conf Comput Learn Theory"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=536"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/colt/TzenR19",
                "MAG": "2964968771",
                "ArXiv": "1903.01608",
                "CorpusId": 71148500
            },
            "abstract": "We introduce and study a class of probabilistic generative models, where the latent object is a finite-dimensional diffusion process on a finite time interval and the observed variable is drawn conditionally on the terminal point of the diffusion. We make the following contributions: \nWe provide a unified viewpoint on both sampling and variational inference in such generative models through the lens of stochastic control. \nWe quantify the expressiveness of diffusion-based generative models. Specifically, we show that one can efficiently sample from a wide class of terminal target distributions by choosing the drift of the latent diffusion from the class of multilayer feedforward neural nets, with the accuracy of sampling measured by the Kullback-Leibler divergence to the target distribution. \nFinally, we present and analyze a scheme for unbiased simulation of generative models with latent diffusions and provide bounds on the variance of the resulting estimators. This scheme can be implemented as a deep generative model with a random number of layers.",
            "referenceCount": 47,
            "citationCount": 50,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-03-05",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tzen2019TheoreticalGF,\n author = {Belinda Tzen and M. Raginsky},\n booktitle = {Annual Conference Computational Learning Theory},\n pages = {3084-3114},\n title = {Theoretical guarantees for sampling and inference in generative models with latent diffusions},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:10fe7c4c7caffc572a856e53221da0fc8f110418",
            "@type": "ScholarlyArticle",
            "paperId": "10fe7c4c7caffc572a856e53221da0fc8f110418",
            "corpusId": 46924864,
            "url": "https://www.semanticscholar.org/paper/10fe7c4c7caffc572a856e53221da0fc8f110418",
            "title": "Content and misrepresentation in hierarchical generative models",
            "venue": "Synthese",
            "publicationVenue": {
                "id": "urn:research:cfb7bc3b-4dad-4d1f-aea6-f5d1f2499ce8",
                "name": "Synthese",
                "alternate_names": null,
                "issn": "0039-7857",
                "url": "http://www.springer.com/11229"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2619035405",
                "DBLP": "journals/synthese/KieferH18",
                "DOI": "10.1007/s11229-017-1435-7",
                "CorpusId": 46924864
            },
            "abstract": null,
            "referenceCount": 94,
            "citationCount": 86,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "Synthese",
                "volume": "195"
            },
            "citationStyles": {
                "bibtex": "@Article{Kiefer2018ContentAM,\n author = {Alex B. Kiefer and J. Hohwy},\n booktitle = {Synthese},\n journal = {Synthese},\n pages = {2387-2415},\n title = {Content and misrepresentation in hierarchical generative models},\n volume = {195},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1ed1b8fa1a57b860701071300dc6a9c441db6866",
            "@type": "ScholarlyArticle",
            "paperId": "1ed1b8fa1a57b860701071300dc6a9c441db6866",
            "corpusId": 211221927,
            "url": "https://www.semanticscholar.org/paper/1ed1b8fa1a57b860701071300dc6a9c441db6866",
            "title": "Scalable Reversible Generative Models with Free-form Continuous Dynamics",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "CorpusId": 211221927
            },
            "abstract": "A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson\u2019s trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on highdimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.",
            "referenceCount": 20,
            "citationCount": 83,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Grathwohl2018ScalableRG,\n author = {Will Grathwohl},\n title = {Scalable Reversible Generative Models with Free-form Continuous Dynamics},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:50ade4f1bc354d1c2e5046c2ac80773d49a2acd1",
            "@type": "ScholarlyArticle",
            "paperId": "50ade4f1bc354d1c2e5046c2ac80773d49a2acd1",
            "corpusId": 53281608,
            "url": "https://www.semanticscholar.org/paper/50ade4f1bc354d1c2e5046c2ac80773d49a2acd1",
            "title": "The Anatomy of Inference: Generative Models and Brain Structure",
            "venue": "Frontiers in Computational Neuroscience",
            "publicationVenue": {
                "id": "urn:research:8c456f98-9892-42ac-9b16-418755f01550",
                "name": "Frontiers in Computational Neuroscience",
                "alternate_names": [
                    "Front Comput Neurosci"
                ],
                "issn": "1662-5188",
                "url": "http://www.frontiersin.org/computational_neuroscience"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/ficn/ParrF18",
                "PubMedCentral": "6243103",
                "MAG": "2901597764",
                "DOI": "10.3389/fncom.2018.00090",
                "CorpusId": 53281608,
                "PubMed": "30483088"
            },
            "abstract": "To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioral phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments.",
            "referenceCount": 228,
            "citationCount": 126,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-11-13",
            "journal": {
                "name": "Frontiers in Computational Neuroscience",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Parr2018TheAO,\n author = {Thomas Parr and Karl J. Friston},\n booktitle = {Frontiers in Computational Neuroscience},\n journal = {Frontiers in Computational Neuroscience},\n title = {The Anatomy of Inference: Generative Models and Brain Structure},\n volume = {12},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2",
            "@type": "ScholarlyArticle",
            "paperId": "ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2",
            "corpusId": 2156851,
            "url": "https://www.semanticscholar.org/paper/ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2",
            "title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
            "venue": "2004 Conference on Computer Vision and Pattern Recognition Workshop",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2004,
            "externalIds": {
                "DBLP": "conf/cvpr/LiFP04",
                "MAG": "2155904486",
                "DOI": "10.1016/j.cviu.2005.09.012",
                "CorpusId": 2156851
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 4254,
            "influentialCitationCount": 459,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2004-06-27",
            "journal": {
                "name": "2004 Conference on Computer Vision and Pattern Recognition Workshop",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Fei-Fei2004LearningGV,\n author = {Li Fei-Fei and R. Fergus and P. Perona},\n booktitle = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n journal = {2004 Conference on Computer Vision and Pattern Recognition Workshop},\n pages = {178-178},\n title = {Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:186b6ce02ab2c2927df096bcb93da858fbf341ff",
            "@type": "ScholarlyArticle",
            "paperId": "186b6ce02ab2c2927df096bcb93da858fbf341ff",
            "corpusId": 53214072,
            "url": "https://www.semanticscholar.org/paper/186b6ce02ab2c2927df096bcb93da858fbf341ff",
            "title": "Bias and Generalization in Deep Generative Models: An Empirical Study",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2889599215",
                "DBLP": "conf/nips/ZhaoRYSGE18",
                "ArXiv": "1811.03259",
                "CorpusId": 53214072
            },
            "abstract": "In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.",
            "referenceCount": 33,
            "citationCount": 102,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-11-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2018BiasAG,\n author = {Shengjia Zhao and Hongyu Ren and Arianna Yuan and Jiaming Song and Noah D. Goodman and Stefano Ermon},\n booktitle = {Neural Information Processing Systems},\n pages = {10815-10824},\n title = {Bias and Generalization in Deep Generative Models: An Empirical Study},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:195e5772ac2c0816f4a701b37db657baead38006",
            "@type": "ScholarlyArticle",
            "paperId": "195e5772ac2c0816f4a701b37db657baead38006",
            "corpusId": 208617501,
            "url": "https://www.semanticscholar.org/paper/195e5772ac2c0816f4a701b37db657baead38006",
            "title": "A path towards quantum advantage in training deep generative models with quantum annealers",
            "venue": "Machine Learning: Science and Technology",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1912-02119",
                "MAG": "2991870932",
                "ArXiv": "1912.02119",
                "DOI": "10.1088/2632-2153/aba220",
                "CorpusId": 208617501
            },
            "abstract": "The development of quantum-classical hybrid (QCH) algorithms is critical to achieve state-of-the-art computational models. A QCH variational autoencoder (QVAE) was introduced in reference [] by some of the authors of this paper. QVAE consists of a classical auto-encoding structure realized by traditional deep neural networks to perform inference to and generation from, a discrete latent space. The latent generative process is formalized as thermal sampling from a quantum Boltzmann machine (QBM). This setup allows quantum-assisted training of deep generative models by physically simulating the generative process with quantum annealers. In this paper, we have successfully employed D-Wave quantum annealers as Boltzmann samplers to perform quantum-assisted, end-to-end training of QVAE. The hybrid structure of QVAE allows us to deploy current-generation quantum annealers in QCH generative models to achieve competitive performance on datasets such as MNIST. The results presented in this paper suggest that commercially available quantum annealers can be deployed, in conjunction with well-crafted classical deep neutral networks, to achieve competitive results in unsupervised and semisupervised tasks on large-scale datasets. We also provide evidence that our setup is able to exploit large latent-space QBMs, which develop slowly mixing modes. This expressive latent space results in slow and inefficient classical sampling and paves the way to achieve quantum advantage with quantum annealing in realistic sampling applications.",
            "referenceCount": 73,
            "citationCount": 50,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-12-04",
            "journal": {
                "name": "Machine Learning: Science and Technology",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Winci2019APT,\n author = {Walter Winci and L. Buffoni and Hossein Sadeghi and Amir Khoshaman and E. Andriyash and Mohammad H. Amin},\n booktitle = {Machine Learning: Science and Technology},\n journal = {Machine Learning: Science and Technology},\n title = {A path towards quantum advantage in training deep generative models with quantum annealers},\n volume = {1},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32b918246037976ba5f8363104ec042f56db42d6",
            "@type": "ScholarlyArticle",
            "paperId": "32b918246037976ba5f8363104ec042f56db42d6",
            "corpusId": 62775631,
            "url": "https://www.semanticscholar.org/paper/32b918246037976ba5f8363104ec042f56db42d6",
            "title": "Learning Localized Generative Models for 3D Point Clouds via Graph Convolution",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2910792243",
                "DBLP": "conf/iclr/ValsesiaFM19",
                "CorpusId": 62775631
            },
            "abstract": "Point clouds are an important type of geometric data and have widespread use in computer graphics and vision. However, learning representations for point clouds is particularly challenging due to their nature as being an unordered collection of points irregularly distributed in 3D space. Graph convolution, a generalization of the convolution operation for data defined over graphs, has been recently shown to be very successful at extracting localized features from point clouds in supervised or semi-supervised tasks such as classification or segmentation. This paper studies the unsupervised problem of a generative model exploiting graph convolution. We focus on the generator of a GAN and define methods for graph convolution when the graph is not known in advance as it is the very output of the generator. The proposed architecture learns to generate localized features that approximate graph embeddings of the output geometry. We also study the problem of defining an upsampling layer in the graph-convolutional generator, such that it learns to exploit a self-similarity prior on the data distribution to sample more effectively.",
            "referenceCount": 27,
            "citationCount": 146,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Valsesia2018LearningLG,\n author = {D. Valsesia and Giulia Fracastoro and E. Magli},\n booktitle = {International Conference on Learning Representations},\n title = {Learning Localized Generative Models for 3D Point Clouds via Graph Convolution},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:be7f2245d56206b41d5dcc979b861bf156e5e0cb",
            "@type": "ScholarlyArticle",
            "paperId": "be7f2245d56206b41d5dcc979b861bf156e5e0cb",
            "corpusId": 54475947,
            "url": "https://www.semanticscholar.org/paper/be7f2245d56206b41d5dcc979b861bf156e5e0cb",
            "title": "A quantum machine learning algorithm based on generative models",
            "venue": "Science Advances",
            "publicationVenue": {
                "id": "urn:research:cb30f0c9-2980-4b7d-bbcb-68fc5472b97c",
                "name": "Science Advances",
                "alternate_names": [
                    "Sci Adv"
                ],
                "issn": "2375-2548",
                "url": "http://www.scienceadvances.org/"
            },
            "year": 2018,
            "externalIds": {
                "PubMedCentral": "6286170",
                "MAG": "2903891684",
                "DOI": "10.1126/sciadv.aat9004",
                "CorpusId": 54475947,
                "PubMed": "30539141"
            },
            "abstract": "We propose a quantum learning algorithm for a quantum generative model and prove its advantages compared with classical models. Quantum computing and artificial intelligence, combined together, may revolutionize future technologies. A significant school of thought regarding artificial intelligence is based on generative models. Here, we propose a general quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is more capable of representing probability distributions compared with classical generative models and has exponential speedup in learning and inference at least for some instances if a quantum computer cannot be efficiently simulated classically. Our result opens a new direction for quantum machine learning and offers a remarkable example where a quantum algorithm shows exponential improvement over classical algorithms in an important application field.",
            "referenceCount": 41,
            "citationCount": 90,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://advances.sciencemag.org/content/advances/4/12/eaat9004.full.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-01",
            "journal": {
                "name": "Science Advances",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Gao2018AQM,\n author = {Xun Gao and Z.-Y. Zhang and Z.-Y. Zhang and Luming Duan and Luming Duan},\n booktitle = {Science Advances},\n journal = {Science Advances},\n title = {A quantum machine learning algorithm based on generative models},\n volume = {4},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:64a1e4d18c2fab4904f85b3ee1a9af4263dae348",
            "@type": "ScholarlyArticle",
            "paperId": "64a1e4d18c2fab4904f85b3ee1a9af4263dae348",
            "corpusId": 19243216,
            "url": "https://www.semanticscholar.org/paper/64a1e4d18c2fab4904f85b3ee1a9af4263dae348",
            "title": "Deep generative models: Survey",
            "venue": "International Symposium on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:1d8d1e5b-972b-4809-bf00-64e9da59fdb2",
                "name": "International Symposium on Computer Vision",
                "alternate_names": [
                    "ISCV",
                    "Int Symp Comput Vis"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2799397159",
                "DOI": "10.1109/ISACV.2018.8354080",
                "CorpusId": 19243216
            },
            "abstract": "Generative models have found their way to the forefront of deep learning the last decade and so far, it seems that the hype will not fade away any time soon. In this paper, we give an overview of the most important building blocks of most recent revolutionary deep generative models such as RBM, DBM, DBN, VAE and GAN. We will also take a look at three of state-of-the-art generative models, namely PixelRNN, DRAW and NADE. We will delve into their unique architectures, the learning procedures and their potential and limitations. We will also review some of the known issues that arise when trying to design and train deep generative architectures using shallow ones and how different models deal with these issues. This paper is not meant to be a comprehensive study of these models, but rather a starting point for those who bear an interest in the field.",
            "referenceCount": 38,
            "citationCount": 95,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference",
                "Review"
            ],
            "publicationDate": "2018-04-02",
            "journal": {
                "name": "2018 International Conference on Intelligent Systems and Computer Vision (ISCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Oussidi2018DeepGM,\n author = {Achraf Oussidi and Azeddine Elhassouny},\n booktitle = {International Symposium on Computer Vision},\n journal = {2018 International Conference on Intelligent Systems and Computer Vision (ISCV)},\n pages = {1-8},\n title = {Deep generative models: Survey},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fff34334f4a1a7afe8ddbb27911af52decbac987",
            "@type": "ScholarlyArticle",
            "paperId": "fff34334f4a1a7afe8ddbb27911af52decbac987",
            "corpusId": 54465063,
            "url": "https://www.semanticscholar.org/paper/fff34334f4a1a7afe8ddbb27911af52decbac987",
            "title": "Counterfactuals uncover the modular structure of deep generative models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1812-03253",
                "MAG": "2995191098",
                "ArXiv": "1812.03253",
                "CorpusId": 54465063
            },
            "abstract": "Deep generative models can emulate the perceptual properties of complex image datasets, providing a latent representation of the data. However, manipulating such representation to perform meaningful and controllable transformations in the data space remains challenging without some form of supervision. While previous work has focused on exploiting statistical independence to disentangle latent factors, we argue that such requirement is too restrictive and propose instead a non-statistical framework that relies on counterfactual manipulations to uncover a modular structure of the network composed of disentangled groups of internal variables. Experiments with a variety of generative models trained on complex image datasets show the obtained modules can be used to design targeted interventions. This opens the way to applications such as computationally efficient style transfer and the automated assessment of robustness to contextual changes in pattern recognition systems.",
            "referenceCount": 48,
            "citationCount": 94,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.03253"
            },
            "citationStyles": {
                "bibtex": "@Article{Besserve2018CounterfactualsUT,\n author = {M. Besserve and R\u00e9my Sun and B. Scholkopf},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Counterfactuals uncover the modular structure of deep generative models},\n volume = {abs/1812.03253},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3cd6941ffd95d58461d8b3e9b52660a43d8d9833",
            "@type": "ScholarlyArticle",
            "paperId": "3cd6941ffd95d58461d8b3e9b52660a43d8d9833",
            "corpusId": 67855454,
            "url": "https://www.semanticscholar.org/paper/3cd6941ffd95d58461d8b3e9b52660a43d8d9833",
            "title": "Improving Missing Data Imputation with Deep Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/corr/abs-1902-10666",
                "ArXiv": "1902.10666",
                "MAG": "2934600230",
                "CorpusId": 67855454
            },
            "abstract": "Datasets with missing values are very common on industry applications, and they can have a negative impact on machine learning models. Recent studies introduced solutions to the problem of imputing missing values based on deep generative models. Previous experiments with Generative Adversarial Networks and Variational Autoencoders showed interesting results in this domain, but it is not clear which method is preferable for different use cases. The goal of this work is twofold: we present a comparison between missing data imputation solutions based on deep generative models, and we propose improvements over those methodologies. We run our experiments using known real life datasets with different characteristics, removing values at random and reconstructing them with several imputation techniques. Our results show that the presence or absence of categorical variables can alter the selection of the best model, and that some models are more stable than others after similar runs with different random number generator seeds.",
            "referenceCount": 36,
            "citationCount": 46,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1902.10666"
            },
            "citationStyles": {
                "bibtex": "@Article{Camino2019ImprovingMD,\n author = {R. Camino and Christian A. Hammerschmidt and R. State},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Improving Missing Data Imputation with Deep Generative Models},\n volume = {abs/1902.10666},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb1f2813687763cd91c795b2471cf6ef31a4e34b",
            "@type": "ScholarlyArticle",
            "paperId": "bb1f2813687763cd91c795b2471cf6ef31a4e34b",
            "corpusId": 23822765,
            "url": "https://www.semanticscholar.org/paper/bb1f2813687763cd91c795b2471cf6ef31a4e34b",
            "title": "Conditional molecular design with deep generative models",
            "venue": "Journal of Chemical Information and Modeling",
            "publicationVenue": {
                "id": "urn:research:3f16aef5-6b9f-4f87-baca-cbf8147e352f",
                "name": "Journal of Chemical Information and Modeling",
                "alternate_names": [
                    "J Chem Inf Model"
                ],
                "issn": "1549-9596",
                "url": "http://pubs.acs.org/jcim"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1805-00108",
                "MAG": "3106536628",
                "ArXiv": "1805.00108",
                "DOI": "10.1021/acs.jcim.8b00263",
                "CorpusId": 23822765,
                "PubMed": "30016587"
            },
            "abstract": "Although machine learning has been successfully used to propose novel molecules that satisfy desired properties, it is still challenging to explore a large chemical space efficiently. In this paper, we present a conditional molecular design method that facilitates generating new molecules with desired properties. The proposed model, which simultaneously performs both property prediction and molecule generation, is built as a semisupervised variational autoencoder trained on a set of existing molecules with only a partial annotation. We generate new molecules with desired properties by sampling from the generative distribution estimated by the model. We demonstrate the effectiveness of the proposed model by evaluating it on drug-like molecules. The model improves the performance of property prediction by exploiting unlabeled molecules and efficiently generates novel molecules fulfilling various target conditions.",
            "referenceCount": 51,
            "citationCount": 137,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1805.00108",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-04-30",
            "journal": {
                "name": "Journal of chemical information and modeling",
                "volume": "59 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Kang2018ConditionalMD,\n author = {Seokho Kang and Kyunghyun Cho},\n booktitle = {Journal of Chemical Information and Modeling},\n journal = {Journal of chemical information and modeling},\n pages = {\n          43-52\n        },\n title = {Conditional molecular design with deep generative models},\n volume = {59 1},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2cfb6271a5b9e47951ccd9a79ddd74b1cf457fef",
            "@type": "ScholarlyArticle",
            "paperId": "2cfb6271a5b9e47951ccd9a79ddd74b1cf457fef",
            "corpusId": 211132391,
            "url": "https://www.semanticscholar.org/paper/2cfb6271a5b9e47951ccd9a79ddd74b1cf457fef",
            "title": "Understanding the Limitations of Conditional Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1906.01171",
                "MAG": "2996303980",
                "DBLP": "conf/iclr/FetayaJGZ20",
                "CorpusId": 211132391
            },
            "abstract": "Class-conditional generative models hold promise to overcome the shortcomings of their discriminative counterparts. They are a natural choice to solve discriminative tasks in a robust manner as they jointly optimize for predictive performance and accurate modeling of the input distribution. In this work, we investigate robust classification with likelihood-based generative models from a theoretical and practical perspective to investigate if they can deliver on their promises. Our analysis focuses on a spectrum of robustness properties: (1) Detection of worst-case outliers in the form of adversarial examples; (2) Detection of average-case outliers in the form of ambiguous inputs and (3) Detection of incorrectly labeled in-distribution inputs. Our theoretical result reveals that it is impossible to guarantee detectability of adversarially-perturbed inputs even for near-optimal generative classifiers. Experimentally, we find that while we are able to train robust models for MNIST, robustness completely breaks down on CIFAR10. We relate this failure to various undesirable model properties that can be traced to the maximum likelihood training objective. Despite being a common choice in the literature, our results indicate that likelihood-based conditional generative models may are surprisingly ineffective for robust classification.",
            "referenceCount": 33,
            "citationCount": 39,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-04",
            "journal": {
                "name": "arXiv: Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Fetaya2019UnderstandingTL,\n author = {Ethan Fetaya and J. Jacobsen and Will Grathwohl and R. Zemel},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Learning},\n title = {Understanding the Limitations of Conditional Generative Models},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:35259b3802bf27b7650d301ad34c26f3a8794d88",
            "@type": "ScholarlyArticle",
            "paperId": "35259b3802bf27b7650d301ad34c26f3a8794d88",
            "corpusId": 18793465,
            "url": "https://www.semanticscholar.org/paper/35259b3802bf27b7650d301ad34c26f3a8794d88",
            "title": "Generative Models",
            "venue": "Encyclopedia of Database Systems",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "reference/db/X09wi",
                "DOI": "10.1007/978-0-387-39940-9_2675",
                "CorpusId": 18793465
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 39,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2019-11-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Trappenberg2019GenerativeM,\n author = {Thomas P. Trappenberg},\n booktitle = {Encyclopedia of Database Systems},\n pages = {1224},\n title = {Generative Models},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de96cd450baca1a76d89a72081239935e0aa43a3",
            "@type": "ScholarlyArticle",
            "paperId": "de96cd450baca1a76d89a72081239935e0aa43a3",
            "corpusId": 49570496,
            "url": "https://www.semanticscholar.org/paper/de96cd450baca1a76d89a72081239935e0aa43a3",
            "title": "Modeling Sparse Deviations for Compressed Sensing using Generative Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963031612",
                "DBLP": "conf/icml/DharGE18",
                "ArXiv": "1807.01442",
                "CorpusId": 49570496
            },
            "abstract": "In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.",
            "referenceCount": 48,
            "citationCount": 70,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-07-03",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dhar2018ModelingSD,\n author = {Manik Dhar and Aditya Grover and Stefano Ermon},\n booktitle = {International Conference on Machine Learning},\n pages = {1222-1231},\n title = {Modeling Sparse Deviations for Compressed Sensing using Generative Models},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:529007fa5761e68c3ca2a8bc1c36034816adc0ca",
            "@type": "ScholarlyArticle",
            "paperId": "529007fa5761e68c3ca2a8bc1c36034816adc0ca",
            "corpusId": 49426397,
            "url": "https://www.semanticscholar.org/paper/529007fa5761e68c3ca2a8bc1c36034816adc0ca",
            "title": "Deep Generative Models with Learnable Knowledge Constraints",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2810565934",
                "DBLP": "conf/nips/HuYSQLDX18",
                "ArXiv": "1806.09764",
                "CorpusId": 49426397
            },
            "abstract": "The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified {\\it a priori}, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.",
            "referenceCount": 62,
            "citationCount": 73,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2018DeepGM,\n author = {Zhiting Hu and Zichao Yang and R. Salakhutdinov and Xiaodan Liang and Lianhui Qin and Haoye Dong and E. Xing},\n booktitle = {Neural Information Processing Systems},\n pages = {10522-10533},\n title = {Deep Generative Models with Learnable Knowledge Constraints},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
            "@type": "ScholarlyArticle",
            "paperId": "2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
            "corpusId": 8122361,
            "url": "https://www.semanticscholar.org/paper/2b5f51588f1c4cdca0865de20c1e2e1ff3570fd1",
            "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2951070067",
                "DBLP": "conf/nips/EslamiHWTSKH16",
                "ArXiv": "1603.08575",
                "CorpusId": 8122361
            },
            "abstract": "We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.",
            "referenceCount": 44,
            "citationCount": 489,
            "influentialCitationCount": 62,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1603.08575"
            },
            "citationStyles": {
                "bibtex": "@Article{Eslami2016AttendIR,\n author = {S. Eslami and N. Heess and T. Weber and Yuval Tassa and David Szepesvari and K. Kavukcuoglu and Geoffrey E. Hinton},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Attend, Infer, Repeat: Fast Scene Understanding with Generative Models},\n volume = {abs/1603.08575},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b47cf1e0fc42c2bf99878d88cd1ce55e1fa50a70",
            "@type": "ScholarlyArticle",
            "paperId": "b47cf1e0fc42c2bf99878d88cd1ce55e1fa50a70",
            "corpusId": 53258271,
            "url": "https://www.semanticscholar.org/paper/b47cf1e0fc42c2bf99878d88cd1ce55e1fa50a70",
            "title": "On the evaluation of generative models in music",
            "venue": "Neural computing & applications (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2898827701",
                "DBLP": "journals/nca/YangL20",
                "DOI": "10.1007/s00521-018-3849-7",
                "CorpusId": 53258271
            },
            "abstract": null,
            "referenceCount": 67,
            "citationCount": 100,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2018-11-03",
            "journal": {
                "name": "Neural Computing and Applications",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018OnTE,\n author = {Li-Chia Yang and Alexander Lerch},\n booktitle = {Neural computing & applications (Print)},\n journal = {Neural Computing and Applications},\n pages = {4773 - 4784},\n title = {On the evaluation of generative models in music},\n volume = {32},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:60ee030773ba1b68eb222a265b052ca028353362",
            "@type": "ScholarlyArticle",
            "paperId": "60ee030773ba1b68eb222a265b052ca028353362",
            "corpusId": 249152323,
            "url": "https://www.semanticscholar.org/paper/60ee030773ba1b68eb222a265b052ca028353362",
            "title": "GIT: A Generative Image-to-text Transformer for Vision and Language",
            "venue": "Trans. Mach. Learn. Res.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2205-14100",
                "ArXiv": "2205.14100",
                "DOI": "10.48550/arXiv.2205.14100",
                "CorpusId": 249152323
            },
            "abstract": "In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks. Codes are released at \\url{https://github.com/microsoft/GenerativeImage2Text}.",
            "referenceCount": 147,
            "citationCount": 232,
            "influentialCitationCount": 47,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2205.14100",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-05-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2205.14100"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2022GITAG,\n author = {Jianfeng Wang and Zhengyuan Yang and Xiaowei Hu and Linjie Li and Kevin Lin and Zhe Gan and Zicheng Liu and Ce Liu and Lijuan Wang},\n booktitle = {Trans. Mach. Learn. Res.},\n journal = {ArXiv},\n title = {GIT: A Generative Image-to-text Transformer for Vision and Language},\n volume = {abs/2205.14100},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:47900aca2f0b50da3010ad59b394c870f0e6c02e",
            "@type": "ScholarlyArticle",
            "paperId": "47900aca2f0b50da3010ad59b394c870f0e6c02e",
            "corpusId": 1282515,
            "url": "https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e",
            "title": "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "648143168",
                "DBLP": "conf/nips/DentonCSF15",
                "ArXiv": "1506.05751",
                "CorpusId": 1282515
            },
            "abstract": "In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach [11]. Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40% of the time, compared to 10% for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.",
            "referenceCount": 36,
            "citationCount": 2124,
            "influentialCitationCount": 109,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-06-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1506.05751"
            },
            "citationStyles": {
                "bibtex": "@Article{Denton2015DeepGI,\n author = {Emily L. Denton and Soumith Chintala and Arthur Szlam and R. Fergus},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks},\n volume = {abs/1506.05751},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36a2a633dfec3ca8746f520ebe780343a8a2aee8",
            "@type": "ScholarlyArticle",
            "paperId": "36a2a633dfec3ca8746f520ebe780343a8a2aee8",
            "corpusId": 44145142,
            "url": "https://www.semanticscholar.org/paper/36a2a633dfec3ca8746f520ebe780343a8a2aee8",
            "title": "Deep Generative Models for Distribution-Preserving Lossy Compression",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2950602870",
                "DBLP": "conf/nips/TschannenAL18",
                "ArXiv": "1805.11057",
                "CorpusId": 44145142
            },
            "abstract": "We propose and study the problem of distribution-preserving lossy compression. Motivated by recent advances in extreme image compression which allow to maintain artifact-free reconstructions even at very low bitrates, we propose to optimize the rate-distortion tradeoff under the constraint that the reconstructed samples follow the distribution of the training data. The resulting compression system recovers both ends of the spectrum: On one hand, at zero bitrate it learns a generative model of the data, and at high enough bitrates it achieves perfect reconstruction. Furthermore, for intermediate bitrates it smoothly interpolates between learning a generative model of the training data and perfectly reconstructing the training samples. We study several methods to approximately solve the proposed optimization problem, including a novel combination of Wasserstein GAN and Wasserstein Autoencoder, and present an extensive theoretical and empirical characterization of the proposed compression systems.",
            "referenceCount": 46,
            "citationCount": 101,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tschannen2018DeepGM,\n author = {M. Tschannen and E. Agustsson and Mario Lucic},\n booktitle = {Neural Information Processing Systems},\n pages = {5933-5944},\n title = {Deep Generative Models for Distribution-Preserving Lossy Compression},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4bcaa3787e354fc9777ad4f14604137f644ef7fd",
            "@type": "ScholarlyArticle",
            "paperId": "4bcaa3787e354fc9777ad4f14604137f644ef7fd",
            "corpusId": 243020769,
            "url": "https://www.semanticscholar.org/paper/4bcaa3787e354fc9777ad4f14604137f644ef7fd",
            "title": "Generative Models",
            "venue": "Erkenntnis: An International Journal of Scientific Philosophy",
            "publicationVenue": {
                "id": "urn:research:638d1b1c-6c05-4b65-aa2e-ace42ee26e95",
                "name": "Erkenntnis: An International Journal of Scientific Philosophy",
                "alternate_names": [
                    "Erkenn Int J Sci Philos",
                    "Erkenntnis"
                ],
                "issn": "0165-0106",
                "url": "https://www.springer.com/philosophy/journal/10670"
            },
            "year": 2020,
            "externalIds": {
                "DOI": "10.1007/s10670-020-00338-w",
                "CorpusId": 243020769
            },
            "abstract": null,
            "referenceCount": 106,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2020-11-22",
            "journal": {
                "name": "Erkenntnis",
                "volume": "88"
            },
            "citationStyles": {
                "bibtex": "@Article{Tee2020GenerativeM,\n author = {Sim-Hui Tee},\n booktitle = {Erkenntnis: An International Journal of Scientific Philosophy},\n journal = {Erkenntnis},\n pages = {23-41},\n title = {Generative Models},\n volume = {88},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8e5d0c73eb29e3da8d6d3a0c8560b23680122bb2",
            "@type": "ScholarlyArticle",
            "paperId": "8e5d0c73eb29e3da8d6d3a0c8560b23680122bb2",
            "corpusId": 6462244,
            "url": "https://www.semanticscholar.org/paper/8e5d0c73eb29e3da8d6d3a0c8560b23680122bb2",
            "title": "Adversarial Examples for Generative Models",
            "venue": "2018 IEEE Security and Privacy Workshops (SPW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.06832",
                "DBLP": "journals/corr/KosFS17",
                "MAG": "2594717275",
                "DOI": "10.1109/SPW.2018.00014",
                "CorpusId": 6462244
            },
            "abstract": "We explore methods of producing adversarial examples on deep generative models such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning architectures are known to be vulnerable to adversarial examples, but previous work has focused on the application of adversarial examples to classification tasks. Deep generative models have recently become popular due to their ability to model input data distributions and generate realistic examples from those distributions. We present three classes of attacks on the VAE and VAE-GAN architectures and demonstrate them against networks trained on MNIST, SVHN and CelebA. Our first attack leverages classification-based adversaries by attaching a classifier to the trained encoder of the target generative model, which can then be used to indirectly manipulate the latent representation. Our second attack directly uses the VAE loss function to generate a target reconstruction image from the adversarial example. Our third attack moves beyond relying on classification or the standard loss for the gradient and directly optimizes against differences in source and target latent representations. We also motivate why an attacker might be interested in deploying such techniques against a target generative network.",
            "referenceCount": 29,
            "citationCount": 246,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/8420091/8424589/08424630.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-22",
            "journal": {
                "name": "2018 IEEE Security and Privacy Workshops (SPW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kos2017AdversarialEF,\n author = {Jernej Kos and Ian Fischer and D. Song},\n booktitle = {2018 IEEE Security and Privacy Workshops (SPW)},\n journal = {2018 IEEE Security and Privacy Workshops (SPW)},\n pages = {36-42},\n title = {Adversarial Examples for Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:aaec96d6e9a0a4877dde4382dc7889d47c074524",
            "@type": "ScholarlyArticle",
            "paperId": "aaec96d6e9a0a4877dde4382dc7889d47c074524",
            "corpusId": 219964093,
            "url": "https://www.semanticscholar.org/paper/aaec96d6e9a0a4877dde4382dc7889d47c074524",
            "title": "GHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/cvpr/XuBZFSS20",
                "MAG": "3035581100",
                "DOI": "10.1109/cvpr42600.2020.00622",
                "CorpusId": 219964093
            },
            "abstract": "We present a statistical, articulated 3D human shape modeling pipeline, within a fully trainable, modular, deep learning framework. Given high-resolution complete 3D body scans of humans, captured in various poses, together with additional closeups of their head and facial expressions, as well as hand articulation, and given initial, artist designed, gender neutral rigged quad-meshes, we train all model parameters including non-linear shape spaces based on variational auto-encoders, pose-space deformation correctives, skeleton joint center predictors, and blend skinning functions, in a single consistent learning loop. The models are simultaneously trained with all the 3d dynamic scan data (over 60,000 diverse human configurations in our new dataset) in order to capture correlations and ensure consistency of various components. Models support facial expression analysis, as well as body (with detailed hand) shape and pose estimation. We provide fully train-able generic human models of different resolutions- the moderate-resolution GHUM consisting of 10,168 vertices and the low-resolution GHUML(ite) of 3,194 vertices\u2013, run comparisons between them, analyze the impact of different components and illustrate their reconstruction from image data. The models will be available for research.",
            "referenceCount": 44,
            "citationCount": 217,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-06-01",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2020GHUMG,\n author = {Hongyi Xu and Eduard Gabriel Bazavan and Andrei Zanfir and W. Freeman and R. Sukthankar and C. Sminchisescu},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6183-6192},\n title = {GHUM & GHUML: Generative 3D Human Shape and Articulated Pose Models},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90c10cc419420e18ed8649967c2cf3ae45c42057",
            "@type": "ScholarlyArticle",
            "paperId": "90c10cc419420e18ed8649967c2cf3ae45c42057",
            "corpusId": 3350443,
            "url": "https://www.semanticscholar.org/paper/90c10cc419420e18ed8649967c2cf3ae45c42057",
            "title": "AdaGAN: Boosting Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2952533959",
                "ArXiv": "1701.02386",
                "DBLP": "conf/nips/TolstikhinGBSS17",
                "CorpusId": 3350443
            },
            "abstract": "Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.",
            "referenceCount": 23,
            "citationCount": 210,
            "influentialCitationCount": 22,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-01-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.02386"
            },
            "citationStyles": {
                "bibtex": "@Article{Tolstikhin2017AdaGANBG,\n author = {I. Tolstikhin and S. Gelly and O. Bousquet and Carl-Johann Simon-Gabriel and B. Scholkopf},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {AdaGAN: Boosting Generative Models},\n volume = {abs/1701.02386},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9f696b7156716c978b62a92714e7038a99f7a53c",
            "@type": "ScholarlyArticle",
            "paperId": "9f696b7156716c978b62a92714e7038a99f7a53c",
            "corpusId": 51780574,
            "url": "https://www.semanticscholar.org/paper/9f696b7156716c978b62a92714e7038a99f7a53c",
            "title": "Latent Space Oddity: on the Curvature of Deep Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2964011399",
                "ArXiv": "1710.11379",
                "DBLP": "conf/iclr/ArvanitidisHH18",
                "CorpusId": 51780574
            },
            "abstract": "Deep generative models provide a systematic way to learn nonlinear data distributions, through a set of latent variables and a nonlinear \"generator\" function that maps latent points into the input space. The nonlinearity of the generator imply that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and demonstrate that distances and interpolants are significantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalize to other deep generative models.",
            "referenceCount": 20,
            "citationCount": 210,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-31",
            "journal": {
                "name": "arXiv: Machine Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Arvanitidis2017LatentSO,\n author = {Georgios Arvanitidis and L. K. Hansen and S\u00f8ren Hauberg},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Machine Learning},\n title = {Latent Space Oddity: on the Curvature of Deep Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a3fce9324329581d484dfc8c3f018bc31751ca82",
            "@type": "ScholarlyArticle",
            "paperId": "a3fce9324329581d484dfc8c3f018bc31751ca82",
            "corpusId": 49215526,
            "url": "https://www.semanticscholar.org/paper/a3fce9324329581d484dfc8c3f018bc31751ca82",
            "title": "Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2808121541",
                "ArXiv": "1806.05452",
                "DBLP": "journals/corr/abs-1806-05452",
                "CorpusId": 49215526
            },
            "abstract": "Recent advances in deep learning led to novel generative modeling techniques that achieve unprecedented quality in generated samples and performance in learning complex distributions in imaging data. These new models in medical image computing have important applications that form clinically relevant and very challenging unsupervised learning problems. In this paper, we explore the feasibility of using state-of-the-art auto-encoder-based deep generative models, such as variational and adversarial auto-encoders, for one such task: abnormality detection in medical imaging. We utilize typical, publicly available datasets with brain scans from healthy subjects and patients with stroke lesions and brain tumors. We use the data from healthy subjects to train different auto-encoder based models to learn the distribution of healthy images and detect pathologies as outliers. Models that can better learn the data distribution should be able to detect outliers more accurately. We evaluate the detection performance of deep generative models and compare them with non-deep learning based approaches to provide a benchmark of the current state of research. We conclude that abnormality detection is a challenging task for deep generative models and large room exists for improvement. In order to facilitate further research, we aim to provide carefully pre-processed imaging data available to the research community.",
            "referenceCount": 46,
            "citationCount": 45,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1806.05452"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018DeepGM,\n author = {Xiaoran Chen and Nick Pawlowski and Martin Rajchl and Ben Glocker and E. Konukoglu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Generative Models in the Real-World: An Open Challenge from Medical Imaging},\n volume = {abs/1806.05452},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7389d3fc2e8abc43964c96c16a94d96a04573f96",
            "@type": "ScholarlyArticle",
            "paperId": "7389d3fc2e8abc43964c96c16a94d96a04573f96",
            "corpusId": 46750965,
            "url": "https://www.semanticscholar.org/paper/7389d3fc2e8abc43964c96c16a94d96a04573f96",
            "title": "Deep Generative Models for Molecular Science",
            "venue": "Molecular Informatics",
            "publicationVenue": {
                "id": "urn:research:5b118ecf-59a2-431d-8c47-4656f9e92e08",
                "name": "Molecular Informatics",
                "alternate_names": [
                    "Mol Informatics"
                ],
                "issn": "1868-1743",
                "url": "http://onlinelibrary.wiley.com/journal/10.1002/(ISSN)1868-1751"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2792130717",
                "DOI": "10.1002/minf.201700133",
                "CorpusId": 46750965,
                "PubMed": "29405647"
            },
            "abstract": "Generative deep machine learning models now rival traditional quantum\u2010mechanical computations in predicting properties of new structures, and they come with a significantly lower computational cost, opening new avenues in computational molecular science. In the last few years, a variety of deep generative models have been proposed for modeling molecules, which differ in both their model structure and choice of input features. We review these recent advances within deep generative models for predicting molecular properties, with particular focus on models based on the probabilistic autoencoder (or variational autoencoder, VAE) approach in which the molecular structure is embedded in a latent vector space from which its properties can be predicted and its structure can be restored.",
            "referenceCount": 35,
            "citationCount": 57,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/minf.201700133",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2018-01-01",
            "journal": {
                "name": "Molecular Informatics",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{J\u00f8rgensen2018DeepGM,\n author = {P. B. J\u00f8rgensen and Mikkel N. Schmidt and O. Winther},\n booktitle = {Molecular Informatics},\n journal = {Molecular Informatics},\n title = {Deep Generative Models for Molecular Science},\n volume = {37},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8017cc9e122e260f4b400ec0877354efcc524745",
            "@type": "ScholarlyArticle",
            "paperId": "8017cc9e122e260f4b400ec0877354efcc524745",
            "corpusId": 54462620,
            "url": "https://www.semanticscholar.org/paper/8017cc9e122e260f4b400ec0877354efcc524745",
            "title": "Physics-informed deep generative models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2903926864",
                "ArXiv": "1812.03511",
                "DBLP": "journals/corr/abs-1812-03511",
                "CorpusId": 54462620
            },
            "abstract": "We consider the application of deep generative models in propagating uncertainty through complex physical systems. Specifically, we put forth an implicit variational inference formulation that constrains the generative model output to satisfy given physical laws expressed by partial differential equations. Such physics-informed constraints provide a regularization mechanism for effectively training deep probabilistic models for modeling physical systems in which the cost of data acquisition is high and training data-sets are typically small. This provides a scalable framework for characterizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations. We demonstrate the effectiveness of our approach through a canonical example in transport dynamics.",
            "referenceCount": 32,
            "citationCount": 49,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.03511"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018PhysicsinformedDG,\n author = {Yibo Yang and P. Perdikaris},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Physics-informed deep generative models},\n volume = {abs/1812.03511},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1dd1d4dcca2d7ead9358bc74205f70936723b8a0",
            "@type": "ScholarlyArticle",
            "paperId": "1dd1d4dcca2d7ead9358bc74205f70936723b8a0",
            "corpusId": 105748004,
            "url": "https://www.semanticscholar.org/paper/1dd1d4dcca2d7ead9358bc74205f70936723b8a0",
            "title": "Advances and challenges in deep generative models for de novo molecule generation",
            "venue": "WIREs Computational Molecular Science",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2896506587",
                "DOI": "10.1002/wcms.1395",
                "CorpusId": 105748004
            },
            "abstract": "The de novo molecule generation problem involves generating novel or modified molecular structures with desirable properties. Taking advantage of the great representation learning ability of deep learning models, deep generative models, which differ from discriminative models in their traditional machine learning approach, provide the possibility of generation of desirable molecules directly. Although deep generative models have been extensively discussed in the machine learning community, a specific investigation of the computational issues related to deep generative models for de novo molecule generation is needed. A concise and insightful discussion of recent advances in applying deep generative models for de novo molecule generation is presented, with particularly emphasizing the most important challenges for successful application of deep generative models in this specific area.",
            "referenceCount": 46,
            "citationCount": 52,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-10-19",
            "journal": {
                "name": "Wiley Interdisciplinary Reviews: Computational Molecular Science",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Xue2018AdvancesAC,\n author = {Dongyu Xue and Yukang Gong and Zhao-Yi Yang and Guohui Chuai and Sheng Qu and Ai-Zong Shen and Jing Yu and Qi Liu},\n booktitle = {WIREs Computational Molecular Science},\n journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},\n title = {Advances and challenges in deep generative models for de novo molecule generation},\n volume = {9},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:36e21d8093361027088c1977ebaa2acf105c2b28",
            "@type": "ScholarlyArticle",
            "paperId": "36e21d8093361027088c1977ebaa2acf105c2b28",
            "corpusId": 257050406,
            "url": "https://www.semanticscholar.org/paper/36e21d8093361027088c1977ebaa2acf105c2b28",
            "title": "On Provable Copyright Protection for Generative Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2023,
            "externalIds": {
                "DBLP": "conf/icml/VyasKB23",
                "ArXiv": "2302.10870",
                "CorpusId": 257050406
            },
            "abstract": "There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.",
            "referenceCount": 40,
            "citationCount": 28,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Law",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2023-02-21",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vyas2023OnPC,\n author = {Nikhil Vyas and S. Kakade and B. Barak},\n booktitle = {International Conference on Machine Learning},\n pages = {35277-35299},\n title = {On Provable Copyright Protection for Generative Models},\n year = {2023}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8f13c7e47f51943d98302ed25363f50baf02cb22",
            "@type": "ScholarlyArticle",
            "paperId": "8f13c7e47f51943d98302ed25363f50baf02cb22",
            "corpusId": 54447962,
            "url": "https://www.semanticscholar.org/paper/8f13c7e47f51943d98302ed25363f50baf02cb22",
            "title": "Differentially Private Data Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1812.02274",
                "MAG": "2902027132",
                "DBLP": "journals/corr/abs-1812-02274",
                "CorpusId": 54447962
            },
            "abstract": "Deep neural networks (DNNs) have recently been widely adopted in various applications, and such success is largely due to a combination of algorithmic breakthroughs, computation resource improvements, and access to a large amount of data. However, the large-scale data collections required for deep learning often contain sensitive information, therefore raising many privacy concerns. Prior research has shown several successful attacks in inferring sensitive training data information, such as model inversion, membership inference, and generative adversarial networks (GAN) based leakage attacks against collaborative deep learning. In this paper, to enable learning efficiency as well as to generate data with privacy guarantees and high utility, we propose a differentially private autoencoder-based generative model (DP-AuGM) and a differentially private variational autoencoder-based generative model (DP-VaeGM). We evaluate the robustness of two proposed models. We show that DP-AuGM can effectively defend against the model inversion, membership inference, and GAN-based attacks. We also show that DP-VaeGM is robust against the membership inference attack. We conjecture that the key to defend against the model inversion and GAN-based attacks is not due to differential privacy but the perturbation of training data. Finally, we demonstrate that both DP-AuGM and DP-VaeGM can be easily integrated with real-world machine learning applications, such as machine learning as a service and federated learning, which are otherwise threatened by the membership inference attack and the GAN-based attack, respectively.",
            "referenceCount": 64,
            "citationCount": 66,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1812.02274"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018DifferentiallyPD,\n author = {Qingrong Chen and Chong Xiang and Minhui Xue and Bo Li and N. Borisov and Dali Kaafar and Haojin Zhu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Differentially Private Data Generative Models},\n volume = {abs/1812.02274},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:21b786b3f870fc7fa247c143aa41de88b1fc6141",
            "@type": "ScholarlyArticle",
            "paperId": "21b786b3f870fc7fa247c143aa41de88b1fc6141",
            "corpusId": 49657329,
            "url": "https://www.semanticscholar.org/paper/21b786b3f870fc7fa247c143aa41de88b1fc6141",
            "title": "Glow: Generative Flow with Invertible 1x1 Convolutions",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963139417",
                "DBLP": "journals/corr/abs-1807-03039",
                "ArXiv": "1807.03039",
                "CorpusId": 49657329
            },
            "abstract": "Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at this https URL",
            "referenceCount": 29,
            "citationCount": 2393,
            "influentialCitationCount": 460,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.03039"
            },
            "citationStyles": {
                "bibtex": "@Article{Kingma2018GlowGF,\n author = {Diederik P. Kingma and Prafulla Dhariwal},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Glow: Generative Flow with Invertible 1x1 Convolutions},\n volume = {abs/1807.03039},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
            "@type": "ScholarlyArticle",
            "paperId": "5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
            "corpusId": 248157108,
            "url": "https://www.semanticscholar.org/paper/5288b9f3a9f575543f44c39e1d3b78b3ca4c99da",
            "title": "InCoder: A Generative Model for Code Infilling and Synthesis",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2204-05999",
                "ArXiv": "2204.05999",
                "DOI": "10.48550/arXiv.2204.05999",
                "CorpusId": 248157108
            },
            "abstract": "Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first generative model that is able to directly perform zero-shot code infilling, which we evaluate on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. The InCoder models and code are publicly released. https://sites.google.com/view/incoder-code-models",
            "referenceCount": 81,
            "citationCount": 257,
            "influentialCitationCount": 52,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2204.05999",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-04-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2204.05999"
            },
            "citationStyles": {
                "bibtex": "@Article{Fried2022InCoderAG,\n author = {Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida I. Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Wen-tau Yih and Luke Zettlemoyer and M. Lewis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {InCoder: A Generative Model for Code Infilling and Synthesis},\n volume = {abs/2204.05999},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a124978220b2214929d6e6c6cee0a4598d8f3d1",
            "@type": "ScholarlyArticle",
            "paperId": "2a124978220b2214929d6e6c6cee0a4598d8f3d1",
            "corpusId": 14570343,
            "url": "https://www.semanticscholar.org/paper/2a124978220b2214929d6e6c6cee0a4598d8f3d1",
            "title": "Learning in Implicit Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2530741948",
                "DBLP": "journals/corr/MohamedL16",
                "ArXiv": "1610.03483",
                "CorpusId": 14570343
            },
            "abstract": "Generative adversarial networks (GANs) provide an algorithmic framework for constructing generative models with several appealing properties: they do not require a likelihood function to be specified, only a generating procedure; they provide samples that are sharp and compelling; and they allow us to harness our knowledge of building highly accurate neural network classifiers. Here, we develop our understanding of GANs with the aim of forming a rich view of this growing area of machine learning---to build connections to the diverse set of statistical thinking on this topic, of which much can be gained by a mutual exchange of ideas. We frame GANs within the wider landscape of algorithms for learning in implicit generative models--models that only specify a stochastic procedure with which to generate data--and relate these ideas to modelling problems in related fields, such as econometrics and approximate Bayesian computation. We develop likelihood-free inference methods and highlight hypothesis testing as a principle for learning in implicit generative models, using which we are able to derive the objective function used by GANs, and many other related objectives. The testing viewpoint directs our focus to the general problem of density ratio estimation. There are four approaches for density ratio estimation, one of which is a solution using classifiers to distinguish real from generated data. Other approaches such as divergence minimisation and moment matching have also been explored in the GAN literature, and we synthesise these views to form an understanding in terms of the relationships between them and the wider literature, highlighting avenues for future exploration and cross-pollination.",
            "referenceCount": 61,
            "citationCount": 371,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-10-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1610.03483"
            },
            "citationStyles": {
                "bibtex": "@Article{Mohamed2016LearningII,\n author = {S. Mohamed and Balaji Lakshminarayanan},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning in Implicit Generative Models},\n volume = {abs/1610.03483},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c597874535c1537d7ddff3b3723015b4dc79d30",
            "@type": "ScholarlyArticle",
            "paperId": "7c597874535c1537d7ddff3b3723015b4dc79d30",
            "corpusId": 246680316,
            "url": "https://www.semanticscholar.org/paper/7c597874535c1537d7ddff3b3723015b4dc79d30",
            "title": "MaskGIT: Masked Generative Image Transformer",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2202-04200",
                "ArXiv": "2202.04200",
                "DOI": "10.1109/CVPR52688.2022.01103",
                "CorpusId": 246680316
            },
            "abstract": "Generative transformers have experienced rapid popularity growth in the computer vision community in synthesizing high-fidelity and high-resolution images. The best generative transformer models so far, however, still treat an image naively as a sequence of tokens, and decode an image sequentially following the raster scan ordering (i.e. line-by-line). We find this strategy neither optimal nor efficient. This paper proposes a novel image synthesis paradigm using a bidirectional transformer decoder, which we term MaskGIT. During training, MaskGIT learns to predict randomly masked tokens by attending to tokens in all directions. At inference time, the model begins with generating all tokens of an image simultaneously, and then refines the image iteratively conditioned on the previous generation. Our experiments demonstrate that MaskGIT significantly outperforms the state-of-the-art transformer model on the ImageNet dataset, and accelerates autoregressive decoding by up to 48x. Besides, we illustrate that MaskGIT can be easily extended to various image editing tasks, such as inpainting, extrapolation, and image manipulation. Project page: masked-generative-image-transformer.github.io.",
            "referenceCount": 60,
            "citationCount": 218,
            "influentialCitationCount": 47,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2202.04200",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-02-08",
            "journal": {
                "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chang2022MaskGITMG,\n author = {Huiwen Chang and Han Zhang and Lu Jiang and Ce Liu and W. Freeman},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11305-11315},\n title = {MaskGIT: Masked Generative Image Transformer},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:19db8f6acb84546930c6ba22b6fde1c73cfcc4ba",
            "@type": "ScholarlyArticle",
            "paperId": "19db8f6acb84546930c6ba22b6fde1c73cfcc4ba",
            "corpusId": 13056261,
            "url": "https://www.semanticscholar.org/paper/19db8f6acb84546930c6ba22b6fde1c73cfcc4ba",
            "title": "Joint Multimodal Learning with Deep Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/SuzukiNM16",
                "MAG": "2950626700",
                "ArXiv": "1611.01891",
                "CorpusId": 13056261
            },
            "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.",
            "referenceCount": 22,
            "citationCount": 189,
            "influentialCitationCount": 40,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01891"
            },
            "citationStyles": {
                "bibtex": "@Article{Suzuki2016JointML,\n author = {Masahiro Suzuki and Kotaro Nakayama and Y. Matsuo},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Joint Multimodal Learning with Deep Generative Models},\n volume = {abs/1611.01891},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c6437e7fb44f13a5ff610c9daf8fa7ce4a6c1ec3",
            "@type": "ScholarlyArticle",
            "paperId": "c6437e7fb44f13a5ff610c9daf8fa7ce4a6c1ec3",
            "corpusId": 9150168,
            "url": "https://www.semanticscholar.org/paper/c6437e7fb44f13a5ff610c9daf8fa7ce4a6c1ec3",
            "title": "The Riemannian Geometry of Deep Generative Models",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2768548271",
                "DBLP": "journals/corr/abs-1711-08014",
                "ArXiv": "1711.08014",
                "DOI": "10.1109/CVPRW.2018.00071",
                "CorpusId": 9150168
            },
            "abstract": "Deep generative models learn a mapping from a low-dimensional latent space to a high-dimensional data space. Under certain regularity conditions, these models parameterize nonlinear manifolds in the data space. In this paper, we investigate the Riemannian geometry of these generated manifolds. First, we develop efficient algorithms for computing geodesic curves, which provide an intrinsic notion of distance between points on the manifold. Second, we develop an algorithm for parallel translation of a tangent vector along a path on the manifold. We show how parallel translation can be used to generate analogies, i.e., to transport a change in one data point into a semantically similar change of another data point. Our experiments on real image data show that the manifolds learned by deep generative models, while nonlinear, are surprisingly close to zero curvature. The practical implication is that linear paths in the latent space closely approximate geodesics on the generated manifold.",
            "referenceCount": 25,
            "citationCount": 142,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1711.08014",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-21",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shao2017TheRG,\n author = {Hang Shao and Abhishek Kumar and P. Fletcher},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {428-4288},\n title = {The Riemannian Geometry of Deep Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4729ac7bfdb707e3207b0a91b57a2f907f5351b",
            "@type": "ScholarlyArticle",
            "paperId": "e4729ac7bfdb707e3207b0a91b57a2f907f5351b",
            "corpusId": 51932739,
            "url": "https://www.semanticscholar.org/paper/e4729ac7bfdb707e3207b0a91b57a2f907f5351b",
            "title": "Histopathology stain-color normalization using deep generative models",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2894257256",
                "CorpusId": 51932739
            },
            "abstract": "Performance of designed CAD algorithms for histopathology image analysis is affected by the amount of variations in the samples such as color and intensity of stained images. Stain-color normalization is a well-studied technique for compensating such effects at the input of CAD systems. In this paper, we introduce unsupervised generative neural networks for performing stain-color normalization. For color normalization in stained hematoxylin and eosin (H&E) images, we present three methods based on three frameworks for deep generative models: variational auto-encoder (VAE), generative adversarial networks (GAN) and deep convolutional Gaussian mixture models (DCGMM). Our contribution is defining the color normalization as a learning generative model that is able to generate various color copies of the input image through a nonlinear parametric transformation. In contrast to earlier generative models proposed for stain-color normalization, our approach does not need any labels for data or any other assumptions about the H&E image content. Furthermore, our models learn a parametric transformation during training and can convert the color information of an input image to resemble any arbitrary reference image. This property is essential in time-critical CAD systems in case of changing the reference image, since our approach does not need retraining in contrast to other proposed generative models for stain-color normalization. Experiments on histopathological H&E images with high staining variations, collected from different laboratories, show that our proposed models outperform quantitatively state-of-the-art methods in the measure of color constancy with at least 10-15%, while the converted images are visually in agreement with this performance improvement.",
            "referenceCount": 35,
            "citationCount": 39,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-07-04",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Zanjani2018HistopathologySN,\n author = {F. G. Zanjani and S. Zinger and B. Bejnordi and J. Laak},\n pages = {1-11},\n title = {Histopathology stain-color normalization using deep generative models},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6b2db002cbc5312e4796de4d4b14573df2c01648",
            "@type": "ScholarlyArticle",
            "paperId": "6b2db002cbc5312e4796de4d4b14573df2c01648",
            "corpusId": 592386,
            "url": "https://www.semanticscholar.org/paper/6b2db002cbc5312e4796de4d4b14573df2c01648",
            "title": "Learning Hierarchical Features from Deep Generative Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/ZhaoSE17",
                "MAG": "2963664914",
                "CorpusId": 592386
            },
            "abstract": "Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with some existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that does not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no taskspecific regularization.",
            "referenceCount": 27,
            "citationCount": 98,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2017LearningHF,\n author = {Shengjia Zhao and Jiaming Song and Stefano Ermon},\n booktitle = {International Conference on Machine Learning},\n pages = {4091-4099},\n title = {Learning Hierarchical Features from Deep Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:354fb2f2607040fb461095cf3b2c90fbc66bd852",
            "@type": "ScholarlyArticle",
            "paperId": "354fb2f2607040fb461095cf3b2c90fbc66bd852",
            "corpusId": 5751352,
            "url": "https://www.semanticscholar.org/paper/354fb2f2607040fb461095cf3b2c90fbc66bd852",
            "title": "Learning the Structure of Generative Models without Labeled Data",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2592362528",
                "ArXiv": "1703.00854",
                "DBLP": "conf/icml/BachHRR17",
                "CorpusId": 5751352,
                "PubMed": "30882087"
            },
            "abstract": "Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model's dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the \u2113 1-regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100\u00d7 faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.",
            "referenceCount": 37,
            "citationCount": 144,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-02",
            "journal": {
                "name": "Proceedings of machine learning research",
                "volume": "70"
            },
            "citationStyles": {
                "bibtex": "@Article{Bach2017LearningTS,\n author = {Stephen H. Bach and Bryan D. He and Alexander J. Ratner and C. R\u00e9},\n booktitle = {International Conference on Machine Learning},\n journal = {Proceedings of machine learning research},\n pages = {\n          273-82\n        },\n title = {Learning the Structure of Generative Models without Labeled Data},\n volume = {70},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:097889e0b93591d75de08f9da661ff882a1532f6",
            "@type": "ScholarlyArticle",
            "paperId": "097889e0b93591d75de08f9da661ff882a1532f6",
            "corpusId": 5024767,
            "url": "https://www.semanticscholar.org/paper/097889e0b93591d75de08f9da661ff882a1532f6",
            "title": "Learning Disentangled Representations with Semi-Supervised Deep Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2950442810",
                "ArXiv": "1706.00400",
                "DBLP": "conf/nips/NarayanaswamyPM17",
                "DOI": "10.17863/CAM.42159",
                "CorpusId": 5024767
            },
            "abstract": "Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.",
            "referenceCount": 37,
            "citationCount": 334,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.00400"
            },
            "citationStyles": {
                "bibtex": "@Article{Narayanaswamy2017LearningDR,\n author = {Siddharth Narayanaswamy and Brooks Paige and Jan-Willem van de Meent and Alban Desmaison and Noah D. Goodman and Pushmeet Kohli and Frank D. Wood and Philip H. S. Torr},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},\n volume = {abs/1706.00400},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac53b6d4d1e94af319fdb19ceaa06e77f3ff27c1",
            "@type": "ScholarlyArticle",
            "paperId": "ac53b6d4d1e94af319fdb19ceaa06e77f3ff27c1",
            "corpusId": 4879029,
            "url": "https://www.semanticscholar.org/paper/ac53b6d4d1e94af319fdb19ceaa06e77f3ff27c1",
            "title": "Metrics for Deep Generative Models",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/aistats/ChenKKJBS18",
                "MAG": "2951643992",
                "ArXiv": "1711.01204",
                "CorpusId": 4879029
            },
            "abstract": "Neural samplers such as variational autoencoders (VAEs) or generative adversarial networks (GANs) approximate distributions by transforming samples from a simple random source---the latent space---to samples from a more complex distribution represented by a dataset. While the manifold hypothesis implies that the density induced by a dataset contains large regions of low density, the training criterions of VAEs and GANs will make the latent space densely covered. Consequently points that are separated by low-density regions in observation space will be pushed together in latent space, making stationary distances poor proxies for similarity. We transfer ideas from Riemannian geometry to this setting, letting the distance between two points be the shortest path on a Riemannian manifold induced by the transformation. The method yields a principled distance measure, provides a tool for visual inspection of deep generative models, and an alternative to linear interpolation in latent space. In addition, it can be applied for robot movement generalization using previously learned skills. The method is evaluated on a synthetic dataset with known ground truth; on a simulated robot arm dataset; on human motion capture data; and on a generative model of handwritten digits.",
            "referenceCount": 36,
            "citationCount": 92,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2017MetricsFD,\n author = {Nutan Chen and Alexej Klushyn and Richard Kurle and Xueyan Jiang and Justin Bayer and Patrick van der Smagt},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {1540-1550},\n title = {Metrics for Deep Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:858dc7408c27702ec42778599fc8d11f73ef3f76",
            "@type": "ScholarlyArticle",
            "paperId": "858dc7408c27702ec42778599fc8d11f73ef3f76",
            "corpusId": 13583585,
            "url": "https://www.semanticscholar.org/paper/858dc7408c27702ec42778599fc8d11f73ef3f76",
            "title": "On the Quantitative Analysis of Decoder-Based Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.04273",
                "MAG": "2953116488",
                "DBLP": "conf/iclr/WuBSG17",
                "CorpusId": 13583585
            },
            "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at this https URL. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.",
            "referenceCount": 38,
            "citationCount": 214,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.04273"
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2016OnTQ,\n author = {Yuhuai Wu and Yuri Burda and R. Salakhutdinov and R. Grosse},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the Quantitative Analysis of Decoder-Based Generative Models},\n volume = {abs/1611.04273},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0811597b0851b7ebe21aadce7cb4daac4664b44f",
            "@type": "ScholarlyArticle",
            "paperId": "0811597b0851b7ebe21aadce7cb4daac4664b44f",
            "corpusId": 5985692,
            "url": "https://www.semanticscholar.org/paper/0811597b0851b7ebe21aadce7cb4daac4664b44f",
            "title": "One-Shot Generalization in Deep Generative Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2949247522",
                "DBLP": "journals/corr/RezendeMDGW16",
                "ArXiv": "1603.05106",
                "CorpusId": 5985692
            },
            "abstract": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples-- having seen new examples just once--providing an important class of general-purpose models for one-shot machine learning.",
            "referenceCount": 33,
            "citationCount": 239,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-03-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1603.05106"
            },
            "citationStyles": {
                "bibtex": "@Article{Rezende2016OneShotGI,\n author = {Danilo Jimenez Rezende and S. Mohamed and Ivo Danihelka and Karol Gregor and Daan Wierstra},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {One-Shot Generalization in Deep Generative Models},\n volume = {abs/1603.05106},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c0c3ea2aa244920a206c196af3da1b81934a8c2e",
            "@type": "ScholarlyArticle",
            "paperId": "c0c3ea2aa244920a206c196af3da1b81934a8c2e",
            "corpusId": 3508638,
            "url": "https://www.semanticscholar.org/paper/c0c3ea2aa244920a206c196af3da1b81934a8c2e",
            "title": "On Unifying Deep Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/HuYSX17",
                "ArXiv": "1706.00550",
                "MAG": "2621386477",
                "CorpusId": 3508638
            },
            "abstract": "Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.",
            "referenceCount": 65,
            "citationCount": 117,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-06-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.00550"
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2017OnUD,\n author = {Zhiting Hu and Zichao Yang and R. Salakhutdinov and E. Xing},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On Unifying Deep Generative Models},\n volume = {abs/1706.00550},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a57d47b762341340656d5b5caa84f370d9d31063",
            "@type": "ScholarlyArticle",
            "paperId": "a57d47b762341340656d5b5caa84f370d9d31063",
            "corpusId": 252438648,
            "url": "https://www.semanticscholar.org/paper/a57d47b762341340656d5b5caa84f370d9d31063",
            "title": "GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2209-11163",
                "ArXiv": "2209.11163",
                "DOI": "10.48550/arXiv.2209.11163",
                "CorpusId": 252438648
            },
            "abstract": "As several industries are moving towards modeling massive 3D virtual worlds, the need for content creation tools that can scale in terms of the quantity, quality, and diversity of 3D content is becoming evident. In our work, we aim to train performant 3D generative models that synthesize textured meshes which can be directly consumed by 3D rendering engines, thus immediately usable in downstream applications. Prior works on 3D generative modeling either lack geometric details, are limited in the mesh topology they can produce, typically do not support textures, or utilize neural renderers in the synthesis process, which makes their use in common 3D software non-trivial. In this work, we introduce GET3D, a Generative model that directly generates Explicit Textured 3D meshes with complex topology, rich geometric details, and high-fidelity textures. We bridge recent success in the differentiable surface modeling, differentiable rendering as well as 2D Generative Adversarial Networks to train our model from 2D image collections. GET3D is able to generate high-quality 3D textured meshes, ranging from cars, chairs, animals, motorbikes and human characters to buildings, achieving significant improvements over previous methods.",
            "referenceCount": 72,
            "citationCount": 182,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.11163",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-09-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2209.11163"
            },
            "citationStyles": {
                "bibtex": "@Article{Gao2022GET3DAG,\n author = {Jun Gao and Tianchang Shen and Zian Wang and Wenzheng Chen and K. Yin and Daiqing Li and O. Litany and Zan Gojcic and S. Fidler},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {GET3D: A Generative Model of High Quality 3D Textured Shapes Learned from Images},\n volume = {abs/2209.11163},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7ebb535dc6c9f1227558e53eabb798c4d4ad424",
            "@type": "ScholarlyArticle",
            "paperId": "b7ebb535dc6c9f1227558e53eabb798c4d4ad424",
            "corpusId": 52012645,
            "url": "https://www.semanticscholar.org/paper/b7ebb535dc6c9f1227558e53eabb798c4d4ad424",
            "title": "Skill Rating for Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2886824226",
                "ArXiv": "1808.04888",
                "DBLP": "journals/corr/abs-1808-04888a",
                "CorpusId": 52012645
            },
            "abstract": "We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.",
            "referenceCount": 37,
            "citationCount": 33,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1808.04888"
            },
            "citationStyles": {
                "bibtex": "@Article{Olsson2018SkillRF,\n author = {Catherine Olsson and Surya Bhupatiraju and Tom B. Brown and Augustus Odena and I. Goodfellow},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Skill Rating for Generative Models},\n volume = {abs/1808.04888},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f1865727b9b3c681c09b7e8429affe316cc4eb17",
            "@type": "ScholarlyArticle",
            "paperId": "f1865727b9b3c681c09b7e8429affe316cc4eb17",
            "corpusId": 21978173,
            "url": "https://www.semanticscholar.org/paper/f1865727b9b3c681c09b7e8429affe316cc4eb17",
            "title": "Generating and designing DNA with deep generative models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1712.06148",
                "DBLP": "journals/corr/abs-1712-06148",
                "MAG": "2781256610",
                "CorpusId": 21978173
            },
            "abstract": "We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network; a DNA-based variant of the activation maximization (\"deep dream\") design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays, allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.",
            "referenceCount": 40,
            "citationCount": 126,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.06148"
            },
            "citationStyles": {
                "bibtex": "@Article{Killoran2017GeneratingAD,\n author = {N. Killoran and Leo J. Lee and Andrew Delong and D. Duvenaud and B. Frey},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generating and designing DNA with deep generative models},\n volume = {abs/1712.06148},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ea35877e5cc4f361183f784092522afb5954ccdc",
            "@type": "ScholarlyArticle",
            "paperId": "ea35877e5cc4f361183f784092522afb5954ccdc",
            "corpusId": 52274965,
            "url": "https://www.semanticscholar.org/paper/ea35877e5cc4f361183f784092522afb5954ccdc",
            "title": "Geodesic Clustering in Deep Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1809-04747",
                "ArXiv": "1809.04747",
                "MAG": "2890571755",
                "CorpusId": 52274965
            },
            "abstract": "Deep generative models are tremendously successful in learning low-dimensional latent representations that well-describe the data. These representations, however, tend to much distort relationships between points, i.e. pairwise distances tend to not reflect semantic similarities well. This renders unsupervised tasks, such as clustering, difficult when working with the latent representations. We demonstrate that taking the geometry of the generative model into account is sufficient to make simple clustering algorithms work well over latent representations. Leaning on the recent finding that deep generative models constitute stochastically immersed Riemannian manifolds, we propose an efficient algorithm for computing geodesics (shortest paths) and computing distances in the latent space, while taking its distortion into account. We further propose a new architecture for modeling uncertainty in variational autoencoders, which is essential for understanding the geometry of deep generative models. Experiments show that the geodesic distance is very likely to reflect the internal structure of the data.",
            "referenceCount": 36,
            "citationCount": 23,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-13",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1809.04747"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2018GeodesicCI,\n author = {Tao Yang and Georgios Arvanitidis and Dongmei Fu and Xiaogang Li and S\u00f8ren Hauberg},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Geodesic Clustering in Deep Generative Models},\n volume = {abs/1809.04747},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c78468c2f87efabf8845ccd210ced364d45e5eab",
            "@type": "ScholarlyArticle",
            "paperId": "c78468c2f87efabf8845ccd210ced364d45e5eab",
            "corpusId": 10480989,
            "url": "https://www.semanticscholar.org/paper/c78468c2f87efabf8845ccd210ced364d45e5eab",
            "title": "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/MiaoB16",
                "MAG": "2951652470",
                "ACL": "D16-1031",
                "ArXiv": "1609.07317",
                "DOI": "10.18653/v1/D16-1031",
                "CorpusId": 10480989
            },
            "abstract": "In this work we explore deep generative models of text in which the latent representation of a document is itself drawn from a discrete language model distribution. We formulate a variational auto-encoder for inference in this model and apply it to the task of compressing sentences. In this application the generative model first draws a latent summary sentence from a background language model, and then subsequently draws the observed sentence conditioned on this latent summary. In our empirical evaluation we show that generative formulations of both abstractive and extractive compression yield state-of-the-art results when trained on a large amount of supervised data. Further, we explore semi-supervised compression scenarios where we show that it is possible to achieve performance competitive with previously proposed supervised models while training on a fraction of the supervised data.",
            "referenceCount": 50,
            "citationCount": 219,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D16-1031.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1609.07317"
            },
            "citationStyles": {
                "bibtex": "@Article{Miao2016LanguageAA,\n author = {Yishu Miao and Phil Blunsom},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n journal = {ArXiv},\n title = {Language as a Latent Variable: Discrete Generative Models for Sentence Compression},\n volume = {abs/1609.07317},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ba0dfca191265ffd6630291dcd59c14150e8b22a",
            "@type": "ScholarlyArticle",
            "paperId": "ba0dfca191265ffd6630291dcd59c14150e8b22a",
            "corpusId": 54461643,
            "url": "https://www.semanticscholar.org/paper/ba0dfca191265ffd6630291dcd59c14150e8b22a",
            "title": "Recent Trends in Deep Generative Models: a Review",
            "venue": "2018 3rd International Conference on Computer Science and Engineering (UBMK)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2905027213",
                "DOI": "10.1109/UBMK.2018.8566353",
                "CorpusId": 54461643
            },
            "abstract": "With the recent improvements in computation power and high scale datasets, many interesting studies have been presented based on discriminative models such as Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) for various classification problems. These models have achieved current state-of-the-art results in almost all applications of computer vision but not sufficient sampling out-of-data, understanding of data distribution. By pioneers of the deep learning community, generative adversarial training is defined as the most exciting topic of computer vision field nowadays. With the influence of these views and potential usages of generative models, many kinds of researches were conducted using generative models especially Generative Adversarial Network (GAN) and Autoencoder (AE) based models with an increasing trend. In this study, a comprehensive review of generative models with defining relations among them is presented for a better understanding of GANs and AEs by pointing the importance of generative models.",
            "referenceCount": 64,
            "citationCount": 32,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Conference",
                "Review"
            ],
            "publicationDate": "2018-09-01",
            "journal": {
                "name": "2018 3rd International Conference on Computer Science and Engineering (UBMK)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Conference{Turhan2018RecentTI,\n author = {C. G. Turhan and H. \u015e. Bilge},\n booktitle = {2018 3rd International Conference on Computer Science and Engineering (UBMK)},\n journal = {2018 3rd International Conference on Computer Science and Engineering (UBMK)},\n pages = {574-579},\n title = {Recent Trends in Deep Generative Models: a Review},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7c4f52328c2869bdff8034d2867baa5b67d0ce27",
            "@type": "ScholarlyArticle",
            "paperId": "7c4f52328c2869bdff8034d2867baa5b67d0ce27",
            "corpusId": 3052834,
            "url": "https://www.semanticscholar.org/paper/7c4f52328c2869bdff8034d2867baa5b67d0ce27",
            "title": "LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2617174679",
                "ArXiv": "1705.07663",
                "DBLP": "journals/corr/HayesMDC17",
                "CorpusId": 3052834
            },
            "abstract": "Generative models are increasingly used to artificially generate various kinds of data, including high-quality images and videos. These models are used to estimate the underlying distribution of a dataset and randomly generate realistic samples according to their estimated distribution. However, the data used to train these models is often sensitive, thus prompting the need to evaluate information leakage from producing synthetic samples with generative models---specifically, whether an adversary can infer information about the data used to train the models. In this paper, we present the first membership inference attack on generative models. To mount the attack, we train a Generative Adversarial Network (GAN), which combines a discriminative and a generative model, to detect overfitting and recognize inputs that are part of training datasets by relying on the discriminator's capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, and show how to improve the latter using limited auxiliary knowledge of dataset samples. We test our attacks on several state-of-the-art models, such as Deep Convolutional GAN (DCGAN), Boundary Equilibrium GAN (BEGAN), and the combination of DCGAN with a Variational Autoencoder (DCGAN+VAE), using datasets consisting of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). The white-box attacks are 100% successful at inferring which samples were used to train the target model, and the black-box ones succeeds with 80% accuracy. Finally, we discuss the sensitivity of our attacks to different training parameters, and their robustness against mitigation strategies, finding that successful defenses often result in significant worse performances of the generative models in terms of training stability and/or sample quality.",
            "referenceCount": 48,
            "citationCount": 92,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.07663"
            },
            "citationStyles": {
                "bibtex": "@Article{Hayes2017LOGANEP,\n author = {Jamie Hayes and Luca Melis and G. Danezis and Emiliano De Cristofaro},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {LOGAN: Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks},\n volume = {abs/1705.07663},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
            "@type": "ScholarlyArticle",
            "paperId": "329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
            "corpusId": 7451980,
            "url": "https://www.semanticscholar.org/paper/329f14d68f8e4cbc3ee5109e70c76c6f3b63a19a",
            "title": "CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1709-02023",
                "ArXiv": "1709.02023",
                "MAG": "2964133637",
                "CorpusId": 7451980
            },
            "abstract": "We propose an adversarial training procedure for learning a causal implicit generative model for a given causal graph. We show that adversarial training can be used to learn a generative model with true observational and interventional distributions if the generator architecture is consistent with the given causal graph. We consider the application of generating faces based on given binary labels where the dependency structure between the labels is preserved with a causal graph. This problem can be seen as learning a causal implicit generative model for the image and labels. We devise a two-stage procedure for this problem. First we train a causal implicit generative model over binary labels using a neural network consistent with a causal graph as the generator. We empirically show that WassersteinGAN can be used to output discrete labels. Later, we propose two new conditional GAN architectures, which we call CausalGAN and CausalBEGAN. We show that the optimal generator of the CausalGAN, given the labels, samples from the image distributions conditioned on these labels. The conditional GAN combined with a trained causal implicit generative model for the labels is then a causal implicit generative model over the labels and the generated image. We show that the proposed architectures can be used to sample from observational and interventional image distributions, even for interventions which do not naturally occur in the dataset.",
            "referenceCount": 37,
            "citationCount": 202,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.02023"
            },
            "citationStyles": {
                "bibtex": "@Article{Kocaoglu2017CausalGANLC,\n author = {M. Kocaoglu and Christopher Snyder and A. Dimakis and S. Vishwanath},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training},\n volume = {abs/1709.02023},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e9882bd009c414d4c6d153a5cf340b2d23213d0f",
            "@type": "ScholarlyArticle",
            "paperId": "e9882bd009c414d4c6d153a5cf340b2d23213d0f",
            "corpusId": 19115748,
            "url": "https://www.semanticscholar.org/paper/e9882bd009c414d4c6d153a5cf340b2d23213d0f",
            "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963919714",
                "DBLP": "conf/aaai/GroverDE18",
                "DOI": "10.1609/aaai.v32i1.11829",
                "CorpusId": 19115748
            },
            "abstract": "\n \n Adversarial learning of probabilistic models has recently emerged as a promising alternative to maximum likelihood. Implicit models such as generative adversarial networks (GAN) often generate better samples compared to explicit models trained by maximum likelihood. Yet, GANs sidestep the characterization of an explicit density which makes quantitative evaluations challenging. To bridge this gap, we propose Flow-GANs, a generative adversarial network for which we can perform exact likelihood evaluation, thus supporting both adversarial and maximum likelihood training. When trained adversarially, Flow-GANs generate high-quality samples but attain extremely poor log-likelihood scores, inferior even to a mixture model memorizing the training data; the opposite is true when trained by maximum likelihood. Results on MNIST and CIFAR-10 demonstrate that hybrid training can attain high held-out likelihoods while retaining visual fidelity in the generated samples.\n \n",
            "referenceCount": 39,
            "citationCount": 178,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11829/11688",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Grover2017FlowGANCM,\n author = {Aditya Grover and Manik Dhar and Stefano Ermon},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {3069-3076},\n title = {Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1f22de83d912176cb8857efa1c6d65b14d6a2f5c",
            "@type": "ScholarlyArticle",
            "paperId": "1f22de83d912176cb8857efa1c6d65b14d6a2f5c",
            "corpusId": 255749330,
            "url": "https://www.semanticscholar.org/paper/1f22de83d912176cb8857efa1c6d65b14d6a2f5c",
            "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2023,
            "externalIds": {
                "DBLP": "journals/corr/abs-2301-04655",
                "ArXiv": "2301.04655",
                "DOI": "10.48550/arXiv.2301.04655",
                "CorpusId": 255749330
            },
            "abstract": "During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.",
            "referenceCount": 35,
            "citationCount": 89,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2301.04655",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2023-01-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2301.04655"
            },
            "citationStyles": {
                "bibtex": "@Article{Gozalo-Brizuela2023ChatGPTIN,\n author = {Roberto Gozalo-Brizuela and E.C. Garrido-Merch\u00e1n},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {ChatGPT is not all you need. A State of the Art Review of large Generative AI models},\n volume = {abs/2301.04655},\n year = {2023}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7603cd7bdc0b686971ceb2a26b31b2e2bd874184",
            "@type": "ScholarlyArticle",
            "paperId": "7603cd7bdc0b686971ceb2a26b31b2e2bd874184",
            "corpusId": 1481635,
            "url": "https://www.semanticscholar.org/paper/7603cd7bdc0b686971ceb2a26b31b2e2bd874184",
            "title": "Zero-Shot Learning via Class-Conditioned Deep Generative Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2769182847",
                "DBLP": "journals/corr/abs-1711-05820",
                "ArXiv": "1711.05820",
                "DOI": "10.1609/aaai.v32i1.11600",
                "CorpusId": 1481635
            },
            "abstract": "\n \n We present a deep generative model for Zero-Shot Learning (ZSL). Unlike most existing methods for this problem, that represent each class as a point (via a semantic embedding), we represent each seen/unseen class using a class-specific latent-space distribution, conditioned on class attributes. We use these latent-space distributions as a prior for a supervised variational autoencoder (VAE), which also facilitates learning highly discriminative feature representations for the inputs. The entire framework is learned end-to-end using only the seen-class training data. At test time, the label for an unseen-class test input is the class that maximizes the VAE lower bound. We further extend the model to a (i) semi-supervised/transductive setting by leveraging unlabeled unseen-class data via an unsupervised learning module, and (ii) few-shot learning where we also have a small number of labeled inputs from the unseen classes. We compare our model with several state-of-the-art methods through a comprehensive set of experiments on a variety of benchmark data sets.\n \n",
            "referenceCount": 50,
            "citationCount": 134,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11600/11459",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.05820"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2017ZeroShotLV,\n author = {Wenlin Wang and Yunchen Pu and V. Verma and Kai Fan and Yizhe Zhang and Changyou Chen and Piyush Rai and L. Carin},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Zero-Shot Learning via Class-Conditioned Deep Generative Models},\n volume = {abs/1711.05820},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2d7e2e5e87a84021ddaa85446ed0859cb6c91193",
            "@type": "ScholarlyArticle",
            "paperId": "2d7e2e5e87a84021ddaa85446ed0859cb6c91193",
            "corpusId": 26026058,
            "url": "https://www.semanticscholar.org/paper/2d7e2e5e87a84021ddaa85446ed0859cb6c91193",
            "title": "Learning Universal Adversarial Perturbations with Generative Models",
            "venue": "2018 IEEE Security and Privacy Workshops (SPW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2781758978",
                "DBLP": "conf/sp/HayesD18",
                "DOI": "10.1109/SPW.2018.00015",
                "CorpusId": 26026058
            },
            "abstract": "Neural networks are known to be vulnerable to adversarial examples, inputs that have been intentionally perturbed to remain visually similar to the source input, but cause a misclassification. It was recently shown that given a dataset and classifier, there exists so called universal adversarial perturbations, a single perturbation that causes a misclassification when applied to any input. In this work, we introduce universal adversarial networks, a generative network that is capable of fooling a target classifier when it's generated output is added to a clean sample from a dataset. We show that this technique improves on known universal adversarial attacks.",
            "referenceCount": 58,
            "citationCount": 122,
            "influentialCitationCount": 15,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/8420091/8424589/08424631.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-17",
            "journal": {
                "name": "2018 IEEE Security and Privacy Workshops (SPW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hayes2017LearningUA,\n author = {Jamie Hayes and G. Danezis},\n booktitle = {2018 IEEE Security and Privacy Workshops (SPW)},\n journal = {2018 IEEE Security and Privacy Workshops (SPW)},\n pages = {43-49},\n title = {Learning Universal Adversarial Perturbations with Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:657fca895e9217a0306739f8f58332c224b8a82e",
            "@type": "ScholarlyArticle",
            "paperId": "657fca895e9217a0306739f8f58332c224b8a82e",
            "corpusId": 5342684,
            "url": "https://www.semanticscholar.org/paper/657fca895e9217a0306739f8f58332c224b8a82e",
            "title": "Learning Hierarchical Features from Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.08396",
                "DBLP": "journals/corr/ZhaoSE17",
                "MAG": "2591674338",
                "CorpusId": 5342684
            },
            "abstract": "Deep neural networks have been shown to be very successful at learning feature hierarchies in supervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multiple layers of latent variables. In this paper, we prove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and provide some limitations on the kind of features existing models can learn. Finally we propose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interpretable and disentangled hierarchical features on several natural image datasets with no task specific regularization or prior knowledge.",
            "referenceCount": 29,
            "citationCount": 65,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.08396"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhao2017LearningHF,\n author = {Shengjia Zhao and Jiaming Song and Stefano Ermon},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Learning Hierarchical Features from Generative Models},\n volume = {abs/1702.08396},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3897b3d49b23033c9e12c78f684534b602f0c659",
            "@type": "ScholarlyArticle",
            "paperId": "3897b3d49b23033c9e12c78f684534b602f0c659",
            "corpusId": 11242291,
            "url": "https://www.semanticscholar.org/paper/3897b3d49b23033c9e12c78f684534b602f0c659",
            "title": "Generative models for network neuroscience: prospects and promise",
            "venue": "Journal of the Royal Society Interface",
            "publicationVenue": {
                "id": "urn:research:f6537e0e-c3a9-4de5-bd36-19eda0434065",
                "name": "Journal of the Royal Society Interface",
                "alternate_names": [
                    "J R Soc Interface"
                ],
                "issn": "1742-5662",
                "url": "http://rsif.royalsocietypublishing.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963284032",
                "PubMedCentral": "5721166",
                "ArXiv": "1708.07958",
                "DOI": "10.1098/rsif.2017.0623",
                "CorpusId": 11242291,
                "PubMed": "29187640"
            },
            "abstract": "Network neuroscience is the emerging discipline concerned with investigating the complex patterns of interconnections found in neural systems, and identifying principles with which to understand them. Within this discipline, one particularly powerful approach is network generative modelling, in which wiring rules are algorithmically implemented to produce synthetic network architectures with the same properties as observed in empirical network data. Successful models can highlight the principles by which a network is organized and potentially uncover the mechanisms by which it grows and develops. Here, we review the prospects and promise of generative models for network neuroscience. We begin with a primer on network generative models, with a discussion of compressibility and predictability, and utility in intuiting mechanisms, followed by a short history on their use in network science, broadly. We then discuss generative models in practice and application, paying particular attention to the critical need for cross-validation. Next, we review generative models of biological neural networks, both at the cellular and large-scale level, and across a variety of species including Caenorhabditis elegans, Drosophila, mouse, rat, cat, macaque and human. We offer a careful treatment of a few relevant distinctions, including differences between generative models and null models, sufficiency and redundancy, inferring and claiming mechanism, and functional and structural connectivity. We close with a discussion of future directions, outlining exciting frontiers both in empirical data collection efforts as well as in method and theory development that, together, further the utility of the generative network modelling approach for network neuroscience.",
            "referenceCount": 245,
            "citationCount": 81,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2017.0623",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2017-08-26",
            "journal": {
                "name": "Journal of the Royal Society Interface",
                "volume": "14"
            },
            "citationStyles": {
                "bibtex": "@Article{Betzel2017GenerativeMF,\n author = {Richard F. Betzel and D. Bassett},\n booktitle = {Journal of the Royal Society Interface},\n journal = {Journal of the Royal Society Interface},\n title = {Generative models for network neuroscience: prospects and promise},\n volume = {14},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44279244407a64431810f982be6d0c7da4429dd7",
            "@type": "ScholarlyArticle",
            "paperId": "44279244407a64431810f982be6d0c7da4429dd7",
            "corpusId": 252542956,
            "url": "https://www.semanticscholar.org/paper/44279244407a64431810f982be6d0c7da4429dd7",
            "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
            "venue": "Briefings Bioinform.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2210.10341",
                "DBLP": "journals/bib/LuoSXQZPL22",
                "DOI": "10.1093/bib/bbac409",
                "CorpusId": 252542956,
                "PubMed": "36156661"
            },
            "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.",
            "referenceCount": 58,
            "citationCount": 243,
            "influentialCitationCount": 28,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2210.10341",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-09-24",
            "journal": {
                "name": "Briefings in bioinformatics",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Luo2022BioGPTGP,\n author = {Renqian Luo and Liai Sun and Yingce Xia and Tao Qin and Sheng Zhang and Hoifung Poon and Tie-Yan Liu},\n booktitle = {Briefings Bioinform.},\n journal = {Briefings in bioinformatics},\n title = {BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:11fadfe035f08b786279014e82203d0270443165",
            "@type": "ScholarlyArticle",
            "paperId": "11fadfe035f08b786279014e82203d0270443165",
            "corpusId": 88520948,
            "url": "https://www.semanticscholar.org/paper/11fadfe035f08b786279014e82203d0270443165",
            "title": "Inference in generative models using the Wasserstein distance",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2581258669",
                "ArXiv": "1701.05146",
                "DOI": "10.14288/1.0354574",
                "CorpusId": 88520948
            },
            "abstract": "In purely generative models, one can simulate data given parameters but not necessarily evaluate the likelihood. We use Wasserstein distances between empirical distributions of observed data and empirical distributions of synthetic data drawn from such models to estimate their parameters. Previous interest in the Wasserstein distance for statistical inference has been mainly theoretical, due to computational limitations. Thanks to recent advances in numerical transport, the computation of these distances has become feasible, up to controllable approximation errors. We leverage these advances to propose point estimators and quasi-Bayesian distributions for parameter inference, first for independent data. For dependent data, we extend the approach by using delay reconstruction and residual reconstruction techniques. For large data sets, we propose an alternative distance using the Hilbert space-filling curve, which computation scales as nlogn where n is the size of the data. We provide a theoretical study of the proposed estimators, and adaptive Monte Carlo algorithms to approximate them. The approach is illustrated on four examples: a quantile g-and-k distribution, a toggle switch model from systems biology, a Lotka-Volterra model for plankton population sizes and a L\\'evy-driven stochastic volatility model.",
            "referenceCount": 78,
            "citationCount": 75,
            "influentialCitationCount": 6,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-01-18",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Bernton2017InferenceIG,\n author = {Espen Bernton and P. Jacob and Mathieu Gerber and C. Robert},\n title = {Inference in generative models using the Wasserstein distance},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:079723d3687fab106f9532379b492be907549d6d",
            "@type": "ScholarlyArticle",
            "paperId": "079723d3687fab106f9532379b492be907549d6d",
            "corpusId": 202402201,
            "url": "https://www.semanticscholar.org/paper/079723d3687fab106f9532379b492be907549d6d",
            "title": "Capsule Generative Models",
            "venue": "International Conference on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                "name": "International Conference on Artificial Neural Networks",
                "alternate_names": [
                    "Int Conf Artif Neural Netw",
                    "ICANN"
                ],
                "issn": null,
                "url": "http://www.e-nns.org/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icann/LiZ19",
                "MAG": "2972432820",
                "DOI": "10.1007/978-3-030-30487-4_22",
                "CorpusId": 202402201
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 4,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-09-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2019CapsuleGM,\n author = {Yifeng Li and Xiao-Dan Zhu},\n booktitle = {International Conference on Artificial Neural Networks},\n pages = {281-295},\n title = {Capsule Generative Models},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3df0df43e225794b27f3bcfbaf55098bc6134ad6",
            "@type": "ScholarlyArticle",
            "paperId": "3df0df43e225794b27f3bcfbaf55098bc6134ad6",
            "corpusId": 29154705,
            "url": "https://www.semanticscholar.org/paper/3df0df43e225794b27f3bcfbaf55098bc6134ad6",
            "title": "Interpretable dimensionality reduction of single cell transcriptome data with deep generative models",
            "venue": "Nature Communications",
            "publicationVenue": {
                "id": "urn:research:43b3f0f9-489a-4566-8164-02fafde3cd98",
                "name": "Nature Communications",
                "alternate_names": [
                    "Nat Commun"
                ],
                "issn": "2041-1723",
                "url": "https://www.nature.com/ncomms/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949272108",
                "PubMedCentral": "5962608",
                "DOI": "10.1038/s41467-018-04368-5",
                "CorpusId": 29154705,
                "PubMed": "29784946"
            },
            "abstract": null,
            "referenceCount": 68,
            "citationCount": 274,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2017/09/01/178624.full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-09-01",
            "journal": {
                "name": "Nature Communications",
                "volume": "9"
            },
            "citationStyles": {
                "bibtex": "@Article{Ding2017InterpretableDR,\n author = {Jiarui Ding and A. Condon and Sohrab P. Shah},\n booktitle = {Nature Communications},\n journal = {Nature Communications},\n title = {Interpretable dimensionality reduction of single cell transcriptome data with deep generative models},\n volume = {9},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c1ff08b59f00c44f34dfdde55cd53370733a2c19",
            "@type": "ScholarlyArticle",
            "paperId": "c1ff08b59f00c44f34dfdde55cd53370733a2c19",
            "corpusId": 235606261,
            "url": "https://www.semanticscholar.org/paper/c1ff08b59f00c44f34dfdde55cd53370733a2c19",
            "title": "Alias-Free Generative Adversarial Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3174807077",
                "DBLP": "conf/nips/KarrasALHHLA21",
                "ArXiv": "2106.12423",
                "CorpusId": 235606261
            },
            "abstract": "We observe that despite their hierarchical convolutional nature, the synthesis process of typical generative adversarial networks depends on absolute pixel coordinates in an unhealthy manner. This manifests itself as, e.g., detail appearing to be glued to image coordinates instead of the surfaces of depicted objects. We trace the root cause to careless signal processing that causes aliasing in the generator network. Interpreting all signals in the network as continuous, we derive generally applicable, small architectural changes that guarantee that unwanted information cannot leak into the hierarchical synthesis process. The resulting networks match the FID of StyleGAN2 but differ dramatically in their internal representations, and they are fully equivariant to translation and rotation even at subpixel scales. Our results pave the way for generative models better suited for video and animation.",
            "referenceCount": 74,
            "citationCount": 943,
            "influentialCitationCount": 132,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-23",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Karras2021AliasFreeGA,\n author = {Tero Karras and M. Aittala and S. Laine and Erik H\u00e4rk\u00f6nen and Janne Hellsten and J. Lehtinen and Timo Aila},\n booktitle = {Neural Information Processing Systems},\n pages = {852-863},\n title = {Alias-Free Generative Adversarial Networks},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d20af35f65fda506b605182d9b93025b559b1433",
            "@type": "ScholarlyArticle",
            "paperId": "d20af35f65fda506b605182d9b93025b559b1433",
            "corpusId": 1736260,
            "url": "https://www.semanticscholar.org/paper/d20af35f65fda506b605182d9b93025b559b1433",
            "title": "Variational Memory Addressing in Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949101027",
                "ArXiv": "1709.07116",
                "DBLP": "conf/nips/BornscheinMZR17",
                "CorpusId": 1736260
            },
            "abstract": "Aiming to augment generative models with external memory, we interpret the output of a memory module with stochastic addressing as a conditional mixture distribution, where a read operation corresponds to sampling a discrete memory address and retrieving the corresponding content from memory. This perspective allows us to apply variational inference to memory addressing, which enables effective training of the memory module by using the target information to guide memory lookups. Stochastic addressing is particularly well-suited for generative models as it naturally encourages multimodality which is a prominent aspect of most high-dimensional datasets. Treating the chosen address as a latent variable also allows us to quantify the amount of information gained with a memory lookup and measure the contribution of the memory module to the generative process. To illustrate the advantages of this approach we incorporate it into a variational autoencoder and apply the resulting model to the task of generative few-shot learning. The intuition behind this architecture is that the memory module can pick a relevant template from memory and the continuous part of the model can concentrate on modeling remaining variations. We demonstrate empirically that our model is able to identify and access the relevant memory contents even with hundreds of unseen Omniglot characters in memory",
            "referenceCount": 27,
            "citationCount": 60,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1709.07116"
            },
            "citationStyles": {
                "bibtex": "@Article{Bornschein2017VariationalMA,\n author = {J. Bornschein and A. Mnih and Daniel Zoran and Danilo Jimenez Rezende},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Variational Memory Addressing in Generative Models},\n volume = {abs/1709.07116},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d772a21f2f687985b41bec2dc47bc2760c57ed2f",
            "@type": "ScholarlyArticle",
            "paperId": "d772a21f2f687985b41bec2dc47bc2760c57ed2f",
            "corpusId": 2263947,
            "url": "https://www.semanticscholar.org/paper/d772a21f2f687985b41bec2dc47bc2760c57ed2f",
            "title": "Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/SutherlandTSDRS16",
                "MAG": "2950863313",
                "ArXiv": "1611.04488",
                "CorpusId": 2263947
            },
            "abstract": "We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.",
            "referenceCount": 38,
            "citationCount": 226,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.04488"
            },
            "citationStyles": {
                "bibtex": "@Article{Sutherland2016GenerativeMA,\n author = {Danica J. Sutherland and H. Tung and Heiko Strathmann and Soumyajit De and Aaditya Ramdas and Alex Smola and A. Gretton},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy},\n volume = {abs/1611.04488},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af68f10ab5078bfc519caae377c90ee6d9c504e9",
            "@type": "ScholarlyArticle",
            "paperId": "af68f10ab5078bfc519caae377c90ee6d9c504e9",
            "corpusId": 252734897,
            "url": "https://www.semanticscholar.org/paper/af68f10ab5078bfc519caae377c90ee6d9c504e9",
            "title": "Flow Matching for Generative Modeling",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2210-02747",
                "ArXiv": "2210.02747",
                "DOI": "10.48550/arXiv.2210.02747",
                "CorpusId": 252734897
            },
            "abstract": "We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.",
            "referenceCount": 57,
            "citationCount": 107,
            "influentialCitationCount": 29,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2210.02747",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-10-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2210.02747"
            },
            "citationStyles": {
                "bibtex": "@Article{Lipman2022FlowMF,\n author = {Y. Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matt Le},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Flow Matching for Generative Modeling},\n volume = {abs/2210.02747},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:eafc69e017378cfebc3c5a1238410361f615533a",
            "@type": "ScholarlyArticle",
            "paperId": "eafc69e017378cfebc3c5a1238410361f615533a",
            "corpusId": 125154984,
            "url": "https://www.semanticscholar.org/paper/eafc69e017378cfebc3c5a1238410361f615533a",
            "title": "Sinkhorn-AutoDiff: Tractable Wasserstein Learning of Generative Models",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2620743165",
                "CorpusId": 125154984
            },
            "abstract": "The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles both these issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.",
            "referenceCount": 8,
            "citationCount": 48,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-06-01",
            "journal": {
                "name": "arXiv: Machine Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Genevay2017SinkhornAutoDiffTW,\n author = {Aude Genevay and G. Peyr\u00e9 and Marco Cuturi},\n journal = {arXiv: Machine Learning},\n title = {Sinkhorn-AutoDiff: Tractable Wasserstein Learning of Generative Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:226f08bb8fd049fdca2f6a8a41ffe485a0ee6ebc",
            "@type": "ScholarlyArticle",
            "paperId": "226f08bb8fd049fdca2f6a8a41ffe485a0ee6ebc",
            "corpusId": 3461974,
            "url": "https://www.semanticscholar.org/paper/226f08bb8fd049fdca2f6a8a41ffe485a0ee6ebc",
            "title": "Generative Models of Visually Grounded Imagination",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.10762",
                "MAG": "2951901373",
                "DBLP": "journals/corr/VedantamFHM17",
                "CorpusId": 3461974
            },
            "abstract": "It is easy for people to imagine what a man with pink hair looks like, even if they have never seen such a person before. We call the ability to create images of novel semantic concepts visually grounded imagination. In this paper, we show how we can modify variational auto-encoders to perform this task. Our method uses a novel training objective, and a novel product-of-experts inference network, which can handle partially specified (abstract) concepts in a principled and efficient way. We also propose a set of easy-to-compute evaluation metrics that capture our intuitive notions of what it means to have good visual imagination, namely correctness, coverage, and compositionality (the 3 C's). Finally, we perform a detailed comparison of our method with two existing joint image-attribute VAE methods (the JMVAE method of Suzuki et.al. and the BiVCCA method of Wang et.al.) by applying them to two datasets: the MNIST-with-attributes dataset (which we introduce here), and the CelebA dataset.",
            "referenceCount": 64,
            "citationCount": 125,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.10762"
            },
            "citationStyles": {
                "bibtex": "@Article{Vedantam2017GenerativeMO,\n author = {Ramakrishna Vedantam and Ian S. Fischer and Jonathan Huang and K. Murphy},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generative Models of Visually Grounded Imagination},\n volume = {abs/1705.10762},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a7c7d9c381f296eb0ab32c4ddcdcc5b72a1e38c1",
            "@type": "ScholarlyArticle",
            "paperId": "a7c7d9c381f296eb0ab32c4ddcdcc5b72a1e38c1",
            "corpusId": 4530385,
            "url": "https://www.semanticscholar.org/paper/a7c7d9c381f296eb0ab32c4ddcdcc5b72a1e38c1",
            "title": "Domain Randomization and Generative Models for Robotic Grasping",
            "venue": "IEEE/RJS International Conference on Intelligent RObots and Systems",
            "publicationVenue": {
                "id": "urn:research:37275deb-3fcf-4d16-ae77-95db9899b1f3",
                "name": "IEEE/RJS International Conference on Intelligent RObots and Systems",
                "alternate_names": [
                    "IROS",
                    "Intelligent Robots and Systems",
                    "Intell Robot Syst",
                    "IEEE/RJS Int Conf Intell Robot Syst"
                ],
                "issn": null,
                "url": "http://www.iros.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2950961737",
                "ArXiv": "1710.06425",
                "DBLP": "conf/iros/TobinBDAHKMRSWZ18",
                "DOI": "10.1109/IROS.2018.8593933",
                "CorpusId": 4530385
            },
            "abstract": "Deep learning-based robotic grasping has made significant progress thanks to algorithmic improvements and increased data availability. However, state-of-the-art models are often trained on as few as hundreds or thousands of unique object instances, and as a result generalization can be a challenge. In this work, we explore a novel data generation pipeline for training a deep neural network to perform grasp planning that applies the idea of domain randomization to object synthesis. We generate millions of unique, unrealistic procedurally generated objects, and train a deep neural network to perform grasp planning on these objects. Since the distribution of successful grasps for a given object can be highly multimodal, we propose an autoregressive grasp planning model that maps sensor inputs of a scene to a probability distribution over possible grasps. This model allows us to sample grasps efficiently at test time (or avoid sampling entirely). We evaluate our model architecture and data generation pipeline in simulation and the real world. We find we can achieve a >90% success rate on previously unseen realistic objects at test time in simulation despite having only been trained on random objects. We also demonstrate an 80% success rate on real-world grasp attempts despite having only been trained on random simulated objects.",
            "referenceCount": 57,
            "citationCount": 149,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1710.06425",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-17",
            "journal": {
                "name": "2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tobin2017DomainRA,\n author = {Joshua Tobin and Wojciech Zaremba and P. Abbeel},\n booktitle = {IEEE/RJS International Conference on Intelligent RObots and Systems},\n journal = {2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},\n pages = {3482-3489},\n title = {Domain Randomization and Generative Models for Robotic Grasping},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:92e2f30ab8c0010b7ba8c296d6964a3088c700f4",
            "@type": "ScholarlyArticle",
            "paperId": "92e2f30ab8c0010b7ba8c296d6964a3088c700f4",
            "corpusId": 8070055,
            "url": "https://www.semanticscholar.org/paper/92e2f30ab8c0010b7ba8c296d6964a3088c700f4",
            "title": "Deep Directed Generative Models with Energy-Based Probability Estimation",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/KimB16",
                "MAG": "2416272112",
                "ArXiv": "1606.03439",
                "CorpusId": 8070055
            },
            "abstract": "Training energy-based probabilistic models is confronted with apparently intractable sums, whose Monte Carlo estimation requires sampling from the estimated probability distribution in the inner loop of training. This can be approximately achieved by Markov chain Monte Carlo methods, but may still face a formidable obstacle that is the difficulty of mixing between modes with sharp concentrations of probability. Whereas an MCMC process is usually derived from a given energy function based on mathematical considerations and requires an arbitrarily long time to obtain good and varied samples, we propose to train a deep directed generative model (not a Markov chain) so that its sampling distribution approximately matches the energy function that is being trained. Inspired by generative adversarial networks, the proposed framework involves training of two models that represent dual views of the estimated probability distribution: the energy function (mapping an input configuration to a scalar energy value) and the generator (mapping a noise vector to a generated configuration), both represented by deep neural networks.",
            "referenceCount": 18,
            "citationCount": 129,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-02-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.03439"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2016DeepDG,\n author = {Taesup Kim and Yoshua Bengio},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Directed Generative Models with Energy-Based Probability Estimation},\n volume = {abs/1606.03439},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9949dab0b0c035740ba34cee179fa1b58052dbb2",
            "@type": "ScholarlyArticle",
            "paperId": "9949dab0b0c035740ba34cee179fa1b58052dbb2",
            "corpusId": 16306616,
            "url": "https://www.semanticscholar.org/paper/9949dab0b0c035740ba34cee179fa1b58052dbb2",
            "title": "Accelerated Generative Models for 3D Point Cloud Data",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/cvpr/EckartKTKK16",
                "MAG": "2429678572",
                "DOI": "10.1109/CVPR.2016.593",
                "CorpusId": 16306616
            },
            "abstract": "Finding meaningful, structured representations of 3D point cloud data (PCD) has become a core task for spatial perception applications. In this paper we introduce a method for constructing compact generative representations of PCD at multiple levels of detail. As opposed to deterministic structures such as voxel grids or octrees, we propose probabilistic subdivisions of the data through local mixture modeling, and show how these subdivisions can provide a maximum likelihood segmentation of the data. The final representation is hierarchical, compact, parametric, and statistically derived, facilitating run-time occupancy calculations through stochastic sampling. Unlike traditional deterministic spatial subdivision methods, our technique enables dynamic creation of voxel grids according the application's best needs. In contrast to other generative models for PCD, we explicitly enforce sparsity among points and mixtures, a technique which we call expectation sparsification. This leads to a highly parallel hierarchical Expectation Maximization (EM) algorithm well-suited for the GPU and real-time execution. We explore the trade-offs between model fidelity and model size at various levels of detail, our tests showing favorable performance when compared to octree and NDT-based methods.",
            "referenceCount": 20,
            "citationCount": 57,
            "influentialCitationCount": 9,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-01",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Eckart2016AcceleratedGM,\n author = {B. Eckart and Kihwan Kim and Alejandro J. Troccoli and A. Kelly and J. Kautz},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5497-5505},\n title = {Accelerated Generative Models for 3D Point Cloud Data},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:da7bb24431b37c8fe08c86c57e10f534a7ec0386",
            "@type": "ScholarlyArticle",
            "paperId": "da7bb24431b37c8fe08c86c57e10f534a7ec0386",
            "corpusId": 7088133,
            "url": "https://www.semanticscholar.org/paper/da7bb24431b37c8fe08c86c57e10f534a7ec0386",
            "title": "Discriminative Regularization for Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1602.03220",
                "MAG": "2269752429",
                "DBLP": "journals/corr/LambDC16",
                "CorpusId": 7088133
            },
            "abstract": "We explore the question of whether the representations learned by classifiers can be used to enhance the quality of generative models. Our conjecture is that labels correspond to characteristics of natural data which are most salient to humans: identity in faces, objects in images, and utterances in speech. We propose to take advantage of this by using the representations from discriminative classifiers to augment the objective function corresponding to a generative model. In particular we enhance the objective function of the variational autoencoder, a popular generative model, with a discriminative regularization term. We show that enhancing the objective function in this way leads to samples that are clearer and have higher visual quality than the samples from the standard variational autoencoders.",
            "referenceCount": 24,
            "citationCount": 62,
            "influentialCitationCount": 5,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-02-09",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1602.03220"
            },
            "citationStyles": {
                "bibtex": "@Article{Lamb2016DiscriminativeRF,\n author = {Alex Lamb and Vincent Dumoulin and Aaron C. Courville},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Discriminative Regularization for Generative Models},\n volume = {abs/1602.03220},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:390d4fb96c4ed396acd9dcf7f1d92eb08624d2f1",
            "@type": "ScholarlyArticle",
            "paperId": "390d4fb96c4ed396acd9dcf7f1d92eb08624d2f1",
            "corpusId": 11922095,
            "url": "https://www.semanticscholar.org/paper/390d4fb96c4ed396acd9dcf7f1d92eb08624d2f1",
            "title": "Semi-Supervised Generation with Cluster-aware Generative Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1704.00637",
                "MAG": "2608412030",
                "DBLP": "journals/corr/MaaloeFW17",
                "CorpusId": 11922095
            },
            "abstract": "Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.",
            "referenceCount": 33,
            "citationCount": 28,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-04-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1704.00637"
            },
            "citationStyles": {
                "bibtex": "@Article{Maal\u00f8e2017SemiSupervisedGW,\n author = {Lars Maal\u00f8e and Marco Fraccaro and O. Winther},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Semi-Supervised Generation with Cluster-aware Generative Models},\n volume = {abs/1704.00637},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c2a08137657ac2312ae70991e74aa73ce93051ef",
            "@type": "ScholarlyArticle",
            "paperId": "c2a08137657ac2312ae70991e74aa73ce93051ef",
            "corpusId": 13460250,
            "url": "https://www.semanticscholar.org/paper/c2a08137657ac2312ae70991e74aa73ce93051ef",
            "title": "An Architecture for Deep, Hierarchical Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950501883",
                "DBLP": "journals/corr/Bachman16",
                "ArXiv": "1612.04739",
                "CorpusId": 13460250
            },
            "abstract": "We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.",
            "referenceCount": 25,
            "citationCount": 52,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.04739"
            },
            "citationStyles": {
                "bibtex": "@Article{Bachman2016AnAF,\n author = {Philip Bachman},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {An Architecture for Deep, Hierarchical Generative Models},\n volume = {abs/1612.04739},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1aba2eeb13751b35ee85143a91ec3ea7a2bb26b9",
            "@type": "ScholarlyArticle",
            "paperId": "1aba2eeb13751b35ee85143a91ec3ea7a2bb26b9",
            "corpusId": 13940256,
            "url": "https://www.semanticscholar.org/paper/1aba2eeb13751b35ee85143a91ec3ea7a2bb26b9",
            "title": "Boosted Generative Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2962691366",
                "DBLP": "conf/aaai/GroverE18",
                "ArXiv": "1702.08484",
                "DOI": "10.1609/aaai.v32i1.11827",
                "CorpusId": 13940256
            },
            "abstract": "\n \n We propose a novel approach for using unsupervised boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our meta-algorithmic framework can leverage any existing base learner that permits likelihood evaluation, including recent deep expressive models. Further, our approach allows the ensemble to include discriminative models trained to distinguish real data from model-generated data. We show theoretical conditions under which incorporating a new model in the ensemble will improve the fit and empirically demonstrate the effectiveness of our black-box boosting algorithms on density estimation, classification, and sample generation on benchmark datasets for a wide range of generative models.\n \n",
            "referenceCount": 70,
            "citationCount": 44,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11827/11686",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.08484"
            },
            "citationStyles": {
                "bibtex": "@Article{Grover2016BoostedGM,\n author = {Aditya Grover and Stefano Ermon},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Boosted Generative Models},\n volume = {abs/1702.08484},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bead8973ff87831f7ee41c5adda777f6021cdcea",
            "@type": "ScholarlyArticle",
            "paperId": "bead8973ff87831f7ee41c5adda777f6021cdcea",
            "corpusId": 3331356,
            "url": "https://www.semanticscholar.org/paper/bead8973ff87831f7ee41c5adda777f6021cdcea",
            "title": "IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.10513",
                "DBLP": "journals/corr/WangYZGXWZZ17",
                "MAG": "2619206542",
                "DOI": "10.1145/3077136.3080786",
                "CorpusId": 3331356
            },
            "abstract": "This paper provides a unified account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a query-document pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards fitting the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an attacker to the current discriminative model, generates difficult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the unified framework takes advantage of both schools of thinking: (i) the generative model learns to fit the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a better estimation for document ranking. Our experimental results have demonstrated significant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.",
            "referenceCount": 55,
            "citationCount": 532,
            "influentialCitationCount": 87,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://eprints.bbk.ac.uk/id/eprint/18782/6/18782.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-30",
            "journal": {
                "name": "Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Wang2017IRGANAM,\n author = {Jun Wang and Lantao Yu and Weinan Zhang and Yu Gong and Yinghui Xu and Benyou Wang and P. Zhang and Dell Zhang},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n journal = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},\n title = {IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:401dfc5efbd182eaef6945249aea8a0a1e3cced5",
            "@type": "ScholarlyArticle",
            "paperId": "401dfc5efbd182eaef6945249aea8a0a1e3cced5",
            "corpusId": 195316841,
            "url": "https://www.semanticscholar.org/paper/401dfc5efbd182eaef6945249aea8a0a1e3cced5",
            "title": "Shaping Belief States with Generative Environment Models for RL",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2970364795",
                "ArXiv": "1906.09237",
                "DBLP": "journals/corr/abs-1906-09237",
                "CorpusId": 195316841
            },
            "abstract": "When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.",
            "referenceCount": 69,
            "citationCount": 100,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-21",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.09237"
            },
            "citationStyles": {
                "bibtex": "@Article{Gregor2019ShapingBS,\n author = {Karol Gregor and Danilo Jimenez Rezende and F. Besse and Yan Wu and Hamza Merzic and A\u00e4ron van den Oord},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Shaping Belief States with Generative Environment Models for RL},\n volume = {abs/1906.09237},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5f4bee489f595bd3d3dda7fd88de8d79b006aa52",
            "@type": "ScholarlyArticle",
            "paperId": "5f4bee489f595bd3d3dda7fd88de8d79b006aa52",
            "corpusId": 49742003,
            "url": "https://www.semanticscholar.org/paper/5f4bee489f595bd3d3dda7fd88de8d79b006aa52",
            "title": "Avoiding Latent Variable Collapse With Generative Skip Models",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2963987720",
                "DBLP": "journals/corr/abs-1807-04863",
                "ArXiv": "1807.04863",
                "CorpusId": 49742003
            },
            "abstract": "Variational autoencoders learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as \"latent variable collapse,\" especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior \"collapses\" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.",
            "referenceCount": 36,
            "citationCount": 155,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.04863"
            },
            "citationStyles": {
                "bibtex": "@Article{Dieng2018AvoidingLV,\n author = {Adji B. Dieng and Yoon Kim and Alexander M. Rush and D. Blei},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n journal = {ArXiv},\n title = {Avoiding Latent Variable Collapse With Generative Skip Models},\n volume = {abs/1807.04863},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6",
            "@type": "ScholarlyArticle",
            "paperId": "ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6",
            "corpusId": 235390993,
            "url": "https://www.semanticscholar.org/paper/ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6",
            "title": "Score-based Generative Modeling in Latent Space",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2106.05931",
                "DBLP": "conf/nips/VahdatKK21",
                "CorpusId": 235390993
            },
            "abstract": "Score-based generative models (SGMs) have recently demonstrated impressive results in terms of both sample quality and distribution coverage. However, they are usually applied directly in data space and often require thousands of network evaluations for sampling. Here, we propose the Latent Score-based Generative Model (LSGM), a novel approach that trains SGMs in a latent space, relying on the variational autoencoder framework. Moving from data to latent space allows us to train more expressive generative models, apply SGMs to non-continuous data, and learn smoother SGMs in a smaller space, resulting in fewer network evaluations and faster sampling. To enable training LSGMs end-to-end in a scalable and stable manner, we (i) introduce a new score-matching objective suitable to the LSGM setting, (ii) propose a novel parameterization of the score function that allows SGM to focus on the mismatch of the target distribution with respect to a simple Normal one, and (iii) analytically derive multiple techniques for variance reduction of the training objective. LSGM obtains a state-of-the-art FID score of 2.10 on CIFAR-10, outperforming all existing generative results on this dataset. On CelebA-HQ-256, LSGM is on a par with previous SGMs in sample quality while outperforming them in sampling time by two orders of magnitude. In modeling binary images, LSGM achieves state-of-the-art likelihood on the binarized OMNIGLOT dataset. Our project page and code can be found at https://nvlabs.github.io/LSGM .",
            "referenceCount": 110,
            "citationCount": 357,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-06-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vahdat2021ScorebasedGM,\n author = {Arash Vahdat and Karsten Kreis and J. Kautz},\n booktitle = {Neural Information Processing Systems},\n pages = {11287-11302},\n title = {Score-based Generative Modeling in Latent Space},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b321e8eca4dbf424a9e30dd938fb423786c90b76",
            "@type": "ScholarlyArticle",
            "paperId": "b321e8eca4dbf424a9e30dd938fb423786c90b76",
            "corpusId": 85531885,
            "url": "https://www.semanticscholar.org/paper/b321e8eca4dbf424a9e30dd938fb423786c90b76",
            "title": "On distinguishability criteria for estimating generative models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2014,
            "externalIds": {
                "ArXiv": "1412.6515",
                "MAG": "2963873420",
                "DBLP": "journals/corr/Goodfellow14",
                "CorpusId": 85531885
            },
            "abstract": "Two recently introduced criteria for estimation of generative models are both based on a reduction to binary classification. Noise-contrastive estimation (NCE) is an estimation procedure in which a generative model is trained to be able to distinguish data samples from noise samples. Generative adversarial networks (GANs) are pairs of generator and discriminator networks, with the generator network learning to generate samples by attempting to fool the discriminator network into believing its samples are real data. Both estimation procedures use the same function to drive learning, which naturally raises questions about how they are related to each other, as well as whether this function is related to maximum likelihood estimation (MLE). NCE corresponds to training an internal data model belonging to the {\\em discriminator} network but using a fixed generator network. We show that a variant of NCE, with a dynamic generator network, is equivalent to maximum likelihood estimation. Since pairing a learned discriminator with an appropriate dynamically selected generator recovers MLE, one might expect the reverse to hold for pairing a learned generator with a certain discriminator. However, we show that recovering MLE for a learned generator requires departing from the distinguishability game. Specifically: \n(i) The expected gradient of the NCE discriminator can be made to match the expected gradient of \nMLE, if one is allowed to use a non-stationary noise distribution for NCE, \n(ii) No choice of discriminator network can make the expected gradient for the GAN generator match that of MLE, and \n(iii) The existing theory does not guarantee that GANs will converge in the non-convex case. \nThis suggests that the key next step in GAN research is to determine whether GANs converge, and if not, to modify their training algorithm to force convergence.",
            "referenceCount": 2,
            "citationCount": 124,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-12-01",
            "journal": {
                "name": "arXiv: Machine Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2014OnDC,\n author = {I. Goodfellow},\n booktitle = {International Conference on Learning Representations},\n journal = {arXiv: Machine Learning},\n title = {On distinguishability criteria for estimating generative models},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15d739e2c184a6844bdbd9a2550d007de6ddb085",
            "@type": "ScholarlyArticle",
            "paperId": "15d739e2c184a6844bdbd9a2550d007de6ddb085",
            "corpusId": 35911567,
            "url": "https://www.semanticscholar.org/paper/15d739e2c184a6844bdbd9a2550d007de6ddb085",
            "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/GuimaraesSFA17",
                "ArXiv": "1705.10843",
                "MAG": "2618625858",
                "CorpusId": 35911567
            },
            "abstract": "In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.",
            "referenceCount": 50,
            "citationCount": 446,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.10843"
            },
            "citationStyles": {
                "bibtex": "@Article{Guimaraes2017ObjectiveReinforcedGA,\n author = {G. L. Guimaraes and Benjam\u00edn S\u00e1nchez-Lengeling and Pedro Luis Cunha Farias and Al\u00e1n Aspuru-Guzik},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models},\n volume = {abs/1705.10843},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f",
            "@type": "ScholarlyArticle",
            "paperId": "71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f",
            "corpusId": 252090040,
            "url": "https://www.semanticscholar.org/paper/71cc838d8a50a0d62cc9c679536f1f25b2ea6b7f",
            "title": "A Survey on Generative Diffusion Model",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2209-02646",
                "DOI": "10.48550/arXiv.2209.02646",
                "CorpusId": 252090040
            },
            "abstract": "\u2014Deep learning shows excellent potential in generation tasks thanks to deep latent representation. Generative models are classes of models that can generate observations randomly with respect to certain implied parameters. Recently, the diffusion Model has become a raising class of generative models by virtue of its power-generating ability. Nowadays, great achievements have been reached. More applications except for computer vision, speech generation, bioinformatics, and natural language processing are to be explored in this \ufb01eld. However, the diffusion model has its genuine drawback of a slow generation process, leading to many enhanced works. This survey makes a summary of the \ufb01eld of the diffusion model. We \ufb01rst state the main problem with two landmark works \u2013 DDPM and DSM. Then, we present a diverse range of advanced techniques to speed up the diffusion models \u2013 training schedule, training-free sampling, mixed-modeling, and score & diffusion uni\ufb01cation. Regarding existing models, we also provide a benchmark of FID score, IS, and NLL according to speci\ufb01c NFE. Moreover, applications with diffusion models are introduced including computer vision, sequence modeling, audio, and AI for science. Finally, there is a summarization of this \ufb01eld together with limitations & further directions.",
            "referenceCount": 357,
            "citationCount": 91,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2209.02646",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2209.02646"
            },
            "citationStyles": {
                "bibtex": "@Article{Cao2022ASO,\n author = {Hanqun Cao and Cheng Tan and Zhangyang Gao and Yilun Xu and Guangyong Chen and P. Heng and Stan Z. Li},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Survey on Generative Diffusion Model},\n volume = {abs/2209.02646},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:215999a0e155a21255e4655d4eac312858058a84",
            "@type": "ScholarlyArticle",
            "paperId": "215999a0e155a21255e4655d4eac312858058a84",
            "corpusId": 5737841,
            "url": "https://www.semanticscholar.org/paper/215999a0e155a21255e4655d4eac312858058a84",
            "title": "Structured Generative Models of Natural Source Code",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/icml/MaddisonT14",
                "MAG": "2962725091",
                "ArXiv": "1401.0514",
                "CorpusId": 5737841
            },
            "abstract": "We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have three key properties: First, they incorporate both sequential and hierarchical structure. Second, we learn a distributed representation of source code elements. Finally, they integrate closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the models, measured by the probability of generating test programs.",
            "referenceCount": 31,
            "citationCount": 160,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-01-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1401.0514"
            },
            "citationStyles": {
                "bibtex": "@Article{Maddison2014StructuredGM,\n author = {Chris J. Maddison and Daniel Tarlow},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Structured Generative Models of Natural Source Code},\n volume = {abs/1401.0514},\n year = {2014}\n}\n"
            }
        }
    }
]