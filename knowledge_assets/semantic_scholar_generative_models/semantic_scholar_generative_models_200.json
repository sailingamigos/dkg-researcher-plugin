[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7e7ecb6d72cc6584082d412355641121bc91db34",
            "@type": "ScholarlyArticle",
            "paperId": "7e7ecb6d72cc6584082d412355641121bc91db34",
            "corpusId": 10082291,
            "url": "https://www.semanticscholar.org/paper/7e7ecb6d72cc6584082d412355641121bc91db34",
            "title": "Fast Adaptation in Generative Models with Generative Matching Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2964135650",
                "DBLP": "conf/iclr/BartunovV17",
                "ArXiv": "1612.02192",
                "CorpusId": 10082291
            },
            "abstract": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. We develop a new generative model called Generative Matching Network which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks. By conditioning on the additional input dataset, our model can instantly learn new concepts that were not available in the training data but conform to a similar generative process. The proposed framework does not explicitly restrict diversity of the conditioning data and also does not require an extensive inference procedure for training or adaptation. Our experiments on the Omniglot dataset demonstrate that Generative Matching Networks significantly improve predictive performance on the fly as more additional data is available and outperform existing state of the art conditional generative models.",
            "referenceCount": 26,
            "citationCount": 23,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.02192"
            },
            "citationStyles": {
                "bibtex": "@Article{Bartunov2016FastAI,\n author = {Sergey Bartunov and D. Vetrov},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Fast Adaptation in Generative Models with Generative Matching Networks},\n volume = {abs/1612.02192},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:215999a0e155a21255e4655d4eac312858058a84",
            "@type": "ScholarlyArticle",
            "paperId": "215999a0e155a21255e4655d4eac312858058a84",
            "corpusId": 5737841,
            "url": "https://www.semanticscholar.org/paper/215999a0e155a21255e4655d4eac312858058a84",
            "title": "Structured Generative Models of Natural Source Code",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/icml/MaddisonT14",
                "MAG": "2962725091",
                "ArXiv": "1401.0514",
                "CorpusId": 5737841
            },
            "abstract": "We study the problem of building generative models of natural source code (NSC); that is, source code written and understood by humans. Our primary contribution is to describe a family of generative models for NSC that have three key properties: First, they incorporate both sequential and hierarchical structure. Second, we learn a distributed representation of source code elements. Finally, they integrate closely with a compiler, which allows leveraging compiler logic and abstractions when building structure into the model. We also develop an extension that includes more complex structure, refining how the model generates identifier tokens based on what variables are currently in scope. Our models can be learned efficiently, and we show empirically that including appropriate structure greatly improves the models, measured by the probability of generating test programs.",
            "referenceCount": 31,
            "citationCount": 160,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-01-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1401.0514"
            },
            "citationStyles": {
                "bibtex": "@Article{Maddison2014StructuredGM,\n author = {Chris J. Maddison and Daniel Tarlow},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Structured Generative Models of Natural Source Code},\n volume = {abs/1401.0514},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f19dfc360088922cf1d423c538662aae8d542c28",
            "@type": "ScholarlyArticle",
            "paperId": "f19dfc360088922cf1d423c538662aae8d542c28",
            "corpusId": 254044710,
            "url": "https://www.semanticscholar.org/paper/f19dfc360088922cf1d423c538662aae8d542c28",
            "title": "Is Conditional Generative Modeling all you need for Decision-Making?",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/iclr/AjayDGTJA23",
                "ArXiv": "2211.15657",
                "DOI": "10.48550/arXiv.2211.15657",
                "CorpusId": 254044710
            },
            "abstract": "Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.",
            "referenceCount": 71,
            "citationCount": 93,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.15657",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-11-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2211.15657"
            },
            "citationStyles": {
                "bibtex": "@Article{Ajay2022IsCG,\n author = {Anurag Ajay and Yilun Du and Abhi Gupta and J. Tenenbaum and T. Jaakkola and Pulkit Agrawal},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Is Conditional Generative Modeling all you need for Decision-Making?},\n volume = {abs/2211.15657},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d651f7b09f554482bc7a5ffa1381dda1dc73050c",
            "@type": "ScholarlyArticle",
            "paperId": "d651f7b09f554482bc7a5ffa1381dda1dc73050c",
            "corpusId": 244906742,
            "url": "https://www.semanticscholar.org/paper/d651f7b09f554482bc7a5ffa1381dda1dc73050c",
            "title": "A 3D Generative Model for Structure-Based Drug Design",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2203.10446",
                "DBLP": "conf/nips/LuoGMP21",
                "CorpusId": 244906742
            },
            "abstract": "We study a fundamental problem in structure-based drug design -- generating molecules that bind to specific protein binding sites. While we have witnessed the great success of deep generative models in drug design, the existing methods are mostly string-based or graph-based. They are limited by the lack of spatial information and thus unable to be applied to structure-based design tasks. Particularly, such models have no or little knowledge of how molecules interact with their target proteins exactly in 3D space. In this paper, we propose a 3D generative model that generates molecules given a designated 3D protein binding site. Specifically, given a binding site as the 3D context, our model estimates the probability density of atom's occurrences in 3D space -- positions that are more likely to have atoms will be assigned higher probability. To generate 3D molecules, we propose an auto-regressive sampling scheme -- atoms are sampled sequentially from the learned distribution until there is no room for new atoms. Combined with this sampling scheme, our model can generate valid and diverse molecules, which could be applicable to various structure-based molecular design tasks such as molecule sampling and linker design. Experimental results demonstrate that molecules sampled from our model exhibit high binding affinity to specific targets and good drug properties such as drug-likeness even if the model is not explicitly optimized for them.",
            "referenceCount": 38,
            "citationCount": 80,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-03-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Luo2022A3G,\n author = {Shitong Luo},\n booktitle = {Neural Information Processing Systems},\n pages = {6229-6239},\n title = {A 3D Generative Model for Structure-Based Drug Design},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8",
            "@type": "ScholarlyArticle",
            "paperId": "7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8",
            "corpusId": 17777862,
            "url": "https://www.semanticscholar.org/paper/7df22e88a86d7e7e914a9cf3ad5b8fbd62b35cb8",
            "title": "Hierarchical Neural Network Generative Models for Movie Dialogues",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2296712013",
                "DBLP": "journals/corr/SerbanSBCP15",
                "CorpusId": 17777862
            },
            "abstract": "We consider the task of generative dialogue modeling for movie scripts. To this end, we extend the recently proposed hierarchical recurrent encoder decoder neural network and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We show that its performance can be improved considerably by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings.",
            "referenceCount": 46,
            "citationCount": 185,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-07-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1507.04808"
            },
            "citationStyles": {
                "bibtex": "@Article{Serban2015HierarchicalNN,\n author = {Iulian Serban and Alessandro Sordoni and Yoshua Bengio and Aaron C. Courville and Joelle Pineau},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Hierarchical Neural Network Generative Models for Movie Dialogues},\n volume = {abs/1507.04808},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2af058e2377f3641403435f0c9930d955fedfc8f",
            "@type": "ScholarlyArticle",
            "paperId": "2af058e2377f3641403435f0c9930d955fedfc8f",
            "corpusId": 3173270,
            "url": "https://www.semanticscholar.org/paper/2af058e2377f3641403435f0c9930d955fedfc8f",
            "title": "A Test of Relative Similarity For Model Selection in Generative Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2952086028",
                "DBLP": "journals/corr/BounliphoneBBAG15",
                "ArXiv": "1511.04581",
                "CorpusId": 3173270
            },
            "abstract": "Probabilistic generative models provide a powerful framework for representing data that avoids the expense of manual annotation typically needed by discriminative approaches. Model selection in this generative setting can be challenging, however, particularly when likelihoods are not easily accessible. To address this issue, we introduce a statistical test of relative similarity, which is used to determine which of two models generates samples that are significantly closer to a real-world reference dataset of interest. We use as our test statistic the difference in maximum mean discrepancies (MMDs) between the reference dataset and each model dataset, and derive a powerful, low-variance test based on the joint asymptotic distribution of the MMDs between each reference-model pair. In experiments on deep generative models, including the variational auto-encoder and generative moment matching network, the tests provide a meaningful ranking of model performance as a function of parameter and training settings.",
            "referenceCount": 33,
            "citationCount": 71,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-14",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.04581"
            },
            "citationStyles": {
                "bibtex": "@Article{Bounliphone2015ATO,\n author = {Wacha Bounliphone and Eugene Belilovsky and Matthew B. Blaschko and Ioannis Antonoglou and A. Gretton},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {A Test of Relative Similarity For Model Selection in Generative Models},\n volume = {abs/1511.04581},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:355f98e4827a1b6ad3f29d07ea2bcf9ad078295c",
            "@type": "ScholarlyArticle",
            "paperId": "355f98e4827a1b6ad3f29d07ea2bcf9ad078295c",
            "corpusId": 17572062,
            "url": "https://www.semanticscholar.org/paper/355f98e4827a1b6ad3f29d07ea2bcf9ad078295c",
            "title": "Video (language) modeling: a baseline for generative models of natural videos",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/RanzatoSBMCC14",
                "MAG": "1568514080",
                "ArXiv": "1412.6604",
                "CorpusId": 17572062
            },
            "abstract": "We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.",
            "referenceCount": 33,
            "citationCount": 444,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-12-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1412.6604"
            },
            "citationStyles": {
                "bibtex": "@Article{Ranzato2014VideoM,\n author = {Marc'Aurelio Ranzato and Arthur Szlam and Joan Bruna and Micha\u00ebl Mathieu and Ronan Collobert and S. Chopra},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Video (language) modeling: a baseline for generative models of natural videos},\n volume = {abs/1412.6604},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fa60521dabd2b64137392b4885e4d989f4b86430",
            "@type": "ScholarlyArticle",
            "paperId": "fa60521dabd2b64137392b4885e4d989f4b86430",
            "corpusId": 51909334,
            "url": "https://www.semanticscholar.org/paper/fa60521dabd2b64137392b4885e4d989f4b86430",
            "title": "Physics-Based Generative Adversarial Models for Image Restoration and Beyond",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1808-00605",
                "ArXiv": "1808.00605",
                "MAG": "2885442750",
                "DOI": "10.1109/TPAMI.2020.2969348",
                "CorpusId": 51909334,
                "PubMed": "31995475"
            },
            "abstract": "We present an algorithm to directly solve numerous image restoration problems (e.g., image deblurring, image dehazing, and image deraining). These problems are ill-posed, and the common assumptions for existing methods are usually based on heuristic image priors. In this paper, we show that these problems can be solved by generative models with adversarial learning. However, a straightforward formulation based on a straightforward generative adversarial network (GAN) does not perform well in these tasks, and some structures of the estimated images are usually not preserved well. Motivated by an interesting observation that the estimated results should be consistent with the observed inputs under the physics models, we propose an algorithm that guides the estimation process of a specific task within the GAN framework. The proposed model is trained in an end-to-end fashion and can be applied to a variety of image restoration and low-level vision problems. Extensive experiments demonstrate that the proposed method performs favorably against state-of-the-art algorithms.",
            "referenceCount": 68,
            "citationCount": 106,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-08-02",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "43"
            },
            "citationStyles": {
                "bibtex": "@Article{Pan2018PhysicsBasedGA,\n author = {Jin-shan Pan and Yang Liu and Jiangxin Dong and Jiawei Zhang and Jimmy S. J. Ren and Jinhui Tang and Yu-Wing Tai and Ming-Hsuan Yang},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {2449-2462},\n title = {Physics-Based Generative Adversarial Models for Image Restoration and Beyond},\n volume = {43},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fcdad6690b9cbfc3b131d108eed0d7746ed0bb1",
            "@type": "ScholarlyArticle",
            "paperId": "1fcdad6690b9cbfc3b131d108eed0d7746ed0bb1",
            "corpusId": 2653122,
            "url": "https://www.semanticscholar.org/paper/1fcdad6690b9cbfc3b131d108eed0d7746ed0bb1",
            "title": "Generative models of the human connectome",
            "venue": "NeuroImage",
            "publicationVenue": {
                "id": "urn:research:fd4c7628-c16e-4b50-8555-3ac3ad6da2d7",
                "name": "NeuroImage",
                "alternate_names": null,
                "issn": "1053-8119",
                "url": "http://www.elsevier.com/locate/ynimg"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1506.06795",
                "MAG": "2595990221",
                "PubMedCentral": "4655950",
                "DBLP": "journals/neuroimage/BetzelAGHRGVMTH16",
                "DOI": "10.1016/j.neuroimage.2015.09.041",
                "CorpusId": 2653122,
                "PubMed": "26427642"
            },
            "abstract": null,
            "referenceCount": 77,
            "citationCount": 237,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology",
                "Biology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-06-22",
            "journal": {
                "name": "Neuroimage",
                "volume": "124"
            },
            "citationStyles": {
                "bibtex": "@Article{Betzel2015GenerativeMO,\n author = {Richard F. Betzel and Andrea Avena-Koenigsberger and J. Go\u00f1i and Ye He and M. A. Reus and A. Griffa and P. V\u00e9rtes and B. Mi\u0161i\u0107 and J. Thiran and P. Hagmann and M. P. Heuvel and X. Zuo and E. Bullmore and O. Sporns},\n booktitle = {NeuroImage},\n journal = {Neuroimage},\n pages = {1054 - 1064},\n title = {Generative models of the human connectome},\n volume = {124},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:64ea8f180d0682e6c18d1eb688afdb2027c02794",
            "@type": "ScholarlyArticle",
            "paperId": "64ea8f180d0682e6c18d1eb688afdb2027c02794",
            "corpusId": 234357997,
            "url": "https://www.semanticscholar.org/paper/64ea8f180d0682e6c18d1eb688afdb2027c02794",
            "title": "Diffusion Models Beat GANs on Image Synthesis",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2105.05233",
                "DBLP": "journals/corr/abs-2105-05233",
                "CorpusId": 234357997
            },
            "abstract": "We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion",
            "referenceCount": 81,
            "citationCount": 2644,
            "influentialCitationCount": 427,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-05-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2105.05233"
            },
            "citationStyles": {
                "bibtex": "@Article{Dhariwal2021DiffusionMB,\n author = {Prafulla Dhariwal and Alex Nichol},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Diffusion Models Beat GANs on Image Synthesis},\n volume = {abs/2105.05233},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:965359b3008ab50dd04e171551220ec0e7f83aba",
            "@type": "ScholarlyArticle",
            "paperId": "965359b3008ab50dd04e171551220ec0e7f83aba",
            "corpusId": 196470871,
            "url": "https://www.semanticscholar.org/paper/965359b3008ab50dd04e171551220ec0e7f83aba",
            "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2959300817",
                "DBLP": "conf/nips/SongE19",
                "ArXiv": "1907.05600",
                "CorpusId": 196470871
            },
            "abstract": "We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.",
            "referenceCount": 68,
            "citationCount": 1701,
            "influentialCitationCount": 235,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Song2019GenerativeMB,\n author = {Yang Song and Stefano Ermon},\n booktitle = {Neural Information Processing Systems},\n pages = {11895-11907},\n title = {Generative Modeling by Estimating Gradients of the Data Distribution},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e32cf24268f079a76180f19edac267a539ea5d53",
            "@type": "ScholarlyArticle",
            "paperId": "e32cf24268f079a76180f19edac267a539ea5d53",
            "corpusId": 5233282,
            "url": "https://www.semanticscholar.org/paper/e32cf24268f079a76180f19edac267a539ea5d53",
            "title": "Bidirectional Recurrent Neural Networks as Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/nips/BerglundRHKVK15",
                "MAG": "2184587602",
                "ArXiv": "1504.01575",
                "CorpusId": 5233282
            },
            "abstract": "Bidirectional recurrent neural networks (RNN) are trained to predict both in the positive and negative time directions simultaneously. They have not been used commonly in unsupervised tasks, because a probabilistic interpretation of the model has been difficult. Recently, two different frameworks, GSN and NADE, provide a connection between reconstruction and probabilistic modeling, which makes the interpretation possible. As far as we know, neither GSN or NADE have been studied in the context of time series before. As an example of an unsupervised task, we study the problem of filling in gaps in high-dimensional time series with complex dynamics. Although unidirectional RNNs have recently been trained successfully to model such time series, inference in the negative time direction is non-trivial. We propose two probabilistic interpretations of bidirectional RNNs that can be used to reconstruct missing gaps efficiently. Our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions, although a bit less accurate than a computationally complex bidirectional Bayesian inference on the unidirectional RNN. We also provide results on music data for which the Bayesian inference is computationally infeasible, demonstrating the scalability of the proposed methods.",
            "referenceCount": 28,
            "citationCount": 114,
            "influentialCitationCount": 13,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-04-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1504.01575"
            },
            "citationStyles": {
                "bibtex": "@Article{Berglund2015BidirectionalRN,\n author = {Mathias Berglund and T. Raiko and M. Honkala and L. K\u00e4rkk\u00e4inen and A. Vetek and J. Karhunen},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Bidirectional Recurrent Neural Networks as Generative Models},\n volume = {abs/1504.01575},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7d2ff802094eb24bed1faa363a8d07947905be3e",
            "@type": "ScholarlyArticle",
            "paperId": "7d2ff802094eb24bed1faa363a8d07947905be3e",
            "corpusId": 246634114,
            "url": "https://www.semanticscholar.org/paper/7d2ff802094eb24bed1faa363a8d07947905be3e",
            "title": "Riemannian Score-Based Generative Modelling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/nips/BortoliMHTTD22",
                "ArXiv": "2202.02763",
                "CorpusId": 246634114
            },
            "abstract": "Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a ``noising'' stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a ``denoising'' process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data.",
            "referenceCount": 141,
            "citationCount": 57,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-02-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bortoli2022RiemannianSG,\n author = {Valentin De Bortoli and Emile Mathieu and M. Hutchinson and James Thornton and Y. Teh and A. Doucet},\n booktitle = {Neural Information Processing Systems},\n title = {Riemannian Score-Based Generative Modelling},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d4154cbd76c4753ba3cb7a5b89ab29bab53384f",
            "@type": "ScholarlyArticle",
            "paperId": "0d4154cbd76c4753ba3cb7a5b89ab29bab53384f",
            "corpusId": 245144350,
            "url": "https://www.semanticscholar.org/paper/0d4154cbd76c4753ba3cb7a5b89ab29bab53384f",
            "title": "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2112.07804",
                "DBLP": "journals/corr/abs-2112-07804",
                "CorpusId": 245144350
            },
            "abstract": "A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000$\\times$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively. Project page and code can be found at https://nvlabs.github.io/denoising-diffusion-gan",
            "referenceCount": 96,
            "citationCount": 251,
            "influentialCitationCount": 38,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-12-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2112.07804"
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2021TacklingTG,\n author = {Zhisheng Xiao and Karsten Kreis and Arash Vahdat},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Tackling the Generative Learning Trilemma with Denoising Diffusion GANs},\n volume = {abs/2112.07804},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8702f27c64292c55e891fbed88350a0a28e991f6",
            "@type": "ScholarlyArticle",
            "paperId": "8702f27c64292c55e891fbed88350a0a28e991f6",
            "corpusId": 61033231,
            "url": "https://www.semanticscholar.org/paper/8702f27c64292c55e891fbed88350a0a28e991f6",
            "title": "An Overview of Deep Generative Models",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1974618110",
                "DOI": "10.1080/02564602.2014.987328",
                "CorpusId": 61033231
            },
            "abstract": "ABSTRACT As an important category of deep models, deep generative model has attracted more and more attention with the proposal of Deep Belief Networks (DBNs) and the fast greedy training algorithm based on restricted Boltzmann machines (RBMs). In the past few years, many different deep generative models are proposed and used in the area of Artificial Intelligence. In this paper, three important deep generative models including DBNs, deep autoencoder, and deep Boltzmann machine are reviewed. In addition, some successful applications of deep generative models in image processing, speech recognition and information retrieval are also introduced and analysed.",
            "referenceCount": 71,
            "citationCount": 43,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2015-03-04",
            "journal": {
                "name": "IETE Technical Review",
                "volume": "32"
            },
            "citationStyles": {
                "bibtex": "@Article{Xu2015AnOO,\n author = {Jungang Xu and Hui Li and Shilong Zhou},\n journal = {IETE Technical Review},\n pages = {131 - 139},\n title = {An Overview of Deep Generative Models},\n volume = {32},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7cfa3ed908e28472a6b0aec6acae5d8df559eb7b",
            "@type": "ScholarlyArticle",
            "paperId": "7cfa3ed908e28472a6b0aec6acae5d8df559eb7b",
            "corpusId": 1671059,
            "url": "https://www.semanticscholar.org/paper/7cfa3ed908e28472a6b0aec6acae5d8df559eb7b",
            "title": "A Brief History of Generative Models for Power Law and Lognormal Distributions",
            "venue": "Internet Mathematics",
            "publicationVenue": {
                "id": "urn:research:4f96108e-f62c-48be-a31a-b3d1ad831330",
                "name": "Internet Mathematics",
                "alternate_names": [
                    "Internet Math"
                ],
                "issn": "1542-7951",
                "url": "http://www.tandfonline.com/loi/uinm20"
            },
            "year": 2004,
            "externalIds": {
                "MAG": "1993803315",
                "DBLP": "journals/im/Mitzenmacher03",
                "DOI": "10.1080/15427951.2004.10129088",
                "CorpusId": 1671059
            },
            "abstract": "Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a lognormal distribution. In trying to learn enough about these distributions to settle the question, I found a rich and long history, spanning many fields. Indeed, several recently proposed models from the computer science community have antecedents in work from decades ago. Here, I briefly survey some of this history, focusing on underlying generative models that lead to these distributions. One finding is that lognormal and power law distributions connect quite naturally, and hence, it is not surprising that lognormal distributions have arisen as a possible alternative to power law distributions across many fields.",
            "referenceCount": 110,
            "citationCount": 1752,
            "influentialCitationCount": 89,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.internetmathematicsjournal.com/article/1385.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2004-01-01",
            "journal": {
                "name": "Internet Mathematics",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Mitzenmacher2004ABH,\n author = {M. Mitzenmacher},\n booktitle = {Internet Mathematics},\n journal = {Internet Mathematics},\n pages = {226 - 251},\n title = {A Brief History of Generative Models for Power Law and Lognormal Distributions},\n volume = {1},\n year = {2004}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a31d0e5668b311b1ec74c5607a9f96c35b395fa8",
            "@type": "ScholarlyArticle",
            "paperId": "a31d0e5668b311b1ec74c5607a9f96c35b395fa8",
            "corpusId": 3248959,
            "url": "https://www.semanticscholar.org/paper/a31d0e5668b311b1ec74c5607a9f96c35b395fa8",
            "title": "A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1611.03852",
                "DBLP": "journals/corr/FinnCAL16",
                "MAG": "2566467060",
                "CorpusId": 3248959
            },
            "abstract": "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.",
            "referenceCount": 28,
            "citationCount": 305,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-11",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.03852"
            },
            "citationStyles": {
                "bibtex": "@Article{Finn2016ACB,\n author = {Chelsea Finn and P. Christiano and P. Abbeel and S. Levine},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models},\n volume = {abs/1611.03852},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e36300219e4b6c44a51a706bcfa0df441d431239",
            "@type": "ScholarlyArticle",
            "paperId": "e36300219e4b6c44a51a706bcfa0df441d431239",
            "corpusId": 14809217,
            "url": "https://www.semanticscholar.org/paper/e36300219e4b6c44a51a706bcfa0df441d431239",
            "title": "Max-Margin Deep Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/nips/LiZSZ15",
                "MAG": "2950748681",
                "ArXiv": "1504.06787",
                "CorpusId": 14809217
            },
            "abstract": "Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) max-margin learning can significantly improve the prediction performance of DGMs and meanwhile retain the generative ability; and (2) mmDGMs are competitive to the state-of-the-art fully discriminative networks by employing deep convolutional neural networks (CNNs) as both recognition and generative models.",
            "referenceCount": 45,
            "citationCount": 31,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-04-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2015MaxMarginDG,\n author = {Chongxuan Li and Jun Zhu and Tianlin Shi and Bo Zhang},\n booktitle = {Neural Information Processing Systems},\n pages = {1837-1845},\n title = {Max-Margin Deep Generative Models},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4e468d3da1797d791db8d514d695b183acb027ee",
            "@type": "ScholarlyArticle",
            "paperId": "4e468d3da1797d791db8d514d695b183acb027ee",
            "corpusId": 222291664,
            "url": "https://www.semanticscholar.org/paper/4e468d3da1797d791db8d514d695b183acb027ee",
            "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3092028330",
                "ArXiv": "2010.05646",
                "DBLP": "journals/corr/abs-2010-05646",
                "CorpusId": 222291664
            },
            "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.",
            "referenceCount": 28,
            "citationCount": 990,
            "influentialCitationCount": 191,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-12",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.05646"
            },
            "citationStyles": {
                "bibtex": "@Article{Kong2020HiFiGANGA,\n author = {Jungil Kong and Jaehyeon Kim and Jaekyoung Bae},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis},\n volume = {abs/2010.05646},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
            "@type": "ScholarlyArticle",
            "paperId": "302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
            "corpusId": 9417016,
            "url": "https://www.semanticscholar.org/paper/302207c149bdf7beb6e46e4d4afbd2fa9ac02c64",
            "title": "StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2952170190",
                "DBLP": "conf/cvpr/ChoiCKH0C18",
                "ArXiv": "1711.09020",
                "DOI": "10.1109/CVPR.2018.00916",
                "CorpusId": 9417016
            },
            "abstract": "Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.",
            "referenceCount": 33,
            "citationCount": 3021,
            "influentialCitationCount": 563,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1711.09020",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-24",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Choi2017StarGANUG,\n author = {Yunjey Choi and Min-Je Choi and M. Kim and Jung-Woo Ha and Sunghun Kim and J. Choo},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {8789-8797},\n title = {StarGAN: Unified Generative Adversarial Networks for Multi-domain Image-to-Image Translation},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "@type": "ScholarlyArticle",
            "paperId": "e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "corpusId": 14336127,
            "url": "https://www.semanticscholar.org/paper/e45c2420e6dc59ba6d357fb0c996ebf43c861560",
            "title": "Exploiting Generative Models in Discriminative Classifiers",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 1998,
            "externalIds": {
                "DBLP": "conf/nips/JaakkolaH98",
                "MAG": "2166473218",
                "CorpusId": 14336127
            },
            "abstract": "Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.",
            "referenceCount": 14,
            "citationCount": 1665,
            "influentialCitationCount": 178,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "1998-12-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jaakkola1998ExploitingGM,\n author = {T. Jaakkola and D. Haussler},\n booktitle = {Neural Information Processing Systems},\n pages = {487-493},\n title = {Exploiting Generative Models in Discriminative Classifiers},\n year = {1998}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b2a675bb617ae1a920e8e29d535cdf27826e999",
            "@type": "ScholarlyArticle",
            "paperId": "3b2a675bb617ae1a920e8e29d535cdf27826e999",
            "corpusId": 248006185,
            "url": "https://www.semanticscholar.org/paper/3b2a675bb617ae1a920e8e29d535cdf27826e999",
            "title": "Video Diffusion Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2204-03458",
                "ArXiv": "2204.03458",
                "DOI": "10.48550/arXiv.2204.03458",
                "CorpusId": 248006185
            },
            "abstract": "Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on established benchmarks for video prediction and unconditional video generation. Supplementary material is available at https://video-diffusion.github.io/",
            "referenceCount": 82,
            "citationCount": 522,
            "influentialCitationCount": 61,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2204.03458",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2022-04-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2204.03458"
            },
            "citationStyles": {
                "bibtex": "@Article{Ho2022VideoDM,\n author = {Jonathan Ho and Tim Salimans and Alexey Gritsenko and William Chan and Mohammad Norouzi and David J. Fleet},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Video Diffusion Models},\n volume = {abs/2204.03458},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1e91fa21b890a8f5d615578f4ddf46c3cb394691",
            "@type": "ScholarlyArticle",
            "paperId": "1e91fa21b890a8f5d615578f4ddf46c3cb394691",
            "corpusId": 246240274,
            "url": "https://www.semanticscholar.org/paper/1e91fa21b890a8f5d615578f4ddf46c3cb394691",
            "title": "RePaint: Inpainting using Denoising Diffusion Probabilistic Models",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2201.09865",
                "DBLP": "journals/corr/abs-2201-09865",
                "DOI": "10.1109/CVPR52688.2022.01117",
                "CorpusId": 246240274
            },
            "abstract": "Free-form inpainting is the task of adding new content to an image in the regions specified by an arbitrary binary mask. Most existing approaches train for a certain distribution of masks, which limits their generalization capabilities to unseen mask types. Furthermore, training with pixel-wise and perceptual losses often leads to simple textural extensions towards the missing areas instead of semantically meaningful generation. In this work, we propose RePaint: A Denoising Diffusion Probabilistic Model (DDPM) based inpainting approach that is applicable to even extreme masks. We employ a pretrained unconditional DDPM as the generative prior. To condition the generation process, we only alter the reverse diffusion iterations by sampling the unmasked regions using the given image infor-mation. Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. Re-Paint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. Github Repository: git.io/RePaint",
            "referenceCount": 60,
            "citationCount": 513,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2201.09865",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-01-24",
            "journal": {
                "name": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lugmayr2022RePaintIU,\n author = {Andreas Lugmayr and Martin Danelljan and Andr\u00e9s Romero and F. Yu and R. Timofte and L. Gool},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11451-11461},\n title = {RePaint: Inpainting using Denoising Diffusion Probabilistic Models},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:de18baa4964804cf471d85a5a090498242d2e79f",
            "@type": "ScholarlyArticle",
            "paperId": "de18baa4964804cf471d85a5a090498242d2e79f",
            "corpusId": 231979499,
            "url": "https://www.semanticscholar.org/paper/de18baa4964804cf471d85a5a090498242d2e79f",
            "title": "Improved Denoising Diffusion Probabilistic Models",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "conf/icml/NicholD21",
                "ArXiv": "2102.09672",
                "CorpusId": 231979499
            },
            "abstract": "Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion",
            "referenceCount": 47,
            "citationCount": 1396,
            "influentialCitationCount": 207,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-02-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2102.09672"
            },
            "citationStyles": {
                "bibtex": "@Article{Nichol2021ImprovedDD,\n author = {Alex Nichol and Prafulla Dhariwal},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Improved Denoising Diffusion Probabilistic Models},\n volume = {abs/2102.09672},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:160742f8a787878dc85595484d2e48378016b4b0",
            "@type": "ScholarlyArticle",
            "paperId": "160742f8a787878dc85595484d2e48378016b4b0",
            "corpusId": 28565151,
            "url": "https://www.semanticscholar.org/paper/160742f8a787878dc85595484d2e48378016b4b0",
            "title": "Combining Generative Models for Multifocal Glioma Segmentation and Registration",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/miccai/KwonSAD14",
                "MAG": "33787415",
                "DOI": "10.1007/978-3-319-10404-1_95",
                "CorpusId": 28565151,
                "PubMed": "25333188"
            },
            "abstract": null,
            "referenceCount": 18,
            "citationCount": 100,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-10404-1_95.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Study"
            ],
            "publicationDate": "2014-09-14",
            "journal": {
                "name": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "volume": "17 Pt 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Kwon2014CombiningGM,\n author = {Dongjin Kwon and R. Shinohara and H. Akbari and C. Davatzikos},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {\n          763-70\n        },\n title = {Combining Generative Models for Multifocal Glioma Segmentation and Registration},\n volume = {17 Pt 1},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
            "@type": "ScholarlyArticle",
            "paperId": "d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
            "corpusId": 5554756,
            "url": "https://www.semanticscholar.org/paper/d9704f8119d6ba748230b4f2ad59f0e8c64fdfb0",
            "title": "Generalized Denoising Auto-Encoders as Generative Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "DBLP": "conf/nips/BengioYAV13",
                "ArXiv": "1305.6663",
                "MAG": "2134842679",
                "CorpusId": 5554756
            },
            "abstract": "Recent work has shown how denoising and contractive autoencoders implicitly capture the structure of the data-generating density, in the case where the corruption noise is Gaussian, the reconstruction error is the squared error, and the data is continuous-valued. This has led to various proposals for sampling from this implicitly learned density function, using Langevin and Metropolis-Hastings MCMC. However, it remained unclear how to connect the training procedure of regularized auto-encoders to the implicit estimation of the underlying data-generating distribution when the data are discrete, or using other forms of corruption process and reconstruction errors. Another issue is the mathematical justification which is only valid in the limit of small corruption noise. We propose here a different attack on the problem, which deals with all these issues: arbitrary (but noisy enough) corruption, arbitrary reconstruction loss (seen as a log-likelihood), handling both discrete and continuous-valued variables, and removing the bias due to non-infinitesimal corruption noise (or non-infinitesimal contractive penalty).",
            "referenceCount": 19,
            "citationCount": 480,
            "influentialCitationCount": 37,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-05-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1305.6663"
            },
            "citationStyles": {
                "bibtex": "@Article{Bengio2013GeneralizedDA,\n author = {Yoshua Bengio and L. Yao and Guillaume Alain and Pascal Vincent},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Generalized Denoising Auto-Encoders as Generative Models},\n volume = {abs/1305.6663},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
            "@type": "ScholarlyArticle",
            "paperId": "bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
            "corpusId": 219781060,
            "url": "https://www.semanticscholar.org/paper/bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8",
            "title": "Generative Pretraining From Pixels",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/ChenRC0JLS20",
                "MAG": "3034445277",
                "CorpusId": 219781060
            },
            "abstract": "Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we \ufb01nd that a GPT-2 scale model learns strong image representations as measured by linear probing, \ufb01ne-tuning, and low-data classi\ufb01cation. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0% accuracy with full \ufb01ne-tuning, matching the top supervised pre-trained models. An even larger model trained on a mix-ture of ImageNet and web images is competitive with self-supervised benchmarks on ImageNet, achieving 72.0% top-1 accuracy on a linear probe of our features.",
            "referenceCount": 81,
            "citationCount": 1061,
            "influentialCitationCount": 89,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2020GenerativePF,\n author = {Mark Chen and Alec Radford and Jeff Wu and Heewoo Jun and Prafulla Dhariwal and D. Luan and Ilya Sutskever},\n booktitle = {International Conference on Machine Learning},\n pages = {1691-1703},\n title = {Generative Pretraining From Pixels},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76d8ccc5a35affb08271fee720a24418c8943b44",
            "@type": "ScholarlyArticle",
            "paperId": "76d8ccc5a35affb08271fee720a24418c8943b44",
            "corpusId": 23787675,
            "url": "https://www.semanticscholar.org/paper/76d8ccc5a35affb08271fee720a24418c8943b44",
            "title": "Visually-Aware Fashion Recommendation and Design with Generative Image Models",
            "venue": "Industrial Conference on Data Mining",
            "publicationVenue": {
                "id": "urn:research:67d15a94-d523-4b5f-be58-03fe2ef9dcfb",
                "name": "Industrial Conference on Data Mining",
                "alternate_names": [
                    "Ind Conf Data Min",
                    "ICDM"
                ],
                "issn": null,
                "url": "http://www.data-mining-forum.de/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2767814563",
                "DBLP": "conf/icdm/KangFWM17",
                "ArXiv": "1711.02231",
                "DOI": "10.1109/ICDM.2017.30",
                "CorpusId": 23787675
            },
            "abstract": "Building effective recommender systems for domains like fashion is challenging due to the high level of subjectivity and the semantic complexity of the features involved (i.e., fashion styles). Recent work has shown that approaches to 'visual' recommendation (e.g. clothing, art, etc.) can be made more accurate by incorporating visual signals directly into the recommendation objective, using 'off-the-shelf' feature representations derived from deep networks. Here, we seek to extend this contribution by showing that recommendation performance can be significantly improved by learning 'fashion aware' image representations directly, i.e., by training the image representation (from the pixel level) and the recommender system jointly; this contribution is related to recent work using Siamese CNNs, though we are able to show improvements over state-of-the-art recommendation techniques such as BPR and variants that make use of pretrained visual features. Furthermore, we show that our model can be used generatively, i.e., given a user and a product category, we can generate new images (i.e., clothing items) that are most consistent with their personal taste. This represents a first step towards building systems that go beyond recommending existing items from a product corpus, but which can be used to suggest styles and aid the design of new products.",
            "referenceCount": 42,
            "citationCount": 211,
            "influentialCitationCount": 20,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1711.02231",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Art",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-01",
            "journal": {
                "name": "2017 IEEE International Conference on Data Mining (ICDM)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kang2017VisuallyAwareFR,\n author = {Wang-Cheng Kang and Chen Fang and Zhaowen Wang and Julian McAuley},\n booktitle = {Industrial Conference on Data Mining},\n journal = {2017 IEEE International Conference on Data Mining (ICDM)},\n pages = {207-216},\n title = {Visually-Aware Fashion Recommendation and Design with Generative Image Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bb8656979f38d95062ba55640c1be65535f57c6a",
            "@type": "ScholarlyArticle",
            "paperId": "bb8656979f38d95062ba55640c1be65535f57c6a",
            "corpusId": 227151657,
            "url": "https://www.semanticscholar.org/paper/bb8656979f38d95062ba55640c1be65535f57c6a",
            "title": "GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3109420014",
                "DBLP": "conf/cvpr/Niemeyer021",
                "ArXiv": "2011.12100",
                "DOI": "10.1109/CVPR46437.2021.01129",
                "CorpusId": 227151657
            },
            "abstract": "Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects\u2019 shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.",
            "referenceCount": 101,
            "citationCount": 651,
            "influentialCitationCount": 83,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2011.12100",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-11-24",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Niemeyer2020GIRAFFERS,\n author = {Michael Niemeyer and Andreas Geiger},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11448-11459},\n title = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6",
            "@type": "ScholarlyArticle",
            "paperId": "0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6",
            "corpusId": 16440891,
            "url": "https://www.semanticscholar.org/paper/0ca6cccbfcf3df972a470c7fe18f7eaed9420cd6",
            "title": "Learning Generative Models with Visual Attention",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2013,
            "externalIds": {
                "ArXiv": "1312.6110",
                "DBLP": "journals/corr/TangSS13",
                "MAG": "2950181755",
                "CorpusId": 16440891
            },
            "abstract": "Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.",
            "referenceCount": 36,
            "citationCount": 83,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2013-12-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1312.6110"
            },
            "citationStyles": {
                "bibtex": "@Article{Tang2013LearningGM,\n author = {Yichuan Tang and Nitish Srivastava and R. Salakhutdinov},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Learning Generative Models with Visual Attention},\n volume = {abs/1312.6110},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:837a7355d9e136eae80f6ced7a837d86b6299bc6",
            "@type": "ScholarlyArticle",
            "paperId": "837a7355d9e136eae80f6ced7a837d86b6299bc6",
            "corpusId": 7918469,
            "url": "https://www.semanticscholar.org/paper/837a7355d9e136eae80f6ced7a837d86b6299bc6",
            "title": "Dirichlet Multinomial Mixtures: Generative Models for Microbial Metagenomics",
            "venue": "PLoS ONE",
            "publicationVenue": {
                "id": "urn:research:0aed7a40-85f3-4c66-9e1b-c1556c57001b",
                "name": "PLoS ONE",
                "alternate_names": [
                    "Plo ONE",
                    "PLOS ONE",
                    "PLO ONE"
                ],
                "issn": "1932-6203",
                "url": "https://journals.plos.org/plosone/"
            },
            "year": 2012,
            "externalIds": {
                "PubMedCentral": "3272020",
                "MAG": "2053801811",
                "DOI": "10.1371/journal.pone.0030126",
                "CorpusId": 7918469,
                "PubMed": "22319561"
            },
            "abstract": "We introduce Dirichlet multinomial mixtures (DMM) for the probabilistic modelling of microbial metagenomics data. This data can be represented as a frequency matrix giving the number of times each taxa is observed in each sample. The samples have different size, and the matrix is sparse, as communities are diverse and skewed to rare taxa. Most methods used previously to classify or cluster samples have ignored these features. We describe each community by a vector of taxa probabilities. These vectors are generated from one of a finite number of Dirichlet mixture components each with different hyperparameters. Observed samples are generated through multinomial sampling. The mixture components cluster communities into distinct \u2018metacommunities\u2019, and, hence, determine envirotypes or enterotypes, groups of communities with a similar composition. The model can also deduce the impact of a treatment and be used for classification. We wrote software for the fitting of DMM models using the \u2018evidence framework\u2019 (http://code.google.com/p/microbedmm/). This includes the Laplace approximation of the model evidence. We applied the DMM model to human gut microbe genera frequencies from Obese and Lean twins. From the model evidence four clusters fit this data best. Two clusters were dominated by Bacteroides and were homogenous; two had a more variable community composition. We could not find a significant impact of body mass on community structure. However, Obese twins were more likely to derive from the high variance clusters. We propose that obesity is not associated with a distinct microbiota but increases the chance that an individual derives from a disturbed enterotype. This is an example of the \u2018Anna Karenina principle (AKP)\u2019 applied to microbial communities: disturbed states having many more configurations than undisturbed. We verify this by showing that in a study of inflammatory bowel disease (IBD) phenotypes, ileal Crohn's disease (ICD) is associated with a more variable community.",
            "referenceCount": 35,
            "citationCount": 615,
            "influentialCitationCount": 46,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0030126&type=printable",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Environmental Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-02-03",
            "journal": {
                "name": "PLoS ONE",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Holmes2012DirichletMM,\n author = {I. Holmes and Keith Harris and C. Quince},\n booktitle = {PLoS ONE},\n journal = {PLoS ONE},\n title = {Dirichlet Multinomial Mixtures: Generative Models for Microbial Metagenomics},\n volume = {7},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:44e6a301714a17b05908da426411b43fa3ebedf0",
            "@type": "ScholarlyArticle",
            "paperId": "44e6a301714a17b05908da426411b43fa3ebedf0",
            "corpusId": 227247980,
            "url": "https://www.semanticscholar.org/paper/44e6a301714a17b05908da426411b43fa3ebedf0",
            "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2012.00926",
                "DBLP": "conf/cvpr/ChanMK0W21",
                "MAG": "3107517429",
                "DOI": "10.1109/CVPR46437.2021.00574",
                "CorpusId": 227247980
            },
            "abstract": "We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: first, they may lack an under-lying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (\u03c0-GAN or pi-GAN), for high-quality 3D-aware image synthesis. \u03c0-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.",
            "referenceCount": 75,
            "citationCount": 569,
            "influentialCitationCount": 98,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2012.00926",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-12-02",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chan2020piGANPI,\n author = {Eric Chan and M. Monteiro and Petr Kellnhofer and Jiajun Wu and Gordon Wetzstein},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5795-5805},\n title = {pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:293fd0e760a3e4845292b998000ba1b143c0a731",
            "@type": "ScholarlyArticle",
            "paperId": "293fd0e760a3e4845292b998000ba1b143c0a731",
            "corpusId": 2750116,
            "url": "https://www.semanticscholar.org/paper/293fd0e760a3e4845292b998000ba1b143c0a731",
            "title": "Wasserstein Learning of Deep Generative Point Process Models",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.08051",
                "MAG": "2617104629",
                "DBLP": "conf/nips/XiaoFYYYSZ17",
                "CorpusId": 2750116
            },
            "abstract": "Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.",
            "referenceCount": 32,
            "citationCount": 145,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2017WassersteinLO,\n author = {Shuai Xiao and Mehrdad Farajtabar and X. Ye and Junchi Yan and Xiaokang Yang and Le Song and H. Zha},\n booktitle = {Neural Information Processing Systems},\n pages = {3247-3257},\n title = {Wasserstein Learning of Deep Generative Point Process Models},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e342165a614588878ad0f4bc9bacf3905df34d08",
            "@type": "ScholarlyArticle",
            "paperId": "e342165a614588878ad0f4bc9bacf3905df34d08",
            "corpusId": 252070859,
            "url": "https://www.semanticscholar.org/paper/e342165a614588878ad0f4bc9bacf3905df34d08",
            "title": "Diffusion Models: A Comprehensive Survey of Methods and Applications",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2022,
            "externalIds": {
                "ArXiv": "2209.00796",
                "DBLP": "journals/corr/abs-2209-00796",
                "DOI": "10.1145/3626235",
                "CorpusId": 252070859
            },
            "abstract": "Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy",
            "referenceCount": 392,
            "citationCount": 327,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2209.00796",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2022-09-02",
            "journal": {
                "name": "ACM Computing Surveys",
                "volume": "56"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2022DiffusionMA,\n author = {Ling Yang and Zhilong Zhang and Shenda Hong and Runsheng Xu and Yue Zhao and Yingxia Shao and Wentao Zhang and Ming-Hsuan Yang and Bin Cui},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys},\n pages = {1 - 39},\n title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},\n volume = {56},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dd1a13640716b2c0812b2af98640f297fa26c18f",
            "@type": "ScholarlyArticle",
            "paperId": "dd1a13640716b2c0812b2af98640f297fa26c18f",
            "corpusId": 23277467,
            "url": "https://www.semanticscholar.org/paper/dd1a13640716b2c0812b2af98640f297fa26c18f",
            "title": "Prediction, explanation, and the role of generative models in language processing.",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1999404211",
                "DOI": "10.1017/S0140525X12002312",
                "CorpusId": 23277467,
                "PubMed": "23663410"
            },
            "abstract": "We propose, following Clark, that generative models also play a central role in the perception and interpretation of linguistic signals. The data explanation approach provides a rationale for the role of prediction in language processing and unifies a number of phenomena, including multiple-cue integration, adaptation effects, and cortical responses to violations of linguistic expectations.",
            "referenceCount": 23,
            "citationCount": 49,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "LettersAndComments",
                "JournalArticle"
            ],
            "publicationDate": "2013-06-01",
            "journal": {
                "name": "The Behavioral and brain sciences",
                "volume": "36 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Farmer2013PredictionEA,\n author = {Thomas A. Farmer and Meredith Brown and M. Tanenhaus},\n booktitle = {Behavioral and Brain Sciences},\n journal = {The Behavioral and brain sciences},\n pages = {\n          211-2\n        },\n title = {Prediction, explanation, and the role of generative models in language processing.},\n volume = {36 3},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9b41d745fe3a76443bd0420bc5f2df28be2bd65f",
            "@type": "ScholarlyArticle",
            "paperId": "9b41d745fe3a76443bd0420bc5f2df28be2bd65f",
            "corpusId": 248811081,
            "url": "https://www.semanticscholar.org/paper/9b41d745fe3a76443bd0420bc5f2df28be2bd65f",
            "title": "Diffusion Models for Adversarial Purification",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/icml/NieGHXVA22",
                "ArXiv": "2205.07460",
                "DOI": "10.48550/arXiv.2205.07460",
                "CorpusId": 248811081
            },
            "abstract": "Adversarial purification refers to a class of defense methods that remove adversarial perturbations using a generative model. These methods do not make assumptions on the form of attack and the classification model, and thus can defend pre-existing classifiers against unseen threats. However, their performance currently falls behind adversarial training methods. In this work, we propose DiffPure that uses diffusion models for adversarial purification: Given an adversarial example, we first diffuse it with a small amount of noise following a forward diffusion process, and then recover the clean image through a reverse generative process. To evaluate our method against strong adaptive attacks in an efficient and scalable way, we propose to use the adjoint method to compute full gradients of the reverse generative process. Extensive experiments on three image datasets including CIFAR-10, ImageNet and CelebA-HQ with three classifier architectures including ResNet, WideResNet and ViT demonstrate that our method achieves the state-of-the-art results, outperforming current adversarial training and adversarial purification methods, often by a large margin. Project page: https://diffpure.github.io.",
            "referenceCount": 69,
            "citationCount": 157,
            "influentialCitationCount": 32,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2205.07460",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-05-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nie2022DiffusionMF,\n author = {Weili Nie and Brandon Guo and Yujia Huang and Chaowei Xiao and Arash Vahdat and Anima Anandkumar},\n booktitle = {International Conference on Machine Learning},\n pages = {16805-16827},\n title = {Diffusion Models for Adversarial Purification},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a28cdccba07dbf977795e15ff2c9b7ec80dac050",
            "@type": "ScholarlyArticle",
            "paperId": "a28cdccba07dbf977795e15ff2c9b7ec80dac050",
            "corpusId": 245131359,
            "url": "https://www.semanticscholar.org/paper/a28cdccba07dbf977795e15ff2c9b7ec80dac050",
            "title": "Score-Based Generative Modeling with Critically-Damped Langevin Diffusion",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2021,
            "externalIds": {
                "ArXiv": "2112.07068",
                "DBLP": "journals/corr/abs-2112-07068",
                "CorpusId": 245131359
            },
            "abstract": "Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the generative model learns to denoise. The complexity of this denoising task is, apart from the data distribution itself, uniquely determined by the diffusion process. We argue that current SGMs employ overly simplistic diffusions, leading to unnecessarily complex denoising processes, which limit generative modeling performance. Based on connections to statistical mechanics, we propose a novel critically-damped Langevin diffusion (CLD) and show that CLD-based SGMs achieve superior performance. CLD can be interpreted as running a joint diffusion in an extended space, where the auxiliary variables can be considered \u201cvelocities\u201d that are coupled to the data variables as in Hamiltonian dynamics. We derive a novel score matching objective for CLD and show that the model only needs to learn the score function of the conditional distribution of the velocity given data, an easier task than learning scores of the data directly. We also derive a new sampling scheme for ef\ufb01cient synthesis from CLD-based diffusion models. We \ufb01nd that CLD outperforms previous SGMs in synthesis quality for similar network architectures and sampling compute budgets. We show that our novel sampler for CLD signi\ufb01cantly outperforms solvers such as Euler\u2013Maruyama. Our framework provides new insights into score-based denoising diffusion models and can be readily used for high-resolution image synthesis. VPSDE. We leave the study of CLD with maximum likelihood training for high-dimensional (image) datasets to future work.",
            "referenceCount": 111,
            "citationCount": 132,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-12-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2112.07068"
            },
            "citationStyles": {
                "bibtex": "@Article{Dockhorn2021ScoreBasedGM,\n author = {Tim Dockhorn and Arash Vahdat and Karsten Kreis},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Score-Based Generative Modeling with Critically-Damped Langevin Diffusion},\n volume = {abs/2112.07068},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:988a1ab0c1ebd7eb4817fbc8ce41f962dd519998",
            "@type": "ScholarlyArticle",
            "paperId": "988a1ab0c1ebd7eb4817fbc8ce41f962dd519998",
            "corpusId": 232168940,
            "url": "https://www.semanticscholar.org/paper/988a1ab0c1ebd7eb4817fbc8ce41f962dd519998",
            "title": "An introduction to deep generative modeling",
            "venue": "GAMM-Mitteilungen",
            "publicationVenue": {
                "id": "urn:research:b388508d-7838-440c-a44d-bb2a1bfbe204",
                "name": "GAMM-Mitteilungen",
                "alternate_names": [
                    "Gamm-mitteilungen"
                ],
                "issn": "0936-7195",
                "url": "https://onlinelibrary.wiley.com/journal/15222608"
            },
            "year": 2021,
            "externalIds": {
                "MAG": "3172949370",
                "DBLP": "journals/corr/abs-2103-05180",
                "ArXiv": "2103.05180",
                "DOI": "10.1002/gamm.202100008",
                "CorpusId": 232168940
            },
            "abstract": "Deep generative models (DGM) are neural networks with many hidden layers trained to approximate complicated, high\u2010dimensional probability distributions using samples. When trained successfully, we can use the DGM to estimate the likelihood of each observation and to create new samples from the underlying distribution. Developing DGMs has become one of the most hotly researched fields in artificial intelligence in recent years. The literature on DGMs has become vast and is growing rapidly. Some advances have even reached the public sphere, for example, the recent successes in generating realistic\u2010looking images, voices, or movies; so\u2010called deep fakes. Despite these successes, several mathematical and practical issues limit the broader use of DGMs: given a specific dataset, it remains challenging to design and train a DGM and even more challenging to find out why a particular model is or is not effective. To help advance the theoretical understanding of DGMs, we introduce DGMs and provide a concise mathematical framework for modeling the three most popular approaches: normalizing flows, variational autoencoders, and generative adversarial networks. We illustrate the advantages and disadvantages of these basic approaches using numerical experiments. Our goal is to enable and motivate the reader to contribute to this proliferating research area. Our presentation also emphasizes relations between generative modeling and optimal transport.",
            "referenceCount": 55,
            "citationCount": 120,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2103.05180",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-03-09",
            "journal": {
                "name": "GAMM\u2010Mitteilungen",
                "volume": "44"
            },
            "citationStyles": {
                "bibtex": "@Article{Ruthotto2021AnIT,\n author = {Lars Ruthotto and E. Haber},\n booktitle = {GAMM-Mitteilungen},\n journal = {GAMM\u2010Mitteilungen},\n title = {An introduction to deep generative modeling},\n volume = {44},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
            "@type": "ScholarlyArticle",
            "paperId": "07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
            "corpusId": 221655075,
            "url": "https://www.semanticscholar.org/paper/07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6",
            "title": "GeDi: Generative Discriminator Guided Sequence Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/emnlp/KrauseGMKJSR21",
                "ArXiv": "2009.06367",
                "MAG": "3085190015",
                "DOI": "10.18653/v1/2021.findings-emnlp.424",
                "CorpusId": 221655075
            },
            "abstract": "While large-scale language models (LMs) are able to imitate the distribution of natural language well enough to generate realistic text, it is difficult to control which regions of the distribution they generate. This is especially problematic because datasets used for training large LMs usually contain significant toxicity, hate, bias, and negativity. We propose GeDi as an efficient method for using smaller LMs as generative discriminators to guide generation from large LMs to make them safer and more controllable. GeDi guides generation at each step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class-conditional distributions; one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute, or anti control code. We find that GeDi gives stronger controllability than the state of the art method while also achieving generation speeds more than 30 times faster. Additionally, training GeDi on only four topics allows us to controllably generate new topics zero-shot from just a keyword, unlocking a new capability that previous controllable generation methods do not have. Lastly, we show that GeDi can make GPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic quality, making it by far the most practical existing method for detoxifying large language models while maintaining a fast generation speed.",
            "referenceCount": 64,
            "citationCount": 251,
            "influentialCitationCount": 54,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.findings-emnlp.424.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-09-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Krause2020GeDiGD,\n author = {Ben Krause and Akhilesh Deepak Gotmare and Bryan McCann and N. Keskar and Shafiq R. Joty and R. Socher and Nazneen Rajani},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {4929-4952},\n title = {GeDi: Generative Discriminator Guided Sequence Generation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2ed0d4931d89528bd53482a0b6587ebcbba6d096",
            "@type": "ScholarlyArticle",
            "paperId": "2ed0d4931d89528bd53482a0b6587ebcbba6d096",
            "corpusId": 218862956,
            "url": "https://www.semanticscholar.org/paper/2ed0d4931d89528bd53482a0b6587ebcbba6d096",
            "title": "Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-11129",
                "MAG": "3026874504",
                "ArXiv": "2005.11129",
                "CorpusId": 218862956
            },
            "abstract": "Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.",
            "referenceCount": 37,
            "citationCount": 279,
            "influentialCitationCount": 52,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-05-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2005.11129"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2020GlowTTSAG,\n author = {Jaehyeon Kim and Sungwon Kim and Jungil Kong and Sungroh Yoon},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search},\n volume = {abs/2005.11129},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ca959b31692cf41b163cebc656a208e48f6f07d2",
            "@type": "ScholarlyArticle",
            "paperId": "ca959b31692cf41b163cebc656a208e48f6f07d2",
            "corpusId": 2801340,
            "url": "https://www.semanticscholar.org/paper/ca959b31692cf41b163cebc656a208e48f6f07d2",
            "title": "Generative Temporal Models with Memory",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.04649",
                "DBLP": "journals/corr/GemiciHSWMRAL17",
                "MAG": "2588768352",
                "CorpusId": 2801340
            },
            "abstract": "We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.",
            "referenceCount": 45,
            "citationCount": 56,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-15",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.04649"
            },
            "citationStyles": {
                "bibtex": "@Article{Gemici2017GenerativeTM,\n author = {Mevlana Gemici and Chia-Chun Hung and Adam Santoro and Greg Wayne and S. Mohamed and Danilo Jimenez Rezende and David Amos and T. Lillicrap},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generative Temporal Models with Memory},\n volume = {abs/1702.04649},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
            "@type": "ScholarlyArticle",
            "paperId": "32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
            "corpusId": 3439214,
            "url": "https://www.semanticscholar.org/paper/32c4e19f4a757f6c6984416b97d69e287d1d0ecd",
            "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1609.05473",
                "DBLP": "conf/aaai/YuZWY17",
                "MAG": "2964268978",
                "DOI": "10.1609/aaai.v31i1.10804",
                "CorpusId": 3439214
            },
            "abstract": "\n \n As a new way of training generative models, Generative Adversarial Net (GAN) that uses a discriminative model to guide the training of the generative model has enjoyed considerable success in generating real-valued data. However, it has limitations when the goal is for generating sequences of discrete tokens. A major reason lies in that the discrete outputs from the generative model make it difficult to pass the gradient update from the discriminative model to the generative model. Also, the discriminative model can only assess a complete sequence, while for a partially generated sequence, it is non-trivial to balance its current score and the future one once the entire sequence has been generated. In this paper, we propose a sequence generation framework, called SeqGAN, to solve the problems. Modeling the data generator as a stochastic policy in reinforcement learning (RL), SeqGAN bypasses the generator differentiation problem by directly performing gradient policy update. The RL reward signal comes from the GAN discriminator judged on a complete sequence, and is passed back to the intermediate state-action steps using Monte Carlo search. Extensive experiments on synthetic data and real-world tasks demonstrate significant improvements over strong baselines.\n \n",
            "referenceCount": 41,
            "citationCount": 2066,
            "influentialCitationCount": 276,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/10804/10663",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-09-18",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2016SeqGANSG,\n author = {Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {2852-2858},\n title = {SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:efa1647594b236361610a20d507127f0586a379b",
            "@type": "ScholarlyArticle",
            "paperId": "efa1647594b236361610a20d507127f0586a379b",
            "corpusId": 252199918,
            "url": "https://www.semanticscholar.org/paper/efa1647594b236361610a20d507127f0586a379b",
            "title": "Diffusion Models in Vision: A Survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "journals/corr/abs-2209-04747",
                "ArXiv": "2209.04747",
                "DOI": "10.1109/TPAMI.2023.3261988",
                "CorpusId": 252199918,
                "PubMed": "37030794"
            },
            "abstract": "Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.",
            "referenceCount": 171,
            "citationCount": 255,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2209.04747",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2022-09-10",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "45"
            },
            "citationStyles": {
                "bibtex": "@Article{Croitoru2022DiffusionMI,\n author = {Florinel-Alin Croitoru and Vlad Hondru and Radu Tudor Ionescu and M. Shah},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {10850-10869},\n title = {Diffusion Models in Vision: A Survey},\n volume = {45},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c3294425af6e2c059835ec7f0dca7290b48a8faf",
            "@type": "ScholarlyArticle",
            "paperId": "c3294425af6e2c059835ec7f0dca7290b48a8faf",
            "corpusId": 54457478,
            "url": "https://www.semanticscholar.org/paper/c3294425af6e2c059835ec7f0dca7290b48a8faf",
            "title": "Learning Implicit Fields for Generative Shape Modeling",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2954625812",
                "ArXiv": "1812.02822",
                "DBLP": "journals/corr/abs-1812-02822",
                "DOI": "10.1109/CVPR.2019.00609",
                "CorpusId": 54457478
            },
            "abstract": "We advocate the use of implicit fields for learning generative models of shapes and introduce an implicit field decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit field assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classifier. Specifically, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.",
            "referenceCount": 51,
            "citationCount": 1197,
            "influentialCitationCount": 116,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1812.02822",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-06",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018LearningIF,\n author = {Zhiqin Chen and Hao Zhang},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {5932-5941},\n title = {Learning Implicit Fields for Generative Shape Modeling},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:79c390d256f259735bf78f5f81295db8f532b488",
            "@type": "ScholarlyArticle",
            "paperId": "79c390d256f259735bf78f5f81295db8f532b488",
            "corpusId": 3330361,
            "url": "https://www.semanticscholar.org/paper/79c390d256f259735bf78f5f81295db8f532b488",
            "title": "Using probabilistic generative models for ranking risks of Android apps",
            "venue": "Conference on Computer and Communications Security",
            "publicationVenue": {
                "id": "urn:research:73f7fe95-b68b-468f-b7ba-3013ca879e50",
                "name": "Conference on Computer and Communications Security",
                "alternate_names": [
                    "Int Workshop Cogn Cell Syst",
                    "CCS",
                    "Comput Commun Secur",
                    "CcS",
                    "International Symposium on Community-centric Systems",
                    "International Workshop on Cognitive Cellular Systems",
                    "Conf Comput Commun Secur",
                    "Comb Comput Sci",
                    "Int Symp Community-centric Syst",
                    "Combinatorics and Computer Science",
                    "Circuits, Signals, and Systems",
                    "Computer and Communications Security",
                    "Circuit Signal Syst"
                ],
                "issn": null,
                "url": "https://dl.acm.org/conference/ccs"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2058180826",
                "DBLP": "conf/ccs/PengGSLQPNM12",
                "DOI": "10.1145/2382196.2382224",
                "CorpusId": 3330361
            },
            "abstract": "One of Android's main defense mechanisms against malicious apps is a risk communication mechanism which, before a user installs an app, warns the user about the permissions the app requires, trusting that the user will make the right decision. This approach has been shown to be ineffective as it presents the risk information of each app in a \"tand-alone\" ashion and in a way that requires too much technical knowledge and time to distill useful information. We introduce the notion of risk scoring and risk ranking for Android apps, to improve risk communication for Android apps, and identify three desiderata for an effective risk scoring scheme. We propose to use probabilistic generative models for risk scoring schemes, and identify several such models, ranging from the simple Naive Bayes, to advanced hierarchical mixture models. Experimental results conducted using real-world datasets show that probabilistic general models significantly outperform existing approaches, and that Naive Bayes models give a promising risk scoring approach.",
            "referenceCount": 29,
            "citationCount": 403,
            "influentialCitationCount": 24,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://homes.cerias.purdue.edu/~crisn/papers/android_ccs_2012.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Book",
                "Conference"
            ],
            "publicationDate": "2012-10-16",
            "journal": {
                "name": "Proceedings of the 2012 ACM conference on Computer and communications security",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Peng2012UsingPG,\n author = {Hao Peng and Christopher S. Gates and B. Sarma and Ninghui Li and Y. Qi and Rahul Potharaju and C. Nita-Rotaru and Ian Molloy},\n booktitle = {Conference on Computer and Communications Security},\n journal = {Proceedings of the 2012 ACM conference on Computer and communications security},\n title = {Using probabilistic generative models for ranking risks of Android apps},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ac4d4414f7555346dfad7c6a441e8d0e675a30a4",
            "@type": "ScholarlyArticle",
            "paperId": "ac4d4414f7555346dfad7c6a441e8d0e675a30a4",
            "corpusId": 232185555,
            "url": "https://www.semanticscholar.org/paper/ac4d4414f7555346dfad7c6a441e8d0e675a30a4",
            "title": "SMPLicit: Topology-aware Generative Model for Clothed People",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2103-06871",
                "ArXiv": "2103.06871",
                "DOI": "10.1109/CVPR46437.2021.01170",
                "CorpusId": 232185555
            },
            "abstract": "In this paper we introduce SMPLicit, a novel generative model to jointly represent body pose, shape and clothing geometry. In contrast to existing learning-based approaches that require training specific models for each type of garment, SMPLicit can represent in a unified manner different garment topologies (e.g. from sleeveless tops to hoodies and to open jackets), while controlling other properties like the garment size or tightness/looseness. We show our model to be applicable to a large variety of garments including T-shirts, hoodies, jackets, shorts, pants, skirts, shoes and even hair. The representation flexibility of SMPLicit builds upon an implicit model conditioned with the SMPL human body parameters and a learnable latent space which is semantically interpretable and aligned with the clothing attributes. The proposed model is fully differentiable, allowing for its use into larger end-to-end trainable systems. In the experimental section, we demonstrate SMPLicit can be readily used for fitting 3D scans and for 3D reconstruction in images of dressed people. In both cases we are able to go beyond state of the art, by retrieving complex garment geometries, handling situations with multiple clothing layers and providing a tool for easy outfit editing. To stimulate further research in this direction, we will make our code and model publicly available at http://www.iri.upc.edu/people/ecorona/smplicit/.",
            "referenceCount": 71,
            "citationCount": 122,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://upcommons.upc.edu/bitstream/2117/365385/1/2511-SMPLicit_-Topology-Aware-Generative-Model-for-Clothed-People.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-03-11",
            "journal": {
                "name": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Corona2021SMPLicitTG,\n author = {Enric Corona and Albert Pumarola and G. Aleny\u00e0 and Gerard Pons-Moll and F. Moreno-Noguer},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {11870-11880},\n title = {SMPLicit: Topology-aware Generative Model for Clothed People},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:014576b866078524286802b1d0e18628520aa886",
            "@type": "ScholarlyArticle",
            "paperId": "014576b866078524286802b1d0e18628520aa886",
            "corpusId": 222140788,
            "url": "https://www.semanticscholar.org/paper/014576b866078524286802b1d0e18628520aa886",
            "title": "Denoising Diffusion Implicit Models",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.02502",
                "DBLP": "journals/corr/abs-2010-02502",
                "MAG": "3092442149",
                "CorpusId": 222140788
            },
            "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples $10 \\times$ to $50 \\times$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.",
            "referenceCount": 48,
            "citationCount": 1974,
            "influentialCitationCount": 467,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-06",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.02502"
            },
            "citationStyles": {
                "bibtex": "@Article{Song2020DenoisingDI,\n author = {Jiaming Song and Chenlin Meng and Stefano Ermon},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Denoising Diffusion Implicit Models},\n volume = {abs/2010.02502},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ec721b4b280ce593428499d013bc01ca19dbcac3",
            "@type": "ScholarlyArticle",
            "paperId": "ec721b4b280ce593428499d013bc01ca19dbcac3",
            "corpusId": 119285380,
            "url": "https://www.semanticscholar.org/paper/ec721b4b280ce593428499d013bc01ca19dbcac3",
            "title": "Joint Discriminative and Generative Learning for Person Re-Identification",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2963049565",
                "DBLP": "conf/cvpr/ZhengYY00K19",
                "ArXiv": "1904.07223",
                "DOI": "10.1109/CVPR.2019.00224",
                "CorpusId": 119285380
            },
            "abstract": "Person re-identification (re-id) remains challenging due to significant intra-class variations across different cameras. Recently, there has been a growing interest in using generative models to augment training data and enhance the invariance to input changes. The generative pipelines in existing methods, however, stay relatively separate from the discriminative re-id learning stages. Accordingly, re-id models are often trained in a straightforward manner on the generated data. In this paper, we seek to improve learned re-id embeddings by better leveraging the generated data. To this end, we propose a joint learning framework that couples re-id learning and data generation end-to-end. Our model involves a generative module that separately encodes each person into an appearance code and a structure code, and a discriminative module that shares the appearance encoder with the generative module. By switching the appearance or structure codes, the generative module is able to generate high-quality cross-id composed images, which are online fed back to the appearance encoder and used to improve the discriminative module. The proposed joint learning framework renders significant improvement over the baseline without using generated data, leading to the state-of-the-art performance on several benchmark datasets.",
            "referenceCount": 63,
            "citationCount": 628,
            "influentialCitationCount": 63,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://opus.lib.uts.edu.au/bitstream/10453/147874/3/1346660-ManualAPI.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-04-15",
            "journal": {
                "name": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zheng2019JointDA,\n author = {Zhedong Zheng and Xiaodong Yang and Zhiding Yu and Liang Zheng and Yi Yang and J. Kautz},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2133-2142},\n title = {Joint Discriminative and Generative Learning for Person Re-Identification},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:94bcd712aed610b8eaeccc57136d65ec988356f2",
            "@type": "ScholarlyArticle",
            "paperId": "94bcd712aed610b8eaeccc57136d65ec988356f2",
            "corpusId": 235694314,
            "url": "https://www.semanticscholar.org/paper/94bcd712aed610b8eaeccc57136d65ec988356f2",
            "title": "Variational Diffusion Models",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2107-00630",
                "ArXiv": "2107.00630",
                "CorpusId": 235694314
            },
            "abstract": "Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .",
            "referenceCount": 54,
            "citationCount": 495,
            "influentialCitationCount": 75,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-07-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2107.00630"
            },
            "citationStyles": {
                "bibtex": "@Article{Kingma2021VariationalDM,\n author = {Diederik P. Kingma and Tim Salimans and Ben Poole and Jonathan Ho},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Variational Diffusion Models},\n volume = {abs/2107.00630},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ffdcad14d2f6a12f607b59f88da4a939f4821691",
            "@type": "ScholarlyArticle",
            "paperId": "ffdcad14d2f6a12f607b59f88da4a939f4821691",
            "corpusId": 107645,
            "url": "https://www.semanticscholar.org/paper/ffdcad14d2f6a12f607b59f88da4a939f4821691",
            "title": "f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.00709",
                "DBLP": "journals/corr/NowozinCT16",
                "MAG": "2963800509",
                "CorpusId": 107645
            },
            "abstract": "Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.",
            "referenceCount": 39,
            "citationCount": 1479,
            "influentialCitationCount": 177,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-06-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.00709"
            },
            "citationStyles": {
                "bibtex": "@Article{Nowozin2016fGANTG,\n author = {Sebastian Nowozin and Botond Cseke and Ryota Tomioka},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},\n volume = {abs/1606.00709},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "@type": "ScholarlyArticle",
            "paperId": "e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "corpusId": 17427022,
            "url": "https://www.semanticscholar.org/paper/e163a2e89c136cb4442e34c72f7173a0ff46dc79",
            "title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery",
            "venue": "Information Processing in Medical Imaging",
            "publicationVenue": {
                "id": "urn:research:8c4f911f-84c7-4e8e-87ed-96b3d4c55774",
                "name": "Information Processing in Medical Imaging",
                "alternate_names": [
                    "Inf Process Med Imaging",
                    "IPMI"
                ],
                "issn": null,
                "url": "https://en.wikipedia.org/wiki/Information_Processing_in_Medical_Imaging"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2951312444",
                "DBLP": "journals/corr/SchleglSWSL17",
                "ArXiv": "1703.05921",
                "DOI": "10.1007/978-3-319-59050-9_12",
                "CorpusId": 17427022
            },
            "abstract": null,
            "referenceCount": 20,
            "citationCount": 1761,
            "influentialCitationCount": 244,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1703.05921",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-17",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Schlegl2017UnsupervisedAD,\n author = {T. Schlegl and Philipp Seeb\u00f6ck and S. Waldstein and U. Schmidt-Erfurth and G. Langs},\n booktitle = {Information Processing in Medical Imaging},\n pages = {146-157},\n title = {Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:91b32fc0a23f0af53229fceaae9cce43a0406d2e",
            "@type": "ScholarlyArticle",
            "paperId": "91b32fc0a23f0af53229fceaae9cce43a0406d2e",
            "corpusId": 235755106,
            "url": "https://www.semanticscholar.org/paper/91b32fc0a23f0af53229fceaae9cce43a0406d2e",
            "title": "Structured Denoising Diffusion Models in Discrete State-Spaces",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2107-03006",
                "ArXiv": "2107.03006",
                "CorpusId": 235755106
            },
            "abstract": "Denoising diffusion probabilistic models (DDPMs) (Ho et al. 2020) have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. 2021, by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss. For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.",
            "referenceCount": 71,
            "citationCount": 330,
            "influentialCitationCount": 65,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2021-07-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2107.03006"
            },
            "citationStyles": {
                "bibtex": "@Article{Austin2021StructuredDD,\n author = {Jacob Austin and Daniel D. Johnson and Jonathan Ho and Daniel Tarlow and Rianne van den Berg},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Structured Denoising Diffusion Models in Discrete State-Spaces},\n volume = {abs/2107.03006},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2c740e574eea66fdcf473e15ed2c228baef2eccd",
            "@type": "ScholarlyArticle",
            "paperId": "2c740e574eea66fdcf473e15ed2c228baef2eccd",
            "corpusId": 5216145,
            "url": "https://www.semanticscholar.org/paper/2c740e574eea66fdcf473e15ed2c228baef2eccd",
            "title": "NIPS 2016 Tutorial: Generative Adversarial Networks",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2577946330",
                "DBLP": "journals/corr/Goodfellow17",
                "ArXiv": "1701.00160",
                "CorpusId": 5216145
            },
            "abstract": "This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.",
            "referenceCount": 71,
            "citationCount": 1461,
            "influentialCitationCount": 139,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2016-12-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.00160"
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2016NIPS2T,\n author = {I. Goodfellow},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {NIPS 2016 Tutorial: Generative Adversarial Networks},\n volume = {abs/1701.00160},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef2b790aae2c1e1c7eb8a8777515266b5094de88",
            "@type": "ScholarlyArticle",
            "paperId": "ef2b790aae2c1e1c7eb8a8777515266b5094de88",
            "corpusId": 174801267,
            "url": "https://www.semanticscholar.org/paper/ef2b790aae2c1e1c7eb8a8777515266b5094de88",
            "title": "Residual Flows for Invertible Generative Modeling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2948700496",
                "DBLP": "conf/nips/ChenBDJ19",
                "ArXiv": "1906.02735",
                "CorpusId": 174801267
            },
            "abstract": "Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a \"Russian roulette\" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.",
            "referenceCount": 53,
            "citationCount": 302,
            "influentialCitationCount": 44,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-06-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1906.02735"
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2019ResidualFF,\n author = {Ricky T. Q. Chen and Jens Behrmann and D. Duvenaud and J. Jacobsen},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Residual Flows for Invertible Generative Modeling},\n volume = {abs/1906.02735},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4df3b534beeba05ba930814162ea6e19948c5fcd",
            "@type": "ScholarlyArticle",
            "paperId": "4df3b534beeba05ba930814162ea6e19948c5fcd",
            "corpusId": 211988680,
            "url": "https://www.semanticscholar.org/paper/4df3b534beeba05ba930814162ea6e19948c5fcd",
            "title": "Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2020,
            "externalIds": {
                "MAG": "3010486187",
                "DBLP": "conf/cvpr/DurallKK20",
                "ArXiv": "2003.01826",
                "DOI": "10.1109/CVPR42600.2020.00791",
                "CorpusId": 211988680
            },
            "abstract": "Generative convolutional deep neural networks, e.g. popular GAN architectures, are relying on convolution based up-sampling methods to produce non-scalar outputs like images or video sequences. In this paper, we show that common up-sampling methods, i.e. known as up-convolution or transposed convolution, are causing the inability of such models to reproduce spectral distributions of natural training data correctly. This effect is independent of the underlying architecture and we show that it can be used to easily detect generated data like deepfakes with up to 100% accuracy on public benchmarks. To overcome this drawback of current generative models, we propose to add a novel spectral regularization term to the training optimization objective. We show that this approach not only allows to train spectral consistent GANs that are avoiding high frequency errors. Also, we show that a correct approximation of the frequency spectrum has positive effects on the training stability and output quality of generative networks.",
            "referenceCount": 64,
            "citationCount": 209,
            "influentialCitationCount": 19,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2003.01826",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-03-03",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Durall2020WatchYU,\n author = {R. Durall and M. Keuper and J. Keuper},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {7887-7896},\n title = {Watch Your Up-Convolution: CNN Based Generative Deep Neural Networks Are Failing to Reproduce Spectral Distributions},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f6284d750cf12669ca3bc12a1b485545af776239",
            "@type": "ScholarlyArticle",
            "paperId": "f6284d750cf12669ca3bc12a1b485545af776239",
            "corpusId": 57373848,
            "url": "https://www.semanticscholar.org/paper/f6284d750cf12669ca3bc12a1b485545af776239",
            "title": "EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1901.00212",
                "DBLP": "journals/corr/abs-1901-00212",
                "MAG": "2907097116",
                "CorpusId": 57373848
            },
            "abstract": "Over the last few years, deep learning techniques have yielded significant improvements in image inpainting. However, many of these techniques fail to reconstruct reasonable structures as they are commonly over-smoothed and/or blurry. This paper develops a new approach for image inpainting that does a better job of reproducing filled regions exhibiting fine details. We propose a two-stage adversarial model EdgeConnect that comprises of an edge generator followed by an image completion network. The edge generator hallucinates edges of the missing region (both regular and irregular) of the image, and the image completion network fills in the missing regions using hallucinated edges as a priori. We evaluate our model end-to-end over the publicly available datasets CelebA, Places2, and Paris StreetView, and show that it outperforms current state-of-the-art techniques quantitatively and qualitatively. Code and models available at: this https URL",
            "referenceCount": 60,
            "citationCount": 574,
            "influentialCitationCount": 143,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.00212"
            },
            "citationStyles": {
                "bibtex": "@Article{Nazeri2019EdgeConnectGI,\n author = {Kamyar Nazeri and Eric Ng and Tony Joseph and F. Qureshi and Mehran Ebrahimi},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},\n volume = {abs/1901.00212},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:23c98518cea776c7ea142841de79c708a981dabc",
            "@type": "ScholarlyArticle",
            "paperId": "23c98518cea776c7ea142841de79c708a981dabc",
            "corpusId": 52863226,
            "url": "https://www.semanticscholar.org/paper/23c98518cea776c7ea142841de79c708a981dabc",
            "title": "Emergence of a 'visual number sense' in hierarchical generative models",
            "venue": "Nature Neuroscience",
            "publicationVenue": {
                "id": "urn:research:7892f01e-f701-4d07-8c36-e108b84ec6ab",
                "name": "Nature Neuroscience",
                "alternate_names": [
                    "Nat Neurosci"
                ],
                "issn": "1097-6256",
                "url": "http://www.nature.com/neuro/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2064658612",
                "DOI": "10.1038/nn.2996",
                "CorpusId": 52863226,
                "PubMed": "22231428"
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 273,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-01-08",
            "journal": {
                "name": "Nature Neuroscience",
                "volume": "15"
            },
            "citationStyles": {
                "bibtex": "@Article{Stoianov2012EmergenceOA,\n author = {I. Stoianov and M. Zorzi},\n booktitle = {Nature Neuroscience},\n journal = {Nature Neuroscience},\n pages = {194-196},\n title = {Emergence of a 'visual number sense' in hierarchical generative models},\n volume = {15},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:08d350a25720d865a52c00f3af6cb80e7af52d58",
            "@type": "ScholarlyArticle",
            "paperId": "08d350a25720d865a52c00f3af6cb80e7af52d58",
            "corpusId": 202774781,
            "url": "https://www.semanticscholar.org/paper/08d350a25720d865a52c00f3af6cb80e7af52d58",
            "title": "Time-series Generative Adversarial Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2970360512",
                "DBLP": "conf/nips/YoonJS19",
                "CorpusId": 202774781
            },
            "abstract": "A good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between variables across time. Existing methods that bring generative adversarial networks (GANs) into the sequential setting do not adequately attend to the temporal correlations unique to time-series data. At the same time, supervised models for sequence prediction - which allow finer control over network dynamics - are inherently deterministic. We propose a novel framework for generating realistic time-series data that combines the flexibility of the unsupervised paradigm with the control afforded by supervised training. Through a learned embedding space jointly optimized with both supervised and adversarial objectives, we encourage the network to adhere to the dynamics of the training data during sampling. Empirically, we evaluate the ability of our method to generate realistic samples using a variety of real and synthetic time-series datasets. Qualitatively and quantitatively, we find that the proposed framework consistently and significantly outperforms state-of-the-art benchmarks with respect to measures of similarity and predictive ability.",
            "referenceCount": 34,
            "citationCount": 483,
            "influentialCitationCount": 77,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yoon2019TimeseriesGA,\n author = {Jinsung Yoon and Daniel Jarrett and M. Schaar},\n booktitle = {Neural Information Processing Systems},\n pages = {5509-5519},\n title = {Time-series Generative Adversarial Networks},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:37e52ff4714c7a08900b518127e438a195b84611",
            "@type": "ScholarlyArticle",
            "paperId": "37e52ff4714c7a08900b518127e438a195b84611",
            "corpusId": 202777813,
            "url": "https://www.semanticscholar.org/paper/37e52ff4714c7a08900b518127e438a195b84611",
            "title": "MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1910.06711",
                "DBLP": "journals/corr/abs-1910-06711",
                "MAG": "2980709326",
                "CorpusId": 202777813
            },
            "abstract": "Previous works (Donahue et al., 2018a; Engel et al., 2019a) have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks.",
            "referenceCount": 52,
            "citationCount": 687,
            "influentialCitationCount": 91,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Engineering",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kumar2019MelGANGA,\n author = {Kundan Kumar and Rithesh Kumar and T. Boissi\u00e8re and L. Gestin and Wei Zhen Teoh and Jose M. R. Sotelo and A. D. Br\u00e9bisson and Yoshua Bengio and Aaron C. Courville},\n booktitle = {Neural Information Processing Systems},\n pages = {14881-14892},\n title = {MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:59a922212153d3407e658109f36c11a34ee7d283",
            "@type": "ScholarlyArticle",
            "paperId": "59a922212153d3407e658109f36c11a34ee7d283",
            "corpusId": 1888776,
            "url": "https://www.semanticscholar.org/paper/59a922212153d3407e658109f36c11a34ee7d283",
            "title": "Continual Learning with Deep Generative Replay",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2963559848",
                "DBLP": "conf/nips/ShinLKK17",
                "ArXiv": "1705.08690",
                "CorpusId": 1888776
            },
            "abstract": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\"generator\") and a task solving model (\"solver\"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.",
            "referenceCount": 36,
            "citationCount": 1458,
            "influentialCitationCount": 137,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Shin2017ContinualLW,\n author = {Hanul Shin and Jung Kwon Lee and Jaehong Kim and Jiwon Kim},\n booktitle = {Neural Information Processing Systems},\n pages = {2990-2999},\n title = {Continual Learning with Deep Generative Replay},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cb8cf0b6ba93526ce5b95962e8ebc8adb731b073",
            "@type": "ScholarlyArticle",
            "paperId": "cb8cf0b6ba93526ce5b95962e8ebc8adb731b073",
            "corpusId": 21531835,
            "url": "https://www.semanticscholar.org/paper/cb8cf0b6ba93526ce5b95962e8ebc8adb731b073",
            "title": "Online Signature Verification Based on Generative Models",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "journals/tsmc/RuaA12",
                "MAG": "2080936997",
                "DOI": "10.1109/TSMCB.2012.2188508",
                "CorpusId": 21531835,
                "PubMed": "22491089"
            },
            "abstract": "The success of generative models for online signature verification has motivated many research works on this topic. These systems may use hidden Markov models (HMMs) in two different modes: user-specific HMM (US-HMM) and user-adapted universal background models (UBMs) (UA-UBMs). Verification scores can be obtained from likelihood ratios and a distance measure on the Viterbi decoded state sequences. This paper analyzes several factors that can modify the behavior of these systems and which have not been deeply studied yet. First, we study the influence of the feature set choice, paying special attention to the role of dynamic information order, suitability of feature sets on each kind of generative model-based system, and the importance of inclination angles and pressure. Moreover, this analysis is also extended to the influence of the HMM complexity in the performance of the different approaches. For this study, a set of experiments is performed on the publicly available MCYT-100 database using only skilled forgeries. These experiments provide interesting outcomes. First, the Viterbi path evidences a notable stability for most of the feature sets and systems. Second, in the case of US-HMM systems, likelihood evidence obtains better results when lowest order dynamics are included in the feature set, while likelihood ratio obtains better results in UA-UBM systems when lowest dynamics are not included in the feature set. Finally, US-HMM and UA-UBM systems can be used together for improved verification performance by fusing at the score level the Viterbi path information from the US-HMM system and the likelihood ratio evidence from the UA-UBM system. Additional comparisons to other state-of-the-art systems, from the ESRA 2011 signature evaluation contest, are also reported, reinforcing the high performance of the systems and the generality of the experimental results described in this paper.",
            "referenceCount": 35,
            "citationCount": 68,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-08-01",
            "journal": {
                "name": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)",
                "volume": "42"
            },
            "citationStyles": {
                "bibtex": "@Article{Argones-R\u00faa2012OnlineSV,\n author = {Enrique Argones-R\u00faa and J. Alba-Castro},\n booktitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},\n pages = {1231-1242},\n title = {Online Signature Verification Based on Generative Models},\n volume = {42},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9fac2ad81a561398506ce566d897f7169583b4f8",
            "@type": "ScholarlyArticle",
            "paperId": "9fac2ad81a561398506ce566d897f7169583b4f8",
            "corpusId": 196831582,
            "url": "https://www.semanticscholar.org/paper/9fac2ad81a561398506ce566d897f7169583b4f8",
            "title": "On the \"steerability\" of generative adversarial networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2959108703",
                "ArXiv": "1907.07171",
                "DBLP": "journals/corr/abs-1907-07171",
                "CorpusId": 196831582
            },
            "abstract": "An open secret in contemporary machine learning is that many models work beautifully on standard benchmarks but fail to generalize outside the lab. This has been attributed to biased training data, which provide poor coverage over real world events. Generative models are no exception, but recent advances in generative adversarial networks (GANs) suggest otherwise - these models can now synthesize strikingly realistic and diverse images. Is generative modeling of photos a solved problem? We show that although current GANs can fit standard datasets very well, they still fall short of being comprehensive models of the visual manifold. In particular, we study their ability to fit simple transformations such as camera movements and color changes. We find that the models reflect the biases of the datasets on which they are trained (e.g., centered objects), but that they also exhibit some capacity for generalization: by \"steering\" in latent space, we can shift the distribution while still creating realistic images. We hypothesize that the degree of distributional shift is related to the breadth of the training data distribution. Thus, we conduct experiments to quantify the limits of GAN transformations and introduce techniques to mitigate the problem. Code is released on our project page: this https URL",
            "referenceCount": 34,
            "citationCount": 354,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.07171"
            },
            "citationStyles": {
                "bibtex": "@Article{Jahanian2019OnT,\n author = {Ali Jahanian and Lucy Chai and Phillip Isola},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {On the \"steerability\" of generative adversarial networks},\n volume = {abs/1907.07171},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76cee11c6a9f1424f03571378a966c1417ff2935",
            "@type": "ScholarlyArticle",
            "paperId": "76cee11c6a9f1424f03571378a966c1417ff2935",
            "corpusId": 3248075,
            "url": "https://www.semanticscholar.org/paper/76cee11c6a9f1424f03571378a966c1417ff2935",
            "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/0001ZXFT16",
                "MAG": "2546066744",
                "ArXiv": "1610.07584",
                "CorpusId": 3248075
            },
            "abstract": "We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.",
            "referenceCount": 49,
            "citationCount": 1695,
            "influentialCitationCount": 139,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-10-24",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wu2016LearningAP,\n author = {Jiajun Wu and Chengkai Zhang and Tianfan Xue and Bill Freeman and J. Tenenbaum},\n booktitle = {Neural Information Processing Systems},\n pages = {82-90},\n title = {Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:738336fb5f1aafee19a52ec7aa524ad8d8c7c7de",
            "@type": "ScholarlyArticle",
            "paperId": "738336fb5f1aafee19a52ec7aa524ad8d8c7c7de",
            "corpusId": 208139345,
            "url": "https://www.semanticscholar.org/paper/738336fb5f1aafee19a52ec7aa524ad8d8c7c7de",
            "title": "The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "3035616549",
                "ArXiv": "1911.07135",
                "DBLP": "journals/corr/abs-1911-07135",
                "DOI": "10.1109/cvpr42600.2020.00033",
                "CorpusId": 208139345
            },
            "abstract": "This paper studies model-inversion attacks, in which the access to a model is abused to infer information about the training data. Since its first introduction by~\\cite{fredrikson2014privacy}, such attacks have raised serious concerns given that training data usually contain privacy sensitive information. Thus far, successful model-inversion attacks have only been demonstrated on simple models, such as linear regression and logistic regression. Previous attempts to invert neural networks, even the ones with simple architectures, have failed to produce convincing results. Here we present a novel attack method, termed the \\emph{generative model-inversion attack}, which can invert deep neural networks with high success rates. Rather than reconstructing private training data from scratch, we leverage partial public information, which can be very generic, to learn a distributional prior via generative adversarial networks (GANs) and use it to guide the inversion process. Moreover, we theoretically prove that a model's predictive power and its vulnerability to inversion attacks are indeed two sides of the same coin---highly predictive models are able to establish a strong correlation between features and labels, which coincides exactly with what an adversary exploits to mount the attacks. Our extensive experiments demonstrate that the proposed attack improves identification accuracy over the existing work by about $75\\%$ for reconstructing face images from a state-of-the-art face recognition classifier. We also show that differential privacy, in its canonical form, is of little avail to defend against our attacks.",
            "referenceCount": 29,
            "citationCount": 270,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1911.07135",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-11-17",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2019TheSR,\n author = {Yuheng Zhang and R. Jia and Hengzhi Pei and Wenxiao Wang and Bo Li and D. Song},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {250-258},\n title = {The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:220ac48a22547a455d05f416e1fd22bbd0b0788d",
            "@type": "ScholarlyArticle",
            "paperId": "220ac48a22547a455d05f416e1fd22bbd0b0788d",
            "corpusId": 206595056,
            "url": "https://www.semanticscholar.org/paper/220ac48a22547a455d05f416e1fd22bbd0b0788d",
            "title": "Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/BousmalisSDEK16",
                "MAG": "2949212125",
                "ArXiv": "1612.05424",
                "DOI": "10.1109/CVPR.2017.18",
                "CorpusId": 206595056
            },
            "abstract": "Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",
            "referenceCount": 54,
            "citationCount": 1377,
            "influentialCitationCount": 102,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1612.05424",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-12-16",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bousmalis2016UnsupervisedPD,\n author = {Konstantinos Bousmalis and N. Silberman and David Dohan and D. Erhan and Dilip Krishnan},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {95-104},\n title = {Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:def1049b5aae96c8e1eab0ca58d77ac9c2f0e3e9",
            "@type": "ScholarlyArticle",
            "paperId": "def1049b5aae96c8e1eab0ca58d77ac9c2f0e3e9",
            "corpusId": 44100802,
            "url": "https://www.semanticscholar.org/paper/def1049b5aae96c8e1eab0ca58d77ac9c2f0e3e9",
            "title": "MolGAN: An implicit generative model for small molecular graphs",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2806351858",
                "ArXiv": "1805.11973",
                "DBLP": "journals/corr/abs-1805-11973",
                "CorpusId": 44100802
            },
            "abstract": "eep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is pos-sible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuris-tics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforce-ment learning objective to encourage the genera-tion of molecules with specific desired chemical properties. In experiments on the QM9 chemi-cal database, we demonstrate that our model is capable of generating close to 100% valid com-pounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, al-beit being susceptible to mode collapse.",
            "referenceCount": 48,
            "citationCount": 711,
            "influentialCitationCount": 63,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-05-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1805.11973"
            },
            "citationStyles": {
                "bibtex": "@Article{Cao2018MolGANAI,\n author = {Nicola De Cao and Thomas Kipf},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {MolGAN: An implicit generative model for small molecular graphs},\n volume = {abs/1805.11973},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:953811701447daa3f5e97a744dd3aca00f930ee1",
            "@type": "ScholarlyArticle",
            "paperId": "953811701447daa3f5e97a744dd3aca00f930ee1",
            "corpusId": 8718058,
            "url": "https://www.semanticscholar.org/paper/953811701447daa3f5e97a744dd3aca00f930ee1",
            "title": "Sparse Additive Generative Models of Text",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2165279024",
                "DBLP": "conf/icml/EisensteinAX11",
                "CorpusId": 8718058
            },
            "abstract": "Generative models of text typically associate a multinomial with every class label or topic. Even in simple models this requires the estimation of thousands of parameters; in multi-faceted latent variable models, standard approaches require additional latent \"switching\" variables for every token, complicating inference. In this paper, we propose an alternative generative model for text. The central idea is that each class label or latent topic is endowed with a model of the deviation in log-frequency from a constant background distribution. This approach has two key advantages: we can enforce sparsity to prevent overfitting, and we can combine generative facets through simple addition in log space, avoiding the need for latent switching variables. We demonstrate the applicability of this idea to a range of scenarios: classification, topic modeling, and more complex multifaceted generative models.",
            "referenceCount": 23,
            "citationCount": 348,
            "influentialCitationCount": 73,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-28",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Eisenstein2011SparseAG,\n author = {Jacob Eisenstein and Amr Ahmed and E. Xing},\n booktitle = {International Conference on Machine Learning},\n pages = {1041-1048},\n title = {Sparse Additive Generative Models of Text},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3efbcfeeb0ea1051a71101d3318da4411081f0b8",
            "@type": "ScholarlyArticle",
            "paperId": "3efbcfeeb0ea1051a71101d3318da4411081f0b8",
            "corpusId": 225094178,
            "url": "https://www.semanticscholar.org/paper/3efbcfeeb0ea1051a71101d3318da4411081f0b8",
            "title": "Scaling Laws for Autoregressive Generative Modeling",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2010-14701",
                "ArXiv": "2010.14701",
                "MAG": "3095645723",
                "CorpusId": 225094178
            },
            "abstract": "We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. \nThe cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\\mathrm{KL}}$) in nats/image for other resolutions. \nWe find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question \"Is a picture worth a thousand words?\"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.",
            "referenceCount": 36,
            "citationCount": 203,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-10-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2010.14701"
            },
            "citationStyles": {
                "bibtex": "@Article{Henighan2020ScalingLF,\n author = {T. Henighan and J. Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and S. Gray and Chris Hallacy and Benjamin Mann and Alec Radford and A. Ramesh and Nick Ryder and Daniel M. Ziegler and J. Schulman and Dario Amodei and Sam McCandlish},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Scaling Laws for Autoregressive Generative Modeling},\n volume = {abs/2010.14701},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0fba409648eb6087a1e7d741655967815b9f0377",
            "@type": "ScholarlyArticle",
            "paperId": "0fba409648eb6087a1e7d741655967815b9f0377",
            "corpusId": 88492544,
            "url": "https://www.semanticscholar.org/paper/0fba409648eb6087a1e7d741655967815b9f0377",
            "title": "Recent Progress on Generative Adversarial Networks (GANs): A Survey",
            "venue": "IEEE Access",
            "publicationVenue": {
                "id": "urn:research:2633f5b2-c15c-49fe-80f5-07523e770c26",
                "name": "IEEE Access",
                "alternate_names": null,
                "issn": "2169-3536",
                "url": "http://www.ieee.org/publications_standards/publications/ieee_access.html"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2921353139",
                "DBLP": "journals/access/PanYYKYZ19",
                "DOI": "10.1109/ACCESS.2019.2905015",
                "CorpusId": 88492544
            },
            "abstract": "Generative adversarial network (GANs) is one of the most important research avenues in the field of artificial intelligence, and its outstanding data generation capacity has received wide attention. In this paper, we present the recent progress on GANs. First, the basic theory of GANs and the differences among different generative models in recent years were analyzed and summarized. Then, the derived models of GANs are classified and introduced one by one. Third, the training tricks and evaluation metrics were given. Fourth, the applications of GANs were introduced. Finally, the problem, we need to address, and future directions were discussed.",
            "referenceCount": 79,
            "citationCount": 354,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ieeexplore.ieee.org/ielx7/6287639/8600701/08667290.pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-03-14",
            "journal": {
                "name": "IEEE Access",
                "volume": "7"
            },
            "citationStyles": {
                "bibtex": "@Article{Pan2019RecentPO,\n author = {Zhaoqing Pan and Weijie Yu and Xiaokai Yi and Asifullah Khan and Feng Yuan and Yuhui Zheng},\n booktitle = {IEEE Access},\n journal = {IEEE Access},\n pages = {36322-36333},\n title = {Recent Progress on Generative Adversarial Networks (GANs): A Survey},\n volume = {7},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b16492ec402d3d38b2d61de9c4ad37f03966ab9f",
            "@type": "ScholarlyArticle",
            "paperId": "b16492ec402d3d38b2d61de9c4ad37f03966ab9f",
            "corpusId": 211677799,
            "url": "https://www.semanticscholar.org/paper/b16492ec402d3d38b2d61de9c4ad37f03966ab9f",
            "title": "Permutation Invariant Graph Generation via Score-Based Generative Modeling",
            "venue": "International Conference on Artificial Intelligence and Statistics",
            "publicationVenue": {
                "id": "urn:research:2d136b11-c2b5-484b-b008-7f4a852fd61e",
                "name": "International Conference on Artificial Intelligence and Statistics",
                "alternate_names": [
                    "AISTATS",
                    "Int Conf Artif Intell Stat"
                ],
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2003-00638",
                "MAG": "3037879915",
                "ArXiv": "2003.00638",
                "CorpusId": 211677799
            },
            "abstract": "Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.",
            "referenceCount": 45,
            "citationCount": 125,
            "influentialCitationCount": 27,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Niu2020PermutationIG,\n author = {Chenhao Niu and Yang Song and Jiaming Song and Shengjia Zhao and Aditya Grover and Stefano Ermon},\n booktitle = {International Conference on Artificial Intelligence and Statistics},\n pages = {4474-4484},\n title = {Permutation Invariant Graph Generation via Score-Based Generative Modeling},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cda3fbbac6734b603bee363b0938e9baa924aa78",
            "@type": "ScholarlyArticle",
            "paperId": "cda3fbbac6734b603bee363b0938e9baa924aa78",
            "corpusId": 236950721,
            "url": "https://www.semanticscholar.org/paper/cda3fbbac6734b603bee363b0938e9baa924aa78",
            "title": "ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2021,
            "externalIds": {
                "DBLP": "journals/corr/abs-2108-02938",
                "ArXiv": "2108.02938",
                "DOI": "10.1109/iccv48922.2021.01410",
                "CorpusId": 236950721
            },
            "abstract": "Denoising diffusion probabilistic models (DDPM) have shown remarkable performance in unconditional image generation. However, due to the stochasticity of the generative process in DDPM, it is challenging to generate images with the desired semantics. In this work, we propose Iterative Latent Variable Refinement (ILVR), a method to guide the generative process in DDPM to generate high-quality images based on a given reference image. Here, the refinement of the generative process in DDPM enables a single DDPM to sample images from various sets directed by the reference image. The proposed ILVR method generates high-quality images while controlling the generation. The controllability of our method allows adaptation of a single DDPM without any additional learning in various image generation tasks, such as generation from various downsampling factors, multi-domain image translation, paint-to-image, and editing with scribbles.",
            "referenceCount": 59,
            "citationCount": 328,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2108.02938",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2021-08-06",
            "journal": {
                "name": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Choi2021ILVRCM,\n author = {Jooyoung Choi and Sungwon Kim and Yonghyun Jeong and Youngjune Gwon and Sungroh Yoon},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},\n pages = {14347-14356},\n title = {ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models},\n year = {2021}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2df45c89dfb41d611af08806d891cd1eaff472cb",
            "@type": "ScholarlyArticle",
            "paperId": "2df45c89dfb41d611af08806d891cd1eaff472cb",
            "corpusId": 3357865,
            "url": "https://www.semanticscholar.org/paper/2df45c89dfb41d611af08806d891cd1eaff472cb",
            "title": "Differentially Private Generative Adversarial Network",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "ArXiv": "1802.06739",
                "DBLP": "journals/corr/abs-1802-06739",
                "MAG": "2789226849",
                "CorpusId": 3357865
            },
            "abstract": "Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.",
            "referenceCount": 38,
            "citationCount": 369,
            "influentialCitationCount": 82,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-19",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.06739"
            },
            "citationStyles": {
                "bibtex": "@Article{Xie2018DifferentiallyPG,\n author = {Liyang Xie and Kaixiang Lin and Shu Wang and Fei Wang and Jiayu Zhou},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Differentially Private Generative Adversarial Network},\n volume = {abs/1802.06739},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d54d8c402785006faaf5de19e81f04eb484a3aa2",
            "@type": "ScholarlyArticle",
            "paperId": "d54d8c402785006faaf5de19e81f04eb484a3aa2",
            "corpusId": 219558812,
            "url": "https://www.semanticscholar.org/paper/d54d8c402785006faaf5de19e81f04eb484a3aa2",
            "title": "A Survey on Generative Adversarial Networks: Variants, Applications, and Training",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2006.05132",
                "DBLP": "journals/corr/abs-2006-05132",
                "MAG": "3034429116",
                "DOI": "10.1145/3463475",
                "CorpusId": 219558812
            },
            "abstract": "The Generative Models have gained considerable attention in unsupervised learning via a new and practical framework called Generative Adversarial Networks (GAN) due to their outstanding data generation capability. Many GAN models have been proposed, and several practical applications have emerged in various domains of computer vision and machine learning. Despite GANs excellent success, there are still obstacles to stable training. The problems are Nash equilibrium, internal covariate shift, mode collapse, vanishing gradient, and lack of proper evaluation metrics. Therefore, stable training is a crucial issue in different applications for the success of GANs. Herein, we survey several training solutions proposed by different researchers to stabilize GAN training. We discuss (I) the original GAN model and its modified versions, (II) a detailed analysis of various GAN applications in different domains, and (III) a detailed study about the various GAN training obstacles as well as training solutions. Finally, we reveal several issues as well as research outlines to the topic.",
            "referenceCount": 262,
            "citationCount": 150,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2006.05132",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-06-09",
            "journal": {
                "name": "ACM Computing Surveys (CSUR)",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Jabbar2020ASO,\n author = {Abdul Jabbar and Xi Li and Bourahla Omar},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys (CSUR)},\n pages = {1 - 49},\n title = {A Survey on Generative Adversarial Networks: Variants, Applications, and Training},\n volume = {54},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05ba96cc8ce02f065fc5bdf74e7907f5e13a9b03",
            "@type": "ScholarlyArticle",
            "paperId": "05ba96cc8ce02f065fc5bdf74e7907f5e13a9b03",
            "corpusId": 218470071,
            "url": "https://www.semanticscholar.org/paper/05ba96cc8ce02f065fc5bdf74e7907f5e13a9b03",
            "title": "Generative Adversarial Networks (GANs)",
            "venue": "ACM Computing Surveys",
            "publicationVenue": {
                "id": "urn:research:7b2adce0-d53f-49d6-8784-b0645604fe62",
                "name": "ACM Computing Surveys",
                "alternate_names": [
                    "ACM Comput Surv"
                ],
                "issn": "0360-0300",
                "url": "http://www.acm.org/pubs/surveys/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2005-00065",
                "MAG": "3020825954",
                "ArXiv": "2005.00065",
                "DOI": "10.1145/3446374",
                "CorpusId": 218470071
            },
            "abstract": "Generative Adversarial Networks (GANs) is a novel class of deep generative models that has recently gained significant attention. GANs learn complex and high-dimensional distributions implicitly over images, audio, and data. However, there exist major challenges in training of GANs, i.e., mode collapse, non-convergence, and instability, due to inappropriate design of network architectre, use of objective function, and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions, and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on the broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present promising research directions in this rapidly growing field.",
            "referenceCount": 256,
            "citationCount": 151,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://ira.lib.polyu.edu.hk/bitstream/10397/94420/1/COMP-0061_Saxena_Generative_Adversarial_Networks.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2020-04-30",
            "journal": {
                "name": "ACM Computing Surveys (CSUR)",
                "volume": "54"
            },
            "citationStyles": {
                "bibtex": "@Article{Saxena2020GenerativeAN,\n author = {Divya Saxena and Jiannong Cao},\n booktitle = {ACM Computing Surveys},\n journal = {ACM Computing Surveys (CSUR)},\n pages = {1 - 42},\n title = {Generative Adversarial Networks (GANs)},\n volume = {54},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7b1453416d3d95af19ff465104e78968920354d",
            "@type": "ScholarlyArticle",
            "paperId": "f7b1453416d3d95af19ff465104e78968920354d",
            "corpusId": 19140125,
            "url": "https://www.semanticscholar.org/paper/f7b1453416d3d95af19ff465104e78968920354d",
            "title": "GraphGAN: Graph Representation Learning with Generative Adversarial Nets",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1711-08267",
                "MAG": "2963169753",
                "ArXiv": "1711.08267",
                "DOI": "10.1609/aaai.v32i1.11872",
                "CorpusId": 19140125
            },
            "abstract": "\n \n The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces \"fake\" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.\n \n",
            "referenceCount": 35,
            "citationCount": 503,
            "influentialCitationCount": 72,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11872/11731",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.08267"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2017GraphGANGR,\n author = {Hongwei Wang and Jia Wang and Jialin Wang and Miao Zhao and Weinan Zhang and Fuzheng Zhang and Xing Xie and M. Guo},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {GraphGAN: Graph Representation Learning with Generative Adversarial Nets},\n volume = {abs/1711.08267},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:37a052144b510b8827634c38146b190d8b2c8d0b",
            "@type": "ScholarlyArticle",
            "paperId": "37a052144b510b8827634c38146b190d8b2c8d0b",
            "corpusId": 50786007,
            "url": "https://www.semanticscholar.org/paper/37a052144b510b8827634c38146b190d8b2c8d0b",
            "title": "Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks",
            "venue": "SASHIMI@MICCAI",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2884065486",
                "DBLP": "journals/corr/abs-1807-10225",
                "ArXiv": "1807.10225",
                "DOI": "10.1007/978-3-030-00536-8_1",
                "CorpusId": 50786007
            },
            "abstract": null,
            "referenceCount": 22,
            "citationCount": 444,
            "influentialCitationCount": 13,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1807.10225",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-26",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.10225"
            },
            "citationStyles": {
                "bibtex": "@Article{Shin2018MedicalIS,\n author = {Hoo-Chang Shin and Neil A. Tenenholtz and Jameson K. Rogers and C. Schwarz and M. Senjem and J. Gunter and K. Andriole and Mark H. Michalski},\n booktitle = {SASHIMI@MICCAI},\n journal = {ArXiv},\n title = {Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks},\n volume = {abs/1807.10225},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:84fae4c57e9f65459cf404866f83ff0b70fd7b75",
            "@type": "ScholarlyArticle",
            "paperId": "84fae4c57e9f65459cf404866f83ff0b70fd7b75",
            "corpusId": 211296328,
            "url": "https://www.semanticscholar.org/paper/84fae4c57e9f65459cf404866f83ff0b70fd7b75",
            "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/icml/NashGEB20",
                "MAG": "3008758407",
                "ArXiv": "2002.10880",
                "CorpusId": 211296328
            },
            "abstract": "Polygon meshes are an efficient representation of 3D geometry, and are of central importance in computer graphics, robotics and games development. Existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using alternative object representations that are more compatible with neural architectures and training approaches. We present an approach which models the mesh directly, predicting mesh vertices and faces sequentially using a Transformer-based architecture. Our model can condition on a range of inputs, including object classes, voxels, and images, and because the model is probabilistic it can produce samples that capture uncertainty in ambiguous scenarios. We show that the model is capable of producing high-quality, usable meshes, and establish log-likelihood benchmarks for the mesh-modelling task. We also evaluate the conditional models on surface reconstruction metrics against alternative methods, and demonstrate competitive performance despite not training directly on this task.",
            "referenceCount": 41,
            "citationCount": 152,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-02-23",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2002.10880"
            },
            "citationStyles": {
                "bibtex": "@Article{Nash2020PolyGenAA,\n author = {C. Nash and Yaroslav Ganin and A. Eslami and P. Battaglia},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {PolyGen: An Autoregressive Generative Model of 3D Meshes},\n volume = {abs/2002.10880},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
            "@type": "ScholarlyArticle",
            "paperId": "5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
            "corpusId": 221535103,
            "url": "https://www.semanticscholar.org/paper/5fe0a4af3bd1479d5e39fbda2215c86bce54722b",
            "title": "Generative Language Modeling for Automated Theorem Proving",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "journals/corr/abs-2009-03393",
                "MAG": "3083835029",
                "ArXiv": "2009.03393",
                "CorpusId": 221535103
            },
            "abstract": "We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.",
            "referenceCount": 48,
            "citationCount": 148,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-09-07",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2009.03393"
            },
            "citationStyles": {
                "bibtex": "@Article{Polu2020GenerativeLM,\n author = {Stanislas Polu and Ilya Sutskever},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generative Language Modeling for Automated Theorem Proving},\n volume = {abs/2009.03393},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a50b7a45f704f30d7f97dd229d4d53433d5df3b1",
            "@type": "ScholarlyArticle",
            "paperId": "a50b7a45f704f30d7f97dd229d4d53433d5df3b1",
            "corpusId": 198986015,
            "url": "https://www.semanticscholar.org/paper/a50b7a45f704f30d7f97dd229d4d53433d5df3b1",
            "title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/iclr/EngelckeKJP20",
                "ArXiv": "1907.13052",
                "MAG": "2994971263",
                "CorpusId": 198986015
            },
            "abstract": "Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.",
            "referenceCount": 59,
            "citationCount": 240,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1907.13052"
            },
            "citationStyles": {
                "bibtex": "@Article{Engelcke2019GENESISGS,\n author = {Martin Engelcke and Adam R. Kosiorek and Oiwi Parker Jones and I. Posner},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations},\n volume = {abs/1907.13052},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f93844c68d96f2f01da973b2ed3c236c8a369e57",
            "@type": "ScholarlyArticle",
            "paperId": "f93844c68d96f2f01da973b2ed3c236c8a369e57",
            "corpusId": 12196480,
            "url": "https://www.semanticscholar.org/paper/f93844c68d96f2f01da973b2ed3c236c8a369e57",
            "title": "On deep generative models with applications to recognition",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "1995997122",
                "DBLP": "conf/cvpr/RanzatoSMH11",
                "DOI": "10.1109/CVPR.2011.5995710",
                "CorpusId": 12196480
            },
            "abstract": "The most popular way to use probabilistic models in vision is first to extract some descriptors of small image patches or object parts using well-engineered features, and then to use statistical learning tools to model the dependencies among these features and eventual labels. Learning probabilistic models directly on the raw pixel values has proved to be much more difficult and is typically only used for regularizing discriminative methods. In this work, we use one of the best, pixel-level, generative models of natural images\u2013a gated MRF\u2013as the lowest level of a deep belief network (DBN) that has several hidden layers. We show that the resulting DBN is very good at coping with occlusion when predicting expression categories from face images, and it can produce features that perform comparably to SIFT descriptors for discriminating different types of scene. The generative ability of the model also makes it easy to see what information is captured and what is lost at each level of representation.",
            "referenceCount": 28,
            "citationCount": 225,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.cs.toronto.edu/%7Ehinton/absps/ranzato_cvpr2011.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-06-20",
            "journal": {
                "name": "CVPR 2011",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ranzato2011OnDG,\n author = {Marc'Aurelio Ranzato and J. Susskind and Volodymyr Mnih and Geoffrey E. Hinton},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {CVPR 2011},\n pages = {2857-2864},\n title = {On deep generative models with applications to recognition},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2015fdfa00d67f6cf75e880724428c8e5f3995b8",
            "@type": "ScholarlyArticle",
            "paperId": "2015fdfa00d67f6cf75e880724428c8e5f3995b8",
            "corpusId": 214795159,
            "url": "https://www.semanticscholar.org/paper/2015fdfa00d67f6cf75e880724428c8e5f3995b8",
            "title": "Generative Adversarial Networks for Crystal Structure Prediction",
            "venue": "ACS Central Science",
            "publicationVenue": {
                "id": "urn:research:df882f0f-d88c-4139-8462-219dcb05d97c",
                "name": "ACS Central Science",
                "alternate_names": [
                    "AC Central Sci",
                    "ACS central science",
                    "AC central sci"
                ],
                "issn": "2374-7943",
                "url": "https://pubs.acs.org/journal/acscii"
            },
            "year": 2020,
            "externalIds": {
                "PubMedCentral": "7453563",
                "MAG": "3041603413",
                "ArXiv": "2004.01396",
                "DOI": "10.1021/acscentsci.0c00426",
                "CorpusId": 214795159,
                "PubMed": "32875082"
            },
            "abstract": "The constant demand for novel functional materials calls for efficient strategies to accelerate the materials discovery, and crystal structure prediction is one of the most fundamental tasks along that direction. In addressing this challenge, generative models can offer new opportunities since they allow for the continuous navigation of chemical space via latent spaces. In this work, we employ a crystal representation that is inversion-free based on unit cell and fractional atomic coordinates and build a generative adversarial network for crystal structures. The proposed model is applied to generate the Mg\u2013Mn\u2013O ternary materials with the theoretical evaluation of their photoanode properties for high-throughput virtual screening (HTVS). The proposed generative HTVS framework predicts 23 new crystal structures with reasonable calculated stability and band gap. These findings suggest that the generative model can be an effective way to explore hidden portions of the chemical space, an area that is usually unreachable when conventional substitution-based discovery is employed.",
            "referenceCount": 79,
            "citationCount": 111,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Medicine",
                "Materials Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Materials Science",
                    "source": "external"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-04-03",
            "journal": {
                "name": "ACS Central Science",
                "volume": "6"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2020GenerativeAN,\n author = {Sungwon Kim and Juhwan Noh and Geun Ho Gu and A. Aspuru\u2010Guzik and Yousung Jung},\n booktitle = {ACS Central Science},\n journal = {ACS Central Science},\n pages = {1412 - 1420},\n title = {Generative Adversarial Networks for Crystal Structure Prediction},\n volume = {6},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:51bf7a3aee6b1f61b902625f6badffedf200d31a",
            "@type": "ScholarlyArticle",
            "paperId": "51bf7a3aee6b1f61b902625f6badffedf200d31a",
            "corpusId": 220871229,
            "url": "https://www.semanticscholar.org/paper/51bf7a3aee6b1f61b902625f6badffedf200d31a",
            "title": "Rewriting a Deep Generative Model",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2007.15646",
                "DBLP": "conf/eccv/BauLWZT20",
                "MAG": "3046090006",
                "DOI": "10.1007/978-3-030-58452-8_21",
                "CorpusId": 220871229
            },
            "abstract": null,
            "referenceCount": 89,
            "citationCount": 106,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/137596/2/2007.15646.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-07-30",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2007.15646"
            },
            "citationStyles": {
                "bibtex": "@Article{Bau2020RewritingAD,\n author = {David Bau and Steven Liu and Tongzhou Wang and Jun-Yan Zhu and A. Torralba},\n booktitle = {European Conference on Computer Vision},\n journal = {ArXiv},\n title = {Rewriting a Deep Generative Model},\n volume = {abs/2007.15646},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:698d3b667a7f3073eed8368d9daf84f990c24a65",
            "@type": "ScholarlyArticle",
            "paperId": "698d3b667a7f3073eed8368d9daf84f990c24a65",
            "corpusId": 23519254,
            "url": "https://www.semanticscholar.org/paper/698d3b667a7f3073eed8368d9daf84f990c24a65",
            "title": "Stabilizing Training of Generative Adversarial Networks through Regularization",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/RothLNH17",
                "ArXiv": "1705.09367",
                "MAG": "2617573776",
                "DOI": "10.3929/ETHZ-B-000223162",
                "CorpusId": 23519254
            },
            "abstract": "Deep generative models based on Generative Adversarial Networks (GANs) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters. This fragility is in part due to a dimensional mismatch or non-overlapping support between the model distribution and the data distribution, causing their density ratio and the associated f -divergence to be undefined. We overcome this fundamental limitation and propose a new regularization approach with low computational cost that yields a stable GAN training procedure. We demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image generation tasks. Our regularization turns GAN models into reliable building blocks for deep learning.",
            "referenceCount": 31,
            "citationCount": 403,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-05-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Roth2017StabilizingTO,\n author = {Kevin Roth and Aur\u00e9lien Lucchi and Sebastian Nowozin and Thomas Hofmann},\n booktitle = {Neural Information Processing Systems},\n pages = {2018-2028},\n title = {Stabilizing Training of Generative Adversarial Networks through Regularization},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd571fd71d3b4b87b7cd6453813169aaa72e0e30",
            "@type": "ScholarlyArticle",
            "paperId": "cd571fd71d3b4b87b7cd6453813169aaa72e0e30",
            "corpusId": 8989909,
            "url": "https://www.semanticscholar.org/paper/cd571fd71d3b4b87b7cd6453813169aaa72e0e30",
            "title": "Learning generative models for protein fold families",
            "venue": "Proteins: Structure, Function, and Bioinformatics",
            "publicationVenue": {
                "id": "urn:research:8baa4515-fac4-4b34-be0d-49385d586277",
                "name": "Proteins: Structure, Function, and Bioinformatics",
                "alternate_names": [
                    "Protein Struct Funct Bioinform",
                    "Proteins"
                ],
                "issn": "0887-3585",
                "url": "http://eu.wiley.com/WileyCDA/WileyTitle/productCd-PROT.html"
            },
            "year": 2011,
            "externalIds": {
                "MAG": "2169478909",
                "DOI": "10.1002/prot.22934",
                "CorpusId": 8989909,
                "PubMed": "21268112"
            },
            "abstract": "We introduce a new approach to learning statistical models from multiple sequence alignments (MSA) of proteins. Our method, called GREMLIN (Generative REgularized ModeLs of proteINs), learns an undirected probabilistic graphical model of the amino acid composition within the MSA. The resulting model encodes both the position\u2010specific conservation statistics and the correlated mutation statistics between sequential and long\u2010range pairs of residues. Existing techniques for learning graphical models from MSA either make strong, and often inappropriate assumptions about the conditional independencies within the MSA (e.g., Hidden Markov Models), or else use suboptimal algorithms to learn the parameters of the model. In contrast, GREMLIN makes no a priori assumptions about the conditional independencies within the MSA. We formulate and solve a convex optimization problem, thus guaranteeing that we find a globally optimal model at convergence. The resulting model is also generative, allowing for the design of new protein sequences that have the same statistical properties as those in the MSA. We perform a detailed analysis of covariation statistics on the extensively studied WW and PDZ domains and show that our method out\u2010performs an existing algorithm for learning undirected probabilistic graphical models from MSA. We then apply our approach to 71 additional families from the PFAM database and demonstrate that the resulting models significantly out\u2010perform Hidden Markov Models in terms of predictive accuracy. Proteins 2011; \u00a9 2011 Wiley\u2010Liss, Inc.",
            "referenceCount": 50,
            "citationCount": 310,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/prot.22934",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-04-01",
            "journal": {
                "name": "Proteins: Structure",
                "volume": "79"
            },
            "citationStyles": {
                "bibtex": "@Article{Balakrishnan2011LearningGM,\n author = {Sivaraman Balakrishnan and Hetunandan Kamisetty and J. Carbonell and Su-In Lee and C. Langmead},\n booktitle = {Proteins: Structure, Function, and Bioinformatics},\n journal = {Proteins: Structure},\n title = {Learning generative models for protein fold families},\n volume = {79},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:15168665f4b8eb11466086e69780ed98e5280059",
            "@type": "ScholarlyArticle",
            "paperId": "15168665f4b8eb11466086e69780ed98e5280059",
            "corpusId": 4547917,
            "url": "https://www.semanticscholar.org/paper/15168665f4b8eb11466086e69780ed98e5280059",
            "title": "Generate to Adapt: Aligning Domains Using Generative Adversarial Networks",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/Sankaranarayanan17a",
                "ArXiv": "1704.01705",
                "MAG": "2950634055",
                "DOI": "10.1109/CVPR.2018.00887",
                "CorpusId": 4547917
            },
            "abstract": "Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.",
            "referenceCount": 44,
            "citationCount": 602,
            "influentialCitationCount": 67,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1704.01705",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-06",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Sankaranarayanan2017GenerateTA,\n author = {S. Sankaranarayanan and Y. Balaji and C. Castillo and R. Chellappa},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {8503-8512},\n title = {Generate to Adapt: Aligning Domains Using Generative Adversarial Networks},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:df7ad8eeb595da5f7774e91dae06075be952acff",
            "@type": "ScholarlyArticle",
            "paperId": "df7ad8eeb595da5f7774e91dae06075be952acff",
            "corpusId": 53729760,
            "url": "https://www.semanticscholar.org/paper/df7ad8eeb595da5f7774e91dae06075be952acff",
            "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2964074081",
                "DBLP": "journals/corr/abs-1811-10597",
                "ArXiv": "1811.10597",
                "CorpusId": 53729760
            },
            "abstract": "Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. \nIn this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.",
            "referenceCount": 42,
            "citationCount": 390,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1811.10597"
            },
            "citationStyles": {
                "bibtex": "@Article{Bau2018GANDV,\n author = {David Bau and Jun-Yan Zhu and Hendrik Strobelt and Bolei Zhou and J. Tenenbaum and W. Freeman and A. Torralba},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {GAN Dissection: Visualizing and Understanding Generative Adversarial Networks},\n volume = {abs/1811.10597},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4463fe262306eaac336fa5cae38e98811bffa80",
            "@type": "ScholarlyArticle",
            "paperId": "d4463fe262306eaac336fa5cae38e98811bffa80",
            "corpusId": 59606217,
            "url": "https://www.semanticscholar.org/paper/d4463fe262306eaac336fa5cae38e98811bffa80",
            "title": "BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1902.02102",
                "MAG": "2970014727",
                "DBLP": "journals/corr/abs-1902-02102",
                "CorpusId": 59606217
            },
            "abstract": "With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.",
            "referenceCount": 64,
            "citationCount": 184,
            "influentialCitationCount": 32,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-06",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Maal\u00f8e2019BIVAAV,\n author = {Lars Maal\u00f8e and Marco Fraccaro and Valentin Li\u00e9vin and O. Winther},\n booktitle = {Neural Information Processing Systems},\n pages = {6548-6558},\n title = {BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2904a9932f4cd0f0886121dc1f2d4aaac0455176",
            "@type": "ScholarlyArticle",
            "paperId": "2904a9932f4cd0f0886121dc1f2d4aaac0455176",
            "corpusId": 536962,
            "url": "https://www.semanticscholar.org/paper/2904a9932f4cd0f0886121dc1f2d4aaac0455176",
            "title": "Generative Moment Matching Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2015,
            "externalIds": {
                "ArXiv": "1502.02761",
                "DBLP": "journals/corr/LiSZ15",
                "MAG": "1487641199",
                "CorpusId": 536962
            },
            "abstract": "We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer perceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.",
            "referenceCount": 51,
            "citationCount": 766,
            "influentialCitationCount": 108,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-02-09",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2015GenerativeMM,\n author = {Yujia Li and Kevin Swersky and R. Zemel},\n booktitle = {International Conference on Machine Learning},\n pages = {1718-1727},\n title = {Generative Moment Matching Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:803364ad73f938d0086c71b4a50210560a581170",
            "@type": "ScholarlyArticle",
            "paperId": "803364ad73f938d0086c71b4a50210560a581170",
            "corpusId": 226832,
            "url": "https://www.semanticscholar.org/paper/803364ad73f938d0086c71b4a50210560a581170",
            "title": "Structured Relation Discovery using Generative Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2011,
            "externalIds": {
                "ACL": "D11-1135",
                "MAG": "2739400675",
                "DBLP": "conf/emnlp/YaoHRM11",
                "CorpusId": 226832
            },
            "abstract": "We explore unsupervised approaches to relation extraction between two named entities; for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.",
            "referenceCount": 31,
            "citationCount": 164,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-07-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yao2011StructuredRD,\n author = {Limin Yao and A. Haghighi and S. Riedel and A. McCallum},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {1456-1466},\n title = {Structured Relation Discovery using Generative Models},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29831b8830e278c8c28e45c8e9c41c619c89f86a",
            "@type": "ScholarlyArticle",
            "paperId": "29831b8830e278c8c28e45c8e9c41c619c89f86a",
            "corpusId": 605416,
            "url": "https://www.semanticscholar.org/paper/29831b8830e278c8c28e45c8e9c41c619c89f86a",
            "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/icml/MeschederNG17",
                "MAG": "2579277680",
                "ArXiv": "1701.04722",
                "CorpusId": 605416
            },
            "abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.",
            "referenceCount": 46,
            "citationCount": 481,
            "influentialCitationCount": 65,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-01-17",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1701.04722"
            },
            "citationStyles": {
                "bibtex": "@Article{Mescheder2017AdversarialVB,\n author = {L. Mescheder and Sebastian Nowozin and Andreas Geiger},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},\n volume = {abs/1701.04722},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c791202344a72b50e5dba6f35cc830e29afe7e63",
            "@type": "ScholarlyArticle",
            "paperId": "c791202344a72b50e5dba6f35cc830e29afe7e63",
            "corpusId": 46945616,
            "url": "https://www.semanticscholar.org/paper/c791202344a72b50e5dba6f35cc830e29afe7e63",
            "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations",
            "venue": "Computer graphics forum (Print)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2797074791",
                "DBLP": "journals/corr/abs-1806-02071",
                "ArXiv": "1806.02071",
                "DOI": "10.1111/cgf.13619",
                "CorpusId": 46945616
            },
            "abstract": "This paper presents a novel generative model to synthesize fluid simulations from a set of reduced parameters. A convolutional neural network is trained on a collection of discrete, parameterizable fluid simulation velocity fields. Due to the capability of deep learning architectures to learn representative features of the data, our generative model is able to accurately approximate the training data set, while providing plausible interpolated in\u2010betweens. The proposed generative model is optimized for fluids by a novel loss function that guarantees divergence\u2010free velocity fields at all times. In addition, we demonstrate that we can handle complex parameterizations in reduced spaces, and advance simulations in time by integrating in the latent space with a second network. Our method models a wide variety of fluid behaviors, thus enabling applications such as fast construction of simulations, interpolation of fluids with different parameters, time re\u2010sampling, latent space simulations, and compression of fluid simulation data. Reconstructed velocity fields are generated up to 700\u00d7 faster than re\u2010simulating the data with the underlying CPU solver, while achieving compression rates of up to 1300\u00d7.",
            "referenceCount": 63,
            "citationCount": 336,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1806.02071",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-06",
            "journal": {
                "name": "Computer Graphics Forum",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2018DeepFA,\n author = {Byungsoo Kim and V. C. Azevedo and N. Th\u00fcrey and Theodore Kim and M. Gross and B. Solenthaler},\n booktitle = {Computer graphics forum (Print)},\n journal = {Computer Graphics Forum},\n title = {Deep Fluids: A Generative Network for Parameterized Fluid Simulations},\n volume = {38},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
            "@type": "ScholarlyArticle",
            "paperId": "3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
            "corpusId": 227127234,
            "url": "https://www.semanticscholar.org/paper/3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
            "title": "Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2020,
            "externalIds": {
                "DBLP": "conf/iclr/Child21",
                "ArXiv": "2011.10650",
                "MAG": "3106570356",
                "CorpusId": 227127234
            },
            "abstract": "We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains the performance gap by by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.",
            "referenceCount": 41,
            "citationCount": 240,
            "influentialCitationCount": 58,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-11-20",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2011.10650"
            },
            "citationStyles": {
                "bibtex": "@Article{Child2020VeryDV,\n author = {Rewon Child},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images},\n volume = {abs/2011.10650},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f708482e99ed40a2fb9e363577479e1fd15afc9f",
            "@type": "ScholarlyArticle",
            "paperId": "f708482e99ed40a2fb9e363577479e1fd15afc9f",
            "corpusId": 2546662,
            "url": "https://www.semanticscholar.org/paper/f708482e99ed40a2fb9e363577479e1fd15afc9f",
            "title": "Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/ijcai/JiangZTTZ17",
                "MAG": "2952006246",
                "ArXiv": "1611.05148",
                "DOI": "10.24963/ijcai.2017/273",
                "CorpusId": 2546662
            },
            "abstract": "Clustering is among the most fundamental tasks in machine learning and artificial intelligence. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into an observable. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using the Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 5 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training.",
            "referenceCount": 58,
            "citationCount": 560,
            "influentialCitationCount": 109,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.ijcai.org/proceedings/2017/0273.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-16",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Jiang2016VariationalDE,\n author = {Zhuxi Jiang and Yin Zheng and Huachun Tan and Bangsheng Tang and Hanning Zhou},\n booktitle = {International Joint Conference on Artificial Intelligence},\n pages = {1965-1972},\n title = {Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3b1115226eb73af88681fc2dac9bd35ff01c8991",
            "@type": "ScholarlyArticle",
            "paperId": "3b1115226eb73af88681fc2dac9bd35ff01c8991",
            "corpusId": 206687484,
            "url": "https://www.semanticscholar.org/paper/3b1115226eb73af88681fc2dac9bd35ff01c8991",
            "title": "druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico.",
            "venue": "Molecular Pharmaceutics",
            "publicationVenue": {
                "id": "urn:research:bfba025f-b9ce-44b1-a200-eec39dd09964",
                "name": "Molecular Pharmaceutics",
                "alternate_names": [
                    "Mol Pharm"
                ],
                "issn": "1543-8384",
                "url": "https://pubs.acs.org/journal/mpohbp"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2736137960",
                "DOI": "10.1021/acs.molpharmaceut.7b00346",
                "CorpusId": 206687484,
                "PubMed": "28703000"
            },
            "abstract": "Deep generative adversarial networks (GANs) are the emerging technology in drug discovery and biomarker development. In our recent work, we demonstrated a proof-of-concept of implementing deep generative adversarial autoencoder (AAE) to identify new molecular fingerprints with predefined anticancer properties. Another popular generative model is the variational autoencoder (VAE), which is based on deep neural architectures. In this work, we developed an advanced AAE model for molecular feature extraction problems, and demonstrated its advantages compared to VAE in terms of (a) adjustability in generating molecular fingerprints; (b) capacity of processing very large molecular data sets; and (c) efficiency in unsupervised pretraining for regression model. Our results suggest that the proposed AAE model significantly enhances the capacity and efficiency of development of the new molecules with specific anticancer properties using the deep generative models.",
            "referenceCount": 28,
            "citationCount": 388,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-08-04",
            "journal": {
                "name": "Molecular pharmaceutics",
                "volume": "14 9"
            },
            "citationStyles": {
                "bibtex": "@Article{Kadurin2017druGANAA,\n author = {Artur Kadurin and S. Nikolenko and Kuzma Khrabrov and A. Aliper and A. Zhavoronkov},\n booktitle = {Molecular Pharmaceutics},\n journal = {Molecular pharmaceutics},\n pages = {\n          3098-3104\n        },\n title = {druGAN: An Advanced Generative Adversarial Autoencoder Model for de Novo Generation of New Molecules with Desired Molecular Properties in Silico.},\n volume = {14 9},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d5381bfb45abf0f92a7344deb9d530963fa91408",
            "@type": "ScholarlyArticle",
            "paperId": "d5381bfb45abf0f92a7344deb9d530963fa91408",
            "corpusId": 209386419,
            "url": "https://www.semanticscholar.org/paper/d5381bfb45abf0f92a7344deb9d530963fa91408",
            "title": "Learning to Dress 3D People in Generative Clothing",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2995807908",
                "DBLP": "conf/cvpr/MaYRPPTB20",
                "DOI": "10.1109/cvpr42600.2020.00650",
                "CorpusId": 209386419
            },
            "abstract": "Three-dimensional human body models are widely used in the analysis of human pose and motion. Existing models, however, are learned from minimally-clothed 3D scans and thus do not generalize to the complexity of dressed people in common images and videos. Additionally, current models lack the expressive power needed to represent the complex non-linear geometry of pose-dependent clothing shapes. To address this, we learn a generative 3D mesh model of clothed people from 3D scans with varying pose and clothing. Specifically, we train a conditional Mesh-VAE-GAN to learn the clothing deformation from the SMPL body model, making clothing an additional term in SMPL. Our model is conditioned on both pose and clothing type, giving the ability to draw samples of clothing to dress different body shapes in a variety of styles and poses. To preserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to 3D meshes. Our model, named CAPE, represents global shape and fine local structure, effectively extending the SMPL body model to clothing. To our knowledge, this is the first generative model that directly dresses 3D human body meshes and generalizes to different poses. The model, code and data are available for research purposes at https://cape.is.tue.mpg.de.",
            "referenceCount": 97,
            "citationCount": 236,
            "influentialCitationCount": 34,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.research-collection.ethz.ch/bitstream/20.500.11850/466890/1/CAPE_paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-07-31",
            "journal": {
                "name": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ma2019LearningTD,\n author = {Qianli Ma and Jinlong Yang and Anurag Ranjan and S. Pujades and Gerard Pons-Moll and Siyu Tang and Michael J. Black},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {6468-6477},\n title = {Learning to Dress 3D People in Generative Clothing},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f83ef3250ba1166d7c1c7585da7dd78e0641fae7",
            "@type": "ScholarlyArticle",
            "paperId": "f83ef3250ba1166d7c1c7585da7dd78e0641fae7",
            "corpusId": 19098155,
            "url": "https://www.semanticscholar.org/paper/f83ef3250ba1166d7c1c7585da7dd78e0641fae7",
            "title": "MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/aaai/DongHYY18",
                "MAG": "2952928149",
                "ArXiv": "1709.06298",
                "DOI": "10.1609/aaai.v32i1.11312",
                "CorpusId": 19098155
            },
            "abstract": "\n \n Generating music has a few notable differences from generating images and videos. First, music is an art of time, necessitating a temporal model. Second, music is usually composed of multiple instruments/tracks with their own temporal dynamics, but collectively they unfold over time interdependently. Lastly, musical notes are often grouped into chords, arpeggios or melodies in polyphonic music, and thereby introducing a chronological ordering of notes is not naturally suitable. In this paper, we propose three models for symbolic multi-track music generation under the framework of generative adversarial networks (GANs). The three models, which differ in the underlying assumptions and accordingly the network architectures, are referred to as the jamming model, the composer model and the hybrid model. We trained the proposed models on a dataset of over one hundred thousand bars of rock music and applied them to generate piano-rolls of five tracks: bass, drums, guitar, piano and strings. A few intra-track and inter-track objective metrics are also proposed to evaluate the generative results, in addition to a subjective user study. We show that our models can generate coherent music of four bars right from scratch (i.e. without human inputs). We also extend our models to human-AI cooperative music generation: given a specific track composed by human, we can generate four additional tracks to accompany it. All code, the dataset and the rendered audio samples are available at https://salu133445.github.io/musegan/.\n \n",
            "referenceCount": 35,
            "citationCount": 414,
            "influentialCitationCount": 49,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11312/11171",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2017MuseGANMS,\n author = {Hao-Wen Dong and Wen-Yi Hsiao and Li-Chia Yang and Yi-Hsuan Yang},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {34-41},\n title = {MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50",
            "@type": "ScholarlyArticle",
            "paperId": "05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50",
            "corpusId": 16021391,
            "url": "https://www.semanticscholar.org/paper/05c6b2a59b021f2d5e5a580ded1681f8a1ae2a50",
            "title": "Discovering Binary Codes for Documents by Learning Deep Generative Models",
            "venue": "Topics in Cognitive Science",
            "publicationVenue": {
                "id": "urn:research:c2da8960-5aa5-430f-938c-2c2c811e5c96",
                "name": "Topics in Cognitive Science",
                "alternate_names": [
                    "Top Cogn Sci"
                ],
                "issn": "1756-8757",
                "url": "http://www3.interscience.wiley.com/journal/121673067/toc"
            },
            "year": 2011,
            "externalIds": {
                "DBLP": "journals/topics/HintonS11",
                "MAG": "2138806976",
                "DOI": "10.1111/j.1756-8765.2010.01109.x",
                "CorpusId": 16021391,
                "PubMed": "25164175"
            },
            "abstract": "We describe a deep generative model in which the lowest layer represents the word-count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top-down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF-IDF we achieve higher accuracy than TF-IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.",
            "referenceCount": 27,
            "citationCount": 114,
            "influentialCitationCount": 7,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": null,
            "journal": {
                "name": "Topics in cognitive science",
                "volume": "3 1"
            },
            "citationStyles": {
                "bibtex": "@Article{Hinton2011DiscoveringBC,\n author = {Geoffrey E. Hinton and R. Salakhutdinov},\n booktitle = {Topics in Cognitive Science},\n journal = {Topics in cognitive science},\n pages = {\n          74-91\n        },\n title = {Discovering Binary Codes for Documents by Learning Deep Generative Models},\n volume = {3 1},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:86ee1835a56722b76564119437070782fc90eb19",
            "@type": "ScholarlyArticle",
            "paperId": "86ee1835a56722b76564119437070782fc90eb19",
            "corpusId": 261560300,
            "url": "https://www.semanticscholar.org/paper/86ee1835a56722b76564119437070782fc90eb19",
            "title": "Generative Adversarial Nets",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "conf/nips/GoodfellowPMXWOCB14",
                "ArXiv": "1406.2661",
                "MAG": "2099471712",
                "CorpusId": 261560300
            },
            "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to \u00bd everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
            "referenceCount": 35,
            "citationCount": 697,
            "influentialCitationCount": 135,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-06-10",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Goodfellow2014GenerativeAN,\n author = {I. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},\n booktitle = {Neural Information Processing Systems},\n pages = {2672-2680},\n title = {Generative Adversarial Nets},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fa6ba95b8383fad600bcbd6033c6eec73296381",
            "@type": "ScholarlyArticle",
            "paperId": "1fa6ba95b8383fad600bcbd6033c6eec73296381",
            "corpusId": 2002865,
            "url": "https://www.semanticscholar.org/paper/1fa6ba95b8383fad600bcbd6033c6eec73296381",
            "title": "MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation",
            "venue": "International Society for Music Information Retrieval Conference",
            "publicationVenue": {
                "id": "urn:research:cfc287e4-4c04-4848-ab16-633b33a61a09",
                "name": "International Society for Music Information Retrieval Conference",
                "alternate_names": [
                    "International Symposium/Conference on Music Information Retrieval",
                    "ISMIR",
                    "Int Soc Music Inf Retr Conf",
                    "Int Symp Music Inf Retr"
                ],
                "issn": null,
                "url": "http://www.ismir.net/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2746068898",
                "DBLP": "journals/corr/YangCY17",
                "ArXiv": "1703.10847",
                "CorpusId": 2002865
            },
            "abstract": "Most existing neural network models for music generation use recurrent neural networks. However, the recent WaveNet model proposed by DeepMind shows that convolutional neural networks (CNNs) can also generate realistic musical waveforms in the audio domain. Following this light, we investigate using CNNs for generating melody (a series of MIDI notes) one bar after another in the symbolic domain. In addition to the generator, we use a discriminator to learn the distributions of melodies, making it a generative adversarial network (GAN). Moreover, we propose a novel conditional mechanism to exploit available prior knowledge, so that the model can generate melodies either from scratch, by following a chord sequence, or by conditioning on the melody of previous bars (e.g. a priming melody), among other possibilities. The resulting model, named MidiNet, can be expanded to generate music with multiple MIDI channels (i.e. tracks). We conduct a user study to compare the melody of eight-bar long generated by MidiNet and by Google's MelodyRNN models, each time using the same priming melody. Result shows that MidiNet performs comparably with MelodyRNN models in being realistic and pleasant to listen to, yet MidiNet's melodies are reported to be much more interesting.",
            "referenceCount": 40,
            "citationCount": 391,
            "influentialCitationCount": 29,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.10847"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2017MidiNetAC,\n author = {Li-Chia Yang and Szu-Yu Chou and Yi-Hsuan Yang},\n booktitle = {International Society for Music Information Retrieval Conference},\n journal = {ArXiv},\n title = {MidiNet: A Convolutional Generative Adversarial Network for Symbolic-Domain Music Generation},\n volume = {abs/1703.10847},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:024d30897e0a2b036bc122163a954b7f1a1d0679",
            "@type": "ScholarlyArticle",
            "paperId": "024d30897e0a2b036bc122163a954b7f1a1d0679",
            "corpusId": 13002849,
            "url": "https://www.semanticscholar.org/paper/024d30897e0a2b036bc122163a954b7f1a1d0679",
            "title": "Mode Regularized Generative Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2585630030",
                "ArXiv": "1612.02136",
                "DBLP": "journals/corr/CheLJBL16",
                "CorpusId": 13002849
            },
            "abstract": "Although Generative Adversarial Networks achieve state-of-the-art results on a variety of generative tasks, they are regarded as highly unstable and prone to miss modes. We argue that these bad behaviors of GANs are due to the very particular functional shape of the trained discriminators in high dimensional spaces, which can easily make training stuck or push probability mass in the wrong direction, towards that of higher concentration than that of the data generating distribution. We introduce several ways of regularizing the objective, which can dramatically stabilize the training of GAN models. We also show that our regularizers can help the fair distribution of probability mass across the modes of the data generating distribution, during the early phases of training and thus providing a unified solution to the missing modes problem.",
            "referenceCount": 28,
            "citationCount": 494,
            "influentialCitationCount": 51,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-05",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1612.02136"
            },
            "citationStyles": {
                "bibtex": "@Article{Che2016ModeRG,\n author = {Tong Che and Yanran Li and Athul Paul Jacob and Yoshua Bengio and Wenjie Li},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Mode Regularized Generative Adversarial Networks},\n volume = {abs/1612.02136},\n year = {2016}\n}\n"
            }
        }
    }
]