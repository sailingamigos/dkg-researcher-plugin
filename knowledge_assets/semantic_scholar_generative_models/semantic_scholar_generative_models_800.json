[
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a4501315f04f11b21db9a6070f0fd5f5c469e86c",
            "@type": "ScholarlyArticle",
            "paperId": "a4501315f04f11b21db9a6070f0fd5f5c469e86c",
            "corpusId": 196834740,
            "url": "https://www.semanticscholar.org/paper/a4501315f04f11b21db9a6070f0fd5f5c469e86c",
            "title": "Content-aware generative modeling of graphic design layouts",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2960053204",
                "DBLP": "journals/tog/ZhengQCL19",
                "DOI": "10.1145/3306346.3322971",
                "CorpusId": 196834740
            },
            "abstract": "Layout is fundamental to graphic designs. For visual attractiveness and efficient communication of messages and ideas, graphic design layouts often have great variation, driven by the contents to be presented. In this paper, we study the problem of content-aware graphic design layout generation. We propose a deep generative model for graphic design layouts that is able to synthesize layout designs based on the visual and textual semantics of user inputs. Unlike previous approaches that are oblivious to the input contents and rely on heuristic criteria, our model captures the effect of visual and textual contents on layouts, and implicitly learns complex layout structure variations from data without the use of any heuristic rules. To train our model, we build a large-scale magazine layout dataset with fine-grained layout annotations and keyword labeling. Experimental results show that our model can synthesize high-quality layouts based on the visual semantics of input images and keyword-based summary of input text. We also demonstrate that our model internally learns powerful features that capture the subtle interaction between contents and layouts, which are useful for layout-aware design retrieval.",
            "referenceCount": 48,
            "citationCount": 127,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-07-12",
            "journal": {
                "name": "ACM Transactions on Graphics (TOG)",
                "volume": "38"
            },
            "citationStyles": {
                "bibtex": "@Article{Zheng2019ContentawareGM,\n author = {Xinru Zheng and Xiaotian Qiao and Ying Cao and Rynson W. H. Lau},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 15},\n title = {Content-aware generative modeling of graphic design layouts},\n volume = {38},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:30d092f82e772c0597418eddef9beb0f1cc46327",
            "@type": "ScholarlyArticle",
            "paperId": "30d092f82e772c0597418eddef9beb0f1cc46327",
            "corpusId": 202774460,
            "url": "https://www.semanticscholar.org/paper/30d092f82e772c0597418eddef9beb0f1cc46327",
            "title": "Defending Neural Backdoors via Generative Distribution Modeling",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "ArXiv": "1910.04749",
                "MAG": "2970200861",
                "DBLP": "journals/corr/abs-1910-04749",
                "CorpusId": 202774460
            },
            "abstract": "Neural backdoor attack is emerging as a severe security threat to deep learning, while the capability of existing defense methods is limited, especially for complex backdoor triggers. In the work, we explore the space formed by the pixel values of all possible backdoor triggers. An original trigger used by an attacker to build the backdoored model represents only a point in the space. It then will be generalized into a distribution of valid triggers, all of which can influence the backdoored model. Thus, previous methods that model only one point of the trigger distribution is not sufficient. Getting the entire trigger distribution, e.g., via generative modeling, is a key of effective defense. However, existing generative modeling techniques for image generation are not applicable to the backdoor scenario as the trigger distribution is completely unknown. In this work, we propose max-entropy staircase approximator (MESA) for high-dimensional sampling-free generative modeling and use it to recover the trigger distribution. We also develop a defense technique to remove the triggers from the backdoored model. Our experiments on Cifar10/100 dataset demonstrate the effectiveness of MESA in modeling the trigger distribution and the robustness of the proposed defense method.",
            "referenceCount": 25,
            "citationCount": 132,
            "influentialCitationCount": 18,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-10-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Qiao2019DefendingNB,\n author = {Ximing Qiao and Yukun Yang and H. Li},\n booktitle = {Neural Information Processing Systems},\n pages = {14004-14013},\n title = {Defending Neural Backdoors via Generative Distribution Modeling},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67e4c44b8b7cae11ed48d5cb241c6e915eeefe5a",
            "@type": "ScholarlyArticle",
            "paperId": "67e4c44b8b7cae11ed48d5cb241c6e915eeefe5a",
            "corpusId": 146783064,
            "url": "https://www.semanticscholar.org/paper/67e4c44b8b7cae11ed48d5cb241c6e915eeefe5a",
            "title": "Eight Ways to Promote Generative Learning",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2173798076",
                "DOI": "10.1007/S10648-015-9348-9",
                "CorpusId": 146783064
            },
            "abstract": null,
            "referenceCount": 149,
            "citationCount": 410,
            "influentialCitationCount": 26,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "2016-12-01",
            "journal": {
                "name": "Educational Psychology Review",
                "volume": "28"
            },
            "citationStyles": {
                "bibtex": "@Article{Fiorella2016EightWT,\n author = {Logan Fiorella and R. Mayer},\n journal = {Educational Psychology Review},\n pages = {717-741},\n title = {Eight Ways to Promote Generative Learning},\n volume = {28},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18ffa15d72a9c2e1be3cfce66b69bbc8114f8ef0",
            "@type": "ScholarlyArticle",
            "paperId": "18ffa15d72a9c2e1be3cfce66b69bbc8114f8ef0",
            "corpusId": 51987847,
            "url": "https://www.semanticscholar.org/paper/18ffa15d72a9c2e1be3cfce66b69bbc8114f8ef0",
            "title": "A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1712.01381",
                "MAG": "2949061752",
                "DBLP": "conf/cvpr/ZhuEL0E18",
                "DOI": "10.1109/CVPR.2018.00111",
                "CorpusId": 51987847
            },
            "abstract": "Most existing zero-shot learning methods consider the problem as a visual semantic embedding one. Given the demonstrated capability of Generative Adversarial Networks(GANs) to generate images, we instead leverage GANs to imagine unseen categories from text descriptions and hence recognize novel classes with no examples being seen. Specifically, we propose a simple yet effective generative model that takes as input noisy text descriptions about an unseen class (e.g. Wikipedia articles) and generates synthesized visual features for this class. With added pseudo data, zero-shot learning is naturally converted to a traditional classification problem. Additionally, to preserve the inter-class discrimination of the generated features, a visual pivot regularization is proposed as an explicit supervision. Unlike previous methods using complex engineered regularizers, our approach can suppress the noise well without additional regularization. Empirically, we show that our method consistently outperforms the state of the art on the largest available benchmarks on Text-based Zero-shot Learning.",
            "referenceCount": 53,
            "citationCount": 345,
            "influentialCitationCount": 56,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1712.01381",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-04",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhu2017AGA,\n author = {Yizhe Zhu and Mohamed Elhoseiny and Bingchen Liu and Xi Peng and A. Elgammal},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {1004-1013},\n title = {A Generative Adversarial Approach for Zero-Shot Learning from Noisy Texts},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3321263fd0b2be6011f20d7b74b8ae801741eb21",
            "@type": "ScholarlyArticle",
            "paperId": "3321263fd0b2be6011f20d7b74b8ae801741eb21",
            "corpusId": 52986403,
            "url": "https://www.semanticscholar.org/paper/3321263fd0b2be6011f20d7b74b8ae801741eb21",
            "title": "Hierarchical Generative Modeling for Controllable Speech Synthesis",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2897548994",
                "DBLP": "conf/iclr/HsuZWZWWCJCSNP19",
                "ArXiv": "1810.07217",
                "CorpusId": 52986403
            },
            "abstract": "This paper proposes a neural sequence-to-sequence text-to-speech (TTS) model which can control latent attributes in the generated speech that are rarely annotated in the training data, such as speaking style, accent, background noise, and recording conditions. The model is formulated as a conditional generative model based on the variational autoencoder (VAE) framework, with two levels of hierarchical latent variables. The first level is a categorical variable, which represents attribute groups (e.g. clean/noisy) and provides interpretability. The second level, conditioned on the first, is a multivariate Gaussian variable, which characterizes specific attribute configurations (e.g. noise level, speaking rate) and enables disentangled fine-grained control over these attributes. This amounts to using a Gaussian mixture model (GMM) for the latent distribution. Extensive evaluation demonstrates its ability to control the aforementioned attributes. In particular, we train a high-quality controllable TTS model on real found data, which is capable of inferring speaker and style attributes from a noisy utterance and use it to synthesize clean speech with controllable speaking style.",
            "referenceCount": 45,
            "citationCount": 252,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-10-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1810.07217"
            },
            "citationStyles": {
                "bibtex": "@Article{Hsu2018HierarchicalGM,\n author = {Wei-Ning Hsu and Yu Zhang and Ron J. Weiss and H. Zen and Yonghui Wu and Yuxuan Wang and Yuan Cao and Ye Jia and Z. Chen and Jonathan Shen and Patrick Nguyen and Ruoming Pang},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Hierarchical Generative Modeling for Controllable Speech Synthesis},\n volume = {abs/1810.07217},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7a068d5c5ca129cc91617f0e90bcb38439e491b2",
            "@type": "ScholarlyArticle",
            "paperId": "7a068d5c5ca129cc91617f0e90bcb38439e491b2",
            "corpusId": 119193778,
            "url": "https://www.semanticscholar.org/paper/7a068d5c5ca129cc91617f0e90bcb38439e491b2",
            "title": "A generative modeling approach for benchmarking and training shallow quantum circuits",
            "venue": "npj Quantum Information",
            "publicationVenue": {
                "id": "urn:research:f8411b17-d726-4af8-a6ae-1ee0b6a5877f",
                "name": "npj Quantum Information",
                "alternate_names": [
                    "npj Quantum Inf"
                ],
                "issn": "2056-6387",
                "url": "http://www.nature.com/npjqi/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3104014430",
                "ArXiv": "1801.07686",
                "DOI": "10.1038/s41534-019-0157-8",
                "CorpusId": 119193778
            },
            "abstract": null,
            "referenceCount": 73,
            "citationCount": 251,
            "influentialCitationCount": 18,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2018-01-23",
            "journal": {
                "name": "npj Quantum Information",
                "volume": "5"
            },
            "citationStyles": {
                "bibtex": "@Article{Benedetti2018AGM,\n author = {Marcello Benedetti and Delfina Garcia-Pintos and O. Perdomo and Vicente Leyton-Ortega and Y. Nam and A. Perdomo-Ortiz},\n booktitle = {npj Quantum Information},\n journal = {npj Quantum Information},\n title = {A generative modeling approach for benchmarking and training shallow quantum circuits},\n volume = {5},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a",
            "@type": "ScholarlyArticle",
            "paperId": "e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a",
            "corpusId": 49273733,
            "url": "https://www.semanticscholar.org/paper/e4f0bf3074d5ae6d55d22068dd50158dc81b2a0a",
            "title": "Molecular generative model based on conditional variational autoencoder for de novo molecular design",
            "venue": "Journal of Cheminformatics",
            "publicationVenue": {
                "id": "urn:research:fd4675fe-4136-446c-aefd-3658aae698ac",
                "name": "Journal of Cheminformatics",
                "alternate_names": [
                    "J Cheminformatics"
                ],
                "issn": "1758-2946",
                "url": "https://jcheminf.biomedcentral.com/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1806-05805",
                "ArXiv": "1806.05805",
                "MAG": "2808177712",
                "PubMedCentral": "6041224",
                "DOI": "10.1186/s13321-018-0286-7",
                "CorpusId": 49273733,
                "PubMed": "29995272"
            },
            "abstract": null,
            "referenceCount": 31,
            "citationCount": 256,
            "influentialCitationCount": 8,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://jcheminf.biomedcentral.com/track/pdf/10.1186/s13321-018-0286-7",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-15",
            "journal": {
                "name": "Journal of Cheminformatics",
                "volume": "10"
            },
            "citationStyles": {
                "bibtex": "@Article{Lim2018MolecularGM,\n author = {Jaechang Lim and Seongok Ryu and Jin Woo Kim and W. Kim},\n booktitle = {Journal of Cheminformatics},\n journal = {Journal of Cheminformatics},\n title = {Molecular generative model based on conditional variational autoencoder for de novo molecular design},\n volume = {10},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:543f21d81bbea89f901dfcc01f4e332a9af6682d",
            "@type": "ScholarlyArticle",
            "paperId": "543f21d81bbea89f901dfcc01f4e332a9af6682d",
            "corpusId": 6230637,
            "url": "https://www.semanticscholar.org/paper/543f21d81bbea89f901dfcc01f4e332a9af6682d",
            "title": "Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2963250052",
                "DBLP": "journals/corr/Springenberg15",
                "ArXiv": "1511.06390",
                "CorpusId": 6230637
            },
            "abstract": "In this paper we present a method for learning a discriminative classifier from unlabeled or partially labeled data. Our approach is based on an objective function that trades-off mutual information between observed examples and their predicted categorical class distribution, against robustness of the classifier to an adversarial generative model. The resulting algorithm can either be interpreted as a natural generalization of the generative adversarial networks (GAN) framework or as an extension of the regularized information maximization (RIM) framework to robust classification against an optimal adversary. We empirically evaluate our method - which we dub categorical generative adversarial networks (or CatGAN) - on synthetic data as well as on challenging image classification tasks, demonstrating the robustness of the learned classifiers. We further qualitatively assess the fidelity of samples generated by the adversarial generator that is learned alongside the discriminative classifier, and identify links between the CatGAN objective and discriminative clustering algorithms (such as RIM).",
            "referenceCount": 45,
            "citationCount": 691,
            "influentialCitationCount": 60,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1511.06390"
            },
            "citationStyles": {
                "bibtex": "@Article{Springenberg2015UnsupervisedAS,\n author = {J. T. Springenberg},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Unsupervised and Semi-supervised Learning with Categorical Generative Adversarial Networks},\n volume = {abs/1511.06390},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:76d13ce5d6b684ba164988ef08c6f1b61dd8febf",
            "@type": "ScholarlyArticle",
            "paperId": "76d13ce5d6b684ba164988ef08c6f1b61dd8febf",
            "corpusId": 173188778,
            "url": "https://www.semanticscholar.org/paper/76d13ce5d6b684ba164988ef08c6f1b61dd8febf",
            "title": "Scaffold-based molecular design with a graph generative model",
            "venue": "Chemical Science",
            "publicationVenue": {
                "id": "urn:research:2a0713ba-a4af-4a0a-ae49-81b8edeca660",
                "name": "Chemical Science",
                "alternate_names": [
                    "Chem Sci",
                    "Chem sci",
                    "Chemical science"
                ],
                "issn": "2041-6520",
                "url": "http://www.rsc.org/chemicalscience"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2992072991",
                "PubMedCentral": "8146476",
                "DOI": "10.1039/c9sc04503a",
                "CorpusId": 173188778,
                "PubMed": "34084372"
            },
            "abstract": "Searching for new molecules in areas like drug discovery often starts from the core structures of known molecules. Such a method has called for a strategy of designing derivative compounds retaining a particular scaffold as a substructure. On this account, our present work proposes a graph generative model that targets its use in scaffold-based molecular design. Our model accepts a molecular scaffold as input and extends it by sequentially adding atoms and bonds. The generated molecules are then guaranteed to contain the scaffold with certainty, and their properties can be controlled by conditioning the generation process on desired properties. The learned rule of extending molecules can well generalize to arbitrary kinds of scaffolds, including those unseen during learning. In the conditional generation of molecules, our model can simultaneously control multiple chemical properties despite the search space constrained by fixing the substructure. As a demonstration, we applied our model to designing inhibitors of the epidermal growth factor receptor and show that our model can employ a simple semi-supervised extension to broaden its applicability to situations where only a small amount of data is available.",
            "referenceCount": 42,
            "citationCount": 91,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://pubs.rsc.org/en/content/articlepdf/2020/sc/c9sc04503a",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-31",
            "journal": {
                "name": "Chemical Science",
                "volume": "11"
            },
            "citationStyles": {
                "bibtex": "@Article{Lim2019ScaffoldbasedMD,\n author = {Jaechang Lim and Sang-Yeon Hwang and Seokhyun Moon and Seungsu Kim and W. Kim},\n booktitle = {Chemical Science},\n journal = {Chemical Science},\n pages = {1153 - 1164},\n title = {Scaffold-based molecular design with a graph generative model},\n volume = {11},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:324a797799f87a1d100d38230974b7a0cb03c007",
            "@type": "ScholarlyArticle",
            "paperId": "324a797799f87a1d100d38230974b7a0cb03c007",
            "corpusId": 57721154,
            "url": "https://www.semanticscholar.org/paper/324a797799f87a1d100d38230974b7a0cb03c007",
            "title": "Tree Tensor Networks for Generative Modeling",
            "venue": "Physical review B",
            "publicationVenue": {
                "id": "urn:research:52113867-f77b-4f26-a1cf-8e577dd325ea",
                "name": "Physical review B",
                "alternate_names": [
                    "Phys rev B",
                    "Phys Rev B",
                    "Physical Review B"
                ],
                "issn": "2469-9950",
                "url": "https://journals.aps.org/prb"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2963021603",
                "ArXiv": "1901.02217",
                "DBLP": "journals/corr/abs-1901-02217",
                "DOI": "10.1103/PhysRevB.99.155131",
                "CorpusId": 57721154
            },
            "abstract": "Matrix product states (MPSs), a tensor network designed for one-dimensional quantum systems, were recently proposed for generative modeling of natural data (such as images) in terms of the ``Born machine.'' However, the exponential decay of correlation in MPSs restricts its representation power heavily for modeling complex data such as natural images. In this work, we push forward the effort of applying tensor networks to machine learning by employing the tree tensor network (TTN), which exhibits balanced performance in expressibility and efficient training and sampling. We design the tree tensor network to utilize the two-dimensional prior of the natural images and develop sweeping learning and sampling algorithms which can be efficiently implemented utilizing graphical processing units. We apply our model to random binary patterns and the binary MNIST data sets of handwritten digits. We show that the TTN is superior to MPSs for generative modeling in keeping the correlation of pixels in natural images, as well as giving better log-likelihood scores in standard data sets of handwritten digits. We also compare its performance with state-of-the-art generative models such as variational autoencoders, restricted Boltzmann machines, and PixelCNN. Finally, we discuss the future development of tensor network states in machine learning problems.",
            "referenceCount": 49,
            "citationCount": 91,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1901.02217",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics",
                "Physics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-01-08",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1901.02217"
            },
            "citationStyles": {
                "bibtex": "@Article{Cheng2019TreeTN,\n author = {Song Cheng and Lei Wang and T. Xiang and Pan Zhang},\n booktitle = {Physical review B},\n journal = {ArXiv},\n title = {Tree Tensor Networks for Generative Modeling},\n volume = {abs/1901.02217},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90c97804c31287a35212538e85c5f34c0424341d",
            "@type": "ScholarlyArticle",
            "paperId": "90c97804c31287a35212538e85c5f34c0424341d",
            "corpusId": 59606195,
            "url": "https://www.semanticscholar.org/paper/90c97804c31287a35212538e85c5f34c0424341d",
            "title": "Mol-CycleGAN: a generative model for molecular optimization",
            "venue": "Journal of Cheminformatics",
            "publicationVenue": {
                "id": "urn:research:fd4675fe-4136-446c-aefd-3658aae698ac",
                "name": "Journal of Cheminformatics",
                "alternate_names": [
                    "J Cheminformatics"
                ],
                "issn": "1758-2946",
                "url": "https://jcheminf.biomedcentral.com/"
            },
            "year": 2019,
            "externalIds": {
                "PubMedCentral": "6950853",
                "DBLP": "journals/jcheminf/MaziarkaPKRDW20",
                "MAG": "2913351693",
                "ArXiv": "1902.02119",
                "DOI": "10.1186/s13321-019-0404-1",
                "CorpusId": 59606195,
                "PubMed": "33431006"
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 174,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007%2F978-3-030-30493-5_77.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Mathematics",
                "Materials Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Materials Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-02-06",
            "journal": {
                "name": "Journal of Cheminformatics",
                "volume": "12"
            },
            "citationStyles": {
                "bibtex": "@Article{Maziarka2019MolCycleGANAG,\n author = {\u0141ukasz Maziarka and Agnieszka Pocha and Jan Kaczmarczyk and Krzysztof Rataj and Tomasz Danel and M. Warcho\u0142},\n booktitle = {Journal of Cheminformatics},\n journal = {Journal of Cheminformatics},\n title = {Mol-CycleGAN: a generative model for molecular optimization},\n volume = {12},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c21d2d07cda6d8cb113de40fae726582d677ab3c",
            "@type": "ScholarlyArticle",
            "paperId": "c21d2d07cda6d8cb113de40fae726582d677ab3c",
            "corpusId": 202749839,
            "url": "https://www.semanticscholar.org/paper/c21d2d07cda6d8cb113de40fae726582d677ab3c",
            "title": "A Generative Model for Molecular Distance Geometry",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "conf/icml/SimmH20",
                "ArXiv": "1909.11459",
                "MAG": "3034219952",
                "CorpusId": 202749839
            },
            "abstract": "Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties.",
            "referenceCount": 49,
            "citationCount": 76,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2019-09-25",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Simm2019AGM,\n author = {G. Simm and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato},\n booktitle = {International Conference on Machine Learning},\n pages = {8949-8958},\n title = {A Generative Model for Molecular Distance Geometry},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef6abdcbd0871cb356e8669b6cdb31ed8b013cc2",
            "@type": "ScholarlyArticle",
            "paperId": "ef6abdcbd0871cb356e8669b6cdb31ed8b013cc2",
            "corpusId": 16628328,
            "url": "https://www.semanticscholar.org/paper/ef6abdcbd0871cb356e8669b6cdb31ed8b013cc2",
            "title": "Medical Image Synthesis with Context-Aware Generative Adversarial Networks",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2952038462",
                "DBLP": "journals/corr/NieTPRS16",
                "ArXiv": "1612.05362",
                "DOI": "10.1007/978-3-319-66179-7_48",
                "CorpusId": 16628328,
                "PubMed": "30009283"
            },
            "abstract": null,
            "referenceCount": 26,
            "citationCount": 614,
            "influentialCitationCount": 25,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc6044459?pdf=render",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Engineering",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-12-16",
            "journal": {
                "name": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "volume": "10435"
            },
            "citationStyles": {
                "bibtex": "@Article{Nie2016MedicalIS,\n author = {Dong Nie and Roger Trullo and C. Petitjean and S. Ruan and D. Shen},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {\n          417-425\n        },\n title = {Medical Image Synthesis with Context-Aware Generative Adversarial Networks},\n volume = {10435},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8928371206f313d409eeb5242d646a8e71061d90",
            "@type": "ScholarlyArticle",
            "paperId": "8928371206f313d409eeb5242d646a8e71061d90",
            "corpusId": 46940780,
            "url": "https://www.semanticscholar.org/paper/8928371206f313d409eeb5242d646a8e71061d90",
            "title": "Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2952484912",
                "ArXiv": "1806.01794",
                "DBLP": "journals/corr/abs-1806-01794",
                "CorpusId": 46940780
            },
            "abstract": "We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep generative model for image sequences. It can reliably discover and track objects through the sequence; it can also conditionally generate future frames, thereby simulating expected motion of objects. This is achieved by explicitly encoding object numbers, locations and appearances in the latent variables of the model. SQAIR retains all strengths of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al. 2016), including unsupervised learning, made possible by inductive biases present in the model structure. We use a moving multi-\\textsc{mnist} dataset to show limitations of AIR in detecting overlapping or partially occluded objects, and show how \\textsc{sqair} overcomes them by leveraging temporal consistency of objects. Finally, we also apply SQAIR to real-world pedestrian CCTV data, where it learns to reliably detect, track and generate walking pedestrians with no supervision.",
            "referenceCount": 49,
            "citationCount": 222,
            "influentialCitationCount": 16,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kosiorek2018SequentialAI,\n author = {Adam R. Kosiorek and Hyunjik Kim and I. Posner and Y. Teh},\n booktitle = {Neural Information Processing Systems},\n pages = {8615-8625},\n title = {Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f2fa3a0cd1ade8af75cba1c11e8236a8027e2ef1",
            "@type": "ScholarlyArticle",
            "paperId": "f2fa3a0cd1ade8af75cba1c11e8236a8027e2ef1",
            "corpusId": 4401118,
            "url": "https://www.semanticscholar.org/paper/f2fa3a0cd1ade8af75cba1c11e8236a8027e2ef1",
            "title": "Generative Modeling Using the Sliced Wasserstein Distance",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/cvpr/DeshpandeZS18",
                "MAG": "2952494443",
                "ArXiv": "1803.11188",
                "DOI": "10.1109/CVPR.2018.00367",
                "CorpusId": 4401118
            },
            "abstract": "Generative Adversarial Nets (GANs) are very successful at modeling distributions from given samples, even in the high-dimensional case. However, their formulation is also known to be hard to optimize and often not stable. While this is particularly true for early GAN formulations, there has been significant empirically motivated and theoretically founded progress to improve stability, for instance, by using the Wasserstein distance rather than the Jenson-Shannon divergence. Here, we consider an alternative formulation for generative modeling based on random projections which, in its simplest form, results in a single objective rather than a saddle-point formulation. By augmenting this approach with a discriminator we improve its accuracy. We found our approach to be significantly more stable compared to even the improved Wasserstein GAN. Further, unlike the traditional GAN loss, the loss formulated in our method is a good measure of the actual distance between the distributions and, for the first time for GAN training, we are able to show estimates for the same.",
            "referenceCount": 37,
            "citationCount": 175,
            "influentialCitationCount": 37,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1803.11188",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-03-29",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Deshpande2018GenerativeMU,\n author = {Ishani Deshpande and Ziyu Zhang and A. Schwing},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n pages = {3483-3491},\n title = {Generative Modeling Using the Sliced Wasserstein Distance},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:062c41dad67bb68fefd9ff0c5c4d296e796004dc",
            "@type": "ScholarlyArticle",
            "paperId": "062c41dad67bb68fefd9ff0c5c4d296e796004dc",
            "corpusId": 6945308,
            "url": "https://www.semanticscholar.org/paper/062c41dad67bb68fefd9ff0c5c4d296e796004dc",
            "title": "Temporal Generative Adversarial Nets with Singular Value Clipping",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2952493843",
                "DBLP": "conf/iccv/SaitoMS17",
                "ArXiv": "1611.06624",
                "DOI": "10.1109/ICCV.2017.308",
                "CorpusId": 6945308
            },
            "abstract": "In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.",
            "referenceCount": 56,
            "citationCount": 377,
            "influentialCitationCount": 65,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1611.06624",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-11-21",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Saito2016TemporalGA,\n author = {Masaki Saito and Eiichi Matsumoto and S. Saito},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2849-2858},\n title = {Temporal Generative Adversarial Nets with Singular Value Clipping},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1643f48027cd18aeaf713bf7b5b18bb91a765503",
            "@type": "ScholarlyArticle",
            "paperId": "1643f48027cd18aeaf713bf7b5b18bb91a765503",
            "corpusId": 40464958,
            "url": "https://www.semanticscholar.org/paper/1643f48027cd18aeaf713bf7b5b18bb91a765503",
            "title": "Generative OpenMax for Multi-Class Open Set Classification",
            "venue": "British Machine Vision Conference",
            "publicationVenue": {
                "id": "urn:research:78a7fbcc-41c5-4258-b633-04b8637d4a9f",
                "name": "British Machine Vision Conference",
                "alternate_names": [
                    "Br Mach Vis Conf",
                    "BMVC"
                ],
                "issn": null,
                "url": "http://www.bmva.org/bmvc/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/GeDCG17",
                "ArXiv": "1707.07418",
                "MAG": "2963875483",
                "DOI": "10.5244/C.31.42",
                "CorpusId": 40464958
            },
            "abstract": "We present a conceptually new and flexible method for multi-class open set classification. Unlike previous methods where unknown classes are inferred with respect to the feature or decision distance to the known classes, our approach is able to provide explicit modelling and decision score for unknown classes. The proposed method, called Gener- ative OpenMax (G-OpenMax), extends OpenMax by employing generative adversarial networks (GANs) for novel category image synthesis. We validate the proposed method on two datasets of handwritten digits and characters, resulting in superior results over previous deep learning based method OpenMax Moreover, G-OpenMax provides a way to visualize samples representing the unknown classes from open space. Our simple and effective approach could serve as a new direction to tackle the challenging multi-class open set classification problem.",
            "referenceCount": 31,
            "citationCount": 270,
            "influentialCitationCount": 30,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://www.bmva.org/bmvc/2017/papers/paper042/paper042.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-07-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1707.07418"
            },
            "citationStyles": {
                "bibtex": "@Article{Ge2017GenerativeOF,\n author = {ZongYuan Ge and S. Demyanov and Zetao Chen and R. Garnavi},\n booktitle = {British Machine Vision Conference},\n journal = {ArXiv},\n title = {Generative OpenMax for Multi-Class Open Set Classification},\n volume = {abs/1707.07418},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a525cb3779d5eb19880faf8aa7a34a34cfbdef40",
            "@type": "ScholarlyArticle",
            "paperId": "a525cb3779d5eb19880faf8aa7a34a34cfbdef40",
            "corpusId": 12066020,
            "url": "https://www.semanticscholar.org/paper/a525cb3779d5eb19880faf8aa7a34a34cfbdef40",
            "title": "Multi-style Generative Network for Real-time Transfer",
            "venue": "ECCV Workshops",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949848065",
                "DBLP": "conf/eccv/ZhangD18",
                "ArXiv": "1703.06953",
                "DOI": "10.1007/978-3-030-11018-5_32",
                "CorpusId": 12066020
            },
            "abstract": null,
            "referenceCount": 56,
            "citationCount": 247,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1703.06953",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-20",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017MultistyleGN,\n author = {Hang Zhang and Kristin J. Dana},\n booktitle = {ECCV Workshops},\n pages = {349-365},\n title = {Multi-style Generative Network for Real-time Transfer},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29af8829e70aec87731dd5022a71cd77589e9b87",
            "@type": "ScholarlyArticle",
            "paperId": "29af8829e70aec87731dd5022a71cd77589e9b87",
            "corpusId": 52880246,
            "url": "https://www.semanticscholar.org/paper/29af8829e70aec87731dd5022a71cd77589e9b87",
            "title": "Generative replay with feedback connections as a general strategy for continual learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1809-10635",
                "ArXiv": "1809.10635",
                "MAG": "2894094671",
                "CorpusId": 52880246
            },
            "abstract": "A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as \"soft targets\") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.",
            "referenceCount": 41,
            "citationCount": 186,
            "influentialCitationCount": 15,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-09-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1809.10635"
            },
            "citationStyles": {
                "bibtex": "@Article{Ven2018GenerativeRW,\n author = {Gido M. van de Ven and A. Tolias},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Generative replay with feedback connections as a general strategy for continual learning},\n volume = {abs/1809.10635},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
            "@type": "ScholarlyArticle",
            "paperId": "7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
            "corpusId": 88514467,
            "url": "https://www.semanticscholar.org/paper/7ecd7c88018d8fd27194b8ae7bf48b6a9dac9823",
            "title": "Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis",
            "venue": "Computing and Software for Big Science",
            "publicationVenue": {
                "id": "urn:research:3a6a506a-7577-4f58-be4a-b4f786c2b892",
                "name": "Computing and Software for Big Science",
                "alternate_names": [
                    "Comput Softw Big Sci"
                ],
                "issn": "2510-2044",
                "url": "https://link.springer.com/journal/41781"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2581875816",
                "ArXiv": "1701.05927",
                "DOI": "10.1007/s41781-017-0004-6",
                "CorpusId": 88514467
            },
            "abstract": null,
            "referenceCount": 46,
            "citationCount": 259,
            "influentialCitationCount": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1701.05927",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Physics",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Physics",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-01-20",
            "journal": {
                "name": "Computing and Software for Big Science",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Oliveira2017LearningPP,\n author = {Luke de Oliveira and Michela Paganini and B. Nachman},\n booktitle = {Computing and Software for Big Science},\n journal = {Computing and Software for Big Science},\n title = {Learning Particle Physics by Example: Location-Aware Generative Adversarial Networks for Physics Synthesis},\n volume = {1},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16481307a862ddae514045b13b944ca043c36a4b",
            "@type": "ScholarlyArticle",
            "paperId": "16481307a862ddae514045b13b944ca043c36a4b",
            "corpusId": 67856031,
            "url": "https://www.semanticscholar.org/paper/16481307a862ddae514045b13b944ca043c36a4b",
            "title": "Generative Adversarial User Model for Reinforcement Learning Based Recommendation System",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/icml/Chen0LJQS19",
                "ArXiv": "1812.10613",
                "MAG": "2953017267",
                "CorpusId": 67856031
            },
            "abstract": "There are great interests as well as many challenges in applying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the application of RL challenging. In this paper, we propose a novel model-based reinforcement learning framework for recommendation systems, where we develop a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develop a novel Cascading DQN algorithm to obtain a combinatorial recommendation policy which can handle a large number of candidate items efficiently. In our experiments with real data, we show this generative adversarial user model can better explain user behavior than alternatives, and the RL policy based on this model can lead to a better long-term reward for the user and higher click rate for the system.",
            "referenceCount": 30,
            "citationCount": 151,
            "influentialCitationCount": 25,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-12-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chen2018GenerativeAU,\n author = {Xinshi Chen and Shuang Li and Hui Li and Shaohua Jiang and Yuan Qi and Le Song},\n booktitle = {International Conference on Machine Learning},\n pages = {1052-1061},\n title = {Generative Adversarial User Model for Reinforcement Learning Based Recommendation System},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b1df2dfda502516fa0346e0dff57851d223fffd9",
            "@type": "ScholarlyArticle",
            "paperId": "b1df2dfda502516fa0346e0dff57851d223fffd9",
            "corpusId": 49665488,
            "url": "https://www.semanticscholar.org/paper/b1df2dfda502516fa0346e0dff57851d223fffd9",
            "title": "Phase Retrieval Under a Generative Prior",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/nips/HandLV18",
                "MAG": "2952787867",
                "ArXiv": "1807.04261",
                "CorpusId": 49665488
            },
            "abstract": "The phase retrieval problem asks to recover a natural signal $y_0 \\in \\mathbb{R}^n$ from $m$ quadratic observations, where $m$ is to be minimized. As is common in many imaging problems, natural signals are considered sparse with respect to a known basis, and the generic sparsity prior is enforced via $\\ell_1$ regularization. While successful in the realm of linear inverse problems, such $\\ell_1$ methods have encountered possibly fundamental limitations, as no computationally efficient algorithm for phase retrieval of a $k$-sparse signal has been proven to succeed with fewer than $O(k^2\\log n)$ generic measurements, exceeding the theoretical optimum of $O(k \\log n)$. In this paper, we propose a novel framework for phase retrieval by 1) modeling natural signals as being in the range of a deep generative neural network $G : \\mathbb{R}^k \\rightarrow \\mathbb{R}^n$ and 2) enforcing this prior directly by optimizing an empirical risk objective over the domain of the generator. Our formulation has provably favorable global geometry for gradient methods, as soon as $m = O(kd^2\\log n)$, where $d$ is the depth of the network. Specifically, when suitable deterministic conditions on the generator and measurement matrix are met, we construct a descent direction for any point outside of a small neighborhood around the unique global minimizer and its negative multiple, and show that such conditions hold with high probability under Gaussian ensembles of multilayer fully-connected generator networks and measurement matrices. This formulation for structured phase retrieval thus has two advantages over sparsity based methods: 1) deep generative priors can more tightly represent natural signals and 2) information theoretically optimal sample complexity. We corroborate these results with experiments showing that exploiting generative models in phase retrieval tasks outperforms sparse phase retrieval methods.",
            "referenceCount": 41,
            "citationCount": 159,
            "influentialCitationCount": 20,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-11",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hand2018PhaseRU,\n author = {Paul Hand and Oscar Leong and V. Voroninski},\n booktitle = {Neural Information Processing Systems},\n pages = {9154-9164},\n title = {Phase Retrieval Under a Generative Prior},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ea2ac6e39d249a98604021070b8e0a3c1fad3cb",
            "@type": "ScholarlyArticle",
            "paperId": "4ea2ac6e39d249a98604021070b8e0a3c1fad3cb",
            "corpusId": 46900050,
            "url": "https://www.semanticscholar.org/paper/4ea2ac6e39d249a98604021070b8e0a3c1fad3cb",
            "title": "Image Colorization Using Generative Adversarial Networks",
            "venue": "Articulated Motion and Deformable Objects",
            "publicationVenue": {
                "id": "urn:research:b7c08467-bc2f-4204-a568-6fcae40b324c",
                "name": "Articulated Motion and Deformable Objects",
                "alternate_names": [
                    "Articul Motion Deform Object",
                    "AMDO"
                ],
                "issn": null,
                "url": "http://www.wikicfp.com/cfp/program?id=151"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "3102726805",
                "ArXiv": "1803.05400",
                "DBLP": "conf/amdo/NazeriNE18",
                "DOI": "10.1007/978-3-319-94544-6_9",
                "CorpusId": 46900050
            },
            "abstract": null,
            "referenceCount": 23,
            "citationCount": 123,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/1803.05400",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-03-14",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Nazeri2018ImageCU,\n author = {Kamyar Nazeri and Eric Ng and Mehran Ebrahimi},\n booktitle = {Articulated Motion and Deformable Objects},\n pages = {85-94},\n title = {Image Colorization Using Generative Adversarial Networks},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:423fab20c1c39cb6b4d6cf6098f61c092e6d1e3f",
            "@type": "ScholarlyArticle",
            "paperId": "423fab20c1c39cb6b4d6cf6098f61c092e6d1e3f",
            "corpusId": 16367617,
            "url": "https://www.semanticscholar.org/paper/423fab20c1c39cb6b4d6cf6098f61c092e6d1e3f",
            "title": "Generative Multi-Adversarial Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2554506842",
                "DBLP": "conf/iclr/DurugkarGM17",
                "ArXiv": "1611.01673",
                "CorpusId": 16367617
            },
            "abstract": "Generative adversarial networks (GANs) are a framework for producing a generative model by way of a two-player minimax game. In this paper, we propose the \\emph{Generative Multi-Adversarial Network} (GMAN), a framework that extends GANs to multiple discriminators. In previous work, the successful training of GANs requires modifying the minimax objective to accelerate training early on. In contrast, GMAN can be reliably trained with the original, untampered objective. We explore a number of design perspectives with the discriminator role ranging from formidable adversary to forgiving teacher. Image generation tasks comparing the proposed framework to standard GANs demonstrate GMAN produces higher quality samples in a fraction of the iterations when measured by a pairwise GAM-type metric.",
            "referenceCount": 48,
            "citationCount": 295,
            "influentialCitationCount": 24,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01673"
            },
            "citationStyles": {
                "bibtex": "@Article{Durugkar2016GenerativeMN,\n author = {Ishan Durugkar and I. Gemp and S. Mahadevan},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Generative Multi-Adversarial Networks},\n volume = {abs/1611.01673},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:185d09aab77f64239b754a9cef41fc5b23b7e47e",
            "@type": "ScholarlyArticle",
            "paperId": "185d09aab77f64239b754a9cef41fc5b23b7e47e",
            "corpusId": 235667179,
            "url": "https://www.semanticscholar.org/paper/185d09aab77f64239b754a9cef41fc5b23b7e47e",
            "title": "Generative Modeling",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "CorpusId": 235667179
            },
            "abstract": "(1) Discriminative modeling is used to teach machines how to categorize existing data. With a large enough dataset, a machine can learn to categorize a dataset of handwritten numbers by digit, to differentiate positive and negative reviews on Yelp, to detect the probability of an x-ray containing a broken bone. The model is trained on a dataset for which we already have the correct labels for, compared to the ground truth, and modified to improve its accuracy.",
            "referenceCount": 4,
            "citationCount": 7,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Dai2020GenerativeM,\n author = {Anna Dai},\n title = {Generative Modeling},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c58b7466f2855ffdcff1bebfad6b6a027b8c5ee1",
            "@type": "ScholarlyArticle",
            "paperId": "c58b7466f2855ffdcff1bebfad6b6a027b8c5ee1",
            "corpusId": 35258007,
            "url": "https://www.semanticscholar.org/paper/c58b7466f2855ffdcff1bebfad6b6a027b8c5ee1",
            "title": "Ultra-Resolving Face Images by Discriminative Generative Networks",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "conf/eccv/YuP16",
                "MAG": "2520930090",
                "DOI": "10.1007/978-3-319-46454-1_20",
                "CorpusId": 35258007
            },
            "abstract": null,
            "referenceCount": 37,
            "citationCount": 260,
            "influentialCitationCount": 40,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-319-46454-1_20.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-10-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Yu2016UltraResolvingFI,\n author = {Xin Yu and F. Porikli},\n booktitle = {European Conference on Computer Vision},\n pages = {318-333},\n title = {Ultra-Resolving Face Images by Discriminative Generative Networks},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fb3a2ba6cedaeb56a8cac9c61be8fee58efe6de3",
            "@type": "ScholarlyArticle",
            "paperId": "fb3a2ba6cedaeb56a8cac9c61be8fee58efe6de3",
            "corpusId": 1508909,
            "url": "https://www.semanticscholar.org/paper/fb3a2ba6cedaeb56a8cac9c61be8fee58efe6de3",
            "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "publicationVenue": {
                "id": "urn:research:41bf9ed3-85b3-4c90-b015-150e31690253",
                "name": "Conference on Empirical Methods in Natural Language Processing",
                "alternate_names": [
                    "Empir Method Nat Lang Process",
                    "Empirical Methods in Natural Language Processing",
                    "Conf Empir Method Nat Lang Process",
                    "EMNLP"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/emnlp"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/emnlp/LiLBW17",
                "MAG": "2953168555",
                "ArXiv": "1708.00625",
                "ACL": "D17-1222",
                "DOI": "10.18653/v1/D17-1222",
                "CorpusId": 1508909
            },
            "abstract": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.",
            "referenceCount": 43,
            "citationCount": 198,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/D17-1222.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-08-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Li2017DeepRG,\n author = {Piji Li and Wai Lam and Lidong Bing and Zihao Wang},\n booktitle = {Conference on Empirical Methods in Natural Language Processing},\n pages = {2091-2100},\n title = {Deep Recurrent Generative Decoder for Abstractive Text Summarization},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:157f77a508c1645a2609c9b265391e9d1bfa95e4",
            "@type": "ScholarlyArticle",
            "paperId": "157f77a508c1645a2609c9b265391e9d1bfa95e4",
            "corpusId": 7752494,
            "url": "https://www.semanticscholar.org/paper/157f77a508c1645a2609c9b265391e9d1bfa95e4",
            "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1706.04317",
                "MAG": "2962764591",
                "DBLP": "conf/icml/KanskySMELLDSPG17",
                "CorpusId": 7752494
            },
            "abstract": "The recent adaptation of deep neural network-based methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.",
            "referenceCount": 28,
            "citationCount": 210,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Physics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-06-14",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1706.04317"
            },
            "citationStyles": {
                "bibtex": "@Article{Kansky2017SchemaNZ,\n author = {Ken Kansky and Tom Silver and David A. M\u00e9ly and Mohamed Eldawy and M. L\u00e1zaro-Gredilla and Xinghua Lou and N. Dorfman and Szymon Sidor and Scott Phoenix and Dileep George},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics},\n volume = {abs/1706.04317},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0ef296ae607ca441b4fd50fec30aa28f517eb09e",
            "@type": "ScholarlyArticle",
            "paperId": "0ef296ae607ca441b4fd50fec30aa28f517eb09e",
            "corpusId": 52909509,
            "url": "https://www.semanticscholar.org/paper/0ef296ae607ca441b4fd50fec30aa28f517eb09e",
            "title": "Augmenting Image Classifiers Using Data Augmentation Generative Adversarial Networks",
            "venue": "International Conference on Artificial Neural Networks",
            "publicationVenue": {
                "id": "urn:research:3e64b1c1-745f-4edf-bd92-b8ef122bb49c",
                "name": "International Conference on Artificial Neural Networks",
                "alternate_names": [
                    "Int Conf Artif Neural Netw",
                    "ICANN"
                ],
                "issn": null,
                "url": "http://www.e-nns.org/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2894737833",
                "DBLP": "conf/icann/AntoniouSE18",
                "DOI": "10.1007/978-3-030-01424-7_58",
                "CorpusId": 52909509
            },
            "abstract": null,
            "referenceCount": 44,
            "citationCount": 107,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.pure.ed.ac.uk/ws/files/132575435/Augmenting_Image_Classifiers_ANTONIOU_DOA10072018_AFV.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-10-04",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Antoniou2018AugmentingIC,\n author = {Antreas Antoniou and A. Storkey and Harrison Edwards},\n booktitle = {International Conference on Artificial Neural Networks},\n pages = {594-603},\n title = {Augmenting Image Classifiers Using Data Augmentation Generative Adversarial Networks},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:77be85f6c3c465ef8e17d3ec6251794cf4ff5940",
            "@type": "ScholarlyArticle",
            "paperId": "77be85f6c3c465ef8e17d3ec6251794cf4ff5940",
            "corpusId": 52960124,
            "url": "https://www.semanticscholar.org/paper/77be85f6c3c465ef8e17d3ec6251794cf4ff5940",
            "title": "Generative Domain-Migration Hashing for Sketch-to-Image Retrieval",
            "venue": "European Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:167fa0ca-e88a-4ef7-a16f-bc66c457c806",
                "name": "European Conference on Computer Vision",
                "alternate_names": [
                    "ECCV",
                    "Eur Conf Comput Vis"
                ],
                "issn": null,
                "url": "https://link.springer.com/conference/eccv"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "conf/eccv/ZhangSLZYSSG18",
                "MAG": "2898012907",
                "DOI": "10.1007/978-3-030-01216-8_19",
                "CorpusId": 52960124
            },
            "abstract": null,
            "referenceCount": 59,
            "citationCount": 78,
            "influentialCitationCount": 12,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2018-09-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018GenerativeDH,\n author = {Jingyi Zhang and Fumin Shen and Li Liu and Fan Zhu and Mengyang Yu and Ling Shao and Heng Tao Shen and L. Gool},\n booktitle = {European Conference on Computer Vision},\n pages = {304-321},\n title = {Generative Domain-Migration Hashing for Sketch-to-Image Retrieval},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c3124b0491d479e8a869c61f059ffa08dad49362",
            "@type": "ScholarlyArticle",
            "paperId": "c3124b0491d479e8a869c61f059ffa08dad49362",
            "corpusId": 28482216,
            "url": "https://www.semanticscholar.org/paper/c3124b0491d479e8a869c61f059ffa08dad49362",
            "title": "A Generative Model for Zero Shot Learning Using Conditional Variational Autoencoders",
            "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2949151163",
                "DBLP": "conf/cvpr/MishraRMM18",
                "ArXiv": "1709.00663",
                "DOI": "10.1109/CVPRW.2018.00294",
                "CorpusId": 28482216
            },
            "abstract": "Zero shot learning in Image Classification refers to the setting where images from some novel classes are absent in the training data but other information such as natural language descriptions or attribute vectors of the classes are available. This setting is important in the real world since one may not be able to obtain images of all the possible classes at training. While previous approaches have tried to model the relationship between the class attribute space and the image space via some kind of a transfer function in order to model the image space correspondingly to an unseen class, we take a different approach and try to generate the samples from the given attributes, using a conditional variational autoencoder, and use the generated samples for classification of the unseen classes. By extensive testing on four benchmark datasets, we show that our model outperforms the state of the art, particularly in the more realistic generalized setting, where the training classes can also appear at the test time along with the novel classes.",
            "referenceCount": 48,
            "citationCount": 263,
            "influentialCitationCount": 41,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1709.00663",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-09-03",
            "journal": {
                "name": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Mishra2017AGM,\n author = {Ashish Mishra and M. K. Reddy and Anurag Mittal and H. Murthy},\n booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n journal = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n pages = {2269-22698},\n title = {A Generative Model for Zero Shot Learning Using Conditional Variational Autoencoders},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:cd9673c39a14aa5ac6898c8e132247e47fc81aaa",
            "@type": "ScholarlyArticle",
            "paperId": "cd9673c39a14aa5ac6898c8e132247e47fc81aaa",
            "corpusId": 3084814,
            "url": "https://www.semanticscholar.org/paper/cd9673c39a14aa5ac6898c8e132247e47fc81aaa",
            "title": "Deep Generative Adversarial Compression Artifact Removal",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/iccv/GalteriSBB17",
                "ArXiv": "1704.02518",
                "MAG": "2962687329",
                "DOI": "10.1109/ICCV.2017.517",
                "CorpusId": 3084814
            },
            "abstract": "Compression artifacts arise in images whenever a lossy compression algorithm is applied. These artifacts eliminate details present in the original image, or add noise and small structures; because of these effects they make images less pleasant for the human eye, and may also lead to decreased performance of computer vision algorithms such as object detectors. To eliminate such artifacts, when decompressing an image, it is required to recover the original image from a disturbed version. To this end, we present a feed-forward fully convolutional residual network model trained using a generative adversarial framework. To provide a baseline, we show that our model can be also trained optimizing the Structural Similarity (SSIM), which is a better loss with respect to the simpler Mean Squared Error (MSE). Our GAN is able to produce images with more photorealistic details than MSE or SSIM based networks. Moreover we show that our approach can be used as a pre-processing step for object detection in case images are degraded by compression to a point that state-of-the art detectors fail. In this task, our GAN method obtains better performance than MSE or SSIM trained networks.",
            "referenceCount": 40,
            "citationCount": 186,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1704.02518",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-04-08",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Galteri2017DeepGA,\n author = {L. Galteri and Lorenzo Seidenari and M. Bertini and A. Bimbo},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {4836-4845},\n title = {Deep Generative Adversarial Compression Artifact Removal},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6dd3b0c56f738564167e3509b097124bde4b4f64",
            "@type": "ScholarlyArticle",
            "paperId": "6dd3b0c56f738564167e3509b097124bde4b4f64",
            "corpusId": 12185715,
            "url": "https://www.semanticscholar.org/paper/6dd3b0c56f738564167e3509b097124bde4b4f64",
            "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks",
            "venue": "ECML/PKDD",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.06674",
                "DBLP": "conf/pkdd/CaoZZY17",
                "MAG": "2950394421",
                "DOI": "10.1007/978-3-319-71249-9_10",
                "CorpusId": 12185715
            },
            "abstract": null,
            "referenceCount": 32,
            "citationCount": 165,
            "influentialCitationCount": 14,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1702.06674",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-02-22",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1702.06674"
            },
            "citationStyles": {
                "bibtex": "@Article{Cao2017UnsupervisedDC,\n author = {Yun Cao and Zhiming Zhou and Weinan Zhang and Yong Yu},\n booktitle = {ECML/PKDD},\n journal = {ArXiv},\n title = {Unsupervised Diverse Colorization via Generative Adversarial Networks},\n volume = {abs/1702.06674},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1fe7ce8e3eec85c9a35f213044d1fa3e855c3897",
            "@type": "ScholarlyArticle",
            "paperId": "1fe7ce8e3eec85c9a35f213044d1fa3e855c3897",
            "corpusId": 4468961,
            "url": "https://www.semanticscholar.org/paper/1fe7ce8e3eec85c9a35f213044d1fa3e855c3897",
            "title": "A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs",
            "venue": "Science",
            "publicationVenue": {
                "id": "urn:research:f59506a8-d8bb-4101-b3d4-c4ac3ed03dad",
                "name": "Science",
                "alternate_names": null,
                "issn": "0193-4511",
                "url": "https://www.jstor.org/journal/science"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2765332150",
                "DOI": "10.1126/science.aag2612",
                "CorpusId": 4468961,
                "PubMed": "29074582"
            },
            "abstract": "Computer or human? Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based CAPTCHA, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George et al. developed a hierarchical model for computer vision that was able to solve CAPTCHAs with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based CAPTCHAs, as some online services have done, may be a good idea. Science, this issue p. eaag2612 A hierarchical computer vision model solves CAPTCHAs with a high accuracy rate using relatively little training data. INTRODUCTION Compositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. CAPTCHAs add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers\u2014the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. CAPTCHAs also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific CAPTCHA style required millions of labeled examples, whereas humans solve new styles without explicit training. By drawing inspiration from systems neuroscience, we introduce recursive cortical network (RCN), a probabilistic generative model for vision in which message-passing\u2013based inference handles recognition, segmentation, and reasoning in a unified manner. RCN learns with very little training data and fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters. In addition, RCN outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient. RATIONALE Modern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation (BP) is an inference algorithm that can match the cortical computational speed. The representational choices in RCN were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using BP. RESULTS RCN was effective in breaking a wide variety of CAPTCHAs with very little training data and without using CAPTCHA-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot MNIST (modified National Institute of Standards and Technology handwritten digit data set) classification, where RCN was significantly more robust to clutter introduced during testing. As a generative model, RCN outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. RCN also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, RCN outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data. CONCLUSION Our work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model\u2019s effectiveness in breaking text-based CAPTCHAs with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions. Breaking CAPTCHAs using a generative vision model. Text-based CAPTCHAs exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based CAPTCHAs. Shown are the parses by our model for a variety of CAPTCHAs . Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing\u2013based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.",
            "referenceCount": 111,
            "citationCount": 229,
            "influentialCitationCount": 21,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://science.sciencemag.org/content/sci/358/6368/eaag2612.full.pdf",
                "status": "CLOSED"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-12-08",
            "journal": {
                "name": "Science",
                "volume": "358"
            },
            "citationStyles": {
                "bibtex": "@Article{George2017AGV,\n author = {Dileep George and Wolfgang Lehrach and Ken Kansky and M. L\u00e1zaro-Gredilla and C. Laan and B. Marthi and Xinghua Lou and Zhaoshi Meng and Yi Liu and Hua-Yan Wang and Alexander Lavin and D. Phoenix},\n booktitle = {Science},\n journal = {Science},\n title = {A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs},\n volume = {358},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af55ef5f60e5a33e688b28fa518cfb5bef35342b",
            "@type": "ScholarlyArticle",
            "paperId": "af55ef5f60e5a33e688b28fa518cfb5bef35342b",
            "corpusId": 3625590,
            "url": "https://www.semanticscholar.org/paper/af55ef5f60e5a33e688b28fa518cfb5bef35342b",
            "title": "SCH-GAN: Semi-Supervised Cross-Modal Hashing by Generative Adversarial Network",
            "venue": "IEEE Transactions on Cybernetics",
            "publicationVenue": {
                "id": "urn:research:404813e7-95da-4137-be14-2ba73d2df4fd",
                "name": "IEEE Transactions on Cybernetics",
                "alternate_names": [
                    "IEEE Trans Cybern"
                ],
                "issn": "2168-2267",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=6221036"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tcyb/ZhangPY20",
                "MAG": "2962805368",
                "ArXiv": "1802.02488",
                "DOI": "10.1109/TCYB.2018.2868826",
                "CorpusId": 3625590,
                "PubMed": "30273169"
            },
            "abstract": "Cross-modal hashing maps heterogeneous multimedia data into a common Hamming space to realize fast and flexible cross-modal retrieval. Supervised cross-modal hashing methods have achieved considerable progress by incorporating semantic side information. However, they heavily rely on large-scale labeled cross-modal training data which are hard to obtain, since multiple modalities are involved. They also ignore the rich information contained in the large amount of unlabeled data across different modalities, which can help to model the correlations between different modalities. To address these problems, in this paper, we propose a novel semi-supervised cross-modal hashing approach by generative adversarial network (SCH-GAN). The main contributions can be summarized as follows: 1) we propose a novel generative adversarial network for cross-modal hashing, in which the generative model tries to select margin examples of one modality from unlabeled data when given a query of another modality (e.g., giving a text query to retrieve images and vice versa). The discriminative model tries to distinguish the selected examples and true positive examples of the query. These two models play a minimax game so that the generative model can promote the hashing performance of the discriminative model and 2) we propose a reinforcement learning-based algorithm to drive the training of proposed SCH-GAN. The generative model takes the correlation score predicted by discriminative model as a reward, and tries to select the examples close to the margin to promote a discriminative model. Extensive experiments verify the effectiveness of our proposed approach, compared with nine state-of-the-art methods on three widely used datasets.",
            "referenceCount": 62,
            "citationCount": 100,
            "influentialCitationCount": 5,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1802.02488",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-07",
            "journal": {
                "name": "IEEE Transactions on Cybernetics",
                "volume": "50"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2018SCHGANSC,\n author = {Jian Zhang and Yuxin Peng and Mingkuan Yuan},\n booktitle = {IEEE Transactions on Cybernetics},\n journal = {IEEE Transactions on Cybernetics},\n pages = {489-502},\n title = {SCH-GAN: Semi-Supervised Cross-Modal Hashing by Generative Adversarial Network},\n volume = {50},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:16f48e8b7f1f6c03c888e3f4664ce3fa1261296b",
            "@type": "ScholarlyArticle",
            "paperId": "16f48e8b7f1f6c03c888e3f4664ce3fa1261296b",
            "corpusId": 16502769,
            "url": "https://www.semanticscholar.org/paper/16f48e8b7f1f6c03c888e3f4664ce3fa1261296b",
            "title": "Steganographic generative adversarial networks",
            "venue": "International Conference on Machine Vision",
            "publicationVenue": {
                "id": "urn:research:c4e26c05-cc78-4a34-9cc8-567aa54a8877",
                "name": "International Conference on Machine Vision",
                "alternate_names": [
                    "Int Conf Mach Vis",
                    "ICMV"
                ],
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "MAG": "3004271471",
                "DBLP": "conf/icmv/VolkhonskiyNB19",
                "ArXiv": "1703.05502",
                "DOI": "10.1117/12.2559429",
                "CorpusId": 16502769
            },
            "abstract": "Steganography is collection of methods to hide secret information (\u201cpayload\u201d) within non-secret information \u201ccontainer\u201d). Its counterpart, Steganalysis, is the practice of determining if a message contains a hidden payload, and recovering it if possible. Presence of hidden payloads is typically detected by a binary classifier. In the present study, we propose a new model for generating image-like containers based on Deep Convolutional Generative Adversarial Networks (DCGAN). This approach allows to generate more setganalysis-secure message embedding using standard steganography algorithms. Experiment results demonstrate that the new model successfully deceives the steganography analyzer, and for this reason, can be used in steganographic applications.",
            "referenceCount": 31,
            "citationCount": 156,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1703.05502",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-16",
            "journal": {
                "name": null,
                "volume": "11433"
            },
            "citationStyles": {
                "bibtex": "@Article{Volkhonskiy2017SteganographicGA,\n author = {Denis Volkhonskiy and I. Nazarov and B. Borisenko and Evgeny Burnaev},\n booktitle = {International Conference on Machine Vision},\n pages = {114333M - 114333M-15},\n title = {Steganographic generative adversarial networks},\n volume = {11433},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2056a1eedc7c0e912b68906e218081df824723c4",
            "@type": "ScholarlyArticle",
            "paperId": "2056a1eedc7c0e912b68906e218081df824723c4",
            "corpusId": 1412297,
            "url": "https://www.semanticscholar.org/paper/2056a1eedc7c0e912b68906e218081df824723c4",
            "title": "Generative Adversarial Network for Abstractive Text Summarization",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/corr/abs-1711-09357",
                "MAG": "2963521413",
                "ArXiv": "1711.09357",
                "DOI": "10.1609/aaai.v32i1.12141",
                "CorpusId": 1412297
            },
            "abstract": "\n \n In this paper, we propose an adversarial process for abstractive text summarization, in which we simultaneously train a generative model G and a discriminative model D. In particular, we build the generator G as an agent of reinforcement learning, which takes the raw text as input and predicts the abstractive summarization. We also build a discriminator which attempts to distinguish the generated summary from the ground truth summary. Extensive experiments demonstrate that our model achieves competitive ROUGE scores with the state-of-the-art methods on CNN/Daily Mail dataset. Qualitatively, we show that our model is able to generate more abstractive, readable and diverse summaries.\n \n",
            "referenceCount": 6,
            "citationCount": 149,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/12141/12000",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-26",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Liu2017GenerativeAN,\n author = {Linqing Liu and Yao Lu and Min Yang and Qiang Qu and Jia Zhu and Hongyan Li},\n booktitle = {AAAI Conference on Artificial Intelligence},\n pages = {8109-8110},\n title = {Generative Adversarial Network for Abstractive Text Summarization},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:68ac962c19ab8c924d9beec8473a0ac9f5cfdfa1",
            "@type": "ScholarlyArticle",
            "paperId": "68ac962c19ab8c924d9beec8473a0ac9f5cfdfa1",
            "corpusId": 19153102,
            "url": "https://www.semanticscholar.org/paper/68ac962c19ab8c924d9beec8473a0ac9f5cfdfa1",
            "title": "Unsupervised Generative Adversarial Cross-modal Hashing",
            "venue": "AAAI Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:bdc2e585-4e48-4e36-8af1-6d859763d405",
                "name": "AAAI Conference on Artificial Intelligence",
                "alternate_names": [
                    "National Conference on Artificial Intelligence",
                    "National Conf Artif Intell",
                    "AAAI Conf Artif Intell",
                    "AAAI"
                ],
                "issn": null,
                "url": "http://www.aaai.org/"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/aaai/ZhangPY18",
                "MAG": "2963288100",
                "ArXiv": "1712.00358",
                "DOI": "10.1609/aaai.v32i1.11263",
                "CorpusId": 19153102
            },
            "abstract": "\n \n Cross-modal hashing aims to map heterogeneous multimedia data into a common Hamming space, which can realize fast and flexible retrieval across different modalities. Unsupervised cross-modal hashing is more flexible and applicable than supervised methods, since no intensive labeling work is involved. However, existing unsupervised methods learn hashing functions by preserving inter and intra correlations, while ignoring the underlying manifold structure across different modalities, which is extremely helpful to capture meaningful nearest neighbors of different modalities for cross-modal retrieval. To address the above problem, in this paper we propose an Unsupervised Generative Adversarial Cross-modal Hashing approach (UGACH), which makes full use of GAN's ability for unsupervised representation learning to exploit the underlying manifold structure of cross-modal data. The main contributions can be summarized as follows: (1) We propose a generative adversarial network to model cross-modal hashing in an unsupervised fashion. In the proposed UGACH, given a data of one modality, the generative model tries to fit the distribution over the manifold structure, and select informative data of another modality to challenge the discriminative model. The discriminative model learns to distinguish the generated data and the true positive data sampled from correlation graph to achieve better retrieval accuracy. These two models are trained in an adversarial way to improve each other and promote hashing function learning. (2) We propose a correlation graph based approach to capture the underlying manifold structure across different modalities, so that data of different modalities but within the same manifold can have smaller Hamming distance and promote retrieval accuracy. Extensive experiments compared with 6 state-of-the-art methods on 2 widely-used datasets verify the effectiveness of our proposed approach.\n \n",
            "referenceCount": 28,
            "citationCount": 146,
            "influentialCitationCount": 17,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11263/11122",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-12-01",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1712.00358"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017UnsupervisedGA,\n author = {Jian Zhang and Yuxin Peng and Mingkuan Yuan},\n booktitle = {AAAI Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Unsupervised Generative Adversarial Cross-modal Hashing},\n volume = {abs/1712.00358},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0a2605ca2c38fe45ac87b1d196a322857d8cb912",
            "@type": "ScholarlyArticle",
            "paperId": "0a2605ca2c38fe45ac87b1d196a322857d8cb912",
            "corpusId": 88521144,
            "url": "https://www.semanticscholar.org/paper/0a2605ca2c38fe45ac87b1d196a322857d8cb912",
            "title": "From optimal transport to generative modeling: the VEGAN cookbook",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.07642",
                "MAG": "2620025707",
                "CorpusId": 88521144
            },
            "abstract": "We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution $P_X$ and the latent variable model distribution $P_G$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.",
            "referenceCount": 15,
            "citationCount": 134,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-05-22",
            "journal": {
                "name": "arXiv: Machine Learning",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Article{Bousquet2017FromOT,\n author = {O. Bousquet and S. Gelly and I. Tolstikhin and Carl-Johann Simon-Gabriel and B. Schoelkopf},\n journal = {arXiv: Machine Learning},\n title = {From optimal transport to generative modeling: the VEGAN cookbook},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b7b6437f883148ef56351d8b68466650e3b9f4d5",
            "@type": "ScholarlyArticle",
            "paperId": "b7b6437f883148ef56351d8b68466650e3b9f4d5",
            "corpusId": 8029498,
            "url": "https://www.semanticscholar.org/paper/b7b6437f883148ef56351d8b68466650e3b9f4d5",
            "title": "Context-Aware Generative Adversarial Privacy",
            "venue": "Entropy",
            "publicationVenue": {
                "id": "urn:research:8270cfe1-3713-4325-a7bd-c6a87eed889e",
                "name": "Entropy",
                "alternate_names": null,
                "issn": "1099-4300",
                "url": "http://www.e-helvetica.nb.admin.ch/directAccess?callnumber=bel-155606"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1710.09549",
                "MAG": "3099111404",
                "DBLP": "journals/entropy/HuangKCSR17",
                "DOI": "10.3390/e19120656",
                "CorpusId": 8029498
            },
            "abstract": "Preserving the utility of published datasets while simultaneously providing provable privacy guarantees is a well-known challenge. On the one hand, context-free privacy solutions, such as differential privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility. On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve an improved privacy-utility tradeoff, but assume that the data holder has access to dataset statistics. We circumvent these limitations by introducing a novel context-aware privacy framework called generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial networks (GANs) to allow the data holder to learn privatization schemes from the dataset itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks on the individuals' private variables, and an adversary that tries to infer the private variables from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical) statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model. For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that the privacy mechanisms learned from data (in a generative adversarial fashion) match the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice, even in the absence of dataset statistics.",
            "referenceCount": 101,
            "citationCount": 140,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.mdpi.com/1099-4300/19/12/656/pdf?version=1512392036",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-26",
            "journal": {
                "name": "Entropy",
                "volume": "19"
            },
            "citationStyles": {
                "bibtex": "@Article{Huang2017ContextAwareGA,\n author = {Chong Huang and P. Kairouz and Xiao Chen and L. Sankar and R. Rajagopal},\n booktitle = {Entropy},\n journal = {Entropy},\n pages = {656},\n title = {Context-Aware Generative Adversarial Privacy},\n volume = {19},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c789f8e3f40f57335aef2565f4d2bac69d2941d8",
            "@type": "ScholarlyArticle",
            "paperId": "c789f8e3f40f57335aef2565f4d2bac69d2941d8",
            "corpusId": 1840346,
            "url": "https://www.semanticscholar.org/paper/c789f8e3f40f57335aef2565f4d2bac69d2941d8",
            "title": "LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "DBLP": "journals/corr/YangKBP17",
                "MAG": "2592101326",
                "ArXiv": "1703.01560",
                "CorpusId": 1840346
            },
            "abstract": "We present LR-GAN: an adversarial image generation model which takes scene structure and context into account. Unlike previous generative adversarial networks (GANs), the proposed GAN learns to generate image background and foregrounds separately and recursively, and stitch the foregrounds on the background in a contextually relevant manner to produce a complete natural image. For each foreground, the model learns to generate its appearance, shape and pose. The whole model is unsupervised, and is trained in an end-to-end manner with gradient descent methods. The experiments demonstrate that LR-GAN can generate more natural images with objects that are more human recognizable than DCGAN.",
            "referenceCount": 35,
            "citationCount": 224,
            "influentialCitationCount": 21,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1703.01560"
            },
            "citationStyles": {
                "bibtex": "@Article{Yang2016LRGANLR,\n author = {Jianwei Yang and A. Kannan and Dhruv Batra and Devi Parikh},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {LR-GAN: Layered Recursive Generative Adversarial Networks for Image Generation},\n volume = {abs/1703.01560},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f7bbda432b5aed0c9f767e05850ab29bb2cb3f17",
            "@type": "ScholarlyArticle",
            "paperId": "f7bbda432b5aed0c9f767e05850ab29bb2cb3f17",
            "corpusId": 54148599,
            "url": "https://www.semanticscholar.org/paper/f7bbda432b5aed0c9f767e05850ab29bb2cb3f17",
            "title": "Global-to-local generative model for 3D shapes",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2903545703",
                "DBLP": "journals/tog/WangSHHCH18",
                "DOI": "10.1145/3272127.3275025",
                "CorpusId": 54148599
            },
            "abstract": "We introduce a generative model for 3D man-made shapes. The presented method takes a global-to-local (G2L) approach. An adversarial network (GAN) is built first to construct the overall structure of the shape, segmented and labeled into parts. A novel conditional auto-encoder (AE) is then augmented to act as a part-level refiner. The GAN, associated with additional local discriminators and quality losses, synthesizes a voxel-based model, and assigns the voxels with part labels that are represented in separate channels. The AE is trained to amend the initial synthesis of the parts, yielding more plausible part geometries. We also introduce new means to measure and evaluate the performance of an adversarial generative model. We demonstrate that our global-to-local generative model produces significantly better results than a plain three-dimensional GAN, in terms of both their shape variety and the distribution with respect to the training data.",
            "referenceCount": 44,
            "citationCount": 61,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-12-04",
            "journal": {
                "name": "ACM Transactions on Graphics (TOG)",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2018GlobaltolocalGM,\n author = {Hao Wang and Nadav Schor and Ruizhen Hu and Haibin Huang and D. Cohen-Or and Hui Huang},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 10},\n title = {Global-to-local generative model for 3D shapes},\n volume = {37},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:5cd47e5d004d75fe773a252bde35b56d5d56ce06",
            "@type": "ScholarlyArticle",
            "paperId": "5cd47e5d004d75fe773a252bde35b56d5d56ce06",
            "corpusId": 3559987,
            "url": "https://www.semanticscholar.org/paper/5cd47e5d004d75fe773a252bde35b56d5d56ce06",
            "title": "Conditional generative adversarial nets for convolutional face generation",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "CorpusId": 3559987
            },
            "abstract": "We apply an extension of generative adversarial networks (GANs) [8] to a conditional setting. In the GAN framework, a \u201cgenerator\u201d network is tasked with fooling a \u201cdiscriminator\u201d network into believing that its own samples are real data. We add the capability for each network to condition on some arbitrary external data which describes the image being generated or discriminated. By varying the conditional information provided to this extended GAN, we can use the resulting generative model to generate faces with specific attributes from nothing but random noise. We evaluate the likelihood of real-world faces under the generative model, and examine how to deterministically control face attributes by modifying the conditional information provided to the model.",
            "referenceCount": 21,
            "citationCount": 356,
            "influentialCitationCount": 23,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Gauthier2015ConditionalGA,\n author = {Jon Gauthier},\n title = {Conditional generative adversarial nets for convolutional face generation},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:85568efbb7658c96254b39815a7662f56c9a333d",
            "@type": "ScholarlyArticle",
            "paperId": "85568efbb7658c96254b39815a7662f56c9a333d",
            "corpusId": 3125754,
            "url": "https://www.semanticscholar.org/paper/85568efbb7658c96254b39815a7662f56c9a333d",
            "title": "Deep Generative Dual Memory Network for Continual Learning",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2765895026",
                "DBLP": "journals/corr/abs-1710-10368",
                "ArXiv": "1710.10368",
                "CorpusId": 3125754
            },
            "abstract": "Despite advances in deep learning, artificial neural networks do not learn the same way as humans do. Today, neural networks can learn multiple tasks when trained on them jointly, but cannot maintain performance on learnt tasks when tasks are presented one at a time -- this phenomenon called catastrophic forgetting is a fundamental challenge to overcome before neural networks can learn continually from incoming data. In this work, we derive inspiration from human memory to develop an architecture capable of learning continuously from sequentially incoming tasks, while averting catastrophic forgetting. Specifically, our model consists of a dual memory architecture to emulate the complementary learning systems (hippocampus and the neocortex) in the human brain, and maintains a consolidated long-term memory via generative replay of past experiences. We (i) substantiate our claim that replay should be generative, (ii) show the benefits of generative replay and dual memory via experiments, and (iii) demonstrate improved performance retention even for small models with low capacity. Our architecture displays many important characteristics of the human memory and provides insights on the connection between sleep and learning in humans.",
            "referenceCount": 44,
            "citationCount": 133,
            "influentialCitationCount": 3,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-28",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.10368"
            },
            "citationStyles": {
                "bibtex": "@Article{Kamra2017DeepGD,\n author = {Nitin Kamra and Umang Gupta and Yan Liu},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Deep Generative Dual Memory Network for Continual Learning},\n volume = {abs/1710.10368},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f5e78eb72c3da3f1f6ed1afdd4686927518f85a2",
            "@type": "ScholarlyArticle",
            "paperId": "f5e78eb72c3da3f1f6ed1afdd4686927518f85a2",
            "corpusId": 13795836,
            "url": "https://www.semanticscholar.org/paper/f5e78eb72c3da3f1f6ed1afdd4686927518f85a2",
            "title": "A Generative Model for category text generation",
            "venue": "Information Sciences",
            "publicationVenue": {
                "id": "urn:research:e46002a1-d7a6-4681-aae9-36bc3a6a1f93",
                "name": "Information Sciences",
                "alternate_names": [
                    "Information Scientist",
                    "Inf Sci"
                ],
                "issn": "0020-0255",
                "url": "http://www.sciencedirect.com/science/journal/00200255"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/isci/LiPWYC18",
                "MAG": "2792593939",
                "DOI": "10.1016/J.INS.2018.03.050",
                "CorpusId": 13795836
            },
            "abstract": null,
            "referenceCount": 51,
            "citationCount": 126,
            "influentialCitationCount": 11,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-01",
            "journal": {
                "name": "Inf. Sci.",
                "volume": "450"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2018AGM,\n author = {Yang Li and Q. Pan and Suhang Wang and Tao Yang and E. Cambria},\n booktitle = {Information Sciences},\n journal = {Inf. Sci.},\n pages = {301-315},\n title = {A Generative Model for category text generation},\n volume = {450},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1a7d0b7e47badf47a493ab0b6e6b26ff09d74566",
            "@type": "ScholarlyArticle",
            "paperId": "1a7d0b7e47badf47a493ab0b6e6b26ff09d74566",
            "corpusId": 46948276,
            "url": "https://www.semanticscholar.org/paper/1a7d0b7e47badf47a493ab0b6e6b26ff09d74566",
            "title": "Multi-chart generative surface modeling",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/tog/HamuMKAL18",
                "ArXiv": "1806.02143",
                "MAG": "3101023854",
                "DOI": "10.1145/3272127.3275052",
                "CorpusId": 46948276
            },
            "abstract": "This paper introduces a 3D shape generative model based on deep neural networks. A new image-like (i.e., tensor) data representation for genus-zero 3D shapes is devised. It is based on the observation that complicated shapes can be well represented by multiple parameterizations (charts), each focusing on a different part of the shape. The new tensor data representation is used as input to Generative Adversarial Networks for the task of 3D shape generation. The 3D shape tensor representation is based on a multi-chart structure that enjoys a shape covering property and scale-translation rigidity. Scale-translation rigidity facilitates high quality 3D shape learning and guarantees unique reconstruction. The multi-chart structure uses as input a dataset of 3D shapes (with arbitrary connectivity) and a sparse correspondence between them. The output of our algorithm is a generative model that learns the shape distribution and is able to generate novel shapes, interpolate shapes, and explore the generated shape space. The effectiveness of the method is demonstrated for the task of anatomic shape generation including human body and bone (teeth) shape generation.",
            "referenceCount": 48,
            "citationCount": 65,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1806.02143",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-06-06",
            "journal": {
                "name": "ACM Transactions on Graphics (TOG)",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Ben-Hamu2018MultichartGS,\n author = {Heli Ben-Hamu and Haggai Maron and Itay Kezurer and G. Avineri and Y. Lipman},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 15},\n title = {Multi-chart generative surface modeling},\n volume = {37},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2afa9966c37b7747d954a4dcd61e986247783683",
            "@type": "ScholarlyArticle",
            "paperId": "2afa9966c37b7747d954a4dcd61e986247783683",
            "corpusId": 125920802,
            "url": "https://www.semanticscholar.org/paper/2afa9966c37b7747d954a4dcd61e986247783683",
            "title": "GraphRNN: A Deep Generative Model for Graphs",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "journals/corr/abs-1802-08773",
                "MAG": "2963685158",
                "CorpusId": 125920802
            },
            "abstract": "Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. \nIn order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.",
            "referenceCount": 33,
            "citationCount": 96,
            "influentialCitationCount": 19,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-02-24",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1802.08773"
            },
            "citationStyles": {
                "bibtex": "@Article{You2018GraphRNNAD,\n author = {Jiaxuan You and Rex Ying and Xiang Ren and William L. Hamilton and J. Leskovec},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {GraphRNN: A Deep Generative Model for Graphs},\n volume = {abs/1802.08773},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:90a3d7563104512c390aa80cc22f6904d0028d96",
            "@type": "ScholarlyArticle",
            "paperId": "90a3d7563104512c390aa80cc22f6904d0028d96",
            "corpusId": 10775694,
            "url": "https://www.semanticscholar.org/paper/90a3d7563104512c390aa80cc22f6904d0028d96",
            "title": "Online generative model personalization for hand tracking",
            "venue": "ACM Transactions on Graphics",
            "publicationVenue": {
                "id": "urn:research:aab03e41-f80d-48b3-89bd-60eeeceafc7d",
                "name": "ACM Transactions on Graphics",
                "alternate_names": [
                    "ACM Trans Graph"
                ],
                "issn": "0730-0301",
                "url": "http://www.acm.org/tog/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2770331385",
                "DBLP": "journals/tog/TkachTRPF17",
                "DOI": "10.1145/3130800.3130830",
                "CorpusId": 10775694
            },
            "abstract": "We present a new algorithm for real-time hand tracking on commodity depth-sensing devices. Our method does not require a user-specific calibration session, but rather learns the geometry as the user performs live in front of the camera, thus enabling seamless virtual interaction at the consumer level. The key novelty in our approach is an online optimization algorithm that jointly estimates pose and shape in each frame, and determines the uncertainty in such estimates. This knowledge allows the algorithm to integrate per-frame estimates over time, and build a personalized geometric model of the captured user. Our approach can easily be integrated in state-of-the-art continuous generative motion tracking software. We provide a detailed evaluation that shows how our approach achieves accurate motion tracking for real-time applications, while significantly simplifying the workflow of accurate hand performance capture. We also provide quantitative evaluation datasets at http://gfx.uvic.ca/datasets/handy",
            "referenceCount": 44,
            "citationCount": 87,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://infoscience.epfl.ch/record/233927/files/paper.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-11-20",
            "journal": {
                "name": "ACM Transactions on Graphics (TOG)",
                "volume": "36"
            },
            "citationStyles": {
                "bibtex": "@Article{Tkach2017OnlineGM,\n author = {Anastasia Tkach and A. Tagliasacchi and Edoardo Remelli and M. Pauly and A. Fitzgibbon},\n booktitle = {ACM Transactions on Graphics},\n journal = {ACM Transactions on Graphics (TOG)},\n pages = {1 - 11},\n title = {Online generative model personalization for hand tracking},\n volume = {36},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f792f75f6d2bf265569d4e63dd139c4d04ec7fdb",
            "@type": "ScholarlyArticle",
            "paperId": "f792f75f6d2bf265569d4e63dd139c4d04ec7fdb",
            "corpusId": 11697881,
            "url": "https://www.semanticscholar.org/paper/f792f75f6d2bf265569d4e63dd139c4d04ec7fdb",
            "title": "Introspective Neural Networks for Generative Modeling",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2780786077",
                "DBLP": "conf/iccv/LazarowJT17",
                "DOI": "10.1109/ICCV.2017.302",
                "CorpusId": 11697881
            },
            "abstract": "We study unsupervised learning by developing a generative model built from progressively learned deep convolutional neural networks. The resulting generator is additionally a discriminator, capable of \"introspection\" in a sense \u2014 being able to self-evaluate the difference between its generated samples and the given training data. Through repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. Specifically, our model learns a sequence of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and unsupervised feature learning.",
            "referenceCount": 50,
            "citationCount": 64,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-10-01",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Lazarow2017IntrospectiveNN,\n author = {Justin Lazarow and Long Jin and Z. Tu},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {2793-2802},\n title = {Introspective Neural Networks for Generative Modeling},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
            "@type": "ScholarlyArticle",
            "paperId": "67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
            "corpusId": 11091552,
            "url": "https://www.semanticscholar.org/paper/67cab3bafc8fa9e1ae3ff89791ad43c81441d271",
            "title": "TransG : A Generative Model for Knowledge Graph Embedding",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2015,
            "externalIds": {
                "ACL": "P16-1219",
                "ArXiv": "1509.05488",
                "DBLP": "conf/acl/0005HZ16",
                "MAG": "2514852614",
                "DOI": "10.18653/v1/P16-1219",
                "CorpusId": 11091552
            },
            "abstract": "Recently, knowledge graph embedding, which projects symbolic entities and relations into continuous vector space, has become a new, hot topic in artificial intelligence. This paper addresses a new issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples, and proposes a novel Gaussian mixture model for embedding, TransG. The new model can discover latent semantics for a relation and leverage a mixture of relation component vectors for embedding a fact triple. To the best of our knowledge, this is the first generative model for knowledge graph embedding, which is able to deal with multiple relation semantics. Extensive experiments show that the proposed model achieves substantial improvements against the state-of-the-art baselines.",
            "referenceCount": 39,
            "citationCount": 249,
            "influentialCitationCount": 26,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/P16-1219.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-09-18",
            "journal": {
                "name": "",
                "volume": "1"
            },
            "citationStyles": {
                "bibtex": "@Article{Xiao2015TransGA,\n author = {Han Xiao and Minlie Huang and Xiaoyan Zhu},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {2316-2325},\n title = {TransG : A Generative Model for Knowledge Graph Embedding},\n volume = {1},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:932b72d84245accd9f2bed3a1e828ff0e9697025",
            "@type": "ScholarlyArticle",
            "paperId": "932b72d84245accd9f2bed3a1e828ff0e9697025",
            "corpusId": 43676739,
            "url": "https://www.semanticscholar.org/paper/932b72d84245accd9f2bed3a1e828ff0e9697025",
            "title": "A Geometric View of Optimal Transportation and Generative Model",
            "venue": "Computer Aided Geometric Design",
            "publicationVenue": {
                "id": "urn:research:54aca59a-f906-42b2-a54f-d50f044aa9e6",
                "name": "Computer Aided Geometric Design",
                "alternate_names": [
                    "Comput Aided Geom Des"
                ],
                "issn": "0167-8396",
                "url": "http://www.elsevier.com/wps/find/journaldescription.cws_home/505604/description#description"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "journals/cagd/LeiSCYG19",
                "ArXiv": "1710.05488",
                "MAG": "2963875852",
                "DOI": "10.1016/j.cagd.2018.10.005",
                "CorpusId": 43676739
            },
            "abstract": null,
            "referenceCount": 58,
            "citationCount": 122,
            "influentialCitationCount": 6,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://manuscript.elsevier.com/S0167839618301249/pdf/S0167839618301249.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-16",
            "journal": {
                "name": "Comput. Aided Geom. Des.",
                "volume": "68"
            },
            "citationStyles": {
                "bibtex": "@Article{Lei2017AGV,\n author = {Na Lei and Kehua Su and Li Cui and S. Yau and X. Gu},\n booktitle = {Computer Aided Geometric Design},\n journal = {Comput. Aided Geom. Des.},\n pages = {1-21},\n title = {A Geometric View of Optimal Transportation and Generative Model},\n volume = {68},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:b8dc4ae9de13be5f4c8333b026cb3ad956ae5b65",
            "@type": "ScholarlyArticle",
            "paperId": "b8dc4ae9de13be5f4c8333b026cb3ad956ae5b65",
            "corpusId": 14039866,
            "url": "https://www.semanticscholar.org/paper/b8dc4ae9de13be5f4c8333b026cb3ad956ae5b65",
            "title": "Neural Generative Question Answering",
            "venue": "International Joint Conference on Artificial Intelligence",
            "publicationVenue": {
                "id": "urn:research:67f7f831-711a-43c8-8785-1e09005359b5",
                "name": "International Joint Conference on Artificial Intelligence",
                "alternate_names": [
                    "Int Jt Conf Artif Intell",
                    "IJCAI"
                ],
                "issn": null,
                "url": "http://www.ijcai.org/"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/ijcai/YinJLSLL16",
                "ArXiv": "1512.01337",
                "MAG": "2950583024",
                "ACL": "W16-0106",
                "DOI": "10.18653/v1/W16-0106",
                "CorpusId": 14039866
            },
            "abstract": "This paper presents an end-to-end neural network model, named Neural Generative Question Answering (GENQA), that can generate answers to simple factoid questions, based on the facts in a knowledge-base. More specifically, the model is built on the encoder-decoder framework for sequence-to-sequence learning, while equipped with the ability to enquire the knowledge-base, and is trained on a corpus of question-answer pairs, with their associated triples in the knowledge-base. Empirical study shows the proposed model can effectively deal with the variations of questions and answers, and generate right and natural answers by referring to the facts in the knowledge-base. The experiment on question answering demonstrates that the proposed model can outperform an embedding-based QA model as well as a neural dialogue model trained on the same data.",
            "referenceCount": 30,
            "citationCount": 202,
            "influentialCitationCount": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/W16-0106.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-12-04",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1512.01337"
            },
            "citationStyles": {
                "bibtex": "@Article{Yin2015NeuralGQ,\n author = {Jun Yin and Xin Jiang and Zhengdong Lu and Lifeng Shang and Hang Li and Xiaoming Li},\n booktitle = {International Joint Conference on Artificial Intelligence},\n journal = {ArXiv},\n title = {Neural Generative Question Answering},\n volume = {abs/1512.01337},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2b93c4c99559d5f3d12568cb42bfa29391bd1d38",
            "@type": "ScholarlyArticle",
            "paperId": "2b93c4c99559d5f3d12568cb42bfa29391bd1d38",
            "corpusId": 90802459,
            "url": "https://www.semanticscholar.org/paper/2b93c4c99559d5f3d12568cb42bfa29391bd1d38",
            "title": "CytoGAN: Generative Modeling of Cell Images",
            "venue": "bioRxiv",
            "publicationVenue": {
                "id": "urn:research:027ffd21-ebb0-4af8-baf5-911124292fd0",
                "name": "bioRxiv",
                "alternate_names": null,
                "issn": null,
                "url": "http://biorxiv.org/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2775908481",
                "DOI": "10.1101/227645",
                "CorpusId": 90802459
            },
            "abstract": "We explore the application of Generative Adversarial Networks to the domain of morphological profiling of human cultured cells imaged by fluorescence microscopy. When evaluated for their ability to group cell images responding to treatment by chemicals of known classes, we find that adversarially learned representations are superior to autoencoder-based approaches. While currently inferior to classical computer vision and transfer learning, the adversarial framework enables useful visualization of the variation of cellular images due to their generative capabilities.",
            "referenceCount": 18,
            "citationCount": 57,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2017/12/02/227645.full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2017-12-02",
            "journal": {
                "name": "bioRxiv",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Goldsborough2017CytoGANGM,\n author = {Peter Goldsborough and Nick Pawlowski and Juan C. Caicedo and Shantanu Singh and Anne E Carpenter},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {CytoGAN: Generative Modeling of Cell Images},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:dcb6455d985788c6309dbb61786340d6a32426f6",
            "@type": "ScholarlyArticle",
            "paperId": "dcb6455d985788c6309dbb61786340d6a32426f6",
            "corpusId": 9610402,
            "url": "https://www.semanticscholar.org/paper/dcb6455d985788c6309dbb61786340d6a32426f6",
            "title": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2130485404",
                "DBLP": "journals/corr/KimRS15",
                "ArXiv": "1503.01161",
                "CorpusId": 9610402
            },
            "abstract": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the \"quintessential\" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.",
            "referenceCount": 32,
            "citationCount": 281,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-12-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Kim2014TheBC,\n author = {Been Kim and C. Rudin and J. Shah},\n booktitle = {Neural Information Processing Systems},\n pages = {1952-1960},\n title = {The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:6fb2fc8c515a05a6e4b964331c3e5ebe38bb1bf0",
            "@type": "ScholarlyArticle",
            "paperId": "6fb2fc8c515a05a6e4b964331c3e5ebe38bb1bf0",
            "corpusId": 16240824,
            "url": "https://www.semanticscholar.org/paper/6fb2fc8c515a05a6e4b964331c3e5ebe38bb1bf0",
            "title": "COM: a generative model for group recommendation",
            "venue": "Knowledge Discovery and Data Mining",
            "publicationVenue": {
                "id": "urn:research:a0edb93b-1e95-4128-a295-6b1659149cef",
                "name": "Knowledge Discovery and Data Mining",
                "alternate_names": [
                    "KDD",
                    "Knowl Discov Data Min"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigkdd/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2064702560",
                "DBLP": "conf/kdd/YuanCL14",
                "DOI": "10.1145/2623330.2623616",
                "CorpusId": 16240824
            },
            "abstract": "With the rapid development of online social networks, a growing number of people are willing to share their group activities, e.g. having dinners with colleagues, and watching movies with spouses. This motivates the studies on group recommendation, which aims to recommend items for a group of users. Group recommendation is a challenging problem because different group members have different preferences, and how to make a trade-off among their preferences for recommendation is still an open problem. In this paper, we propose a probabilistic model named COM (COnsensus Model) to model the generative process of group activities, and make group recommendations. Intuitively, users in a group may have different influences, and those who are expert in topics relevant to the group are usually more influential. In addition, users in a group may behave differently as group members from as individuals. COM is designed based on these intuitions, and is able to incorporate both users' selection history and personal considerations of content factors. When making recommendations, COM estimates the preference of a group to an item by aggregating the preferences of the group members with different weights. We conduct extensive experiments on four datasets, and the results show that the proposed model is effective in making group recommendations, and outperforms baseline methods significantly.",
            "referenceCount": 34,
            "citationCount": 170,
            "influentialCitationCount": 34,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-08-24",
            "journal": {
                "name": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Yuan2014COMAG,\n author = {Quan Yuan and G. Cong and Chin-Yew Lin},\n booktitle = {Knowledge Discovery and Data Mining},\n journal = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},\n title = {COM: a generative model for group recommendation},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:83eedad58bbc3c88fa656af8e10bec1490b185a5",
            "@type": "ScholarlyArticle",
            "paperId": "83eedad58bbc3c88fa656af8e10bec1490b185a5",
            "corpusId": 22723179,
            "url": "https://www.semanticscholar.org/paper/83eedad58bbc3c88fa656af8e10bec1490b185a5",
            "title": "A Generative Probabilistic Model and Discriminative Extensions for Brain Lesion Segmentation\u2014 With Application to Tumor and Stroke",
            "venue": "IEEE Transactions on Medical Imaging",
            "publicationVenue": {
                "id": "urn:research:e0cda45d-3074-4ac0-80b8-e5250df00b89",
                "name": "IEEE Transactions on Medical Imaging",
                "alternate_names": [
                    "IEEE Trans Med Imaging"
                ],
                "issn": "0278-0062",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2747144133",
                "DBLP": "journals/tmi/MenzeLLRGAGWWSA16",
                "DOI": "10.1109/TMI.2015.2502596",
                "CorpusId": 22723179,
                "PubMed": "26599702"
            },
            "abstract": "We introduce a generative probabilistic model for segmentation of brain lesions in multi-dimensional images that generalizes the EM segmenter, a common approach for modelling brain images using Gaussian mixtures and a probabilistic tissue atlas that employs expectation-maximization (EM), to estimate the label map for a new image. Our model augments the probabilistic atlas of the healthy tissues with a latent atlas of the lesion. We derive an estimation algorithm with closed-form EM update equations. The method extracts a latent atlas prior distribution and the lesion posterior distributions jointly from the image data. It delineates lesion areas individually in each channel, allowing for differences in lesion appearance across modalities, an important feature of many brain tumor imaging sequences. We also propose discriminative model extensions to map the output of the generative model to arbitrary labels with semantic and biological meaning, such as \u201ctumor core\u201d or \u201cfluid-filled structure\u201d, but without a one-to-one correspondence to the hypo- or hyper-intense lesion areas identified by the generative model. We test the approach in two image sets: the publicly available BRATS set of glioma patient scans, and multimodal brain images of patients with acute and subacute ischemic stroke. We find the generative model that has been designed for tumor lesions to generalize well to stroke images, and the extended discriminative -discriminative model to be one of the top ranking methods in the BRATS evaluation.",
            "referenceCount": 80,
            "citationCount": 81,
            "influentialCitationCount": 4,
            "isOpenAccess": true,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-04-01",
            "journal": {
                "name": "IEEE Transactions on Medical Imaging",
                "volume": "35"
            },
            "citationStyles": {
                "bibtex": "@Article{Menze2016AGP,\n author = {Bjoern H Menze and K. V. Leemput and D. Lashkari and Tammy Riklin-Raviv and Ezequiel Geremia and Esther Alberts and P. Gruber and S. Wegener and M. Weber and G. Sz\u00e9kely and N. Ayache and P. Golland},\n booktitle = {IEEE Transactions on Medical Imaging},\n journal = {IEEE Transactions on Medical Imaging},\n pages = {933-946},\n title = {A Generative Probabilistic Model and Discriminative Extensions for Brain Lesion Segmentation\u2014 With Application to Tumor and Stroke},\n volume = {35},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7754596e9cd3e616c75c2d04825973091e9ebc82",
            "@type": "ScholarlyArticle",
            "paperId": "7754596e9cd3e616c75c2d04825973091e9ebc82",
            "corpusId": 831675,
            "url": "https://www.semanticscholar.org/paper/7754596e9cd3e616c75c2d04825973091e9ebc82",
            "title": "Lifelong Generative Modeling",
            "venue": "Neurocomputing",
            "publicationVenue": {
                "id": "urn:research:df12d289-f447-47d3-8846-75e39de3ab57",
                "name": "Neurocomputing",
                "alternate_names": null,
                "issn": "0925-2312",
                "url": "http://www.elsevier.com/locate/neucom"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1705.09847",
                "MAG": "2617118670",
                "DBLP": "journals/corr/RamapuramGK17",
                "DOI": "10.1016/J.NEUCOM.2020.02.115",
                "CorpusId": 831675
            },
            "abstract": null,
            "referenceCount": 167,
            "citationCount": 79,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1705.09847",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-05-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1705.09847"
            },
            "citationStyles": {
                "bibtex": "@Article{Ramapuram2017LifelongGM,\n author = {Jason Ramapuram and Magda Gregorov\u00e1 and Alexandros Kalousis},\n booktitle = {Neurocomputing},\n journal = {ArXiv},\n title = {Lifelong Generative Modeling},\n volume = {abs/1705.09847},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fcee052adc792f72c1c5b7c7a015744794c6748a",
            "@type": "ScholarlyArticle",
            "paperId": "fcee052adc792f72c1c5b7c7a015744794c6748a",
            "corpusId": 67748413,
            "url": "https://www.semanticscholar.org/paper/fcee052adc792f72c1c5b7c7a015744794c6748a",
            "title": "Generative Model",
            "venue": "Encyclopedia of Social Network Analysis and Mining. 2nd Ed.",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2018,
            "externalIds": {
                "DBLP": "reference/snam/X18mf",
                "DOI": "10.1007/978-1-4939-7131-2_100405",
                "CorpusId": 67748413
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 21,
            "influentialCitationCount": 2,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Storkey2018GenerativeM,\n author = {A. Storkey},\n booktitle = {Encyclopedia of Social Network Analysis and Mining. 2nd Ed.},\n title = {Generative Model},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2822a883d149956934a20614d6934c6ddaac6857",
            "@type": "ScholarlyArticle",
            "paperId": "2822a883d149956934a20614d6934c6ddaac6857",
            "corpusId": 10355303,
            "url": "https://www.semanticscholar.org/paper/2822a883d149956934a20614d6934c6ddaac6857",
            "title": "A survey of appearance models in visual object tracking",
            "venue": "ACM Transactions on Intelligent Systems and Technology",
            "publicationVenue": {
                "id": "urn:research:0d993d4a-09ba-4df8-90a4-7dfe25f0cb9e",
                "name": "ACM Transactions on Intelligent Systems and Technology",
                "alternate_names": [
                    "ACM Trans Intell Syst Technol"
                ],
                "issn": "2157-6904",
                "url": "http://portal.acm.org/tist"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "1984914017",
                "DBLP": "journals/tist/LiHSZDH13",
                "ArXiv": "1303.4803",
                "DOI": "10.1145/2508037.2508039",
                "CorpusId": 10355303
            },
            "abstract": "Visual object tracking is a significant computer vision task which can be applied to many domains, such as visual surveillance, human computer interaction, and video compression. Despite extensive research on this topic, it still suffers from difficulties in handling complex object appearance changes caused by factors such as illumination variation, partial occlusion, shape deformation, and camera motion. Therefore, effective modeling of the 2D appearance of tracked objects is a key issue for the success of a visual tracker. In the literature, researchers have proposed a variety of 2D appearance models. To help readers swiftly learn the recent advances in 2D appearance models for visual object tracking, we contribute this survey, which provides a detailed review of the existing 2D appearance models. In particular, this survey takes a module-based architecture that enables readers to easily grasp the key points of visual object tracking. In this survey, we first decompose the problem of appearance modeling into two different processing stages: visual representation and statistical modeling. Then, different 2D appearance models are categorized and discussed with respect to their composition modules. Finally, we address several issues of interest as well as the remaining challenges for future research on this topic. The contributions of this survey are fourfold. First, we review the literature of visual representations according to their feature-construction mechanisms (i.e., local and global). Second, the existing statistical modeling schemes for tracking-by-detection are reviewed according to their model-construction mechanisms: generative, discriminative, and hybrid generative-discriminative. Third, each type of visual representations or statistical modeling techniques is analyzed and discussed from a theoretical or practical viewpoint. Fourth, the existing benchmark resources (e.g., source codes and video datasets) are examined in this survey.",
            "referenceCount": 226,
            "citationCount": 740,
            "influentialCitationCount": 27,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1303.4803",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2013-03-19",
            "journal": {
                "name": "ACM Transactions on Intelligent Systems and Technology (TIST)",
                "volume": "4"
            },
            "citationStyles": {
                "bibtex": "@Article{Li2013ASO,\n author = {Xi Li and Weiming Hu and Chunhua Shen and Zhongfei Zhang and A. Dick and A. Hengel},\n booktitle = {ACM Transactions on Intelligent Systems and Technology},\n journal = {ACM Transactions on Intelligent Systems and Technology (TIST)},\n pages = {1 - 48},\n title = {A survey of appearance models in visual object tracking},\n volume = {4},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2f8c0531cdb6217a08da6cf21c2640d99a5ad1b7",
            "@type": "ScholarlyArticle",
            "paperId": "2f8c0531cdb6217a08da6cf21c2640d99a5ad1b7",
            "corpusId": 15285520,
            "url": "https://www.semanticscholar.org/paper/2f8c0531cdb6217a08da6cf21c2640d99a5ad1b7",
            "title": "Generative Modeling of Convolutional Neural Networks",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2014,
            "externalIds": {
                "MAG": "2952312143",
                "ArXiv": "1412.6296",
                "DBLP": "journals/corr/DaiW14",
                "DOI": "10.4310/SII.2016.V9.N4.A8",
                "CorpusId": 15285520
            },
            "abstract": "The convolutional neural networks (CNNs) have proven to be a powerful tool for discriminative learning. Recently researchers have also started to show interest in the generative aspects of CNNs in order to gain a deeper understanding of what they have learned and how to further improve them. This paper investigates generative modeling of CNNs. The main contributions include: (1) We construct a generative model for the CNN in the form of exponential tilting of a reference distribution. (2) We propose a generative gradient for pre-training CNNs by a non-parametric importance sampling scheme, which is fundamentally different from the commonly used discriminative gradient, and yet has the same computational architecture and cost as the latter. (3) We propose a generative visualization method for the CNNs by sampling from an explicit parametric image distribution. The proposed visualization method can directly draw synthetic samples for any given node in a trained CNN by the Hamiltonian Monte Carlo (HMC) algorithm, without resorting to any extra hold-out images. Experiments on the challenging ImageNet benchmark show that the proposed generative gradient pre-training consistently helps improve the performances of CNNs, and the proposed generative visualization method generates meaningful and varied samples of synthetic images from a large-scale deep CNN.",
            "referenceCount": 28,
            "citationCount": 73,
            "influentialCitationCount": 3,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.intlpress.com/site/pub/files/_fulltext/journals/sii/2016/0009/0004/SII-2016-0009-0004-a008.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2014-12-19",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1412.6296"
            },
            "citationStyles": {
                "bibtex": "@Article{Dai2014GenerativeMO,\n author = {Jifeng Dai and Y. Wu},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Generative Modeling of Convolutional Neural Networks},\n volume = {abs/1412.6296},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:38c6319fb1b241380769202a873a88a1c44bad25",
            "@type": "ScholarlyArticle",
            "paperId": "38c6319fb1b241380769202a873a88a1c44bad25",
            "corpusId": 16068754,
            "url": "https://www.semanticscholar.org/paper/38c6319fb1b241380769202a873a88a1c44bad25",
            "title": "A Type Composition Logic for Generative Lexicon",
            "venue": "Advances in Generative Lexicon Theory",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2404975721",
                "DBLP": "books/daglib/p/AsherP13",
                "DOI": "10.1007/978-94-007-5189-7_3",
                "CorpusId": 16068754
            },
            "abstract": null,
            "referenceCount": 60,
            "citationCount": 72,
            "influentialCitationCount": 8,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Asher2013ATC,\n author = {Nicholas Asher and J. Pustejovsky},\n booktitle = {Advances in Generative Lexicon Theory},\n pages = {39-66},\n title = {A Type Composition Logic for Generative Lexicon},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d2aa1fe60882aa94cb5f2ee62a2aa9db4c37b04",
            "@type": "ScholarlyArticle",
            "paperId": "0d2aa1fe60882aa94cb5f2ee62a2aa9db4c37b04",
            "corpusId": 145448939,
            "url": "https://www.semanticscholar.org/paper/0d2aa1fe60882aa94cb5f2ee62a2aa9db4c37b04",
            "title": "Dynamics of Organizational Routines: A Generative Model",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2112321795",
                "DOI": "10.1111/j.1467-6486.2012.01064.x",
                "CorpusId": 145448939
            },
            "abstract": "This paper introduces a generative model of organizational routines and their change over time. The model demonstrates that variation and selective retention of patterns of action are necessary and sufficient to explain the features of organizational routines that are most relevant in relation to dynamic capabilities, such as formation, inertia, endogenous change, and learning. The model directly links micro\u2010level actions to the macro\u2010level dynamics of routines. The results suggest that focusing on action provides a useful and parsimonious foundation for a theory of organizational routines and capabilities.",
            "referenceCount": 113,
            "citationCount": 407,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Business",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2012-12-01",
            "journal": {
                "name": "ORG: Emerging Research Methodologies in Organizational Behavior (Topic)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Pentland2012DynamicsOO,\n author = {B. Pentland and M. Feldman and Markus C. Becker and Peng Liu},\n journal = {ORG: Emerging Research Methodologies in Organizational Behavior (Topic)},\n title = {Dynamics of Organizational Routines: A Generative Model},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9285f3c70c6cab770d2f7d4ccb32ecae0397b134",
            "@type": "ScholarlyArticle",
            "paperId": "9285f3c70c6cab770d2f7d4ccb32ecae0397b134",
            "corpusId": 16901637,
            "url": "https://www.semanticscholar.org/paper/9285f3c70c6cab770d2f7d4ccb32ecae0397b134",
            "title": "Exploring social influence for recommendation: a generative model approach",
            "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
            "publicationVenue": {
                "id": "urn:research:8dce23a9-44e0-4381-a39e-2acc1edff700",
                "name": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "alternate_names": [
                    "International ACM SIGIR Conference on Research and Development in Information Retrieval",
                    "Int ACM SIGIR Conf Res Dev Inf Retr",
                    "SIGIR",
                    "Annu Int ACM SIGIR Conf Res Dev Inf Retr"
                ],
                "issn": null,
                "url": "http://www.acm.org/sigir/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2080567541",
                "DBLP": "conf/sigir/YeLL12",
                "DOI": "10.1145/2348283.2348373",
                "CorpusId": 16901637
            },
            "abstract": "Social friendship has been shown beneficial for item recommendation for years. However, existing approaches mostly incorporate social friendship into recommender systems by heuristics. In this paper, we argue that social influence between friends can be captured quantitatively and propose a probabilistic generative model, called social influenced selection(SIS), to model the decision making of item selection (e.g., what book to buy or where to dine). Based on SIS, we mine the social influence between linked friends and the personal preferences of users through statistical inference. To address the challenges arising from multiple layers of hidden factors in SIS, we develop a new parameter learning algorithm based on expectation maximization (EM). Moreover, we show that the mined social influence and user preferences are valuable for group recommendation and viral marketing. Finally, we conduct a comprehensive performance evaluation using real datasets crawled from last.fm and whrrl.com to validate our proposal. Experimental results show that social influence captured based on our SIS model is effective for enhancing both item recommendation and group recommendation, essential for viral marketing, and useful for various user analysis.",
            "referenceCount": 34,
            "citationCount": 293,
            "influentialCitationCount": 10,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2012-08-12",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Ye2012ExploringSI,\n author = {Mao Ye and Xingjie Liu and Wang-Chien Lee},\n booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},\n pages = {671-680},\n title = {Exploring social influence for recommendation: a generative model approach},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:af61e43831d35ec4f4db5e60f227629d2f39a437",
            "@type": "ScholarlyArticle",
            "paperId": "af61e43831d35ec4f4db5e60f227629d2f39a437",
            "corpusId": 124424860,
            "url": "https://www.semanticscholar.org/paper/af61e43831d35ec4f4db5e60f227629d2f39a437",
            "title": "On the Sample Complexity of Reinforcement Learning with a Generative Model",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2012,
            "externalIds": {
                "MAG": "2951326042",
                "DBLP": "conf/icml/AzarMK12",
                "ArXiv": "1206.6461",
                "CorpusId": 124424860
            },
            "abstract": "We consider the problem of learning the optimal action-value function in the discounted-reward Markov decision processes (MDPs). We prove a new PAC bound on the sample-complexity of model-based value iteration algorithm in the presence of the generative model, which indicates that for an MDP with N state-action pairs and the discount factor \u03b3 \u2208 [0, 1) only O(N log(N/\u03b4)/(1 - \u03b3)3e2)) samples are required to find an e-optimal estimation of the action-value function with the probability 1 - \u03b4. We also prove a matching lower bound of \u0398(N log(N/\u03b4)/((1 - \u03b3)3e2)) on the sample complexity of estimating the optimal action-value function by every RL algorithm. To the best of our knowledge, this is the first matching result on the sample complexity of estimating the optimal (action-) value function in which the upper bound matches the lower bound of RL in terms of N, e, \u03b4 and 1/(1-\u03b3). Also, both our lower bound and our upper bound significantly improve on the state-of-the-art in terms of 1/(1 - \u03b3).",
            "referenceCount": 19,
            "citationCount": 216,
            "influentialCitationCount": 43,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-06-26",
            "journal": {
                "name": "",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Azar2012OnTS,\n author = {M. G. Azar and R. Munos and H. Kappen},\n booktitle = {International Conference on Machine Learning},\n pages = {1707-1714},\n title = {On the Sample Complexity of Reinforcement Learning with a Generative Model},\n volume = {29},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9cd5780f6b33aca0833b68eb50f309f953c44cf6",
            "@type": "ScholarlyArticle",
            "paperId": "9cd5780f6b33aca0833b68eb50f309f953c44cf6",
            "corpusId": 149361007,
            "url": "https://www.semanticscholar.org/paper/9cd5780f6b33aca0833b68eb50f309f953c44cf6",
            "title": "Generative social science: Studies in agent-based computational modeling",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2008,
            "externalIds": {
                "MAG": "1595959637",
                "DOI": "10.1111/J.1467-9787.2008.00591_1.X",
                "CorpusId": 149361007
            },
            "abstract": null,
            "referenceCount": 0,
            "citationCount": 922,
            "influentialCitationCount": 66,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Sociology",
                "Political Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Sociology",
                    "source": "external"
                },
                {
                    "category": "Political Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": "2008-10-01",
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Batty2008GenerativeSS,\n author = {M. Batty},\n title = {Generative social science: Studies in agent-based computational modeling},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:bd6023341be8105298568655a5446e225da07e03",
            "@type": "ScholarlyArticle",
            "paperId": "bd6023341be8105298568655a5446e225da07e03",
            "corpusId": 570516,
            "url": "https://www.semanticscholar.org/paper/bd6023341be8105298568655a5446e225da07e03",
            "title": "Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2007,
            "externalIds": {
                "DBLP": "conf/iccv/ChumPSIZ07",
                "MAG": "2100398441",
                "DOI": "10.1109/ICCV.2007.4408891",
                "CorpusId": 570516
            },
            "abstract": "Given a query image of an object, our objective is to retrieve all instances of that object in a large (1M+) image database. We adopt the bag-of-visual-words architecture which has proven successful in achieving high precision at low recall. Unfortunately, feature detection and quantization are noisy processes and this can result in variation in the particular visual words that appear in different images of the same object, leading to missed results. In the text retrieval literature a standard method for improving performance is query expansion. A number of the highly ranked documents from the original query are reissued as a new query. In this way, additional relevant terms can be added to the query. This is a form of blind rele- vance feedback and it can fail if 'outlier' (false positive) documents are included in the reissued query. In this paper we bring query expansion into the visual domain via two novel contributions. Firstly, strong spatial constraints between the query image and each result allow us to accurately verify each return, suppressing the false positives which typically ruin text-based query expansion. Secondly, the verified images can be used to learn a latent feature model to enable the controlled construction of expanded queries. We illustrate these ideas on the 5000 annotated image Oxford building database together with more than 1M Flickr images. We show that the precision is substantially boosted, achieving total recall in many cases.",
            "referenceCount": 19,
            "citationCount": 928,
            "influentialCitationCount": 87,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2007-12-26",
            "journal": {
                "name": "2007 IEEE 11th International Conference on Computer Vision",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Chum2007TotalRA,\n author = {Ond\u0159ej Chum and James Philbin and Josef Sivic and M. Isard and Andrew Zisserman},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2007 IEEE 11th International Conference on Computer Vision},\n pages = {1-8},\n title = {Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:966eb7a1aad596d324b9e4330927246642998f72",
            "@type": "ScholarlyArticle",
            "paperId": "966eb7a1aad596d324b9e4330927246642998f72",
            "corpusId": 13976978,
            "url": "https://www.semanticscholar.org/paper/966eb7a1aad596d324b9e4330927246642998f72",
            "title": "Scene Classification Using a Hybrid Generative/Discriminative Approach",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "publicationVenue": {
                "id": "urn:research:25248f80-fe99-48e5-9b8e-9baef3b8e23b",
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "alternate_names": [
                    "IEEE Trans Pattern Anal Mach Intell"
                ],
                "issn": "0162-8828",
                "url": "http://www.computer.org/tpami/"
            },
            "year": 2008,
            "externalIds": {
                "MAG": "2125574651",
                "DBLP": "journals/pami/BoschZM08",
                "DOI": "10.1109/TPAMI.2007.70716",
                "CorpusId": 13976978,
                "PubMed": "18276975"
            },
            "abstract": "We investigate whether dimensionality reduction using a latent generative model is beneficial for the task of weakly supervised scene classification. In detail, we are given a set of labeled images of scenes (for example, coast, forest, city, river, etc.), and our objective is to classify a new image into one of these categories. Our approach consists of first discovering latent \";topics\"; using probabilistic Latent Semantic Analysis (pLSA), a generative model from the statistical text literature here applied to a bag of visual words representation for each image, and subsequently, training a multiway classifier on the topic distribution vector for each image. We compare this approach to that of representing each image by a bag of visual words vector directly and training a multiway classifier on these vectors. To this end, we introduce a novel vocabulary using dense color SIFT descriptors and then investigate the classification performance under changes in the size of the visual vocabulary, the number of latent topics learned, and the type of discriminative classifier used (k-nearest neighbor or SVM). We achieve superior classification performance to recent publications that have used a bag of visual word representation, in all cases, using the authors' own data sets and testing protocols. We also investigate the gain in adding spatial information. We show applications to image retrieval with relevance feedback and to scene classification in videos.",
            "referenceCount": 37,
            "citationCount": 800,
            "influentialCitationCount": 80,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dugi-doc.udg.edu/bitstream/10256/2317/1/354.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2008-04-01",
            "journal": {
                "name": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
                "volume": "30"
            },
            "citationStyles": {
                "bibtex": "@Article{Bosch2008SceneCU,\n author = {Anna Bosch and Andrew Zisserman and X. Mu\u00f1oz},\n booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n pages = {712-727},\n title = {Scene Classification Using a Hybrid Generative/Discriminative Approach},\n volume = {30},\n year = {2008}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7dbb386a617eacc954940c9540d9cb262529b8b1",
            "@type": "ScholarlyArticle",
            "paperId": "7dbb386a617eacc954940c9540d9cb262529b8b1",
            "corpusId": 247839510,
            "url": "https://www.semanticscholar.org/paper/7dbb386a617eacc954940c9540d9cb262529b8b1",
            "title": "Equivariant Diffusion for Molecule Generation in 3D",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2022,
            "externalIds": {
                "DBLP": "conf/icml/HoogeboomSVW22",
                "ArXiv": "2203.17003",
                "DOI": "10.48550/arXiv.2203.17003",
                "CorpusId": 247839510
            },
            "abstract": "This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and efficiency at training time.",
            "referenceCount": 53,
            "citationCount": 237,
            "influentialCitationCount": 35,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2203.17003",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Biology",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2022-03-31",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/2203.17003"
            },
            "citationStyles": {
                "bibtex": "@Article{Hoogeboom2022EquivariantDF,\n author = {E. Hoogeboom and Victor Garcia Satorras and Cl\u00e9ment Vignac and M. Welling},\n booktitle = {International Conference on Machine Learning},\n journal = {ArXiv},\n title = {Equivariant Diffusion for Molecule Generation in 3D},\n volume = {abs/2203.17003},\n year = {2022}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:093096bd4694ef1d3d87ebf34fa7b3aa368ffa25",
            "@type": "ScholarlyArticle",
            "paperId": "093096bd4694ef1d3d87ebf34fa7b3aa368ffa25",
            "corpusId": 14322900,
            "url": "https://www.semanticscholar.org/paper/093096bd4694ef1d3d87ebf34fa7b3aa368ffa25",
            "title": "A Generative Model for Image Segmentation Based on Label Fusion",
            "venue": "IEEE Transactions on Medical Imaging",
            "publicationVenue": {
                "id": "urn:research:e0cda45d-3074-4ac0-80b8-e5250df00b89",
                "name": "IEEE Transactions on Medical Imaging",
                "alternate_names": [
                    "IEEE Trans Med Imaging"
                ],
                "issn": "0278-0062",
                "url": "http://ieeexplore.ieee.org/servlet/opac?punumber=42"
            },
            "year": 2010,
            "externalIds": {
                "DBLP": "journals/tmi/SabuncuYLFG10",
                "MAG": "2102595307",
                "DOI": "10.1109/TMI.2010.2050897",
                "CorpusId": 14322900,
                "PubMed": "20562040"
            },
            "abstract": "We propose a nonparametric, probabilistic model for the automatic segmentation of medical images, given a training set of images and corresponding label maps. The resulting inference algorithms rely on pairwise registrations between the test image and individual training images. The training labels are then transferred to the test image and fused to compute the final segmentation of the test subject. Such label fusion methods have been shown to yield accurate segmentation, since the use of multiple registrations captures greater inter-subject anatomical variability and improves robustness against occasional registration failures. To the best of our knowledge, this manuscript presents the first comprehensive probabilistic framework that rigorously motivates label fusion as a segmentation approach. The proposed framework allows us to compare different label fusion algorithms theoretically and practically. In particular, recent label fusion or multiatlas segmentation algorithms are interpreted as special cases of our framework. We conduct two sets of experiments to validate the proposed methods. In the first set of experiments, we use 39 brain MRI scans - with manually segmented white matter, cerebral cortex, ventricles and subcortical structures - to compare different label fusion algorithms and the widely-used FreeSurfer whole-brain segmentation tool. Our results indicate that the proposed framework yields more accurate segmentation than FreeSurfer and previous label fusion algorithms. In a second experiment, we use brain MRI scans of 282 subjects to demonstrate that the proposed segmentation tool is sufficiently sensitive to robustly detect hippocampal volume changes in a study of aging and Alzheimer's Disease.",
            "referenceCount": 74,
            "citationCount": 507,
            "influentialCitationCount": 36,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dspace.mit.edu/bitstream/1721.1/64791/1/Golland_A%20generative.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-06-17",
            "journal": {
                "name": "IEEE Transactions on Medical Imaging",
                "volume": "29"
            },
            "citationStyles": {
                "bibtex": "@Article{Sabuncu2010AGM,\n author = {M. Sabuncu and B. Yeo and K. V. Leemput and B. Fischl and P. Golland},\n booktitle = {IEEE Transactions on Medical Imaging},\n journal = {IEEE Transactions on Medical Imaging},\n pages = {1714-1729},\n title = {A Generative Model for Image Segmentation Based on Label Fusion},\n volume = {29},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a8e3cda5acff77a8214d783a51313ccbdcd420ce",
            "@type": "ScholarlyArticle",
            "paperId": "a8e3cda5acff77a8214d783a51313ccbdcd420ce",
            "corpusId": 15287331,
            "url": "https://www.semanticscholar.org/paper/a8e3cda5acff77a8214d783a51313ccbdcd420ce",
            "title": "A flexible generative model for preference aggregation",
            "venue": "The Web Conference",
            "publicationVenue": {
                "id": "urn:research:e07422f9-c065-40c3-a37b-75e98dce79fe",
                "name": "The Web Conference",
                "alternate_names": [
                    "Web Conf",
                    "WWW"
                ],
                "issn": null,
                "url": "http://www.iw3c2.org/"
            },
            "year": 2012,
            "externalIds": {
                "DBLP": "conf/www/VolkovsZ12",
                "MAG": "2051275384",
                "DOI": "10.1145/2187836.2187902",
                "CorpusId": 15287331
            },
            "abstract": "Many areas of study, such as information retrieval, collaborative filtering, and social choice face the preference aggregation problem, in which multiple preferences over objects must be combined into a consensus ranking. Preferences over items can be expressed in a variety of forms, which makes the aggregation problem difficult. In this work we formulate a flexible probabilistic model over pairwise comparisons that can accommodate all these forms. Inference in the model is very fast, making it applicable to problems with hundreds of thousands of preferences. Experiments on benchmark datasets demonstrate superior performance to existing methods",
            "referenceCount": 31,
            "citationCount": 73,
            "influentialCitationCount": 4,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Book",
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2012-04-16",
            "journal": {
                "name": "Proceedings of the 21st international conference on World Wide Web",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Book{Volkovs2012AFG,\n author = {M. Volkovs and R. Zemel},\n booktitle = {The Web Conference},\n journal = {Proceedings of the 21st international conference on World Wide Web},\n title = {A flexible generative model for preference aggregation},\n year = {2012}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:8fd4f61732b68228ce37863efcf70f5f62a4376f",
            "@type": "ScholarlyArticle",
            "paperId": "8fd4f61732b68228ce37863efcf70f5f62a4376f",
            "corpusId": 1662965,
            "url": "https://www.semanticscholar.org/paper/8fd4f61732b68228ce37863efcf70f5f62a4376f",
            "title": "A Generative Entity-Mention Model for Linking Entities with Knowledge Base",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:1e33b3be-b2ab-46e9-96e8-d4eb4bad6e44",
                "name": "Annual Meeting of the Association for Computational Linguistics",
                "alternate_names": [
                    "Annu Meet Assoc Comput Linguistics",
                    "Meeting of the Association for Computational Linguistics",
                    "ACL",
                    "Meet Assoc Comput Linguistics"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/anthology/venues/acl/"
            },
            "year": 2011,
            "externalIds": {
                "ACL": "P11-1095",
                "DBLP": "conf/acl/HanS11",
                "MAG": "2162638401",
                "CorpusId": 1662965
            },
            "abstract": "Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base. Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. In this paper, we propose a generative probabilistic model, called entity-mention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task. In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s\\e), and the distribution of possible contexts of a specific entity P(c\\e). To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s\\e) and P(c\\e). Experimental results show that our method can significantly outperform the traditional methods.",
            "referenceCount": 20,
            "citationCount": 170,
            "influentialCitationCount": 17,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2011-06-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Han2011AGE,\n author = {Xianpei Han and Le Sun},\n booktitle = {Annual Meeting of the Association for Computational Linguistics},\n pages = {945-954},\n title = {A Generative Entity-Mention Model for Linking Entities with Knowledge Base},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
            "@type": "ScholarlyArticle",
            "paperId": "ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
            "corpusId": 211146177,
            "url": "https://www.semanticscholar.org/paper/ef4f5a50837a7c1b3e87b9300ffc7ba00d461a0f",
            "title": "AUTO-ENCODING VARIATIONAL BAYES",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2020,
            "externalIds": {
                "CorpusId": 211146177
            },
            "abstract": "To make decisions based on a model fit by Auto-Encoding Variational Bayes (AEVB), practitioners typically use importance sampling to estimate a functional of the posterior distribution. The variational distribution found by AEVB serves as the proposal distribution for importance sampling. However, this proposal distribution may give unreliable (high variance) importance sampling estimates, thus leading to poor decisions. We explore how changing the objective function for learning the variational distribution, while continuing to learn the generative model based on the ELBO, affects the quality of downstream decisions. For a particular model, we characterize the error of importance sampling as a function of posterior variance and show that proposal distributions learned with evidence upper bounds are better. Motivated by these theoretical results, we propose a novel variant of the VAE. In addition to experimenting with MNIST, we present a full-fledged application of the proposed method to single-cell RNA sequencing. In this challenging instance of multiple hypothesis testing, the proposed method surpasses the current state of the art.",
            "referenceCount": 54,
            "citationCount": 3687,
            "influentialCitationCount": 633,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": null,
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": null,
            "publicationDate": null,
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Lopez2020AUTOENCODINGVB,\n author = {Romain Lopez and Pierre Boyeau and N. Yosef and Michael I. Jordan and J. Regier},\n title = {AUTO-ENCODING VARIATIONAL BAYES},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:9a3f9d1d86a8f080dca8414ac1b1ff8e0478276d",
            "@type": "ScholarlyArticle",
            "paperId": "9a3f9d1d86a8f080dca8414ac1b1ff8e0478276d",
            "corpusId": 1328490,
            "url": "https://www.semanticscholar.org/paper/9a3f9d1d86a8f080dca8414ac1b1ff8e0478276d",
            "title": "A Generative Model for Brain Tumor Segmentation in Multi-Modal Images",
            "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
            "publicationVenue": {
                "id": "urn:research:61a709e3-2060-423c-8de5-ffd3885aa31c",
                "name": "International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "alternate_names": [
                    "Medical Image Computing and Computer-Assisted Intervention",
                    "MICCAI",
                    "Med Image Comput Comput Interv",
                    "Int Conf Med Image Comput Comput Interv"
                ],
                "issn": null,
                "url": "http://www.miccai.org/"
            },
            "year": 2010,
            "externalIds": {
                "MAG": "1597605992",
                "DBLP": "conf/miccai/MenzeLLWAG10",
                "DOI": "10.1007/978-3-642-15745-5_19",
                "CorpusId": 1328490,
                "PubMed": "20879310"
            },
            "abstract": null,
            "referenceCount": 25,
            "citationCount": 251,
            "influentialCitationCount": 10,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://link.springer.com/content/pdf/10.1007/978-3-642-15745-5_19.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2010-09-20",
            "journal": {
                "name": "Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention",
                "volume": "13 Pt 2"
            },
            "citationStyles": {
                "bibtex": "@Article{Menze2010AGM,\n author = {Bjoern H Menze and K. V. Leemput and D. Lashkari and M. Weber and N. Ayache and P. Golland},\n booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},\n journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},\n pages = {\n          151-9\n        },\n title = {A Generative Model for Brain Tumor Segmentation in Multi-Modal Images},\n volume = {13 Pt 2},\n year = {2010}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:74276a37bfa50f90dfae37f767b2b67784bd402a",
            "@type": "ScholarlyArticle",
            "paperId": "74276a37bfa50f90dfae37f767b2b67784bd402a",
            "corpusId": 225040574,
            "url": "https://www.semanticscholar.org/paper/74276a37bfa50f90dfae37f767b2b67784bd402a",
            "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "publicationVenue": {
                "id": "urn:research:01103732-3808-4930-b8e4-7e9e68d5c68d",
                "name": "North American Chapter of the Association for Computational Linguistics",
                "alternate_names": [
                    "North Am Chapter Assoc Comput Linguistics",
                    "NAACL"
                ],
                "issn": null,
                "url": "https://www.aclweb.org/portal/naacl"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2010.11934",
                "DBLP": "conf/naacl/XueCRKASBR21",
                "MAG": "3169483174",
                "ACL": "2021.naacl-main.41",
                "DOI": "10.18653/V1/2021.NAACL-MAIN.41",
                "CorpusId": 225040574
            },
            "abstract": "The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
            "referenceCount": 55,
            "citationCount": 1339,
            "influentialCitationCount": 203,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://aclanthology.org/2021.naacl-main.41.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2020-10-22",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Xue2020mT5AM,\n author = {Linting Xue and Noah Constant and Adam Roberts and Mihir Kale and Rami Al-Rfou and Aditya Siddhant and Aditya Barua and Colin Raffel},\n booktitle = {North American Chapter of the Association for Computational Linguistics},\n pages = {483-498},\n title = {mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:40651d89610e2e84a0e94d5c73f83c6372f828b5",
            "@type": "ScholarlyArticle",
            "paperId": "40651d89610e2e84a0e94d5c73f83c6372f828b5",
            "corpusId": 56328554,
            "url": "https://www.semanticscholar.org/paper/40651d89610e2e84a0e94d5c73f83c6372f828b5",
            "title": "Generative Social Science: Studies in Agent-Based Computational Modeling (Princeton Studies in Complexity)",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2007,
            "externalIds": {
                "MAG": "567313566",
                "DOI": "10.5038/2162-4593.11.1.8",
                "CorpusId": 56328554
            },
            "abstract": "Generative Social Science: Studies in Agent-Based Computational Modeling JOSHUA M. EPSTEIN PRINCETON UNIVERSITY PRESS, PRINCETON, NJ, 2007 352 PP. CLOTH $49.50 REVIEWED BY ERIC C. JONES This book calls for a generative social science. Generative social science rests on the idea that you cannot explain current phenomena without describing the rules or preceding conditions that produced these current phenomena. In other words, the author believes that we must not only explore causality in terms of 'A affects B,' but also in terms of how a specific suite of physical, biological, social or cultural tendencies play out across time for a given population, producing some observed state or phenomenon. Epstein argues that anything short of being able to model the flow between prior and present conditions is mere description. He says his naming of the Generative approach took inspiration from Chomsky's generative syntactic structures. Generative social science is tightly wed to the methodology of Agent-Based Modeling made more feasible lately by faster computers. However, Epstein warns against its identification solely as a computer-driven technique. His point is that past behavior of individuals, households, firms or other agents must be accounted for when understanding a phenomenon. Following the lead of mathematicians and most modelers, the author seeks parsimonious or small sets of rules to explain the arrival at any current condition. This 'new' kind of social science is probably too mathematical for most ethnographically oriented social scientists to adopt, although this historicist/ evolutionary approach is one that must regularly be injected into the social sciences in order to augment the complimentary yet more dominant functionalist and ideationist approaches. Ecosystem researchers would certainly be able to make use of the agentbased modeling approach, perhaps even being able to better account for the individual agents in their systems. Population researchers similarly could better develop models and parameters for animal/plant/ agent behaviors. Generative Social Science is generally an update to the 1996 book Growing Artificial Societies (Brookings Institution and MIT Press) by Epstein and Robert Axtell, although this new book is a compilation of works with all but three chapters (Introduction, Chapters 2 and 13) published separately elsewhere in books or journals. Preludes by Epstein for each chapter make the flow awkward, but provide contextual insights or connections between chapters. All chapters have Epstein as an author-typically the primary author-and half of the chapters are single-authored by Epstein; as such, the publisher considers the book a single-authored work. A CD with several of the models accompanies the book, so that you can change a few of the parameters and graphically view the results (hundreds of colored pixels on a square space). The agent-based modeling technique is one way to bridge the micro-macro gulf, producing non-intuitive macro results along the way. Epstein is careful to define such emergence as the computable result of agent actions, and not as the old (and even contemporary, in some cases) idea of emergence as something that can never be reduced to its parts. Despite proposing this form of reductionism, the book allows that emergent properties maybe something that the individuals themselves might not possess, so emergence is not so much a sum of parts as a product of parts. Different agent-based models with different suites of variables might produce the same social phenomena, in which case field data and theoretical plausibility assist in determining which model to pursue. Models can also be used to find out which rules will not account for observed behavior. The first three chapters constitute the introductory material, primarily advocacy for the approach as well as delimiting the domain. The domain of generative social science is based upon the following: heterogeneous agents, bounded rationality, explicit/ geographic space, local interactions, non-equilibrium dynamics and initial autonomy of agents. \u2026",
            "referenceCount": 1,
            "citationCount": 613,
            "influentialCitationCount": 11,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://digitalcommons.usf.edu/cgi/viewcontent.cgi?article=1043&context=jea",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Engineering",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": null,
            "journal": {
                "name": "",
                "volume": ""
            },
            "citationStyles": {
                "bibtex": "@Inproceedings{Epstein2007GenerativeSS,\n author = {J. Epstein},\n title = {Generative Social Science: Studies in Agent-Based Computational Modeling (Princeton Studies in Complexity)},\n year = {2007}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:03e115ea4cbce1209b1c3d7f79be6ca8f705256d",
            "@type": "ScholarlyArticle",
            "paperId": "03e115ea4cbce1209b1c3d7f79be6ca8f705256d",
            "corpusId": 7449553,
            "url": "https://www.semanticscholar.org/paper/03e115ea4cbce1209b1c3d7f79be6ca8f705256d",
            "title": "A Generative Model of Speech Production in Broca\u2019s and Wernicke\u2019s Areas",
            "venue": "Front. Psychology",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2011,
            "externalIds": {
                "PubMedCentral": "3174393",
                "MAG": "2050735676",
                "DOI": "10.3389/fpsyg.2011.00237",
                "CorpusId": 7449553,
                "PubMed": "21954392"
            },
            "abstract": "Speech production involves the generation of an auditory signal from the articulators and vocal tract. When the intended auditory signal does not match the produced sounds, subsequent articulatory commands can be adjusted to reduce the difference between the intended and produced sounds. This requires an internal model of the intended speech output that can be compared to the produced speech. The aim of this functional imaging study was to identify brain activation related to the internal model of speech production after activation related to vocalization, auditory feedback, and movement in the articulators had been controlled. There were four conditions: silent articulation of speech, non-speech mouth movements, finger tapping, and visual fixation. In the speech conditions, participants produced the mouth movements associated with the words \u201cone\u201d and \u201cthree.\u201d We eliminated auditory feedback from the spoken output by instructing participants to articulate these words without producing any sound. The non-speech mouth movement conditions involved lip pursing and tongue protrusions to control for movement in the articulators. The main difference between our speech and non-speech mouth movement conditions is that prior experience producing speech sounds leads to the automatic and covert generation of auditory and phonological associations that may play a role in predicting auditory feedback. We found that, relative to non-speech mouth movements, silent speech activated Broca\u2019s area in the left dorsal pars opercularis and Wernicke\u2019s area in the left posterior superior temporal sulcus. We discuss these results in the context of a generative model of speech production and propose that Broca\u2019s and Wernicke\u2019s areas may be involved in predicting the speech output that follows articulation. These predictions could provide a mechanism by which rapid movement of the articulators is precisely matched to the intended speech outputs during future articulations.",
            "referenceCount": 64,
            "citationCount": 96,
            "influentialCitationCount": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.frontiersin.org/articles/10.3389/fpsyg.2011.00237/pdf",
                "status": "GOLD"
            },
            "fieldsOfStudy": [
                "Psychology",
                "Medicine"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2011-09-16",
            "journal": {
                "name": "Frontiers in Psychology",
                "volume": "2"
            },
            "citationStyles": {
                "bibtex": "@Article{Price2011AGM,\n author = {C. Price and J. Crinion and M. MacSweeney},\n booktitle = {Front. Psychology},\n journal = {Frontiers in Psychology},\n title = {A Generative Model of Speech Production in Broca\u2019s and Wernicke\u2019s Areas},\n volume = {2},\n year = {2011}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:28716e1e8cc9620209ac0e663cc1a3477279d531",
            "@type": "ScholarlyArticle",
            "paperId": "28716e1e8cc9620209ac0e663cc1a3477279d531",
            "corpusId": 145164169,
            "url": "https://www.semanticscholar.org/paper/28716e1e8cc9620209ac0e663cc1a3477279d531",
            "title": "Generative Processes of Comprehension",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 1989,
            "externalIds": {
                "MAG": "2089636244",
                "DOI": "10.1207/S15326985EP2404_2",
                "CorpusId": 145164169
            },
            "abstract": "This article presents a model of the generative processes of reading comprehension. The article begins with a discussion of the four parts of the model: generation, motivation, attention, and memory. The discussion then reviews laboratory and classroom research relevant to the model. A series of experiments by the author and his colleagues are presented to support the instructional utility of the model. The article concludes with a discussion of the model and its relation to the teaching of reading comprehension in schools.",
            "referenceCount": 42,
            "citationCount": 893,
            "influentialCitationCount": 30,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review"
            ],
            "publicationDate": "1989-09-01",
            "journal": {
                "name": "Educational Psychologist",
                "volume": "24"
            },
            "citationStyles": {
                "bibtex": "@Article{Wittrock1989GenerativePO,\n author = {M. Wittrock},\n journal = {Educational Psychologist},\n pages = {345-376},\n title = {Generative Processes of Comprehension},\n volume = {24},\n year = {1989}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:1c71771c701aadfd72c5866170a9f5d71464bb88",
            "@type": "ScholarlyArticle",
            "paperId": "1c71771c701aadfd72c5866170a9f5d71464bb88",
            "corpusId": 147704286,
            "url": "https://www.semanticscholar.org/paper/1c71771c701aadfd72c5866170a9f5d71464bb88",
            "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2971274815",
                "ArXiv": "1905.03197",
                "DBLP": "journals/corr/abs-1905-03197",
                "CorpusId": 147704286
            },
            "abstract": "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.",
            "referenceCount": 59,
            "citationCount": 1229,
            "influentialCitationCount": 175,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-05-08",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Dong2019UnifiedLM,\n author = {Li Dong and Nan Yang and Wenhui Wang and Furu Wei and Xiaodong Liu and Yu Wang and Jianfeng Gao and M. Zhou and H. Hon},\n booktitle = {Neural Information Processing Systems},\n pages = {13042-13054},\n title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:18a93dc1558bf9d7534d0b416633cebaf75c1145",
            "@type": "ScholarlyArticle",
            "paperId": "18a93dc1558bf9d7534d0b416633cebaf75c1145",
            "corpusId": 155162335,
            "url": "https://www.semanticscholar.org/paper/18a93dc1558bf9d7534d0b416633cebaf75c1145",
            "title": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
            "venue": "Proceedings of the National Academy of Sciences of the United States of America",
            "publicationVenue": {
                "id": "urn:research:bb95bf2e-8383-4748-bf9d-d6906d091085",
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "alternate_names": [
                    "PNAS",
                    "PNAS online",
                    "Proceedings of the National Academy of Sciences of the United States of America.",
                    "Proc National Acad Sci",
                    "Proceedings of the National Academy of Sciences",
                    "Proc National Acad Sci u s Am"
                ],
                "issn": "0027-8424",
                "url": "https://www.jstor.org/journal/procnatiacadscie"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/pnas/RivesMSGLLGOZMF21",
                "PubMedCentral": "8053943",
                "MAG": "2943495267",
                "DOI": "10.1101/622803",
                "CorpusId": 155162335,
                "PubMed": "33876751"
            },
            "abstract": "Significance Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue\u2013residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction. In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",
            "referenceCount": 148,
            "citationCount": 1089,
            "influentialCitationCount": 130,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2019/05/29/622803.full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Medicine",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-04-29",
            "journal": {
                "name": "Proceedings of the National Academy of Sciences of the United States of America",
                "volume": "118"
            },
            "citationStyles": {
                "bibtex": "@Article{Rives2019BiologicalSA,\n author = {Alexander Rives and Siddharth Goyal and J. Meier and Demi Guo and Myle Ott and C. L. Zitnick and Jerry Ma and R. Fergus},\n booktitle = {Proceedings of the National Academy of Sciences of the United States of America},\n journal = {Proceedings of the National Academy of Sciences of the United States of America},\n title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},\n volume = {118},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:501c02c7caa7fc2c7077405299b4cbe7d294b170",
            "@type": "ScholarlyArticle",
            "paperId": "501c02c7caa7fc2c7077405299b4cbe7d294b170",
            "corpusId": 208637478,
            "url": "https://www.semanticscholar.org/paper/501c02c7caa7fc2c7077405299b4cbe7d294b170",
            "title": "Normalizing Flows for Probabilistic Modeling and Inference",
            "venue": "Journal of machine learning research",
            "publicationVenue": {
                "id": "urn:research:c22e7c36-3bfa-43e1-bb7b-edccdea2a780",
                "name": "Journal of machine learning research",
                "alternate_names": [
                    "Journal of Machine Learning Research",
                    "J mach learn res",
                    "J Mach Learn Res"
                ],
                "issn": "1532-4435",
                "url": "http://www.ai.mit.edu/projects/jmlr/"
            },
            "year": 2019,
            "externalIds": {
                "DBLP": "journals/jmlr/PapamakariosNRM21",
                "ArXiv": "1912.02762",
                "MAG": "2992035660",
                "CorpusId": 208637478
            },
            "abstract": "Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.",
            "referenceCount": 150,
            "citationCount": 1045,
            "influentialCitationCount": 91,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Review"
            ],
            "publicationDate": "2019-12-05",
            "journal": {
                "name": "J. Mach. Learn. Res.",
                "volume": "22"
            },
            "citationStyles": {
                "bibtex": "@Article{Papamakarios2019NormalizingFF,\n author = {G. Papamakarios and Eric T. Nalisnick and Danilo Jimenez Rezende and S. Mohamed and Balaji Lakshminarayanan},\n booktitle = {Journal of machine learning research},\n journal = {J. Mach. Learn. Res.},\n pages = {57:1-57:64},\n title = {Normalizing Flows for Probabilistic Modeling and Inference},\n volume = {22},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:744fe47157477235032f7bb3777800f9f2f45e52",
            "@type": "ScholarlyArticle",
            "paperId": "744fe47157477235032f7bb3777800f9f2f45e52",
            "corpusId": 3568073,
            "url": "https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52",
            "title": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2766527293",
                "DBLP": "conf/iclr/KarrasALL18",
                "ArXiv": "1710.10196",
                "CorpusId": 3568073
            },
            "abstract": "We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",
            "referenceCount": 65,
            "citationCount": 5822,
            "influentialCitationCount": 1099,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-10-27",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1710.10196"
            },
            "citationStyles": {
                "bibtex": "@Article{Karras2017ProgressiveGO,\n author = {Tero Karras and Timo Aila and S. Laine and J. Lehtinen},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Progressive Growing of GANs for Improved Quality, Stability, and Variation},\n volume = {abs/1710.10196},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:29e944711a354c396fad71936f536e83025b6ce0",
            "@type": "ScholarlyArticle",
            "paperId": "29e944711a354c396fad71936f536e83025b6ce0",
            "corpusId": 2428314,
            "url": "https://www.semanticscholar.org/paper/29e944711a354c396fad71936f536e83025b6ce0",
            "title": "Categorical Reparameterization with Gumbel-Softmax",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950151997",
                "DBLP": "journals/corr/JangGP16",
                "ArXiv": "1611.01144",
                "CorpusId": 2428314
            },
            "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.",
            "referenceCount": 33,
            "citationCount": 4124,
            "influentialCitationCount": 595,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-11-03",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1611.01144"
            },
            "citationStyles": {
                "bibtex": "@Article{Jang2016CategoricalRW,\n author = {Eric Jang and S. Gu and Ben Poole},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Categorical Reparameterization with Gumbel-Softmax},\n volume = {abs/1611.01144},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:69304ba2c5d2bff09f7059916ab0aa117bdbea41",
            "@type": "ScholarlyArticle",
            "paperId": "69304ba2c5d2bff09f7059916ab0aa117bdbea41",
            "corpusId": 220661158,
            "url": "https://www.semanticscholar.org/paper/69304ba2c5d2bff09f7059916ab0aa117bdbea41",
            "title": "Whatever next? Predictive brains, situated agents, and the future of cognitive science.",
            "venue": "Behavioral and Brain Sciences",
            "publicationVenue": {
                "id": "urn:research:f51399af-b5cb-4819-9d81-57ec1d17ebf0",
                "name": "Behavioral and Brain Sciences",
                "alternate_names": [
                    "Behav Brain Sci"
                ],
                "issn": "0140-525X",
                "url": "http://www.bbsonline.org/"
            },
            "year": 2013,
            "externalIds": {
                "MAG": "2153791616",
                "DOI": "10.1017/S0140525X12000477",
                "CorpusId": 220661158,
                "PubMed": "23663408"
            },
            "abstract": "Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this \"hierarchical prediction machine\" approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.",
            "referenceCount": 555,
            "citationCount": 3155,
            "influentialCitationCount": 297,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/33542C736E17E3D1D44E8D03BE5F4CD9/S0140525X12000477a.pdf/div-class-title-whatever-next-predictive-brains-situated-agents-and-the-future-of-cognitive-science-div.pdf",
                "status": "BRONZE"
            },
            "fieldsOfStudy": [
                "Medicine",
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Philosophy",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Psychology",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "Review",
                "JournalArticle"
            ],
            "publicationDate": "2013-06-01",
            "journal": {
                "name": "The Behavioral and brain sciences",
                "volume": "36 3"
            },
            "citationStyles": {
                "bibtex": "@Article{Clark2013WhateverNP,\n author = {A. Clark},\n booktitle = {Behavioral and Brain Sciences},\n journal = {The Behavioral and brain sciences},\n pages = {\n          181-204\n        },\n title = {Whatever next? Predictive brains, situated agents, and the future of cognitive science.},\n volume = {36 3},\n year = {2013}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "@type": "ScholarlyArticle",
            "paperId": "f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "corpusId": 20282961,
            "url": "https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "title": "Neural Discrete Representation Learning",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2752796333",
                "ArXiv": "1711.00937",
                "DBLP": "conf/nips/OordVK17",
                "CorpusId": 20282961
            },
            "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",
            "referenceCount": 43,
            "citationCount": 2618,
            "influentialCitationCount": 470,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-11-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1711.00937"
            },
            "citationStyles": {
                "bibtex": "@Article{Oord2017NeuralDR,\n author = {A\u00e4ron van den Oord and Oriol Vinyals and K. Kavukcuoglu},\n booktitle = {Neural Information Processing Systems},\n journal = {ArXiv},\n title = {Neural Discrete Representation Learning},\n volume = {abs/1711.00937},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:345afa0e85cb2f5cb438ae44027499ff2c392409",
            "@type": "ScholarlyArticle",
            "paperId": "345afa0e85cb2f5cb438ae44027499ff2c392409",
            "corpusId": 4357800,
            "url": "https://www.semanticscholar.org/paper/345afa0e85cb2f5cb438ae44027499ff2c392409",
            "title": "Adversarial Discriminative Domain Adaptation",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.05464",
                "MAG": "2593768305",
                "DBLP": "journals/corr/TzengHSD17",
                "DOI": "10.1109/CVPR.2017.316",
                "CorpusId": 4357800
            },
            "abstract": "Adversarial learning methods are a promising approach to training robust deep networks, and can generate complex samples across diverse domains. They can also improve recognition despite the presence of domain shift or dataset bias: recent adversarial approaches to unsupervised domain adaptation reduce the difference between the training and test domain distributions and thus improve generalization performance. However, while generative adversarial networks (GANs) show compelling visualizations, they are not optimal on discriminative tasks and can be limited to smaller shifts. On the other hand, discriminative approaches can handle larger domain shifts, but impose tied weights on the model and do not exploit a GAN-based loss. In this work, we first outline a novel generalized framework for adversarial adaptation, which subsumes recent state-of-the-art approaches as special cases, and use this generalized view to better relate prior approaches. We then propose a previously unexplored instance of our general framework which combines discriminative modeling, untied weight sharing, and a GAN loss, which we call Adversarial Discriminative Domain Adaptation (ADDA). We show that ADDA is more effective yet considerably simpler than competing domain-adversarial methods, and demonstrate the promise of our approach by exceeding state-of-the-art unsupervised adaptation results on standard domain adaptation tasks as well as a difficult cross-modality object classification task.",
            "referenceCount": 32,
            "citationCount": 3912,
            "influentialCitationCount": 533,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1702.05464",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-17",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Tzeng2017AdversarialDD,\n author = {Eric Tzeng and Judy Hoffman and Kate Saenko and Trevor Darrell},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {2962-2971},\n title = {Adversarial Discriminative Domain Adaptation},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "@type": "ScholarlyArticle",
            "paperId": "d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "corpusId": 1169492,
            "url": "https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0",
            "title": "Show and tell: A neural image caption generator",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2014,
            "externalIds": {
                "DBLP": "journals/corr/VinyalsTBE14",
                "MAG": "2951912364",
                "ArXiv": "1411.4555",
                "DOI": "10.1109/CVPR.2015.7298935",
                "CorpusId": 1169492
            },
            "abstract": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
            "referenceCount": 35,
            "citationCount": 5394,
            "influentialCitationCount": 662,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1411.4555",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2014-11-17",
            "journal": {
                "name": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Vinyals2014ShowAT,\n author = {Oriol Vinyals and Alexander Toshev and Samy Bengio and D. Erhan},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {3156-3164},\n title = {Show and tell: A neural image caption generator},\n year = {2014}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d82b55c35c8673774a708353838918346f6c006f",
            "@type": "ScholarlyArticle",
            "paperId": "d82b55c35c8673774a708353838918346f6c006f",
            "corpusId": 748227,
            "url": "https://www.semanticscholar.org/paper/d82b55c35c8673774a708353838918346f6c006f",
            "title": "Generating Sentences from a Continuous Space",
            "venue": "Conference on Computational Natural Language Learning",
            "publicationVenue": {
                "id": "urn:research:3779a5a7-9119-4f69-84fe-f7eef193eb49",
                "name": "Conference on Computational Natural Language Learning",
                "alternate_names": [
                    "CoNLL",
                    "Conf Comput Nat Lang Learn"
                ],
                "issn": null,
                "url": null
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "conf/conll/BowmanVVDJB16",
                "MAG": "2210838531",
                "ACL": "K16-1002",
                "ArXiv": "1511.06349",
                "DOI": "10.18653/v1/K16-1002",
                "CorpusId": 748227
            },
            "abstract": "The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.",
            "referenceCount": 49,
            "citationCount": 2116,
            "influentialCitationCount": 365,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/K16-1002.pdf",
                "status": "HYBRID"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-19",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Bowman2015GeneratingSF,\n author = {Samuel R. Bowman and L. Vilnis and Oriol Vinyals and Andrew M. Dai and R. J\u00f3zefowicz and Samy Bengio},\n booktitle = {Conference on Computational Natural Language Learning},\n pages = {10-21},\n title = {Generating Sentences from a Continuous Space},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:04f739a0c29b75877243731aeead512bf0ed1dff",
            "@type": "ScholarlyArticle",
            "paperId": "04f739a0c29b75877243731aeead512bf0ed1dff",
            "corpusId": 49868626,
            "url": "https://www.semanticscholar.org/paper/04f739a0c29b75877243731aeead512bf0ed1dff",
            "title": "Meta-Learning with Latent Embedding Optimization",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2018,
            "externalIds": {
                "MAG": "2884901161",
                "DBLP": "journals/corr/abs-1807-05960",
                "ArXiv": "1807.05960",
                "CorpusId": 49868626
            },
            "abstract": "Gradient-based meta-learning techniques are both widely applicable and proficient at solving challenging few-shot learning and fast adaptation problems. However, they have practical difficulties when operating on high-dimensional parameter spaces in extreme low-data regimes. We show that it is possible to bypass these limitations by learning a data-dependent latent generative representation of model parameters, and performing gradient-based meta-learning in this low-dimensional latent space. The resulting approach, latent embedding optimization (LEO), decouples the gradient-based adaptation procedure from the underlying high-dimensional space of model parameters. Our evaluation shows that LEO can achieve state-of-the-art performance on the competitive miniImageNet and tieredImageNet few-shot classification tasks. Further analysis indicates LEO is able to capture uncertainty in the data, and can perform adaptation more effectively by optimizing in latent space.",
            "referenceCount": 63,
            "citationCount": 1161,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Mathematics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2018-07-16",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1807.05960"
            },
            "citationStyles": {
                "bibtex": "@Article{Rusu2018MetaLearningWL,\n author = {Andrei A. Rusu and Dushyant Rao and Jakub Sygnowski and Oriol Vinyals and Razvan Pascanu and Simon Osindero and R. Hadsell},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Meta-Learning with Latent Embedding Optimization},\n volume = {abs/1807.05960},\n year = {2018}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:a072c2a400f62f720b68dc54a662fb1ae115bf06",
            "@type": "ScholarlyArticle",
            "paperId": "a072c2a400f62f720b68dc54a662fb1ae115bf06",
            "corpusId": 4689304,
            "url": "https://www.semanticscholar.org/paper/a072c2a400f62f720b68dc54a662fb1ae115bf06",
            "title": "Tacotron: Towards End-to-End Speech Synthesis",
            "venue": "Interspeech",
            "publicationVenue": {
                "id": "urn:research:af90489e-312f-4514-bea2-bcb399cb8ece",
                "name": "Interspeech",
                "alternate_names": [
                    "Conf Int Speech Commun Assoc",
                    "INTERSPEECH",
                    "Conference of the International Speech Communication Association"
                ],
                "issn": "2308-457X",
                "url": "https://www.isca-speech.org/iscaweb/index.php/conferences/interspeech"
            },
            "year": 2017,
            "externalIds": {
                "DBLP": "conf/interspeech/WangSSWWJYXCBLA17",
                "MAG": "2963609956",
                "ArXiv": "1703.10135",
                "DOI": "10.21437/Interspeech.2017-1452",
                "CorpusId": 4689304
            },
            "abstract": "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",
            "referenceCount": 26,
            "citationCount": 1500,
            "influentialCitationCount": 286,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1703.10135",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2017-03-29",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Wang2017TacotronTE,\n author = {Yuxuan Wang and R. Skerry-Ryan and Daisy Stanton and Yonghui Wu and Ron J. Weiss and N. Jaitly and Zongheng Yang and Y. Xiao and Z. Chen and Samy Bengio and Quoc V. Le and Yannis Agiomyrgiannakis and R. Clark and R. Saurous},\n booktitle = {Interspeech},\n pages = {4006-4010},\n title = {Tacotron: Towards End-to-End Speech Synthesis},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
            "@type": "ScholarlyArticle",
            "paperId": "f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
            "corpusId": 2683207,
            "url": "https://www.semanticscholar.org/paper/f9b7783448f65205e085bd4e6fdfa2c8bfa9a4df",
            "title": "Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro",
            "venue": "IEEE International Conference on Computer Vision",
            "publicationVenue": {
                "id": "urn:research:7654260e-79f9-45c5-9663-d72027cf88f3",
                "name": "IEEE International Conference on Computer Vision",
                "alternate_names": [
                    "ICCV",
                    "IEEE Int Conf Comput Vis",
                    "ICCV Workshops",
                    "ICCV Work"
                ],
                "issn": null,
                "url": "https://ieeexplore.ieee.org/xpl/conhome/1000149/all-proceedings"
            },
            "year": 2017,
            "externalIds": {
                "MAG": "2585635281",
                "DBLP": "conf/iccv/ZhengZY17",
                "ArXiv": "1701.07717",
                "DOI": "10.1109/ICCV.2017.405",
                "CorpusId": 2683207
            },
            "abstract": "The main contribution of this paper is a simple semisupervised pipeline that only uses the original training set without collecting extra data. It is challenging in 1) how to obtain more training data only from the training set and 2) how to use the newly generated data. In this work, the generative adversarial network (GAN) is used to generate unlabeled samples. We propose the label smoothing regularization for outliers (LSRO). This method assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the baseline. We verify the proposed method on a practical problem: person re-identification (re-ID). This task aims to retrieve a query person from other cameras. We adopt the deep convolutional generative adversarial network (DCGAN) for sample generation, and a baseline convolutional neural network (CNN) for representation learning. Experiments show that adding the GAN-generated data effectively improves the discriminative ability of learned CNN embeddings. On three large-scale datasets, Market- 1501, CUHK03 and DukeMTMC-reID, we obtain +4.37%, +1.6% and +2.46% improvement in rank-1 precision over the baseline CNN, respectively. We additionally apply the proposed method to fine-grained bird recognition and achieve a +0.6% improvement over a strong baseline. The code is available at https://github.com/layumi/ Person-reID_GAN.",
            "referenceCount": 64,
            "citationCount": 1654,
            "influentialCitationCount": 428,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://opus.lib.uts.edu.au/bitstream/10453/118067/4/FF67E427-6528-4081-B0B7-C3EB797E0421.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-01-26",
            "journal": {
                "name": "2017 IEEE International Conference on Computer Vision (ICCV)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zheng2017UnlabeledSG,\n author = {Zhedong Zheng and Liang Zheng and Yi Yang},\n booktitle = {IEEE International Conference on Computer Vision},\n journal = {2017 IEEE International Conference on Computer Vision (ICCV)},\n pages = {3774-3782},\n title = {Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in Vitro},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
            "@type": "ScholarlyArticle",
            "paperId": "c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
            "corpusId": 5092785,
            "url": "https://www.semanticscholar.org/paper/c8c04ed972d38e2326a53d322a6f2d7e0f8218c1",
            "title": "Adversarial Autoencoders",
            "venue": "arXiv.org",
            "publicationVenue": {
                "id": "urn:research:1901e811-ee72-4b20-8f7e-de08cd395a10",
                "name": "arXiv.org",
                "alternate_names": [
                    "ArXiv"
                ],
                "issn": "2331-8422",
                "url": "https://arxiv.org"
            },
            "year": 2015,
            "externalIds": {
                "DBLP": "journals/corr/MakhzaniSJG15",
                "ArXiv": "1511.05644",
                "CorpusId": 5092785
            },
            "abstract": "In this paper, we propose the\"adversarial autoencoder\"(AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.",
            "referenceCount": 26,
            "citationCount": 1883,
            "influentialCitationCount": 269,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-11-18",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1511.05644"
            },
            "citationStyles": {
                "bibtex": "@Article{Makhzani2015AdversarialA,\n author = {Alireza Makhzani and Jonathon Shlens and N. Jaitly and I. Goodfellow},\n booktitle = {arXiv.org},\n journal = {ArXiv},\n title = {Adversarial Autoencoders},\n volume = {abs/1511.05644},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
            "@type": "ScholarlyArticle",
            "paperId": "fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
            "corpusId": 6104263,
            "url": "https://www.semanticscholar.org/paper/fcf43325529c8b1cc26aeb52fd5d7e532abb0a40",
            "title": "Adversarially Learned Inference",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2016,
            "externalIds": {
                "ArXiv": "1606.00704",
                "MAG": "2411541852",
                "DBLP": "journals/corr/DumoulinBPLAMC16",
                "CorpusId": 6104263
            },
            "abstract": "We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.",
            "referenceCount": 44,
            "citationCount": 1236,
            "influentialCitationCount": 173,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2016-06-02",
            "journal": {
                "name": "ArXiv",
                "volume": "abs/1606.00704"
            },
            "citationStyles": {
                "bibtex": "@Article{Dumoulin2016AdversariallyLI,\n author = {Vincent Dumoulin and Ishmael Belghazi and Ben Poole and Alex Lamb and Mart\u00edn Arjovsky and Olivier Mastropietro and Aaron C. Courville},\n booktitle = {International Conference on Learning Representations},\n journal = {ArXiv},\n title = {Adversarially Learned Inference},\n volume = {abs/1606.00704},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:c5f7074a264356c9a022a8dff24df79d1db8c3d3",
            "@type": "ScholarlyArticle",
            "paperId": "c5f7074a264356c9a022a8dff24df79d1db8c3d3",
            "corpusId": 214725226,
            "url": "https://www.semanticscholar.org/paper/c5f7074a264356c9a022a8dff24df79d1db8c3d3",
            "title": "ProGen: Language Modeling for Protein Generation",
            "venue": "bioRxiv",
            "publicationVenue": {
                "id": "urn:research:027ffd21-ebb0-4af8-baf5-911124292fd0",
                "name": "bioRxiv",
                "alternate_names": null,
                "issn": null,
                "url": "http://biorxiv.org/"
            },
            "year": 2020,
            "externalIds": {
                "ArXiv": "2004.03497",
                "MAG": "3015531900",
                "DBLP": "journals/corr/abs-2004-03497",
                "DOI": "10.1101/2020.03.07.982272",
                "CorpusId": 214725226
            },
            "abstract": "Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on \u223c280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",
            "referenceCount": 53,
            "citationCount": 198,
            "influentialCitationCount": 9,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.biorxiv.org/content/biorxiv/early/2020/03/13/2020.03.07.982272.full.pdf",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Biology",
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Biology",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Materials Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2020-03-08",
            "journal": {
                "name": "bioRxiv",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Madani2020ProGenLM,\n author = {Ali Madani and Bryan McCann and N. Naik and N. Keskar and N. Anand and Raphael R. Eguchi and Po-Ssu Huang and R. Socher},\n booktitle = {bioRxiv},\n journal = {bioRxiv},\n title = {ProGen: Language Modeling for Protein Generation},\n year = {2020}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6",
            "@type": "ScholarlyArticle",
            "paperId": "0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6",
            "corpusId": 8643626,
            "url": "https://www.semanticscholar.org/paper/0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6",
            "title": "Texture Synthesis Using Convolutional Neural Networks",
            "venue": "Neural Information Processing Systems",
            "publicationVenue": {
                "id": "urn:research:d9720b90-d60b-48bc-9df8-87a30b9a60dd",
                "name": "Neural Information Processing Systems",
                "alternate_names": [
                    "Neural Inf Process Syst",
                    "NeurIPS",
                    "NIPS"
                ],
                "issn": null,
                "url": "http://neurips.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "2952565892",
                "DBLP": "conf/nips/GatysEB15",
                "CorpusId": 8643626
            },
            "abstract": "Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.",
            "referenceCount": 31,
            "citationCount": 1180,
            "influentialCitationCount": 116,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2015-05-27",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Gatys2015TextureSU,\n author = {Leon A. Gatys and Alexander S. Ecker and M. Bethge},\n booktitle = {Neural Information Processing Systems},\n pages = {262-270},\n title = {Texture Synthesis Using Convolutional Neural Networks},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
            "@type": "ScholarlyArticle",
            "paperId": "3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
            "corpusId": 11383178,
            "url": "https://www.semanticscholar.org/paper/3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
            "title": "Importance Weighted Autoencoders",
            "venue": "International Conference on Learning Representations",
            "publicationVenue": {
                "id": "urn:research:939c6e1d-0d17-4d6e-8a82-66d960df0e40",
                "name": "International Conference on Learning Representations",
                "alternate_names": [
                    "Int Conf Learn Represent",
                    "ICLR"
                ],
                "issn": null,
                "url": "https://iclr.cc/"
            },
            "year": 2015,
            "externalIds": {
                "MAG": "1779483307",
                "DBLP": "journals/corr/BurdaGS15",
                "ArXiv": "1509.00519",
                "CorpusId": 11383178
            },
            "abstract": "The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",
            "referenceCount": 34,
            "citationCount": 1125,
            "influentialCitationCount": 275,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2015-09-01",
            "journal": {
                "name": "CoRR",
                "volume": "abs/1509.00519"
            },
            "citationStyles": {
                "bibtex": "@Article{Burda2015ImportanceWA,\n author = {Yuri Burda and R. Grosse and R. Salakhutdinov},\n booktitle = {International Conference on Learning Representations},\n journal = {CoRR},\n title = {Importance Weighted Autoencoders},\n volume = {abs/1509.00519},\n year = {2015}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:2a215755d7548ffc82079ce734c4ac60b62f6f56",
            "@type": "ScholarlyArticle",
            "paperId": "2a215755d7548ffc82079ce734c4ac60b62f6f56",
            "corpusId": 20981275,
            "url": "https://www.semanticscholar.org/paper/2a215755d7548ffc82079ce734c4ac60b62f6f56",
            "title": "Toward Controlled Generation of Text",
            "venue": "International Conference on Machine Learning",
            "publicationVenue": {
                "id": "urn:research:fc0a208c-acb7-47dc-a0d4-af8190e21d29",
                "name": "International Conference on Machine Learning",
                "alternate_names": [
                    "ICML",
                    "Int Conf Mach Learn"
                ],
                "issn": null,
                "url": "https://icml.cc/"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1703.00955",
                "DBLP": "conf/icml/HuYLSX17",
                "MAG": "2953029759",
                "CorpusId": 20981275
            },
            "abstract": "Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible natural language sentences, whose attributes are dynamically controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for effective imposition of semantic structures. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns highly interpretable representations from even only word annotations, and produces realistic sentences with desired attributes. Quantitative evaluation validates the accuracy of sentence and attribute generation.",
            "referenceCount": 41,
            "citationCount": 890,
            "influentialCitationCount": 102,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-03-02",
            "journal": {
                "name": null,
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hu2017TowardCG,\n author = {Zhiting Hu and Zichao Yang and Xiaodan Liang and R. Salakhutdinov and E. Xing},\n booktitle = {International Conference on Machine Learning},\n pages = {1587-1596},\n title = {Toward Controlled Generation of Text},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:7bcebd481bb1161843efdd135e4ba59dfac4b61c",
            "@type": "ScholarlyArticle",
            "paperId": "7bcebd481bb1161843efdd135e4ba59dfac4b61c",
            "corpusId": 810708,
            "url": "https://www.semanticscholar.org/paper/7bcebd481bb1161843efdd135e4ba59dfac4b61c",
            "title": "Age Progression/Regression by Conditional Adversarial Autoencoder",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2017,
            "externalIds": {
                "ArXiv": "1702.08423",
                "DBLP": "conf/cvpr/ZhangSQ17",
                "MAG": "2952288113",
                "DOI": "10.1109/CVPR.2017.463",
                "CorpusId": 810708
            },
            "abstract": "If I provide you a face image of mine (without telling you the actual age when I took the picture) and a large amount of face images that I crawled (containing labeled faces of different ages but not necessarily paired), can you show me what I would look like when I am 80 or what I was like when I was 5? The answer is probably a No. Most existing face aging works attempt to learn the transformation between age groups and thus would require the paired samples as well as the labeled query image. In this paper, we look at the problem from a generative modeling perspective such that no paired samples is required. In addition, given an unlabeled image, the generative model can directly produce the image with desired age attribute. We propose a conditional adversarial autoencoder (CAAE) that learns a face manifold, traversing on which smooth age progression and regression can be realized simultaneously. In CAAE, the face is first mapped to a latent vector through a convolutional encoder, and then the vector is projected to the face manifold conditional on age through a deconvolutional generator. The latent vector preserves personalized face features (i.e., personality) and the age condition controls progression vs. regression. Two adversarial networks are imposed on the encoder and generator, respectively, forcing to generate more photo-realistic faces. Experimental results demonstrate the appealing performance and flexibility of the proposed framework by comparing with the state-of-the-art and ground truth.",
            "referenceCount": 33,
            "citationCount": 861,
            "influentialCitationCount": 198,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1702.08423",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Mathematics",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2017-02-27",
            "journal": {
                "name": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Zhang2017AgePB,\n author = {Zhifei Zhang and Yang Song and H. Qi},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {4352-4360},\n title = {Age Progression/Regression by Conditional Adversarial Autoencoder},\n year = {2017}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
            "@type": "ScholarlyArticle",
            "paperId": "97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
            "corpusId": 2429016,
            "url": "https://www.semanticscholar.org/paper/97e7c94a78ae17cfb90848c1cfca8c431082a7b2",
            "title": "Learning Temporal Regularity in Video Sequences",
            "venue": "Computer Vision and Pattern Recognition",
            "publicationVenue": {
                "id": "urn:research:768b87bb-8a18-4d9c-a161-4d483c776bcf",
                "name": "Computer Vision and Pattern Recognition",
                "alternate_names": [
                    "CVPR",
                    "Comput Vis Pattern Recognit"
                ],
                "issn": "1063-6919",
                "url": "https://ieeexplore.ieee.org/xpl/conhome.jsp?punumber=1000147"
            },
            "year": 2016,
            "externalIds": {
                "MAG": "2950773078",
                "ArXiv": "1604.04574",
                "DBLP": "journals/corr/0003CNRD16",
                "DOI": "10.1109/CVPR.2016.86",
                "CorpusId": 2429016
            },
            "abstract": "Perceiving meaningful activities in a long video sequence is a challenging problem due to ambiguous definition of 'meaningfulness' as well as clutters in the scene. We approach this problem by learning a generative model for regular motion patterns (termed as regularity) using multiple sources with very limited supervision. Specifically, we propose two methods that are built upon the autoencoders for their ability to work with little to no supervision. We first leverage the conventional handcrafted spatio-temporal local features and learn a fully connected autoencoder on them. Second, we build a fully convolutional feed-forward autoencoder to learn both the local features and the classifiers as an end-to-end learning framework. Our model can capture the regularities from multiple datasets. We evaluate our methods in both qualitative and quantitative ways - showing the learned regularity of videos in various aspects and demonstrating competitive performance on anomaly detection datasets as an application.",
            "referenceCount": 61,
            "citationCount": 872,
            "influentialCitationCount": 182,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/1604.04574",
                "status": "GREEN"
            },
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Computer Science",
                    "source": "external"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle",
                "Conference"
            ],
            "publicationDate": "2016-04-15",
            "journal": {
                "name": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
                "volume": null
            },
            "citationStyles": {
                "bibtex": "@Article{Hasan2016LearningTR,\n author = {Mahmudul Hasan and Jonghyun Choi and J. Neumann and A. Roy-Chowdhury and L. Davis},\n booktitle = {Computer Vision and Pattern Recognition},\n journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n pages = {733-742},\n title = {Learning Temporal Regularity in Video Sequences},\n year = {2016}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "@type": "ScholarlyArticle",
            "paperId": "d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "corpusId": 201716327,
            "url": "https://www.semanticscholar.org/paper/d44ac0a7fd4734187bccafc4a2771027b8bb595e",
            "title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "venue": "Nature Biotechnology",
            "publicationVenue": {
                "id": "urn:research:458166b3-de17-4bf3-bbbb-e53782de2f0f",
                "name": "Nature Biotechnology",
                "alternate_names": [
                    "Nat Biotechnol"
                ],
                "issn": "1087-0156",
                "url": "http://www.nature.com/nbt/"
            },
            "year": 2019,
            "externalIds": {
                "MAG": "2971690404",
                "DOI": "10.1038/s41587-019-0224-x",
                "CorpusId": 201716327,
                "PubMed": "31477924"
            },
            "abstract": null,
            "referenceCount": 28,
            "citationCount": 635,
            "influentialCitationCount": 14,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Medicine",
                "Biology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Medicine",
                    "source": "external"
                },
                {
                    "category": "Biology",
                    "source": "external"
                },
                {
                    "category": "Medicine",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Chemistry",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Computer Science",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2019-09-01",
            "journal": {
                "name": "Nature Biotechnology",
                "volume": "37"
            },
            "citationStyles": {
                "bibtex": "@Article{Zhavoronkov2019DeepLE,\n author = {A. Zhavoronkov and Y. Ivanenkov and A. Aliper and M. Veselov and V. Aladinskiy and Anastasiya V Aladinskaya and V. Terentiev and Daniil Polykovskiy and Maksim Kuznetsov and Arip Asadulaev and Yury Volkov and Artem Zholus and Shayakhmetov Rim and Alexander Zhebrak and L. Minaeva and B. Zagribelnyy and Lennart H Lee and R. Soll and D. Madge and Li Xing and Tao Guo and Al\u00e1n Aspuru-Guzik},\n booktitle = {Nature Biotechnology},\n journal = {Nature Biotechnology},\n pages = {1038 - 1040},\n title = {Deep learning enables rapid identification of potent DDR1 kinase inhibitors},\n volume = {37},\n year = {2019}\n}\n"
            }
        }
    },
    {
        "public": {
            "@context": "https://schema.org",
            "@id": "urn:research:4ba0285cd51aa70ac7f80bc5da2ce52c23d4e7d1",
            "@type": "ScholarlyArticle",
            "paperId": "4ba0285cd51aa70ac7f80bc5da2ce52c23d4e7d1",
            "corpusId": 145485598,
            "url": "https://www.semanticscholar.org/paper/4ba0285cd51aa70ac7f80bc5da2ce52c23d4e7d1",
            "title": "Toward a Theory of Generative Change in Culturally and Linguistically Complex Classrooms",
            "venue": "",
            "publicationVenue": {
                "id": null,
                "name": null,
                "alternate_names": null,
                "issn": null,
                "url": null
            },
            "year": 2009,
            "externalIds": {
                "MAG": "2155568376",
                "DOI": "10.3102/0002831208323277",
                "CorpusId": 145485598
            },
            "abstract": "This article situates the preparation of teachers to teach in culturally and linguistically complex classrooms in international contexts. It investigates long-term social and institutional effects of professional development and documents processes that facilitate teachers\u2019 continued learning. Data from a decade-long study of U.S. and South African teachers supported a model of generative change that explained how professional development could be internalized by teachers, subsequently serving as a heuristic to help them organize their individual programs of instruction. Drawing primarily on two case studies, this article documents teachers\u2019 development of generative knowledge and illustrates how they drew on that knowledge in thinking about students and teaching. The results were to facilitate generative thinking on the part of their students as well.",
            "referenceCount": 45,
            "citationCount": 219,
            "influentialCitationCount": 28,
            "isOpenAccess": false,
            "openAccessPdf": null,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "s2FieldsOfStudy": [
                {
                    "category": "Psychology",
                    "source": "external"
                },
                {
                    "category": "Education",
                    "source": "s2-fos-model"
                },
                {
                    "category": "Linguistics",
                    "source": "s2-fos-model"
                }
            ],
            "publicationTypes": [
                "JournalArticle"
            ],
            "publicationDate": "2009-03-01",
            "journal": {
                "name": "American Educational Research Journal",
                "volume": "46"
            },
            "citationStyles": {
                "bibtex": "@Article{Ball2009TowardAT,\n author = {A. Ball},\n journal = {American Educational Research Journal},\n pages = {45 - 72},\n title = {Toward a Theory of Generative Change in Culturally and Linguistically Complex Classrooms},\n volume = {46},\n year = {2009}\n}\n"
            }
        }
    }
]